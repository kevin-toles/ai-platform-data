{
  "metadata": {
    "title": "Machine Learning Design Patterns",
    "source_file": "Machine Learning Design Patterns_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "summary": "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Machine Learning Design Patterns, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. The views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWho Is This Book For?\nIntroductory machine learning books usually focus on the what and how of machine learning (ML).\nThis book, on the other hand, brings together hard-earned experience around the “why” that underlies the tips and tricks that experienced ML practitioners employ when applying machine learning to real-world problems.\nInstead, this book is for you if you are a data scientist, data engineer, or ML engineer who is looking for a second book on practical machine learning.\nWe purposefully do not discuss areas of active research—you will find very little here, for example, on machine learning model architecture (bidirectional encoders, or the attention mechanism, or short-circuit layers, for example) because we assume that you will be using a pre-built model architecture (such as ResNet-50 or GRUCell), not writing your own image classification or recurrent neural network.\nnetworks in this book.\nIn this book, we have tried to include only common patterns of the kind that machine learning engineers in enterprises will employ in their day-to-day work.\nIt is for a pragmatic practitioner in machine learning that this book is written.\nWe provide code for machine learning (sometimes in Keras/TensorFlow, and other times in scikit-learn or BigQuery ML) and data processing (in SQL) as a way to show how the techniques we are discussing are implemented in practice.\nAll the code that is referenced in the book is part of our GitHub repository, where you will find fully working ML models.\nOur aim has been that the topic and principles should remain relevant regardless of changes to TensorFlow or Keras, and we can easily imagine updating the GitHub repository to include other ML frameworks, for example, while keeping the book text unchanged.\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\nIn general, if example code is offered with this book, you may use it in your programs and documentation.\nFor example, writing a program that uses several chunks of code from this book does not require permission.\nSelling or distributing examples from O’Reilly books does require permission.\nAnswering a question by citing this book and quoting example code does not require permission.\nIncorporating a significant amount of example code from this book into your product’s documentation does require permission.\nFor example: “Machine Learning Design Patterns by Valliappa Lakshmanan, Sara Robinson, and Michael Munn (O’Reilly).\nCopyright 2021 Valliappa Lakshmanan, Sara Robinson, and Michael Munn, 978-1-098-11578-4.” If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.",
      "keywords": [
        "Machine Learning",
        "Machine Learning Design",
        "Learning Design Patterns",
        "Design Patterns Solutions",
        "Book",
        "Learning",
        "Sara Robinson",
        "Learning Design",
        "Valliappa Lakshmanan",
        "Machine",
        "Michael Munn",
        "machine learning books",
        "Patterns Solutions",
        "Design Patterns",
        "MLOps Valliappa Lakshmanan"
      ],
      "concepts": [
        "model",
        "data",
        "learning",
        "examples",
        "permission",
        "permissions",
        "work",
        "machine",
        "editor",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "permission",
          "learning",
          "machine",
          "reilly"
        ],
        "semantic": [],
        "merged": [
          "book",
          "permission",
          "learning",
          "machine",
          "reilly"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28562232449813735,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.724789+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "summary": "Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform.\nO’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers.\nFor news and information about our books and courses, visit http://oreilly.com.\nA book like this would not be possible without the generosity of numerous Googlers, especially our colleagues in the Cloud AI, Solution Engineering, Professional Services, and Developer Relations teams.\nThanks to our managers, Karl Weinmeister, Steve Cellini, Hamidou Dia, Abdul Razack, Chris Hallenbeck, Patrick Cole, Louise Byrne, and Rochana Golani for fostering the spirit of openness within Google, giving us the freedom to catalog these patterns, and publish this book.\nO’Reilly is the publisher of choice for technical books, and the professionalism of our team illustrates why.\nTo Phil, thank you for patiently bearing with my less-than-bearable schedule while working on this book.\nThe Need for Machine Learning Design Patterns\nIn engineering disciplines, design patterns capture best practices and solutions to commonly occurring problems.\nThis book is a catalog of machine learning design patterns that we have observed in the course of working with hundreds of machine learning teams.\nThe idea of patterns, and a catalog of proven patterns, was introduced in the field of architecture by Christopher Alexander and five coauthors in a hugely influential book titled A Pattern Language (Oxford University Press, 1977).\nIn their book, they catalog 253 patterns, introducing them this way:\nEach pattern describes a problem which occurs over and over again in our environment, and then describes the core of the solution to that problem, in such a way that you can use this solution a million times over, without ever doing it the same way twice.\nFor example, a couple of the patterns that incorporate human details when building a home are Light on Two Sides of Every Room and Six-Foot Balcony.\nErich Gamma, Richard Helm, Ralph Johnson, and John Vlissides brought the idea to software by cataloging 23 object-oriented design patterns in a 1994 book entitled Design Patterns: Elements of Reusable Object-Oriented Software (Addison-Wesley, 1995).\nBuilding production machine learning models is increasingly becoming an engineering discipline, taking advantage of ML methods that have been proven in research settings and applying them to business problems.\nOne benefit of our jobs in the customer-facing part of Google Cloud is that it brings us in contact with a wide variety of machine learning and data science teams and individual developers from around the world.\nFinally, we have been fortunate to work with the TensorFlow, Keras, BigQuery ML, TPU, and Cloud AI Platform teams that are driving the democratization of machine learning research and infrastructure.\nThis book is a catalog of design patterns or repeatable solutions to commonly occurring problems in ML engineering.\nFor each pattern, we describe the commonly occurring problem that is being addressed and then walk through a variety of potential solutions to the problem, the trade-offs of these solutions, and recommendations for choosing between these solutions.\nThis is a catalog of patterns that we have observed in practice, among multiple teams.\ncolleagues, and refer back to the book when faced with problems you remember reading about.\nEach pattern has a brief problem statement, a canonical solution, an explanation of why the solution works, and a many-part discussion on tradeoffs and alternatives.\nMachine learning models are algorithms that learn patterns from data.",
      "keywords": [
        "width italic Shows",
        "machine learning",
        "book",
        "italic Shows text",
        "Design Patterns",
        "Learning Design Patterns",
        "patterns",
        "Learning",
        "Machine Learning Design",
        "italic Shows",
        "machine",
        "online learning platform",
        "machine learning models",
        "Online Learning NOTE",
        "design"
      ],
      "concepts": [
        "patterns",
        "learning",
        "books",
        "thanks",
        "problems",
        "teams",
        "solution",
        "solutions",
        "engineering",
        "engineers"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "patterns",
          "book",
          "learning",
          "catalog",
          "design patterns"
        ],
        "semantic": [],
        "merged": [
          "patterns",
          "book",
          "learning",
          "catalog",
          "design patterns"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31305633836518537,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.724887+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 18-27)",
      "start_page": 18,
      "end_page": 27,
      "summary": "Instead, we can train a machine learning model to estimate moving costs based on past data on previous households our company has moved.\nscikit-learn, XGBoost, and PyTorch, which are other popular open source frameworks that provide utilities for preparing your data, along with APIs for building linear and deep models.\nWe’ll use BigQuery ML as an example of this, especially in situations where we want to combine data preprocessing and model creation.\nLinear models represent the patterns they’ve learned from data using a linear function.\nDecision trees are machine learning models that use your data to create a subset of paths with various branches.\nYou feed this labeled data to your model in hopes that it can learn enough to label new examples.\nWith unsupervised learning, you do not know the labels for your data in advance, and the goal is to build a model that can find natural groupings of your data (called clustering), compress the information content (dimensionality reduction), or find association rules.\nWhen we talk about datasets, we’re referring to the data used for training, validating, and testing a machine learning model.\nThe bulk of your data will be training data: the data fed to your model during the training process.\nPerformance reports of the machine learning model must be computed on the independent test data, rather than the training or validation tests.\nThe data you use to train your model can take many forms depending on the model type.\nNumeric data can often be fed directly to a machine learning model, where other data requires various data preprocessing before it’s ready to be sent to a model.\nA training example refers to a single instance (row) of data from your dataset that will be fed to your model.\nOnce you’ve assembled your dataset and determined the features for your model, data validation is the process of computing statistics on your data, understanding your schema, and evaluating the dataset to identify problems like drift and training-serving skew.\nThe first step in a typical machine learning workflow is training—the process of passing training data to a model so that it can learn to identify patterns.\nAfter training, the next step in the process is testing how your model performs on data outside of your training set.\nThe process of sending new data to your model and making use of its output is called prediction.\nOften, the processes of collecting training data, feature engineering, training, and evaluating your model are handled separately from the production pipeline.\nIn other situations, you may have new data being ingested continuously and need to process this data immediately before sending it to your model for training or prediction.\nData and Model Tooling\nTo build some of the models in our examples, we’ll use BigQuery Machine Learning (or BigQuery ML).\nBigQuery ML is a tool for building models from data stored in BigQuery.\nCloud AI Platform includes a variety of products for training and serving custom machine learning models on Google Cloud.\nAs it relates to machine learning, a data scientist may work on data collection, feature engineering, model building, and more.\nData scientists often work in Python or R in a notebook environment, and are usually the first to build out an organization’s machine learning models.\nMachine learning engineers do similar tasks to data engineers, but for ML models.\nThey take models developed by data scientists, and manage the infrastructure and operations around training and deploying those models.\nThen, you transition to the data scientist role and build the ML model(s).\nResearch scientists, data analysts, and developers may also build and use AI models, but these job roles are not a focus audience for this book.\nFigure 1-2 illustrates how these different roles work together throughout an organization’s machine learning model development process.\nThere are many different job roles related to data and machine learning, and these roles collaborate on the ML workflow, from data ingestion to model serving and the end user interface.\nMachine learning models are only as reliable as the data used to train them.\nIf you train a machine learning model on an incomplete dataset, on data with poorly selected features, or on data that doesn’t accurately represent the population using the model, your model’s predictions will be a direct reflection of that data.\nAs a result, machine learning models are often referred to as “garbage in, garbage out.” Here we’ll highlight four important components of data quality: accuracy, completeness, consistency, and timeliness.",
      "keywords": [
        "data",
        "machine learning",
        "Machine learning models",
        "model",
        "learning",
        "machine",
        "learning models",
        "training",
        "predictions",
        "’ll",
        "training data",
        "process",
        "dataset",
        "data scientists",
        "Google Cloud"
      ],
      "concepts": [
        "data",
        "models",
        "prediction",
        "predicting",
        "learned",
        "train",
        "included",
        "includes",
        "process",
        "processed"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 5,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 43,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "machine learning",
          "machine",
          "learning",
          "training",
          "models"
        ],
        "semantic": [],
        "merged": [
          "machine learning",
          "machine",
          "learning",
          "training",
          "models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32189010726548634,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.724950+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 28-35)",
      "start_page": 28,
      "end_page": 35,
      "summary": "Duplicates in your training dataset, for example, can cause your model to incorrectly assign more weight to these data points.\nYour model relies solely on the ground truth labels in your training data to update its weights and minimize loss.\nAs a result, incorrectly labeled training examples can cause misleading model accuracy.\nFor example, let’s say you’re building a sentiment analysis model and 25% of your “positive” training examples have been incorrectly labeled as “negative.” Your model will have an inaccurate picture of what should be considered negative sentiment, and this will be directly reflected in its predictions.\nTo understand data completeness, let’s say you’re training a model to identify cat breeds.\nAdditionally, there’s no way your model will be able to return “not a cat” if this data and label weren’t included in the training dataset.\nTo look at a tabular data example, if you are building a model to predict the\nTo deal with timeliness, it’s useful to record as much information as possible about a particular data point, and make sure that information is reflected when you transform your data into features for a machine learning model.\nThese weights then converge during training as the model iterates and learns from the data.\nBecause of this, the same model code given the same training data will produce slightly different results across training runs.\nIn order to address this problem of repeatability, it’s common to set the random seed value used by your model to ensure that the same randomness will be applied each time you run training.\nKeep in mind that you’ll need to use the same data and the same random seed when training your model to ensure repeatable, reproducible results across different experiments.\nTraining an ML model involves several artifacts that need to be fixed in order to ensure reproducibility: the data used, the splitting mechanism used to generate datasets for training and validation, data preparation and model hyperparameters, and variables like the batch size and learning rate schedule.\nAs a concrete example, if one version of a framework’s train() method makes 13 calls to rand(), and a newer version of the same framework makes 14 calls, using different versions between experiments will cause slightly different results, even with the same data and model code.\nWhile machine learning models typically represent a static relationship between inputs and outputs, data can change significantly over time.\nData drift refers to the challenge of ensuring your machine learning models stay\nFor example, let’s say you’re training a model to classify news article headlines into categories like “politics,” “business,” and “technology.” If you train and evaluate your model on historical news articles from the 20th century, it likely won’t perform as well on current data.\nHowever, a model trained on historical data would have no knowledge of this word.\nTo solve for drift, it’s important to continually update your training dataset, retrain your model, and modify the weight your model assigns to particular groups of input data.\nFrom this trend, we can see that training a model on data before 2000 to generate predictions on storms today would lead to inaccurate predictions.\nWhen ingesting and preparing data for a machine learning model, the size of the dataset will dictate the tooling required for your solution.\nDepending on the type and size of the dataset, model training can be time consuming and computationally expensive, requiring infrastructure (like GPUs) designed specifically for ML workloads.\nImage models, for instance, typically require much more training infrastructure than models trained entirely on tabular data.",
      "keywords": [
        "model",
        "data",
        "training",
        "training data",
        "machine learning",
        "machine learning model",
        "dataset",
        "cat",
        "learning",
        "learning model",
        "patterns",
        "model training",
        "machine",
        "training dataset",
        "’re"
      ],
      "concepts": [
        "data",
        "model",
        "training",
        "different",
        "differ",
        "differences",
        "differing",
        "likely",
        "labels",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 17,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.487,
          "base_score": 0.337,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.471,
          "base_score": 0.321,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "incorrectly",
          "cat",
          "learning",
          "training model"
        ],
        "semantic": [],
        "merged": [
          "training",
          "incorrectly",
          "cat",
          "learning",
          "training model"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3096187095154631,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725000+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 36-43)",
      "start_page": 36,
      "end_page": 43,
      "summary": "At the heart of any machine learning model is a mathematical function that is defined to operate on specific types of data only.\nAt the same time, real- world machine learning models need to operate on data that may not be directly pluggable into the mathematical function.\nNote that we are talking here about the mathematical core of a decision tree—decision tree machine learning software will typically also include functions to learn an optimal tree from data and ways to read in and process different types of numeric and categorical data.\nThe heart of a decision tree machine learning model to predict whether or not a baby requires intensive care is a mathematical model that operates on boolean variables.\nThis is an example of how input data (hospital, a complex object or baby weight, a floating point number) can be represented in the form (boolean) expected by the model.\nIn this book, we will use the term input to represent the real-world data fed to the model (for example, the baby weight) and the term feature to represent the transformed data that the model actually operates on (for example, whether the baby weight is less than 3 kilograms).\nThe process of creating features to represent the input data is called feature engineering, and so we can think of feature engineering as a way of selecting the data representation.\nOf course, rather than hardcoding parameters such as the threshold value of 3 kilograms, we’d prefer the machine learning model to learn how to create each node by selecting the input variable and the threshold.\nDecision trees are an example of machine learning models that are capable of learning the data representation.\nThe process of learning features to represent the input data is called feature extraction, and we can think of\nThe data representation doesn’t even need to be of a single input variable— an oblique decision tree, for example, creates a boolean feature by thresholding a linear combination of two or more input variables.\nA decision tree classifier where each node can threshold only one input value (x1 or x2) will result in a stepwise linear boundary function, whereas an oblique tree classifier where a node can threshold a linear combination of input variables will result in a piecewise linear boundary function.\nThe Hashed Feature design pattern is deterministic, but doesn’t require a model to know all the potential values that a particular input can take.\nMost modern, large-scale machine learning models (random forests, support vector machines, neural networks) operate on numerical values, and so if our input is numeric, we can pass it through to the model unchanged.\nOften, because the ML framework uses an optimizer that is tuned to work well with numbers in the [–1, 1] range, scaling the numeric values to lie in that range can be beneficial.\nThe numeric value is linearly scaled so that the minimum value that the input can take is scaled to –1 and the maximum possible value to 1:\nThe problem with min-max scaling is that the maximum and minimum value (max_x1 and min_x1) have to be estimated from the training dataset, and they are often outlier values.\nAddresses the problem of outliers without requiring prior knowledge of what the reasonable range is by linearly scaling the input using the mean and standard deviation estimated over the training dataset:\nAll the methods discussed so far scale the data linearly (in the case of clipping and winsorizing, linear within the typical range).",
      "keywords": [
        "Data",
        "decision tree",
        "Data Representation",
        "input",
        "machine learning model",
        "machine learning",
        "model",
        "input data",
        "tree",
        "learning",
        "feature",
        "decision",
        "raw",
        "linear",
        "decision tree machine"
      ],
      "concepts": [
        "value",
        "data",
        "feature",
        "scaling",
        "scale",
        "inputs",
        "linear",
        "function",
        "functions",
        "models"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 3,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.545,
          "base_score": 0.395,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 46,
          "title": "",
          "score": 0.531,
          "base_score": 0.381,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.523,
          "base_score": 0.373,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.487,
          "base_score": 0.337,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tree",
          "decision tree",
          "decision",
          "input",
          "mathematical"
        ],
        "semantic": [],
        "merged": [
          "tree",
          "decision tree",
          "decision",
          "input",
          "mathematical"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2517704371730468,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725061+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 44-51)",
      "start_page": 44,
      "end_page": 51,
      "summary": "However, we are not justified in throwing away rows where mother_age is 50 because 50 is a perfectly valid input and we expect to encounter 50-year-old mothers once the model is deployed in production.\nIn Figure 2-3, note that minmax_scaled gets the x values into the desired range of [–1, 1] but continues to retain values at the extreme ends of the distribution where there are not enough examples.\nClipping rolls up many of the problematic values, but requires getting the clipping thresholds exactly correct—here, the slow decline in the number of babies with mothers’ ages above 40 poses problems in setting a hard threshold.\nThe histogram of mother_age in the baby weight prediction example is shown in the top- left panel, and different scaling functions (see the x-axis label) are shown in the remaining panels.\nIn that case, it is better to apply a nonlinear transform to the input before scaling it.\nOne common trick is to take the logarithm of the input value before scaling it.\nThe number of views of pages in Wikipedia is, however, highly skewed and occupies a large dynamic range (see the left panel of Figure 2-4: the distribution is highly skewed toward rarely viewed pages, but the most common pages are viewed tens of millions of times).\nThe second panel demonstrates that problems can be addressed by transforming the number of views using the logarithm, a power function, and linear scaling in succession.\nSometimes, the input data is an array of numbers.\nFor example, one of the inputs to the model to predict the sales of a nonfiction book might be the sales of all previous books on the topic.\nAn example input might be:\nIf the array is ordered in a specific way (for example, in order of time or by size), representing the input array by the last three or some other fixed number of items.\nBy treating the sales of previous books as an array input, we are assuming that the most important factors in predicting a book’s sales are characteristics of the book itself (author, publisher, reviews, and so on) and not the temporal continuity of the sales amounts.\nCategorical Inputs\nBecause most modern, large-scale machine learning models (random forests, support vector machines, neural networks) operate on numerical values, categorical inputs have to be represented as numbers.\nSuppose that one of the inputs to the model that predicts the sales of a nonfiction book is the language that the book is written in.\nCategorical input Numeric feature\nBecause there is no ordinal relationship between languages, we need to use a categorical to numeric mapping that allows the model to learn the market for books written in these languages independently.\nIn our example, the categorical input variable would be converted into a three-element feature vector using the following mapping:\nCategorical input Numeric feature\nOne-hot encoding requires us to know the vocabulary of the categorical input beforehand.\nCategorical input Numeric feature\nBecause dummy coding is a more compact representation, it is preferred in statistical models that perform better when the inputs are linearly independent.\nCategorical input Numeric feature\nIn some circumstances, it can be helpful to treat a numeric input as categorical and map it to a one-hot encoded column:\nSuch a mapping where the number of distinct inputs (here, seven) is",
      "keywords": [
        "Categorical input Numeric",
        "input Numeric feature",
        "input",
        "Categorical input",
        "Numeric feature English",
        "Categorical",
        "input Numeric",
        "feature",
        "Array",
        "input array",
        "book",
        "feature English",
        "model",
        "number",
        "Numeric"
      ],
      "concepts": [
        "value",
        "input",
        "feature",
        "scaling",
        "scale",
        "transformations",
        "transform",
        "distribution",
        "distributed",
        "distributions"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 38,
          "title": "",
          "score": 0.41,
          "base_score": 0.26,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 19,
          "title": "",
          "score": 0.379,
          "base_score": 0.379,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "categorical input",
          "categorical",
          "input numeric",
          "numeric",
          "numeric feature"
        ],
        "semantic": [],
        "merged": [
          "categorical input",
          "categorical",
          "input numeric",
          "numeric",
          "numeric feature"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.17672630288868227,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725101+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 52-61)",
      "start_page": 52,
      "end_page": 61,
      "summary": "The Hashed Feature design pattern addresses three possible problems associated with categorical features: incomplete vocabulary, model size due\nOne of the inputs to the model is the departure airport.\nSome airports had as few as one to three flights over the entire time period, and so we expect that the training data vocabulary will be incomplete.\nAll three problems (incomplete vocabulary, high cardinality, cold start) will exist if we one-hot encode the departure airport.\nThe Hashed Feature design pattern represents a categorical input variable by doing the following:\ntf.feature_column.categorical_column_with_hash_bucket( airport, num_buckets, dtype=tf.dtypes.string)\nFor example, Table 2-1 shows the FarmHash of some IATA airport codes when hashed into 3, 10, and 1,000 buckets.\nThe FarmHash of some IATA airport codes when hashed into different numbers of buckets\nAssume that we have chosen to hash the airport code using 10 buckets (hash10 in Table 2-1).\nEven if an airport with a handful of flights is not part of the training dataset, its hashed feature value will be in the range [0–9].\nTherefore, there is no resilience problem during serving—the unknown airport will get the predictions corresponding with other airports in the hash bucket.\nIf we have 347 airports, an average of 35 airports will get the same hash bucket code if we hash it into 10 buckets.\nAn airport that is missing from the training dataset will “borrow” its characteristics from the other similar ~35 airports in the hash bucket.\nChoose the number of hash buckets by balancing the need to handle out-of- vocabulary inputs reasonably and the need to have the model accurately reflect the categorical input.\nWith 10 hash buckets, ~35 airports get commingled.\nIt’s easy to see that the high cardinality problem is addressed as long as we choose a small enough number of hash buckets.\nEven if we have millions of airports or hospitals or physicians, we can hash them into a few hundred buckets, thus keeping the system’s memory and model size requirements practical.\nWe don’t need to store the vocabulary because the transformation code is independent of the actual data value and the core of the model only deals with num_buckets inputs, not the full vocabulary.\nIt is true that hashing is lossy—since we have 347 airports, an average of 35 airports will get the same hash bucket code if we hash it into 10 buckets.\nIf a new airport gets added to the system, it will initially get the predictions corresponding to other airports in the hash bucket.\nBy choosing a hash bucket size of 100, we are choosing to have 3–4 airports share a bucket.\nWe are explicitly compromising on the ability to accurately represent the data (with a fixed vocabulary and one-hot encoding) in order to handle out-of-vocabulary inputs, cardinality/model size constraints, and cold-start problems.\nDo not choose Hashed Feature if you know the vocabulary beforehand, if the vocabulary size is relatively small (in the thousands is acceptable for a dataset with millions of examples), and if cold start is not a concern.\nEven if we raise the number of buckets to 100,000 with only 347 airports, the probability that at least two airports share the same hash bucket is 45%—unacceptably high (see Table 2-2).\nTherefore, we should use Hashed Features only if we are willing to tolerate multiple categorical inputs sharing the same hash bucket value.\nThe expected number of entries per bucket and the probability of at least one collision when IATA airport codes are hashed into different numbers of buckets\nConsider the case of the hash bucket that contains ORD (Chicago, one of the busiest airports in the world).\nSELECT departure_airport, num_flights FROM airports WHERE hashed(departure_airport, 100) = hashed('ORD', 100)\nThe model accuracy for BTV and MCI (Kansas City airport) will be quite poor because there are so many flights out of Chicago.\nIn cases where the distribution of a categorical variable is skewed or where the number of buckets is so small that bucket collisions are frequent, we might find it helpful to add an aggregate feature as an input to our model.\nFor example, for every airport, we could find the probability of on-time flights in the training dataset and add it as a feature to our model.\nIn some cases, we might be able to avoid using the airport name as a feature entirely, since the relative frequency of on-time flights might be sufficient.",
      "keywords": [
        "airport",
        "hash bucket",
        "Hashed Feature",
        "buckets",
        "hash",
        "feature",
        "model",
        "Hashed Feature design",
        "categorical",
        "IATA airport codes",
        "number",
        "vocabulary",
        "Array",
        "hash bucket code",
        "categorical input"
      ],
      "concepts": [
        "airport",
        "hashed",
        "buckets",
        "input",
        "vocabulary",
        "feature",
        "model",
        "problems",
        "frequency",
        "baby"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 8,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 39,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.431,
          "base_score": 0.431,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hash",
          "airports",
          "airport",
          "hash bucket",
          "buckets"
        ],
        "semantic": [],
        "merged": [
          "hash",
          "airports",
          "airport",
          "hash bucket",
          "buckets"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30776677466405544,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725149+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 62-71)",
      "start_page": 62,
      "end_page": 71,
      "summary": "However, binary encoding does not solve the problem of out-of-vocabulary inputs or cold start (only the problem of high cardinality).\nIn the Hashed Feature design pattern, we have to use a fingerprint hashing algorithm and not a cryptographic hashing algorithm.\nIf you think about it, this is a key requirement of preprocessing functions in machine learning, since we need to apply the same function during model serving and get the same hashed value.\nTherefore, a cryptographic hash is not usable in a feature engineering context where the hashed value computed for a given input during prediction has to be the same as the hash computed during training, and where the hash function should not slow down the machine learning model.\nEmbeddings are a learnable data representation that map high-cardinality data into a lower-dimensional space in such a way that the information relevant to the learning problem is preserved.\nMachine learning models systematically look for patterns in data that capture how the properties of the model’s input features relate to the output label.\nWhile handling structured, numeric input is fairly straightforward, the data needed to train a machine learning model can come in myriad varieties, such as categorical features, text, images, audio, time series, and many more.\nFor these data representations, we need a meaningful numeric value to supply our machine learning model so these features can fit within the typical training paradigm.\nOne-hot encoding is a common way to represent categorical input variables.\nThis is a categorical input that has six possible values: ['Single(1)', 'Multiple(2+)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)'].\nWe can handle this categorical input using a one-hot 6 encoding that maps each potential input string value to a unit vector in R , as shown in Table 2-3.\nAn example of one-hot encoding categorical inputs for the natality dataset\nOne- hot encoding high-cardinality categorical features like video_ids or\nThe Embeddings design pattern addresses the problem of representing high- cardinality data densely in a lower dimension by passing the input data through an embedding layer that has trainable weights.\nBecause embeddings capture closeness relationships in the input data in a lower- dimensional representation, we can use an embedding layer as a replacement for clustering techniques (e.g., customer segmentation) and dimensionality reduction methods like principal components analysis (PCA).\nThe weights in the embedding layer would be learned as part of the gradient descent procedure when training the natality model.\nAt the end of training, the weights of the embedding layer might be such that the encoding for the categorical variables is as shown in Table 2-5.\nOne-hot and learned encodings for the plurality column in the natality dataset\nThe embedding maps a sparse, one-hot encoded vector to a dense vector in 2 R .\nplurality = tf.feature_column.categorical_column_with_vocabulary_list( 'plurality', ['Single(1)', 'Multiple(2+)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)']) plurality_embed = tf.feature_column.embedding_column(plurality, dimension=2)\nThe resulting feature column (plurality_embed) is used as input to the downstream nodes of the neural network instead of the one-hot encoded feature column (plurality).\nTherefore, we use a dense word embedding to vectorize the discrete text input before passing to our model.\nTo implement a text embedding in Keras, we first create a tokenization for each word in our vocabulary, as shown in Figure 2-6.\nThen we use this tokenization to map to an embedding layer, similar to how it was done for the plurality column.\nThe tokenizer creates a lookup table that maps each word to an index.\nThe tokenization is a lookup table that maps each word in our vocabulary to an index.\nWe can think of this as a one-hot encoding of each word where the tokenized index is the location of the nonzero element in the one-hot encoding.\nThis maps each sequence of words in the text input being represented (here, we assume that they are titles of articles) to a sequence of tokens corresponding to each word as in Figure 2-7:\nUsing the tokenizer, each title is mapped to a sequence of integer index values.\nThe function create_sequences takes both titles as well as the maximum sentence length as input and returns a list of the integers corresponding to our tokens padded to the sentence maximum length:",
      "keywords": [
        "encoding",
        "One-hot encoding",
        "embedding",
        "input",
        "embedding layer",
        "plurality",
        "fingerprint",
        "model",
        "data",
        "hash",
        "Feature",
        "Plurality One-hot encoding",
        "word",
        "farm fingerprint",
        "One-hot"
      ],
      "concepts": [
        "embeddings",
        "feature",
        "hashed",
        "hashes",
        "values",
        "tokenization",
        "text",
        "model",
        "data",
        "inputs"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.478,
          "base_score": 0.478,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.437,
          "base_score": 0.437,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 46,
          "title": "",
          "score": 0.417,
          "base_score": 0.417,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 25,
          "title": "",
          "score": 0.392,
          "base_score": 0.392,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hot",
          "encoding",
          "plurality",
          "hot encoding",
          "embedding"
        ],
        "semantic": [],
        "merged": [
          "hot",
          "encoding",
          "plurality",
          "hot encoding",
          "embedding"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2821255168016617,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.725195+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 72-80)",
      "start_page": 72,
      "end_page": 80,
      "summary": "Next, we’ll build a deep neural network (DNN) model in Keras that implements a simple embedding layer to transform the word integers into dense vectors.\nThis feature vector contains all the relevant information of the image so it is essentially a low-dimensional embedding of the input image.\nFor the image translation task, the encoder produces a low-dimensional embedding representation of the image.\nBy training this model architecture on a massive dataset of image/caption pairs, the encoder learns an efficient vector representation for images.\nThis means that the resulting vector embeddings represent the most efficient low-dimensional representation of those feature values with respect to the learning task.\nBy forcing our categorical variable into a lower-dimensional embedding space, we can also learn relationships between the different categories.\nSingle(1) Multiple(2+) Twins(2) Triplets(3) Quadruplets(4) Quintuplets(5)\nWhen the features are embedded in two dimensions, the similarity matrix gives us more information\nSingle(1) Multiple(2+) Twins(2) Triplets(3) Quadruplets(4) Quintuplets(5)\nThus, a learned embedding allows us to extract inherent similarities between two separate categories and, given there is a numeric vector representation, we can precisely quantify the similarity between two categorical features.\nFurthermore, these user and item embeddings can be combined with other features when training a separate machine learning model.\nUsing pre-trained embeddings in machine learning models is referred to as transfer learning.\nBy learning a low-dimensional, dense embedding vector for each customer and video, an embedding-based model is able to generalize well with less of a manual feature engineering burden.\nBy choosing a very small output dimension of an embedding layer, too much information is forced into a small vector space and context can be lost.\nOn the other hand, when the embedding dimension is too large, the embedding loses the learned contextual importance of the features.\nThe optimal embedding dimension is often found through experimentation, similar to choosing the number of neurons in a deep neural network layer.\nFor an image classification model like Inception to be able to produce useful image embeddings, it is trained on ImageNet, which has 14 million labeled images.\nSimilar to how PCA achieves linear dimension reduction, the bottleneck layer of an autoencoder is able to obtain nonlinear dimension reduction through the embedding.\nThen, we solve the actual image classification problem for which we typically have much less labeled data using the embedding produced by the auxiliary autoencoder task.\nallows the model to learn embeddings from structured data via a feature transformer.\nContext language models like Word2Vec and masked language models like Bidirectional Encoding Representations from Transformers (BERT) change the learning task to a problem so that there is no scarcity of labels.\nWhile the goal of both models is to learn the context of a word by mapping input word(s) to the target word(s) with an intermediate embedding layer, an auxiliary goal is achieved that learns low- dimensional embeddings that best capture the context of words.\nThe resulting word embeddings learned through Word2Vec capture the semantic relationships between words so that, in the embedding space, the vector representations maintain meaningful distance and directionality (Figure 2- 12).\nA pre-trained text embedding, like Word2Vec, NNLM, GLoVE, or BERT, can be added to a machine learning model to process text features in conjunction with structured inputs and other learned embeddings from our customer and video dataset (Figure 2-13).\nUltimately, embeddings learn to preserve information relevant to the prescribed training task.",
      "keywords": [
        "embedding",
        "embedding layer",
        "model",
        "Keras Embedding layer",
        "embedding dimension",
        "layer",
        "Image",
        "word embeddings",
        "Image embeddings",
        "input",
        "dimension",
        "word",
        "Keras Lambda layer",
        "Keras Embedding",
        "BERT word embeddings"
      ],
      "concepts": [
        "embeddings",
        "word",
        "learns",
        "image",
        "model",
        "similarly",
        "similarities",
        "vectors",
        "likely",
        "layers"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 25,
          "title": "",
          "score": 0.648,
          "base_score": 0.498,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "",
          "score": 0.534,
          "base_score": 0.384,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "embedding",
          "embeddings",
          "dimension",
          "word",
          "low dimensional"
        ],
        "semantic": [],
        "merged": [
          "embedding",
          "embeddings",
          "dimension",
          "word",
          "low dimensional"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.291784452312967,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725271+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 81-92)",
      "start_page": 81,
      "end_page": 92,
      "summary": "The Feature Cross design pattern helps models learn relationships between inputs faster by explicitly making each combination of input values a separate feature.\nFeature crosses provide a way to have the ML model learn relationships between the features faster.\nWhile more complex models like neural networks and trees can learn feature crosses on their own, using feature crosses explicitly can allow us to get away with training just a linear model.\nConsequently, feature crosses can speed up model training (less expensive) and reduce model complexity (less training data is needed).\nA feature cross of these bucketized features introduces four new boolean features for our model:\nSince the original dataset was split perfectly by the buckets we created, a feature cross of A and B is able to linearly separate the dataset.\nFor this dataset, it makes sense to consider a feature cross of day_of_week and hour_of_day since it’s reasonable to assume that taxi rides at 5pm on Monday should be treated differently than taxi rides at 5 p.m. on Friday (see Table 2-9).\nA preview of the data we’re using to create a feature cross: the day of week and hour of day columns\nWhile the two features are important on their own, allowing for a feature cross of hour_of_day and day_of_week makes it easier for a taxi fare prediction model to recognize that end-of-the-week rush hour influences the taxi ride duration and thus the taxi fare in its own way.\nTo create the feature cross in BigQuery, we can use the function ML.FEATURE_CROSS and pass in a STRUCT of the features day_of_week and hour_of_day:\nML.FEATURE_CROSS(STRUCT(day_of_week,hour_of_week)) AS day_X_hour\nA complete training example for the natality problem is shown below, with a feature cross of the is_male and plurality columns used as a feature; see the full code in this book’s repository:\nCREATE OR REPLACE MODEL babyweight.natality_model_feat_eng TRANSFORM(weight_pounds, is_male, plurality, gestation_weeks, mother_age, CAST(mother_race AS string) AS mother_race, ML.FEATURE_CROSS( STRUCT( is_male, plurality) ) AS gender_X_plurality) OPTIONS (MODEL_TYPE='linear_reg', INPUT_LABEL_COLS=['weight_pounds'], DATA_SPLIT_METHOD=\"NO_SPLIT\") AS SELECT * FROM babyweight.babyweight_data_train\nThis also allows the model to “remember” to carry out the feature cross of the input data fields during prediction.\nWhen we have enough data, the Feature Cross pattern allows models to become simpler.\nOn the natality dataset, the RMSE for the evaluation set for a linear model with the Feature Cross pattern is 1.056.\nAlternatively, training a deep neural network in BigQuery ML on the same dataset with no feature crosses yields an RMSE of 1.074.\ngender_x_plurality = fc.crossed_column([\"is_male\", \"plurality\"], hash_bucket_size=1000) crossed_feature = fc.embedding_column(gender_x_plurality, dimension=2)\ngender_x_plurality = fc.crossed_column([\"is_male\", \"plurality\"], hash_bucket_size=1000) crossed_feature = fc.indicator_column(gender_x_plurality)\nWhen we use an indicator_column, the model is able to treat each of the resulting crosses as an independent variable, essentially adding 18 additional binary categorical features to the model (see Figure 2-16 ).\nOn the natality dataset, we observed that a linear model with a feature cross trained in BigQuery ML performs comparably with a DNN trained without a feature cross.\nA feature cross between is_male and plurality creates an additional 18 binary features in our ML model.\nTable 2-10 compares the training time in BigQuery ML and evaluation loss for both a linear model with a feature cross of (is_male, plurality) and a deep neural network without any feature cross.\nA comparison of BigQuery ML training metrics for models with and without feature crosses\nCombining feature crosses with massive data is an alternative strategy for learning complex relationships in training data.\nWe would never want to create a feature cross with a continuous input.\nInstead, if our data is continuous, then we can bucketize the data to make it categorical before applying a feature cross.\n# Create a feature cross of latitude and longitude lat_x_lon = fc.crossed_column([lat_bucketized, lon_bucketized], hash_bucket_size=nbuckets**4)\ncrossed_feature = fc.indicator_column(lat_x_lon)",
      "keywords": [
        "Feature Cross",
        "Feature",
        "Cross",
        "Feature crosses",
        "Feature Cross pattern",
        "model",
        "data",
        "column",
        "day",
        "create",
        "plurality",
        "Feature Cross design",
        "day feature cross",
        "bucket",
        "bucket feature column"
      ],
      "concepts": [
        "features",
        "model",
        "data",
        "cross",
        "crosses",
        "crossed",
        "training",
        "create",
        "creating",
        "bucket"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 39,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.4,
          "base_score": 0.4,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.4,
          "base_score": 0.4,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature cross",
          "cross",
          "feature",
          "crosses",
          "is_male plurality"
        ],
        "semantic": [],
        "merged": [
          "feature cross",
          "cross",
          "feature",
          "crosses",
          "is_male plurality"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30009386712748926,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725317+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 93-100)",
      "start_page": 93,
      "end_page": 100,
      "summary": "An embedding layer is a useful way to address the sparsity of a feature cross.\nBecause the Embeddings design pattern allows us to capture closeness relationships, passing the feature cross through an embedding layer allows the model to generalize how certain feature crosses coming from pairs of hour and day combinations affect the output of the model.\nIf we were to take very fine buckets for latitude and longitude, then a feature cross would be so precise it would allow the model to memorize every point on the map.\nCREATE OR REPLACE MODEL mlpatterns.taxi_l2reg TRANSFORM( fare_amount , ML.FEATURE_CROSS(STRUCT(CAST(EXTRACT(DAYOFWEEK FROM pickup_datetime) AS STRING) AS dayofweek, CAST(EXTRACT(HOUR FROM pickup_datetime) AS STRING) AS hourofday), 2) AS day_hr , CONCAT( ML.BUCKETIZE(pickuplon, GENERATE_ARRAY(-78, -70, 0.01)), ML.BUCKETIZE(pickuplat, GENERATE_ARRAY(37, 45, 0.01)), ML.BUCKETIZE(dropofflon, GENERATE_ARRAY(-78, -70, 0.01)), ML.BUCKETIZE(dropofflat, GENERATE_ARRAY(37, 45, 0.01)) ) AS pickup_and_dropoff ) OPTIONS(input_label_cols=['fare_amount'], model_type='linear_reg', l2_reg=0.1) AS SELECT * FROM mlpatterns.taxi_data\nThe Multimodal Input design pattern addresses the problem of representing different types of data or data that can be expressed in complex ways by concatenating all the available data representations.\nTypically, an input to a model can be represented as a number or as a category, an image, or free-form text.\nThis problem also occurs when training a structured data model where one of the inputs is free-form text.\nAs a result, we’ll need to represent image and text inputs in a way our model can understand (usually using the 7 Embeddings design pattern), then combine these inputs with other tabular features.\nModel combining free-form text input with tabular data to predict the rating of a restaurant review.\nIf our model had only text, we could represent it as an embedding layer using the following tf.keras code:\nThis is because we need to pass Input layers when we build a Model with the functional API.\nNext, we’ll create a concatenated layer, feed that into our output layer, and finally create the model by passing in the original Input layers we defined above:\nmodel = Model(inputs=[embedding_input, tabular_input], outputs=output) merged_dense = Dense(16, activation='relu')(merged_input) output = Dense(1)(merged_dense)\nmodel = Model(inputs=[embedding_input, tabular_input], outputs=output)\nAs we just saw, the Multimodal Input design pattern explores how to represent different input formats in the same model.\nIn addition to mixing different types of data, we may also want to represent the same data in different ways to make it easier for our model to identify patterns.\nTo see how we can represent tabular data in different ways for the same model, let’s return to the restaurant review example.\nWe’ll imagine instead that rating is an input to our model and we’re trying to predict the review’s usefulness (how many people liked the review).\nAs an input, the rating can be represented both as an integer value ranging from 1 to 5 and as a categorical feature.",
      "keywords": [
        "input",
        "model",
        "feature cross",
        "embedding",
        "feature",
        "data",
        "Dense",
        "cross",
        "rating",
        "Multimodal Input",
        "layer",
        "ARRAY",
        "tabular",
        "embedding layer",
        "Embeddings design pattern"
      ],
      "concepts": [
        "input",
        "model",
        "feature",
        "buckets",
        "rating",
        "ratings",
        "data",
        "embeddings",
        "keras",
        "represented"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 39,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 38,
          "title": "",
          "score": 0.456,
          "base_score": 0.456,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 41,
          "title": "",
          "score": 0.38,
          "base_score": 0.38,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "generate_array",
          "rating",
          "01",
          "ml bucketize",
          "bucketize"
        ],
        "semantic": [],
        "merged": [
          "generate_array",
          "rating",
          "01",
          "ml bucketize",
          "bucketize"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2735016115170059,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725360+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 101-108)",
      "start_page": 101,
      "end_page": 108,
      "summary": "We’ll build on our discussion of text models in the preceding section by looking at different approaches for representing text data.\nThe Embeddings design pattern enables a model to group similar words together, identify relationships between words, and understand syntactic elements of text.\nWhile representing text through word embeddings most closely mirrors how humans innately understand language, there are additional text representations that can maximize our model’s ability to perform a given prediction task.\nIn this section, we’ll look at the bag of words approach to representing text, along with extracting tabular features from text.\nTo demonstrate text data representation, we’ll be referencing a dataset that contains the text of millions of questions and answers from Stack Overflow, following query will give us a subset of questions tagged as either “keras,” “matplotlib,” or “pandas,” along with the number of answers each question received:\nWhen representing text using the bag of words (BOW) approach, we imagine each text input to our model as a bag of Scrabble tiles, with each tile containing a single word instead of a letter.\nBOW does not preserve the order of our text, but it does detect the presence or absence of certain words in each piece of text we send to our model.\nThe first step in BOW encoding is choosing our vocabulary size, which will include the top N most frequently occurring words in our text corpus.\nTo understand BOW encoding from the perspective of our model, imagine we’re learning a new language and the 10 words above are the only words we know.\nTo summarize, Figure 2-21 shows how we transformed our input from raw text to a BOW-encoded array based on our vocabulary.\nKeras has some utility methods for encoding text as a bag of words, so we don’t need to write the code for identifying the top words from our text corpus and encoding raw text into multi-hot arrays from scratch.\nRaw input text → identifying words present in this text from our vocabulary → transforming to a multi-hot BOW encoding.\nEmbeddings add an extra layer to our model and provide extra information about word meaning that is not available from the BOW encoding.\nUnlike embeddings, BOW doesn’t take into account the order or meaning of words in a text document.\nThere may also be benefits to building a deep model that combines both bag of words and text embedding representations to extract more patterns from our data.\nTo do this, we can use the Multimodal Input approach, except that instead of concatenating text and tabular features, we can concatenate the Embedding and BOW representations (see code on GitHub).\nEmbeddings can encode the frequency of words in text, where the BOW treats the presence of each word as a boolean value.\nBOW encoding can identify patterns between reviews that all contain the word “amazing,” while an embedding can learn to correlate the phrase “not amazing” with a below-average review.\nWe could then combine these tabular features with our encoded text and feed both representations into our model using Keras’s Concatenate layer to combine the BOW-encoded text array with the tabular metadata describing our text.\nSimilar to our analysis of embeddings and BOW encoding for text, there are many ways to represent image data when preparing it for an ML model.\nLike raw text, images cannot be fed directly into a model and need to be transformed into a numerical format that the model can understand.\nThe Multimodal Input design pattern provides a way to use more than one representation of an image in our model.\nWe could therefore represent a 28×28-pixel black-and-white image in a model as a 28×28 array with integer values ranging from 0 to 255.",
      "keywords": [
        "BOW",
        "text",
        "BOW encoding",
        "words",
        "model",
        "input",
        "vocabulary",
        "BOW representation",
        "images",
        "representing text",
        "question",
        "encoding",
        "understand BOW encoding",
        "Embeddings",
        "array"
      ],
      "concepts": [
        "word",
        "text",
        "images",
        "model",
        "embeddings",
        "questions",
        "question",
        "bow",
        "likely",
        "extracting"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 47,
          "title": "",
          "score": 0.592,
          "base_score": 0.442,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.583,
          "base_score": 0.433,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bow",
          "text",
          "words",
          "bow encoding",
          "encoding"
        ],
        "semantic": [],
        "merged": [
          "bow",
          "text",
          "words",
          "bow encoding",
          "encoding"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3102653065963214,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725419+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 109-116)",
      "start_page": 109,
      "end_page": 116,
      "summary": "In this example, we’ve got a 4×4 grid where each square represents pixel values on our image.\nWhile this tiling method preserves more detail than representing images as arrays of pixel values, quite a bit of information is lost after each pooling step.\nKeras provides convolution layers to build models that split images into smaller, windowed chunks.\nLet’s say we’re building a model to classify 28×28 color images as either “dog” or “cat.” Since these images are color, each image will be represented as a 28×28×3-dimensional array, since each pixel has three color channels.\nIn this example, we’re dividing our input images into 3×3 chunks before passing them through a max pooling layer.\nBuilding a model architecture that splits images into chunks of sliding windows allows our model to recognize more granular details in an image like edges and shapes.\nIn addition, as with the bag of words and text embedding, it may be useful to represent the same image data in multiple ways.\n# Define image input layer (same shape for both pixel and tiled # representation) image_input = Input(shape=(28,28,3))\n# Define pixel representation pixel_layer = Flatten()(image_input)\n# Define tiled representation tiled_layer = Conv2D(filters=16, kernel_size=3, activation='relu')(image_input) tiled_layer = MaxPooling2D()(tiled_layer) tiled_layer = tf.keras.layers.Flatten()(tiled_layer)\nmodel = Model(inputs=image_input, outputs=merged_output)\nRepresenting images as pixel values allows the model to identify higher-level focus points in an image like dominant, high- contrast objects.\nOur model can extract many patterns from the traffic images on their own, but there may be other data available that could improve our model’s accuracy.\nIf we’re collecting image data from multiple intersections, knowing the location of our image might also be useful to our model.\nWe’ve now identified three additional tabular features that could enhance our image model:\nWe could then combine these tabular features into a single array for each example, so that our model’s input shape would be 10.\nWe could feed this input into a Dense fully connected layer, and the output of our model would be a single value between 0 and 1 indicating whether or not the instance contains a traffic violation.\nTo combine this with our image data, we’ll use a similar approach to what we discussed for text models.\nFirst, we’d define a convolution layer to handle our image data, then a Dense layer to handle our tabular data, and finally we’d concatenate both into a single output.\ndata representations in a single model, however, these features become dependent on one another.\nIn this chapter, we learned different approaches to representing data for our model.\nFinally, we looked at Multimodal Input representations by addressing the problem of how to combine inputs of different types into the same model, and how a single feature can be represented multiple ways.",
      "keywords": [
        "CONVOLUTIONAL NEURAL NETWORK",
        "NEURAL NETWORK LAYERS",
        "CONVOLUTIONAL NEURAL",
        "NEURAL NETWORK",
        "model",
        "image",
        "layer",
        "data",
        "input",
        "NETWORK LAYERS",
        "image data",
        "pixel",
        "features",
        "Figure 2-22",
        "representations"
      ],
      "concepts": [
        "image",
        "feature",
        "representations",
        "data",
        "inputs",
        "model",
        "layers",
        "represented",
        "categorical",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 24,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 23,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.556,
          "base_score": 0.406,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 26,
          "title": "",
          "score": 0.545,
          "base_score": 0.395,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "image",
          "pixel",
          "tiled_layer",
          "image data",
          "28"
        ],
        "semantic": [],
        "merged": [
          "image",
          "pixel",
          "tiled_layer",
          "image data",
          "28"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28685993884999533,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725474+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 117-125)",
      "start_page": 117,
      "end_page": 125,
      "summary": "2 If twins, the plurality is 2.\nIf triplets, the plurality is 3.\nFor example, values like age, type of car, price, or number of hours worked.\nTabular data does not include free-form text like descriptions or reviews.\n8 When we pass an encoded 30-word array to our model, the Keras layer will transform it into a 64-dimensional embedding representation, so we’ll have a [64×30] matrix representing the review.\nProblem Representation Design Patterns\nTo limit our discussion and stay away from areas of active research, we will ignore patterns and idioms associated with specialized machine learning domains.\nThe Neutral Class design pattern looks at how to handle situations where experts disagree.\nProblem\nIf it is a supervised problem, what are the\nOf course, the answers to these questions must be considered in context with the training data, the task at hand, and the metrics for success.\nAlternately, because the label (the amount of rainfall) is a real number, we could build a regression model.\nAny of these adjustments could improve our model.\nFor the same set of features, it sometimes rains 0.3 cm and other times it rains 0.5 cm.\nModeling a distribution in this way is advantageous since precipitation does not exhibit the typical bell-shaped curve of a normal distribution and instead follows a Tweedie distribution, which allows for a preponderance of points at zero.\nFor example, suppose we are trying to build a recommendation system for videos.\nWhereas, if only predicting a single numeric value, this information would be lost.\nDepending on the use case, this could make the task easier to learn and substantially more advantageous.\nThe code to produce the graph in Figure 3-3 can be found in the repository for this book.\nHowever, notice the width of the distribution—even though the distribution peaks at 7.5 pounds, there is a nontrivial likelihood (actually 33%) that a given baby is less than 6.5 pounds or more than 8.5 pounds!\nThat is, we obtain a discrete PDF giving the relative likelihood of any specific weight.\nIn some scenarios, reframing a classification task as a regression could be beneficial.\nSuppose we wanted to predict which city will experience the next viral outbreak or which New York neighborhood will have a real estate pricing surge.\nWith any reframing technique, being aware of data limitations or the risk of introducing label bias is important.\nFor example, if our model is to be used to indicate when a baby might need critical care upon birth, the categories in Table 3-1 could be sufficient.\nBucketized outputs for baby weight\nHigh birth weight\nBetween 3.31 lbs and 5.5 lbs\nComparing these two models is difficult since one evaluation metric is RMSE and the other is accuracy.\nThere are other ways to capture uncertainty in regression.\nA simple approach is to carry out quantile regression.\nQuantile regression is an extension of linear regression.\nHowever, we have to explicitly model the distribution of the output.\nOn the other hand, if we know the variance increases with the mean, we might be able to model it using the lambda function.\nReframing, on the other hand, doesn’t require us to model the posterior distribution.\nFurthermore, this need for massive data only increases with the complexity of the task.\nPrecision of predictions\nThe precision of the multiclass classification is controlled by the width of the bins for the label.",
      "keywords": [
        "model",
        "regression",
        "classification",
        "Problem",
        "regression model",
        "regression problem",
        "classification model",
        "distribution",
        "classification problem",
        "machine learning",
        "machine learning model",
        "learning",
        "machine learning problem",
        "probability distribution",
        "Reframing"
      ],
      "concepts": [
        "model",
        "predict",
        "prediction",
        "predictions",
        "patterns",
        "data",
        "distribution",
        "distributions",
        "distributed",
        "problem"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.537,
          "base_score": 0.387,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.51,
          "base_score": 0.36,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 17,
          "title": "",
          "score": 0.478,
          "base_score": 0.328,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.471,
          "base_score": 0.321,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "distribution",
          "regression",
          "pounds",
          "reframing",
          "problem"
        ],
        "semantic": [],
        "merged": [
          "distribution",
          "regression",
          "pounds",
          "reframing",
          "problem"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26417528805577883,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725519+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 126-133)",
      "start_page": 126,
      "end_page": 133,
      "summary": "If we train a regression model where the output layer is a linear activation function, there is always the possibility that the model predictions will fall outside this range.\nFor example, suppose we reframed our recommendation model to a classification task that predicts the likelihood a user will click on a certain video thumbnail.\nIn this context, we could have two heads to our model: one to predict a regression output and another to predict classification output.\nFor example, this paper trains a computer vision model using a classification output of softmax probabilities together with a regression output to predict bounding boxes.\nThe idea is that through parameter sharing, the tasks are learned simultaneously and the gradient updates from the two loss functions inform both outputs and result in a more generalizable model.\nFor neural networks, this design requires changing the activation function used in the final output layer of the model and choosing how our application will parse model output.\nNote that this is different from multiclass classification problems, where a single example is assigned exactly one label from a group of many (> 1) possible classes.\nOften, model prediction tasks involve applying a single classification to a given training example.\nFor example, if our model is classifying images as cats, dogs, or rabbits, the softmax output might look like this for a given image: [.89, .02, .09].\nBecause each image can have only one possible label in this scenario, we can take the argmax (index of the highest probability) to determine our model’s predicted class.\nThe solution for building models that can assign more than one label to a given training example is to use the sigmoid activation function in our final output layer.\nThe length of the multi-hot array corresponds with the number of classes in our model, and each output in this label array will be a sigmoid value.\nA version of this model for 28×28-pixel images with sigmoid output might look like this, using the Keras Sequential API:\nThe main difference in output between the sigmoid model here and the softmax example in the Problem section is that the softmax array is guaranteed to contain three values that sum to 1, whereas the sigmoid output will contain three values, each between 0 and 1.\nSigmoid is a nonlinear, continuous, and differentiable activation function that takes the outputs of each neuron in the previous layer in the ML model and squashes the value of those outputs between 0 and 1.\nIn a multiclass classification problem where each example can only have one label, use softmax as the last layer to get a probability distribution.\nNext, we’ll explore how to structure models that have two possible label classes, how to make sense of sigmoid results, and other important considerations for Multilabel models.\nSigmoid output for models with two classes\nThere are two types of models where the output can belong to two possible classes:\nThe first case (binary classification) is unique in that it is the only type of single-label classification problem where we would consider using sigmoid as our activation function.\nHad we used a softmax output in this example, here’s what a fraudulent model prediction might look like:\nBecause each input can only be assigned a single class, we can infer from this output of .98 that the model has predicted a 98% chance of fraud and a 2% chance of nonfraud.\nTherefore, for binary classification models, it is optimal to use an output shape of 1 with a sigmoid activation function.\nHere is what the output layer of a binary classification model would look like:\nFor the second case where a training example could belong to both possible classes and fits into the Multilabel design pattern, we’ll also want to use sigmoid, this time with a two-element output:\nFor the binary classification case where our model has a one-element output, use binary cross-entropy loss.\nInterestingly, we also use binary cross-entropy loss for multilabel models with sigmoid output.\nTo extract the predicted label for a model with softmax output, we can simply take the argmax (highest value index) of the output array to get the predicted class.\nWhile thresholding is something we’ll need to consider for any type of classification model, it’s especially relevant to the Multilabel design pattern since we’ll need to determine thresholds for each class and they may be different.\nTo look at a specific example, let’s take the Stack Overflow dataset in BigQuery and use it to build a model that predicts the tags associated with a Stack Overflow question given its title.\nWhen evaluating model predictions, we’ll need to iterate over every element in the output array and determine how we want to display those results to our end users.",
      "keywords": [
        "model",
        "output",
        "sigmoid",
        "classification",
        "sigmoid output",
        "Multilabel",
        "Multilabel design pattern",
        "Label",
        "function",
        "Multilabel design",
        "output layer",
        "activation function",
        "softmax",
        "binary classification",
        "activation"
      ],
      "concepts": [
        "model",
        "output",
        "label",
        "classification",
        "classifications",
        "sigmoid",
        "values",
        "examples",
        "train",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 17,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.53,
          "base_score": 0.53,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.51,
          "base_score": 0.36,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "sigmoid",
          "output",
          "activation function",
          "softmax",
          "classification"
        ],
        "semantic": [],
        "merged": [
          "sigmoid",
          "output",
          "activation function",
          "softmax",
          "classification"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3542766362168167,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725574+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 134-144)",
      "start_page": 134,
      "end_page": 144,
      "summary": "In order for our model to learn what is unique to each tag, we’ll want to ensure the training dataset consists of varied combinations of each tag.\nTo account for this, think about the different relationships between labels that might be present in our model and count the number of training examples that belong to each overlapping combination of labels.\nImageNet, the popular image classification dataset, contains thousands of labeled images and is often used as a starting point for transfer learning on image models.\nHowever, if we build a multiclass classification model on this data, passing it multiple examples of the same image with different labels, we’ll likely encounter situations where the model labels similar images differently when making predictions.\nWhen evaluating this model, we need to take note of the average prediction confidence the model returns for each label and use this to iteratively improve our dataset and label quality.\nThe disadvantage of this approach is the added complexity of training many different classifiers, requiring us to build our application in a way that generates predictions from each of these models rather than having just one.\nThe Ensembles design pattern refers to techniques in machine learning that combine multiple machine learning models and aggregate their results to make predictions.\nEnsembles can be an effective means to improve performance and produce predictions that are better than any single model.\nSuppose we’ve trained our baby weight prediction model, engineering special features and adding additional layers to our neural network so that the error on our training set is nearly zero.\nThe irreducible error is the inherent error in the model resulting from noise in the dataset, the framing of the problem, or bad training examples, like measurement errors or confounding factors.\nIn short, the bias is the model’s inability to learn enough about the relationship between the model’s features and labels, while the variance captures the model’s inability to generalize on new, unseen examples.\nA model with high variance has learned too much about the training data and is said to be overfit.\nEnsemble methods are meta-algorithms that combine several machine learning models as a technique to decrease the bias and/or variance and improve model performance.\nBagging (short for bootstrap aggregating) is a type of parallel ensembling method and is used to address high variance in machine learning models.\nThe aggregation takes place on the output of the multiple ensemble model members—either an average in the case of a regression task or a majority vote in the case of classification.\nA good example of a bagging ensemble method is the random forest: multiple decision trees are trained on randomly sampled subsets of the entire training data, then the tree predictions are aggregated to produce a prediction, as shown in Figure 3-11.\nBagging is good for decreasing variance in machine learning model output.\nAs we’ll see, different ensemble methods combine multiple submodels in different ways, sometimes using different models, different algorithms, or even different objective functions.\nHowever, unlike bagging, boosting ultimately constructs an ensemble model with more capacity than the individual member models.\nThe idea behind boosting is to iteratively build an ensemble of models where each successive model focuses on learning the examples the previous model got wrong.\nAfter many iterations, the residuals tend toward zero and the prediction gets better and better at modeling the original training dataset.\nStacking is an ensemble method that combines the outputs of a collection of models to make a prediction.\nThis second meta-model learns how to best combine the outcomes of the initial models to decrease the training error and can be any type of machine learning model.\nThe following code calls a function, fit_model, that takes as arguments a model and the training dataset inputs X_train and label Y_train.\nThis way members is a list containing all the trained models in our ensemble.\nThese submodels are incorporated into a larger stacking ensemble model as individual inputs.\nSince these input models are trained alongside the secondary ensemble model, we fix the weights of these input models.\nModel averaging methods like bagging work because typically the individual models that make up the ensemble model will not all make the same errors on the test set.\nWith each iteration, the ensemble model is encouraged to get better and better at predicting those hard-to-predict examples.\nMore precisely, suppose we’ve trained k neural network regression models and average their results to create an ensemble model.\nIf each model has error error_i on each example, where error_i is drawn from a zero-mean multivariate normal distribution with variance var and covariance cov, then the ensemble predictor will have an error:\nSo, the expected square error decreases linearly with the number k of models in the ensemble.\nTo summarize, on average, the ensemble will perform at least as well as any of the individual models in the ensemble.\nFurthermore, if the models in the ensemble make independent errors (for example, cov = 0), then the ensemble will perform significantly better.\nThis also explains why bagging is typically less effective for more stable learners like k-nearest neighbors (kNN), naive Bayes, linear models, or support vector machines (SVMs) since the size of the training set is reduced through bootstrapping.\nEven when using the same training data, neural networks can reach a variety of solutions due to random weight initializations or random mini-batch selection or different hyperparameters, creating models whose errors are partially independent.\nThus, model averaging can even benefit neural networks trained on the same dataset.\nIn fact, one recommended solution to fix the high variance of neural networks is to train multiple models and aggregate their predictions.\nThe boosting algorithm works by iteratively improving the model to reduce the prediction error.\nThus, the resulting ensemble model becomes successively more and more complex, having more capacity than any one of its members.\nBy iteratively focusing on the hard-to-predict examples, boosting effectively decreases the bias of the resulting model.\nStacking can be thought of as an extension of simple model averaging where we train k models to completion on the training dataset, then average the results to determine a prediction.\nSimple model averaging is similar to bagging, but the models in the ensemble could be of different types, while for bagging, the models are of the same type.\nMore generally, we could modify the averaging step to take a weighted average, for example, to give more weight to one model in our ensemble over the others, as shown in Figure 3-14.\nYou can think of stacking as a more advanced version of model averaging, where instead of taking an average or weighted average, we train a second machine learning model on the outputs to learn how best to combine the results to the models in our ensemble to produce a prediction as shown in Figure 3-15.\nStacking is an ensemble learning technique that combines the outputs of several different ML models as the input to a secondary ML model that makes predictions.\nFor example, for a stacked ensemble model, choosing the ensemble member models can require its own level of expertise and poses its own questions: Is it best to reuse the same architectures or encourage diversity?\ndropout, the ensemble member models would only be trained for a single training step because different nodes are dropped out in each iteration of the training loop.\nThis problem is compounded with ensemble models.\nIn short, the Ensemble design pattern encompasses techniques that combine multiple machine learning models to improve overall model performance and can be particularly useful when addressing common training issues like high bias or high variance.",
      "keywords": [
        "model",
        "ensemble model",
        "Ensemble",
        "ensemble member models",
        "training",
        "machine learning models",
        "labels",
        "machine learning",
        "bagging",
        "Model averaging",
        "error",
        "Dataset",
        "learning models",
        "learning",
        "boosting"
      ],
      "concepts": [
        "model",
        "labels",
        "train",
        "ensembles",
        "boosting",
        "predictions",
        "predicting",
        "prediction",
        "likely",
        "error"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 17,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.609,
          "base_score": 0.459,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.537,
          "base_score": 0.387,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ensemble",
          "ensemble model",
          "bagging",
          "models",
          "averaging"
        ],
        "semantic": [],
        "merged": [
          "ensemble",
          "ensemble model",
          "bagging",
          "models",
          "averaging"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31818779814386805,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725624+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 145-152)",
      "start_page": 145,
      "end_page": 152,
      "summary": "For example, suppose we are trying to train a model to predict the likelihood that a customer will return an item that they have purchased.\nIf we train a single model, the resellers’ return behavior will be lost because there are millions of retail buyers (and retail transactions) and only a few thousand resellers.\nOne way to solve this problem is to overweight the reseller instances when training the model.\n2. Training one model on sales to retail buyers\n3. Training the second model on sales to resellers\n4. In production, combining the output of the three separate models to predict return likelihood for every item purchased and the probability that the transaction is by a reseller\nThis allows for the possibility of different decisions on items likely to be returned depending on the type of buyer and ensures that the models in steps 2 and 3 are as accurate as possible on their segment of the training data.\nAt prediction time, we don’t have true labels, just the output of the first classification model.\nSo, the second and third models will be required to make predictions on data that they might have never seen during training.\nIf the first (classification) model makes a mistake and a retail buyer is wrongly identified as a reseller, the cancellation prediction model that is invoked will not have the neighborhood where the customer lives in its vocabulary.\nHow do we train a cascade of models where the output of one model is an input to the following model or determines the selection of subsequent models?\nAny machine learning problem where the output of the one model is an input to the following model or determines the selection of subsequent models is called a cascade.\nOne way to solve this problem is to train a classification model to first classify trips based on whether they are Long or Typical (the full code is in the code repository of this book):\nInstead, after training this classification model, we need to use the predictions of this model to create the training dataset for the next set of models.\nFor example, we could create the training dataset for the model to predict the distance of Typical rentals using:\nCREATE OR REPLACE TABLE mlpatterns.Typical_trips AS SELECT * EXCEPT(predicted_trip_type_probs, predicted_trip_type) FROM ML.PREDICT(MODEL mlpatterns.classify_trips, (SELECT start_date, start_station_name, subscriber_type, ..., ST_Distance(start_station_geom, end_station_geom) AS distance FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`) ) WHERE predicted_trip_type = 'Typical' AND distance IS NOT NULL\nThen, we should use this dataset to train the model to predict distances:\nshould take into account that we need to use three trained models, not just one.\nRather than train the models individually, it is better to automate the entire workflow using the Workflow Pipelines pattern (Chapter 6) as shown in Figure 3-17.\nThe key is to ensure that training datasets for the two downstream models are created each time the experiment is run based on the predictions of upstream models.\nWhenever the output of a machine learning model needs to be fed as the input to another model, the second model needs to be trained on the predictions of the first model.\nA pipeline to train the cascade of models as a single job.\nAvoid having, as in the Cascade pattern, multiple machine learning models in the same pipeline.\nThis problem does not need the Cascade design pattern because it is common enough (a large fraction of customers will be comparison shopping) that the machine learning model should be able to learn it implicitly in the course of training.\nThe Cascade is needed when we need to maintain internal consistency amongst the predictions of multiple models.\nThe reason to use Cascade is that the imbalanced label output is needed as an input to subsequent models and is useful in and of itself.\nSimilarly, suppose that the reason we are training the model to predict a customer’s propensity to buy is to make a discounted offer.\nPre-trained models\nThe Cascade is also needed when we wish to reuse the output of a pre-trained model as an input into our model.\nAlthough it will involve training a classification model for each sales amount bucket, it avoids the need to get the retail versus wholesale classification correct.\n3. Train a regression model to predict the rainfall amount on pixels where the model predicts that rain is likely.\nIt is critical to realize that the classification model is not perfect, and so the regression model has to be trained on the pixels that the classification model predicts as likely to be raining (and not just on pixels that correspond to rain in the labeled dataset).",
      "keywords": [
        "model",
        "Cascade design pattern",
        "Cascade",
        "classification model",
        "Cascade design",
        "design pattern",
        "training",
        "pattern",
        "problem",
        "Cascade pattern",
        "classification",
        "predict",
        "model predicts",
        "pipeline",
        "design"
      ],
      "concepts": [
        "model",
        "train",
        "predict",
        "prediction",
        "predictions",
        "pipelines",
        "problem",
        "differently",
        "input",
        "pattern"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.478,
          "base_score": 0.328,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cascade",
          "retail",
          "classification",
          "classification model",
          "models"
        ],
        "semantic": [],
        "merged": [
          "cascade",
          "retail",
          "classification",
          "classification model",
          "models"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3201904355596645,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725683+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 153-163)",
      "start_page": 153,
      "end_page": 163,
      "summary": "Naturally, this random assignment will cause the overall accuracy of a model trained on just two classes to be low.\nBecause 80% of the training examples have random labels, the best that the model can do is to guess half of them correctly.\nThe remaining 20% of the training examples have systematic labels, and an ideal model will learn this, so we expect that overall accuracy can be at best 60%.\nThe purpose of the synthetic data was to illustrate that, provided there is random assignment at work, the Neutral Class design pattern can help us avoid losing model accuracy because of arbitrarily labeled data.\nThe accuracy of this binary classification model when trained on the natality dataset and evaluated on held-out data is 0.56.\nThis model achieves an accuracy of 0.79 on a held-out evaluation dataset, much higher than the 0.56 that was achieved with two classes.\n2. If we are training a cascade of models, then downstream models will be extremely sensitive to the neutral classes.\nThe need for a neutral class also arises with models that attempt to predict customer satisfaction.\nThe solution, then, is to create a training dataset consisting of three classes:\nRather than train a regression model on how much stocks will go up, we can now train a classification model with these three classes and pick the most confident predictions from our model.\nThe Rebalancing design pattern primarily addresses how to build models with datasets where few examples exist for a specific class or classes.\nMachine learning models learn best when they are given a similar number of examples for each label class in a dataset.\nTake for example a fraud detection use case, where you are building a model to identify fraudulent credit card transactions.\nFraudulent transactions are much rarer than regular transactions, and as such, there is less data on fraud cases available to train a model.\nA common pitfall in training models with imbalanced label classes is relying on misleading accuracy values for model evaluation.\nIf we train a fraud detection model and only 5% of our dataset contains fraudulent transactions, chances are our model will train to 95% accuracy without any modifications to the dataset or underlying model architecture.\nWhile this 95% accuracy number is technically correct, there’s a good chance the model is guessing the majority class (in this case, nonfraud) for each example.\nAs such, it’s not learning anything about how to distinguish the minority class from other examples in our dataset.\nThe confusion matrix for a poorly performing model trained on an imbalanced dataset often looks something like Figure 3-18.\nConfusion matrix for a model trained on an imbalanced dataset without dataset or model adjustments.\nIn this example, the model correctly guesses the majority class 95% of the time, but only guesses the minority class correctly 12% of the time.\nFirst, since accuracy can be misleading on imbalanced datasets, it’s important to choose an appropriate evaluation metric when building our model.\nDownsampling changes the balance of our underlying dataset, while weighting changes how our model handles certain classes.\nFor imbalanced datasets like the one in our fraud detection example, it’s best to use metrics like precision, recall, or F-measure to get a complete picture of how our model is performing.\nFor precision, the denominator is the total number of positive class predictions made by our model.\nTherefore, for models trained on imbalanced datasets, metrics other than accuracy are preferred.\nNote that, when evaluating models trained on imbalanced datasets, we need to use unsampled data when calculating success metrics.\nThis is because average precision-recall places more emphasis on how many predictions the model got right out of the total number it assigned to the positive class.\nWith downsampling, we decrease the number of examples from the majority class used during model training.\nWhile a large dataset can often improve a model’s ability to identify patterns, it’s less helpful when the data is significantly imbalanced.\nIf we train a model on this entire dataset (6.3M rows) without any modifications, chances are we’ll see a misleading accuracy of 99.9% as a result of the model randomly guessing the nonfraudulent class each time.\nWe’ll take all 8,000 of the fraudulent examples and set them aside to use when training the model.\nWe’ll then combine with our 8,000 fraudulent examples, reshuffle the data, and use this new, smaller dataset to train a model.\nAnother approach to handling imbalanced datasets is to change the weight our model gives to examples from each class.\nBy weighting classes, we tell our model to treat specific label classes with more importance during training.\nWe’ll want our model to assign more weight to examples from the minority class.\nIn Keras, we can pass a class_weights parameter to our model when we train it with fit().\nFor example, if the minority class accounts for only 0.1% of the dataset, a reasonable conclusion is that our model should treat examples from that class with 1000× more weight than the majority class.\nmodel.fit( train_data, train_labels, class_weight=keras_class_weights )\nIn BigQuery ML, we can set AUTO_CLASS_WEIGHTS = True in the OPTIONS block when creating our model to have different classes weighted based on their frequency of occurrence in the training data.\nWhile it can be helpful to follow a heuristic of class balance for setting class weights, the business application of a model might also dictate the class weights we choose to assign.\nIn conjunction with assigning class weights, it is also helpful to initialize the model’s output layer with a bias to account for dataset imbalance.\nThis is because the bias of the last (prediction) layer of a trained model will output, on average, the log of the ratio of minority to majority examples in the dataset.\n34% of this dataset contains examples of patients who had diabetes, so we’ll consider this our minority class.\nA subset of features for two training examples from the minority class (has diabetes) in the Pima Indian Diabetes Dataset\nFor example, if we’re building a model to distinguish between Bengal and Siamese cats and only 10% of our dataset contains images of Bengals, we can generate additional variations of the Bengal cats in our dataset through image augmentation using the Keras ImageDataGenerator class.\nThere are a few other alternative solutions for building models with inherently imbalanced datasets, including reframing the problem and handling cases of anomaly detection.\nWe’ll also explore several important considerations for imbalanced datasets: overall dataset size, the optimal model architectures for different problem types, and explaining minority class prediction.\nImagine we’re building a model to predict baby weight using the BigQuery natality dataset.",
      "keywords": [
        "model",
        "dataset",
        "minority class",
        "neutral class",
        "Imbalanced datasets",
        "data",
        "majority class",
        "minority",
        "neutral",
        "weight",
        "training",
        "imbalanced",
        "class weights",
        "accuracy",
        "majority"
      ],
      "concepts": [
        "models",
        "dataset",
        "classes",
        "data",
        "case",
        "labeling",
        "predict",
        "predictions",
        "prediction",
        "problem"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 14,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 17,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 15,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 16,
          "title": "",
          "score": 0.609,
          "base_score": 0.459,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "class",
          "imbalanced",
          "minority",
          "dataset",
          "imbalanced datasets"
        ],
        "semantic": [],
        "merged": [
          "class",
          "imbalanced",
          "minority",
          "dataset",
          "imbalanced datasets"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3106695899145199,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725732+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 164-171)",
      "start_page": 164,
      "end_page": 171,
      "summary": "SELECT CASE WHEN weight_pounds < 5.5 THEN \"underweight\" WHEN weight_pounds > 9.5 THEN \"overweight\" ELSE \"average\" END AS weight, COUNT(*) AS num_examples, round(count(*) / sum(count(*)) over(), 4) as percent_of_dataset FROM `bigquery-public-data.samples.natality` GROUP BY 1\nFor demo purposes, we’ll take 100,000 examples from each class to train a model on an updated, balanced dataset:\nWe can save the results of that query to a table, and with a more balanced dataset, we can now train a classification model to label babies as “underweight,” “average,” or “overweight”:\nCREATE OR REPLACE MODEL `project.dataset.baby_weight_classification` OPTIONS(model_type='logistic_reg', input_label_cols=['weight']) AS SELECT is_male, weight_pounds, mother_age, gestation_weeks, weight FROM `project.dataset.baby_weight`\nAnother approach is to use the Cascade pattern, training three separate regression models for each class.\nThen, we can use our multidesign pattern solution by passing our initial classification model an example and using the result of that classification to decide which regression model to send the example to for numeric prediction.\nCREATE OR REPLACE MODEL `project-name.dataset-name.baby_weight` OPTIONS(model_type='kmeans', num_clusters=4) AS SELECT weight_pounds, mother_age, gestation_weeks FROM `bigquery-public-data.samples.natality` LIMIT 10000\nThe resulting model will cluster our data into four groups.\nOnce the model has been created, we can then generate predictions on new data and look at that prediction’s distance from existing clusters.\nTo generate a cluster prediction on our model, we can run the following query, passing it a made-up average example from the dataset:\nSELECT * FROM ML.PREDICT (MODEL `project-name.dataset-name.baby_weight`, ( SELECT 7.0 as weight_pounds, 28 as mother_age, 40 as gestation_weeks ) )\nThe query results in Table 3-6 show us the distance between this data point and the model’s generated clusters, called centroids.\nThe distance between our average weight example data point and each of the clusters generated by our k-means model\nThe distance between our underweight example data point and each of the clusters generated by our k-means model\nOnce we’ve generated cluster predictions on enough examples, we could then build a supervised learning model using the predicted clusters as labels.\nWhile the minority class in our first fraud detection example only made up 0.1% of the data, the dataset was large enough that we still had 8,000 fraudulent data points to work with.\nFor datasets with even fewer examples of the minority class, downsampling may make the resulting dataset too small for a model to learn from.\nFor example, if we’re building a fraud detection model, we’re likely much more concerned about the transactions our model flags as “fraud” rather than the ones it flags as “nonfraud.” Additionally, as mentioned by SMOTE, the approach of generating synthetic examples from the minority class is often combined with removing a random sample of examples from the minority class.\nUsing this approach, instead of entirely removing a random sample of our majority class, we use different subsets of it to train multiple models and then ensemble those models.\nDepending on our prediction task, there are different model architectures to consider when solving problems with the Rebalancing design pattern.\nWe can use downsampling and class weights in each of these frameworks to further optimize our model using the Rebalancing design pattern.\nClustering models are also an option for tabular data with imbalanced classes.\nRegardless of the data modality we’re working with, it’s useful to experiment with different model architectures to see which performs best on our imbalanced data.\nWhen building models for flagging rare occurrences in data such as anomalies, it’s especially important to understand how our model is making predictions.\nAfter training a TensorFlow model on the synthetic fraud detection dataset from Kaggle and deploying it to Explainable AI on Google Cloud, let’s take a look at some examples of instance-level attributions.\nExplanations are important for any type of machine learning model, but we can see how they are especially useful for models following the Rebalancing design pattern.\nWhereas the Reframing and Multilabel patterns focus on formatting model output, the Ensemble design pattern addresses model architecture and includes various methods for combining multiple models to improve upon machine learning results from a single model.\nChapters 2 and 3 focused on the initial steps for structuring your machine learning problem, specifically formatting input data, model architecture options, and model output representation.",
      "keywords": [
        "model",
        "data",
        "weight",
        "dataset",
        "prediction",
        "data point",
        "minority class",
        "pattern",
        "design pattern",
        "model architecture",
        "SELECT",
        "model CENTROID",
        "k-means model CENTROID",
        "model output",
        "Rebalancing design pattern"
      ],
      "concepts": [
        "model",
        "data",
        "prediction",
        "predict",
        "predictions",
        "patterns",
        "dataset",
        "weight",
        "examples",
        "cluster"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 39,
          "title": "",
          "score": 0.413,
          "base_score": 0.413,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.379,
          "base_score": 0.379,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.352,
          "base_score": 0.352,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 49,
          "title": "",
          "score": 0.336,
          "base_score": 0.336,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "",
          "score": 0.33,
          "base_score": 0.33,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "class",
          "clusters",
          "weight_pounds",
          "minority class",
          "minority"
        ],
        "semantic": [],
        "merged": [
          "class",
          "clusters",
          "weight_pounds",
          "minority class",
          "minority"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2349146528214706,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.725781+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 172-179)",
      "start_page": 172,
      "end_page": 179,
      "summary": "Model Training Patterns\nMachine learning models are usually trained iteratively, and this iterative process is informally called the training loop.\nMachine learning models can be trained using different types of optimization.\nOn large datasets, gradient descent is applied to mini-batches of the input data to train everything from linear models and boosted trees to deep neural networks (DNNs) and support vector machines (SVMs).\nBecause SGD requires training to take place iteratively on small batches of the training dataset, training a machine learning model happens in a loop.\nTherefore, evaluation needs to be done within the training loop, and error metrics on a withheld split of the training data, called the validation dataset, have to be monitored as well.\nHere, the model uses the Adam optimizer to carry out SGD on the cross entropy over the training dataset and reports out the final accuracy obtained on the testing dataset.\nThe model fitting loops over the training dataset three times (each traversal over the training dataset is termed an epoch) with the model seeing batches consisting of 64 training examples at a time.\nWhen we use checkpoints, we usually also use virtual epochs, wherein we decide to carry out the inner loop of the fit() function, not on the full training dataset but on a fixed number of training examples.\nIn Transfer Learning, we take part of a previously trained model, freeze the weights, and incorporate these nontrainable layers into a new model that solves the same problem, but on a smaller dataset.\nFinally, in Hyperparameter Tuning, the training loop is itself inserted into an optimization method to find the optimal set of model hyperparameters.\nIf your model overfits the training data (for example, it continues to decrease the training error beyond the point at which validation error starts to increase), then its ability to generalize suffers and so do your future predictions.\nIt is then possible to create a training dataset for the machine learning system consisting of the complete input space and calculate the labels using the physical model.\nSplitting such a dataset into a training dataset and an evaluation dataset is counterproductive because we would then be expecting the model to learn parts of the input space it will not have seen in the training dataset.\nIt is possible to use machine learning to build a model that approximates solutions to the forward radiative transfer model (see Figure 4-3).\nThere is an important difference between training an ML model to approximate the solution to a dynamical system like this and training an ML model to predict baby weight based on natality data collected over the years.\nWe want our ML model to fit the training data as perfectly as possible, to “overfit.”\nTraditional training says that it is possible for a model to learn the training data “too well,” and that training your model so that the train loss function is equal to zero is more of a red flag than cause for celebration.\nOverfitting of the training dataset in this way causes the model to give misguided predictions on new, unseen data points.\nIf all possible inputs can be tabulated, then as shown by the dotted curve in Figure 4-4, an overfit model will still make the same predictions as the “true” model if all possible input points are trained for.",
      "keywords": [
        "training dataset",
        "Training",
        "training loop",
        "Model",
        "typical training loop",
        "dataset",
        "Machine learning",
        "Machine learning models",
        "learning",
        "loop",
        "Model Training Patterns",
        "typical training",
        "Training Loop Machine",
        "training data",
        "Machine"
      ],
      "concepts": [
        "model",
        "training",
        "dataset",
        "machines",
        "learn",
        "computational",
        "compute",
        "predictions",
        "prediction",
        "predicts"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "training",
          "dataset",
          "loop",
          "training dataset",
          "training loop"
        ],
        "semantic": [],
        "merged": [
          "training",
          "dataset",
          "loop",
          "training dataset",
          "training loop"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40047736635877057,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725842+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 180-188)",
      "start_page": 180,
      "end_page": 188,
      "summary": "The machine learning model essentially functions as an approximation to a lookup table of inputs to outputs.\nThere is no need to approximate it by a machine learning model.\nIt is when the lookup table is too unwieldy that it becomes better to treat it as the training dataset for a machine learning model that approximates the lookup table.\nMachine learning models interpolate by weighting unseen values by the distance of these unseen values from training examples.\nTherefore, using a combination of low-complexity models and mild regularization provides a practical way to avoid unacceptable overfitting in the case of Monte Carlo selection of the input space.\nAnother situation where overfitting is warranted is in distilling, or transferring knowledge, from a large machine learning model into a smaller one.\nHowever, it is also the case that training smaller models is harder.\nThe solution is to train the smaller model on a large amount of generated data that is labeled by the larger model.\nOverfitting on a small batch is a good sanity check both for the model code as well as the data input pipeline.\nA complex enough model should be able to overfit on a small enough batch of data, assuming everything is set up correctly.\nSo, if you’re not able to overfit a small batch with any model, it’s worth rechecking your model code, input pipeline, and loss function for any errors or simple bugs.\nOverfitting on a batch is a useful technique when training and troubleshooting neural networks.\nThen, once you have a large model that overfits the training set, you can apply regularization to improve the validation accuracy, even though training accuracy may decrease.\nFor example, if your training data input pipeline is called trainds, we’ll use batch() to pull a single batch of data.\nThen, when training the model, instead of calling the full trainds dataset inside the fit() method, use the single batch that we created:\nmodel.fit(single_batch.repeat(), validation_data=evalds, …)\nIn Checkpoints, we store the full state of the model periodically so that we have partially trained models available.\nThe more complex a model is (for example, the more layers and nodes a neural network has), the larger the dataset that is needed to train it effectively.\nAs model sizes increase, the time it takes to fit one batch of examples also increases.\nAt the time of writing, training an English-to-German translation model on a state-of-the-art tensor processing unit (TPU) pod on a relatively small dataset takes about two hours.\nThen, if the training loop is interrupted for any reason, we can go back to the saved model state and restart.\nOnce training is complete, we save or export the model so that we can deploy it for inference.\nWhat data on model state do we need when restoring from a checkpoint that an exported model does not contain?\nAn exported model does not contain which epoch and batch number the model is currently processing, which is obviously important in order to resume training.\nBut there is more information that a model training loop can contain.\nSaving the full model state so that model training can resume from a point is called checkpointing, and the saved model files are called checkpoints.\nTo checkpoint a model in Keras, provide a callback to the fit() method:\ncheckpoint_path = '{}/checkpoints/taxi'.format(OUTDIR) cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=False, verbose=1) history = model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=(x_val, y_val), verbose=2, callbacks=[cp_callback])",
      "keywords": [
        "model",
        "machine learning model",
        "model state",
        "training",
        "learning model",
        "batch",
        "Deep Galerkin Method",
        "overfitting",
        "learning",
        "exported model",
        "machine learning",
        "smaller model",
        "Monte Carlo methods",
        "full model state",
        "Monte Carlo"
      ],
      "concepts": [
        "models",
        "training",
        "useful",
        "data",
        "dataset",
        "points",
        "inputs",
        "method",
        "network",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.58,
          "base_score": 0.43,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "batch",
          "state",
          "model state",
          "overfitting",
          "training"
        ],
        "semantic": [],
        "merged": [
          "batch",
          "state",
          "model state",
          "overfitting",
          "training"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3123286774791811,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725899+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 189-197)",
      "start_page": 189,
      "end_page": 197,
      "summary": "To implement checkpoints in PyTorch, ask for the epoch, model state, optimizer state, and any other information needed to resume training to be serialized along with the model:\nEven though checkpoints are designed primarily to support resilience, the availability of partially trained models opens up a number of other use cases.\nStarting the training by clicking on the arrow at the top left of the image, we see the model slowly start to learn with successive epochs, as shown in Figure 4-8.\nWhat the model learns as training progresses.\nThe graphs at the top are the training loss and validation error, while the images show how the model at that stage would predict the color of a point at each coordinate in the grid.\nBy the time we get to Figure 4-8(e), the adjustment of weights is starting to reflect random perturbations in the training data, and these are counterproductive on the validation dataset.\nIf you are starting to overfit to the training dataset, the validation error might even start to increase, as shown in Figure 4-9.\nTypically, the training loss continues to drop the longer you train, but once overfitting starts, the validation error on a withheld dataset starts to go up.\nIn such cases, it can be helpful to look at the validation error at the end of every epoch and stop the training process when the validation error is more than that of the previous epoch.\nWhile early stopping can be implemented by stopping the training as soon as the validation error starts to increase, we recommend training longer and choosing the optimal run as a postprocessing step.\nIn our example, instead of exporting the model at the end of the training run, we will load up the fourth checkpoint and export our final model from there instead.\nInstead of using early stopping or checkpoint selection, it can be helpful to try to add L2 regularization to your model so that the validation error does not increase and the model never gets into phase 3.\nEven if we are not doing early stopping, displaying the progress of the model training can be helpful, particularly if the model takes a long time to train.\nAlthough the performance and progress of the model training is normally monitored on the validation dataset during the training loop, it is for visualization purposes only.\nThe reason that using regularization might be better than early stopping is that regularization allows you to use the entire dataset to change the weights of the model, whereas early stopping requires you to waste 10% to 20% of your dataset purely to decide when to stop training.\nUsing regularization rather than early stopping or checkpoint selection allows you to use a larger training dataset.\nIn the experimentation phase (when you are exploring different model architectures, training techniques, and hyperparameters), we recommend that you turn off early stopping and train with larger models (see also “Design Pattern 11: Useful Overfitting”).\nAt the end of experimentation, you can use the evaluation dataset to diagnose how well your model does on data it has not encountered during training.\nTurn on early stopping or checkpoint selection and monitor the error metric on the evaluation dataset.\nThis corresponds to the start of phase 2 in our discussion of the phases of model training described earlier in “Why It Works”.\nResume from a checkpoint from before the training loss starts to plateau.\nIn some cases, the final checkpoint (that is used to serve the model) can be used as a warm start for another model training iteration.",
      "keywords": [
        "model",
        "training",
        "early stopping",
        "validation error",
        "checkpoint",
        "validation",
        "model training",
        "dataset",
        "stopping",
        "error",
        "early",
        "training dataset",
        "training loss",
        "Checkpoint selection",
        "validation dataset"
      ],
      "concepts": [
        "models",
        "training",
        "checkpoints",
        "dataset",
        "epochs",
        "start",
        "error",
        "learning",
        "optimal",
        "optimizing"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 23,
          "title": "",
          "score": 0.699,
          "base_score": 0.549,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 26,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 34,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stopping",
          "early",
          "early stopping",
          "validation error",
          "checkpoint"
        ],
        "semantic": [],
        "merged": [
          "stopping",
          "early",
          "early stopping",
          "validation error",
          "checkpoint"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29381163935202537,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.725955+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 198-205)",
      "start_page": 198,
      "end_page": 205,
      "summary": "If you get 100,000 more examples and you train the model and get a higher error, is it because you need to do an early stop, or is the new data corrupt in some way?\nNUM_STEPS = 143000 BATCH_SIZE = 100 NUM_CHECKPOINTS = 15 cp_callback = tf.keras.callbacks.ModelCheckpoint(...) history = model.fit(trainds, validation_data=evalds, epochs=NUM_CHECKPOINTS, steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS, batch_size=BATCH_SIZE, callbacks=[cp_callback])\nThe answer is to keep the total number of training examples shown to the model (not number of steps; see Figure 4-12) constant:\nNUM_TRAINING_EXAMPLES = 1000 * 1000 STOP_POINT = 14.3 TOTAL_TRAINING_EXAMPLES = int(STOP_POINT * NUM_TRAINING_EXAMPLES) BATCH_SIZE = 100 NUM_CHECKPOINTS = 15 steps_per_epoch = (TOTAL_TRAINING_EXAMPLES // (BATCH_SIZE*NUM_CHECKPOINTS)) cp_callback = tf.keras.callbacks.ModelCheckpoint(...) history = model.fit(trainds, validation_data=evalds, epochs=NUM_CHECKPOINTS, steps_per_epoch=steps_per_epoch,\nIn Transfer Learning, we take part of a previously trained model, freeze the weights, and incorporate these nontrainable layers into a new model that solves a similar problem, but on a smaller dataset.\nTraining custom ML models on unstructured data requires extremely large datasets, which are not always readily available.\nBefore your model learns what a broken bone looks like, it needs to first learn to make sense of the pixels, edges, and shapes that are part of the images in your dataset.\nThe same is true for models trained on text data.\nTo see just how much data is required to train high-accuracy models, we can look at ImageNet, a database of over 14 million labeled images.\nBecause such models are often trained on a wide variety of high-level label categories, we wouldn’t expect them to understand conditions present in the images that are specific to our dataset.\nWith the Transfer Learning design pattern, we can take a model that has been trained on the same type of data for a similar task and apply it to a\nTo do transfer learning for image classification, for example, it is better to start with a model that has been trained for image classification, rather than object detection.\nThis isn’t enough to train a high-quality model from scratch, but it is sufficient for transfer learning.\nTo solve this with transfer learning, we’ll need to find a model that has already been trained on a large dataset to do image classification.\nWe’ll then remove the last layer from that model, freeze the weights of that model, and continue training using our 400 x-ray images.\nWe’d ideally find a model trained on a dataset with similar images to our x-rays, like images taken in a lab or another controlled condition.\nYou can use transfer learning for many prediction tasks in addition to image classification, so long as there is an existing pre-trained model that matches the task you’d like to perform on your dataset.\nTransfer learning works because it lets us stand on the shoulders of giants, utilizing models that have already been trained on extremely large, labeled datasets.\nThe idea behind transfer learning is that you can utilize the weights and layers from a model trained in the same domain as your prediction task.\nWith transfer learning, we remove this layer, freeze the model’s trained weights, and replace the final layer with the output for our specialized prediction task before continuing to train.\nTransfer learning involves training a model on a large dataset.\nTo see how this works, let’s continue with a medical imaging example, but this time we’ll build a model with a colorectal histology dataset to classify the histology images into one of eight categories.",
      "keywords": [
        "model",
        "Transfer Learning",
        "training",
        "data",
        "NUM",
        "Steps",
        "Learning",
        "Transfer",
        "dataset",
        "epochs",
        "number",
        "images",
        "checkpoints",
        "size",
        "batch"
      ],
      "concepts": [
        "model",
        "images",
        "imaging",
        "data",
        "training",
        "dataset",
        "layers",
        "learning",
        "examples",
        "transfer"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 22,
          "title": "",
          "score": 0.699,
          "base_score": 0.549,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.644,
          "base_score": 0.494,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 24,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transfer",
          "transfer learning",
          "num_checkpoints",
          "batch_size",
          "steps_per_epoch"
        ],
        "semantic": [],
        "merged": [
          "transfer",
          "transfer learning",
          "num_checkpoints",
          "batch_size",
          "steps_per_epoch"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3078284435096014,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726010+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 206-213)",
      "start_page": 206,
      "end_page": 213,
      "summary": "To explore the model we are going to use for transfer learning, let’s load the VGG model architecture pre-trained on the ImageNet dataset:\nNotice that we’ve set include_top=True, which means we’re loading the full VGG model, including the output layer.\nFor ImageNet, the model classifies images into 1,000 different classes, so the output layer is a 1,000- element array.\nLet’s look at the output of model.summary() to understand which layer will be used as the bottleneck.\nModel: \"vgg19\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 ...more layers here...\nIn this example, we’ll choose the block5_pool layer as the bottleneck layer when we adapt this model to be trained on our medical histology images.\nIt’s also worth noting that setting include_top=False is hardcoded to use block5_pool as the bottleneck layer, but if you want to customize this, you can load the full model and delete any additional layers you don’t want to use.\nBefore this model is ready to be trained, we’ll need to add a few layers on top, specific to our data and classification task.\nFor example, in an autoencoder model with an encoder-decoder architecture, the bottleneck layer is an embedding.\nIn this case, the bottleneck serves as the middle layer of the model, mapping the original input data to a lower-dimensionality representation, which the decoder (the second half of the network) uses to map the input back to its original, higher-dimensional representation.\nIn other words, the entire network up to and including the bottleneck layer is nontrainable, and the weights in the layers after the bottleneck are the only trainable layers in the model.\nWhen you build a model that includes an embedding layer, you can either utilize an existing (pre-trained) embedding lookup, or train your own embedding layer from scratch.\nTransfer learning always makes use of a bottleneck layer with nontrainable, frozen weights.\nIf the purpose is to train a similar model, you would use transfer learning.\nLoading a pre-trained model on your own, removing the layers after the bottleneck, and adding a new final layer with your own data and labels\nTogether, these make up the pre-trained model we’ll be using for transfer learning.\nLoading a pre-trained model and using it to get classifications on the original labels that model was trained on is not transfer learning.\nTransfer learning is going one step further, replacing the final layers of the model with your own prediction task.\nFinally, we can use the Sequential, API to create our new transfer learning model as a stack of layers:\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= vgg19 (Model) (None, 4, 4, 512) 20024384 _________________________________________________________________\nIn this example, the bottleneck layer is the feature vectors from the VGG model.\nWhile we can load a pre-trained model on our own, we can also implement transfer learning by making use of the many pre-trained models available in TF Hub, a library of pre-trained models (called modules).\nThe next layers in the model begin to understand groups of edges—perhaps that there are two edges that meet toward the top-left corner of the image.",
      "keywords": [
        "model",
        "layer",
        "bottleneck layer",
        "VGG model",
        "transfer learning",
        "VGG",
        "learning",
        "bottleneck",
        "output layer",
        "transfer",
        "Output Shape Param",
        "transfer learning model",
        "output",
        "pre-trained model",
        "image"
      ],
      "concepts": [
        "model",
        "layer",
        "images",
        "dimensionality",
        "learning",
        "different",
        "difference",
        "dataset",
        "features",
        "keras"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 25,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.601,
          "base_score": 0.451,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bottleneck",
          "layer",
          "transfer",
          "transfer learning",
          "pre trained"
        ],
        "semantic": [],
        "merged": [
          "bottleneck",
          "layer",
          "transfer",
          "transfer learning",
          "pre trained"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3137719711424509,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726067+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 214-221)",
      "start_page": 214,
      "end_page": 221,
      "summary": "We’ll also discuss why transfer learning is primarily focused on image and text models and look at the relationship between text sentence embeddings and transfer learning.\nFeature extraction describes an approach to transfer learning where you freeze the weights of all layers before the bottleneck layer and train the following layers on your own data and labels.\nAnother option is instead fine-tuning the weights of the pre-trained model’s layers.\nWith fine-tuning, you can either update the weights of each layer in the pre-trained model, or just a few of the layers right before the bottleneck.\nTraining a transfer learning model using fine-tuning typically takes longer than feature extraction.\nWhen fine-tuning, it’s common to leave the weights of the model’s initial layers frozen since these layers have been trained to recognize basic features that are often common across many types of images.\nTo fine-tune a MobileNet model, for example, we’d set trainable=False only for a subset of layers in the model, rather than making every layer non-trainable.\nTo implement progressive fine-tuning, start by unfreezing only the last layer of your transferred model (the layer closest to the output) and calculate your model’s loss after training.\nHow should you determine whether to fine-tune or freeze all layers of your pre-trained model?\nTypically, when you’ve got a small dataset, it’s best to use the pre-trained model as a feature extractor rather than fine-tuning.\nIf you’re retraining the weights of a model that was likely trained on thousands or millions of examples, fine-tuning can cause the updated model to overfit to your small dataset and lose the more general information learned from those millions of examples.\nAnother factor to take into account when deciding whether to fine-tune is how similar your prediction task is to that of the original pre-trained model you’re using.\nWhen the prediction task is similar or a continuation of the previous training, as it was in our movie review sentiment analysis model, fine-tuning can produce higher-accuracy results.\nWhen the task is different or the datasets are significantly different, it’s best to freeze all the layers of the pre-trained model instead of fine-tuning them.\nIs your prediction task the same as that of the pre-trained model?\nBecause these tasks are different, we should use the original model as a feature extractor rather than fine-tune it.\nAn example of different prediction tasks in an image domain might be using our MobileNet model trained on ImageNet as a basis for doing transfer learning on a dataset of medical images.\nModels trained with tabular data, however, cover a potentially infinite number of possible prediction tasks and data types.\nAlthough transfer learning is not yet as common on tabular data as it is for image and text domains, a new model architecture called TabNet presents novel research in this area.\nThere are several approaches for generating sentence embeddings—from averaging a sentence’s word embeddings to training a supervised learning model on a large corpus of text to generate the embeddings.\nIn fact, it’s been shown that increasing the scale of deep learning, with respect to the number of training examples, the number of model parameters, or both, drastically improves model performance.\nHowever, as the size of models and data increases, the computation and memory demands increase proportionally, making the time it takes to train these models one of the biggest problems of deep learning.\nHowever, for very large models trained on massive amounts of data, individual GPUs aren’t enough to make the training time tractible.\nTo implement data parallelism, there must be a method in place for different workers to compute gradients and share that information to make updates to the model parameters.\nLarge models could cause I/O bottlenecks as data is passed from the CPU to the GPU during training, and slow networks could also cause delays.",
      "keywords": [
        "model",
        "Training",
        "pre-trained model",
        "data",
        "transfer learning",
        "learning",
        "layers",
        "fine-tuning",
        "embeddings",
        "learning model",
        "word embeddings",
        "text",
        "feature extraction",
        "feature",
        "text models"
      ],
      "concepts": [
        "model",
        "train",
        "layer",
        "data",
        "different",
        "differ",
        "learning",
        "sentences",
        "distribution",
        "distributed"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 24,
          "title": "",
          "score": 0.721,
          "base_score": 0.571,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.671,
          "base_score": 0.521,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.648,
          "base_score": 0.498,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.607,
          "base_score": 0.457,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fine",
          "fine tuning",
          "layers",
          "tuning",
          "pre trained"
        ],
        "semantic": [],
        "merged": [
          "fine",
          "fine tuning",
          "layers",
          "tuning",
          "pre trained"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3275150672156252,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726125+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 222-231)",
      "start_page": 222,
      "end_page": 231,
      "summary": "In synchronous training, each worker holds a copy of the model and computes gradients using a slice of the training data mini-batch.\nTo implement this mirrored strategy in Keras, you first create an instance of the mirrored distribution strategy, then move the creation and compiling of the model inside the scope of that instance.\nWrapping the model code in the distribution strategy scope is all you need to do to enable distributed training.\nDuring training, each batch of the input data is divided equally among the multiple workers.\nDISTRIBUTED DATA PARALLELISM IN PYTORCH In PyTorch, the code always uses DistributedDataParallel whether you have one GPU or multiple GPUs and whether the model is run on one machine or multiple machines.\nA distributed version of the model that will process its shard of batch is created using DistributedDataParallel:\nIn asynchronous training, the workers train on different slices of the input data independently, and the model weights and parameters are updated asynchronously, typically through a parameter server architecture.\nAs with synchronous training, a mini-batch of data is split among each of the separate workers for each SGD step.\nEach device performs a forward pass with their portion of the mini-batch and computes gradients for each parameter of the model.\nThose gradients are sent to the parameter server, which performs the parameter update and then sends the new model parameters back to the worker with another split of the next mini-batch.\nIn asynchronous training, each worker performs a gradient descent step with a split of the mini-batch.\nIn Keras, ParameterServerStrategy implements asynchronous parameter server training on multiple machines.\nThe parameter servers hold each variable of the model, and computation is performed on the workers, typically GPUs.\nSynchronous training is particularly vulnerable to slow devices or poor network connection because training will stall waiting for updates from all workers.\nDistributed training schemes drastically increase the throughput of data processed by these models and can effectively decrease training times from weeks to hours.\nFigure 4-17 compares the throughput of training data, in this case images, with different distribution setups.\nMost notable is that throughput increases with the number of worker nodes and, even though parameter servers perform tasks not related to the computation done on the GPU’s workers, splitting the workload among more machines is the most advantageous strategy.\nIn addition to data parallelism, there are other aspects of distribution to consider, such as model parallelism, other training accelerators—(such as TPUs) and other considerations (such as I/O limitations and batch size).\nIn order to train models this big, they must be split up over multiple devices, as shown in Figure 4-19.\nEach device operates over the same mini-batch of data during training, but carries out computations related only to their separate components of the model.\nOutside of the training paradigm, model parallelism provides an added benefit for serving very large models where low latency is needed.\nDistributing the computation of a large model across multiple devices can vastly reduce the overall computation time when making online predictions.\nThis is because it is the model weights (and their gradient updates) that are being passed between different workers.\nMesh TensorFlow is a library optimized for distributed deep learning that combines synchronous data parallelism with model parallelism.\nSplitting across the batch layer is synonymous with data parallelism, while splitting over any other dimension—for example, a dimension representing the size of a hidden layer—achieves model parallelism.",
      "keywords": [
        "model",
        "model parallelism",
        "training",
        "data",
        "workers",
        "DATA PARALLELISM",
        "PARALLELISM",
        "distribution",
        "multiple",
        "GPUs",
        "model parameters",
        "parameter",
        "device",
        "parameter server",
        "multiple devices"
      ],
      "concepts": [
        "model",
        "distribution",
        "distributed",
        "distribute",
        "training",
        "worker",
        "data",
        "device",
        "layers",
        "strategy"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.613,
          "base_score": 0.463,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 22,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.595,
          "base_score": 0.445,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "parallelism",
          "workers",
          "parameter",
          "mini batch",
          "batch"
        ],
        "semantic": [],
        "merged": [
          "parallelism",
          "workers",
          "parameter",
          "mini batch",
          "batch"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2796927813338992,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726181+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 232-239)",
      "start_page": 232,
      "end_page": 239,
      "summary": "Large batch sizes have been shown to adversely affect the quality of the final trained model.\nThus, setting the mini-batch size in the context of distributed training is a complex optimization space of its own, as it affects both statistical accuracy (generalization) and hardware efficiency (utilization) of the model.\nFor example, tf.data.Dataset.prefetch overlaps the preprocessing and model execution of a training step so that while the model is executing training step N, the input pipeline is reading and preparing data for training step N + 1, as shown in Figure 4-22.\nIn Hyperparameter Tuning, the training loop is itself inserted into an optimization method to find the optimal set of model hyperparameters.\nIn machine learning, model training involves finding the optimal set of breakpoints (in the case of decision trees), weights (in the case of neural networks), or support vectors (in the case of support vector machines).\nYou do not have direct control over model parameters, since they are largely a function of your training data, model architecture, and many other factors.\nYour model’s weights are initialized with random values and then optimized by your model as it goes through training iterations.\nImagine you are training an image classification model that takes hours to train on GPUs. You settle on a few hyperparameter values to try and then wait for the results of the first training run.\nBased on these results, you tweak the hyperparameters, train the model again, compare the results with the first run, and then settle on the best hyperparameter values by looking at the training run with the best metrics.\nWe’re using the term trial here to refer to a single training run with a set of hyperparameter values.\ngrid search, we choose a list of possible values we’d like to try for each hyperparameter we want to optimize.\nFor example, in scikit-learn’s RandomForestRegressor() model, let’s say we want to try the following combination of values for the model’s max_depth and n_estimators hyperparameters:\nUsing grid search, we’d try every combination of the specified values, then use the combination that yielded the best evaluation metric on our model.\nWe can run grid search by creating an instance of the GridSearchCV class, and training the model passing it the values we defined earlier:\nWe’d want to compare this to the error we’d get training a random forest regressor model without hyperparameter tuning, using scikit-learn’s default values for these parameters.\nThis grid search approach works OK on the small example we’ve defined above, but with more complex models, we’d likely want to optimize more than two hyperparameters, each with a wide range of possible values.\nEventually, grid search will lead to combinatorial explosion—as we add additional hyperparameters and values to our grid of options, the number of possible combinations we need to try and the time required to try them all increases significantly.\nInstead of trying every possible combination of hyperparameters from a set, you determine the number of times you’d like to randomly sample values for each hyperparameter.\nRandom search runs faster than grid search since it doesn’t try every combination in your set of possible values, but it is very likely that the optimal set of hyperparameters will not be among the ones randomly selected.\nFor robust hyperparameter tuning, we need a solution that scales and learns from previous trials to find an optimal combination of hyperparameter values.\nWe can then use hp throughout the function wherever we want to include a hyperparameter, specifying the hyperparameter’s name, data type, the value range we’d like to search, and how much to increment it each time we try a new one.",
      "keywords": [
        "model",
        "training",
        "Grid search",
        "Hyperparameter",
        "Grid",
        "search",
        "training step",
        "model training",
        "TPUs",
        "model parameters",
        "Hyperparameter Tuning",
        "data",
        "distributed training",
        "combination",
        "step"
      ],
      "concepts": [
        "model",
        "training",
        "value",
        "optimization",
        "optimized",
        "optimal",
        "optimize",
        "data",
        "keras",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.762,
          "base_score": 0.612,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 24,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hyperparameter",
          "search",
          "grid search",
          "try",
          "grid"
        ],
        "semantic": [],
        "merged": [
          "hyperparameter",
          "search",
          "grid search",
          "try",
          "grid"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3344687494669501,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726249+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 240-247)",
      "start_page": 240,
      "end_page": 247,
      "summary": "Although grid and random search are more efficient than a trial-and-error approach to hyperparameter tuning, they quickly become expensive for models requiring significant training time or having a large hyperparameter search space.\nSince both machine learning models themselves and the process of hyperparameter search are optimization problems, it would follow that we would be able to use an approach that learns to find the optimal hyperparameter combination within a given range of possible values just like our models learn from training data.\nWe can think of hyperparameter tuning as an outer optimization loop (see Figure 4-24) where the inner loop consists of typical model training.\nParameters related to model training, like the number of epochs, learning rate, and batch size, control the training loop and often have to do with the way that the gradient descent optimizer works.\nOn the other hand, a single trial in the hyperparameter tuning problem involves training a complete model on the training dataset and might take several hours.\nFor a classification model, your optimization metric might be accuracy, and you’d therefore want to find the combination of hyperparameters that leads to the highest model accuracy even if the loss is binary cross entropy.\nThe goal of Bayesian optimization is to directly train our model as few times as possible since doing so is costly.\nRemember that each time we try a new combination of hyperparameters on our model, we need to run through our model’s entire training cycle.\nInstead of training our model each time we try a new combination of hyperparameters, Bayesian optimization defines a new function that emulates our model but is much cheaper to run.\nThe surrogate function is called much more frequently than the objective function, with the goal of finding an optimal combination of hyperparameters before completing a training run on your model.\nFrom there, Bayesian optimization develops a surrogate function to simulate our model training process and uses that function to determine the best combination of hyperparameters to run on our model.\nIt is only once this surrogate arrives at what it thinks is a good combination of hyperparameters that we do a full training run (trial) on our model.\nGenetic algorithms are an alternative to Bayesian methods for hyperparameter tuning, but they tend to require many more model training runs than Bayesian methods.\nWe’ll also show you how to use a managed service for hyperparameter tuning optimization on models built with a variety of ML frameworks.\nFully managed hyperparameter tuning The keras-tuner approach may not scale to large machine learning problems because we’d like the trials to happen in parallel, and the likelihood of machine error and other failure increases as the time for model training stretches into the hours.\nThe underlying concepts of the Cloud service work similarly to keras- tuner: you specify each hyperparameter’s name, type, range, and scale, and these values are referenced in your model training code.\nWe’ll show you how to run hyperparameter tuning in AI Platform using a PyTorch model trained on the BigQuery natality dataset to predict a baby’s birth weight.\nIn this example, we’ll tune three hyperparameters—our model’s learning rate, the optimizer’s momentum value, and the number of neurons in our model’s hidden layer.\nAt the end of our model training code, we’ll create an instance of HyperTune(), and tell it the metric we’re trying to optimize.\nAfter each trial completes, you’ll be able to see the values chosen for each hyperparameter and the resulting value of your optimization metric, as seen in Figure 4-25.",
      "keywords": [
        "model",
        "Bayesian optimization",
        "hyperparameter tuning",
        "hyperparameter",
        "model training",
        "optimization",
        "training",
        "Bayesian",
        "optimization metric",
        "tuning",
        "function",
        "run",
        "metric",
        "model training process",
        "machine learning model"
      ],
      "concepts": [
        "model",
        "optimize",
        "optimal",
        "optimized",
        "optimizing",
        "training",
        "functionality",
        "function",
        "functions",
        "parameter"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 29,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.713,
          "base_score": 0.563,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "",
          "score": 0.561,
          "base_score": 0.411,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hyperparameter",
          "optimization",
          "bayesian",
          "combination hyperparameters",
          "hyperparameter tuning"
        ],
        "semantic": [],
        "merged": [
          "hyperparameter",
          "optimization",
          "bayesian",
          "combination hyperparameters",
          "hyperparameter tuning"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2836485829235996,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726304+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 248-255)",
      "start_page": 248,
      "end_page": 255,
      "summary": "By default, AI Platform Training will use Bayesian optimization for your tuning job, but you can also specify if you’d like to use grid or random search algorithms instead.\nIf we run another training job similar to the one above, but with a few tweaks to our hyperparameters and search space, it’ll use the results of our last job to efficiently choose values for the next set of trials.\nWe’ve shown a PyTorch example here, but you can use AI Platform Training for hyperparameter tuning in any machine learning framework by packaging your training code and providing a setup.py file that installs any library dependencies.\nRather than using a surrogate function as a proxy for model training like in Bayesian optimization, genetic algorithms require training your model for each possible combination of hyperparameter values.\nThis chapter focused on design patterns that modify the typical SGD training loop of machine learning.\nstate of the model periodically during training.\nThe Transfer Learning design pattern covered reusing parts of a previously trained model.\nLastly, the Hyperparameter Tuning design pattern discussed how the SGD training loop itself can be optimized with respect to model hyperparameters.\nThe next chapter looks at design patterns related to resilience (to large numbers of requests, spiky traffic, or change management) when placing models into production.\nThe purpose of a machine learning model is to use it to make inferences on data it hasn’t seen during training.\nThe design patterns in this chapter solve problems associated with resilience under different circumstances as it relates to production ML models.\nThe Stateless Serving Function design pattern allows the serving infrastructure to scale and handle thousands or even millions of prediction requests per second.\nThe Two-Phase Predictions design pattern provides a way to address the problem of keeping models sophisticated and performant when they have to be deployed onto distributed devices.\nDesign Pattern 16: Stateless Serving Function\nThe Stateless Serving Function design pattern makes it possible for a production ML system to synchronously handle thousands to millions of\nThe production ML system is designed around a stateless function that captures the architecture and weights of a trained model.\nA function that maintains a counter of the number of times it has been invoked and returns a different value depending on whether the counter is odd or even is an example of a function that is stateful, not stateless:\nIn a machine learning model, there is a lot of state captured during training.\nBy saying that the model has to be exported as a stateless function, we are requiring the model framework creators to keep track of these stateful variables and not include them in the exported file.\nFor example, some model functions are inherently stateful.",
      "keywords": [
        "design pattern",
        "model",
        "Stateless Serving Function",
        "Training",
        "Stateless",
        "function",
        "Function design pattern",
        "design",
        "pattern",
        "stateless function",
        "Serving Function design",
        "search",
        "hyperparameter tuning",
        "hyperparameter",
        "Genetic algorithms"
      ],
      "concepts": [
        "models",
        "training",
        "pattern",
        "designed",
        "state",
        "learning",
        "distribution",
        "distributed",
        "large",
        "clients"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 31,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stateless",
          "function",
          "design",
          "hyperparameter",
          "design pattern"
        ],
        "semantic": [],
        "merged": [
          "stateless",
          "function",
          "design",
          "hyperparameter",
          "design pattern"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31585667550830326,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726359+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 256-263)",
      "start_page": 256,
      "end_page": 263,
      "summary": "In such cases, needing to export the model as a stateless function requires changing the input from a single word to, for example, a sentence.\nLet’s take a text classification model that uses, as its training data, movie reviews from the Internet Movie Database (IMDb).\nFor the initial layer of the model, we will use a pre-trained embedding that maps text to 20- dimensional embedding vectors (for the full code, see the serving_function.ipynb notebook in the GitHub repository for this book):\nmodel = tf.keras.Sequential() embedding = ( \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\") hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True, name='full_text') model.add(hub_layer) model.add(tf.keras.layers.Dense(16, activation='relu', name='h1_dense')) model.add(tf.keras.layers.Dense(1, name='positive_review_logits'))\nThis model can then be trained on the dataset of movie reviews to learn to predict whether or not a review is positive or negative.\nOnce the model has been trained, we can use it to carry out inferences on how positive a review is:\nI walked out half-way.' logits = model.predict(x=tf.constant([review1, review2, review3]))\nThe model input and output that is most effective for training may not be user friendly.\nThe first step of the solution is to export the model into a format (TensorFlow uses SavedModel, but ONNX is another choice) that captures the mathematical core of the model.\nIn a production system, the model’s formula is restored from the protocol buffer and other associated files as a stateless function that conforms to a specific model signature with input and output variable names and data types.\nWe can use the TensorFlow saved_model_cli tool to examine the exported files to determine the signature of the stateless function that we can use in serving:\nsaved_model_cli show --dir ${export_path} \\ --tag_set serve --signature_def serving_default\nThe given SavedModel SignatureDef contains the following input(s): inputs['full_text_input'] tensor_info: dtype: DT_STRING shape: (-1) name: serving_default_full_text_input:0 The given SavedModel SignatureDef contains the following output(s): outputs['positive_review_logits'] tensor_info: dtype: DT_FLOAT shape: (-1, 1) name: StatefulPartitionedCall_2:0 Method name is: tensorflow/serving/predict\nmodel.add(tf.keras.layers.Dense(1, name='positive_review_logits'))\nserving_fn = tf.keras.models.load_model(export_path).\n\\ signatures['serving_default'] outputs = serving_fn(full_text_input= tf.constant([review1, review2, review3])) logit = outputs['positive_review_logits']\nserving_fn = None def handler(request): global serving_fn if serving_fn is None: serving_fn = (tf.keras.models.load_model(export_path) .signatures['serving_default']) request_json = request.get_json(silent=True) if request_json and 'review' in request_json: review = request_json['review']\noutputs = serving_fn(full_text_input=tf.constant([review])) return outputs['positive_review_logits']\nThe approach of exporting a model to a stateless function and deploying the stateless function in a web application framework works because web application frameworks offer autoscaling, can be fully managed, and are language neutral.\nThus, on Google Cloud, deploying the serving function as a REST API is as simple as running this command-line program providing the location of the SavedModel output:\nBecause the cloud service abstracts the specifics of our ML model, we don’t need to provide any references to Keras or TensorFlow:",
      "keywords": [
        "model",
        "serving",
        "serving function",
        "function",
        "review",
        "stateless function",
        "input",
        "request",
        "export",
        "logits",
        "output",
        "Cloud",
        "JSON",
        "positive",
        "film"
      ],
      "concepts": [
        "models",
        "output",
        "cloud",
        "export",
        "embedding",
        "reviews",
        "tensorflow",
        "likely",
        "predict",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 31,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 25,
          "title": "",
          "score": 0.465,
          "base_score": 0.465,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 12,
          "title": "",
          "score": 0.446,
          "base_score": 0.446,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "",
          "score": 0.416,
          "base_score": 0.416,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 23,
          "title": "",
          "score": 0.412,
          "base_score": 0.412,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "serving_fn",
          "tf",
          "review",
          "positive_review_logits",
          "stateless function"
        ],
        "semantic": [],
        "merged": [
          "serving_fn",
          "tf",
          "review",
          "positive_review_logits",
          "stateless function"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24810042137433622,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.726402+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 264-272)",
      "start_page": 264,
      "end_page": 272,
      "summary": "The Stateless Serving Function design pattern allows us to change the serving signature to provide extra functionality, like additional pre- and postprocessing, beyond what the ML model does.\nThis design pattern can also help with creating low-latency, online prediction for models that are trained on systems, such as data warehouses, that are typically associated with long-running queries.\nWhen we use the model for prediction, the model naturally returns what it was trained to predict and outputs the logits.\nHere is a custom serving function in Keras that adds a probability and returns a dictionary that contains both the logits and the probabilities for each of the reviews provided as input:\n@tf.function(input_signature=[tf.TensorSpec([None], dtype=tf.string)]) def add_prob(reviews): logits = model(reviews, training=False) # call model probs = tf.sigmoid(logits) return { 'positive_review_logits' : logits, 'positive_review_probability' : probs }\nmodel.save(export_path, signatures={'serving_default': add_prob})\nThe serving signature of the exported model reflects the new input name (note the name of the input parameter to add_prob) and the output dictionary keys and data types:\nmodel.save(export_path, signatures={ 'serving_default': func1, 'expensive_result': func2, })\nBecause the exported serving function is ultimately just a file format, it can be used to provide online prediction capabilities when the original machine learning training framework does not natively support online predictions.\nOnce the model is trained, we can carry out prediction using SQL:\nIn order to carry out online prediction, we can ask BigQuery to export the model as a TensorFlow SavedModel:\nNow, we can deploy the SavedModel into a serving framework like Cloud AI Platform that supports SavedModel to get the benefits of low-latency, autoscaled ML model serving.\nInstead of deploying the serving function as a microservice that can be invoked via a REST API, it is possible to implement the prediction code as a library function.\nThe library function would load the exported model the first time it is called, invoke model.predict() with the provided input, and return the result.\nThe Batch Serving design pattern uses software infrastructure commonly used for distributed data processing to carry out inference on a large number of instances all at once.\nTherefore, when you deploy a model into an ML serving framework, it is set up to process one instance, or at most a few thousands of instances, embedded in a single request.\nBecause of this, the ML model needs to make predictions for millions of instances at a time, not one instance at a time.\nThe Batch Serving design pattern uses a distributed data processing infrastructure (MapReduce, Apache Spark, BigQuery, Apache Beam, and so on) to carry out ML inference on a large number of instances asynchronously.\nIn the discussion on the Stateless Serving Function design pattern, we trained a text classification model to output whether a review was positive or negative.\nSELECT * FROM ML.PREDICT(MODEL mlpatterns.imdb_sentiment, (SELECT 'This was very well done.' AS reviews) )\nSELECT * FROM ML.PREDICT(MODEL mlpatterns.imdb_sentiment, (SELECT consumer_complaint_narrative AS reviews FROM `bigquery-public-data`.cfpb_complaints.complaint_database WHERE consumer_complaint_narrative IS NOT NULL ) )\nIf these requests are not latency-sensitive, it is more cost effective to use a distributed data processing architecture to invoke machine learning models on millions of items.\nWITH all_complaints AS ( SELECT * FROM ML.PREDICT(MODEL mlpatterns.imdb_sentiment, (SELECT consumer_complaint_narrative AS reviews FROM `bigquery-public-data`.cfpb_complaints.complaint_database WHERE consumer_complaint_narrative IS NOT NULL )",
      "keywords": [
        "model",
        "Stateless Serving Function",
        "Serving Function",
        "Serving",
        "Serving Function design",
        "function",
        "Custom serving function",
        "Stateless Serving",
        "Function design pattern",
        "batch serving",
        "review",
        "design pattern",
        "Serving design pattern",
        "Batch Serving design",
        "SELECT"
      ],
      "concepts": [
        "model",
        "serving",
        "prediction",
        "predict",
        "predictions",
        "function",
        "processing",
        "process",
        "data",
        "units"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.595,
          "base_score": 0.445,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 30,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 23,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "serving",
          "serving function",
          "function",
          "logits",
          "reviews"
        ],
        "semantic": [],
        "merged": [
          "serving",
          "serving function",
          "function",
          "logits",
          "reviews"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3230328304233581,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726460+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 273-280)",
      "start_page": 273,
      "end_page": 280,
      "summary": "The second step reads the data from this shard (note the $12:shard in the query), but also obtains the file_path and file_contents of the machine learning model imdb_sentiment and applies the model to the data in each shard.\nFrameworks like Apache Spark or Apache Beam are useful when the input needs preprocessing before it can be supplied to the model, if the machine learning model outputs require postprocessing, or if either the preprocessing or postprocessing are hard to express in SQL.\nIf the inputs to the model are images, audio, or video, then SQL is not an option and it is necessary to use a data processing framework that can handle unstructured data.\nA common reason that the client needs to maintain state is if one of the inputs to the ML model is a time-windowed average.\nIn that case, the client code has to carry out moving averages of the incoming stream of data and supply the moving average to the ML model.\nWe discussed batch serving as a way to invoke a model over millions of items when the model is normally served online using the Stateless Serving Function design pattern.\nOf course, it is possible for batch serving to work even if the model does not support online serving.\nSELECT * FROM ML.RECOMMEND(MODEL mlpatterns.recommendation_model, ( SELECT DISTINCT visitorId FROM mlpatterns.analytics_session_data WHERE visitTime > TIME_DIFF(CURRENT_TIME(), 1 HOUR) ))\nIt is possible to take a TensorFlow model and import it into BigQuery for batch serving.\nIt is also possible to take a trained BigQuery ML model and export it as a TensorFlow SavedModel for online serving.\nYou’ve even gone through the painstaking process of deploying your model, taking it from a Jupyter notebook to a machine learning model in production, and are serving predictions via a REST API.\nThe world is dynamic, but developing a machine learning model usually creates a static model from historical data.\nTwo of the main reasons models degrade over time are concept drift and data drift.\nThis often happens because the underlying assumptions of your model have changed, such as models trained to learn adversarial or competitive behavior like fraud detection, spam filters, stock market trading, online ad bidding, or cybersecurity.\nAnother reason for a model’s performance to degrade over time is data drift.\nData drift refers to any change that has occurred to the data being fed to your model for prediction as compared to the data that was used for training.\nETL pipelines for building, training, and predicting with ML models can be brittle and opaque, and any of these changes would have drastic effects on the performance of your model.\nModel deployment is a continuous process, and to solve for concept drift or data drift, it is necessary to update your training dataset and retrain your\nmodel with fresh data to improve predictions.\nData preprocessing and model training can be costly both in time and money and each step of the model development cycle adds additional overhead of development, monitoring, and maintenance.\nThe most direct way to identify model deterioration is to continuously monitor your model’s predictive performance over time, and assess that performance with the same evaluation metrics you used during development.\nContinuous evaluation of this kind requires access to the raw prediction request data and the predictions the model generated as well as the ground truth, all in the same place.\nGoogle Cloud AI Platform provides the ability to configure the deployed model version so that the online prediction input and output are regularly sampled and saved to a table in BigQuery.",
      "keywords": [
        "probability DESC LIMIT",
        "model",
        "DESC LIMIT",
        "Batch Serving",
        "data",
        "Serving",
        "machine learning model",
        "learning model",
        "probability DESC",
        "Batch",
        "online serving",
        "data drift",
        "time",
        "Apache Beam",
        "Stateless Serving Function"
      ],
      "concepts": [
        "model",
        "data",
        "predictions",
        "predictive",
        "serving",
        "time",
        "steps",
        "complaints",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 43,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "drift",
          "serving",
          "online",
          "data drift",
          "desc"
        ],
        "semantic": [],
        "merged": [
          "drift",
          "serving",
          "online",
          "data drift",
          "desc"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36202936738454344,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726517+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 281-288)",
      "start_page": 281,
      "end_page": 288,
      "summary": "To see how continuous evaluation works, we’ll deploy a text classification model trained on the HackerNews dataset to Google Cloud AI Platform.\nContinuous evaluation allows us to monitor model predictions to track how those trends affect our model performance and kick off retraining if necessary.\nAfter deploying this model, when we make an online prediction, the model will return the predicted news source as a string value and a numeric score of that prediction label related to how confident the model is.\nWe can do this in the Continuous Evaluation section of the Google Cloud AI Platform (CAIP) console by specifying the LabelKey (the column that is the output of the model, which in our case will be source since we are predicting the source of the article), a ScoreKey in the prediction outputs (a numeric value, which in our case is confidence), and a table in BigQuery where a portion of the online prediction requests are stored.\nOnce this has been configured, whenever online predictions are made, CAIP streams the model name, the model version, the timestamp of the prediction request, the raw prediction input, and the model’s output to the specified BigQuery table, as shown in Table 5-1.\nIt is also necessary to capture the ground truth for each of the instances sent to the model for prediction.\nGround truth labels can also be derived from how users interact with the model and its predictions.\npossible to obtain implicit feedback for a model’s prediction or to produce a ground truth label.\nMore explicitly, when a user rates a recommended movie, this is a clear indication of the ground truth for a model that is built to predict user ratings in order to surface recommendations.\nSimilarly, if the model allows the user to change the prediction, for example, as in medical settings when a doctor is able to change a model’s suggested diagnosis, this provides a clear signal for the ground truth.\nIt is important to keep in mind how the feedback loop of model predictions and capturing ground truth might affect training data down the road.\nYou can even check the status of the cart at routine intervals to create ground truth labels for model evaluation.\nIn short, you’ve violated the assumptions of the model evaluation design and will need to determine ground truth labels some other way.\nThis task of estimating a particular outcome under a different scenario is referred to as counterfactual reasoning and often arises in use cases like fraud detection, medicine, and advertising where a model’s predictions likely lead to some intervention that can obscure learning the actual ground truth for that example.\nEvaluating model performance Initially, the groundtruth column of the txtcls_eval.swivel table in BigQuery is left empty.\nNote that the ground truth adheres to the same JSON structure as the prediction output from the model:\nOnce the ground truth has been added to the table, it’s possible to easily examine the text input and your model’s prediction and compare with the ground truth as in Table 5-2:\nSELECT model, model_version, time, REGEXP_EXTRACT(raw_data, r'.*\"text\": \"(.*)\"') AS text, REGEXP_EXTRACT(raw_prediction, r'.*\"source\": \"(.*?)\"') AS prediction, REGEXP_EXTRACT(raw_prediction, r'.*\"confidence\": (0.\\d{2}).*') AS confidence, REGEXP_EXTRACT(groundtruth, r'.*\"source\": \"(.*?)\"') AS groundtruth, FROM txtcls_eval.swivel\nOnce ground truth is available, it can be added to the original BigQuery table and the performance of the model can be evaluated\nFigure 5-3 shows the confusion matrix comparing this model’s predictions with the ground truth.\nA confusion matrix shows all pairs of ground truth labels and predictions so you can explore your model performance within different classes.\nWe should make sure the output table also captures the model version and the timestamp of prediction requests so that we can use the same table for continuous evaluation of two different model versions for comparing metrics between the models.\nFor example, if we deploy a newer version of our model, called swivel_v2, that is trained on more recent data or has different hyperparameters, we can compare their performance by slicing the evaluation dataframe according to the model version:\nSimilarly, we can create evaluation slices in time, focusing only on model predictions within the last month or the last week:",
      "keywords": [
        "model",
        "ground truth",
        "prediction",
        "Ground truth labels",
        "model predictions",
        "ground",
        "truth",
        "source",
        "evaluation",
        "continuous evaluation",
        "model version",
        "UTC",
        "confidence",
        "truth labels",
        "text"
      ],
      "concepts": [
        "models",
        "predictions",
        "prediction",
        "predicted",
        "evaluation",
        "evaluating",
        "evaluations",
        "label",
        "text",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 47,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 34,
          "title": "",
          "score": 0.658,
          "base_score": 0.508,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 36,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "truth",
          "ground truth",
          "ground",
          "truth labels",
          "table"
        ],
        "semantic": [],
        "merged": [
          "truth",
          "ground truth",
          "ground",
          "truth labels",
          "table"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33684545880603695,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726576+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 289-296)",
      "start_page": 289,
      "end_page": 296,
      "summary": "When developing a machine learning model, the train, validation, and test data come from the same data distribution.\nContinuous model evaluation provides a framework to evaluate a deployed model’s performance exclusively on new data.\nThis information helps determine how frequently to retrain a model or when to replace it with a new version entirely.\nIn this way, continuous evaluation provides a trigger for when to retrain the model.\nIn this case, it is important to consider tolerance thresholds for model performance, the trade-offs they pose, and the role of scheduled retraining.\nContinuous evaluation allows you to measure precisely how much in a structured way and provides a trigger to retrain the model.\nIn the context of retraining, the cloud event trigger would be a significant change or dip in model accuracy.\nThe function, or action taken, would be to invoke the training pipeline to retrain the model and deploy the new version.\nWorkflow pipelines containerize and orchestrate the end-to-end machine learning workflow from data collection and validation to model building, training, and deployment.\nFigure 5-5 shows this trade-off between the performance threshold and how it affects the number of model retraining jobs.\nIf the model retraining pipeline is automatically triggered by such a threshold, it is important to track and validate the triggers as well.\nSetting a higher threshold for model performance ensures a higher-quality model in production but will require more frequent retraining jobs, which can be costly.\nContinuous evaluation provides a crucial signal for knowing when it’s necessary to retrain your model.\nThis process of retraining is often carried out by fine-tuning the previous model using any newly collected training data.\nContinuous evaluation provides model evaluation each day as new data is collected.\nPeriodic retraining and model comparison provides evaluation at discrete time points.\nIn short, your model has gone stale, and it needs to be retrained on fresh data.\nData drift refers to any change that has occurred to the data being fed to your model for prediction as compared to the data used for training.\nWhile continuous evaluation provides a post hoc way of monitoring a deployed model, it is also valuable to monitor the new data that is received during serving and preemptively identify changes in data distributions.\nA useful and relatively cheap tactic to understand how data and concept drift affect your model is to train a model using only stale data and assess the performance of that model on more current data (Figure 5-8).\nHow much worse does your stale model perform on the current data?\nTraining a model on stale data and evaluating on current data mimics the continued model evaluation process in an offline environment.\nThe Two-Phase Predictions design pattern provides a way to address the problem of keeping large, complex models performant when they have to be deployed on distributed devices by splitting the use cases into two phases, with only the simpler phase being carried out on the edge.",
      "keywords": [
        "model",
        "data",
        "retraining",
        "model performance",
        "performance",
        "evaluation",
        "model evaluation",
        "deployed",
        "Data validation",
        "model retraining",
        "continuous evaluation",
        "Cloud",
        "edge",
        "training",
        "models deployed"
      ],
      "concepts": [
        "model",
        "data",
        "retrain",
        "time",
        "train",
        "performance",
        "performed",
        "pipelines",
        "prediction",
        "predict"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.806,
          "base_score": 0.656,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 45,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "retraining",
          "evaluation",
          "evaluation provides",
          "continuous",
          "continuous evaluation"
        ],
        "semantic": [],
        "merged": [
          "retraining",
          "evaluation",
          "evaluation provides",
          "continuous",
          "continuous evaluation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3409724056997122,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726633+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 297-305)",
      "start_page": 297,
      "end_page": 305,
      "summary": "The Two- Phase Predictions pattern can solve this by deploying the wake word model on-device and the more complex model in the cloud.\nWith Two-Phase Predictions, you could build one offline model to detect anomalous sounds.\nThe first phase of our solution will be a model that predicts whether or not the given sound is a musical instrument.\nThen, for sounds that the first model predicts are an instrument, we’ll get a prediction from a model deployed in the cloud to predict the specific instrument from a total of 18 possible options.\nTo build each of these models, we’ll convert the audio data to spectrograms, which are visual representations of sound.\nThe first model in our Two-Phase Predictions solution should be small enough that it can be loaded on a mobile device for quick inference without relying on internet connectivity.\nBuilding on the instrument example introduced above, we’ll provide an example of the first prediction phase by building a binary classification model optimized for on-device inference.\nOur first model will only have two labels: “instrument” or “not instrument.” We’ll build our model using the MobileNetV2 model architecture trained on the ImageNet dataset.\nTo build a model that fits these requirements, we’ll use TensorFlow Lite, a library optimized for building and serving models directly on mobile and embedded devices that may not have reliable internet connectivity.\nTo prepare the trained model for edge serving, we use TF Lite to export it in an optimized format:\nIt will also quantize inputs at inference time when we make predictions on our model.\nTo generate a prediction on a TF Lite model, you use the TF Lite interpreter, which is optimized for low latency.\nThere are APIs for both platforms, but we’ll show the Python code for generating predictions here so that you can run it from the same notebook where you created your model.\nWe’ll then pass the first image from our validation batch to the loaded TF Lite model for prediction, invoke the interpreter, and get the output:\nIf the model predicts “instrument,” it’s time to proceed by sending the audio clip to a more complex cloud-hosted model.\nPhase 2: Building the cloud model\nSince our cloud-hosted model doesn’t need to be optimized for inference without a network connection, we can follow a more traditional approach for training, exporting, and deploying this model.\nDepending on your Two- Phase Prediction use case, this second model could take many different forms.\nIn the Google Home example, phase 2 might include multiple models: one that converts a speaker’s audio input to text, and a second one that performs NLP to understand the text and route the user’s query.\nSince this model doesn’t need to be deployed on-device, we can use a larger model architecture like VGG as a starting point and then follow the Transfer Learning design pattern outlined in Chapter 4.\nWe’ll load VGG trained on the ImageNet dataset, specify the size of our spectrogram images in the input_shape parameter, and freeze the model’s weights before adding our own softmax classification output layer:\nTo demonstrate deploying the phase 2 model to the cloud, we’ll use Cloud AI Platform Prediction.\nWe’ll need to upload our saved model assets to a Cloud Storage bucket, then deploy the model by specifying the framework and pointing AI Platform Prediction to our storage bucket.\nYou can use any cloud-based custom model deployment tool for the second phase of the Two-Phase Predictions design pattern.\nIn addition to Google Cloud’s AI Platform Prediction, AWS SageMaker and Azure Machine Learning both offer services for deploying custom models.",
      "keywords": [
        "model",
        "prediction",
        "Lite model",
        "cloud",
        "Lite",
        "Platform Prediction",
        "cloud model",
        "Two-Phase Predictions",
        "instrument",
        "Predictions pattern",
        "’ll",
        "Two-Phase Predictions pattern",
        "Google Home",
        "input",
        "complex model"
      ],
      "concepts": [
        "models",
        "predictions",
        "predicts",
        "prediction",
        "device",
        "optimized",
        "optimization",
        "optimize",
        "interpreter",
        "instrument"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 36,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.541,
          "base_score": 0.391,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.505,
          "base_score": 0.355,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.495,
          "base_score": 0.345,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "",
          "score": 0.494,
          "base_score": 0.344,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "phase",
          "instrument",
          "lite",
          "cloud",
          "phase predictions"
        ],
        "semantic": [],
        "merged": [
          "phase",
          "instrument",
          "lite",
          "cloud",
          "phase predictions"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24086860921490127,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726683+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 306-314)",
      "start_page": 306,
      "end_page": 314,
      "summary": "To handle cases where the device calling our model may not always be connected to the internet, we could store audio clips for instrument prediction on the device while it is offline.\nWhen it regains connectivity, we could then send these clips to the cloud-hosted model for prediction.\nWhile the Two-Phase Predictions pattern works for many cases, there are situations where your end users may have very little internet connectivity and you therefore cannot rely on being able to call a cloud-hosted model.\nIn this section, we’ll discuss two offline-only alternatives, a scenario where a client needs to make many prediction requests at a time, and suggestions on how to run continuous evaluation for offline models.\nOne example of an application that provides a simpler offline model is Google Translate.\nThis could involve enabling a few common features offline or caching the results of an ML model’s prediction for later use offline.\nWith this alternative, we’re still employing two prediction phases, but we’re limiting the use cases covered by our offline model.\nWe could further build upon this by storing the user’s queries while their device is offline and sending them to a cloud model when they regain connectivity to provide more detailed results.\nAdditionally, we could even provide a basic recommendation model available offline, with the intention of complementing this with improved results when the app is able to send the user’s queries to a cloud-hosted model.\nIn other cases, end users of your ML model may have reliable connectivity but might need to make hundreds or even thousands of predictions to your model at once.\nThis is a variation of the Two- Phase Predictions pattern described earlier, the main difference being that both the offline and cloud models perform the same prediction task but with different inputs.\nIn this case, models also end up throttling the number of prediction requests sent to the cloud model at one time.\nThis solution is preferred if our offline and cloud models are running similar prediction tasks, like in the translation case mentioned previously.\nBut what if your model accepts a file with a million inputs and sends back a file with a million output predictions?\nYour model will then return (k, d), and so the client will be able to figure out which output instance corresponds to which input instance.\nIn order to get your Keras model to pass through keys, supply a serving signature when exporting the model.\nname='gestation_weeks'), 'key': tf.TensorSpec([None,], dtype=tf.string, name='key') }]) def keyed_prediction(inputs): feats = inputs.copy() key = feats.pop('key') # get the key from input output = model(feats) # invoke model return {'key': key, 'babyweight': output}\nmodel.save(EXPORT_PATH, signatures={'serving_default': keyed_prediction})\nAdding keyed prediction capability to an existing model\nname='gestation_weeks') }]) def nokey_prediction(inputs): output = model(inputs) # invoke model return {'babyweight': output}\nmodel.save(EXPORT_PATH, signatures={'serving_default': nokey_prediction, 'keyed_prediction': keyed_prediction })",
      "keywords": [
        "model",
        "offline",
        "prediction",
        "serving",
        "Serving Function",
        "offline models",
        "cloud model",
        "input",
        "output",
        "key",
        "online",
        "Function",
        "cloud-hosted model",
        "cloud",
        "tf.TensorSpec"
      ],
      "concepts": [
        "model",
        "serving",
        "keyed",
        "keys",
        "inputs",
        "data",
        "output",
        "online",
        "device",
        "translate"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 35,
          "title": "",
          "score": 0.704,
          "base_score": 0.554,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "",
          "score": 0.649,
          "base_score": 0.499,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.645,
          "base_score": 0.495,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "offline",
          "key",
          "cloud",
          "keyed_prediction",
          "cloud hosted"
        ],
        "semantic": [],
        "merged": [
          "offline",
          "key",
          "cloud",
          "keyed_prediction",
          "cloud hosted"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31077611593969406,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726737+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 315-322)",
      "start_page": 315,
      "end_page": 322,
      "summary": "What the server has to do is to assign keys to the inputs it receives before it invokes the model, use the keys to order the outputs, and then remove the keys before sending along the outputs.\nAn alternate solution is to have the model ignore unrecognized inputs and send back not just the prediction outputs but also all inputs, including the unrecognized ones.\nWe started this chapter by looking at how to encapsulate your trained machine learning model as a stateless function using the Stateless Serving Function design pattern.\nWe saw how the Batch Serving design pattern solves this by utilizing distributed data processing infrastructure designed to run many model prediction requests asynchronously as a background job, with output written to a specified location.\nNext, with the Continued Model Evaluation design pattern, we looked at an approach to verifying that your deployed model is still performing well on new data.\nIn the Two-Phase Predictions design pattern, we solved for specific use cases where models need to be deployed at the edge.\nThe Transform design pattern captures data preparation dependencies from the model training pipeline to reproduce them during serving.\nThe Workflow Pipeline design pattern captures all the steps in the machine learning process to ensure that as the model is retrained, parts of the pipeline can be reused.\nVersioning of data and models is a prerequisite to handle many of the design patterns in this chapter.\nThe Transform design pattern makes moving an ML model to production much easier by keeping inputs, features, and transforms carefully separate.\nThe problem is that the inputs to a machine learning model are not the features that the machine learning model uses in its computations.\nWhen we train a machine learning model, we train it with features that are extracted from the raw inputs.\nTake this model that is trained to predict the duration of bicycle rides in London using BigQuery ML:\nCREATE OR REPLACE MODEL ch09eu.bicycle_model OPTIONS(input_label_cols=['duration'], model_type='linear_reg') AS SELECT duration , start_station_name , CAST(EXTRACT(dayofweek from start_date) AS STRING) as dayofweek , CAST(EXTRACT(hour from start_date) AS STRING) as hourofday FROM `bigquery-public-data.london_bicycles.cycle_hire`\nThis model has three features (start_station_name, dayofweek, and hourofday) computed from two inputs, start_station_name and start_date, as shown in Figure 6-1.\nThe model has three features computed from two inputs.\nBecause the model was trained on three features, this is what the prediction signature has to look like:\nNote that, at inference time, we have to know what features the model was trained on, how they should be interpreted, and the details of the transformations that were applied.\nThe solution is to explicitly capture the transformations applied to convert the model inputs into features.\nCREATE OR REPLACE MODEL ch09eu.bicycle_model OPTIONS(input_label_cols=['duration'], model_type='linear_reg') TRANSFORM( SELECT * EXCEPT(start_date) , CAST(EXTRACT(dayofweek from start_date) AS STRING) as dayofweek -- feature1 , CAST(EXTRACT(hour from start_date) AS STRING) as hourofday –- feature2 ) AS SELECT duration, start_station_name, start_date -- inputs FROM `bigquery-public-data.london_bicycles.cycle_hire`\nAs long as we carefully use only the raw inputs in the SELECT statement and put all subsequent processing of the input in the TRANSFORM clause, BigQuery ML will automatically apply these transformations during prediction.\nIf we are using a framework where support for the Transform design pattern is not built in, we should design our model architecture in such a way that the transformations carried out during training are easy to reproduce during serving.\nWe can do this by making sure to save the transformations in the model graph or by creating a repository of transformed features (“Design Pattern 26: Feature Store”).",
      "keywords": [
        "model",
        "design pattern",
        "Transform design pattern",
        "machine learning model",
        "inputs",
        "machine learning",
        "design",
        "pattern",
        "features",
        "prediction",
        "learning model",
        "Transform",
        "start",
        "learning",
        "training"
      ],
      "concepts": [
        "design",
        "prediction",
        "predictions",
        "predict",
        "transform",
        "transformations",
        "inputs",
        "data",
        "feature",
        "pattern"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.804,
          "base_score": 0.654,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 45,
          "title": "",
          "score": 0.773,
          "base_score": 0.623,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 52,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.757,
          "base_score": 0.607,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "start_date",
          "inputs",
          "dayofweek",
          "design",
          "design pattern"
        ],
        "semantic": [],
        "merged": [
          "start_date",
          "inputs",
          "dayofweek",
          "design",
          "design pattern"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40967906764176293,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726800+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 323-331)",
      "start_page": 323,
      "end_page": 331,
      "summary": "Second, maintain a dictionary of transformed features, and make every transformation either a Keras Preprocessing layer or a Lambda layer.\ntransformed = {} for lon_col in ['pickup_longitude', 'dropoff_longitude']: transformed[lon_col] = tf.keras.layers.Lambda( lambda x: (x+78)/8.0, name='scale_{}'.format(lon_col) )(inputs[lon_col]) for lat_col in ['pickup_latitude', 'dropoff_latitude']: transformed[lat_col] = tf.keras.layers.Lambda( lambda x: (x-37)/8.0, name='scale_{}'.format(lat_col) )(inputs[lat_col])\ndef euclidean(params): lon1, lat1, lon2, lat2 = params londiff = lon2 - lon1 latdiff = lat2 - lat1 return tf.sqrt(londiff*londiff + latdiff*latdiff) transformed['euclidean'] = tf.keras.layers.Lambda(euclidean, name='euclidean')([ inputs['pickup_longitude'], inputs['pickup_latitude'], inputs['dropoff_longitude'], inputs['dropoff_latitude'] ])\ntransformed['hourofday'] = tf.keras.layers.Lambda( lambda x: tf.strings.to_number(tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32), name='hourofday' )(inputs['pickup_datetime'])\ndnn_inputs = tf.keras.layers.DenseFeatures(feature_columns.values())(transformed)\nBecause the constructor for DenseFeatures requires a set of feature columns, we will have to specify how to take each of the transformed values and convert them into an input to the neural network.\nBecause the Transform layer is part of the model graph, the usual Serving Function and Batch Serving solutions (see Chapter 5) will work as is.\nIt is helpful to differentiate between instance-level transformations that can be part of the model directly (where the only drawback is applying them on each training iteration) and dataset-level transformations, where we need a full pass to compute overall statistics or the vocabulary of a categorical variable.\nSuch dataset-level transformations cannot be part of the model and have to be applied as a scalable preprocessing step, which produces the Transform, capturing the logic and the artifacts (mean, variance, vocabulary, and so on) to be attached to the model.\nThe tf.transform library (which is part of TensorFlow Extended) provides an efficient way of carrying out transformations over a preprocessing pass through the data and saving the resulting features and transformation artifacts so that the transformations can be applied by TensorFlow Serving during prediction time.\nBefore training, the raw data is read and transformed using the prior function in Apache Beam:\nThe training function reads transformed data and, therefore, the transformations do not have to be repeated within the training loop.\nThe serving function needs to load in these artifacts and create a Transform layer:\nThen, the serving function can apply the Transform layer to the parsed input features and invoke the model with the transformed data to calculate the model output:\ntransformed_features = tf_transform_layer(parsed_features) return model(transformed_features)\nAt the same time, because the model training happens on the transformed data, our training loop does not have to carry out these transformations during each epoch.\nWith image models, there are some transformations (such as data augmentation by random cropping and zooming) that are applied only during training.\nThe computation engine supports low-latency access for inference and batch creation of transformed features while the data repository provides quick access to transformed features for model training.\nAnother way to separate out the programming language and framework used for transformation of the features from the language used to write the model is to carry out the preprocessing in containers and use these custom containers as part of both the training and serving.\nTo ensure that sampling is repeatable and reproducible, it is necessary to use a well-distributed column and a deterministic hash function to split the available data into training, validation, and test datasets.\nMany machine learning tutorials will suggest splitting data randomly into training, validation, and test datasets using code similar to the following:\nFor the airline delay problem, we can use the Farm Fingerprint hashing algorithm on the date column to split the available data into training, validation, and testing datasets.\nTo split on the date column, we compute its hash using the FARM_FINGERPRINT function and then use the modulo function to find an arbitrary 80% subset of the rows.\nIf we want to split our data by arrival_airport (so that 80% of airports are in the training dataset, perhaps because we are trying to predict something about airport amenities), we would compute the hash on arrival_airport instead of date.\nFeatures extracted from date such as day of week or hour of day can be inputs, but we can’t use an actual input as the field with which to split because the trained model will not have seen 20% of the possible input values for the date column if we use 80% of the data for training.",
      "keywords": [
        "model",
        "data",
        "transformed",
        "inputs",
        "layer",
        "feature",
        "Transform layer",
        "training",
        "transformations",
        "Keras model",
        "Lambda layer",
        "Input layer",
        "Feature Store",
        "transformed data",
        "split"
      ],
      "concepts": [
        "input",
        "transformed",
        "transformations",
        "model",
        "function",
        "functions",
        "data",
        "features",
        "training",
        "serving"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.456,
          "base_score": 0.456,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 6,
          "title": "",
          "score": 0.41,
          "base_score": 0.26,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 13,
          "title": "",
          "score": 0.318,
          "base_score": 0.318,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 23,
          "title": "",
          "score": 0.306,
          "base_score": 0.306,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transformed",
          "lambda",
          "transformations",
          "tf",
          "layer"
        ],
        "semantic": [],
        "merged": [
          "transformed",
          "lambda",
          "transformations",
          "tf",
          "layer"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.1736148877391673,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726837+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 332-340)",
      "start_page": 332,
      "end_page": 340,
      "summary": "CREATE OR REPLACE TABLE mydataset.mytable AS SELECT airline, departure_airport, departure_schedule, arrival_airport, arrival_delay, CASE(ABS(MOD(FARM_FINGERPRINT(date), 10))) WHEN 9 THEN 'test' WHEN 8 THEN 'validation' ELSE 'training' END AS split_col FROM `bigquery-samples`.airline_ontime_data.flights\nWe can then use the split_col column to decide which of three datasets any particular row falls in.\nUsing a single query decreases computational time but requires creating a new table or modifying the source table to add the extra split_col column.\nIf we split on a feature cross of multiple columns, we can use arrival_airport as one of the inputs to the model, since there will be examples of any particular airport in both the training and test sets.\nIn the case of time-series models, a common approach is to use sequential splits of data.\nFor example, to train a demand forecasting model where we train a model on the past 45 days of data to predict demand over the next 14 days, we’d train the model (full code) by pulling the necessary data:\nSuch a sequential split of data is also necessary in fast-moving environments even if the goal is not to predict the future value of a time series.\nIt is not sufficient to generate the evaluation data from a random split of the historical dataset because the goal is to predict behavior that the bad actors will exhibit in the future.\nThe indirect goal is the same as that of a time-series model in that a good model will be able to train on historical data and predict future fraud.\nThe data has to be split sequentially in terms of time to correctly evaluate this.\nAnother instance where a sequential split of data is needed is when there are high correlations between successive times.\nOne way to properly evaluate the performance of a forecasting model is to use a sequential split but take seasonality into account by using the first 20 days of every month in the training dataset, the next 5 days in the validation dataset, and the last 5 days in the testing dataset.\nThe example above of how weather patterns are different between different seasons is an example of a situation where the splitting needs to happen after the dataset is stratified.\nWe needed to ensure that there were examples of all seasons in each split, and so we stratified the dataset in terms of months before carrying out the split.\nIf it is critical for our business use case to get the behavior of these flights correct, we should stratify the dataset based on departure hour and split each stratification evenly.\nexamples is quite small), we might want to stratify the dataset by the label and split each stratification evenly.\nFor example, if videos taken on the same day are correlated, use a video’s capture date from its metadata to split the videos among independent datasets.\nSimilarly, if text reviews from the same person tend to be correlated, use the Farm Fingerprint of the user_id of the reviewer to repeatedly split reviews among the datasets.\nSimilarly, a natural way to split image or audio datasets might be to use the hash of the filename for splitting, but it does not address the problem of correlations between images or videos.\nIn our experience, many problems with poor performance of ML can be addressed by designing the data split (and data collection) with potential correlations in mind.\nWhen computing embeddings or pre-training autoencoders, we should make sure to first split the data and perform these pre-computations on the training dataset only.\nThe Bridged Schema design pattern provides ways to adapt the data used to train a model from its older, original data schema to newer, better data.\noften takes time for enough data of the improved schema to be collected for us to adequately train a replacement model.\nThe Bridged Schema pattern allows us to use as much of the newer data as is available, but augment it with some of the older data to improve model accuracy.\nWe cannot train a new model exclusively on the newer data because the quantity of new data will be quite small, limited as it is to transactions after the payment system upgrade.\nThen, we train an ML model using as much of the new data as is available and augment it with the older data.\nImagine that we estimate from the newer training data that of the card transactions, 10% are gift cards, 30% are debit cards, and 60% are credit cards.\nEach time an older training example is loaded into the trainer program, we could choose the card type by generating a uniformly distributed random number in the range [0, 100) and choosing a gift card when the random number is less than 10, a debit card if it is in [10, 40), and a credit card otherwise.\nTo bridge the older data into the newer schema, we can transform the older categorical data into this representation where we insert the a priori probability of the new classes as estimated from the training data.\nIn order to maximize use of the newer data, make sure to use only two splits of the data, which is discussed in “Design Pattern 12: Checkpoints” in Chapter 4.\nTherefore, we need to set aside a sufficient number of examples from the new data to adequately evaluate generalization performance.\nThe evaluation dataset will not contain any older examples that have been bridged to match the newer schema.",
      "keywords": [
        "data",
        "split",
        "dataset",
        "model",
        "card",
        "date",
        "older data",
        "FINGERPRINT",
        "training",
        "FARM",
        "airport",
        "time",
        "MOD",
        "ABS",
        "Schema"
      ],
      "concepts": [
        "data",
        "dataset",
        "model",
        "training",
        "splits",
        "uses",
        "useful",
        "examples",
        "times",
        "date"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 10,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 7,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 41,
          "title": "",
          "score": 0.493,
          "base_score": 0.493,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 19,
          "title": "",
          "score": 0.413,
          "base_score": 0.413,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "split",
          "older",
          "newer",
          "card",
          "days"
        ],
        "semantic": [],
        "merged": [
          "split",
          "older",
          "newer",
          "card",
          "days"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2967489021872106,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.726886+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 341-348)",
      "start_page": 341,
      "end_page": 348,
      "summary": "Because this is the evaluation set of the current production model (which we were able to train with one million examples), the evaluation dataset here might hold hundreds of thousands of examples.\nDetermine the number of evaluation examples needed by evaluating the production model on subsets of varying sizes and tracking the variability of the evaluation metric by the size of the subset.\nFrom Figure 6-3, we see that the number of evaluation examples needs to be at least 2,000, and is ideally 3,000 or more.\nThe training set would contain the remaining 2,500 new examples (the amount of new data available after withholding 2,500 for evaluation) augmented by some number of older examples that have been bridged to match the new schema.\nDetermine the number of older examples to bridge by carrying out hyperparameter tuning.\nFor best results, we should choose the smallest number of older examples that we can get away with—ideally, over time, as the number of new examples grows, we’ll rely less and less on bridged examples.\nIt is worth noting that, on this problem, bridging does bring benefits because when we use no bridged examples, the evaluation metric is worse.\nIf this is not the case, then the imputation method (the method of choosing the static value used for\nIt is extremely important to compare the performance of the newer model trained on bridged examples against the older, unchanged model on the evaluation dataset.\nBecause we will be using the evaluation dataset to test whether or not the bridged model has value, it is critical that the evaluation dataset not be used during training or hyperparameter tuning.\nImputation in statistics is a set of techniques that can be used to replace missing data by some valid value.\nA common imputation technique is to replace a NULL value by the mean value of that column in the training data.\nWe assume that the categorical variable is distributed according to a frequency chart (that we estimate from the training data) and impute the mean one-hot encoded value (according to that frequency distribution) to the “missing” categorical variable.\nThe first model uses whatever new examples we have to train a machine learning model to predict the card type.\nIf we have new input features we want to start using immediately, we should bridge the older data (where this new feature will be missing) by imputing a value for the new feature.\nIf the feature is whether or not it was raining, it is boolean, and so the imputed value would be something like 0.02 if it rains 2% of the time in the training dataset.\nWhen the input provider increases the precision of their data stream, follow the bridging approach to create a training dataset that consists of the higher-resolution data, augmented with some of the older data.\nFor floating-point values, it is not necessary to explicitly bridge the older data to match the newer data’s precision.\nIf we assume that 3.5 in the older data consists of values that would be uniformly distributed in [3.45, 3.55] in the newer data, the statically imputed value would be 3.5, which is precisely the value that is stored in the older data.\nFor categorical values—for example, if the older data stored the location as a state or provincial code and the newer data provided the county or district code—use the frequency distribution of counties within states as described in the main solution to carry out static imputation.\nThis pattern is also useful when a machine learning model requires features that need to be computed from aggregates over time windows.\nBy externalizing the state to a stream pipeline, the Windowed Inference design pattern ensures that features calculated in a dynamic, time-dependent way can be correctly repeated between training and serving.\nThe anomaly detection function, is_anomaly, can be quite sophisticated, but let’s take the simple case of discarding extrema and calling a data value an anomaly if it is more than four standard deviations from the mean in the two-hour window:",
      "keywords": [
        "data",
        "evaluation",
        "model",
        "evaluation metric",
        "older data",
        "feature",
        "evaluation dataset",
        "older",
        "subset",
        "evaluation sizes",
        "subset size",
        "size",
        "time",
        "pattern",
        "training"
      ],
      "concepts": [
        "value",
        "data",
        "model",
        "delay",
        "train",
        "pattern",
        "examples",
        "card",
        "imputation",
        "impute"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 45,
          "title": "",
          "score": 0.557,
          "base_score": 0.557,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 27,
          "title": "",
          "score": 0.472,
          "base_score": 0.472,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.465,
          "base_score": 0.465,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "",
          "score": 0.454,
          "base_score": 0.454,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "older",
          "examples",
          "evaluation",
          "older data",
          "value"
        ],
        "semantic": [],
        "merged": [
          "older",
          "examples",
          "evaluation",
          "older data",
          "value"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3363739245100613,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.726939+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 349-356)",
      "start_page": 349,
      "end_page": 356,
      "summary": "The solution is to carry out stateful stream processing—that is, stream processing that keeps track of the model state through time:\nThe internal model state (this could be the list of flights) is updated with flight information every time a new flight arrives, thus building a 2-hour historical record of flight data.\nEvery time the window is closed (every 10 minutes in our example), a time-series ML model is trained on the 2-hour list of flights.\nThe time-series model parameters are externalized into a state variable.\nTo keep the code understandable, we will use a zero-order regression model, and so our model parameters will be the average flight delay and the variance of the flight delays over the two- hour window.\nThe model is updated by combining all the flight data collected over the past two hours and passing it to a function that we call ModelFn:\nmodel_state = (windowed | 'model' >> beam.transforms.CombineGlobally(ModelFn()))\nHere, the internal model state will consist of a pandas dataframe that is updated with the flights in the window:\nThe externalized model state gets updated every 10 minutes based on a 2-hour rolling window:\nThis allows the code to work in streaming, but the model state is available only within the context of the sliding\nIn order to carry out inference on every arriving flight, we need to externalize the model state (similar to how we export the model weights out to a file in the Stateless Serving Function pattern to decouple it from the context of the training program where these weights are computed):\ndef is_anomaly(flight, model_external_state): result = flight.copy() error = flight['delay'] - model_external_state['prediction'] tolerance = model_external_state['acceptable_deviation'] result['is_anomaly'] = np.abs(error) > tolerance return result\nThe solution suggested above is computationally efficient in the case of high- throughput data streams but can be improved further if the ML model parameters can be updated online.\nbeam.window.SlidingWindows(2 * 60 * 60, 10*60)) model_state = (windowed | 'model' >> beam.transforms.CombineGlobally(ModelFn()))\nThere are meaningful differences between the rolling window in pandas and the sliding window in Apache Beam because of how often the is_anomaly function is called and how often the model parameters (mean and standard deviation) need to be computed.\nThe anomaly detection code computes the model parameters and applies it immediately to the last item in the window.\nIn the Beam pipeline, the model state is also created on every sliding window, but the sliding window in this case is based on time.\nStoring all the received records in order to compute the model parameters at the end of the window can become problematic.\nThe key difference is that the only thing held in memory are three floating point numbers (sum, sum , count) required to extract the output model state, not the entire dataframe of received instances.\nThen, we create the model_state by computing the model parameters over a time window specified as two hours preceding to one second preceding:\nmodel_state AS ( SELECT scheduled_arrival_time, arrival_delay, AVG(arrival_delay) OVER (time_window) AS prediction, 4*STDDEV(arrival_delay) OVER (time_window) AS acceptable_deviation FROM data WINDOW time_window AS (ORDER BY UNIX_SECONDS(TIMESTAMP(scheduled_arrival_time)) RANGE BETWEEN 7200 PRECEDING AND 1 PRECEDING) )\nSELECT *, (ABS(arrival_delay - prediction) > acceptable_deviation) AS is_anomaly FROM model_state\nThe Windowed Inference pattern of passing a sliding window of previous instances to an inference function is useful beyond anomaly detection or even time-series models.\nThe Windowed Inference pattern can be useful if an input feature to the model requires state, even if the model itself is stateless.",
      "keywords": [
        "model",
        "model state",
        "model parameters",
        "window",
        "state",
        "sliding window",
        "time",
        "externalized model state",
        "data",
        "anomaly",
        "internal model state",
        "delay",
        "flight",
        "Apache Beam",
        "arrival"
      ],
      "concepts": [
        "window",
        "models",
        "beam",
        "anomaly",
        "anomalies",
        "flights",
        "returns",
        "time",
        "data",
        "computationally"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 39,
          "title": "",
          "score": 0.493,
          "base_score": 0.493,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 36,
          "title": "",
          "score": 0.414,
          "base_score": 0.414,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.404,
          "base_score": 0.404,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.383,
          "base_score": 0.383,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.38,
          "base_score": 0.38,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "window",
          "state",
          "flight",
          "model state",
          "model parameters"
        ],
        "semantic": [],
        "merged": [
          "window",
          "state",
          "flight",
          "model state",
          "model parameters"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25026186097449193,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.726980+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 357-370)",
      "start_page": 357,
      "end_page": 370,
      "summary": "In the Workflow Pipeline design pattern, we address the problem of creating an end-to-end reproducible pipeline by containerizing and orchestrating the steps in our machine learning process.\nAn individual data scientist may be able to run data preprocessing, training, and model deployment steps from end to end (depicted in Figure 6-6) within a single script or notebook.\nTo scale the ML workflow, we need a way for the team building out the model to run trials independently of the data preprocessing step.\nAdditionally, when initial development for each step is complete, we’ll want to schedule operations like retraining, or create event-triggered pipeline runs that are invoked in response to changes in your environment, like new training data being added to a bucket.\nBecause pipeline steps run in containers, we can run them on a development laptop, with on-premises infrastructure, or with a hosted cloud service.\nThere are many tools for creating pipelines with both on-premise and cloud options available, including Cloud AI Platform Pipelines, TensorFlow Extended (TFX), Kubeflow Pipelines (KFP), MLflow, and Apache Airflow.\nTo demonstrate the Workflow Pipeline design pattern here, we’ll define our pipeline with TFX and run it on Cloud AI Platform Pipelines, a hosted service for running ML pipelines on Google Cloud using Google Kubernetes Engine (GKE) as the underlying container infrastructure.\nSteps in TFX pipelines are known as components, and both pre-built and customizable components are available.\nTypically, the first component in a TFX pipeline is one that ingests data from an external source.\nThere are many other pre-built components provided by TFX—we’ve only included a few here that we’ll use in our sample pipeline.\nWe’ll keep the features, components, and model code relatively short in order to focus on the pipeline tooling.\nLet’s start by discussing the scaffolding for a typical TFX pipeline and the process for running it on AI Platform.\nBuilding the TFX pipeline We’ll use the tfx command-line tools to create and invoke our pipeline.\nAn instance of tfx.orchestration.pipeline where we define our pipeline and the components it includes.\nWe’ll use this to create and run our pipeline.\nOur pipeline (see full code in GitHub) will have the five steps or components defined above, and we can define our pipeline with the following:\npipeline.Pipeline( pipeline_name='huricane_prediction', pipeline_root='path/to/pipeline/code', components=[ bigquery_gen, statistics_gen, schema_gen, train, model_pusher ] )\ny q Another benefit of using pipelines is that it provides tooling to keep track of the input, output artifacts, and logs for each component.\nThe output artifact from the statistics_gen component in a TFX pipeline.\nRunning the pipeline on Cloud AI Platform\nWe can run the TFX pipeline on Cloud AI Platform Pipelines, which will manage low-level details of the infrastructure for us.\nAfter running this command, we’ll be able to see a graph that updates in real time as our pipeline moves through each step.\nWe could train our model directly in our containerized pipeline on GKE, but TFX provides a utility for using Cloud AI Platform Training as part of our process.\nOutput of the schema_gen component for an ML pipeline.\nWe can run the same code we’re demonstrating here with Google’s AI Platform Pipelines on Azure ML Pipelines, Amazon SageMaker, or on-premises.\nTo implement a training step in TFX, we’ll use the Trainer component and pass it information on the training data to use as model input, along with our model training code.\nTFX provides an extension for running the training step on AI Platform that we can use by importing tfx.extensions.google_cloud_ai_platform.trainer and providing details on our AI Platform training configuration.\nWith that, we have a complete pipeline that ingests data, analyzes it, runs data transformation, and finally trains and deploys the model using AI Platform.\nWithout running our ML code as a pipeline, it would be difficult for others to reliably reproduce our work.\nThe Workflow Pipeline design pattern lets others run and monitor our entire ML workflow from end to end in both on-premises and cloud environments, while still being able to debug the output of individual steps.\nThese components run as individual containers wherever we choose to run our pipeline.\nThe main alternative to using a pipeline framework is to run the steps of our ML workflow using a makeshift approach for keeping track of the notebooks and output associated with each step.\nIn this section, we’ll look at some variations and extensions of the Workflow Pipeline design pattern: creating containers manually, automating a pipeline with tools for continuous integration and continuous delivery (CI/CD), processes for moving from a development to production workflow pipeline, and alternative tools for building and orchestrating pipelines.\nInstead of using pre-built or customizable TFX components to construct our pipeline, we can define our own containers to use as components, or convert a Python function to a component.\nIn addition to invoking pipelines via the dashboard or programmatically via the CLI or API, chances are we’ll want to automate runs of our pipeline as we productionize the model.\nThere are many managed services available for setting up triggers to run a pipeline when we want to retrain a model on new data.\nAlternatively, we could use a serverless event-based service like Cloud Functions to invoke our pipeline when new data is added to a storage location.\nOnce enough new training data is available, we can instantiate a pipeline run for retraining and redeploying the model as demonstrated in Figure 6-9.\nA CI/CD workflow using Cloud Functions to invoke a pipeline when enough new data is added to a storage location.\nWhen the code is committed, Cloud Build will then build the containers associated with our pipeline based on the new code and create a run.\nLike TFX, both Airflow and KFP treat pipelines as a DAG where the workflow for each step is defined in a Python script.\nKFP, on the other hand, was designed specifically for ML and operates at a lower level than TFX, providing more flexibility in how pipeline steps are defined.\nTFX operates at the highest level on top of Kubeflow Pipelines, with pre-built components offering specific approaches to common workflow steps.\nWe’ll likely want to build and prototype our pipeline from a notebook, where we can re-invoke our pipeline by running a notebook cell, debug errors, and update code all from the same environment.\nWhen newly available data triggers a pipeline run and trains an updated model, we can add logic to check the output of our evaluation component to execute the deployment component if the accuracy is above our threshold, or end the pipeline run if not.\nIn AI Platform Pipelines, for example, we can use the pipelines dashboard to see which data a model version was trained on, broken down both by data schema and date.\nFigure 6-11 shows the Lineage Explorer dashboard for a TFX pipeline running on AI Platform.\nOne benefit of using lineage tracking to manage artifacts generated during our pipeline run is that it supports both cloud-based and on-premises environments.",
      "keywords": [
        "pipeline",
        "TFX pipeline",
        "Platform Pipelines",
        "TFX",
        "Kubeflow Pipelines",
        "Workflow Pipeline",
        "Platform",
        "data",
        "model",
        "Workflow Pipeline design",
        "component",
        "pipeline run",
        "cloud",
        "Platform Pipelines dashboard",
        "LFT"
      ],
      "concepts": [
        "pipeline",
        "components",
        "train",
        "model",
        "steps",
        "cloud",
        "containers",
        "code",
        "run",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 34,
          "title": "",
          "score": 0.806,
          "base_score": 0.656,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pipeline",
          "tfx",
          "pipelines",
          "run",
          "component"
        ],
        "semantic": [],
        "merged": [
          "pipeline",
          "tfx",
          "pipelines",
          "run",
          "component"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35811225009918163,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727034+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 371-380)",
      "start_page": 371,
      "end_page": 380,
      "summary": "Training is typically done using historical data with batch features that are created offline.\nA typical feature store is built with two key design characteristics: tooling to process large feature data sets quickly, and a way to store features that supports both low-latency access (for inference) and large batch access (for model training).\nA feature store provides a bridge between raw data sources and model training and serving.\nAfter features are created, they are housed in a data store to be retrieved for training and serving.\nFor example, a feature store may use Cassandra or Redis as a data store for online feature retrieval, and Hive or BigQuery for fetching historical, large batch feature sets.\nIn the end, a typical feature store will house many different feature sets containing features created from myriad raw data sources.\nAdding feature data to Feast Data is stored in Feast using FeatureSets.\nFeatureSets are how Feast knows where to source the data it needs for a feature, how to ingest it, and some basic characteristics about the data types.\nOnce our feature set is registered, Feast will start an Apache Beam job to populate the feature store with data from the source.\nA feature set is used to generate both offline and online feature stores, which ensures developers train and serve their model with the same data.\nFeast ensures that the source data complies with the expected schema of the feature set.\nThere are four steps to ingest feature data into Feast, as shown in Figure 6-14.\nThere are four steps to ingesting feature data into Feast: create a FeatureSet, add entities and features, register the FeatureSet, and ingest feature data into the FeatureSet.\n1. Create a FeatureSet. The feature set specifies the entities, features, and source.\n3. Register the FeatureSet. This creates a named feature set within Feast.\nThe feature set contains no feature data.\n4. Load feature data into the FeatureSet.\n# Create a feature set taxi_fs = FeatureSet(\"taxi_rides\")\nEntities are used as keys to look up feature values and are used to join features between different feature sets when creating datasets for training or serving.\nAt this stage, the feature set we created called taxi_rides contains no entities or features.\nDEFINING STREAMING DATA SOURCES WHEN CREATING A FEATURE SET\nUsers can define streaming data sources when creating a feature set.\nOnce a feature set is registered with a source, Feast will automatically start to populate its stores with data from this source.\nThis is an example of a feature set with a user-provided source that retrieves streaming data from a Kafka topic:\nprint(client.get_feature_set(\"taxi_rides\"))\nThis returns a JSON object containing the data schema for the taxi_rides feature set:\nIngesting feature data into the FeatureSet\n# Load feature data into Feast for this specific feature set client.ingest(taxi_fs, taxi_df)\nProgress during this ingestion step is printed to the screen showing that we’ve ingested 28,247 rows into the taxi_rides feature set within Feast:\nIn Feast, once a feature set is created, there are only a few changes that can be made.\nChanges to the feature set name.",
      "keywords": [
        "feature",
        "feature set",
        "feature data",
        "data",
        "feature store",
        "Feast",
        "taxi",
        "feature set taxi",
        "Load feature data",
        "store",
        "Feast feature store",
        "model",
        "feature data sets",
        "rides feature set",
        "sets"
      ],
      "concepts": [
        "features",
        "data",
        "feast",
        "model",
        "client",
        "source",
        "sourced",
        "sets",
        "setting",
        "entities"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.867,
          "base_score": 0.717,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 45,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.661,
          "base_score": 0.511,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature set",
          "feature",
          "feast",
          "feature data",
          "set"
        ],
        "semantic": [],
        "merged": [
          "feature set",
          "feature",
          "feast",
          "feature data",
          "set"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3394827167951271,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727093+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 381-396)",
      "start_page": 381,
      "end_page": 396,
      "summary": "Feature data can be retrieved either offline, using historical features for model training, or online, for serving.\nFor training a model, historical feature retrieval is backed by BigQuery and accessed using .get_batch_features(...) with the batch serving client.\nTo retrieve online features, for example, when making online predictions with the trained model, we use .get_online_features(...) specifying the features we want to capture and the entity:\nTo make an online prediction for this example, we pass the field values from the object returned in online_features as a pandas dataframe called predict_df to model.predict:\nFor example, a model served as a customer-facing application may receive only 10 input values from a client, but those 10 inputs may need to be transformed into many more features via feature engineering before being sent to a model.\nIt is crucial that the pipeline for retrieving features during development is the same as when serving the model.\nA feature store ensures the feature engineering pipelines are consistent between model training and serving.\nFor example, a user-facing online application may operate at very low latency using up-to-the- second features, whereas when training the model, features are pulled offline as a larger batch but with higher latency.\nLastly, a feature store acts as a version-controlled repository for feature datasets, allowing the same CI/CD practices of code and model development to be applied to the feature engineering process.\nAnd, while a feature store itself doesn’t perform the feature transformations, it provides a way to separate the upstream feature engineering steps from model serving and provide point in time correctness.\nDesign Pattern 27: Model Versioning\nIn the Model Versioning design pattern, backward compatibility is achieved by deploying a changed model as a microservice with a different REST endpoint.\nTo handle this, we’ll need a solution that lets users choose an older version of our model if they prefer.\nTo gracefully handle updates to a model, deploy multiple model versions with different REST endpoints.\nThis ensures backward compatibility—by keeping multiple versions of a model deployed at a given time, those users relying on older versions will still be able to use the service.\nTo add support for a new version, our team’s application developers only need to change the name of the API endpoint pointing to the model.\nOf course, if a new model version introduces changes to the model’s response format, we’ll need to make changes to our app to accommodate this, but the model and application\nData scientists or ML engineers can therefore deploy and test a new model version on our own without worrying about breaking our production app.\nIf the format of our model’s response changes, application developers may want to use an older model version until they’ve updated their application code to support the latest response format.\nAlso, if we can break users into distinct groups (i.e., based on their app usage), we can serve each group different model versions based on their preferences.\nTo demonstrate versioning, we’ll build a model that predicts flight delays and deploy this model to Cloud AI Platform Prediction.\nTo deploy this model to AI Platform, we need to create a model version that will point to this model.bst in a Cloud Storage Bucket.\nWith this model deployed, it’s now accessible via the endpoint /models/flight_delay_predictions/versions/v1 in an HTTPS URL tied to our project.\ngcloud ai-platform predict --model 'flight_delay_prediction' --version 'v1' --json-request 'input.json'\nThis is where model versioning can help.\nWe’ll deploy our TensorFlow model as a second version under the same flight_delay_prediction model resource.\nTo deploy our second version, we’ll export the model and copy it to a new subdirectory in the bucket we used previously.\nWe can use the same deploy command as above, replacing the version name with v2 and pointing to the Cloud Storage location of the new model.\nAn ML engineer deploying a new version of a model as an ML model endpoint may want to use an API gateway such as Apigee that determines which model version to call.\nThe API gateway determines which deployed model version to call given a user’s ID or IP address.\nWith multiple model versions deployed, AI Platform allows for performance monitoring and analytics across versions.\nIn addition to handling changes to our model itself, another reason to use versioning is when new training data becomes available.\nAssuming this new data follows the same schema used to train the original model, it’s important to keep track of when the data was captured for each newly trained version.\nFor example, if the latest version of a model is trained on data from 2019, we could name the version v20190101_20191231.\nWe can use this approach in combination with “Design Pattern 18: Continued Model Evaluation” (discussed in Chapter 5) to determine when to take older model versions offline, or how far back training data should go.\nWe’ll also discuss when to create an entirely new model resource instead of a version.\nUnder the hood, each model version is a stateless function with a specified input and output format, deployed behind a REST endpoint.\nInstead of using Cloud AI Platform or another cloud-based serverless offering for model versioning, we could use an open source tool like TensorFlow Serving.\nWith Docker, we could then serve the model using whichever hardware we’d like, including GPUs. The TensorFlow Serving API has built-in support for model versioning, following a similar approach to the one discussed in the Solution section.\nAnother alternative to deploying multiple versions is to define multiple serving functions for a single version of an exported model.\nTo handle requirements for different groups of model end users, we can define multiple serving functions when we export our model.\nThese serving functions are part of one exported model version, and this model is deployed to a single REST endpoint.\nIn the application code where we invoke our deployed model, we would determine which serving function to use based on the data sent from the client.\nBased on the request data from the client, the model framework can determine which serving function to use.\nIt’s also worth noting that we can have multiple serving functions with multiple model versions, though there is a risk that this could create too much complexity.\nIf we’re unsure about whether to use a new version or model, we can think about whether we want existing clients to upgrade.\nIf the answer is yes, chances are we have improved the model without changing the prediction task, and creating a new version will suffice.\nWith our latest regression model, app developers might choose to display the predicted delay when users search for flights, replacing something like “This flight is usually delayed more than 30 minutes” from the first version.",
      "keywords": [
        "model",
        "feature",
        "feature store",
        "serving",
        "model version",
        "version",
        "Model Versioning",
        "Feast",
        "data",
        "feature engineering",
        "online",
        "store",
        "versions",
        "users",
        "deployed model"
      ],
      "concepts": [
        "model",
        "feature",
        "version",
        "versions",
        "user",
        "served",
        "serve",
        "uses",
        "useful",
        "predictions"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 43,
          "title": "",
          "score": 0.867,
          "base_score": 0.717,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 32,
          "title": "",
          "score": 0.708,
          "base_score": 0.558,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 36,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "",
          "score": 0.622,
          "base_score": 0.472,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "version",
          "versions",
          "model version",
          "serving",
          "feature"
        ],
        "semantic": [],
        "merged": [
          "version",
          "versions",
          "model version",
          "serving",
          "feature"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3507990280464175,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727151+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "summary": "This way, app developers can choose which to use, and we can continue to make performance updates to each model by deploying new versions.\nStarting with the Transform design, we saw how this pattern is used to ensure reproducibility of the data preparation dependencies between the model training pipeline and the model serving pipeline.\nThe Repeatable Splitting design pattern captures the way data is split among training, validation, and test datasets to ensure that an example used in training is never used for evaluation or testing even as the dataset grows.\nThe Bridged Schema design pattern looks at how to ensure reproducibility when a training dataset is a hybrid of newer data and older data with a different schema.\nNext, we discussed the Windowed Inference design pattern, which ensures that when features are calculated in a dynamic, time-dependent way, they can be correctly repeated between training and serving.\nThis design pattern is particularly useful when machine learning models require features that are computed from aggregates over time windows.\nLastly, we looked at the Model Versioning design pattern, where backward compatibility is achieved by deploying a changed model as a microservice with a different REST endpoint.\n8 See the Gojek blog, “Feast: Bridging ML Models and Data.”\nUntil this point, we’ve focused on patterns designed to help data and engineering teams prepare, build, train, and scale models for production use.\nThese patterns mainly addressed teams directly involved in the ML model development process.\nStakeholders could include executives whose business objectives dictate a model’s goals, the end users of a model, auditors, and compliance regulators.\nmodels.\nDecide whether or not to incorporate the ML model into their business\nMake use of predictions from an ML model.\nThroughout this chapter, we’ll look at patterns that address a model’s impact on individuals and groups outside the team and organization building a model.\nThe Heuristic Benchmark design pattern provides a way of putting the model’s performance in a context that end users and decision makers can understand.\nThe Explainable Predictions pattern provides approaches to improving trust in ML systems by fostering an understanding of the signals a model is using to make predictions.\nThe Fairness Lens design pattern aims to ensure that models behave equitably across different subsets of users and prediction scenarios.\nRecommended practices for responsible AI include employing a human-centered design approach by engaging with a diverse set of users and use-case scenarios throughout project development, understanding the limitations of datasets and models, and continuing to monitor and update ML systems after deployment.\nThe Heuristic Benchmark pattern compares an ML model against a simple, easy-to-understand heuristic in order to explain the model’s performance to business decision makers.\nAfter training an ML model to predict the duration of a bicycle’s rental period, they evaluate the model on a test dataset and determine that the mean absolute error (MAE) of the trained ML model is 1,200 seconds.\nWhen they present this model to the business decision makers, they will likely be asked: “Is an MAE of 1,200 seconds good or bad?” This is a question we need to be ready to handle whenever we develop a model and present it to business stakeholders.\nIf we train an image classification model on items in a product catalog and the mean average precision (MAP) is 95%, we can expect to be asked: “Is a MAP of 95% good or bad?”\nIf this is the second ML model being developed for a task, an easy answer is to compare the model’s performance against the currently operational version.\nAs long as this task is already being performed in production and evaluation metrics are being collected, we can compare the performance of our new ML model against the current production methodology.\nIn such cases, the solution is to create a simple benchmark for the sole purpose of comparing against our newly developed ML model.\nAvoid the temptation to train even a simple machine learning model, such as a linear regression, on a dataset and use that as a benchmark—linear regression is likely not intuitive enough, especially once we start to include categorical variables, more than a handful of inputs, or engineered features.",
      "keywords": [
        "model",
        "design pattern",
        "Heuristic Benchmark",
        "pattern",
        "design",
        "data",
        "features",
        "Heuristic",
        "Benchmark design pattern",
        "Benchmark",
        "training data",
        "Model Versioning design",
        "training",
        "Heuristic Benchmark pattern",
        "Pipeline design pattern"
      ],
      "concepts": [
        "model",
        "data",
        "pattern",
        "designed",
        "features",
        "time",
        "predictions",
        "prediction",
        "predict",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 52,
          "title": "",
          "score": 0.865,
          "base_score": 0.715,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.773,
          "base_score": 0.623,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 43,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 34,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "benchmark",
          "pattern",
          "heuristic",
          "ml model",
          "design"
        ],
        "semantic": [],
        "merged": [
          "benchmark",
          "pattern",
          "heuristic",
          "ml model",
          "design"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35868907351518425,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727216+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 405-421)",
      "start_page": 405,
      "end_page": 421,
      "summary": "For example, say that we are building a model to predict the duration of rentals and our benchmark is a lookup table of average rental duration given the station name and whether or not it is peak commute hour:\nThe Explainable Predictions design pattern increases user trust in ML systems by providing users with an understanding of how and why models make certain predictions.\nFor all models, it is useful to be able to interpret predictions in order to understand the combinations of features influencing model behavior.\nThey provide data on how correct a model’s predictions are relative to ground truth values in the test set, but they carry no insight on why a model arrived at those predictions.\nIn many ML scenarios, users may be hesitant to accept a model’s prediction at face value.\nIn the model’s current form, there is no way to attribute the prediction to regions in an image, making it difficult for the doctor to trust the model.\nwithout insight into why a model makes the predictions it does, its use may become problematic.\nFinally, as data scientists and ML engineers, we can only improve our model quality to a certain degree without an understanding of the features it’s relying on to make predictions.\nFor example, let’s say we are training a model on tabular data to predict whether a flight will be delayed.\nThe model is trained on 20 features.\nTake for example a linear regression model that predicts fuel efficiency of a car.\nThe learned coefficients from our linear regression fuel efficiency model, which predicts a car’s miles per gallon.\nThe coefficients show us the relationship between each feature and the model’s output, predicted miles per gallon (MPG).\nFor example, from these coefficients, we can conclude that for each additional cylinder in a car, our model’s predicted MPG will decrease.\nThese methods aim to attribute a model’s output—whether it be an image, classification, or numerical value—to its features, by assigning attribution values to each feature indicating how much that feature contributed to the output.\nFeature attributions that explain a model’s output for an individual\nFor example, in a model predicting whether someone should\nGlobal feature attributions analyze the model’s behavior across an\nIn a model predicting whether a flight\nIn order to use these tools, we first need to understand the concept of a baseline as it applies to explaining models with feature attributions.\nThe goal of any explainability method is to answer the question, “Why did the model predict X?” Feature attributions attempt to do this by providing numerical values for each feature indicating how much that feature contributed to the final output.\nWithout context, these attribution values don’t mean much, and our first question will likely be, “0.4 and −0.2 relative to what?” That “what” is the model’s baseline.\nWhenever we get feature attribution values, they are all relative to a predefined baseline prediction value for our model.\nIn a text model, an uninformative baseline could be 0 values for the model’s embedding matrices or stop words like “the,” “is,” or “and.” In a model with numerical inputs, a common approach to choosing a baseline is to generate a prediction using the median value for each feature in the model.\nFor a regression task, a model will have exactly one numerical baseline prediction value.\nConsequently, for every prediction we make to this model, we’ll use 22.9 MPG as the baseline to compare predictions.\nTaking the same median baseline input as above, our classification model now returns the following as our baseline prediction:\nLet’s say we generate a new prediction on an example from our test set, and our model outputs the following array, predicting a 90% probability that this car has “low” fuel efficiency:\nThe resulting feature attribution values should explain why the model predicted 0.9 compared to the baseline prediction value of 0.1 for the “low” class.\nclasses to understand, for example, why our model predicted the same car had a 6% chance of belonging to our “medium” fuel efficiency class.\nFigure 7-2 shows instance-level feature attributions for a model that predicts the duration of a bike trip.\nThe uninformative baseline for this model is a trip duration of 13.6 minutes, which we get by generating a prediction using the median value for each feature in our dataset.\nWhen a model’s prediction is less than the baseline prediction value, we should expect most attribution values to be negative, and vice versa.\nIn this example, we get a predicted duration of 10.71, which is less than the model’s baseline, and explains why many of the attribution values are negative.\nIn this example, the trip’s distance was the most important feature, causing our model’s prediction to decrease 2.4 minutes from the baseline.\nAdditionally, as a sanity check, we should ensure that the feature attribution values roughly add up to the difference between the current prediction and the baseline prediction.\nThe feature attribution values for a single example in a model predicting bike trip duration.\nThe model’s baseline, calculated using the median of each feature value, is 13.6 minutes, and the attribution values show how much each feature influenced the prediction.\nInformative baselines, on the other hand, compare a model’s prediction with a specific alternative scenario.\nIn a model identifying fraudulent transactions, an informative baseline might answer the question, “Why was this transaction flagged as fraud instead of nonfraudulent?” Instead of using the median feature values across the entire training dataset to calculate the baseline, we would take the median of only the nonfraudulent values.\nIn an image model, maybe the training images contain a significant portion of solid black and white pixels, and using these as a baseline would result in inaccurate predictions.\nThe open source library SHAP provides a Python API for getting feature attributions on many types of models, and is based on the concept of Shapley Value introduced in Table 7-2.\nTo determine feature attribution values, SHAP calculates how much adding or removing each feature contributes to a model’s prediction output.\nIt performs this analysis across many different combinations of feature values and model output.\nimport shap explainer = shap.DeepExplainer(model, x_train[:100]) attribution_values = explainer.shap_values(x_test.values[:10])\nIn the code above, explainer.expected_value is our model’s baseline.\nSHAP calculates the baseline as the mean of the model’s output across the dataset we passed when we created the explainer (in this case, x_train[:100]), though we could also pass our own baseline value to force_plot.\nThe ground truth value for this example is 14 miles per gallon, and our model predicts 13.16.\nOur explanation will therefore explain our model’s prediction of 13.16 with feature attribution values.\nIn this case, the attribution values are relative to the model’s baseline of 24.16 MPG.\nThe attribution values should therefore add up to roughly 11, the difference between the model’s baseline and the prediction for this example.\nThe feature attribution values for one example from our fuel efficiency prediction model.\nHad our model’s prediction been above the baseline of 24.16, we would instead see mostly negative attribution values.\nFor this example, the most important indicator of fuel efficiency is weight, pushing our model’s prediction down by about 6 MPG from the baseline.",
      "keywords": [
        "model",
        "feature",
        "feature attributions",
        "baseline",
        "prediction",
        "attribution",
        "baseline prediction",
        "MODEL BASELINES",
        "image",
        "Heuristic benchmark",
        "Instance-level Feature attributions",
        "image model",
        "model ’s prediction",
        "SHAP",
        "Heuristic"
      ],
      "concepts": [
        "models",
        "predicting",
        "prediction",
        "predictions",
        "value",
        "features",
        "data",
        "baseline",
        "image",
        "imaging"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 48,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 47,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.557,
          "base_score": 0.407,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "baseline",
          "attribution",
          "attribution values",
          "feature",
          "values"
        ],
        "semantic": [],
        "merged": [
          "baseline",
          "attribution",
          "attribution values",
          "feature",
          "values"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30691434877055396,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727280+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 422-429)",
      "start_page": 422,
      "end_page": 429,
      "summary": "Explanations from deployed models\nThis works well during model development, but there are scenarios where you’d want to get explanations on a deployed model in addition to the model’s prediction output.\nHere, we’ll demonstrate how to get feature attributions on a deployed model using Google Cloud’s Explainable AI.\nAt the time of this writing, Explainable AI works with custom TensorFlow models and tabular data models built with AutoML.\nWe’ll deploy an image model to AI Platform to show explanations, but we could also use Explainable AI with TensorFlow models trained on tabular or text data.\nTo deploy a model to AI Platform with explanations, we first need to create a metadata file that will be used by the explanation service to calculate feature attributions.\nWhen using this SDK via AI Platform Notebooks, we also have the option to generate explanations locally within a notebook instance without deploying our model to the cloud.\nTo deploy our model to AI Platform, we can copy our model directory to a Cloud Storage bucket and use the gcloud CLI to create a model version.\nimage models deployed on AI Platform, IG returns an image with\nXRAI works only with image models deployed on AI\nOnce the model is deployed, we can get explanations using the Explainable AI SDK:\nIn Figure 7-5, we can see a comparison of the IG and XRAI explanations returned from Explainable AI for our ImageNet model.\nThe feature attributions returned from Explainable AI for an ImageNet model deployed to AI Platform.\nWe can see how pixel attributions can help increase confidence in the model’s prediction.\nExplainable AI also works in AutoML Tables, a tool for training and deploying tabular data models.\nFeature attributions through Explainable AI are enabled by default for models trained in AutoML Tables, and both global and instance-level explanations are provided.\nIf we don’t catch data imbalances before training a model, explainability methods like feature attributions can help bring data selection bias to light.\nLet’s say it correctly labels an image from our test set as “kayak,” but using feature attributions, we find that the model is relying on the boat’s paddle to predict “kayak” rather than the shape of the boat.\nCounterfactual analysis is an instance-level explainability technique that refers to finding examples from our dataset with similar features that resulted in different predictions from our model.\nThis type of explanation is especially useful for understanding how our training dataset affects model behavior.\nExample-based explanations work best on image or text data, and can be more intuitive than feature attributions or counterfactual analysis since they map a model’s prediction directly to the data used for training.",
      "keywords": [
        "model",
        "image",
        "attributions",
        "feature attributions",
        "data",
        "Explainable",
        "Platform",
        "Explanations",
        "image models",
        "feature",
        "image models deployed",
        "XRAI",
        "TensorFlow models",
        "model predictions",
        "model version"
      ],
      "concepts": [
        "model",
        "explanations",
        "explanation",
        "data",
        "image",
        "attribution",
        "explainable",
        "xrai",
        "prediction",
        "predict"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "",
          "score": 0.679,
          "base_score": 0.529,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "attributions",
          "ai",
          "explanations",
          "explainable ai",
          "feature attributions"
        ],
        "semantic": [],
        "merged": [
          "attributions",
          "ai",
          "explanations",
          "explainable ai",
          "feature attributions"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32767439982356455,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727335+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 430-437)",
      "start_page": 430,
      "end_page": 437,
      "summary": "showing how the model correctly predicted “french fries” for the given drawing through examples from the training dataset.\nThat is to say, we can’t expect our explanations to be high quality if our training dataset is an inaccurate representation of the groups reflected by our model, or if the baseline we’ve chosen doesn’t work well for the problem we’re solving.\nFairness in machine learning is a continuously evolving area of research, and there is no single catch-all solution or definition to making a model “fair.” Evaluating an entire end-to-end ML workflow—from data collection to model deployment —through a fairness lens is essential to building successful, high-quality models.\nThe problem with this thinking is that the datasets models learn from are created by humans, not machines, and humans are full of bias.\nIf this type of bias is not accounted for, it can find its way into models, creating adverse effects as production models directly reflect the bias present in the data.\nThis shoe example demonstrates bias in the training data distribution, and although it may seem oversimplified, this type of bias occurs frequently in production settings.\nEven when our dataset does appear balanced with respect to these identity characteristics, it is still subject to bias in the way these groups are represented in the data.\nThis data representation bias will be directly represented by our model.\nThis is also known as reporting bias, since the dataset (here, the “reported” data) doesn’t accurately reflect the real world.\nA common fallacy when dealing with data bias issues is that removing the areas of bias from a dataset will fix the problem.\nBefore using this data to train our model, we should ensure this group of 20 labelers reflects a diverse population.\nIn addition to data, bias can also be introduced during model training by the objective function we choose.\nTo handle problematic bias in machine learning, we need solutions both for identifying areas of harmful bias in data before training a model, and evaluating our trained model through a fairness lens.\nThe Fairness Lens design pattern provides approaches for building datasets and models that treat all groups of users equally.\nIf, on the other hand, the skew in the dataset contains naturally occurring bias that will not have adverse effects on different groups of people, “Design Pattern 10: Rebalancing ” in Chapter 3 provides solutions for handling data that is inherently imbalanced.\nWe will train a loan application approval model on this dataset in order to demonstrate different aspects of fairness.\nBecause ML models are a direct representation of the data used to train them, it’s possible to mitigate a significant amount of bias before building\nor training a model by performing thorough data analysis, and using the results of this analysis to adjust our data.\nDescriptions of different types of data bias\nData that doesn’t contain an equal representation of all possible groups that will use the model in production\nIs there bias in the way different demographic groups are represented in the data?\nIs there subjective bias introduced by data labelers?",
      "keywords": [
        "model",
        "data",
        "bias",
        "dataset",
        "data representation bias",
        "training",
        "training dataset",
        "Fairness Lens",
        "explanations",
        "data bias",
        "representation",
        "Fairness",
        "type",
        "problem",
        "problematic bias"
      ],
      "concepts": [
        "understanding",
        "case",
        "miss",
        "indicative",
        "additionally",
        "year",
        "single",
        "catch",
        "focusing",
        "thicknesses"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 50,
          "title": "",
          "score": 0.669,
          "base_score": 0.669,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 46,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.534,
          "base_score": 0.384,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bias",
          "fairness",
          "data bias",
          "groups",
          "lens"
        ],
        "semantic": [],
        "merged": [
          "bias",
          "fairness",
          "data bias",
          "groups",
          "lens"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2821965034891402,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727390+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 438-445)",
      "start_page": 438,
      "end_page": 445,
      "summary": "Since we haven’t built our model yet, we can initialize the What-If Tool widget by passing it only our data:\nThe What-If Tool’s “Datapoint editor,” where we can see how our data is split by label class and inspect features for individual examples from our dataset.\nKeeping the same color-coding by label, if we select the agency_code column from the Binning | Y-Axis drop-down, the tool now shows a chart of how balanced our data is with regard to the agency underwriting each application’s loan.\nFor example, maybe we want to limit our model to making predictions only on refinancing or home purchase loans since there may not be enough data available for other possible values in the loan_purpose column.\nOnce we’ve refined our dataset and prediction task, we can consider anything else we might want to optimize during model training.\nFor example, maybe we care most about our model’s accuracy on applications it predicts as “approved.” During model training, we’d want to optimize for AUC (or another metric) on the “approved” class in this binary classification model.\nIf we plan to use this data to train a model, it’s important that we look out for data representation bias.\nEven with rigorous data analysis, bias may find its way into a trained model.\nThis can happen as a result of a model’s architecture, optimization metrics, or data bias that wasn’t identified before training.\nTo demonstrate how to use it on a trained model, we’ll build on our mortgage dataset example.\nBased on our previous analysis, we’ve refined the dataset to only include loans for the purpose of refinancing or home purchases, and trained an XGBoost model to predict whether or not an application will be approved.\nWe’ll make a few additions to our What-If Tool initialization code above, this time passing in a function that calls our trained model, along with configs specifying our label column and the name for each label:\nNow that we’ve passed the tool our model, the resulting visualization shown in Figure 7-13 plots our test datapoints according to our model’s prediction confidence indicated on the y-axis.\nThe What-If Tool’s Performance & Fairness tab lets us evaluate our model’s fairness across different data slices.\nThe What-If Tool Performance & Fairness tab, showing our XGBoost model performance across different feature values.\nThis is likely due to the data representation bias identified in the previous section (we purposely left the dataset this way to show how models can amplify data bias).\nOptimizing for “Demographic parity,” for example, would ensure that our model approves the same percentage of applications for both HUD and non-HUD loans.\n12 opportunity fairness metric will ensure that datapoints from both the HUD and non-HUD slice with a ground truth value of “approved” in the test dataset are given an equal chance of being predicted “approved” by the model.\nThere are many other approaches, including rebalancing training data, retraining a model to optimize for a different metric, and more.",
      "keywords": [
        "model",
        "What-If Tool",
        "data",
        "Tool",
        "dataset",
        "HUD",
        "What-If",
        "bias",
        "approved",
        "What-If Tool Performance",
        "fairness",
        "loans",
        "Data representation bias",
        "column",
        "’ve"
      ],
      "concepts": [
        "data",
        "examples",
        "model",
        "dataset",
        "fairness",
        "column",
        "value",
        "bias",
        "tool",
        "loan"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 18,
          "title": "",
          "score": 0.431,
          "base_score": 0.431,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.409,
          "base_score": 0.409,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 52,
          "title": "",
          "score": 0.403,
          "base_score": 0.403,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.393,
          "base_score": 0.393,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 40,
          "title": "",
          "score": 0.383,
          "base_score": 0.383,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "approved",
          "hud",
          "bias",
          "loans"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "approved",
          "hud",
          "bias",
          "loans"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3132795967841766,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.727438+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 446-453)",
      "start_page": 446,
      "end_page": 453,
      "summary": "As seen from this analysis, there is no one-size-fits-all solution or evaluation metric for model fairness.\nThere are many ways to approach model fairness in addition to the pre- and post-training techniques discussed in the Solution section.\nHere, we’ll introduce a few alternative tools and processes for achieving fair models.\nML fairness is a rapidly evolving area of research—the tools included in this section aren’t meant to provide an exhaustive list, but rather a few techniques and tools currently available for improving model fairness.\nFairness Indicators (FI) are a suite of open source tools designed to help in understanding a dataset’s distribution before training, and evaluating model\nThe tools included in FI are TensorFlow Data Validation (TFDV) and TensorFlow Model Analysis (TFMA).\nFrom the Fairness Indicators Python package, TFMA can also be used as a standalone tool that works with both TensorFlow and non-TensorFlow models.\nThe fairness evaluation methods we discussed in the Solution section focused on manual, interactive data and model analysis.\nAs we operationalize our model and shift our focus to maintaining and improving it, finding ways to automate fairness evaluation will improve efficiency and ensure that fairness is integrated throughout our ML process.\nWe can do this through “Design Pattern 18: Continued Model Evaluation” discussed in Chapter 5, or with “Design Pattern 25: Workflow Pipeline” in Chapter 6 using components like those provided by TFX for data analysis and model evaluation.\nWhen we can’t find a way to fix inherent bias in our data or model directly, it’s possible to hardcode rules on top of our production model using allow and disallow lists.\nBecause gender cannot be determined by appearance alone, it would have reinforced unfair biases to return these labels when the model’s prediction is based solely on visual features.\nmodel’s label set in the data collection phase, before a model has been\nIn addition to the data distribution and representation solutions discussed earlier, another approach to minimizing model bias is to perform data augmentation.\nOne specific type of data augmentation is known as ablation, and is especially applicable in text models.\nWe’d then replace all other words throughout the dataset that we didn’t want to influence the model’s sentiment prediction with the same word (we used BLANK here, but anything not present in the rest of the text data will work).\nThe first Model Cards released provide summaries and fairness metrics for the Face Detection and Object Detection features in Google Cloud’s Vision API.\nmodel assets and generates a series of charts with various performance and fairness metrics.\nFairness applies specifically to identifying and removing bias from models, and explainability is one approach for diagnosing the presence of bias.\nFor example, applying explainability to a sentiment analysis model might reveal that the model is relying on identity terms to make its prediction when it should instead be using words like “worst,” “amazing,” or “not.”\nThe Responsible AI patterns outlined in this chapter are an essential part of every ML workflow—they can help us better understand the predictions generated by our models and catch potential adverse behavior before\nIn the Explainable Predictions pattern, we demonstrated how to use feature attributions to see which features were most important in signaling a model’s prediction.\nFinally, the Fairness Lens design pattern presented tools and metrics for ensuring a model’s predictions treat all groups of users in a way that is fair, equitable, and unbiased.",
      "keywords": [
        "model",
        "fairness",
        "data",
        "Model Cards",
        "Design Pattern",
        "Fairness Indicators",
        "model fairness",
        "Fairness Indicator tools",
        "TIP The What-If",
        "Pattern",
        "design",
        "analysis",
        "tools",
        "bias",
        "Model Evaluation"
      ],
      "concepts": [
        "model",
        "data",
        "fairness",
        "tool",
        "gender",
        "prediction",
        "predict",
        "patterns",
        "learning",
        "provide"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 48,
          "title": "",
          "score": 0.669,
          "base_score": 0.669,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.42,
          "base_score": 0.42,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 49,
          "title": "",
          "score": 0.379,
          "base_score": 0.379,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 31,
          "title": "",
          "score": 0.37,
          "base_score": 0.37,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.36,
          "base_score": 0.36,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "fairness",
          "tools",
          "model fairness",
          "analysis",
          "bias"
        ],
        "semantic": [],
        "merged": [
          "fairness",
          "tools",
          "model fairness",
          "analysis",
          "bias"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29610562954684844,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:23.727482+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 454-461)",
      "start_page": 454,
      "end_page": 461,
      "summary": "We set out to create a catalog of machine learning design patterns, solutions to recurring problems when designing, training, and deploying machine learning models and pipelines.\nThus, we had a chapter on input representation and another on model selection.\nWe then discussed patterns that modify the typical training loop and make inference more resilient.\nProblems associated with categorical features such as incomplete vocabulary, model size due to cardinality, and cold start.\nModel complexity insufficient to learn feature relationships.\nHelp models learn relationships between inputs faster by explicitly making each combination of input values a separate feature.\nCombine multiple machine learning models and aggregate their results to make predictions.\nPatterns That Modify Model Training\nUsing machine learning methods to learn a physics-based model or dynamical system.\nLack of large datasets that are needed to train complex machine learning models.\nHow to determine the optimal hyperparameters of a machine learning model.\nTake part of a previously trained model, freeze the weights, and use these nontrainable layers in a new model that solves a similar problem.\nInsert the training loop into an optimization method to find the optimal set of model hyperparameters.\nCarrying out model predictions over large volumes of data using an endpoint that is designed to handle requests one at a time will overwhelm the model.\nModel performance of deployed models degrades over time either due to data drift, concept drift or other changes to the pipelines which feed data to the model.\nHow to map the model predictions that are returned to the corresponding model input when submitting large prediction jobs.\nExport the machine learning model as a stateless function so that it can be shared by multiple clients in a scalable way.\nDetect when a deployed model is no longer fit-for- purpose by continually monitoring model predictions and evaluating model performance.\nThe inputs to a model must be transformed to create the features the model expects and that process must be consistent between training and serving.\nExplicitly capture and store the transformations applied to convert the model inputs into features.\nSome models require an ongoing sequence of instances to run inference, or features must be aggregated across a time window in such a way that avoids training–serving skew.\nExternalize the model state and invoke the model from a stream analytics pipeline to ensure that features calculated in a dynamic, time-dependent way can be correctly repeated between training and serving.\nCreate a feature store, a centralized location to store and document feature datasets that will be used in building machine learning models and can be shared across projects and teams.\nApply model explainability techniques to understand how and why models make predictions and improve user trust in ML systems.\nBias can cause machine learning models to not treat all users equally and can have adverse effects on some populations.\nUse tools to identify bias in datasets before training and evaluate trained models through a fairness lens to ensure model predictions are equitable across different groups of users and different scenarios.\nFor example, when working with categorical features, the Hashed Feature design pattern may be combined with the Embeddings design pattern.\nThese two patterns work together to address high-cardinality model inputs, such as working with text.",
      "keywords": [
        "Design pattern Problem",
        "pattern Problem solved",
        "model",
        "Design pattern",
        "pattern Problem",
        "machine learning",
        "machine learning models",
        "Chapter Design pattern",
        "Problem",
        "Patterns",
        "Representation Design pattern",
        "design",
        "Data",
        "Problem solved",
        "learning"
      ],
      "concepts": [
        "models",
        "data",
        "training",
        "feature",
        "prediction",
        "predictions",
        "problems",
        "solutions",
        "solution",
        "learn"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.757,
          "base_score": 0.607,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "",
          "score": 0.712,
          "base_score": 0.562,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "machine learning",
          "machine",
          "learning",
          "pattern problem",
          "learning models"
        ],
        "semantic": [],
        "merged": [
          "machine learning",
          "machine",
          "learning",
          "pattern problem",
          "learning models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.43901669081999783,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727735+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 462-472)",
      "start_page": 462,
      "end_page": 472,
      "summary": "Meanwhile, the Checkpoints design pattern naturally connects to Transfer Learning since earlier model checkpoints are often used during fine-tuning.\nWe also saw how incorporating the Neutral Class design pattern in a classification model, either naturally or through the Reframing pattern, can improve those learned embeddings.\nOr, in the case of Transfer Learning, the pre- trained model output could be viewed as the initial output of a Cascade pattern.\nThese patterns will likely also lead to the Explainable Predictions pattern, since when dealing with imbalanced data, it is especially important to verify that the model is picking up on the right signals for prediction.\nThe Cascade design pattern might also be helpful when using the Bridged Schema pattern and could be used as an alternative pattern by having a preliminary model that imputes missing values of the secondary schema.\nFor example, a feature store provides a convenient way to maintain and utilize streaming model features that may arise through the Windowed Inference pattern.\nThe feature versioning capability as discussed in the Feature Store pattern also plays a role with the Model Versioning design pattern.\nThis approach to model versioning via the Stateless Serving Function pattern can be connected back to the Reframing pattern where two different model versions could provide their own REST API endpoints for the two different model output representations.\nWe also discussed how, when using the Continued Model Evaluation pattern, it’s often advantageous to explore solutions presented in the Workflow Pipeline pattern as well, both to set up triggers that will initiate the retraining pipeline as well as maintain lineage tracking for various model versions that are created.\nContinued Model Evaluation is also closely connected to the Keyed Predictions pattern since this can provide a mechanism for easily joining ground truth to the model prediction outputs.\nThey provide a platform for automating and accelerating all stages of the ML life cycle, from managing data, to training models, evaluating performance, deploying models, serving predictions, and monitoring performance.\nBuilding a machine learning solution is a cyclical process that begins with a clear understanding of the business goals and ultimately leads to having a machine learning model in production that benefits that goal.\nThe ML life cycle begins with defining the business use case and ultimately leads to having a machine learning model in production that benefits that goal.\nThe discovery stage of an ML project begins with defining the business use case (Step 1 of Figure 8-2).\nAny machine learning project should begin with a thorough understanding of the business opportunity and how a machine learning model can make a tangible improvement on current operations.\nA successful discovery stage requires collaboration between the business domain experts as well as machine learning experts to assess the viability of an ML approach.\nMany machine learning models require a massive dataset for training.\nThese techniques and others can help determine which features are likely to benefit the model as well as further understanding of which data transformations will be needed to prepare the data for modeling.\nWithin the discovery stage, it can be helpful to do a few modeling experiments to see if there really is “signal in the noise.” At this point, it could be beneficial to perform a machine learning feasibility study (Step 3).\nAfter agreeing on key evaluation metrics and business KPIs, the development stage of the machine learning life cycle begins.\nThe details of developing an ML model are covered in detail in many machine learning resources.\nDuring the development stage, we begin by building data pipelines and engineering features (Step 4 of Figure 8-2) to process the data inputs that will be fed to the model.\nFeature engineering is the process of transforming raw input data into features that are more closely aligned with the model’s learning objective and expressed in a format that can be fed to the model for training.\nThe next step (Step 5 in Figure 8-2) of the development stage is focused on building the ML model.\nThe typical ML model training loop is described in detail at the beginning of Chapter 4 where we also address useful design patterns for changing the training loop to attain specific objectives.\nMany steps of the ML life cycle are iterative, and this is particularly true during model development.\nMany times, after some experimentation, it may be necessary to revisit the data, business objectives, and KPIs. New data insights are gleaned during the model development stage and these insights can shed additional light on what is possible (and what is not possible).\nother reproducibility design patterns that address challenges that arise during this iterative phase of model development.\nThroughout development of the model, each new adjustment or approach is measured against the evaluation metrics that were set in the discovery stage.\nAssuming successful completion of the model development and evidence of promising results, the next stage is focused on productionization of the model, with the first step (Step 8 in Figure 8-2) being to plan for deployment.\nTraining a machine learning model requires a substantial amount of work, but to fully realize the value of that effort, the model must run in production to support the business efforts it was designed to improve.\nThe design patterns in Chapter 5 touch on some of the issues that arise when operationalizing an ML model.\nAlso, many times, existing systems do not have a mechanism for supporting predictions coming from a machine learning model, so new applications and workflows must be developed.\nThe next step of the deployment stage is to operationalize the model (Step 9 in Figure 8-2).\nThis field of the practice is typically referred to as MLOps (ML Operations) and covers aspects related to automating, monitoring, testing, managing, and maintaining machine learning models in production.",
      "keywords": [
        "machine learning model",
        "model",
        "machine learning",
        "Feature Store pattern",
        "Cross design pattern",
        "pattern",
        "Feature Cross design",
        "data",
        "design pattern",
        "learning model",
        "learning",
        "embedding feature column",
        "Continued Model Evaluation",
        "model development",
        "bucket feature column"
      ],
      "concepts": [
        "model",
        "data",
        "pattern",
        "learning",
        "feature",
        "design",
        "business",
        "businesses",
        "development",
        "steps"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 45,
          "title": "",
          "score": 0.865,
          "base_score": 0.715,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.769,
          "base_score": 0.619,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 43,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 44,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stage",
          "pattern",
          "learning",
          "development",
          "business"
        ],
        "semantic": [],
        "merged": [
          "stage",
          "pattern",
          "learning",
          "development",
          "business"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3626647254967768,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727799+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 473-480)",
      "start_page": 473,
      "end_page": 480,
      "summary": "Building pipelines to automate these steps enables more efficient workflows and repeatable processes that improve future model development, and allows for increased agility in solving problems that arise.\nFor example, in addition to the code that is used to develop the model, it is important to apply these CI/CD principles to the data, including data cleaning, versioning, and orchestration of data pipelines.\nFor this reason, it is important to have in place mechanisms to efficiently monitor the machine learning model and all the various components that contribute to its performance, from data collection to the quality of the predictions during serving.\nUpon completion of the monitoring step, it can be beneficial to revisit the business use case and objectively, accurately assess how the machine learning model has influenced business performance.\nMachine learning tools in these three phases go from involving primarily manual development in the tactical phase, to using pipelines in the strategic phase, to being fully automated in the transformational phase.\nTypically, in this phase, there is no process to scale solutions consistently, and the ML tools used (see Figure 8-4) are developed on an ad hoc basis.\nThere are no tools in place to automate the various phases of the ML development cycle and there is little attention paid to developing repeatable processes of the workflow.\nManual development of AI models.\nOrganizations in the strategic phase have aligned AI efforts with business objectives and priorities, and ML is seen as a pivotal accelerator for the business.\nThere is infrastructure in place for these teams to easily share assets and develop ML systems that leverage both ready-to-use and custom models.\nData is stored in an enterprise data warehouse, and there is a unified model for centralized data and ML asset management.\nThe development of ML models occurs as an orchestrated experiment.\nThe data pipelines for developing ML models are automated utilizing a fully managed, serverless data service for ingestion and processing and are either scheduled or event driven.\nAdditionally, the ML workflow for training, evaluation, and batch prediction is managed by an automated pipeline so that the stages of the ML life cycle, from data validation and\nThe ML systems leverage a model API that is capable of handling real-time data streams both for inference and to collect data that is fed into the automated ML pipeline to refresh the model for later training.\nPipelines phase of AI development.\nFully automated organizations operate an integrated ML experimentation and production platform where models are built and deployed and ML practices are accessible to everyone in the organization.\nThe development and production environments are similar to the pipeline stage (see Figure 8-6) but have incorporated CI/CD practices into each of the various stages of their ML workflow as well.\nThese CI/CD best practices focus on reliability, reproducibility, and version control for the code to produce the ML models as well as the data and the data pipelines and their orchestration.\nMany of the design patterns discussed in this book are utilized throughout the course of any machine learning development cycle and will likely be used regardless of the production use case—for example, Hyperparameter Tuning, Heuristic Benchmark, Repeatable Splitting, Model Versioning, Distributed Training, Workflow Pipelines, or Checkpoints.",
      "keywords": [
        "model",
        "data",
        "machine learning model",
        "development",
        "phase",
        "machine learning",
        "Google Cloud",
        "pipelines",
        "learning model",
        "Google Cloud documentation",
        "learning",
        "data pipelines",
        "Google",
        "production",
        "organization"
      ],
      "concepts": [
        "data",
        "models",
        "development",
        "organizations",
        "organization",
        "sources",
        "pipelines",
        "likely",
        "predictions",
        "predict"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 54,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.753,
          "base_score": 0.603,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 37,
          "title": "",
          "score": 0.747,
          "base_score": 0.597,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 34,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "development",
          "pipelines",
          "ml",
          "automated",
          "phase"
        ],
        "semantic": [],
        "merged": [
          "development",
          "pipelines",
          "ml",
          "automated",
          "phase"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38198378692252055,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727859+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 481-488)",
      "start_page": 481,
      "end_page": 488,
      "summary": "Predictive modeling uses historical data to find patterns and determine the likelihood of a certain event occurring in the future.\nPredictive models can be found across many different industry domains.\nFor example, businesses might use predictive models to forecast revenue more accurately or anticipate future demand for products.\nIoT models rely on data collected by internet-connected sensors called IoT devices.\nMachine learning of IoT sensor device data can provide predictive models to warn against equipment failure before it happens.\nThese machine learning models are trained to flag transactions that appear fraudulent based on certain characteristics or patterns that have been learned in the data.\nFor example, consider a machine learning model that identifies anomalous train tracks based on images.\nAI Platform Notebooks, Explanations from deployed models\nAI Platform Prediction, Data and Model Tooling, Running the pipeline on Cloud AI Platform, Other serverless versioning tools, After training\nAI Platform Training, Data and Model Tooling, Fully managed hyperparameter tuning\nanomaly detection, Anomaly detection-Choosing a model architecture, Handling many predictions in near real time, Fraud and Anomaly Detection\nApache Beam, Batch and stream pipelines, Efficient transformations with tf.transform, Solution-Reduce computational overhead, Solution, Alternative implementations\nApache Spark, Batch and stream pipelines, Solution, Alternative implementations\nApigee, Model versioning with a managed service\nARIMA, Problem Representation Design Patterns, Solution\nASIC, ASICs for better performance at lower cost, Phase 1: Building the offline model\nAWS Lambda, Create web endpoint, Lambda architecture, Triggers for retraining, Model versioning with a managed service\nAzure, ASICs for better performance at lower cost, Model versioning with a managed service\nAzure Machine Learning, Model versioning with a managed service\nbatch prediction, The Machine Learning Process, Phase 2: Building the cloud model, Trade-Offs and Alternatives\nBERT, Context language models-Context language models, Embeddings of words versus sentences, Choosing a batch size\ndata, Data selection bias, Problem, Before training-Allow and disallow lists\nmodel, Images as tiled structures, Solution, Boosting, Weighted classes, Problem, Limitations of explanations, Problem-Before training, Data augmentation-Fairness versus explainability\nabout, What Are Design Patterns?, Data and Model Tooling\nfeatures of, Why It Works, Lambda architecture, Solution, Alternative implementations\nuses of, Problem, Embeddings in a data warehouse, Solution-Solution, Problem, Parsing sigmoid results, Solution, Solution\nabout, Data and Model Tooling\nuses of, Feature cross in BigQuery ML-Feature cross in BigQuery ML, Anomaly detection, Problem",
      "keywords": [
        "Machine learning",
        "Design Patterns",
        "data",
        "Cascade Neutral Class",
        "Model Tooling features",
        "model",
        "Problem Representation Design",
        "Feature Store",
        "Predictive models",
        "Batch",
        "Representation Design Patterns",
        "Problem",
        "Neutral Class",
        "Learning",
        "Feature Cross Embeddings"
      ],
      "concepts": [
        "models",
        "bias",
        "solution",
        "predictions",
        "predictive",
        "prediction",
        "problem",
        "data",
        "feature",
        "batch"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 53,
          "title": "",
          "score": 0.756,
          "base_score": 0.606,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.705,
          "base_score": 0.555,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 20,
          "title": "",
          "score": 0.698,
          "base_score": 0.548,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "predictive",
          "solution",
          "predictive models",
          "model tooling",
          "versioning managed"
        ],
        "semantic": [],
        "merged": [
          "predictive",
          "solution",
          "predictive models",
          "model tooling",
          "versioning managed"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3890568209207227,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727923+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 489-497)",
      "start_page": 489,
      "end_page": 497,
      "summary": "boolean variables, Data Representation Design Patterns\nCloud AI Platform, What Are Design Patterns?, Data and Model Tooling, Concept, Saving predictions\nCloud AI Platform Pipelines, Solution, Running the pipeline on Cloud AI Platform\nCloud AI Platform Training, Solution, Running the pipeline on Cloud AI Platform\ncontainers, Design Pattern 25: Workflow Pipeline, Solution, Why It Works\nContinued Model Evaluation design pattern, Design Patterns for Resilient Serving, Design Pattern 18: Continued Model Evaluation-Estimating retraining interval, Model versioning with a managed service, Responsible AI, Automating data evaluation, Pattern Interactions\ndata distribution bias, Problem\ndata drift, Data Drift-Data Drift, Problem, Estimating retraining interval, Continuous evaluation for offline models, Problem\ndata engineers, Roles, Scale, Solution\ndata parallelism, Solution-Solution, Synchronous training, Why It Works, Model parallelism\n(see also data transformation, feature engineering)\ndata representation, Data Representation Design Patterns-Data Representation Design Patterns\ndata representation bias, Before training\ndata transformation, Data and Feature Engineering\ndatasets, definition of, Data and Feature Engineering\ndecision trees, Models and Frameworks, Data Representation Design Patterns-Data Representation Design Patterns, Decreased model interpretability, Choosing a model architecture, Typical Training Loop, Solution\ndistributed data processing infrastructure, Solution\nDNN model, Text embeddings, Feature crosses in TensorFlow, Increased training and design time, Stochastic Gradient Descent\nEmbedding design pattern, Data Representation Design Patterns-Data Representation Design Patterns, Design Pattern 2: Embeddings- Embeddings in a data warehouse, Problem-Solution, Text data multiple ways, Pattern Interactions-Pattern Interactions\nEnsemble design pattern, Problem Representation Design Patterns-Problem Representation Design Patterns, Design Pattern 7: Ensembles-Other ensemble methods, Solution, Combining different techniques, Pattern Interactions\ntraining, Data and Feature Engineering, Early stopping\nusing, Keras Training Loop, Solution, Redefining an epoch-Retraining with more data\nexplainability, Problem, Solution, Solution, Explanations from deployed models, Fairness versus explainability-Fairness versus explainability, Pattern Interactions\nExplainable AI, Data and Model Tooling, Importance of explainability, Explanations from deployed models, Explanations from deployed models\nexported model, Solution\nFarm Fingerprint hashing algorithm, Solution, Repeatable sampling, Unstructured data\nFeature Cross design pattern, Data Representation Design Patterns, Design Pattern 3: Feature Cross-Need for regularization, Pattern Interactions\nfeature engineering, Data and Feature Engineering, Data Representation Design Patterns, Text and image transformations, Problem, Discovery\nfeature extraction, Data Representation Design Patterns, Trade-Offs and Alternatives-Fine-tuning versus feature extraction, Solution\nfeature, definition of, Data and Feature Engineering, Data Representation Design Patterns",
      "keywords": [
        "Representation Design Patterns",
        "Data Representation Design",
        "Design Pattern",
        "Schema design pattern",
        "Bridged Schema design",
        "Cascade design pattern",
        "Problem Representation Design",
        "Representation Design",
        "Design Patterns data",
        "Reproducibility Design Patterns",
        "Pattern Interactions",
        "Patterns data representation",
        "Pattern Interactions feature",
        "Pattern Interactions-Pattern Interactions",
        "Embedding design pattern"
      ],
      "concepts": [
        "solution",
        "data",
        "pattern",
        "models",
        "problem",
        "design",
        "feature",
        "transform",
        "transformations",
        "training"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.961,
          "base_score": 0.811,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.778,
          "base_score": 0.628,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "representation design",
          "design",
          "design patterns",
          "data representation",
          "representation"
        ],
        "semantic": [],
        "merged": [
          "representation design",
          "design",
          "design patterns",
          "data representation",
          "representation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3289530907630241,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.727980+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 498-505)",
      "start_page": 498,
      "end_page": 505,
      "summary": "GKE, Solution, Running the pipeline on Cloud AI Platform\nGoogle Cloud Public Datasets, Data and Model Tooling\nGPU, Problem, Problem-Synchronous training, ASICs for better performance at lower cost, Minimizing I/O waits, Problem, Running the pipeline on Cloud AI Platform, Transformational phase: Fully automated processes\nground truth label, Data and Feature Engineering, Data Quality, Capturing ground truth-Why It Works\nHashed Feature design pattern, Data Representation Design Patterns, Design Pattern 1: Hashed Feature-Empty hash buckets, Pattern Interactions\nheuristic benchmark , Solution-Development check, Model baseline, Discovery\nhyperparameters, Data and Feature Engineering, Problem\nImageDataGenerator, Phase 1: Building the offline model\nImageNet, Image embeddings, Autoencoders, Dataset considerations, Problem, Solution, Bottleneck layer\ninference, The Machine Learning Process, Solution\ninput, definition of, Data and Feature Engineering, Data Representation Design Patterns\ninterpretable by design, Solution\nabout, What Are Design Patterns?, Models and Frameworks, Weighted classes\nfeatures of, Weighted classes, Why It Works, Implementing transfer learning , Synchronous training, Solution, Phase 1: Building the offline model\nuses of, Text embeddings-Text embeddings, Solution-Solution, Extracting tabular features from text, Images as tiled structures, Which loss function should we use?, Transformations in TensorFlow and Keras-Transformations in TensorFlow and Keras\nKubeflow Pipelines, Solution, Solution, Running the pipeline on Cloud AI Platform, Apache Airflow and Kubeflow Pipelines\nlabel, definition of, Data and Feature Engineering\nlabeling, Data Quality, Solution, Capturing ground truth, Human experts, Problem\nlinear models, Models and Frameworks, Solution\nlow latency, The Machine Learning Process, Trade-Offs and Alternatives- Trade-Offs and Alternatives, Why It Works, Phase 1: Building the offline model, Problem-Feast, Why It Works\nLSTM, Choosing a model architecture, Solution, Sequence models\nmachine learning engineers (see ML engineers)\nmachine learning models, Models and Frameworks\nmachine learning, definition of, Models and Frameworks\nmatrix factorization, Problem Representation Design Patterns\ntasks of, Problem, Solution, Solution, Model versioning with a managed service\nML pipelines, The Machine Learning Process\nmodel evaluation, The Machine Learning Process, Problem, Lineage tracking in ML pipelines, Limitations of explanations, Solution",
      "keywords": [
        "Representation Design Patterns",
        "Design Patterns",
        "Data Representation Design",
        "Feature design pattern",
        "field-programmable gate array",
        "model Google Cloud",
        "Predictions design pattern",
        "Problem Representation Design",
        "Machine Learning Process",
        "Design",
        "Machine Learning",
        "Google Cloud Public",
        "Tuning design pattern",
        "Benchmark design pattern",
        "Google App Engine"
      ],
      "concepts": [
        "problem",
        "models",
        "solution",
        "patterns",
        "data",
        "learning",
        "phase",
        "process",
        "processes",
        "processing"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.961,
          "base_score": 0.811,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.807,
          "base_score": 0.657,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "solution",
          "design",
          "representation design",
          "problem",
          "learning process"
        ],
        "semantic": [],
        "merged": [
          "solution",
          "design",
          "representation design",
          "problem",
          "learning process"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.368400048723847,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.728037+00:00"
      }
    },
    {
      "chapter_number": 57,
      "title": "Segment 57 (pages 506-513)",
      "start_page": 506,
      "end_page": 513,
      "summary": "(see also Continued Model Evaluation design pattern)\nmodel parallelism, Solution, Model parallelism-Model parallelism\nmodel parameters, Problem-Problem\nModel Versioning design pattern, Reproducibility Design Patterns, Design Pattern 27: Model Versioning-New models versus new model versions, Pattern Interactions\nmodel, pre-trained, Implementing transfer learning -Pre-trained embeddings, Fine-tuning versus feature extraction, Responsible AI, Pattern Interactions\nmodel, text classification, Problem, Custom serving function, Problem, Multiple serving functions\nmulticlass classification problems, Design Pattern 6: Multilabel\nmultilabel classification, Sigmoid output for models with two classes- Parsing sigmoid results, Solution\nMultilabel design pattern, Problem Representation Design Patterns, Design Pattern 6: Multilabel -One versus rest\nMultimodal Input design pattern, Design Pattern 4: Multimodal Input- Multimodal feature representations and model interpretability, Pattern Interactions\nNeutral Class design pattern, Problem Representation Design Patterns, Design Pattern 9: Neutral Class -Reframing with neutral class, Responsible AI, Pattern Interactions\noverfit model, Problem, Problem\nPDE, Problem-Solution, Data-driven discretizations, Unbounded domains\nphysics-based model, Problem\nproblematic bias, Problem-Solution\nPyTorch, Solution, Problem, Synchronous training, Fully managed hyperparameter tuning\nrandom forest, Decreased model interpretability, Grid search and combinatorial explosion\nrandom seed, Problem-Solution\nray-tracing model, Solution\nRebalancing design pattern, Problem, Internal consistency, Design Pattern 10: Rebalancing -Importance of explainability, Before training, Pattern Interactions\nReframing design pattern, Problem Representation Design Patterns- Multitask learning, Model baseline, Pattern Interactions-Pattern Interactions\nregression models, Models and Frameworks, Solution\nREST API, for model serving, Prediction library\nscikit-learn, Reproducibility, Why scaling is desirable, Text data multiple ways, Increased training and design time, Choosing a model architecture, Grid search and combinatorial explosion, Grid search and combinatorial explosion\nserverless, Data and Model Tooling, Trade-Offs and Alternatives\nsigmoid, Sigmoid output for models with two classes, One versus rest-One versus rest\nsoftmax, Text embeddings, Multitask learning, Sigmoid output for models with two classes\nStack Overflow, Text data multiple ways-Text data multiple ways, Extracting tabular features from text, Problem, Parsing sigmoid results",
      "keywords": [
        "Multilabel design pattern",
        "Interactions multimodal inputs",
        "Multimodal Input design",
        "Representation Design Patterns",
        "Pattern Interactions multimodal",
        "design pattern",
        "Input design pattern",
        "Solution Multilabel design",
        "Problem Representation Design",
        "Reproducibility Design Patterns",
        "Pattern Interactions model",
        "Pattern Interactions",
        "Multimodal feature representations",
        "Pattern Interactions NLU",
        "Class design pattern"
      ],
      "concepts": [
        "model",
        "solution",
        "problem",
        "pattern",
        "training",
        "data",
        "design",
        "learning",
        "prediction",
        "predictions"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.791,
          "base_score": 0.641,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pattern interactions",
          "interactions",
          "pattern",
          "design",
          "problem"
        ],
        "semantic": [],
        "merged": [
          "pattern interactions",
          "interactions",
          "pattern",
          "design",
          "problem"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33441492387058164,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.728092+00:00"
      }
    },
    {
      "chapter_number": 58,
      "title": "Segment 58 (pages 514-521)",
      "start_page": 514,
      "end_page": 521,
      "summary": "about, Data and Feature Engineering\nabout, What Are Design Patterns?, Models and Frameworks, Increased training and design time\nfeatures of, Choosing a model architecture, Why It Works, Checkpoint selection, Problem, Synchronous training, Transformations in TensorFlow and Keras, Explanations from deployed models\nTensorFlow Data Validation, Data validation with TFX, Fairness Indicators\nTensorFlow hub, Embeddings in a data warehouse, Implementing transfer learning , Pre-trained embeddings, Explanations from deployed models\nTensorflow Lite, Problem, Phase 1: Building the offline model\nTensorFlow Serving, Autoscaling, Efficient transformations with tf.transform, TensorFlow Serving, After training\ntest data, Data and Feature Engineering, Stochastic Gradient Descent, Problem, Sequential split\nTFX, Data validation with TFX, Solution-Integrating CI/CD with pipelines, Fairness Indicators\nabout, What Are Design Patterns?, Minimizing I/O waits, Transformational phase: Fully automated processes\ntraining data, Data and Feature Engineering\ntraining examples, Data and Feature Engineering\ntraining loop, Model Training Patterns -Training Design Patterns, Solution, Regularization\ntraining, definition of, The Machine Learning Process\ntraining-serving skew, Problem, Alternate pattern approaches, Transform design pattern\nTransfer Learning design pattern, Design Pattern 13: Transfer Learning- Embeddings of words versus sentences, Allow and disallow lists, Pattern Interactions-Pattern Interactions\nTransform design pattern, What Are Design Patterns?, Feature cross in BigQuery ML, Reproducibility Design Patterns-Alternate pattern approaches, Transform design pattern, Pattern Interactions\nUseful Overfitting design pattern, Training Design Patterns-Overfitting a batch\nvalidation data, Data and Feature Engineering, Stochastic Gradient Descent\nvocabulary, One-hot encoding-Problem, Out-of-vocabulary input-Bucket collision, Text embeddings, Text data multiple ways-Text data multiple ways\nWorkflow Pipeline design pattern, Solution-Solution, Triggers for retraining, Reproducibility Design Patterns, Design Pattern 25: Workflow Pipeline-Lineage tracking in ML pipelines, Automating data evaluation, Pattern Interactions\nXGBoost, Text data multiple ways, Boosting, Increased training and design time, Choosing a model architecture, Other serverless versioning tools\nValliappa (Lak) Lakshmanan is Global Head for Data Analytics and AI Solutions on Google Cloud.\nHis team builds software solutions for business problems using Google Cloud’s data analytics and machine learning products.\nMichael Munn is an ML Solutions Engineer at Google where he works with customers of Google Cloud on helping them design, implement, and deploy machine learning models.\nThe animal on the cover of Machine Learning Design Patterns is a sunbittern (Eurypyga helias), a bird found in tropical regions of the Americas, from Guatemala to Brazil.",
      "keywords": [
        "Stateless Serving Function",
        "Feature Engineering training",
        "Transform design pattern",
        "stochastic gradient descent",
        "Minority Over-sampling Technique",
        "Synthetic Minority Over-sampling",
        "Feature Engineering",
        "Serving Function design",
        "Reproducibility Design Patterns",
        "Design Pattern",
        "Function design pattern",
        "Gradient Descent Swivel",
        "training Synthetic Minority",
        "Stateless Serving",
        "Stateless Serving Function-Design"
      ],
      "concepts": [
        "models",
        "data",
        "pattern",
        "solution",
        "solutions",
        "training",
        "problem",
        "tensorflow",
        "design",
        "transformations"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 55,
          "title": "",
          "score": 0.856,
          "base_score": 0.706,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 57,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 42,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "design",
          "pattern",
          "design patterns",
          "feature engineering",
          "patterns"
        ],
        "semantic": [],
        "merged": [
          "design",
          "pattern",
          "design patterns",
          "feature engineering",
          "patterns"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36714532314965515,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:23.728151+00:00"
      }
    }
  ],
  "total_chapters": 58,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Machine Learning Design Patterns_metadata.json",
    "enrichment_date": "2025-12-17T23:06:23.738447+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4263.971375999972,
    "total_similar_chapters": 286
  }
}