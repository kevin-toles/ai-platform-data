{
  "metadata": {
    "title": "Learning Python Ed6",
    "author": "Mark Lutz",
    "publisher": "O'Reilly Media",
    "total_pages": 1978,
    "conversion_date": "2025-11-05T17:37:08.399226",
    "conversion_method": "PyMuPDF + OCR fallback"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "A Python Q&A Session",
      "start_page": 1,
      "end_page": 44,
      "summary": "This chapter covers a python q&a session. Key topics include python, programming, and program. Learning\nPython, the cover image, and related trade dress are trademarks of O’Reilly\nMedia, Inc.",
      "keywords": [
        "Python",
        "Python programs",
        "Python code",
        "Python code runs",
        "Python language",
        "Python program files",
        "Python today",
        "Python interpreter",
        "code",
        "Python program code",
        "book",
        "Python programs run",
        "Python system",
        "run Python code",
        "Python users"
      ],
      "concepts": [
        "python",
        "programming",
        "program",
        "code",
        "coded",
        "development",
        "tools",
        "languages",
        "useful",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 39,
          "title": "Segment 39 (pages 353-355)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 1,
          "title": "Preliminaries",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 2,
      "title": "How Python Runs Programs",
      "start_page": 45,
      "end_page": 76,
      "summary": "This chapter covers how python runs programs. Key topics include python, coded, and coding. Although there is much cross-fertilization of ideas and work between these\nPythons, each is a separately installed software system, with its own project and\nuser base.",
      "keywords": [
        "Python",
        "Python code",
        "run Python code",
        "code",
        "Python programs",
        "Python command",
        "run Python",
        "Python code file",
        "run",
        "file",
        "Python source code",
        "Python language",
        "Debugging Python Code",
        "Python command line",
        "Python REPL prompt"
      ],
      "concepts": [
        "python",
        "coded",
        "coding",
        "programs",
        "programming",
        "files",
        "runs",
        "running",
        "likely",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "Python Essential Reference 4th",
          "chapter": 10,
          "title": "Execution Environment",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 342-350)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 1,
          "title": "Preliminaries",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 2,
          "title": "Python Language Basics, IPython,",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 3,
      "title": "How You Run Programs",
      "start_page": 77,
      "end_page": 108,
      "summary": "While we can’t go into full detail in this chapter, this section briefly\nsurveys the launchers in this department Key topics include python, object, and code.",
      "keywords": [
        "Python",
        "Python code",
        "code",
        "Python code runs",
        "run Python code",
        "module",
        "Python module object",
        "Python module file",
        "objects",
        "file",
        "Python programs",
        "Hack",
        "Debugging Python Code",
        "Python REPL",
        "run"
      ],
      "concepts": [
        "python",
        "object",
        "code",
        "coded",
        "coding",
        "modules",
        "imports",
        "important",
        "file",
        "string"
      ],
      "similar_chapters": [
        {
          "book": "Python Essential Reference 4th",
          "chapter": 10,
          "title": "Execution Environment",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 342-350)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 31,
          "title": "Segment 31 (pages 256-263)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Python Architecture Patterns",
          "chapter": 11,
          "title": "[ 383 ]",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 4,
      "title": "Introducing Python Object Types",
      "start_page": 109,
      "end_page": 180,
      "summary": "This chapter covers introducing python object types. Key topics include python, pythonic, and objects. As a rule of thumb, Python’s toolset is layered: generic\noperations that span multiple object types show up as built-in functions or\nexpressions (e.g., len(X), X[0]), but type-specific operations are method calls\n(e.g., aString.upper()).",
      "keywords": [
        "Python",
        "object types",
        "object",
        "decimal",
        "Python list object",
        "Python code",
        "core object types",
        "list",
        "type",
        "Code",
        "Python Expression Operators",
        "code Python type",
        "strings",
        "numbers",
        "string"
      ],
      "concepts": [
        "python",
        "pythonic",
        "objects",
        "type",
        "typed",
        "typing",
        "string",
        "strings",
        "operator",
        "operation"
      ],
      "similar_chapters": [
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 29,
          "title": "Segment 29 (pages 576-595)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 1,
          "title": "Segment 1 (pages 1-19)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 2,
          "title": "Python Language Basics, IPython,",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 5,
      "title": "Numeric Types",
      "start_page": 181,
      "end_page": 220,
      "summary": "This chapter covers numeric types. Key topics include python, objects, and strings. Among topics\nskipped here, sets also support in-place changes with additional methods, as well\nas assignment operators we’ll study later (e.g., &= and |=).",
      "keywords": [
        "Python",
        "object",
        "Python string object",
        "Python code",
        "type",
        "string",
        "References",
        "strings",
        "PYTHON REFERENCES",
        "code",
        "sets",
        "Python language",
        "string object",
        "Python Boolean type",
        "object types"
      ],
      "concepts": [
        "python",
        "objects",
        "strings",
        "string",
        "type",
        "typed",
        "typing",
        "sets",
        "setting",
        "coded"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 29,
          "title": "Segment 29 (pages 576-595)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 7,
          "title": "Segment 7 (pages 128-146)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 20,
          "title": "Segment 20 (pages 392-414)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 15,
          "title": "Segment 15 (pages 290-311)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 6,
      "title": "The Dynamic Typing Interlude",
      "start_page": 221,
      "end_page": 260,
      "summary": "When opened\nin binary modes, files return raw bytes from the external file as bytes—a string\nvariant that supports most of the syntax and tools in this chapter (you’ll find\nmore on files and bytes in Chapters 4, 9, and 37) Key topics include string, strings, and python. Covers method.",
      "keywords": [
        "string",
        "Python",
        "strings",
        "string methods",
        "Python strings",
        "format string",
        "method",
        "code",
        "Python string formatting",
        "string formatting",
        "string object",
        "format",
        "formatting",
        "object",
        "expression"
      ],
      "concepts": [
        "string",
        "strings",
        "python",
        "format",
        "formatting",
        "codes",
        "coded",
        "coding",
        "methods",
        "lines"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 8,
          "title": "Segment 8 (pages 147-169)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 30,
          "title": "Segment 30 (pages 269-279)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 16,
          "title": "String and Text Handling",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 7,
      "title": "String Fundamentals",
      "start_page": 261,
      "end_page": 320,
      "summary": "This chapter covers string fundamentals. Key topics include list, dictionary, and dictionaries. Covers method. conversion flags for some cases, and requires a string object for s (to flexibly\nallow any type like the expression’s %s, either omit the type code or formatspec\nin full, or use a !s conversion flag as described earlier).",
      "keywords": [
        "Python",
        "Learning Python",
        "list",
        "key",
        "keys",
        "dictionary",
        "code",
        "method",
        "Python expression code",
        "Python list objects",
        "Python list",
        "dictionary keys",
        "object",
        "dictionaries",
        "list methods"
      ],
      "concepts": [
        "list",
        "dictionary",
        "dictionaries",
        "keys",
        "key",
        "value",
        "python",
        "strings",
        "coded",
        "codes"
      ],
      "similar_chapters": [
        {
          "book": "Python Cookbook 3rd",
          "chapter": 2,
          "title": "Strings and Text",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 5,
          "title": "Segment 5 (pages 35-42)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 7,
          "title": "Segment 7 (pages 128-146)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 8,
      "title": "Lists and Dictionaries",
      "start_page": 321,
      "end_page": 380,
      "summary": "This chapter covers lists and dictionaries. Key topics include files, object, and python. Dictionary views and sets\nThough perhaps even more obscure than view objects in general, some\ndictionary views are also set-like and support set operations such as union and\nintersection that we used on true sets in Chapter 5.",
      "keywords": [
        "Python",
        "Python objects",
        "file",
        "object",
        "list",
        "Python file object",
        "file object",
        "tuple",
        "text file",
        "object types",
        "Python object types",
        "type",
        "open",
        "Pat",
        "text"
      ],
      "concepts": [
        "files",
        "object",
        "python",
        "type",
        "typing",
        "typed",
        "list",
        "dictionary",
        "dictionaries",
        "strings"
      ],
      "similar_chapters": [
        {
          "book": "Fluent Python 2nd",
          "chapter": 7,
          "title": "Segment 7 (pages 128-146)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 2,
          "title": "Segment 2 (pages 20-40)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Fluent Python 2nd",
          "chapter": 29,
          "title": "Segment 29 (pages 576-595)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 9,
      "title": "Tuples, Files, and Everything Else",
      "start_page": 381,
      "end_page": 440,
      "summary": "This chapter covers tuples, files, and everything else. Key topics include pythonic, pythons, and statement. What module might you use to store Python objects in a file without\nconverting them to strings yourself.",
      "keywords": [
        "Python",
        "Python statement syntax",
        "statement",
        "Python code",
        "code",
        "statements",
        "assignment",
        "Python statement",
        "Python assignment statement",
        "assignment statements",
        "line",
        "equivalent Python statement",
        "Python language",
        "list",
        "object"
      ],
      "concepts": [
        "pythonic",
        "pythons",
        "statement",
        "statements",
        "coding",
        "code",
        "coded",
        "assigns",
        "assignments",
        "object"
      ],
      "similar_chapters": [
        {
          "book": "Python Essential Reference 4th",
          "chapter": 25,
          "title": "Miscellaneous Library Modules",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 3,
          "title": "Built-In Data Structures,",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 6,
          "title": "Segment 6 (pages 46-53)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "chapter_number": 10,
      "title": "Introducing Python Statements",
      "start_page": 441,
      "end_page": 480,
      "summary": "This chapter covers introducing python statements. Key topics include printing, python, and pythonic. As you can see, most of Python’s reserved words are all lowercase.",
      "keywords": [
        "Python",
        "statement",
        "statements",
        "match",
        "File",
        "code",
        "match statement",
        "line",
        "Python code",
        "Python expression statements",
        "Bad",
        "Print Operations",
        "print statement",
        "object",
        "Python statements"
      ],
      "concepts": [
        "printing",
        "python",
        "pythonic",
        "statement",
        "statements",
        "coded",
        "coding",
        "match",
        "matches",
        "line"
      ],
      "similar_chapters": [
        {
          "book": "Python Essential Reference 4th",
          "chapter": 3,
          "title": "Lexical Conventions and Syntax",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 1,
          "title": "A Tutorial Introduction",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 10,
          "title": "Execution Environment",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": 1,
      "content": "O'REILLY’ %z\n\nLearning\n\nPython\n\nPowerful Object-Oriented Programming\n\nAD\n\nMark Lutz",
      "content_length": 83,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": 1,
      "content": "Learning Python\nSIXTH EDITION\nPowerful Object-Oriented Programming\nMark Lutz",
      "content_length": 76,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": 1,
      "content": "Learning Python\nby Mark Lutz\nCopyright © 2025 Mark Lutz. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\nSebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional\nuse. Online editions are also available for most titles (http://oreilly.com). For\nmore information, contact our corporate/institutional sales department: 800-998-\n9938 or corporate@oreilly.com.\nAcquisitions Editor: Louise Corrigan\nDevelopment Editor: Sara Hunter\nProduction Editor: Kristen Brown\nCopyeditor: nSight, Inc.\nProofreader: Piper Content Partners\nIndexer: nSight, Inc.\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Kate Dullea\nMarch 2025: Sixth Edition\nRevision History for the Sixth Edition\n2025-02-25: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098171308 for release",
      "content_length": 920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": 1,
      "content": "details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Learning\nPython, the cover image, and related trade dress are trademarks of O’Reilly\nMedia, Inc.\nThe views expressed in this work are those of the author and do not represent the\npublisher’s views. While the publisher and the author have used good faith\nefforts to ensure that the information and instructions contained in this work are\naccurate, the publisher and the author disclaim all responsibility for errors or\nomissions, including without limitation responsibility for damages resulting\nfrom the use of or reliance on this work. Use of the information and instructions\ncontained in this work is at your own risk. If any code samples or other\ntechnology this work contains or describes is subject to open source licenses or\nthe intellectual property rights of others, it is your responsibility to ensure that\nyour use thereof complies with such licenses and/or rights.\n978-1-098-17130-8\n[LSI]",
      "content_length": 976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": 1,
      "content": "[Dedication]\nTo Vera.\n\nYou are my life.",
      "content_length": 39,
      "extraction_method": "OCR"
    },
    {
      "page_number": 6,
      "chapter": 1,
      "content": "Preface\nIf you’re browsing a bookstore and trying to make sense of this book, try this:\nPython is one of the most widely used programming languages in the\nworld. It’s part of nearly every role that computers play in our lives, and\nits relative ease of use makes it an ideal way to get started with\nprogramming.\nThis book is a tutorial that teaches Python language fundamentals in\ndepth. Its content is aimed at Python newcomers of all stripes, applies to\nevery role that Python plays, and is based on decades of feedback from\nreal learners like you.\nThis edition updates this book for a decade of changes in Python and its\nworld. It drops coverage of the now-defunct Python 2.X, explores new\ntools added to Python through version 3.12, and applies to other\nPythons past and future.\nThe rest of this preface provides more background info on this book and its\nsubject. It explains what’s changed since the prior edition, debuts the book’s\nexamples package, and may help you get oriented before jumping into details.\nPython\nBy most metrics you’ll find on the web today, Python is now either the most-\nused programming language on the planet or very near the top of the list. The\noft-cited TIOBE popularity index, for example, has ranked Python most popular\nfor several years. As this edition is being written in 2024, it lists Python at #1\nand well ahead of its nearest followers, C, C++, and Java.\nPopularity ranks are prone to change, of course, and rely on usage metrics that\nare open to debate that we’ll skip here. Based on every signal available, though,\nthe Python revolution has clearly happened. It’s now a ubiquitous, go-to\nlanguage in web development, scientific programming, systems administration,",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": 1,
      "content": "AI, and nearly everything else computers do today. Thanks to its relative\nsimplicity, Python is also commonly used to introduce newcomers to computer\nscience across the education spectrum.\nIn fact, it’s now fairly safe to say that Python played a pivotal role in changing\nthe world. By spearheading a shift from statically typed compiled languages to\ndynamically typed scripting languages, Python ushered in changes that were at\nleast as profound as those of the earlier transition from machine language to\ncompiled languages. The scripting-language shift both enabled tasks formerly\nimpractical, and opened the field to nonprofessional contributors. In the process,\nit propelled computers to a prominence that would have been unthinkable in\ndecades past. The internet, for example, simply could not be what it is today\nwithout tools like Python. For better and worse, Python enables the new.\nLofty goals aside, all of this has two tangible implications for this book. First,\nbecause this is now a post-revolution Python world, this edition does much less\ncheerleading than its predecessors. There’s no reason to waste your time\npromoting a tool that’s already arrived. This edition still summarizes Python’s\nvalue proposition in Chapter 1, but the domains and tools that you’re likely to\nexplore after learning the basics here are readily available on the web and\nchange too regularly to cover in a fundamentals book like this in any event.\nSecond, Python’s popularity means that by reading this book, you’ll be adding a\nvaluable skill to your toolset, which will help you in a wide variety of computer-\nsoftware tasks. Learning Python will both make entire domains accessible to you\nand enable you to achieve programming goals that might otherwise be difficult\nor impossible. Python users now enjoy a wealth of prior art ready to be\nleveraged in a language that accelerates their work.\nThat said, Python isn’t the only game out there, and you’d also be well served by\nlearning computer science from the ground up—the full stack in developer\nspeak. Studying lower-level languages like C and Java, for example, can still\ngive you a much more complete perspective than scripting languages alone and\nhelp you solve complex problems as they arise. Python itself, after all, is just a C\nprogram in its most-used flavor.\nEven so, Python is a great place to start, and enough for many a task. While it’s\nnot without warts and suffers from the thrashing that’s endemic to software\ntoday, a multitude of developers still find Python a lot more fun to use than other",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": 1,
      "content": "tools. Java and C++, for example, seem languages designed for middle\nmanagement: they hobble programmers with training wheels and bureaucratic\nhurdles that have little to do with your program’s goals. Python, in sharp\ncontrast, remains more ally than obstacle. That viewpoint is naturally subjective,\nbut you’ve come to the right place to do the math on this yourself.\nThis Book\nThis book is a tutorial on the Python language and a classic in its domain. It’s the\nproduct of three decades spent using, promoting, and teaching Python, and dates\nback to the mid-1990s, when Python was still at version 1.X, and the web was\njust something developers mused about over lunch. Although the focus here is\nfirmly on the present, that legacy naturally adds some historical context that will\nhelp you understand Python more deeply. Despite what you may have heard, the\npast matters, especially in knowledge-based fields.\nJust as importantly, this book has always been based on live-and-in-person\nfeedback from Python beginners struggling to learn Python for the first time.\nThis feedback mostly owes to Python training classes taught over a period of\ntwo decades. While these classes have now gone the way of the dodo and Yahoo,\nthis book takes care to retain its learner-inspired material because that’s much—\nif not most—of its value.\nAs a result, if you’re like most of the thousands of learners whose experiences\nhave been captured here, you’ll probably find that this book works like a self-\npaced version of the Python training sessions from which it arose. You may\nsometimes even find that it answers your questions before they are asked\nbecause a host of learners before you have had the same queries. This isn’t\nclairvoyance; it simply reflects the fact that learning resources do best when they\nlisten to learners.\nIt’s also worth noting up front that this book sometimes critiques Python changes\nwhile presenting them. Critical thinking is crucial in engineering domains—\nespecially in one caught up in an arms race that convolutes tools used by\nmillions of people. On some levels, Python remains a constantly morphing\nsandbox of ideas that too often prioritizes changer hubris over user need, and this\nbook is not shy about calling this out. That said, the main goal here is to educate,",
      "content_length": 2288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": 1,
      "content": "not criticize, and opinions are always, well, opinionated. Although views here\nreflect decades of using and teaching Python, you should always judge the net\nworth of Python changes for yourself in whatever world you’ve been cast.\nThis Edition\nThis edition completely drops coverage of Python 2.X, the earlier version of the\nlanguage, and adds new coverage of recent changes in Python 3.X, the newer and\nincompatible version. When the prior edition was published in 2013, Python 2.X\nwas still widely used and probably even dominant. Because of that, the prior\nedition had to cover both the established 2.X and the new and upcoming 3.X,\nwhich at times made for a twisted tale indeed.\nOver a decade later, 2.X has been officially sunsetted, and the Python world has\nadopted 3.X so fully that 2.X constitutes an unwarranted distraction to today’s\nPython learners. Hence, after a decades-long tenure, 2.X-specific content has\nbeen cut here to make room for new 3.X topics and address book size in general.\nFormally, this edition has been updated to be current with Python 3.12 and its\nera, though it also previews 3.13 mods, and its focus on fundamentals makes it\ngenerally applicable to both older and newer Python versions.\nBefore the emails start flooding in, this book wants to make clear that it regrets\nthe loss of historical context (and secretly pines for the simpler days of 2.X too).\nBut 3.X is a substantial topic all by itself, without bifurcating the story and\nincreasing the page count for a Python version that is now little used. So go well\ninto that good Python night, 2.X, and long live 3.X. Unless stated otherwise,\n“Python” in this edition simply means the 3.X line in general and 3.12 and later\nin particular.\nIn terms of 3.X mods, this edition newly covers f'…' f-string literals, := named-\nassignment expressions, match statements, type hinting, async coroutines, star-\nunpacking proliferation, underscore digit separators, __main__.py package files,\n__getattr__ module hooks, except* exception groups, dictionary-key\ninsertion order, positional-only function arguments, hash-based bytecode files,\nand other additions, deprecations, and mutations that have cropped up over the\nlast decade plus.",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": 1,
      "content": "Among these, type hinting and async coroutines are not covered in depth—by\ndesign. The former is an optional and academic tool wholly unused by Python\nitself and at odds with its core principles. The latter is an advanced applications\ntool and has morphed constantly since its inception. And both quickly head over\ncomplexity cliffs that push them out of scope for Python beginners. When\nneeded, supplemental info on such narrow topics is always just a search away on\nthe web. Here, the goal is learning to walk well before trying to run.\nAmong other noteworthy changes this time around:\nThe Unicode content in the advanced part’s Chapter 37 is new and\nimproved because this topic is now an essential in Python 3.X and the\nworld at large.\nUsage coverage, including the new Appendix A, gives more focus to\nmacOS, Android, Linux, and iOS because not all of this book’s readers\nuse Windows.\nMost code-file examples now have numbered captions because the\nextra formality distinguishes them better in the book, and it’s worth the\nspace.\nSome redundancy has been trimmed, but not all, because repetition is\nuseful and even important in learning resources.\nThe size of this book was reduced by the prior bullet, rewrites and flow\nmods, and the net of 2.X cuts and 3.X inserts because it’s less to grok.\nThe size of the print version of this book was further reduced by\nmoving two advanced but optional chapters online (Chapters 38 and 39)\nbecause it’s less to lug.\nFictitious names in examples are more gender neutral: “Bob” is now an\nambiguous “Pat” unless paired with “Sue” as before because it better\ndefuses bias.\nThe Monty Python references have been dropped because they can be\nconfusing and might be divisive, and borrowing personality from media\nseems cheap.",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": 1,
      "content": "Both first-person voice and personal anecdotes have been globally\nsacked because you’ve bought this book to learn Python, not an author’s\nlife story.\nAbout the last two: Python’s namesake was funny stuff, to be sure, but\ncompulsively aping the work of a nearly all-male comedy group can seem like\nthe secret handshake of an exclusive boys’ club in hindsight. And while an\noccasional “I” or “my” might add color or credibility, overuse tends to come off\nas narcissism. Hence, the former 1k “spam” are now symbols more inclusive,\nand this book’s three-decade tenure will have to speak for itself.\nDespite all the mods, this edition remains much more technical novel than\nreference manual, and meaty enough to be comparable to a full-semester class\non Python and programming. It introduces topics and expands them in later\nchapters as recurring themes, accumulating comprehensive coverage along the\nway. There are Python quick-reference resources at python.org and a multitude\nof blogs and videos that promise to teach Python rapidly. This book is for those\nwho know that learning something well requires a bit more.\nMedia Choices\nAs of this writing, this book is destined to be available in three forms: print (i.e.,\npaper), ebook (e.g., PDF, ePub, and Kindle), and online (a.k.a. web). The latter\nmeans the publisher’s subscription service, currently branded as the O’Reilly\nlearning platform (f.k.a. Safari). Naturally, each medium has valid uses that vary\nper reader. For instance, many prefer print for linear reads and electronic media\nfor random searches and code copy/paste.\nYou’re welcome to use the forms that work best for you, of course, but should\ncarefully weigh the inherent privacy trade-offs of online media. By now, it\nshould be abundantly clear that online anything comes with a potential for covert\nuse and sale of customer information and access, which print and ebook media\nlargely avoid. While monetization schemes vary, online users just might have a\nlegion of salespeople peering over their shoulders as they use products for which\nthey’ve already paid in full.\nSo please be careful out there. Unless you must use an employer’s online",
      "content_length": 2156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": 1,
      "content": "subscription, this book suggests vetting your media options wisely and generally\nrecommends its print and ebook forms to protect your privacy whenever\npossible. Your life really shouldn’t be turned into a revenue stream unless you\nget reimbursed for it.\nOne last media tip: this book may also be fed by its publisher into generative AI\nmodels, the current hot topic in the tech press. Although this may prove useful\nfor looking up isolated facts, it’s not deep learning and isn’t necessarily any\nmore reliable than the least of the gossip it regurgitates. Until the world figures\nthis out, please use wisely.\nUpdates and Examples\nAs most authors would attest, it’s shockingly easy to miss typos in material\nyou’ve read hundreds of times, and incompatible change is a norm in computer\nbook topics. Hence, this edition, like all its predecessors, expects to be updated\nregularly after its publication.\nThis book’s supplements, example files, and clarifications and corrections (a.k.a.\nerrata) will all be maintained on the web. Here are the main coordinates for these\nonline resources; as usual, consult your local search engine if these change over\ntime:\nAuthor website\nThis site will be used to post general notes and updates related to this text or\nPython itself—a hedge against future changes and a sort of virtual appendix\nto this book.\nBook examples\nThis site will host this book’s examples package, with both individual files to\nview and save online and a ZIP file of all the examples to download to your\ndevice.\nPublisher website\nThis site will maintain this edition’s errata list, and chronicle patches applied",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": 1,
      "content": "to the text in reprints. It will also link to other formats, including ebooks.\nThe second of these is home to the examples’ source code. Please see the usage\nnotes there, as well as the examples’ coverage in “Where to Run: Code Folders”.\nThere is no plan to host the examples on GitHub today, because that site’s\nlearning curve is a lot to ask of beginners, and its commercial agendas should be\ncause for concern.\nAt any of the preceding websites, all error reports and suggestions for this book\nare welcome, and this feedback is invaluable for book quality. But please keep it\nfact-based and civil. Posts on the errata list have been mostly constructive, but\nthe list has limited utility, and has been known to attract the usual trolls. Such is\nlife in the age of global conversation.\nConventions and Reuse\nThis book’s mechanics will make more sense once you start reading it, but as a\nreference, this book uses the following typographical conventions:\nItalic\nUsed for email addresses, URLs, filenames, pathnames, and emphasizing\nnew terms when they are first introduced\nConstant width\nUsed for program code, the contents of files and the output from commands,\nand to designate modules, methods, statements, and system commands\nConstant width bold\nUsed in code sections to show commands or text that would be typed by the\nuser, and, occasionally, to highlight portions of code\nConstant width italic\nUsed for replaceables (content you must fill in) and some comments in code\nsections",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": 1,
      "content": "NOTE\nThis element indicates a tip, suggestion, or general note relating to the nearby text. The icon\nmay make more sense if you imagine a crow sounding an alarm.\nThree more quick content notes here: first, you’ll find occasional sidebars\n(delimited by boxes) and footnotes throughout, which are often optional reading\nbut provide additional context on the topics being presented. For instance, the\nsidebars that begin with “Why You Will Care:” amplify language topics with\nreal-world use cases.\nSecond, Python error messages are often shortened in this book to conserve\nspace. Please run offending code on your own device for the full text. Some\nmessages include stack traces, and some have sprouted location indicators and\nspeculative “Did you..?” help in the latest Python that might be useful for\nbeginners, though veterans’ mileage may vary; either way, the extra text is\nexcessive in a book tight on space.\nFinally, the publisher maintains a standard statement about reusing code in this\nbook, though the short, interactive code snippets used broadly here are hardly\nworth the legalese. Please see the book websites described earlier for the formal\nreuse story if you must care.\nAcknowledgments\nIn keeping with the depersonalization goal discussed earlier, this edition will\nforego the usual lengthy acknowledgments of its predecessors. Instead, it\nextends simple gratitude to the scores of former students and readers, who\nlargely shaped this book; the publishing company, which enabled this book to\nboth reach learners and improve with time; the host of users, contributors, and\npromoters, who made Python what it is today; and the subject of this book’s\ndedication page, who patiently tolerated yet another book project. This one\nmight be the last, but you never know what a bored author might next do.",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": 1,
      "content": "Part I. Getting Started",
      "content_length": 23,
      "extraction_method": "OCR"
    },
    {
      "page_number": 16,
      "chapter": 1,
      "content": "Chapter 1. A Python Q&A Session\nIf you’re reading this book, you may already know what Python is and why it’s\nan important tool to learn. If you don’t, you probably won’t be sold on Python\nuntil you’ve learned the language by reading the rest of this book and have done\na project or two. But before we jump into details, this first chapter will briefly\nintroduce some of the main reasons behind Python’s popularity and begin\nsculpting a definition of the language. This takes the form of a question-and-\nanswer session, which addresses some of the most common queries posed by\nbeginners—like you.\nWhy Do People Use Python?\nBecause there are many programming languages to choose from, this is the usual\nfirst question of newcomers and a great place to start. Given that millions of\npeople use Python today, there really is no way to answer this question with\ncomplete accuracy; the choice of development tools is often based on unique\nconstraints or personal preference.\nBut after teaching Python to hundreds of groups and thousands of students, some\ncommon themes have emerged. The primary factors cited by Python users seem\nto be these:\nSoftware quality\nFor many, Python’s focus on readability, coherence, and software quality in\ngeneral sets it apart from other tools in the programing world. Python code is\ndesigned to be readable, and hence reusable and maintainable—much more\nso than traditional scripting languages. The uniformity of Python code makes\nit relatively easy to understand, even if you did not write it. In addition,\nPython has deep support for more advanced software reuse mechanisms,\nsuch as object-oriented (OO) and functional programming, that can further",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": 1,
      "content": "promote code quality.\nDeveloper productivity\nPython boosts developer productivity many times beyond compiled or\nstatically typed languages. As one measure of this, Python code is typically\none-third to one-fifth the size of equivalent C++ or Java code. That means\nthere is less to type, less to debug, and less to maintain after the fact. Most\nPython programs also run immediately, without the lengthy compile and link\nsteps required by some other tools, further boosting programmer speed.\nProgram portability\nPython programs generally run unchanged on all major computer platforms.\nPorting a Python program between Linux and Windows, for example, is\noften just a matter of copying its code between machines. Moreover, Python\noffers multiple options for coding portable graphical user interfaces, database\naccess programs, web-based systems, and more. Even operating system\ninterfaces as proprietary as program launches and directory processing are as\nportable in Python as they can possibly be.\nApplication support\nPython comes with a large collection of prebuilt and portable functionality,\nknown as the standard library. This library supports an array of application-\nlevel programming tasks, from text pattern matching to network scripting. In\naddition, Python can be extended with both homegrown libraries and a vast\ncollection of third-party software. As covered ahead, Python’s third-party\ndomain offers tools for website construction, numeric programming, AI, and\nmuch more. The NumPy extension, for instance, has elevated Python to a\ncore tool in science, technology, engineering, and math (STEM), and Django",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": 1,
      "content": "and PyTorch have done similar for the web and AI.\nComponent integration\nPython scripts can communicate with other parts of an application using a\nvariety of integration mechanisms. Such integrations allow Python to be used\nas a product customization and extension tool. For instance, Python code can\ninvoke compiled libraries and be run by compiled programs, interact with\nother components over networks, and use Android and iOS toolkits on\nsmartphones. In fact, many Python core tools, including files, ultimately use\nprecoded interfaces to system libraries; even if your program is all Python,\nit’s not standalone.\nLove of craft\nBecause of Python’s ease of use and built-in toolset, it can make the act of\nprogramming more pleasure than chore. Although this benefit is intangible\nand subjective, its effect on productivity is an important asset. People do get\npaid for coding Python, but many use it just for fun—a testimonial rare in the\nsoftware field.\nOf these factors, the first two—quality and productivity—are probably the most\ncompelling benefits to most Python users; let’s take a closer look at each.\nSoftware Quality\nBy design, Python has a deliberately simple and readable syntax and a highly\nconsistent programming model. As a slogan at an early Python conference\nattested, the net result is that Python seems to fit your brain—that is, features of\nthe language interact in consistent and limited ways and follow naturally from a\nsmall set of core concepts. This makes the language easier to learn, understand,\nand remember. In practice, Python programmers do not need to constantly refer\nto manuals when reading or writing code; it generally just makes sense.",
      "content_length": 1674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": 1,
      "content": "By philosophy, Python adopts a somewhat minimalist mindset. This means that\nalthough there are usually multiple ways to accomplish a coding task, there is\nusually just one obvious way, a few less obvious alternatives, and a small set of\ncoherent interactions throughout. Moreover, Python usually doesn’t make\narbitrary decisions for you; when interactions are ambiguous, explicit\nintervention is preferred over “magic.” In the Python way of thinking, explicit is\nbetter than implicit, and simple is better than complex.\nBeyond such design themes, Python includes tools such as modules and object-\noriented programming (OOP) that naturally promote code reusability in skilled\nhands of the sort you’re about to acquire. And because Python is focused on\nquality, most Python programmers naturally are too; this can be a crucial\nadvantage when it’s time to use someone else’s Python code.\nDeveloper Productivity\nIf you’ve worked in the software field, you know that it can be a dynamic and\nbumpy ride. During the great internet boom of the mid-to-late 1990s, it was\ndifficult to find enough programmers to implement software projects; developers\nwere asked to implement systems as fast as the internet evolved. In later eras of\neconomic recession and layoffs, the picture shifted; programming staffs were\noften asked to accomplish the same tasks with even fewer people.\nIn both of these scenarios, Python has shined as a tool that allows programmers\nto get more done with less effort. It is explicitly optimized for speed of\ndevelopment—its simple syntax, dynamic typing, lack of compile steps, and\nbuilt-in toolset allow programmers to develop programs in a fraction of the time\nneeded when using some other tools.\nThe net effect is that Python typically increases developer productivity many\ntimes beyond the levels supported by traditional languages and often makes the\nimpossible possible. That’s good news in both boom and bust times, and\neverywhere the software industry goes in between.\nIs Python a “Scripting Language”?\nMaybe. Python is often applied in scripting roles, but not always. It’s regularly",
      "content_length": 2105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": 1,
      "content": "called an object-oriented scripting language—a definition that blends support\nfor OOP with an orientation toward scripting contexts, but this may be too\nnarrow and dismissive. If pressed for a one-liner, Python is probably better\nknown as:\nA general-purpose programming language that blends procedural, functional,\nand object-oriented paradigms and accelerates software development by\nreducing complexity.\nThat may not fit on a t-shirt quite as well, but it captures both the richness and\nscope of today’s Python.\nNevertheless, the term scripting seems to have stuck to Python like glue, perhaps\nas a contrast with the larger efforts required by some other tools. For example,\nsome people use the word “script” instead of “program” to describe a Python\ncode file, because it seems simpler and less formal to code. More usefully, others\nreserve “script” for a top-level file, and “program” for a more sophisticated\nmultifile application, both of which are common in Python.\nBecause the term scripting language has so many different meanings to different\nobservers, though, some would prefer that it not be applied to Python at all. In\nfact, people tend to make three very different assumptions when they hear\nPython labeled as such, some of which are more useful than others:\nShell tools\nSometimes when people hear Python described as a scripting language, they\nthink it means that Python is a tool for coding operating-system-oriented\nscripts. Such programs are often launched from console command lines and\nperform tasks such as processing text files and launching other programs.\nAs you’ll learn ahead, Python programs can and do serve such roles, but this\nis just one of dozens of common Python application domains. It is not just a\nbetter shell-script language.\nControl language\nTo others, scripting refers to a “glue” layer used to control and direct (i.e.,\nscript) other application components. As noted earlier, Python programs are\nindeed often deployed in the context of larger applications. For instance, to",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": 1,
      "content": "test hardware devices, Python programs may call out to components that give\nlow-level access to a device or port. Similarly, programs may run bits of\nPython code at strategic points to support end-user product customization\nwithout the need to ship and recompile the entire system’s source code.\nPython’s simplicity makes it a naturally flexible control tool. Technically,\nthough, this is also just a common Python role; many (and perhaps most)\nPython programmers code standalone scripts without ever using or knowing\nabout any integrated components. It is not just a control language.\nEase of use\nProbably the best use of the term scripting language is to refer to a relatively\nsimple language used for coding tasks quickly. This is especially true when\nthe term is applied to Python, which allows much faster program\ndevelopment than compiled languages like C++ or larger languages like\nJava. Its rapid development cycle fosters an exploratory, incremental mode\nof programming that has to be experienced to be appreciated.\nDon’t be fooled, though—Python is not just for simple tasks. Rather, it\nmakes tasks simple by its ease of use and flexibility. Python has an\napproachable feature set, but it allows programs to scale up in sophistication\nas needed. Because of that, it is commonly used for both quick tactical tasks\nand longer-term strategic development.\nSo, is Python a scripting language or not? It depends on whom you ask (and\nperhaps when you ask them). In general, the term scripting is best reserved for\nthe rapid and flexible mode of development that Python supports, rather than a\nparticular application domain or limiting label.\nOn a related note, people also sometimes call Python an interpreted language to\ndistinguish it from languages like C and C++. While this is also sometimes\nmeant to pigeonhole, it’s also easier to dismiss: there are many implementations\nof Python today, spanning the spectrum from traditional interpreters to\ntraditional compilers, so “interpreted” doesn’t apply. The clearer distinction may\nbe that Python is dynamically typed, not statically typed like languages that are\nnormally compiled. As you’ll soon learn, this accounts for much of the power\nthat Python brings to development tasks.",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": 1,
      "content": "OK, but What’s the Downside?\nThe only universally recognized downside to Python that has emerged over its\nmore than three-decade tenure may also be an inevitable trade-off for its ease of\nuse: Python’s execution speed may not always be as fast as that of fully compiled\nand lower-level languages such as C and C++. Though relatively rare today, you\nmay still occasionally need to get “closer to the iron” for some tasks by using\nlanguages that are more directly mapped to the underlying hardware.\nWe’ll talk about implementation concepts in Chapter 2, but in short, the most-\nused versions of Python today compile (i.e., translate) source code statements to\nan intermediate format known as bytecode and then interpret the bytecode.\nBytecode provides portability, as it is a platform-independent format. However,\nbecause Python is not commonly compiled all the way down to binary machine\ncode (e.g., instructions for an Intel or ARM chip in your PC or phone), some\nprograms will run more slowly in Python than in a fully compiled language like\nC.\nWhether you will ever care about this execution speed difference depends on\nwhat kinds of programs you write. Python is regularly optimized, and Python\ncode runs fast enough by itself in most application domains. Furthermore,\nwhenever you do something “real” in a Python script, like processing a file or\nconstructing a graphical user interface (GUI), your program will actually run at\nC speed, because such tasks are immediately dispatched to compiled C code\ninside the Python interpreter.\nAnd really, Python’s speed is a bit of a red herring today: in truth, Python is\ncommonly used in domains that require optimal execution speeds. Numeric\nprogramming and computer games, for example, often need at least their core\nnumber-crunching components to run at C speed (or better), but are frequently\ncoded in Python nevertheless.\nThere are multiple ways to achieve speed in Python when it counts. For instance,\nsystems such as PyPy compile bytecode further as your program runs; Cython\nallows you to code in a C-and-Python hybrid that’s compiled in full; and crucial\ncode can be split off to compiled extensions linked into Python for use in scripts.\nAs an example, the NumPy numeric-programming extension combines compiled\nand optimized libraries with the Python language, and in the process turns",
      "content_length": 2341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": 1,
      "content": "Python into a numeric programming tool that is simultaneously efficient and\neasy to use. Indeed, NumPy Python code regularly achieves speed parity with\nFortran and C++, without their added complexities.\nMore fundamentally, though, execution speed is not the only priority in most\nsoftware development. Python’s speed-of-development gain is often far more\nimportant than any speed-of-execution loss, especially given modern computer\nspeeds—and modern computer deadlines. Naturally, Python has other potential\ndownsides, including its frequent changes and convolutions, but these are\nsubjective calls best made after you’ve had a chance to vet them in this book.\nWho Uses Python Today?\nBecause Python is a free and open source software (FOSS) tool, an accurate\ncount of its user base is impossible—there are no license registrations to tally.\nMoreover, Python has been automatically included with countless Linux\ndistributions, Macintosh computers, and a wide range of products and hardware,\nfurther clouding the user-base picture.\nIn general, though, Python enjoys a very large user base and an active developer\ncommunity. It is generally considered to be among the top 5 most widely used\nprogramming languages in the world today (and for true sports fans, often\nweighs in at #1). Because Python has been broadly used for over three decades,\nit’s also very stable and robust.\nBecause of all this, Python is regularly applied in real revenue-generating\nproducts by real companies. While user lists are prone to change, a list of notable\nand known companies using Python today reads like a who’s who of the\nsoftware field: Google, Intel, Disney, YouTube, Industrial Light & Magic, Red\nHat, NASA, Eve Online, Seagate, JPL, Hewlett-Packard, JP Morgan Chase,\nDropbox, ESRI, Instagram, Spotify, Pinterest, Reddit, Microsoft, Netflix, and\nmany more.\nMore broadly, Python is deployed by most organizations developing software\ntoday in either strategic or tactical roles and regularly serves as the tool of choice\nin computer science education. It’s not just for one thing, it’s for everything.\nFor a sampling of additional Python users and applications that’s hopefully more",
      "content_length": 2165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": 1,
      "content": "up to date than a book can ever be, try the following pages at Python’s site:\nPython Success Stories, Applications for Python, and Quotes about Python. As\nusual, you may also be able to uncover other Python roles of interest with a web\nsearch in your local browser or app.\nWhat Can I Do with Python?\nCommercial applications may be compelling, but people also use Python for all\nsorts of real-world, day-in/day-out tasks, whether for profit, need, hobby, or fun.\nIn fact, as a general-purpose language, Python’s roles are virtually unlimited:\nyou can use it for everything from gaming and website development to robotics\nand content management.\nThat said, Python roles seem to fall into a few broad categories. The next few\nsections survey some of Python’s most common application domains today, as\nwell as prominent tools used in each domain.\nTwo notes up front: first, we won’t be able to explore these tools in any depth\neither here or in this book at large. NumPy and Django, for example, are book-\nlength topics on their own, and our goal in this book is to learn the Python code\nyou will be writing to use systems like these. Second, apologies in advance to\nthe many other tools omitted here only for space; if you are interested in any of\nthe following topics, please search online for more tools and resources.\nSystems Programming\nPython’s built-in interfaces to operating-system services make it ideal for writing\nportable and maintainable system-administration utilities—sometimes called\nshell or command-line tools, though they may be used in numerous ways. Python\nprograms can search files and directory trees, launch and configure other\nprograms, do parallel processing with processes, threads, and coroutines, and\nmore.\nPython’s standard library comes with Portable Operating System Interface\n(POSIX) bindings and support for all the usual OS tools, including environment\nvariables, files, sockets, pipes, processes, threads, regular-expression pattern\nmatching, command-line arguments, standard-stream interfaces, shell-command",
      "content_length": 2041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": 1,
      "content": "launchers, filename expansion, ZIP file utilities, and XML, JSON, and CSV\nparsers. In addition, the bulk of Python’s system interfaces are designed to be\nportable; for example, a script that copies directory trees typically runs\nunchanged on all platforms that host Python.\nGUIs and UIs\nPython’s simplicity and rapid turnaround also make it a good match for GUI\nprogramming on devices of all kinds. For instance, Python comes with a\nstandard object-oriented interface to the Tk GUI toolkit called Tkinter (and\ntkinter in code), which allows Python programs to implement portable GUIs\nwith a native look and feel. Python/Tkinter GUIs run unchanged on Windows,\nmacOS, and Linux, and on Android with the help of a freeware app today (see\nAppendix A).\nIn addition, third-party tools offer other routes to portable UIs—including both\ntraditional GUIs like Kivy, BeeWare’s Toga, PyQt, and wxPython; and web-\nbrowser based solutions like Django, Flask, and WebAssembly. If your app, like\nmost, must interact with users, Python has multiple ways to write once and run\neverywhere.\nInternet and Web Scripting\nPython comes with standard internet modules that allow programs to perform a\nwide variety of networking tasks, in both client and server modes. Scripts can\ncommunicate over sockets; extract form information sent to server-side CGI web\nscripts; transfer files by FTP; parse and generate XML and JSON documents;\ncompose and send, and receive and parse email; fetch web pages by URLs and\nparse their HTML; and more.\nIn addition, a large collection of third-party tools are available on the web for\ndoing internet programming in Python. For example, web-development\nframeworks, such as Django, Flask, TurboGears, and Zope, support construction\nof full-featured and production-quality websites with Python. Many of these\ninclude tools like object-relational mappers, server-side scripting and templating,\nand AJAX support, to provide complete and enterprise-level web development\nsolutions. The Pyjamas Python-to-JavaScript transpiler; the Beautiful Soup",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": 1,
      "content": "HTML content extractor; and the WebAssembly, Pyodide, py2wasm, and\nPyScript Python-in-the-browser enablers provide even more web possibilities.\nComponent Integration\nWe discussed the component integration role earlier. Python’s ability to be\nextended by and embedded in systems coded in C, C++, and Java makes it useful\nas a flexible glue language for scripting the behavior of other software\ncomponents. For instance, integrating a C library into Python allows Python to\ntest and launch the library’s tools, and embedding Python in a product enables\non-site customizations to be coded without having to recompile the entire\nproduct (or ship its source code at all).\nTools such as SWIG, Boost.Python, CFFI, and HPy automate much of the work\nneeded to link compiled components with Python scripts, and the Cython system\nallows coders to mix Python and C-like code to create compiled extensions.\nOther tools provide more ways to script components—including the pyjnius and\nChaquopy Python/Java bridges; pywin32’s support for Windows Component\nObject Model (COM); the REST, SOAP, and XML-RPC cross-network conduits;\nand the Jython Java and IronPython .NET implementations of Python. In short,\nmost software components are in scope to Python code.\nDatabase Access\nFor traditional database demands, there are Python interfaces to all commonly\nused relational database systems—Oracle, MySQL, PostgreSQL, Informix,\nODBC, SQLite, and more. The Python world has also defined a portable\ndatabase API for accessing SQL database systems from Python scripts, which\nlooks the same on a variety of underlying database implementations. For basic\nprogram-storage needs, Python comes with built-in support for its own pickle\nobjects, language-agnostic JSON documents, and the in-process SQLite\nembedded SQL database engine.\nAlso for Python scripts, PyYAML parses and emits YAML data; ZODB and\nDurus provide object-oriented databases; SQLObject and SQLAlchemy\nimplement object relational mappers (ORMs), which graft Python classes onto\nrelational tables; and PyMongo interfaces to MongoDB, a high-performance",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": 1,
      "content": "JSON-style document database. Python can also access cloud storage options in\nGoogle’s App Engine, Microsoft’s Azure, and Amazon’s AWS.\nRapid Prototyping\nTo Python programs, components written in Python and C look the same.\nBecause of this, it’s possible to prototype systems in Python initially, and then\nmove selected components to a fully compiled language such as C or C++ for\ndelivery. Because Python doesn’t require a complete rewrite once the prototype\nhas solidified, the parts of the system that don’t need the efficiency of a\ncompiled language can remain coded in Python for ease of maintenance and use.\nBut beware: given the many optimization routes you met earlier, your prototype\nmay very well be your deliverable.\nNumeric and Scientific Programming\nPython is also widely used in numeric programming—a domain that would not\ntraditionally have been considered to be in the scope of scripting languages but\nhas grown to become one of Python’s most compelling use cases. Prominent\nhere, the NumPy high-performance numeric-programming extension for Python\nmentioned earlier includes such advanced tools as an array object, interfaces to\nstandard mathematical libraries, and much more. By integrating Python with\nnumeric routines coded in a compiled language for speed, NumPy turns Python\ninto a sophisticated yet easy-to-use numeric-programming tool that can often\nreplace code written in traditional languages like Fortran or C++.\nAdditional numeric tools often use NumPy as a core component, and add\nsupport for visualization, parallel processing, statistical analysis, and more. For\nexample, SciPy provides libraries of scientific programming tools; pandas\nsupports data analysis; matplotlib adds visualization tools; Jupyter notebooks are\ngeared toward math workers’ needs; Numba and PyThran offer just-in-time (JIT)\nand ahead-of-time (AOT) compilers for Python numeric code, respectively; and\nthe Cython and PyPy systems noted earlier can optimize algorithmic code. For\nmore basic needs, Python itself comes with a statistics module; complex, fixed-\npoint, and rational math; and other tools you’ll learn about in this book.",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": 1,
      "content": "And More: AI, Games, Images, QA, Excel, Apps…\nPython is commonly applied in far more domains and with far more tools than\ncan possibly be covered here. For example, it’s also used in:\nArtificial intelligence (see PyTorch, TensorFlow, and Keras)\nGame programming (see pygame, Panda3D, and Kivy)\nImage and graphics processing (see Pillow, PyOpenGL, and OpenCV)\nQuality assurance and testing (see PyTest, unittest, and Selenium)\nExcel spreadsheets (see xlwings, PyXLL, and Excel)\nSmartphone apps (see Kivy, BeeWare, and Appendix A)\nMicrocontrollers and ports (see MicroPython and PySerial)\nAnd of course, much more\nFor links to resources in these and many other fields, try Wikipedia’s Python\nsoftware page; the PyPI website, which hosts extension packages installed by\nPython’s pip; and the normal web searches.\nThough application domains underscore Python’s practical utility, keep in mind\nthat many are largely just instances of Python’s component integration role in\naction again. Adding Python as a frontend to libraries of components written in a\ncompiled language like C makes Python useful for scripting in a wide variety of\nroles. As a general-purpose language that supports integration, Python is\nbroadly, if not universally, applicable.\nWhat Are Python’s Technical Strengths?\nNaturally, this is a developer’s question. If you don’t already have a\nprogramming background, the terminology in the next few sections may be a bit\nbaffling—don’t worry, we’ll explore all of these topics in more detail as we\nproceed through this book. For both current and future developers, though, here\nis a quick rundown of Python’s top technical features. Some have been touched\non earlier, but this section fills in more of the story.",
      "content_length": 1724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": 1,
      "content": "It’s Object-Oriented and Functional\nPython is an object-oriented language, from the ground up. As you’ll find in this\nbook, its class model supports advanced notions such as polymorphism, operator\noverloading, and multiple inheritance; yet, in the context of Python’s simple\nsyntax and typing, OOP is remarkably easy to apply. In fact, if you don’t\nunderstand these terms, you’ll find they are much easier to learn with Python\nthan with just about any other OOP language available.\nOf equal significance, OOP is an option in Python; you can go far without\nhaving to become an object guru all at once. Much like C++, Python supports\nboth procedural and object-oriented programming modes. Its object-oriented\ntools can be applied if and when constraints allow. This is especially useful in\ntactical development modes, which often preclude the design phases that best\nutilize OOP’s benefits.\nIn addition to its original procedural (statement-based) and object-oriented\n(class-based) paradigms, Python today has built-in support for functional\nprogramming—a set that by most measures includes generators,\ncomprehensions, closures, maps, decorators, anonymous-function lambdas, and\nfirst-class function objects. As you’ll also learn in this book, these can serve as\nboth complement and alternative to its OOP tools.\nIt’s Free and Open\nPython is completely free to use and distribute. As with other open source\nsoftware, such as Linux and Apache, you can fetch the entire Python system’s\nsource code for free on the internet. There are no restrictions on copying it,\nembedding it in your systems, or shipping it with your products.\nBut don’t get the wrong idea: “free” doesn’t mean “unsupported.” On the\ncontrary, Python users have access to numerous online resources that respond to\nqueries and issues quicker than most commercial software. Moreover, because\nPython comes with complete source code, it empowers developers in ways that\nclosed commercial software cannot. Although studying or changing a\nprogramming language’s implementation code isn’t everyone’s idea of fun, it’s\ncomforting to know that you can. You’re not dependent on the whims of a\ncommercial vendor, because the ultimate documentation—source code—is at",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": 1,
      "content": "your disposal as a last resort.\nIt’s Portable\nWe touched on portability earlier. The standard implementation of Python is\nwritten in portable ANSI C, and compiles and runs on virtually every major\nplatform currently in use. For example, Python programs run today on\neverything from smartphones to supercomputers. As a partial list, Python is\navailable on Windows and macOS PCs, Linux and Unix workstations and\nservers, Android and iOS smartphones and tablets, real-time systems like\nVxWorks, Cray supercomputers, IBM mainframes, and more. Moreover, this list\nexpands regularly; Android and iOS, for example, are gaining official python.org\nsupport at this writing.\nLike the language itself, the standard-library modules that ship with Python are\ndesigned to be portable across platform boundaries; their file tools, for instance,\nremove many or most of the proprietary aspects of storage on some hosts.\nFurthermore, Python programs are automatically compiled to portable bytecode,\nwhich runs the same on any platform with a compatible version of Python\ninstalled (more on this in the next chapter), and there are multiple ways to code\nportable user interfaces in Python with traditional GUIs and web-based options\ndescribed earlier.\nThis means that programs that use the Python language, its standard libraries,\nand portable extensions run the same on most systems that host a Python\ninterpreter. Python also supports platform-specific extensions (e.g., pywin32 on\nWindows, PyObjC on macOS, and pyjnius on Android), but Python itself works\nthe same everywhere.\nIt’s Powerful\nFrom a features perspective, Python is something of a hybrid. Its toolset places it\nbetween traditional scripting languages (such as Tcl, Scheme, and Perl) and\nsystems development languages (such as C, C++, and Java). Python provides all\nthe simplicity and ease of use of a scripting language, along with more advanced\nsoftware-engineering tools typically found in compiled languages. Unlike some\nscripting languages, this combination makes Python useful for large-scale",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": 1,
      "content": "development projects. As a preview, here are some of the main things you’ll find\nin Python’s toolbox:\nDynamic typing\nPython keeps track of the kinds of objects your program uses when it runs,\nand doesn’t require complicated type and size declarations in your code. In\nfact, as you’ll see in Chapter 6, there is no such thing as a type or variable\ndeclaration anywhere in Python (apart from recent “hinting,” which Python\nitself does not use). Because Python code does not constrain data types, it is\nalso usually automatically applicable to a whole range of objects.\nAutomatic memory management\nPython automatically allocates objects and reclaims (“garbage collects”)\nthem when they are no longer used. As you’ll learn, Python keeps track of\nobjects in use and the memory they hold, so you don’t have to.\nProgramming-in-the-large support\nFor building larger systems, Python includes tools such as modules, classes,\nand exceptions. These tools allow you to organize systems into components,\nuse OOP to reuse and customize code, and handle events and errors\ngracefully. Python’s functional programming tools, described earlier, meet\nsome of the same goals.\nBuilt-in object types\nPython provides commonly used data structures such as lists, dictionaries,\nand strings as intrinsic parts of the language. As you’ll see, its built-in\nobjects are both flexible and easy to use. They can grow and shrink on\ndemand, can be arbitrarily nested to represent complex information, and are\nimmune to common memory errors.",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": 1,
      "content": "Built-in tools\nTo process all those object types, Python comes with powerful and standard\noperations, including concatenation (joining collections), slicing (extracting\nsections), sorting, mapping, and more.\nLibrary utilities\nFor more specific tasks, Python also comes with a large collection of\nprecoded library tools that support everything from regular expression\npattern matching to network servers. Once you learn the language itself,\nPython’s library tools are where much of the application-level action occurs.\nThird-party utilities\nBecause Python is open source, developers are encouraged to contribute\nprecoded tools that support tasks beyond those supported by its built-ins. On\nthe Web, you’ll find free support for image processing, numeric\nprogramming, database access, website development, formal testing, and\nmuch more (see “What Can I Do with Python?”).\nDespite the array of tools in Python, it retains a noticeably simple syntax and\ndesign. The result is a powerful programming tool with all the usability of a\nscripting language.\nIt’s Mixable\nAs noted earlier, Python programs can easily be “glued” to components written\nin other languages in a variety of ways—both locally and across networks. That\nmeans you can add functionality to the Python system as needed and use Python\nprograms within other environments or systems.\nMixing Python into systems coded in more demanding languages, for instance,\nmakes it an easy-to-use frontend language for testing and customization. As also\nmentioned earlier, this makes Python good at rapid prototyping too—systems",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": 1,
      "content": "may be coded in Python first for development speed and later moved to\nextensions one piece at a time for execution speed if and when needed.\nIt’s Relatively Easy to Use\nCompared to alternatives like C++, Java, and C#, Python programming seems\nastonishingly simple to most observers. To run Python code in most contexts,\nyou simply type it and run it. There are no intermediate compile and link steps,\nlike those typical for languages such as C or C++. Python executes programs\nimmediately, which makes for an interactive programming experience and rapid\nturnaround after program changes—in many cases, you can witness the effect of\na program change nearly as fast as you can type it.\nOf course, development cycle turnaround is only one aspect of Python’s ease of\nuse. It also provides a deliberately simple syntax and powerful built-in tools. In\nfact, some have gone so far as to call Python executable pseudocode. Because it\neliminates much of the complexity of its contemporaries, Python programs are\nsimpler, smaller, and more flexible than equivalent programs in other popular\nlanguages.\nWhich is not to say that Python makes programming a no-brainer. Python also\nhas convolutions and dark corners that we’ll tackle head-on in this book, and\nsome of its roles are more rapid than others. Python’s flavor of OOP inheritance,\nfor example, is much more complicated than it once was, and building\nstandalone apps or precompiled programs of the sort you’ll meet in the next\nchapter can still be slow. Measured by its peers, though, Python is dramatically\nmore coder friendly.\nIt’s Relatively Easy to Learn\nFinally, this brings us to the point of this book: especially when compared to\nother widely used programming languages, the core Python language is\nremarkably easy to learn. In fact, if you’re already an experienced programmer,\nyou can expect to be coding small-scale Python programs in a matter of days—\nthough you shouldn’t expect to become an expert quite that fast, despite what\nyou may have heard from marketing departments!\nNaturally, mastering any topic as substantial as today’s Python is not trivial, and",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": 1,
      "content": "we’ll devote the rest of this book to this task. But the investment required to\nmaster Python is worthwhile: in the end, you’ll gain programming skills that\napply to nearly every computer application domain. Moreover, most find\nPython’s learning curve to be much gentler than that of other programming tools,\neven if that curve is not quite as flat as some content publishers claim.\nThat’s good news for both professional developers seeking to add the language\nto their toolbox, and end users of systems that expose a Python layer for\nscripting roles. Today, many systems rely on the fact that people can learn\nenough Python to use the system with little or no support. Moreover, Python has\nspawned a legion of users who program for need or fun instead of career and\nmay never require full-scale software development skills. Although Python has\nadvanced tools you’ll meet in this book, its core fundamentals are accessible to\nbeginners and gurus alike.\nTo see all this for yourself, let’s wrap up this overview and get started coding.",
      "content_length": 1034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": 1,
      "content": "Chapter Summary\nAnd that concludes the “hype” portion of this book. In this chapter, you’ve\nexplored some of the reasons that people pick Python for their programming\ntasks. You’ve also seen how it is applied and looked at a representative sample\nof notable users today. This book’s goal is to teach Python, though, not to sell it.\nThe best way to judge a language is to see it in action, so the rest of this book\nfocuses entirely on the language details glossed over here.\nThe next two chapters begin your technical introduction to the language. In\nthem, you’ll study ways to run Python programs, peek at Python’s execution\nmodel, and learn the basics of module files for saving code. The aim will be to\ngive you just enough information to run the examples and exercises in the rest of\nthe book. You won’t really start programming per se until Chapter 4, but make\nsure you have a handle on the startup details before moving on.\nTest Your Knowledge: Quiz\nIn this book, we will be closing each chapter with a quick open-book quiz about\nthe chapter’s coverage to help you review key concepts. The answers for these\nquizzes appear immediately after the questions, and you are encouraged to read\nthe answers once you’ve taken a crack at the questions yourself, as they\nsometimes give useful summary context.\nIn addition to these end-of-chapter quizzes, you’ll find lab exercises at the end of\neach part of the book, designed to help you start coding Python on your own,\nwith suggested answers available in an appendix. For now, here’s your first quiz.\nGood luck, and be sure to refer back to this chapter’s material as needed.\n1. What are the six main reasons that people choose to use Python?\n2. Name four notable companies or organizations using Python today.\n3. Why might you not want to use Python in an application?\n4. What can you do with Python?",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": 1,
      "content": "Test Your Knowledge: Answers\nHow did you do? Here are the suggested answers, though there may be multiple\nsolutions to some quiz questions. Again, even if you’re sure of your answers,\nyou’re encouraged to look at the suggestions for additional context. See the\nchapter’s coverage for more details if any of these responses don’t make sense to\nyou.\n1. Software quality, developer productivity, program portability, support\nlibraries, component integration, and simple enjoyment. Of these, the\nquality and productivity themes seem to be the main reasons that people\nchoose to use Python, but enjoyment counts, too, in a field that can be\nas challenging as software.\n2. Google, Industrial Light & Magic, JPL, ESRI, Instagram, and many\nmore. Almost every organization doing software development uses\nPython in some fashion, whether for long-term strategic product\ndevelopment or for short-term tactical tasks such as testing and system\nadministration.\n3. Python’s main downside is performance: in its currently most-common\nversion, at least, it won’t run as quickly as fully compiled languages like\nC and C++. On the other hand, it’s quick enough for most applications,\nand typical Python code runs at close to C speed anyhow because it\ninvokes linked-in C code in the interpreter. If speed is critical, compiled\nextensions and other tools like Cython are available to optimize the\nnumber-crunching parts of a Python application.\n4. You can use Python for nearly anything you can do with a computer,\nfrom website development and gaming to AI and spacecraft control.\nNumeric programming and web development may lead the pack today,\nthough probably because those are some of the main things for which\ncomputers are used.",
      "content_length": 1714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": 1,
      "content": "Chapter 2. How Python Runs\nPrograms\nThis chapter and the next cover program execution—how you launch code and\nhow Python runs it. In this chapter, we’ll begin by studying how the Python\ninterpreter executes programs in general, from an abstract vantage point. With\nthat perspective in hand, Chapter 3 will dive into the nuts and bolts of getting\nyour own programs up and running.\nStartup details are inherently platform specific, and some of the material in these\ntwo chapters may not apply to the ways you’ll be using Python, so you should\nfeel free to skip parts not relevant to your goals. Likewise, readers who have\nused similar tools in the past and prefer to get to the meat of the language\nquickly may want to file some of these chapters away for future reference. For\nthe rest of us, let’s take a brief look at the way that Python will run our code,\nbefore we get into the details of writing or running it.\nIntroducing the Python Interpreter\nSo far, we’ve mostly been talking about Python as a programming language. In\nits most widely used form, though, it’s also a software system called an\ninterpreter. An interpreter is a kind of program that executes other programs.\nWhen you write a Python program, the Python interpreter reads your program\nand carries out the instructions it contains. In effect, the interpreter is a layer of\nlogic between your code and the computer hardware on your machine.\nWhen Python is installed, it generates a number of components—minimally, an\ninterpreter and a support library. Depending on how you use it, the Python\ninterpreter may take the form of an executable program, or a set of libraries\nlinked into another program. Depending on which flavor of Python you run, the\ninterpreter itself may be implemented as a C program, a set of Java classes, or\nsomething else. Whatever form it takes, the Python code you write must always\nbe run by this interpreter. And to enable that, you usually must install a Python",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": 1,
      "content": "interpreter on your computer.\nYou don’t need to install Python at this point unless you want to work along with\nthe sole trivial example coming up, and this book won’t assume that you’ve got a\nPython ready to go until the next chapter. When you’re ready, Python installation\ndetails vary by platform and are discussed briefly in Chapter 3, and covered in\nfull in Appendix A.\nProgram Execution\nWhat it means to run a Python script depends on whether you look at this task as\na programmer, or as a Python interpreter. Both views offer important\nperspectives on Python programming.\nThe Programmer’s View\nIn its simplest but most common form, a Python program is just a text file\ncontaining Python statements. For example, the file listed in Example 2-1,\nnamed script0.py, may be one of the most trivial Python scripts one could dream\nup, but it passes for a fully functional Python program.\nExample 2-1. script0.py\nprint('hello world')\nprint(2 ** 100)\nThis file contains two Python print statements, which simply print a string (the\ntext in quotes) and a numeric expression result (2 to the power 100) to the output\nstream (the GUI or window where the file is run, normally). Don’t worry about\nthe syntax of this code yet—for now, we’re interested only in how it runs. This\nbook will explain the print statement, and why you can raise 2 to the power 100\nso easily in Python, in its later chapters.\nYou can create such a file of statements with any text editor you like; see\nAppendix A for suggestions. By convention, Python program files are given\nnames that end in .py; technically, this naming scheme is required only for files\nthat are “imported”—a term clarified in the next chapter—but most Python files\nhave .py names for consistency.\nAfter you’ve typed these statements into a text file, you must tell Python to",
      "content_length": 1815,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": 1,
      "content": "execute the file—which simply means to run all the statements in the file from\ntop to bottom, one after another. As you’ll see in the next chapter, you can\nlaunch Python program files by typing command lines, by clicking their icons,\nfrom within coding GUIs, and with other techniques. If all goes well, when you\nexecute the file, you’ll see the results of the two print statements show up\nsomewhere on your computer—usually and by default, in the same window you\nwere in when you ran the program:\nhello world\n1267650600228229401496703205376\nFor example, here’s what happened when this script was run in a Command\nPrompt window with a command line on a Windows laptop, to make sure it\ndidn’t have any silly typos:\nC:\\Users\\me\\code> py script0.py\nhello world\n1267650600228229401496703205376\nSee Chapter 3 for the full story on this process, especially if you’re new to\nprogramming; we’ll get into all the details of writing and launching programs\nthere. For our purposes here, we’ve just run a Python script that prints a string\nand a number. We probably won’t attract venture capital or go viral on GitHub\nwith this code, but it’s enough to capture the basics of program execution.\nPython’s View\nThe brief description in the prior section is fairly standard for scripting\nlanguages, and it’s usually all that most new Python programmers need to know.\nYou type code into text files, and you run those files through the interpreter.\nUnder the hood, though, a bit more happens when you tell Python to “go.”\nAlthough knowledge of Python internals is not strictly required for Python\nprogramming, having a basic understanding of Python’s runtime structure up\nfront can help you grasp how your code fits into the bigger picture of program\nexecution.\nWhen you instruct Python to run your script, there are a few steps that Python\ncarries out before your code actually starts crunching away. Specifically, it’s first",
      "content_length": 1908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": 1,
      "content": "compiled to something called “bytecode” and then routed to something called a\n“virtual machine.” This holds true only for the most common version of Python,\nand you’ll meet variations on this model in a moment. Since the most common\nis, well, most common, let’s see how this works first.\nBytecode compilation\nInternally, and almost completely hidden from you, when you execute a program\nPython first compiles your source code (the statements in your text file) into a\nformat known as bytecode. Compilation is simply a translation step, and\nbytecode is a lower-level, platform-independent representation of your source\ncode. Roughly, Python translates each of your source statements into a group of\nbytecode instructions by decomposing them into individual steps. This bytecode\ntranslation is performed to speed execution—bytecode can be run much more\nquickly than the original source code statements in your text file.\nYou’ll notice that the prior paragraph said that this is almost completely hidden\nfrom you. If the Python program has write access on your machine, it will save\nthe bytecode of your programs in files that end with a .pyc extension (.pyc means\n.py source, compiled). Technically, this save doesn’t happen when running a\nsingle file as we did in the preceding section but does for all but the topmost file\nin meatier multifile programs (more on this in a moment).\nPython saves its bytecode files in a subdirectory named __pycache__ located in\nthe directory where your source files reside, and in files whose names identify\nthe Python version that created them (e.g., script0.cpython-312.pyc for 3.12).\nThe __pycache__ subdirectory avoids clutter, and the naming convention for\nbytecode files prevents different Python versions installed on the same computer\nfrom overwriting each other’s saved bytecode. We’ll study this bytecode file\nmodel in more detail in Chapter 22, though it’s automatic and irrelevant to most\nPython programs and is free to vary among the alternative Python\nimplementations described ahead.\nSo why all the bother? In short, Python saves bytecode like this as a startup-\nspeed optimization. The next time you run your program, Python will load the\n.pyc files and skip the compilation steps, as long as the bytecode is present, you\nhaven’t changed your source code since the bytecode was last saved, and you\naren’t running with a different Python than the one that created the bytecode. It",
      "content_length": 2427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": 1,
      "content": "works like this:\nSource changes\nPython saves the last-modified timestamp and size (or hash value,\noptionally) of a source code file in its bytecode file, and compares this info\nto the source when the bytecode is loaded to know when it must recompile—\nif you edit and resave your source code, its bytecode is re-created the next\ntime your program is run.\nPython versions\nPython also adds a version-information suffix to bytecode filenames to know\nwhen it must recompile—if you run your program on a different Python\nimplementation or version, its bytecode is generated and saved for that\nPython too.\nThe result is that both source code changes and differing Python versions will\ntrigger a new bytecode file automatically. If Python cannot write the bytecode\nfiles to your machine, your program still works—the bytecode is generated in\nmemory and simply discarded on program exit.\nBecause .pyc files speed startup time, though, you’ll want to make sure they are\nwritten for larger programs. Bytecode files are also one way to ship Python\nprograms—Python is happy to run a program if all it can find are compatible\n.pyc files, even if the original .py source files are absent. This isn’t as simple as\ndeleting the .py files, though, and may require file moves and renames, or special\ntechniques discussed later in Chapter 22. See Python’s compileall module to\nforce compiles when needed for packaging, and frozen binaries (see “Standalone\nExecutables”) for another shipping option.\nStrictly speaking, bytecode is an import optimization. Bytecode is saved in .pyc\nfiles only for files that are imported, not for the top-level files of a program that\nare only run as scripts. Moreover, a given file is imported and possibly compiled\nonly once per program run; later imports use what’s already been loaded. We’ll\nexplore import basics in Chapter 3 and take a deeper look at imports in Part V.",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": 1,
      "content": "For now, keep in mind that bytecode is never saved for code typed at the\ninteractive prompt—a programming mode you’ll learn about in Chapter 3 and\nuse early in this book.\nThe Python Virtual Machine (PVM)\nOnce your program has been compiled to bytecode (or the bytecode has been\nloaded from existing .pyc files), it is shipped off for execution to something\ngenerally known as the Python Virtual Machine (PVM, for acronym-inclined\nreaders). The PVM sounds more impressive than it is; really, it’s not a separate\nprogram, and it need not be installed by itself. In fact, the PVM is just a big code\nloop that iterates through your bytecode instructions, one by one, to carry out\ntheir operations. That is, the PVM is the runtime engine in Python. It’s always\npresent as part of the Python system, is the component that truly runs your\nscripts, and is really just the last step of the “Python interpreter.”\nFigure 2-1 illustrates this runtime structure. Bear in mind that all of this\ncomplexity is deliberately hidden from Python programmers. Bytecode\ncompilation is automatic, and the PVM is just part of the Python system that you\nhave installed on your machine. Again, programmers simply code and run files\nof statements, and Python handles the logistics of running them behind the\nscenes.\nFigure 2-1. Python’s traditional execution model: the PVM runs compiled bytecode\nPerformance implications\nReaders with a background in fully compiled languages such as C and C++\nmight notice some glaring differences in the Python model. For one thing, there\nis usually no build or “make” step in Python work: code runs immediately after\nit is written. For another, Python bytecode is not binary machine code (e.g.,",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": 1,
      "content": "instructions for an Intel or ARM chip, known as a CPU): it’s a Python-specific\nformat. There are exceptions to these rules (e.g., app builds for smartphones can\ntake some time, and full compilers do exist, as you’ll see ahead), but we’re\nfocusing on the common here first.\nThese differences explain why some Python code may not run as fast as C or\nC++ code, as described in Chapter 1—the PVM loop, not the CPU chip, still\nmust interpret the bytecode, and bytecode instructions require more work than\nCPU instructions. On the other hand, unlike in classic interpreters, there is still\nan internal compile step—Python does not need to reanalyze and reparse each\nsource statement’s text repeatedly. The net effect is that pure Python code runs at\nspeeds somewhere between those of a traditional compiled language and a\ntraditional interpreted language.\nDevelopment implications\nAnother ramification of Python’s execution model is that there is really no\ndistinction between the development and execution environments: the systems\nthat compile and execute your source code are really one and the same. This\nsimilarity may have a bit more significance to readers with a background in\ntraditional compiled languages, but in Python, the compiler is always present at\nruntime and is part of the system that runs programs.\nThis makes for a much more rapid development cycle. There is no need to\nprecompile and link before execution can begin; simply type and run the code.\nThis also adds a much more dynamic flavor to the language—it is possible, and\noften very convenient, for Python programs to construct and execute other\nPython programs at runtime. The eval and exec built-ins, for instance, accept\nand run strings containing Python program code. This structure is also why\nPython lends itself to product customization—because Python code can be\nchanged on the fly, users can modify the Python parts of a system on-site\nwithout needing to compile or even possess the rest of the system’s code.\nAt a more fundamental level, keep in mind that all we really have in Python is\nruntime—there is no initial compile-time phase at all, and everything happens as\nthe program is running. This even includes operations such as the creation of\nfunctions and classes and the linkage of modules. Such events often occur before\nexecution in more static languages, but happen during execution in Python. As",
      "content_length": 2385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": 1,
      "content": "you’ll see, this makes for a much more dynamic programming experience than\nthat to which some readers may be accustomed.\nWHY DOES PYTHON USE BYTECODE?\nGiven that bytecode generally runs more slowly than machine code, this is a\ngreat question. The short answer is that Python uses bytecode for the sake of\ndevelopment speed and language flexibility.\nIn more detail, every program must ultimately run as machine code on the\nhost device’s CPU, but program code is just text written per a language’s\nrules. Traditional languages like C bridge this gap by constraining code to\naccommodate the CPU’s expectations and translating the code’s text to\nmachine code ahead of time. This makes programs fast, but translation takes\ntime, and the resulting languages are cumbersome to use.\nPython instead defines an easy-to-use language that’s too far removed from\nmachine code for a direct translation and uses the PVM intermediary to run\nyour program’s bytecode on the CPU. This is a classic speed-versus-usability\ntrade-off, but also a false dichotomy: many of the alternative\nimplementations you’ll meet ahead do compile some Python code to\nmachine code, and Python is quick enough for many roles even with its\nPVM model. For more on this trade-off, see “OK, but What’s the\nDownside?”.\nExecution-Model Variations\nNow that you’ve studied the internal execution flow described in the prior\nsection, you should also know that it reflects just the standard implementation of\nPython today and is not a requirement of the Python language itself. Because of\nthat, the execution model is prone to change with time. In fact, many systems\nalready modify the picture in Figure 2-1 in one way or another. Before moving\non, let’s briefly explore the most prominent of these variations.\nPython Implementation Alternatives",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": 2,
      "content": "As of this writing, there are multiple implementations of the Python language.\nAlthough there is much cross-fertilization of ideas and work between these\nPythons, each is a separately installed software system, with its own project and\nuser base. All but one of these systems are optional reading for most Python\nbeginners, but a quick look at the ways they modify the execution model might\nhelp demystify what it means to run code in general.\nThe short story is that CPython is the standard implementation—what we’ve\ncalled the “common” version so far. It’s the usual Python on PCs and\nsmartphones, and the system that most readers will be using (and if you’re not\nsure, this probably includes you). This is also the version used in this book,\nthough the Python language fundamentals presented here apply to all the\nalternatives too. All the other Python implementations have specific goals and\nmay or may not implement the full Python language defined by CPython, but\ncome close enough to qualify as Pythons.\nFor example, PyPy is a drop-in replacement for CPython that runs many\nprograms quicker by compiling parts of them further as they run. Jython and\nIronPython instead reimplement CPython to provide access to Java and .NET\ncomponents; although standard CPython programs can access such components\ntoo (e.g., via pyjnius and Chaquopy for Java), Jython and IronPython aim to be\nmore seamless. Other options accelerate numeric code, or code in general.\nIn more detail, the following list is a quick rundown on the most prominent\nPython implementations available today—with the usual apologies to current\noptions omitted for space, and future options omitted for lack of a crystal ball:\nCPython: the standard\nThe original, standard, and reference implementation of Python is usually\ncalled CPython when you want to contrast it with the other options (and just\nplain “Python” otherwise). This name comes from the fact that it is coded in\nportable ANSI C language code. This is the Python that you fetch from\npython.org for PCs, get with most alternative distributions and Linux repos,\nand have inside Python apps on Android and iOS smartphones. This is also\nthe flavor whose implementation is captured in Figure 2-1, though this it",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": 2,
      "content": "prone to change (and in fact may: see “Future Possibilities”).\nJython: Python for Java\nThe Jython system is an alternative implementation of the Python language.\nIt’s targeted at integration with the Java programming language, much as\nCPython integrates with C and C++ components. Jython consists of Java\nclasses that compile Python source code to Java bytecode, which is then\nrouted to the Java Virtual Machine (JVM). Programmers still code Python\nstatements in .py text files as usual; the Jython system essentially just\nreplaces the rightmost two bubbles in Figure 2-1 with Java-based equivalents\nfor seamless Java linkage. At this writing, Jython implements the older\nPython 2.X not used in this book but is working toward 3.X.\nIronPython: Python for .NET\nIronPython is another alternative implementation of Python. Coded in C#, it\nis designed to allow Python programs to integrate with applications written\nto work with Microsoft’s .NET Framework for Windows, as well as the\nMono open source equivalent. Like Jython, IronPython replaces the last two\nbubbles in Figure 2-1 with equivalents for execution in the .NET\nenvironment. With it, Python code can gain accessibility both to and from\nother .NET languages and leverage .NET libraries.\nStackless: Python for concurrency\nThe Stackless Python system is an enhanced version of the standard CPython\nlanguage oriented toward concurrency. Because it does not save state on the\nC language call stack, Stackless can make Python easier to port to small-\nstack architectures. Stackless also provides efficient multiprocessing options\nthat some find more straightforward than CPython’s later async coroutines,",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": 2,
      "content": "and fosters novel programming structures. As an example, the game EVE\nOnline uses Stackless Python to achieve high performance for massively\nparallel tasks.\nPyPy: a JIT compiler for speed\nPyPy is a reimplementation of Python for speed. It was one of the first\nsystems to employ a just-in-time (JIT) compiler for normal Python code. A\nJIT compiler is just an extension to the PVM—the rightmost bubble in\nFigure 2-1—that translates portions of your bytecode all the way to machine\ncode for faster execution. PyPy does this as your program is running, not in a\nprerun compile step, and is able to create type-specific machine code for the\ndynamic Python language by keeping track of the data types of the objects\nyour program processes. By replacing portions of your bytecode this way,\nyour program runs faster and faster as it is executing. In addition, some\nPython programs may also take up less memory under PyPy.\nNumba: a JIT compiler for numeric speed\nThe Numba extension for Python adds a JIT compiler that optimizes\nnumerically oriented code by compiling it all the way to machine code while\nyour program runs. To direct Numba’s compiler, functions are augmented\nwith Python “@” decorators supplied with the Numba install. While not all\nPython code can be sped up by Numba, it works well for code that uses\nNumPy arrays and functions, as well as math-oriented loops. Numba also\nsupports code parallelization paradigms commonly used in scientific\nprogramming.\nShed Skin: an AOT compiler for conforming code\nShed Skin is an ahead-of-time (AOT) compiler that translates unadorned\nPython code to C++ code, which is then compiled to machine code before it",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": 2,
      "content": "is run. With an AOT, the two rightmost bubbles in Figure 2-1 are replaced\nwith precompiled machine code. Shed Skin can yield both standalone\nprograms and extension modules for use in other programs. In exchange, it\nimplements a restricted subset of Python that requires Python variables to\nmeet an implicit statically typed constraint and does not support some\nPython features or libraries today. Nevertheless, Shed Skin may outperform\nboth CPython and JIT-based options for some conforming code.\nPyThran: an AOT compiler for numeric speed\nPyThran implements another AOT compiler for a subset of the Python\nlanguage, with a focus on scientific programming. Like Shed Skin, it\ntranslates Python code to C++, using static type declarations provided in\neither formatted comments or separate command files. The result of\ncompiling the generated C++ is a native module that can be imported and\nused by other Python code.\nCython: a Python/C hybrid for speed\nThe Cython system defines a Python/C hybrid language that combines\nPython code with the ability to call C functions and use C type declarations\nfor variables, parameters, and class attributes. Cython code can be AOT-\ncompiled to C code that uses the Python/C API, which may then be compiled\ncompletely to machine code. Though not compatible with standard Python,\nCython can be useful both for wrapping external C libraries and for\nimplementing performance-critical parts of a system as efficient C extensions\nfor use in CPython programs.\nMicroPython: a Python subset for constraints\nThe MicroPython system is an alternative Python with a focus on efficiency.\nIt implements a limited dialect of the CPython language and a small subset\nof its standard library, in order to optimize Python to run in constrained",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": 2,
      "content": "environments. Though originally targeted at microcontrollers, MicroPython\nis also compiled for the WebAssembly system you’ll meet in Chapter 3, to\nsupport Python programs in web browsers without the full heft of CPython.\nAnd (naturally) more\nFor other Python alternatives, see the list at Python’s wiki, as well as the\nresults of a web search. Among the others, Cinder is a performance-focused\nimplementation of CPython with a JIT compiler and more, created by Meta\nand used for Instagram; Pyston is a fork of CPython with a JIT compiler and\nother optimizations, started by Dropbox; and Nuitka is a free and paid\noptimizing AOT compiler that translates standard Python code to C, and is\nused by the emerging py2wasm to translate Python code to WebAssembly for\nuse in browsers.\nAll that being said, unless you have a specific need met only by one of the\nalternatives, you’ll probably want to use the standard CPython system. Because\nit is the reference implementation of the language, it’s always the most complete,\nand also tends to be more up-to-date and robust than others. Still, it’s unlikely\nthat CPython will ever subsume all the optimization projects afoot in the Python\nworld, so be sure to vet the alternatives when your goals gel.\nStandalone Executables\nSometimes when people ask for a “real” Python compiler, what they’re actually\nseeking is simply a way to generate standalone executables from their Python\nprograms. This is more a packaging and shipping topic than an execution-flow\nconcept—and of more interest to software developers than Python beginners—\nbut it’s a related idea.\nIn short, with the help of third-party tools that you can fetch off the web, it is\npossible to turn your Python programs into true self-contained executables,\nsometimes also called frozen binaries in the Python world. Whatever they’re\ncalled, these programs can be run without requiring a Python installation or",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": 2,
      "content": "shipping your source code files.\nStandalone executables bundle together the bytecode of your program files,\nalong with the PVM (interpreter) and any Python support files and libraries your\nprogram needs, into a single package. There are some variations on this theme,\nbut the end result can be a single executable (e.g., a .exe file on Windows) or app\n(e.g., a .app on macOS, and .apk or .aab on Android) that can easily be shipped\nto customers. In Figure 2-1, it is as though the two rightmost bubbles—bytecode\nand PVM—are merged into a single component: a standalone executable bundle.\nToday, a variety of systems are capable of generating standalone executables and\nvary in platforms and features. For example, py2exe builds standalones for\nWindows; PyInstaller and cx_freeze make them for Windows, macOS, and\nLinux; py2app creates them for macOS; and Buildozer and Briefcase generate\napps for Android and iOS.\nFrozen binaries are not the same as the output of a true compiler—they run\nbytecode through a virtual machine. Hence, apart from a possible startup\nimprovement, frozen binaries run at the same speed as the original source files.\nFrozen binaries are also not generally small (they contain a PVM), but by current\nstandards they are not unusually large either. Because Python is embedded in the\nfrozen bundle, though, it does not have to be installed on the receiving end to run\nyour program. Moreover, because your bytecode is embedded in the bundle, it\nisn’t as easily viewed.\nFor more details, see the alternative coverage of standalones in this book’s\nAppendix A, which includes tips on building them on multiple platforms from\nthe same code base.\nFuture Possibilities\nFinally, note that the runtime execution model sketched here is really an artifact\nof the current implementation of Python, not of the language itself. For instance,\nit’s possible that an AOT compiler for translating unrestricted and unadorned\nPython source code to machine code may appear during the shelf life of this\nbook (although the fact that one has not in over three decades makes this seem\nunlikely!), and JIT compilers seem to be cropping up everywhere.\nEither way, the bytecode model will likely be standard for some time to come.",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": 2,
      "content": "The portability and flexibility of bytecode are important features of many Python\nsystems. Moreover, adding type-constraint declarations to support static\ncompilation would break much of the flexibility, conciseness, simplicity, and\nspirit of Python coding we’re about to explore. Python’s type hinting, also\ncovered later, comes close, but is thankfully still unused by Python itself today.\nNOTE\nJIT futurism: While CPython currently follows the bytecode/PVM model in Figure 2-1, it may\naugment it in the future. Version 3.13, still under development as this is being written, will add\nan experimental JIT compiler. As in PyPy and others, this will translate some bytecode all the\nway to machine code as your program is running. In 3.13 it will have a negligible speed boost\nand will be disabled by default. Though prescience is perilous in publishing, the JIT compiler\nmay be enabled by default for CPython in the future if it ever yields a significant net gain for\nPython programs.",
      "content_length": 984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": 2,
      "content": "Chapter Summary\nThis chapter introduced the execution model of Python—how Python runs your\nprograms—and explored some common variations on that model designed for\nboth different roles and better performance. Although you don’t really need to\ncome to grips with Python internals to write Python scripts, a passing\nacquaintance with this chapter’s topics will help you truly understand how your\nprograms run once you start coding them, as you will in the next chapter. First\nthough, here’s the usual chapter quiz to review what you’ve learned so far.\nTest Your Knowledge: Quiz\n1. What is the Python interpreter?\n2. What is source code?\n3. What is bytecode?\n4. What is the PVM?\n5. What is machine code?\n6. Name two or more variations on Python’s standard execution model.\n7. How are CPython, Jython, and IronPython different?\n8. What are PyPy, Shed Skin, and Cython?\nTest Your Knowledge: Answers\n1. The Python interpreter is a program that runs the Python programs you\nwrite. It essentially intermediates between your Python instructions and\nthose available in a CPU’s machine code.\n2. Source code is the statements you write for your program. It consists of\ntext in text files whose names normally end with a .py extension.",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": 2,
      "content": "3. Bytecode is the lower-level form of your program after Python compiles\nit. Python automatically stores bytecode in files with a .pyc extension\nwhen possible, and automatically re-creates it when needed.\n4. The PVM is the Python Virtual Machine—the runtime engine of Python\nthat interprets your compiled bytecode.\n5. Machine code is the low-level instructions of the underlying CPU on a\ncomputing device like a PC or phone. Because this is what every\nprogram ultimately runs, Python code must be translated to this by a\nsoftware layer like the PVM interpreter, or JIT or AOT compilers.\n6. Numba, Shed Skin, and standalone executables are all variations on the\nexecution model. In addition, the alternative implementations of Python\nnamed in the next two answers modify the model in some fashion as\nwell—by replacing bytecode and VMs, or by adding JIT and AOT\ncompilers.\n7. CPython is the standard and reference implementation of the language.\nJython and IronPython process Python programs for use in Java and\n.NET environments, respectively; they are alternative compilers for\nPython.\n8. PyPy and Shed Skin are reimplementations of Python targeted at speed.\nPyPy speeds normal Python programs by using runtime type\ninformation and a JIT compiler to replace some Python bytecode with\nmachine code as the program runs. Shed Skin speeds programs with an\nAOT compiler that translates a restricted subset of Python to C++, from\nwhich it can be fully compiled to machine code to be run as a program\nor used in others. Cython is a Python/C combination that can be\ncompiled into machine-code extensions accessible to CPython code.",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": 2,
      "content": "Chapter 3. How You Run\nPrograms\nOK, it’s time to start running some code. Now that you have a handle on\nPython’s purpose and execution model, you’re finally ready to start some real\nPython programming.\nThere are multiple ways to tell Python to execute the code you type, and this\nchapter covers all the major program launching techniques in common use today.\nAlong the way, you’ll learn both how to run code interactively, and how to save\nit in files to be run using a variety of techniques—with command lines, icon\nclicks, the IDLE GUI, mobile apps, web-based interfaces, module imports, and\nmore.\nAs for the previous chapter, if you have prior programming experience and are\nanxious to start digging into Python itself, you may want to skim this chapter\nand move on to Chapter 4. But don’t skip this chapter’s early coverage of\npreliminaries and conventions, its overview of debugging techniques, or its first\nlook at module imports—a topic essential to understanding Python’s program\narchitecture, which we won’t revisit until Part V. It’s also worthwhile to browse\nthe sections on IDEs and apps, to sample tools that may be more useful once you\nstart coding larger programs.\nInstalling Python\nThis book will generally assume that you have access to a recent version of\nPython on your computer, tablet, or phone. Python installation is not required for\nthis book, and isn’t necessary in some contexts, but you’ll need a Python to work\nalong with examples and do end-of-part exercises, and both are great ways to\nmake concepts more concrete.\nIf you don’t have a Python and wish to set one up, see Appendix A now for per-\nplatform install help. In short:",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": 2,
      "content": "Windows and macOS users fetch and run a self-installing executable file\nthat puts Python on their devices. Simply double-click and say Yes or\nNext at all prompts.\nLinux (including Windows WSL) users may have a usable Python\npreinstalled on their computers, but can install one if needed or desired\nfrom their distribution’s repositories.\nAndroid and iOS users install an app that allows them to run Python\nlocally on their phones and tablets.\nUnix (and some Linux) users often compile Python from its full source\ncode distribution package.\nFor example, smartphone users fetch a Python app at an app store, and Windows\nand macOS PC users get an installer at the downloads page of Python’s main\nwebsite. Python may also be had through other distribution channels, and some\nPython coding modes covered ahead, such as Jupyter notebooks, have unique\ninstall and usage steps. You may not be able to select a Python version in some\ncontexts, but this book uses Python 3.12, so closer to that is better.\nBefore you install, you should generally check to see if Python is already\npresent. Look for it on Windows in the Start menu, and on most platforms by\nrunning a Python command line as described both in Appendix A and this\nchapter’s next section. To see how, let’s take installs as a given, and move on to\nlearning the many ways to run Python code.\nInteractive Code\nThis section gets us started with the basics of interactive Python coding. Because\nit’s our first look at running code, we also cover some startup logistics here, such\nas setting up a working directory, so be sure to read this section first if you’re\nrelatively new to programming. This section also explains some conventions\nused throughout the book, so most readers should probably take at least a quick\nlook here.\nStarting an Interactive REPL",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": 2,
      "content": "By most measures, the simplest way to run Python code is to type it at Python’s\ninteractive command line, sometimes called the interactive prompt, and often\nmore concisely labeled the REPL (which stands for read–eval–print loop).\nThere are a variety of ways to start this command line—in a coding GUI or app,\nin a web notebook, from a system console, and so on. Assuming Python is\ninstalled locally as an executable program on your device, the most platform-\nneutral way to start an interactive interpreter session is to simply type a Python\ncommand at your device’s console prompt, without any arguments. Since this is\nmost common, nearly universal, and arguably simplest, let’s start here.\nDespite this scheme’s generality, both the Python command you’ll type and\nwhere you’ll type it vary per platform. On Windows, macOS, and Android, for\nexample, py, python3, and python, respectively, do the job as follows:\n$ py\nPython 3.12.3 (…etc…, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> ^Z\n$ python3\nPython 3.12.2 (…etc…, Feb  6 2024, 17:02:06) [Clang 13.0.0…)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> ^D\n$ python\nPython 3.11.4 (…etc…, Jul  2 2023, 11:17:00) [Clang 14.0.7…)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> ^D\nA python3 works on Android and Linux, too, if you want to narrow this to one\nexception—for Windows’ py. This book uses python3 in platform-neutral\nexamples just because it works almost everywhere. If you’re working on\nWindows, instead use py (or py -3 to ensure Python 3.X); python3 works on\nWindows, too, but is reserved for a lesser Store install today per Appendix A.\nAlso per that appendix, all these commands must be on your system search path\n(generally known as PATH) and require a full pathname if they’re not, but most\ninstalls set this up automatically.\nTyping a Python command at your system prompt like this begins an interactive\nPython session (i.e., REPL). The $ character at the start of listings here stands",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": 2,
      "content": "for a generic system prompt on all platforms throughout this book—it will likely\nvary on your device and is not input that you type yourself. On Windows, a\nCtrl+Z key combo (followed by Enter) ends this session as shown; on Unix\n(which includes macOS, Linux, and Android), it’s Ctrl+D instead.\nYou’ll notice that the interactive prompt opens with a message that identifies the\nPython being used (e.g., “3.12.3” is Python 3.12) and the platform it’s running\non (e.g., “win32” for Windows, “darwin” for macOS, and “linux” for both Linux\nand Android in Python 3.12), followed by a line with tips for more info. We’re\ngoing to omit these opening lines in this book, except where they’re helpful.\nThe notion of a system prompt (a.k.a. shell or console) where you type the\nPython command is generic, but exactly how you access it varies by platform. It\nmight be Command Prompt or PowerShell on Windows; Terminal on macOS\nand Linux; the Termux app on Android; or a dedicated terminal screen in some\napps like Pydroid 3.\nOn most platforms, you can also start an interactive session in ways that don’t\nrequire typing a command at all, but they vary even more widely. On Windows,\nfor example, a Start-menu option opens a similar REPL, as do the IDLE GUI on\nall PCs, dedicated screens in apps on Android and iOS, and some web-browser\ninterfaces. We’ll cover some of these options ahead, but see Appendix A for\nmore platform tips, and the web for more help with other options.\nAnywhere you see the >>> prompt, though, you’re in an interactive Python\nsession and REPL—you can type any Python statement or expression here and\nrun it immediately by simply pressing the Enter key (or similar button). We will\ndo so in a moment, but first we need to get a few admin details sorted out to\nmake sure all readers are set to go.\nWhere to Run: Code Folders\nNow that you’re starting to learn how to run code, you’ll also need to know\nwhere to run code. You can save and run code anywhere you can make files, but\nthis book has two recommendations for using its examples, especially for\nnewcomers:\nWork in a dedicated code folder",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": 2,
      "content": "To avoid stomping on other content, run this book’s examples from a\ndedicated code folder (a.k.a. directory) on your device. For instance, this\nbook runs all its code in a folder nested in the user account (or “home”)\nfolder on each device. Your code folder can be located wherever you like and\ncalled whatever you wish, but running out of one folder will help you keep\ntrack of your work and simplify some tasks.\nWork in per-chapter subfolders of your code folder\nTo avoid clutter and filename collisions, also organize your code into per-\nchapter subfolders, nested in your dedicated code folder. Per-chapter\nsubfolders will ensure that imports in examples will work without advanced\nsettings (that you’ll learn later). There’s more on imports and directories in\nan upcoming note. For now, keep in mind that console commands in this\nbook will implicitly be run in a per-chapter subfolder of a dedicated code\nfolder on the host. The folder name will normally be omitted for platform\nneutrality and is irrelevant to the code.\nIf you’ll be using this book’s examples package, it’s already done the setup work\nfor you. For example, unzipping the examples creates a code folder named\nLP6E, whose Chapter03 subfolder has this chapter’s code. If you’ll be using this\npackage to avoid typing code or copying from emedia, simply run its examples\nin the subfolder of the chapter you’re studying. This is where REPLs will be\nstarted and where script files will be run. As you’ll learn later, this is also where\ndata files that our scripts create will show up, unless scripts use filenames with\nexplicit folder paths.\nIf you’ll be creating code on your own, you should probably create a similar\ncode directory structure of your own before we move on. On PCs, use your\nsystem’s file explorer, or run a command line: mkdir folder works on\nWindows and Unix. On Android, you can also use a file explorer (or mkdir in\nTermux), but be sure to pick a folder accessible to your Python app. Some",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": 2,
      "content": "coding interfaces may offer other ways to create folders; see your tool for info.\nOnce your per-chapter code folders are set to go, always start there to write,\nsave, and run the examples in this book. How you’ll do this depends on your\nusage mode. In consoles, a portable cd folder command changes directories. In\na GUI like IDLE, opening and running a file may go to its folder. And in other\ninterfaces, you might launch a REPL for a file in the UI or use other schemes too\nvaried to cover here. As a fallback, Python’s os.getcwd() you’ll meet shortly\nshows the current directory, and its os.chdir('path') changes it—as long as\nyou import os first.\nWhat Not to Type: Prompts and Comments\nSpeaking of commands, remember not to type the $ character used at the start of\nthis book’s command lines to denote a system console prompt; type just the text\nafter these prompts. This may sound simple to experienced programmers, but it’s\na very common first error for beginners, and we’re not excluding anyone here.\nSimilarly, do not type the >>> and ... prompt characters shown at the start of\nlines in interpreter interaction listings and used by this book to denote code run\nin a REPL; type just the text after these prompts. These are prompts that\nPython’s standard REPL displays automatically as visual guides for interactive\ncode entry and may or may not appear in your interface. For instance, the ...\nprompt is used for continuation lines in some REPLs, but is just a label in IDLE,\nand is omitted by some of this book’s listings for easier copy/paste; either way,\ndon’t type it yourself.\nTo help you remember this, user inputs you must type are shown in bold in this\nbook, and prompts are not. Also keep in mind that commands typed after these\nsystem and Python prompts are meant to be run immediately and are not\ngenerally intended to be saved in the source files we will be creating; you’ll see\nwhy this distinction matters ahead.\nIn the same vein, you normally don’t need to type text that starts with a #\ncharacter in this book’s code listings—as you’ll learn later, these are comments,\nnot executable code. Except when # is used to introduce a #! directive at the top\nof a script, you can safely omit it and the rest of the line that follows it (there’s",
      "content_length": 2259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": 2,
      "content": "more about #! in Appendix A).\nOther Python REPLs\nHaving said all that, you should also know that Python REPLs can also be had in\nsystems that convolute the traditional model covered in this chapter. IPython, for\nexample, provides an alternative, separately installed, and enhanced Python\ninteractive session, which labels commands by number and doesn’t use >>>\nprompts. Jupyter notebooks provide the IPython REPL, too, and run it in a web\nbrowser instead of system console (we’ll explore Jupyter later in this chapter).\nTo muddy this story further, the PyPy system of Chapter 2 uses >>>> for its\nREPL prompts to distinguish it from CPython’s ternary >>> (though you may\nhave to look hard to tell); the IDLE GUI covered ahead displays >>> off to the\nside (and not in a system console); smartphone apps may vary too (see\nAppendix A); and the interactive prompt can technically be changed to anything\n(it’s sys.ps1 to your code, and can be set in a startup file, per Python docs).\nAll of which means that your Python interactive session may differ from the\nmainstream model that this chapter often employs. Depending on your tools and\ngoals, you might see a different prompt or none at all, and you might type\ninteractive Python code into a web browser, GUI, or app instead of the system\nconsole.\nIn general, though, this book recommends the traditional and simpler options it\ndemos and covers, when you’re first starting out. IPython and Jupyter, for\nexample, have learning curves of their own, and Jupyter is geared toward\nscientific work, which is just one of many Python roles. If and when you opt to\nuse alternative coding interfaces like these, and others sure to arise in the future,\nextrapolate to their REPLs’ minor differences as needed.\nNOTE\nREPL futurism: The standard CPython is scheduled to improve its interactive REPL interface\nin version 3.13, not yet released as this book was written. Per plans, the REPL will gain color\nprompts, automatic indentation, multiline editing and paste, and colorized exceptions borrowed\nfrom other UIs and IDEs. Colors won’t matter in this book’s print version, of course, and the\nfuture is not yet written, but a more colorful future may have arrived by the time you read this\nnote.",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": 2,
      "content": "Tip: Setting environment variable PYTHON_COLORS to 0 disables the new REPL colors if you\nfind that future distracting.\nRunning Code Interactively\nWith those preliminaries out of the way, let’s move on to typing some actual\ncode. However it’s started, the Python interactive session begins by printing\nsome informational text (again, mostly omitted hereafter), then prompts for input\nwith >>> (or similar) when it’s waiting for you to type a new Python statement or\nexpression.\nWhen working interactively, the results of your code are displayed below the >>>\ninput lines after you press the Enter key (or similar). For instance, here are the\nresults of two Python print statements:\n$ python3\n>>> print('Hello world!')\nHello world!\n>>> print(2 ** 8)\n256\nThere it is—we’ve just run some Python code (it’s not much, but it proves the\npoint). We’re still skipping most code details for now, but in brief, print is a\nbuilt-in tool that sends a line displaying whatever you pass to it, to wherever\nyou’re working; because it’s a function call in Python, the parentheses in this\ncode are required. We’ve used it here to display a Python string and an integer,\nas shown by the output lines that appear after each >>> input line.\nWhen coding interactively like this, you can type as many Python commands as\nyou like; each is run immediately after it’s entered. Moreover, because the\ninteractive session automatically prints the results of expressions you type, you\ndon’t usually need to say “print” explicitly at this prompt; the format of the\nfollowing automatic displays can differ slightly from print, but it’s not yet\nimportant to know how:\n>>> language = 'Python'\n>>> language\n'Python'\n>>> 2 ** 8",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": 2,
      "content": "256\n>>> ^Z                     # Use Ctrl-D (on Unix) or Ctrl-Z (on Windows) to exit\n$\nHere, the first line saves a value by assigning it to a variable (language), which\nis created by the = assignment; and the last two lines typed are expressions\n(language and 2 ** 8), whose results are displayed automatically. Again, to\nexit an interactive session like this and return to your system prompt, type\nCtrl+D on Unix-like machines, and Ctrl+Z on Windows. In the IDLE GUI\ndiscussed later, either type Ctrl+D everywhere, or simply close the window.\nNotice the italicized note about this on the right side of this listing—starting with\n# here. This book uses these throughout to add remarks about what is being\nillustrated, but you don’t need to type this text yourself; if you do, they’re\nignored by Python as comments. In fact, much like system $ and Python >>>\nprompts, you shouldn’t type this when it’s on a system command line; the #…\npart is ignored by Python but may be an error in system shells.\nNow, we didn’t do much in this session’s code—just typed some Python print\nand assignment statements, along with a few expressions, all of which we’ll\nstudy in detail later. The main thing to notice is that the Python REPL executes\nthe code entered on each line immediately, when the Enter key (or similar) is\npressed.\nFor example, when we typed the first print statement at the >>> prompt, the\noutput (a Python string) was echoed back right away. There was no need to\ncreate a source code file, and no need to run the code through a compiler and\nlinker first, as you’d normally do when using a language such as C or C++.\nStrictly speaking, interactive code is compiled to bytecode in memory and run\nby the PVM in CPython (see Chapter 2), but you don’t need to care.\nAs you’ll see in later chapters, you can also run multiline statements (e.g., for\nloops) at the interactive prompt; such a statement may prompt for continuation\nlines with ... as noted earlier and runs immediately after you’ve entered all of\nits lines and pressed Enter twice to add a blank line. Blank lines aren’t required\n(and are ignored) in code files, but are needed to let some REPLs know your\nstatements are complete. Also, bear in mind that the current and standard REPL\nruns just one statement at a time—don’t paste large code blocks at its prompt!",
      "content_length": 2324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": 2,
      "content": "Why the Interactive Prompt?\nThe interactive prompt runs code and echoes results as you go, but it doesn’t\nsave your code in a file. Although this means you won’t do the bulk of your real-\nworld coding in interactive sessions, the interactive prompt turns out to be a\ngreat place to both learn the language and test program files on the fly. Here’s a\nquick rundown of these roles.\nLearning\nBecause it executes code immediately, the interactive prompt is ideal for\nexperimenting with code and learning the language, and we’ll be using it\nthroughout this book to demonstrate smaller examples and amplify concepts. In\nfact, this is the first rule of thumb to remember: if you’re ever in doubt about\nhow a piece of Python code works, fire up the interactive command line and try\nthe code out to see what happens.\nFor instance, suppose you’re reading a Python program’s code and you come\nacross an expression like 'Hack!' * 8 whose meaning you don’t understand. At\nthis point, you can spend 10 minutes wading through manuals, books, and the\nweb to try to figure out what the code does, or you can simply run it\ninteractively:\n$ python3\n>>> 'Hack!' * 8                                  # Learning by trying\n'Hack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!'\nThis immediate feedback you receive at the interactive prompt is often the\nquickest way to deduce what a piece of code does. Here, it’s clear that it does\nstring repetition: in Python, * means multiply for numbers, but repeat for strings\n—it’s like concatenating a string to itself repeatedly (there’s more on strings in\nChapter 4).\nChances are good that you won’t break anything by experimenting this way—at\nleast, not yet. To do real damage, like deleting files and running shell commands,\nyou must really try by importing modules explicitly (we’ll sample tools that can\nmake you that dangerous later in this chapter). Straight Python code, though, is\nalmost always safe to run.",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": 2,
      "content": "For instance, watch what happens when you make a mistake at the interactive\nprompt:\n>>> X                                            # Making mistakes is OK\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'X' is not defined\nIn Python, using a variable before it has been assigned a value is always an error\n—otherwise, if names were filled in with defaults, some errors might go\nundetected until it’s too late. This means you must initialize counters to zero\nbefore you can add to them, must initialize lists to empty before extending them,\nand so on. You don’t need to declare variables in Python, but they must be\nassigned before you can fetch their values.\nOther error messages try to be more helpful in Python today with “Did you…?”\ntips—as when looking for the sys.ps1 prompt hook mentioned earlier. You\nmust import modules like sys before using them (though you probably won’t\nneed the reminder fairly soon, and we’ll be truncating some error messages in\nthis book for brevity):\n>>> sys.ps1                                      # Requires \"import sys\"\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sys' is not defined. Did you forget to import 'sys'?\nYou’ll learn more about all that later. The important point here is that you won’t\ncrash Python or your computer when you make a mistake this way. Instead, you\nget a meaningful error message pointing out the mistake and the line of code that\nmade it, and you can continue on in your session or script. In fact, once you get\ncomfortable with Python, its error messages may often provide as much\ndebugging support as you’ll need (watch for more about debugging options in\nthe sidebar “Debugging Python Code”).\nTesting\nBesides serving as a tool for experimenting while you’re learning the language,\nthe interactive interpreter is also an ideal place to test code you’ve written in\nfiles. You can import your module files interactively and run tests on the tools",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": 2,
      "content": "they define by typing calls at the interactive prompt on the fly.\nFor instance, the following tests a function in a precoded module that ships with\nPython in its standard library (detail: os.getcwd prints the name of the directory\nyou’re currently working in, here on a macOS host), but you can do the same\nonce you start writing module files of your own:\n>>> import os\n>>> os.getcwd()                                  # Testing on the fly\n'/Users/me/code'\nMore generally, the interactive prompt is a place to test program components,\nregardless of their source—you can import and test functions and classes in your\nPython files, type calls to linked-in C functions, exercise Java classes under\nJython, and more. Partly because of its interactive nature, Python supports an\nexperimental and exploratory programming style you’ll find convenient.\nAlthough Python programmers also test with in-file code (and you’ll learn ways\nto make this simple later in the book), for many, the interactive prompt is still\ntheir first line of testing defense.\nProgram Files\nAlthough the interactive prompt is great for experimenting and testing, it has one\nbig disadvantage: programs you type there go away as soon as the Python\ninterpreter executes them. Because the code you type interactively is never\nstored in a file, you can’t run it again without retyping it from scratch. Cut-and-\npaste and command recall can help some here, but not much, especially when\nyou start writing larger programs. To cut and paste code from an interactive\nsession, you would have to edit out Python prompts, program outputs, and so on\n—which is too tedious to try.\nTo save programs permanently, you need to write your code in files, which are\nusually known as modules. Modules are simply text files containing Python\nstatements. Once they are coded, you can ask the Python interpreter to execute\nthe statements in such a file any number of times, and in a variety of ways—by\nsystem command lines, by file icon clicks, by options in the IDLE user interface,\nand more. Regardless of how it is run, Python executes all the code in a module",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": 2,
      "content": "file from top to bottom each time you run the file.\nTerminology in this domain can vary by role. For instance, module files are often\nreferred to as programs in Python—that is, a program is considered to be a series\nof precoded statements stored in a file for repeated execution. Module files that\nare run directly are also sometimes called scripts—an informal term usually\nmeaning a top-level program file. Some reserve the term module for a file\nimported from another file, and script for the main file of a program; we\ngenerally will here too (stay tuned for more on the meaning of top-level,\nimports, and main files later in this chapter).\nWhatever you call them, the next few sections explore ways to run code typed\ninto files. In this section, you’ll learn how to run files in the most basic and\nportable way: by listing their names in a Python command line entered at your\ncomputer’s system prompt. Though this might seem primitive to some—and can\noften be avoided altogether by using alternatives discussed later—for many\nprogrammers, a system console window for command lines, together with a text\neditor window, constitutes as much of an integrated development environment as\nthey will ever need and provides more direct control over programs.\nA First Script\nLet’s get started. Open your favorite text editor, type or copy/paste the\nstatements in Example 3-1 into a new text file named script1.py, and save it in\nyour working code directory that you set up earlier (make it now if you skipped\nover that step). Any editor will work, including vi, Notepad, a smartphone app’s\neditor, and the IDLE GUI coming up soon. You can also find this file in the book\nexamples package, but typing code is an important exercise early on.\nExample 3-1. script1.py\n# A first Python script\nimport sys                  # Load a library module\nprint(sys.platform)\nprint(2 ** 100)             # Raise 2 to a power\nx = 'Hack!'\nprint(x * 8)                # String repetition\nThis file is our first official Python script (not counting the two-liner in\nChapter 2). You shouldn’t worry too much about this file’s contents just yet, but\nas a brief description, its code:",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": 2,
      "content": "Imports a Python module (libraries of additional tools) to fetch the\nname of the platform\nRuns three print function calls to display the script’s results\nUses a variable named x, created when it’s assigned, to hold on to a\nstring object\nApplies various object operations that we’ll begin exploring in the next\nchapter\nThe sys.platform here is just a string that identifies the kind of computer\nyou’re working on; it lives in a Python module called sys (part of its standard\nlibrary), which you must import to load (again, more on imports later). Also\nnotice how this file uses explicit print calls; unlike the REPL, output in files is\nnever automatic, so you must say print in files to see their output (and\nforgetting this is a regular first mistake, but at least you’ve been warned!).\nFor color, this file adds some Python comments—the text after the # characters.\nThese were mentioned earlier, but should be more formal now that they’re\nshowing up in scripts. Comments can show up on lines by themselves, or to the\nright of code on a line. The text after a # is simply ignored as a human-readable\nnote and is not considered part of the statement’s syntax. If you’re copying this\ncode, you can ignore the comments; they are just informative. This book uses a\ndifferent formatting style to make comments more visually distinctive, but\nthey’ll be normal text in your code.\nAgain, don’t focus on the syntax of the code in this file for now; you’ll learn\nabout all of it later. The main point to notice is that you’ve typed this code into a\nfile, rather than at the interactive prompt. In the process, you’ve coded a fully\nfunctional Python script.\nNotice, though, that the module file is named script1.py. As for all top-level files\n(i.e., files run directly), it could also be named simply script1, but files of code\nyou want to import in another file or REPL have to end with a .py suffix.\nBecause you may want to import them in the future, it’s a good idea to use .py\nsuffixes for most Python files that you code. Also, some text editors and file\nexplorers detect Python files by their .py suffix; if the suffix is not present, you",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": 2,
      "content": "may not get features like syntax colorization and automatic indentation in editors\nor tap-to-run in explorers.\nRunning Files with Command Lines\nOnce you’ve saved the preceding section’s text file, you can ask Python to run it\nby listing its full filename as the first argument to a Python command—like the\nfollowing typed at the system shell’s $ prompt on a Unix device (but don’t type\nthis at a Python REPL prompt, and read on to the next paragraph if this doesn’t\nwork right away for you):\n$ python3 script1.py\ndarwin\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\nJust as for starting the REPL we studied earlier, the command name may vary\n(e.g., use py instead of python3 on Windows, usually), and you can type such a\ncommand in whatever your system provides for command-line entry\n—Command Prompt on Windows, Terminal on macOS, or the Termux app on\nAndroid, among others. You might also run this code file with a dedicated run\nbutton in a GUI, app, or browser-based UI, but we’ll postpone such options until\nlater in this chapter (see also Appendix A for all things platform specific).\nBe sure to run your Python command in the same working directory where\nyou’ve saved your script file (cd there first if needed), and run it at the system\nprompt, not Python’s >>> prompt. Also like starting the REPL, you may have to\nreplace the command’s first word with a full directory path if Python isn’t on\nyour PATH setting, but most installs automatically ensure that it is (see\nAppendix A for more on Python installs and PATH).\nIf all works as planned, this shell command (or similar) makes Python run the\ncode in this file line by line, and you will see the output of the script’s three\nprint statements—the name of the underlying platform as known to Python, 2\nraised to the power 100, and the result of the same string repetition expression\nwe saw earlier (there’s more on the meaning of the last two of these in Part II).\nIf all didn’t work as planned, you’ll get an error message—make sure you’ve",
      "content_length": 2025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": 2,
      "content": "entered the code in your file exactly as shown, and try again. We’ll talk about\ndebugging options in the sidebar “Debugging Python Code”, but at this point in\nthe book your best bet is probably rote imitation. And if all else fails, you might\nalso try running under the IDLE GUI discussed ahead—a tool that sugarcoats\nsome launching details, though sometimes at the expense of the more explicit\ncontrol you have when using command lines.\nIf copying code grows too tedious or error-prone, you can also fetch this book’s\nexamples on the web, though again, typing code initially will help you learn to\navoid syntax errors. See the Preface for info on obtaining the examples.\nCommand-Line Usage Variations\nWhen you type a command to run a Python code file, the command you type is\nrun by a system shell program (e.g., Bash on Unix). Because of this, all of the\nshell’s syntax is available for more custom runs. For instance, you can route the\nprinted output of a Python script to a file to save it for later use or inspection, by\nusing special shell syntax:\n$ python3 script1.py > saveit.txt\nIn this case, the three output lines shown in the prior run are stored in the file\nsaveit.txt instead of being printed. This is generally known as stream redirection;\nit works for both output (>) and input (<) text and is available on Windows and\nUnix-like systems. This is useful for testing, as you can write programs that\nwatch for changes in other programs’ outputs. It also has little to do with Python,\nthough (Python simply supports it), so we will skip further details on shell\nredirection syntax here. Redirection is for command lines only, though, because\nit’s a function of the system shell.\nOn Windows, you can also type just the name of your script and omit the name\nof Python itself. Because Windows uses filename associations to find a program\nwith which to run a file, the file’s name is enough to run a .py file. The following\ncommand, for example, will automatically be run by Python on Windows\n(technically, by Python’s py Windows launcher described in Appendix A):\n$ script1.py",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": 2,
      "content": "This works just as though you had clicked on the file’s icon in File Explorer (a\nlaunch mode covered later). One fine point here: Command Prompt runs\nprograms this way in its own window, but PowerShell may not; use py to view\nunredirected output in the latter if needed (or use the icon-click input() trick\nalso coming up).\nFinally, remember to give the full path to your script file if it lives in a different\ndirectory than the one in which you are working. For example, the py Python\ncommand in the following, run in PowerShell on Windows, assumes Python is in\nyour system path but runs a file located elsewhere:\nPS C:\\Users\\me\\code> cd D:\\temp\\savecode               # Go to a different folder\nPS D:\\temp\\savecode> py C:\\Users\\me\\code\\script1.py    # Run a script elsewhere\nwin32\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\nIf your PATH doesn’t include Python’s directory, and neither Python nor your\nscript file is in the directory you’re working in, use full paths for both—like the\nfollowing on macOS, which for good measure throws in output stream\nredirection to a file located outside the current working directory (where you are\nwhen this command is run):\n$ /usr/local/bin/python3 /Users/me/code/script1.py > /Users/me/data/saveit.py\nThis is a lot to type, but also pathological and atypical. To keep your commands\nsimpler than this, make sure Python is on your PATH, and cd to script or data\nfolders first. Again, most installs set up PATH automatically, so you need only\nfocus on script and data folders when running commands.\nOther Ways to Run Files\nIf you’re not a fan of command lines, you can generally avoid them by launching\nPython scripts with file icon clicks, development GUIs, and other schemes that\nvary per platform and role. This book generally recommends command-line\nusage for learners, both because it’s simple, and because it’s a common, general,\nportable, and powerful way to run code. But it’s not required. While the Python",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": 2,
      "content": "world is too rich in options for exhaustive coverage here, let’s take a quick tour\nof the most prominent command-line alternatives to close out this chapter.\nClicking and Tapping File Icons\nOn most PC platforms, Python program files can be run by simply clicking or\ntapping their filename or icon in the local file explorer. For example, this works\nautomatically on Windows in File Explorer, thanks to filename associations set\nup during Python’s install. Clicks also run code files on macOS in Finder, if\nmade to Open With the Python Launcher included in the macOS install; drags to\nthe Python Launcher app when available work the same as clicks.\nThe file-clicks story is more involved on Linux, where files likely need\nexecutable permission and a #! first line to name Python. On smartphones,\ntapping a filename in a file explorer on Android (or Files on iOS) may open the\nfile in an associated Python app, but this may work only for some explorer/app\ncombos, and may not give access to all of a program’s files in some contexts.\nFor more insight, consult your platform and app docs, or experiment on your\ndevice.\nAlso see Appendix A for more info. As noted there, Windows and Linux clicks\ndon’t keep the window open for viewing output and error messages after\nprograms end: if a script just prints and exits, it, well, just prints and exits—the\nconsole window appears, and text is printed there, but the console window closes\nand disappears on program exit. Per Appendix A, coding a closing call to\nPython’s input() forces a pause before exit so you can see output, but this\ndoesn’t help for error messages. Use other run techniques when this matters.\nThe IDLE Graphical User Interface\nSo far, we’ve seen how to run Python code with interactive sessions, system\ncommand lines, and icon clicks. If you’re looking for something a bit more\nvisual, IDLE provides a GUI for Python programming, and it’s a standard and\nfree part of the Python system. IDLE is usually referred to as an integrated\ndevelopment environment (IDE), because it binds together multiple development\ntasks into a single view.\nIn short, IDLE lets you edit, run, browse, and debug Python programs, all from",
      "content_length": 2175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": 2,
      "content": "the same GUI. Because it’s coded in Python with the tkinter GUI toolkit, it\nruns portably on all Python PC platforms—Windows, macOS, and Linux. For\nmany, IDLE represents an easy-to-use alternative to typing command lines, a\nless problem-prone alternative to clicking on icons, and a great way for\nnewcomers to get started editing and running code. You’ll sacrifice some control\nin the bargain, but this typically becomes important only later in your Python\ncareer.\nIDLE install and startup are covered in Appendix A, so we won’t repeat the full\ndetails here. In brief, it’s standard with the python.org Python installers for\nWindows and macOS and can be had separately in Linux repositories. Once\ninstalled, it can be launched with the usual suspects: Start on Windows,\nLaunchpad or Finder on macOS, a command line on Linux, and file right-clicks\nwhere supported.\nAppendix A also has screenshots that capture IDLE in action on each platform,\nbut Figure 3-1 captures its dark theme on Windows for both variety and a\nquicker look; as it demos, after running this chapter’s script1.py from\nExample 3-1 in the edit window on top, its output appears in the interactive Shell\nwindow.",
      "content_length": 1177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": 2,
      "content": "Figure 3-1. IDLE with its dark theme on Windows\nTip: because IDLE is just a Python script on the module search path in the\nstandard library, you can also generally run it on any platform and from any\ndirectory by typing the following Python command in a system console window\n(use py instead of python3 on Windows, as usual). Python’s –m flag simply\nlocates a module using the normal import search, but runs it as a top-level script\n(Part V covers both this search, and the “.” package syntax required here):\n$ python3 -m idlelib.idle         # Find and run idle.py in a package folder\nOnce IDLE is started, its usage is straightforward and documented in its in-\nprogram Help. In brief, the Shell window provides the usual interactive REPL\nwith command recall, and each edit window allows you to view, modify, and run\na file of code. The Shell’s File→New File and File→Open… start and open code\nfiles. Edit windows’ Run→Run Module runs code in the window where it’s\nselected, and Run→Run… Customized supports passing command-line\narguments to scripts (which is out of scope here and not a full system shell, but\nuseful for scripts that expect these).",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": 2,
      "content": "IDLE has many more features, but as a sampler: it also does tab completions,\npop-up balloon help for functions, and object attribute lists as you type code, and\nincludes an object browser and GUI debugger. To use IDLE’s debugger, enable\nit in the Debug menu, set breakpoints by right-clicks in edit windows, and run.\nFor simpler debugging needs, right-click on the text of any error message in the\nShell window to jump to the line of code where the error occurred.\nFor more IDLE tips, see this book’s Appendix A, IDLE’s own Help menu, and\nthe notes for your platform in “Python Setup and Usage” in Python’s standard\nmanuals. Like most GUIs, the best way to learn IDLE may be to test-drive it for\nyourself. At the end of the day, its usability may be essential for some beginners,\nbut it comes with an extra learning curve, is not as flexible as command lines,\nand adds sugarcoating that might be a negative when your programming needs\noutgrow its scope. As always, vet for yourself on a PC near you.\nOther IDEs for Python\nBecause IDLE is free, portable, and a standard part of Python, it’s a nice first\ndevelopment tool to become familiar with if you want to use an IDE at all. There\nare, however, a handful of alternative IDEs for Python developers, some of\nwhich are substantially more powerful and robust than IDLE.\nAmong these, PyCharm, PyDev, Wing, VSCode, Spyder, and PyScripter all come\nwith the usual edit-and-run GUIs, but some add additional and advanced tools\nsuch as code refactoring and source-control integration. These IDEs also have\nmajor learning curves and are not recommended for most Python beginners.\nStill, you may wish to file this away for later in your Python career when you’ve\nmastered the language and move on to industrial-strength development.\nSmartphone Apps\nIf you’re running this book’s examples on an Android or iOS smartphone or\ntablet, you’ll be using an app. Some, like Termux on Android, come with a\ntraditional command line and support all the REPL and file commands we’ve\nseen. In others, though, you’ll launch code files with devices in the app’s user\ninterface (e.g., button taps), instead of traditional command lines. This isn’t\nmuch different in spirit from running code in IDLE or other IDEs, but because",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": 2,
      "content": "details vary per app, see Appendix A for mobile platform tips, as well as your\napp’s docs for more info.\nWebAssembly for Browsers\nAlthough an emerging technology, it’s also possible to run Python code in web\nbrowsers. This is enabled today by WebAssembly (a.k.a. Wasm), which defines a\nportable bytecode format that is run by web browsers, much as the Python PVM\nruns its own bytecode (see Chapter 2). By compiling the Python interpreter’s\nsource code to this format with tools like the Emscripten LLVM-based compiler,\nweb browsers are able to run Python, and hence your Python programs. While\nother Python-in-the-browser initiatives of the past have largely fizzled,\nWebAssembly is standardized by the World Wide Web Consortium (W3C) and\nalready supported by all major desktop and mobile web browsers.\nCompiling Python itself to Wasm is not trivial (and far too much to ask of\nPython beginners), but the Pyodide system has already done most of the work\nfor you: it’s a port of CPython to the WebAssembly/Emscripten platform\ncompiled and ready to run, with JavaScript integration for access to the browser\nDocument Object Model (DOM) and web APIs. To use Pyodide in a web page,\nyou’ll load it from a server and initialize it in a browser, with an HTML\ndocument that uses provided JavaScript code and API tools. The end result can\nrun many Python programs in web browsers, with no local installs required.\nThe chief downsides of this model seem to be speed and utility. Downloading a\ncompiled CPython interpreter to run a Python script in a browser is not quick,\nand the speed of Python scripts in this context may vary. Moreover, Pyodide\ncomes with a fixed set of Python tools, and Python scripts run by browsers live\nin a sandbox with limited access to tools and resources on the host device.\nPersistent storage, for example, may support POSIX file calls and paths, but is\nvirtual and ultimately limited in this context to options supported by browsers.\nHence, while this option may avoid some Python installs and open possibilities\nfor Python on the web, it’s not as useful for general software development as\nothers, and its future is impossible to predict. Watch the web for more on this\nevolving story—including the alternative MicroPython for Wasm, which is\nsmaller than CPython but implements a constrained Python subset per Chapter 2,\nand the py2wasm Python-to-Wasm compiler, announced just as this book was",
      "content_length": 2415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": 2,
      "content": "being written.\nJupyter Notebooks for Science\nWe met this option earlier in conjunction with REPLs. Jupyter is a set of tools\nthat allow Python code to be run in web browsers, with a focus on supporting\nscientific-programming tasks. Its primary and classic tools require a server to be\nseparately installed and launched to run the code you enter in a web page, but it\nalso comes in a form that runs code locally in browsers using the WebAssembly\nand Pyodide systems described in the preceding section. In both forms, Jupyter\npages follow a flexible notebook paradigm, with interactive coding using the\nIPython REPL we also met earlier, code cells that run with a button click,\nvisualization using Python numeric tools, and more.\nWhile Jupyter is a useful and popular tool in many STEM roles, it’s not targeted\nat general-purpose software development and isn’t as broadly applicable as\ntraditional tools like command lines stressed in this book. See other resources for\nusage details if Jupyter notebooks may be a part of your Python coding future.\nAhead-of-Time Compilers for Speed\nAlso per Chapter 2, a number of AOT compiler systems, including Nuitka and\nShed Skin, compile Python programs all the way to machine code, much like C\nand C++ compilers. Once you install such a system, you’ll run its compiler on a\nfile of Python code first, and then run the resulting program like any other\nexecutable. AOT compilers can boost program speed but add extra development\nsteps that slow the programming process substantially, and negate some of the\nadvantage of using Python. Especially for Python newcomers, these systems are\nprobably best explored after learning the Python language using more accessible\noptions, like traditional REPLs, IDEs, and command lines.\nRunning Code in Code\nThis chapter has talked about “importing modules” a few times without really\nexplaining what this term means. We’ll study modules and larger program\narchitecture in depth in Part V, but because imports are also a way to launch\nprograms, this section will introduce enough module basics to get you started.",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": 3,
      "content": "Like imports, the Python exec built-in can be used to launch files in code too,\nand tools in standard-library modules let you launch programs with command\nlines. While we can’t go into full detail in this chapter, this section briefly\nsurveys the launchers in this department.\nImporting modules\nIn simple terms, every file of Python source code whose name ends in a .py\nextension is a module. No special code or syntax is required to make a file a\nmodule: any such file will do. Other files can access the items a module defines\nby importing that module—which essentially loads another file and grants access\nto that file’s contents. The contents of a module are made available to the outside\nworld through its attributes—a term defined informally in the next section.\nThis module paradigm turns out to be the core idea behind program architecture\nin Python. Larger programs usually take the form of multiple module files,\nwhich import tools from other module files. One of the modules is designated as\nthe main or top-level file, also often called the script—the file launched to start\nthe entire program, which runs line by line as usual. Below this level, it’s all\nmodules importing modules.\nWe’ll delve into such architectural issues in more detail later in this book. This\nchapter is mostly interested in the fact that import operations run the code in a\nfile that is being loaded as a final step. Because of this, importing a file is yet\nanother way to launch it. For instance, if you start an interactive session (from a\nsystem command line or otherwise), you can run the script1.py file we wrote\nearlier in Example 3-1 with a simple import statement—which is really Python\ncode running other Python code:\n$ python3\n>>> import script1\ndarwin\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\nNOTE\nWhere to run imports: Be sure to run this Python command in the directory (i.e., folder)\ncontaining script1.py. Per this chapter’s earlier coverage of code folders, this is easiest if you",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": 3,
      "content": "save module files and run imports in a per-chapter folder for the chapter you’re working in.\nLater in this book, you’ll learn that imports in a REPL search for a module in the current\ndirectory, plus those listed on environment variable PYTHONPATH or specified otherwise. The\ncurrent directory part of this will suffice for most imports you’re likely to try until then—as\nlong as all your code files reside there.\nReloading modules\nImports work to run a file, but only once per session (really, process—a program\nrun) by default. After the first import, later imports do nothing, even if you\nchange and save the module’s source file again in another window:\n>>> import script1\n>>> import script1\nThis is by design; imports are too expensive an operation to repeat more than\nonce per file in a given program run. As you’ll learn in Chapter 22, imports must\nfind files, compile them to bytecode, and run the code line by line, and importers\nusually care only that the module’s lines have defined its exports.\nIf you really want to force Python to run the file again in the same session\nwithout stopping and restarting the REPL, you need to instead call the reload\nfunction available in the importlib standard-library module (and previously in\nthe now-defunct imp module, and a built-in function before that: that’s three\nincarnations, for anyone counting!):\n>>> from importlib import reload\n>>> reload(script1)\ndarwin\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\n<module 'script1' from '/Users/me/Code/script1.py'>\nThe from statement here simply copies a name out of a module (more on this in\nthe next section). The reload function itself loads and runs the current version\nof your file’s source code, picking up the file’s changes if you’ve modified and\nsaved it in another window.\nThis allows you to edit and use new code on the fly in the current Python",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": 3,
      "content": "interactive session. The reload function expects the name of an already-loaded\nmodule object, so you have to have successfully imported a module before you\ncan reload it (and if the import failed with an error, you can’t yet reload and must\nimport again).\nNotice that reload also expects parentheses around the module object name,\nwhereas import does not—reload is a function that is called with an argument,\nand import is a statement. That’s also why you get back an extra output line\nwhen reloading—the odd last line is just the display representation of the reload\ncall’s return value, a Python module object. You’ll learn more about using\nfunctions in general in Chapter 16; for now, when you hear “function,”\nremember that parentheses are required to run a call, even if there’s nothing to\nsend.\nModule attributes: a first look\nImports and reloads provide a natural program launch option because import\noperations execute files as a last step. In the broader scheme of things, though,\nmodules serve the role of libraries of tools, as you’ll learn in Part V. The basic\nidea is straightforward, though: a module is mostly just a package of variable\nnames, known as a namespace, and the names within that package are called\nattributes—variable names that are attached to a specific object (like a module).\nIn more concrete terms, all the names assigned at the top level of a module’s file\nbecome its attributes, and a module’s importers gain access to all of them. These\nnames are usually assigned to tools exported by the module—functions, classes,\nvariables, and so on—that are intended to be used in other files and other\nprograms. Externally, a module file’s names can be fetched with two Python\nstatements, import and from, and may be reset with the reload call.\nTo illustrate, use a text editor to create a one-line Python module file called\nmyfile.py in your working directory, with the contents in Example 3-2.\nExample 3-2. myfile.py\ntitle = 'Learning Python, 6th Edition'\nThis may be one of the world’s simplest Python modules (it contains a single\nassignment statement), but it’s enough to illustrate the point. When this file is\nimported, its code is run to generate the module’s attribute. That is, the",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": 3,
      "content": "assignment statement creates a variable and module attribute named title.\nYou can access this module’s title attribute in other components in two\ndifferent ways. First, you can load the module as a whole with an import\nstatement, and then qualify the module name with the attribute name to fetch it\n(note that we’re letting the interpreter print automatically here):\n$ python3                          # Start Python REPL\n>>> import myfile                  # Run file, load module as a whole\n>>> myfile.title                   # Use its attribute names: '.' to qualify\n'Learning Python, 6th Edition'\nIn general, the dot expression syntax object.attribute lets you fetch any\nattribute attached to any object and is one of the most common operations in\nPython code. Here, we’ve used it to access the string variable title inside the\nmodule myfile—in other words, myfile.title.\nAlternatively, you can fetch (really, copy) names out of a module with from\nstatements:\n$ python3                          # Restart Python REPL\n>>> from myfile import title         # Run file, copy its names\n>>> title                          # Use name directly: no need to qualify\n'Learning Python, 6th Edition'\nAs you’ll see in more detail later, from is just like an import, with an extra\nassignment to names in the importing code. Technically, from copies a module’s\nattributes, such that they become simple variables in the recipient. So, you can\nrefer to the imported string this time as title (a variable) instead of\nmyfile.title (an attribute).\nNaturally, modules usually define more than one name to be used both in and\noutside their files. Example 3-3, for instance, defines three.\nExample 3-3. threenames.py\na = 'PC'                       # Define three attributes\nb = 'Phone'                    # Exported to other files\nc = 'Tablet'\nprint(a, b, c)                 # Also used as variables in this file\nThis code file, threenames.py, assigns three variables, and so generates three",
      "content_length": 1970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": 3,
      "content": "attributes for the outside world. It also uses its own three variables in a print\nstatement, as you see when this is run as a top-level file from a system prompt:\n$ python3 threenames.py\nPC Phone Tablet\nAll of this file’s code also runs as usual the first time it is imported elsewhere, by\neither an import or from. Clients of this file that use import get a module with\nattributes, while clients that use from get copies of the file’s names:\n$ python3\n>>> import threenames                    # Grab the whole module: it runs here\nPC Phone Tablet\n>>> threenames.b, threenames.c           # Access its attributes: qualify to use\n('Phone', 'Tablet')\n>>> from threenames import b, c          # Copy multiple names out: use directly\n>>> b, c\n('Phone', 'Tablet')\nThe results here are printed in parentheses because they are really tuples—a kind\nof object created by the comma in the inputs (and covered in the next part of this\nbook)—that you can safely accept on faith for now.\nFrom a grander perspective, modules form the highest layer of Python program\narchitecture: as self-contained namespaces, they naturally support code\norganization and reuse, and they automatically minimize name collisions in your\ncode. We’ll deal with their loftier goals later in this book.\nFor this chapter, it’s enough to know that imports and reloads are another way to\nrun your code files, though probably a secondary option: due to complicated\nquirks that we’ll skip here (e.g., reload updates importers using import but not\nfrom), tools like command lines and IDEs are generally better bets for running\nPython code.\nThe exec built-in\nPython also provides a way to launch files with code that does not rely on the\nmodule concepts of the preceding section. The exec built-in function compiles",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": 3,
      "content": "and runs whatever Python source code statements are in the string you pass to it.\nAlong with its eval expression cousin, this supports many dynamic roles you’ll\nmeet in upcoming chapters.\nBy passing a code file’s loaded contents to exec, though, this yields another way\nto launch code files from a REPL or other file without having to import and later\nreload. Each such exec runs the current version of the code read from a file,\nwithout requiring imports or reloads. For instance, using Example 3-1’s\nscript1.py again:\n$ python3\n>>> exec(open('script1.py').read())\ndarwin\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\nTo understand this code, you must first know that the open and read nested\ninside it run first, and left to right, and load the file’s entire contents as a string.\nWe’ll study files in the next part of this book, so take this as a preview for now.\n(This was also a forward knowledge dependency added in Python 3.X, but that’s\nwhat this book presents.)\nOnce the file is loaded, though, the exec call has an effect similar to an import,\nbut it doesn’t actually import the module. Instead, each time you run an\nexec/open combo this way it runs the file’s code anew and unconditionally—as\nthough you had pasted the file’s code at the place where exec is called. As one\nconsequence, this exec/open scheme does not require module reloads after file\nchanges.\nOn the downside, because it works as if you’ve pasted code into the place where\nit is called, exec has the potential to silently overwrite variables you may\ncurrently be using. For example, our script1.py assigns to a variable named x. If\nthat name is also being used in the place where exec is called, the name’s value\nis replaced—sans warning:\n>>> x = 999\n>>> exec(open('script1.py').read())    # Code run in this namespace by default\n…same output…\n>>> x                                  # Its assignments can overwrite names here",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": 3,
      "content": "'Hack!'\nThis potential for name collisions is a downside shared by the last section’s from\nstatement. By contrast, the basic import statement makes the file a separate\nmodule namespace so that its assignments will not change variables in the\nimporter’s scope. The price you pay for its namespace partitioning of modules is\nthe need to reload and qualify.\nCommand-line launchers\nFinally, Python code can also be run by Python code that uses standard-library\ntools that spawn command lines in parallel processes. The os module’s system\ncall, for instance, runs a command as if you typed it at a system console (its 0\nreturn value echoed at the end here means all worked well):\n$ python3\n>>> import os\n>>> os.system('python3 script1.py')\ndarwin\n1267650600228229401496703205376\nHack!Hack!Hack!Hack!Hack!Hack!Hack!Hack!\n0\nThis os module’s popen call does similar but returns a file object from which\nyou can read the spawned command’s printed output as a string for use in the\nspawning code; and Python’s subprocess.run call can be used to launch\nprograms by command lines with much more control over the fine points. We’ll\ndeploy popen in Chapter 21.\nThough related to running code, all these tools are well beyond the scope of this\ngetting-started chapter, so consult Python’s library manuals for the full story if\nand when such utilities become useful in your future work. And a caution: these\ntools, along with exec, will happily run any command line you throw at them\n—including one to erase all your files!—so they’re best limited to running\nPython commands unless and until you’re sure that other commands are safe.\nOther Launch Options\nYou’ve already seen a blizzard of code-launching options for Python, but this",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": 3,
      "content": "accounting is still incomplete. For example, Python code can also be run today\nby:\nPrograms written in C, C++, Java, and more, which embed Python and\nPython code\nText editors that aren’t full IDEs, but know how to run Python code\nyou’re editing\nExcel spreadsheets, when calculating a sheet’s cells coded in Python\nWeb servers that spawn scripts automatically in response to browser UI\nactions\nUsers launching standalone executables, which we explored in\nChapter 2\nAnd (as usual) more\nMoreover, launching techniques tend to evolve as rapidly as everything else in\ncomputing, and future options are impossible to foresee. In general, because\nPython keeps pace with such changes, you should be able to launch Python\nprograms in whatever way makes sense for the machines you use, both now and\nin the future—be that by swiping on your smartphone, grabbing icons in a virtual\nreality, or shouting a script’s name over your coworkers’ conversations.\nWhich Option Should I Use?\nWith all these options, beginners might naturally ask, Which one is best for me?\nIn general, you should try both basic command lines and the IDLE GUI if you\nare just getting started with Python, unless you’ll be working in smartphone apps\nor web-based notebooks with interfaces that are more unique. Command lines\nare simple and powerful, but IDLE provides a user-friendly GUI environment\nthat hides some underlying details and works the same on all PCs.\nIf, on the other hand, you are an experienced programmer, you might be more\ncomfortable with simply the text editor of your choice in one window, and a\nsystem console interface in another for launching edited programs via Python\ncommand lines. Because the choice of development environments is very",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": 3,
      "content": "subjective, this book can’t offer much more in the way of universal guidelines.\nIn general, whatever environment you like to use will be the best for you to use.",
      "content_length": 161,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": 3,
      "content": "Chapter Summary\nIn this chapter, we’ve explored common ways to launch Python programs: by\nrunning code typed interactively and by running code stored in files with system\ncommand lines, file icon clicks, IDE GUIs such as IDLE, and more. We’ve\ncovered a lot of pragmatic startup territory here. This chapter’s goal was to equip\nyou with enough information to enable you to start writing some code, which\nyou’ll do in the next part of this book. There, we will start exploring the Python\nlanguage itself, beginning with its core data types—the objects that are the\nsubjects of your programs.\nFirst, though, take the usual chapter quiz to exercise and review what you’ve\nlearned here. Because this is the last chapter in this part of the book, it’s\nfollowed with a set of more complete exercises that test your mastery of this\nentire part’s topics. For help and answers for the latter set of problems, or just for\na refresher, be sure to turn to Appendix B after you’ve given the exercises a try.\nTest Your Knowledge: Quiz\n1. How can you start a Python interactive interpreter session (REPL)?\n2. Where do you type a system command line to launch a script file?\n3. Name four or more ways to run the code saved in a script file.\n4. What pitfall is related to clicking file icons on Windows and Linux?\n5. Why might you need to reload a module?\n6. How do you run a script from within the IDLE GUI?\n7. How are modules, attributes, and namespaces related?\nTest Your Knowledge: Answers\n1. A Python interactive session can be started by typing a Python\ncommand line in your system’s console window. Type py on Windows",
      "content_length": 1606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": 3,
      "content": "and python3 everywhere else and type it into a Command Prompt or\nPowerShell window on Windows, Terminal on macOS and Linux, and\nthe Termux app on Android. Another alternative is to launch IDLE, as\nits main Shell window is an interactive session. Other IDEs, smartphone\napps, and browser-based systems may offer REPLs in more unique\nways. \n2. You type system command lines in the same interface used to launch an\ninteractive session by command line. This is whatever your platform\nprovides as a system console: again, Command Prompt or PowerShell\non Windows, Terminal on macOS and Linux, Termux on Android, or\nother. You type this at the system’s prompt shown as $ in this book, not\nat the Python interactive interpreter’s >>> prompt used to enter Python\ncode—be careful not to confuse these prompts, because Python and the\nsystem shell are different systems.\n3. Code in a script (really, module) file can be run with system command\nlines, file icon clicks, imports and reloads, the exec built-in function, os\nmodule tools, and IDE GUI devices such as IDLE’s Run→Run Module\nmenu option. Some platforms support more specialized launching\ntechniques, like drag and drop on macOS, app UIs on smartphones, and\nweb notebooks. In addition, some text editors have unique ways to run\nPython code, some Python programs are provided and run as standalone\nexecutables, and some systems use Python code in embedded mode,\nwhere it is run by an enclosing program written in another language.\nThough in its early days, code may also be run in web browsers with a\nPython port to WebAssembly like Pyodide.\n4. Scripts that print and then exit cause the output file to disappear\nimmediately, before you can view the output. input() can pause before\nexit so the output window stays open, but error messages generated by\nyour script also appear in an output window that closes before you can\nexamine its contents—and before an input() pause is reached. Hence,\nsystem command lines and IDEs such as IDLE are better for most\ndevelopment.\n5. Python imports (i.e., loads) a module only once per process, by default,",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": 3,
      "content": "so if you’ve changed its source code and want to run the new version\nwithout stopping and restarting Python, you’ll have to reload it. You\nmust import a module at least once before you can reload it. Running\nfiles of code from a system shell command line, via an icon click, or in\nan IDE such as IDLE generally makes this a nonissue, as those launch\nschemes usually run the current version of the source code file each\ntime. An exec/open pair can avoid reloads too.\n6. Within the text edit window of the file you wish to run, select the\nwindow’s Run→Run Module menu option. This runs the window’s\nsource code as a top-level script file in IDLE and displays its output\nback in the interactive Python “Shell” window.\n7. Each module file is automatically a namespace—that is, a package of\nvariables reflecting the assignments made at the top level of the file.\nEach of the module’s variables becomes an attribute of the module\nwhen it’s imported and are accessed by “.” qualification or from name\ncopies. Namespaces help avoid name collisions in Python programs:\nbecause each module file is a self-contained namespace, files must\nexplicitly import other files in order to use their names.",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": 3,
      "content": "Test Your Knowledge: Part I Exercises\nIt’s time to start doing a little coding on your own. This first exercise session is\nfairly simple, but it is designed to make sure you’re ready to work along with the\nrest of the book, and a few of its questions hint at topics to come in later\nchapters. Be sure to check the section “Part I, Getting Started” in Appendix B for\nthe answers; the exercises and their solutions sometimes contain supplemental\ninformation not discussed in the main text, so you’re encouraged to take a peek\nat the solutions even if you manage to answer all the questions on your own.\n1. Interaction: Using a system command line, IDLE, or any other method\nthat works on your device, start the Python interactive command line\n(the >>> prompt, a.k.a. REPL) and type the expression 'Hello\nWorld!' (including the quotes). The string should be echoed back to\nyou. The purpose of this exercise is to get your environment configured\nto run Python. In rare scenarios, you may need to type the full path to\nthe Python executable or add its path to your PATH environment\nvariable. See Appendix A for tips on environment-variable settings if\nneeded.\n2. Programs: With the text editor of your choice, write a simple module\nfile containing the single statement print('Hello module world!')\nand store it as module1.py. Now, run this file by using any launch option\nyou like: running it in IDLE, clicking or tapping on its file icon, starting\nit by command line in a console (e.g., python3 module1.py),\nexecuting it in code with tools like exec and imports/reloads, or by\nusing UI options in apps, other IDEs, and web notebooks. In fact,\nexperiment by running your file with as many of the launch techniques\ndiscussed in this chapter as you can. Which technique seems easiest to\nyou?\n3. Modules: Start the Python interactive command line (>>> prompt) and\nimport the module you wrote in exercise 2. Try moving the file to a\ndifferent directory and importing it again from its original directory\n(i.e., run Python in the original directory when you import). What",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": 3,
      "content": "happens? Hint: there’s still a .pyc bytecode file for module1 in a\n__pycache__ subdirectory there, but it’s named oddly. In general,\nimports search for modules in the current directory, plus every directory\nlisted on the PYTHONPATH environment variable—as you’ll learn in\nPart V.\n4. Scripts: If your platform supports it, copy your module1.py module file\nto another named script1.py; then add the #! line to the top of\nscript1.py, give this file executable privileges, and run it directly as an\nexecutable (e.g., sans the “python3”). What does the first line need to\ncontain? #! lines traditionally have meaning only on Unix-like\nplatforms (e.g., macOS, Linux, and Android), but also apply to\nWindows today, thanks to the py Windows launcher. If you’re working\non Windows, also try running your file by listing just its name in a\nCommand Prompt window without the word py before it, via the Start\nmenu, Windows+R Run dialog, or other schemes. On macOS, try a drag-\nand-drop to the Python Launcher app in Applications (or elsewhere).\n5. Errors and debugging: Experiment with typing mathematical\nexpressions and assignments at the Python interactive command line\n(that is, REPL). Along the way, type the expressions 2 ** 500 and 1 /\n0, and reference an undefined variable name as we did early on in this\nchapter. What happens?\nYou may not know it yet, but when you make a mistake, you’re doing\nexception processing, a topic we’ll explore in depth in Part VII. As\nyou’ll learn there, you are technically triggering what’s known as the\ndefault exception handler—logic that prints a standard error message. If\nyou do not catch an error, the default handler does and prints the\nstandard error message in response.\nExceptions are also bound up with the notion of debugging in Python.\nWhen you’re first starting out, Python’s default error messages on\nexceptions will probably provide as much error-handling support as you\nneed—they give the cause of the error, as well as showing the lines in\nyour code that were active when the error occurred. For more about\ndebugging, see the upcoming sidebar “Debugging Python Code”.",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": 3,
      "content": "6. Breaks and cycles: In any Python REPL, type:\nL = [1, 2]              # Make a 2-item list\nL.append(L)             # Append L as a single item to itself\nL                       # Print L: a cyclic/circular object\nWhat happens? In all but the most ancient of Python versions, you’ll see\na strange output that is described in the Appendix B solution, and which\nwill make more sense when you study object references in the next part\nof the book. Why do you think your version of Python responds the way\nit does for this code?\n7. Documentation: Spend at least five minutes browsing the Python library\nand language manuals before moving on to get a feel for the available\ntools in the standard library and the structure of the documentation set.\nIt takes at least this long to become familiar with the locations of major\ntopics in the manual set; once you’ve done this, it’s easy to find what\nyou need. You can find this manual via the Python Start menu entry on\nsome Windows, in the Python Docs option on the Help pull-down menu\nin IDLE, or online at http://www.python.org/doc. You’ll also learn more\nabout the manuals and other documentation sources (including PyDoc\nand the help function) in Chapter 15. If you still have time, go explore\nthe Python website, as well as its PyPI third-party extension repository;\npython.org’s About and Search, for example, may be useful when\nyou’re getting started.\nDEBUGGING PYTHON CODE\nNaturally, none of this book’s readers ever have bugs in their code (insert\nsmiley here), but for less fortunate friends of yours who may, here’s a quick\nreview of the strategies commonly used by real-world Python programmers\nto debug errors in their code. The first two may be all you’ll need early in the\nlearning process, but others may grow important when you start writing\nlarger scripts, and all are useful to review now before you start coding in\nearnest in the next chapter—and making the mistakes that are inevitable in\nprogramming:",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": 3,
      "content": "Do nothing. This doesn’t mean that Python programmers don’t\ndebug their code. But when you make a mistake in a Python\nprogram, you get a useful and readable error message. The message\npinpoints the location of the error in your code by file and line and\nmay even offer a suggested “Did you…?” fix as you saw earlier in\nthis chapter. If you already know Python, and especially for your\nown code, this is often enough—read the error message and fix the\ntagged line. In many cases, this is debugging in Python.\nInsert print statements. Probably the main way that Python\nprogrammers debug their code is to insert print statements and run\nagain. Because most Python code runs immediately after changes,\nthis is usually the quickest way to get more information than error\nmessages provide. The print statements don’t have to be\nsophisticated—a simple “I am here” beacon or display of variable\nvalues is usually enough to provide the context you need. If you\nwrite programs that don’t have a console window (e.g., some GUIs\nand apps), you may need to find your printed messages in an\nautomatic logfile, or use the next point.\nInsert calls to the logging module. The beacon messages of the\nprior point may also use Python’s logging module instead of\nprint, to make the process more formal and gain better control of\nthe output. Because this requires some planning, it’s more common\nin larger programs than tactical scripts. See Python’s library manual\nfor logging usage details.\nUse GUI debuggers. For larger systems you didn’t write, and for\nbeginners who want to trace code in more detail, most Python\ndevelopment GUIs have some sort of point-and-click debugging\nsupport. IDLE has a debugger too, but it doesn’t seem to be used\nvery often in practice—perhaps because it has no command line, or\nsimply because adding print statements is often quicker than\nsetting up a debugging session. To learn more about the debugger,\nsee IDLE’s Help menu, or simply try it on your own; its basic\ninterface is described earlier in this chapter. For similar debugging",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": 3,
      "content": "support in other IDEs, see their docs.\nUse the pdb command-line debugger. For full control, Python\ncomes with a source code debugger named pdb, available as a\nmodule in Python’s standard library. With pdb, you type commands\nto step line by line, display variables, set and clear breakpoints,\ncontinue to a breakpoint or error, and so on. You can launch pdb by\neither importing it and calling pdb.run('code') or running it as a\ntop-level script with the command python3 -m pdb file.py. You\ncan also import and call pdb’s postmortem pdb.pm() after an error\noccurs to get more information about what went wrong. We’ll\nrevisit pdb in Chapter 36, but see Python’s library manual and pdb’s\nown help command for more usage tips.\nUse Python’s –i command-line argument. Short of adding prints\nor running debuggers, you can still see what went wrong on errors.\nIf you run your script from a command line and pass a -i argument\nbetween Python and the name of your script (e.g., python3 –i\nscript.py), Python will automatically open its interactive mode\n(the >>> prompt) when your script exits, whether it ends\nsuccessfully or runs into an error. You can then print the final values\nof variables to get more details about what happened in your code,\nor import and run pdb’s debugger or postmortem mode.\nCatch and handle errors in code. Ultimately, errors are a well-\ndefined mechanism in Python known as exceptions, which you can\ncatch, process, and recover from in your own code. You’ll learn\nhow in Part VII.\nPerhaps the best takeaway on debugging is that errors are detected and\nreported in Python as a norm, rather than passing silently or crashing the\nsystem altogether. Making mistakes is never fun, of course, but especially\nfor those who recall when debugging meant getting out a hex calculator and\nporing over piles of memory-dump printouts, Python’s debugging support\nmakes errors a lot less painful than they might otherwise be.",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": 3,
      "content": "Part Il. Objects and Operations",
      "content_length": 31,
      "extraction_method": "OCR"
    },
    {
      "page_number": 95,
      "chapter": 3,
      "content": "Chapter 4. Introducing Python\nObjects\nThis chapter begins our tour of the Python language. In an informal sense, in\nPython we do things with stuff. “Things” take the form of operations like\naddition and concatenation, and “stuff” refers to the objects on which we\nperform those operations. In this part of the book, our focus is on that stuff, and\nthe things our programs can do with it.\nSomewhat more formally, in Python, data takes the form of objects—either built-\nin objects that Python provides, such as strings and lists, or add-on objects we\ncreate with Python classes or external-language tools. As you’ll find, these\nobjects are essentially just pieces of memory, with values and associated\noperations. Moreover, everything is an object in a Python script. Even simple\nnumbers qualify, with values (e.g., 99) and supported operations (+, -, and so\non).\nBecause objects are also the most fundamental notion in Python programming,\nthis chapter gets us started with a survey that previews Python’s built-in object\ntypes. Later chapters in this part provide a second pass that fills in details we’ll\ngloss over in this survey. Here, our goal is a brief tour to introduce the basics.\nThe Python Conceptual Hierarchy\nBefore we get to the code, let’s first establish a clear picture of how this chapter\nfits into the overall Python picture. From a more concrete perspective, Python\nprograms can be decomposed into modules, statements, expressions, and objects,\nas follows:\n1. Programs are composed of modules.\n2. Modules contain statements.\n3. Statements contain expressions.",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": 3,
      "content": "4. Expressions create and process objects.\nThe discussion of modules in Chapter 3 introduced the highest level of this\nhierarchy. This part’s chapters begin at the bottom—exploring both built-in\nobjects and the expressions you can code to use them.\nWe’ll move on to statements in the next part of the book, though you will find\nthat they largely exist to manage the objects you’ll meet here. Furthermore, by\nthe time we reach classes in the OOP part of this book, you’ll discover that they\nallow you to define new object types of your own, by both using and emulating\nthe object types you will explore here. Because of all this, built-in objects are a\nmandatory point of embarkation for all Python journeys.\nNOTE\nTerminology moment: Traditional introductions to programming often stress its three pillars of\nsequence (“Do this, then that”), selection (“Do this if that is true”), and repetition (“Do this\nmany times”). Python has tools in all three categories, and these terms might help you organize\nyour thinking early on. But they are also artificial and simplistic, and prone to confuse. For\nexample, tools such as comprehensions are both repetition and selection; these terms have\nother, more specific meanings in Python; and many later concepts won’t seem to fit this mold\nat all. In Python, the more strongly unifying principle is objects and what we can do with them.\nTo see why, read on.\nWhy Use Built-in Objects?\nIf you’ve used lower-level programming languages, you know that much of your\nwork centers on implementing objects—also known as data structures—to\nrepresent the components in your application’s domain. You may need to lay out\nmemory structures, manage memory allocation, implement search and access\nroutines, and so on. These chores are about as tedious (and error-prone) as they\nsound, and they usually distract from your program’s real goals.\nIn typical Python programs, most of this grunt work goes away. Because Python\nprovides powerful object types as an intrinsic part of the language, there’s\nusually no need to code object implementations before you start solving\nproblems. In fact, unless you have a need for special processing that built-in\nobjects don’t provide, you’re almost always better off using a built-in object",
      "content_length": 2252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": 3,
      "content": "instead of implementing your own. Here are some reasons why:\nBuilt-in objects make programs easy to write. For simpler tasks,\nbuilt-in objects are often all you need to represent the structure of\nproblem domains. Because you get powerful tools such as collections\n(lists) and search tables (dictionaries) for free, you can use them\nimmediately. You can get a lot of work done with Python’s built-in\nobject types alone.\nBuilt-in objects are components of extensions. For more complex\ntasks, you may need to provide your own objects using Python classes\nor C-language interfaces. But as you’ll see in later parts of this book,\nobjects implemented manually are often built on top of built-in objects\nsuch as lists and dictionaries. For instance, a stack data structure may be\nimplemented as a class that manages or customizes a built-in list.\nBuilt-in objects are often more efficient than custom data\nstructures. Python’s built-in objects employ algorithms that have\nalready been optimized and are often implemented in a lower-level\nlanguage like C for speed. Although you can write similar object types\non your own, you’ll usually be hard-pressed to match the level of\nperformance that built-in object types provide.\nBuilt-in objects are a standard part of the language. In some ways,\nPython borrows both from languages that rely on built-in tools (e.g.,\nLisp) and languages that rely on the programmer to provide tool\nimplementations of their own (e.g., C++). Although you can implement\nunique object types in Python, you don’t need to do so just to get\nstarted. Moreover, because Python’s built-ins are standard, they’re\nalways the same; proprietary toolkits, on the other hand, tend to differ\nfrom site to site.\nIn other words, not only do built-in object types make programming easier,\nthey’re also more powerful and accessible than most of what can be created from\nscratch. Regardless of whether you implement new object types, built-in objects\nform the core of every Python program.",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": 3,
      "content": "Python’s Core Object Types\nTable 4-1 previews Python’s built-in objects, and some of the syntax used to\ncode their literals—that is, the expressions that generate these objects. Some of\nthese objects will probably seem familiar if you’ve used other languages; for\ninstance, numbers and strings represent numeric and textual values, respectively,\nand file objects provide an interface for processing real files stored on your\ncomputer.\nTo some readers, though, the object types in Table 4-1 may be more general and\nflexible than what you are accustomed to. For instance, you’ll find that lists and\ndictionaries alone are powerful data representation tools that obviate most of the\nwork you do to support collections and searching in lower-level languages. In\nshort, lists provide ordered collections of other objects, while dictionaries store\nobjects by key, and both come with automatic memory management, support\narbitrarily nesting, can grow and shrink on demand, and may contain objects of\nany kind.\nTable 4-1. Python built-in (core) objects\nObject type\nExample literals/creation\nNumbers\n1234, 3.1415, 0b111, 1_234, 3+4j, Decimal, Fraction\nStrings\n'code', \"app's\", b'a\\x01c', 'h\\u00c4ck', 'hÄck\n'\nLists\n[1, [2, 'three'], 4.5], list(range(10))\nDictionaries\n{'job': 'dev', 'years': 40}, dict(hours=10)\nTuples\n(1, 'app', 4, 'U'), tuple('hack'), namedtuple\nFiles\nopen('docs.txt'), open(r'C:\\data.bin', 'wb')\nSets\nset('abc'), {'a', 'b', 'c'}\nOther core objects\nBooleans, types, None",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": 3,
      "content": "Program-unit objects\nFunctions, modules, classes (Parts IV, V, and VI)\nImplementation objects\nCompiled code, stack tracebacks (Parts IV and VII)\nAlso shown in Table 4-1, program units such as functions, modules, and classes\n—which you’ll meet in later parts of this book—are objects in Python too; they\nare created with statements and expressions such as def, class, import, and\nlambda and may be passed around scripts freely, stored within other objects, and\nso on. Python also provides a set of implementation-related objects such as\ncompiled-code objects, which are generally of interest to tool builders more than\napplication developers; we’ll explore these later, though in less depth due to their\nspecialized roles.\nDespite its title, Table 4-1 isn’t really complete because everything we process in\nPython programs is a kind of object. For instance, when we perform text pattern\nmatching in Python, we create pattern objects, and when we do network\nscripting, we use socket objects. These other kinds of objects are generally\ncreated by importing and using functions in standard or add-on library modules,\nand have behavior all their own. Patterns and sockets, for example, are made by\ncalling tools in the standard library’s re and socket modules, respectively.\nWe usually call the objects in Table 4-1 core object types, though, because they\nare effectively built into the Python language itself—that is, there is specific\nexpression syntax for generating most of them. For instance, when you run the\nfollowing code with characters surrounded by quotes in a REPL or program file:\n>>> 'Python'\nyou are, technically speaking, running a literal expression that generates and\nreturns a new string object. There is Python language syntax to make this object.\nSimilarly, an expression wrapped in square brackets makes a list, one in curly\nbraces makes a dictionary or set, and so on. Even though, as you’ll see, Python\ndoes not require or use type declarations, the syntax of the expressions you run\ndetermines the types of objects you create and use. In fact, object-generation\nexpressions like those in Table 4-1 are generally where types originate in the\nPython language.",
      "content_length": 2177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": 3,
      "content": "Just as importantly, once you create an object, you bind its operation set for all\ntime—you can perform only string operations on a string and list operations on a\nlist. In formal terms, this means that Python is dynamically typed, a model that\nkeeps track of object types for you automatically instead of requiring declaration\ncode, but it is also strongly typed, a constraint that means you can perform on an\nobject only operations that are valid for its type.\nWe’ll study each of the object types in Table 4-1 completely in upcoming\nchapters. Before digging into the full details, though, let’s begin by taking a\nquicker look at Python’s core objects in action. The rest of this chapter provides\na preview of the operations we’ll explore in more depth in the chapters that\nfollow. Don’t expect to find the full story here—the goal of this chapter is just to\nwhet your appetite and introduce some key ideas. Still, the best way to get\nstarted is to get started, so let’s jump right into some real object-wrangling code.\nNumbers\nIf you’ve done any programming or scripting in the past, some of the object\ntypes in Table 4-1 will probably seem familiar. Even if you haven’t, numbers are\nfairly straightforward. Python’s core objects set includes the usual suspects:\nintegers that have no fractional part, floating-point numbers that do, and more\nexotic types—complex numbers with imaginary parts, decimals with flexible\nprecision, rationals with numerator and denominator, and full-featured sets.\nBuilt-in numbers are enough to represent most numeric quantities—from your\nage to your bank balance—but specialized numeric objects like vectors and\nmatrixes are also available as third-party add-ons.\nAlthough they offer fancier options, Python’s basic number objects are, well,\nbasic. Numbers in Python support the normal mathematical operations. For\ninstance, the plus sign (+) performs addition, a star (*) is used for multiplication,\nand two stars (**) are used for exponentiation. Here is a demo in a Python REPL\nof the sort we learned about in Chapter 3:\n$ python3                        # Start up a REPL\n>>> 123 + 222                    # Integer addition\n345\n>>> 1.5 * 4                      # Floating-point multiplication\n6.0",
      "content_length": 2233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": 3,
      "content": ">>> 1_234_567, 0x15, bin(21)     # Separators, hex, binary\n(1234567, 21, '0b10101')\n>>> 2 ** 100                     # 2 to the power 100, again\n1267650600228229401496703205376\nNotice the last result here: Python’s integer object automatically provides extra\nprecision for large numbers like this when needed. You can, for instance,\ncompute 2 to the power 12,345 as an integer in Python, but you probably\nshouldn’t try to grok the result—with nearly 4K digits, it’s a lot to take in:\n>>> len(str(2 ** 12345))         # How many digits in a really BIG number\n3717\nThis nested-call form works from inside out—first computing the ** result, then\nconverting it to a string of digits with the built-in str function, and finally\ngetting the length of the resulting string with len. The end result is the number\nof digits in the number. str and len both work on many object types, and we’ll\nuse them again as we move along.\nBesides expressions, there are a handful of useful numeric modules that ship\nwith Python in modules—which are just packages of additional tools that we\nimport to use, using statements like import introduced in Chapter 3:\n>>> import math\n>>> math.pi\n3.141592653589793\n>>> math.sqrt(85)\n9.219544457292887\nThe math module contains more advanced numeric tools as functions, while the\nrandom module performs random-number generation and random selections\n(here, from a Python list coded in square brackets—an ordered collection of\nother objects to be introduced later in this chapter):\n>>> import random\n>>> random.random()\n0.7082048489415967\n>>> random.choice([1, 2, 3, 4])\n2",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": 3,
      "content": "Python also includes more exotic numeric objects—such as complex, fixed-\nprecision, and rational numbers, as well as sets and Booleans—and the third-\nparty open source extension domain has even more (e.g., matrixes and vectors,\nand extended precision numbers). We’ll defer discussion of these objects until\nlater in this chapter and book.\nSo far, we’ve been using Python much like a simple calculator; to do better\njustice to its built-in objects catalog, let’s move on to explore strings.\nNOTE\nBig numbers versus DOS attacks: Python integers can be arbitrarily large and may even be\nused to represent floating-point values with extended precision. While integer size is limited\nonly by your computer’s memory, though, Python 3.11 and later require extra steps to convert\npathologically large numbers to decimal strings (e.g., for str or display). This avoids rare\ndenial-of-service attacks with a 4,300-digit limit that can be lifted with a sys module tool,\nset_int_max_str_digits; call this to count digits in larger numbers, and see Python’s docs\nfor all the sordid details.\nStrings\nStrings are used to record both textual information (your name, for instance) as\nwell as arbitrary collections of bytes (such as an image file’s contents). They are\nour first example of what in Python we call a sequence—a positionally ordered\ncollection of other objects. Sequences maintain a left-to-right order among the\nitems they contain: their items are stored and fetched by their relative positions.\nStrictly speaking, strings are sequences of one-character strings; other, more\ngeneral sequence objects include lists and tuples, covered later.\nSequence Operations\nAs sequences, strings support operations that assume a positional ordering\namong items. For example, if we have a four-character string coded inside\nquotes (of the single or double straight kind—they mean the same thing, but\nsingle is more common and less busy), we can verify its length with the built-in\nlen function and fetch its components with indexing expressions:",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": 3,
      "content": ">>> S = 'Code'           # Make a 4-character string, and assign it to a name\n>>> len(S)               # Length: number characters\n4\n>>> S[0]                 # The first item in S, indexing by zero-based position\n'C'\n>>> S[1]                 # The second item from the left\n'o'\nIn Python, indexes are coded in square brackets as offsets from the front, so start\nfrom 0: the first item is at index 0, the second is at index 1, and so on.\nNotice how we assign the string to a variable named S here. We’ll go into detail\non how this works later (especially in Chapter 6), but Python variables never\nneed to be declared ahead of time. A variable is created when you assign it a\nvalue, may be assigned any type of object, and is replaced with its value when it\nshows up in an expression. It must also have been previously assigned by the\ntime you use its value. For the purposes of this chapter, it’s enough to know that\nwe need to assign an object to a variable in order to save it for later use.\nIn Python, we can also index backward, from the end—positive indexes count\nforward from the left, and negative indexes count backward from the right.\nContinuing our REPL session:\n>>> S[-1]                # The last item from the end in S\n'e'\n>>> S[-2]                # The second-to-last item from the end\n'd'\nFormally, a negative index is simply added to the string’s length to yield a\npositive offset, so the following two operations are equivalent (though the first is\neasier to code and less easy to get wrong):\n>>> S[-1]                # The last item in S\n'e'\n>>> S[len(S)-1]          # Negative indexing, the hard way\n'e'\nNotice that we can use an arbitrary expression in the square brackets, not just a\nhardcoded number literal—anywhere that Python expects a value, we can use a\nliteral, a variable, or any expression we wish. Python’s syntax is completely\ngeneral this way.",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": 3,
      "content": "In addition to simple positional indexing, sequences also support a more general\nform of indexing known as slicing, which is a way to extract an entire section\n(a.k.a. slice) in a single step. For example:\n>>> S                     # A 4-character string\n'Code'\n>>> S[1:3]                # Slice of S from offsets 1 through 2 (not 3)\n'od'\nProbably the easiest way to think of slices is that they are a way to extract an\nentire column from a string in a single step. Their general form, X[I:J], means\n“give me everything in X from offset I up to but not including offset J.” The\nresult is returned in a new object. The second of the preceding operations, for\ninstance, gives us all the characters in string S from offsets 1 through 2 (that is, 1\nthrough 3 – 1) as a new string. The effect is to slice or “parse out” the two\ncharacters in the middle at offsets 1 and 2.\nIn a slice, the left bound defaults to zero, and the right bound defaults to the\nlength of the sequence being sliced. This leads to some common usage\nvariations:\n>>> S[1:]                 # Everything past the first (1:len(S))\n'ode'\n>>> S                     # S itself hasn't changed\n'Code'\n>>> S[0:3]                # Everything but the last\n'Cod'\n>>> S[:3]                 # Same as S[0:3]\n'Cod'\n>>> S[:-1]                # Everything but the last again, but simpler (0:-1)\n'Cod'\n>>> S[:]                  # All of S as a top-level copy (0:len(S))\n'Code'\nNote in the second-to-last command how negative offsets can be used to give\nbounds for slices, too, and how the last operation effectively copies the entire\nstring. As you’ll learn later, there is no reason to copy a string, but this form can\nbe useful for other sequences like lists.\nFinally, as sequences, strings also support concatenation with the plus sign\n(joining two strings into a new string) and repetition (making a new string by",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": 3,
      "content": "repeating another):\n>>> S\n'Code'\n>>> S + 'xyz'             # Concatenation\n'Codexyz'\n>>> S                     # S is unchanged\n'Code'\n>>> S * 8                 # Repetition\n'CodeCodeCodeCodeCodeCodeCodeCode'\nNotice that the plus sign (+) means different things for different objects: addition\nfor numbers, and concatenation for strings. This is a general property of Python\nthat we’ll regularly call polymorphism in this book—in short, this means that the\nmeaning of an operation depends on the objects being operated on.\nAs you’ll see when we study dynamic typing, this polymorphism property\naccounts for much of the conciseness and flexibility of Python code. Because\nobject types aren’t constrained, a Python-coded operation can normally work on\nmany different types of objects automatically, as long as they support a\ncompatible interface (like the + operation here). This turns out to be a huge idea\nin Python; you’ll learn more about it later on this tour.\nImmutability\nAlso notice in the prior examples that we were not changing the original string\nwith any of the operations we ran on it. As it turns out, every string operation is\ndefined to produce a new string as its result, because strings are immutable in\nPython—they cannot be changed in place after they are created.\nMore generally, you can never overwrite the values of immutable objects. For\nexample, you can’t change a string by assigning to one of its positions, but you\ncan always build a new one and assign it to the same name. Because Python\nautomatically cleans up old objects as you go (as you’ll learn later), this isn’t as\ninefficient as it may sound:\n>>> S\n'Code'\n>>> S[0] = 'z'             # Immutable objects cannot be changed\n…error text omitted…",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": 3,
      "content": "TypeError: 'str' object does not support item assignment\n>>> S = 'Z' + S[1:]        # But we can run expressions to make new objects\n>>> S\n'Zode'\nEvery object in Python is classified as either immutable (unchangeable) or not.\nIn terms of the core objects, numbers, strings, and tuples are immutable; but\nlists, dictionaries, and sets are not—they can be changed in place freely, as can\nmost new objects you’ll code with classes. This distinction turns out to be crucial\nin Python work, in ways that we can’t yet fully explore. Among other things,\nimmutability can be used to guarantee that an object remains constant\nthroughout your program; mutable objects’ values, by contrast, can be changed\nat any time and place (and whether your code expects it or not!).\nStrictly speaking, you can change text-based data in place if you either expand it\ninto a list of individual characters and join it back together with nothing\nbetween, or use the special-purpose bytearray object:\n>>> S = 'Python'\n>>> L = list(S)                             # Expand to a list: […]\n>>> L\n['P', 'y', 't', 'h', 'o', 'n']\n>>> L[0] = 'C'                              # Change it in place\n>>> ''.join(L)                              # Join with empty delimiter\n'Cython'\n>>> B = bytearray(b'app')                   # A bytes/list hybrid (ahead)\n>>> B.extend(b'lication')                   # 'b' means bytes string\n>>> B                                       # B[i] = ord(x) works here too\nbytearray(b'application')\n>>> B.decode()                              # Translate to normal string\n'application'\nThe bytearray supports in-place changes for text, but only for text whose\ncharacters are all at most 8-bits wide (e.g., ASCII). All other strings are still\nimmutable—bytearray is a distinct hybrid of immutable bytes strings (whose\nb'…' syntax distinguishes them from normal text strings) and mutable lists\n(coded and displayed in []), but we have to wait until we learn more about both\nthese and Unicode text ahead to fully grasp this code.",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": 3,
      "content": "Type-Specific Methods\nEvery string operation we’ve studied so far is really a sequence operation—that\nis, these operations will work on other sequences in Python as well, including\nlists and tuples. In addition to generic sequence operations, though, strings also\nhave operations all their own, available as methods—functions that are attached\nto and act upon a specific object, which are triggered with a call expression\nusing parentheses.\nFor example, the string find method is the basic substring search operation (it\nreturns the offset of the passed-in substring, or −1 if it is not present), and the\nstring replace method performs global searches and replacements; both act on\nthe subject that they are attached to and called from:\n>>> S = 'Code'\n>>> S.find('od')                 # Find the offset of a substring in S\n1\n>>> S\n'Code'\n>>> S.replace('od', 'abl')       # Replace all substrings 'od' in S with 'abl'\n'Cable'\n>>> S\n'Code'\nAgain, despite the names of these string methods, we are not changing the\noriginal strings here but creating new strings as the results—because strings are\nimmutable, this is the only way this can work. String methods are the first line of\ntext-processing tools in Python. Other methods split a string into substrings on a\ndelimiter (handy as a simple form of parsing), perform case conversions, test the\ncontent of the string (digits, letters, and so on), and strip whitespace characters\noff the ends of the string:\n>>> line = 'aaa,bbb,ccccc,dd'\n>>> line.split(',')              # Split on a delimiter into a list of substrings\n['aaa', 'bbb', 'ccccc', 'dd']\n>>> S = 'code'\n>>> S.upper()                    # Upper- and lowercase conversions\n'CODE'\n>>> S.isalpha()                  # Content tests: isalpha, isdigit, etc.\nTrue",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": 3,
      "content": ">>> line = 'aaa,bbb,ccccc,dd\\n'\n>>> line.rstrip()                # Remove whitespace characters on the right side\n'aaa,bbb,ccccc,dd'\n>>> line.rstrip().split(',')     # Combine two operations, run left to right\n['aaa', 'bbb', 'ccccc', 'dd']\nNotice the last command here—it strips before it splits because Python runs it\nfrom left to right, making a temporary result along the way. You can chain\nmethod calls this way, as long as the prior call returns an object with methods.\nStrings methods are also one way to run an advanced substitution operation\nknown as formatting, available as an expression (the original), a string method\ncall (newer), and a literal form called f-strings (newest). The first two replace\nkeys with separate values, the third replaces embedded Python expressions with\ntheir results, and all three resolve substitution values and build new strings when\nthey are run:\n>>> tool  = 'Python'\n>>> major = 3\n>>> minor = 3\n \n>>> 'Using %s version %s.%s' % (tool, major, minor + 9)         # Format expression\n'Using Python version 3.12'\n \n>>> 'Using {} version {}.{}'.format(tool, major, minor + 9)     # Format method\n'Using Python version 3.12'\n>>> f'Using {tool} version {major}.{minor + 9}'                 # Format literal\n'Using Python version 3.12'\nAnd yes, this comes in three flavors today. While you may want to pick one for\nyour own code, all three are fair game in code you may read (and inclusive\nbooks). Each form is rich with features, which we’ll postpone discussing until\nlater in this book, and which tend to matter most when you must generate\nreadable output and numeric reports:\n>>> '%.2f | %+05d' % (3.14159, -62)                   # Digits, signs, padding\n'3.14 | −0062'\n>>> '{1:,.2f} | {0}'.format('sapp'[1:], 296999.256)   # Commas, decimal digits\n'296,999.26 | app'\n>>> f'{296999.256:,.2f} | {'sapp'[1:]}'               # Ditto, with nested quotes",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": 4,
      "content": "'296,999.26 | app'\nAlthough sequence operations are generic, methods are not—while some objects\nshare some method names, string method operations generally work only on\nstrings, and nothing else. As a rule of thumb, Python’s toolset is layered: generic\noperations that span multiple object types show up as built-in functions or\nexpressions (e.g., len(X), X[0]), but type-specific operations are method calls\n(e.g., aString.upper()). Finding the tools you need among all these categories\nwill become more natural as you use Python, but the next section gives a few\ntips you can use right now.\nGetting Help\nThe methods introduced in the prior section are a representative, but small,\nsample of what is available for string objects. In general, this book is not\nexhaustive in its coverage of object methods, but for more details, you can\nalways call the built-in dir function. This function lists variables assigned in the\ncaller’s scope when called with no argument; more usefully, it returns a list of all\nthe attributes available for any object passed to it. Because methods are callable\nattributes, they will show up in this list. Assuming S is still the string 'Code',\nhere are its attributes:\n>>> dir(S)\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__',\n'__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', \n…etc…\n'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines',\n'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nRun this live for the full list; its middle was cut at …etc… for space here (strings\nhave 81 attributes today!). The names without underscores in the second half of\nthis list are all the callable methods on string objects. The names with double\nunderscores won’t be important until later in the book, when we study operator\noverloading in classes. In short, they represent the implementation of the string\nobject and support customization. The __add__ method of strings, for example,\nis how concatenation ultimately works; Python maps the first of the following to\nthe second internally, though you shouldn’t usually use the second form yourself",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": 4,
      "content": "(it’s less intuitive, and might even run slower):\n>>> S + 'head!'\n'Codehead!'\n>>> S .__add__('head!')\n'Codehead!'\nThe dir function simply gives methods’ names. To ask what they do, you can\npass them to the help function:\n>>> help(S.replace)                           # Or help(str.replace)\nHelp on built-in function replace:\nreplace(old, new, count=-1, /) method of builtins.str instance\n    Return a copy with all occurrences of substring old replaced by new.\n…etc…\nPress the Q key to exit a help display in most console windows. help is one of a\nhandful of interfaces to a system of code that ships with Python known as Pydoc\n—a tool for extracting documentation from objects. Later in the book, you’ll see\nthat Pydoc can also render its reports in HTML format for display in a web\nbrowser.\nYou can also ask for help on an entire string with help(S), but it may not help:\nthe string’s value is used instead of the string itself, unless it’s empty. To do\nbetter, you need to know either the name of the string type, or that the type\nbuilt-in returns any object’s type as another object: help(str) and\nhelp(type(S)) both give help for strings but may be more than you want—\nbecause they describe every method, help may be best used on specific\nmethods.\nFor more info, you can also consult Python’s standard-library reference manual,\nbut dir and help are the first level of documentation in Python, especially when\nworking in an interactive REPL. Try them soon on an object near you.\nOther Ways to Code Strings\nSo far, we’ve looked at the string object’s sequence operations and type-specific\nmethods. Python also provides a variety of ways for us to code strings, which\nwe’ll explore in greater depth later. For instance, special characters can be",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": 4,
      "content": "represented as backslash escape sequences, which Python displays in \\xNN\nhexadecimal escape notation, unless they stand for printable characters:\n>>> S = 'A\\nB\\tC'            # Escapes: \\n is newline, \\t is tab\n>>> len(S)                   # Each stands for just one character\n5\n>>> S = 'A\\0B\\0C'            # \\0, a binary zero byte, does not terminate string\n>>> len(S)\n5\n>>> S                        # Nonprintables are displayed as \\xNN hex escapes\n'A\\x00B\\x00C'\nAs hinted earlier, Python allows strings to be enclosed in single or double quote\ncharacters—they mean the same thing but allow the other type of quote to be\nembedded without an escape (most programmers prefer single quotes for less\nclutter, unless they’re pining for other-language pasts). You can also code\nmultiline string literals enclosed in triple quotes (single or double)—when used,\nall the lines are concatenated together, and newline characters (\\n) are added\nwhere line breaks appear. This is useful for embedding things like multiline\nHTML, YAML, or JSON code in a Python script, as well as stubbing out lines of\ncode temporarily—just add three quotes above and below:\n>>> msg = \"\"\"\n... aaaaaaaaaaaa\n... bbb'''bbb\"\"bbb\n... cccccccc\n... \"\"\"\n>>> msg\n'\\naaaaaaaaaaaa\\nbbb\\'\\'\\'bbb\"\"bbb\\ncccccccc\\n'\nPython also supports a raw string literal that turns off the backslash escape\nmechanism. Such literals start with the letter r and are useful for strings like\nregular-expression patterns, and directory paths on Windows sans doubled-up\nbackslashes (e.g., r'C:\\Users\\you\\code').\nUnicode Strings\nPython’s strings also come with full Unicode support required for processing\nnon-ASCII text. Such text includes characters in non-English languages, as well",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": 4,
      "content": "as symbols and emojis, and is common today in web pages, emails, GUIs,\ndocuments, and data. Python’s string objects let you process such text\nseamlessly: its normal str string handles Unicode text (including ASCII, which\nis just a simple kind of Unicode); and its bytes string, together with its\nbytearray mutable cousin used earlier, handles raw byte values (including\nmedia and encoded text):\n>>> 'hÄck'                   # Normal str strings are Unicode text\n'hÄck'\n>>> b'a\\x01c'                # bytes strings are byte-based data\nb'a\\x01c'\nFormally, Python’s byte strings are sequences of 8-bit bytes that print with\nASCII characters when possible, and its text strings are sequences of Unicode\ncode points—identifying numbers for characters, which print as the usual glyphs\nwe’ve come to know and do not necessarily map to single bytes when stored in\nmemory or encoded in files. In fact, the notion of bytes doesn’t quite apply to\nUnicode: it includes a host of character code points too large to fit in a byte, and\ndefines encodings for storage and transmission in which characters may be any\nsize at all:\n>>> 'Code'                              # Characters may be any size in memory\n'Code'\n>>> 'Code'.encode('utf-8')              # Encoded to 4 bytes in UTF-8 in files\nb'Code'\n>>> 'Code'.encode('utf-16')             # But encoded to 10 bytes in UTF-16\nb'\\xff\\xfeC\\x00o\\x00d\\x00e\\x00'\nThis is especially true for richer text (ord gives a character’s code point, and hex\ngives its hexadecimal string):\n>>> hex(ord('\n'))                      # Code points too big for a byte\n'0x1f40d'\n>>> len('\nhÄck\n')                     # One character (code point) each\n6\n>>> len('\nhÄck\n'.encode('utf-8'))     # But encoded bytes sizes vary\n13\n>>> len('\nhÄck\n'.encode('utf-16'))\n18",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": 4,
      "content": "To code non-ASCII characters in text strings, use \\x hexadecimal escapes; short\n\\u or long \\U Unicode escapes; or raw text interpreted per source encodings\noptionally declared in program files when code is read (the default for code is\nthe universal UTF-8). To illustrate, here’s our non-ASCII A-with-diaeresis\ncharacter Ä coded four ways in Python:\n>>> 'h\\xc4\\u00c4\\U000000c4Äck'         # Coding non-ASCII: hex, short, long, raw\n'hÄÄÄÄck'\nIn text strings, all these forms specify Unicode code points that stand for\ncharacters. By contrast, byte strings use only \\x hexadecimal escapes to embed\nthe values of raw bytes; this isn’t always text, but when it is, it’s the encoded\nform of text, not its decoded code points, and encoded bytes are the same as\ncode points only for simple text and encodings:\n>>> '\\u00A3', '\\u00A3'.encode('latin1'), b'\\xA3'.decode('latin1')\n('£', b'\\xa3', '£')\nApart from these string types, Unicode processing often reduces to transferring\ntext data to and from files—which automatically encode text to bytes when\nstored in a file and decode it to characters (a.k.a. code points) when read back\ninto memory. Once loaded, we usually process text as strings in decoded form\nonly. To make this work, text files implement encodings and accept and return\ntext strings, but binary files instead deal in bytes strings for raw data.\nYou’ll meet Unicode again in the files coverage later in this chapter, but we will\nsave the rest of the Unicode story for later in this book. It crops up briefly in\nChapters 7, 9, and 15, but for the most part is postponed until this book’s\nadvanced topics part, in Chapter 37. Unicode is crucial in many (or most)\ndomains today, but many Python newcomers can get by with just a passing\nacquaintance until they’ve mastered string basics.\nIn addition to its built-in string objects, Python’s standard toolset includes\nsupport for text pattern matching with its re module, as well as parsing textual\ndata like JSON, CSV, XML, and HTML. You’ll meet additional examples of\nsome of these tools later in this book, but this tutorial intro has already said\nenough about strings and must move on.",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": 4,
      "content": "Lists\nThe Python list object is the most general sequence provided by the language.\nLists are positionally ordered collections of arbitrarily typed objects, and they\nhave no fixed size. They are also mutable—unlike strings, lists can be modified\nin place by assignment to offsets as well as a variety of list method calls.\nAccordingly, they provide a very flexible tool for representing arbitrary\ncollections—files in a folder, employees in a company, emails in your inbox, and\nso on.\nSequence Operations\nBecause they are sequences, lists support all the sequence operations we\ndiscussed for strings; the only difference is that most of them return lists instead\nof strings. For instance, given a three-item list:\n>>> L = [123, 'text', 1.23]            # A list of three different-type objects\n>>> len(L)                             # Number of items in the list\n3\nwe can index, slice, and so on, just as for strings:\n>>> L[0]                               # Indexing by position (offset)\n123\n>>> L[:-1]                             # Slicing a list returns a new list\n[123, 'text']\n>>> L + [4, 5, 6]                      # Concat/repeat make new lists too\n[123, 'text', 1.23, 4, 5, 6]\n>>> L * 2\n[123, 'text', 1.23, 123, 'text', 1.23]\n>>> L                                  # We're not changing the original list\n[123, 'text', 1.23]\nType-Specific Operations\nPython’s lists may be reminiscent of arrays in other languages, but they tend to\nbe more powerful. For one thing, they have no fixed type constraint—the list we\njust looked at, for example, contains three objects of completely different types\n(an integer, a string, and a floating-point number). Further, lists have no fixed",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": 4,
      "content": "size. That is, they can grow and shrink on demand, in response to list-specific\noperations:\n>>> L.append('Py')                     # Growing: add object at end of list\n>>> L\n[123, 'text', 1.23, 'Py']\n>>> L.pop(2)                           # Shrinking: delete an item in the middle\n1.23\n>>> L                                  # \"del L[2]\" deletes from a list too\n[123, 'text', 'Py']\nHere, the list append method expands the list’s size and inserts an item at the\nend; the pop method (or an equivalent del statement) then removes an item at a\ngiven offset, causing the list to shrink. Other list methods insert an item at an\narbitrary position (insert), remove a given item by value (remove), add\nmultiple items at the end (extend), and more. Because lists are mutable, most\nlist methods also change the list object in place, instead of making a new one:\n>>> M = ['bb', 'aa', 'cc']\n>>> M.sort()\n>>> M\n['aa', 'bb', 'cc']\n>>> M.reverse()\n>>> M\n['cc', 'bb', 'aa']\nThe list sort method here, for example, orders the list in ascending fashion by\ndefault, and reverse reverses it; in both cases, the methods modify the list\ndirectly (though similarly named functions we’ll explore later do not).\nBounds Checking\nAlthough lists have no fixed size, Python still doesn’t allow us to reference items\nthat are not present. Indexing off the end of a list is always a mistake, but so is\nassigning off the end (error messages are condensed here and elsewhere):\n>>> L\n[123, 'text', 'Py'] \n>>> L[99]",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": 4,
      "content": "IndexError: list index out of range\n>>> L[99] = 1\nIndexError: list assignment index out of range\nThis is intentional, as it’s usually an error to try to assign off the end of a list\n(and a particularly nasty one in the C language, which doesn’t do as much error\nchecking as Python). Rather than silently growing the list in response, Python\nreports an error. To grow a list, we call list methods such as append instead (or\nmake a new list). Unlike indexing, slicing scales offsets to be in bounds, but this\nwill have to await coverage in the in-depth chapters ahead.\nNesting\nOne nice feature of Python’s core object types is that they support arbitrary\nnesting—we can nest them in any combination, and as deeply as we like. For\nexample, we can have a list that contains a dictionary, which contains another\nlist, and so on—as deeply and mixed as needed to describe things in our real\nworld.\nAn immediate application of this feature is to represent matrixes, or\n“multidimensional arrays” in Python. A list with nested lists like the following\nwill do the job for basic applications (coding fine print: indentation doesn’t\nmatter in this, Python expressions with unclosed brackets can span multiple lines\nthis way, and some REPLs show “...” continuation-line prompts, which are often\nomitted in this book for easier copy and paste from emedia):\n>>> M = [[1, 2, 3],               # A 3 × 3 matrix, as nested lists\n         [4, 5, 6],               # Code can span lines if bracketed\n         [7, 8, 9]]\n>>> M\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nHere, we’ve coded a list that contains three other lists. The effect is to represent\na 3 × 3 matrix of numbers. Such a structure can be accessed in a variety of ways:\n>>> M[1]                          # Get row 2\n[4, 5, 6]\n>>> M[1][2]                       # Get row 2, then get item 3 within the row",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": 4,
      "content": "6\nThe first operation here fetches the entire second row, and the second grabs the\nthird item within that row (it runs left to right, like the earlier string strip and\nsplit combo). Stringing together index operations takes us deeper and deeper into\nour nested-object structure. Tip: this matrix structure works for small-scale tasks,\nbut for more serious number crunching you will probably want to use the open\nsource NumPy extension for Python, which can store and process large matrixes\nmuch more efficiently than our nested list structure (and is well out of scope\nhere; see Chapter 1).\nComprehensions\nIn addition to sequence operations and list methods, Python includes a more\nadvanced operation known as a list comprehension expression, which turns out\nto be a powerful way to process structures like our matrix. Suppose, for instance,\nthat we need to extract the second column of the prior section’s matrix. It’s easy\nto grab rows by simple indexing because the matrix is stored by rows, but it’s\nalmost as easy to get a column with a list comprehension:\n>>> col2 = [row[1] for row in M]             # Collect the items in column 2\n>>> col2\n[2, 5, 8]\n>>> M                                        # The matrix is unchanged\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nList comprehensions are a way to build a new list by running an expression on\neach item in a sequence, one at a time, from left to right. They are coded in\nsquare brackets (to tip you off to the fact that they make a list) and are composed\nof an expression and a looping construct that share a variable name (row, here).\nThe preceding list comprehension means basically what it says: “Give me\nrow[1] for each row in matrix M, in a new list.” The result is a new list\ncontaining column 2 of the matrix.\nList comprehensions can be more complex in practice:\n>>> [row[1] + 1 for row in M]                 # Add 1 to each item in column 2\n[3, 6, 9]",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": 4,
      "content": ">>> [row[1] for row in M if row[1] % 2 == 0]  # Filter out odd items (pick evens)\n[2, 8]\nThe first operation here, for instance, adds 1 to each item as it is collected, and\nthe second uses an if clause to filter odd numbers out of the result using the %\nmodulus expression (remainder of division). List comprehensions make new lists\nof results, but they can be used to iterate over any iterable object—a term we’ll\nflesh out later in this book, but which simply means either a physical sequence\nor a virtual one that produces its items on request. Here, for instance, we use list\ncomprehensions to step over a hardcoded list of coordinates and a string:\n>>> diag = [M[i][i] for i in [0, 1, 2]]      # Collect a diagonal from matrix\n>>> diag\n[1, 5, 9]\n>>> doubles = [c * 2 for c in 'hack']        # Repeat characters in a string\n>>> doubles\n['hh', 'aa', 'cc', 'kk']\nThese expressions can also be used to collect multiple values, as long as we\nwrap those values in a nested collection. The following illustrates using range—\na built-in that generates successive integers and requires a surrounding list to\nforce it to yield all its values for display in the REPL (there’s more on this\nmystery ahead):\n>>> list(range(4))                           # Integers 0..(N-1)\n[0, 1, 2, 3]\n>>> list(range(−6, 7, 2))                    # −6 to +6 by 2\n[−6, −4, −2, 0, 2, 4, 6]\n>>> [[x ** 2, x ** 3] for x in range(4)]     # Multiple values, \"if\" filters\n[[0, 0], [1, 1], [4, 8], [9, 27]]\n>>> [[x, x // 2, x * 2] for x in range(-6, 7, 2) if x > 0]\n[[2, 1, 4], [4, 2, 8], [6, 3, 12]]\nAs you can probably tell, list comprehensions are too involved to cover more\nformally in this preview. The main point of this brief introduction is to illustrate\nthat Python includes both simple and advanced tools in its arsenal. List\ncomprehensions are optional, but they can be very useful in practice and often",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": 4,
      "content": "provide a processing speed advantage.\nIn fact, comprehension syntax is not just for making lists: enclosing it in\nparentheses can also be used to create an iterable object known as a generator,\nwhich produces results on demand per Python’s iteration protocol—in the\nfollowing, summing items in a matrix row with the built-in sum, on each call to\nnext:\n>>> M\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n>>> G = (sum(row) for row in M)              # Make a generator of row sums\n>>> next(G)                                  # Run the iteration protocol (ahead)\n6\n>>> next(G)                                  # A new row sum on each call\n15\n>>> next(G)                                  # Row 3: 7 + 8 + 9\n24\nAnd comprehension syntax can also be used to create sets and dictionaries when\nenclosed in curly braces:\n>>> {sum(row) for row in M}                  # Makes an unordered set of row sums\n{24, 6, 15}\n>>> {i: sum(M[i]) for i in range(3)}         # Makes key:value table of row sums\n{0: 6, 1: 15, 2: 24}\nBut to grasp concepts like the iteration protocol and objects like sets and\ndictionaries, we must move ahead.\nDictionaries\nUnlike strings and lists, Python dictionaries are not sequences at all but are\ninstead the only core member of a category known as mappings. Mappings are\nalso collections of other objects, but they store objects by key instead of by\nrelative position. While dictionaries retain the insertion order of their keys today,\nthis may not apply to your goals, and key-to-value mapping remains their main\nrole. Dictionaries are also mutable: like lists, they may be changed in place and\ncan grow and shrink on demand. Also like lists, they are a flexible tool for",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": 4,
      "content": "coding collections, but their more mnemonic keys are better suited when a\ncollection’s items are named or labeled—as in fields of a database record.\nMapping Operations\nWhen written as literals, dictionaries are coded in curly braces and consist of a\nseries of “key: value” pairs. Dictionaries are useful anytime we need to associate\na set of values with keys—to describe the properties of something. As an\nexample, consider the following three-item dictionary with keys “name,” “job,”\nand “age,” that record the attributes of a fictitious (and wholly generic and\nneutral) worker:\n>>> D = {'name': 'Pat', 'job': 'dev', 'age': 40}\nWe can index this dictionary by key to fetch and change its keys’ associated\nvalues. The dictionary index operation uses the same syntax as that used for\nsequences, but the item in the square brackets is a key, not a relative position:\n>>> D['name']                       # Fetch value of key 'name'\n'Pat'\n>>> D['job'] = 'mgr'                # Change Pat's job description\n>>> D\n{'name': 'Pat', 'job': 'mgr', 'age': 40}\nAlthough the curly-braces literal form does see use, it is perhaps more common\nto see dictionaries built up in different ways (after all, it’s rare to know all your\nprogram’s data before your program runs). The following code, for example,\nstarts with an empty dictionary and fills it out one key at a time. Unlike out-of-\nbounds assignments in lists, which are forbidden, assignments to new dictionary\nkeys create those keys:\n>>> D = {}\n>>> D['name'] = 'Pat'               # Create keys by assignment\n>>> D['job']  = 'dev'\n>>> D['age']  = 40\n>>> D\n{'name': 'Pat', 'job': 'dev', 'age': 40}",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": 4,
      "content": "Here, we’re effectively using dictionary keys as field names in a record that\ndescribes an imaginary person. In other roles, dictionaries can also be used to\nreplace searching operations—indexing a dictionary by key is often the fastest\nway to code a search in Python.\nAs you’ll learn later, we can also make dictionaries by passing to the dict type\nname either keyword arguments (a special name=value syntax in function calls),\nor the result of zipping together sequences of keys and values obtained at\nruntime (e.g., from files). Both of the following make the same dictionary as the\nprior example and its equivalent {} literal form; the first requires string keys but\ntends to make for less typing (and subjective noise), and the second uses the zip\nbuilt-in we’ll study later in this part of the book:\n>>> pat1 = dict(name='Pat', job='dev', age=40)                      # Keywords\n>>> pat1\n{'name': 'Pat', 'job': 'dev', 'age': 40}\n>>> pat2 = dict(zip(['name', 'job', 'age'], ['Pat', 'dev', 40]))    # Zipping\n>>> pat2\n{'name': 'Pat', 'job': 'dev', 'age': 40}\nNotice how the left-to-right order of dictionary keys is always the same. Though\nnot sequences, dictionary keys retain their insertion order, even in the presence\nof changes: the order of keys is the order in which keys were added. This wasn’t\nthe norm until Python 3.7, and prior to this, key order was scrambled and\nsometimes required separate ordering. The new ordering may be more intuitive\nand will be assumed in this book but adds a sequence flavor to dictionaries not\nshared by other nonsequences. Like most mods, it also rewrites history and\ninvalidates Python learning resources that cannot be updated as frequently as\nPython. Change is almost always a double-edged sword.\nNesting Revisited\nIn the prior example, we used a dictionary to describe a hypothetical person,\nwith three keys. Suppose, though, that the information is more complex. Perhaps\nwe need to record a first name and a last name, along with multiple job titles.\nThis leads to another application of Python’s object nesting in action. The\nfollowing dictionary, coded all at once as a literal, captures more-structured",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": 4,
      "content": "information (again, indentation here and “…” in some REPLs are moot):\n>>> rec = {'name': {'first': 'Pat', 'last': 'Smith'},\n           'jobs': ['dev', 'mgr'],\n           'age':  40.5}\nHere, we again have a three-key dictionary at the top (keys “name,” “jobs,” and\n“age”), but the values have become more complex: a nested dictionary for the\nname to support multiple parts, and a nested list for the jobs to support multiple\nroles and future expansion. We can access the components of this structure much\nas we did for our list-based matrix earlier, but this time most indexes are\ndictionary keys, not list offsets:\n>>> rec['name']                         # 'name' is a nested dictionary\n{'first': 'Pat', 'last': 'Smith'}\n>>> rec['name']['last']                 # Index the nested dictionary\n'Smith'\n>>> rec['jobs']                         # 'jobs' is a nested list\n['dev', 'mgr']\n>>> rec['jobs'][-1]                     # Index the nested list\n'mgr'\n>>> rec['jobs'].append('janitor')       # Expand Pat's job description in place\n>>> rec\n{'name': {'first': 'Pat', 'last': 'Smith'}, 'jobs': ['dev', 'mgr', 'janitor'], \n'age': 40.5}\nNotice how the last operation here expands the nested jobs list—because the jobs\nlist is a separate piece of memory from the dictionary that contains it, it can grow\nand shrink freely (the pop method we met earlier is one way to shrink objects).\nThis may seem quite a trick to readers with backgrounds in more rigid\nlanguages, but is natural in Python.\nMore fundamentally, this example demos the flexibility of Python’s core object\ntypes. As you can see, nesting allows us to build up complex information\nstructures directly and easily. Building a similar structure in a low-level language\nlike C would be tedious and require much more code: we would have to lay out\nand declare structures and arrays, fill out values, link everything together, and so\non. In Python, this is all automatic—running the expression creates the entire",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": 4,
      "content": "nested object structure for us. In fact, this is one of the main benefits of scripting\nlanguages like Python.\nJust as importantly, in lower-level languages we may have to allocate memory\nspace for objects ahead of time and be careful to release it when we no longer\nneed it. In Python, this is all automatic: object memory is allotted as needed and\nfreed when we lose the last reference to the object—by assigning its variable to\nsomething else, for example:\n>>> rec = 0           # Now the prior object's space is reclaimed\nTechnically speaking, Python uses a scheme called garbage collection that\nreclaims unused memory as your program runs and frees you from having to\nmanage such details in your code. In standard Python (a.k.a. CPython) this uses\nobject reference counts primarily, along with a supplemental garbage collector\nfor cycles. We’ll study how this works later in Chapter 6; for now, it’s enough to\nknow that you can use objects freely, while Python handles their memory.\nWatch for a record structure similar to the one we just coded in Chapters 8, 9,\nand 27, where we’ll use it to compare and contrast lists, dictionaries, tuples,\nnamed tuples, and classes—an array of data structure options with trade-offs\nwe’ll cover in full later. It’s also worth noting that the record structure we used\nhere can be saved in a file with a variety of techniques in Python, including its\npickle module and support for language-neutral JSON (a data format that’s\nstrikingly similar to Python dictionary objects); more on such tools later in this\nbook.\nMissing Keys: if Tests\nAs mappings, dictionaries support accessing items by key only, with the sorts of\noperations we’ve just seen. In addition, though, they also support type-specific\noperations with methods that are useful in a variety of common roles. For\nexample, the dictionary copy and update methods copy a dictionary and merge\none into another in place, respectively (the | operator does the same, but makes a\nnew dictionary for its result: it’s just a copy plus an update).\nDictionary methods also play parts in common key use cases. For instance, while\nwe can assign to a new key to expand a dictionary, fetching a nonexistent key is",
      "content_length": 2196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": 4,
      "content": "still a mistake:\n>>> D = {'a': 1, 'b': 2, 'c': 3}\n>>> D['d'] = 4                        # Assigning new keys grows dictionaries\n>>> D\n{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n>>> D['e']                            # Referencing a nonexistent key is an error\n…error text omitted…\nKeyError: 'e'\nThis is what we want—it’s usually a programming error to fetch something that\nisn’t really there. But in generalized programs, we can’t always know what keys\nwill be present when we write our code. How do we handle such cases and avoid\nerrors? One solution is to test ahead of time. The dictionary in membership\nexpression allows us to query the existence of a key and branch on the result\nwith a Python if statement.\nWhich brings us to our first Python compound statement—one with nested parts.\nIf you’re working along, here are a few practical bits: in the following, press the\nEnter key twice to run the if interactively after typing its code (an empty line\nmeans “go” in most REPLs, as explained in Chapter 3); the prompt changes to a\n“...” after the first line in some interfaces (as for the earlier multiline dictionaries\nand lists); and indentation matters this time (for reasons up next):\n>>> 'e' in D                          # Boolean result: True or False (see ahead)\nFalse\n>>> if not 'e' in D:                  # Python's main selection statement\n       print('missing key!')\nmissing key!\nThis book has more to say about the if statement in later chapters, but its syntax\nis straightforward. It consists of the word if, followed by an expression whose\nresult is interpreted as true or false, followed by a block of code to run if the\nexpression result is true. In its full regalia, the if statement can also have an\nelse clause for the false case, and one or more elif (“else if”) clauses for other\ntests.",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": 4,
      "content": "Functionally, the if is the main selection tool in Python, and how we code most\nof the logic of choices and decisions in our scripts. It’s joined by its ternary\nif/else expression cousin you’ll meet in a moment, the if comprehension filter\nlookalike we used earlier, and the newer match multiple-selection statement\nyou’ll meet later in this book.\nIf you’ve used some other programming languages in the past, you might now be\nwondering how Python knows when the if statement ends. We’ll explore\nPython’s syntax rules in depth in later chapters, but in short, if you have more\nthan one action to run in a statement block, you simply indent all their\nstatements the same way—which both promotes readable code and reduces the\nnumber of characters you have to type. All multiline statements follow this\npattern: a header line ending in “:” and a block of (usually) indented code, with\nno “{}” around blocks, and no “;” required after statements (though fair\nwarning: forgetting the “:” is the most common beginner’s mistake in Python!):\n>>> if not 'e' in D:\n        print('missing')              # Statement blocks are indented\n        print('no, really...')        # (Unless they're simple: see ahead)\nmissing\nno, really...\nBesides the in test, there are a variety of other ways to avoid accessing\nnonexistent keys in the dictionaries we create: the dictionary get method, which\nis a conditional index with a default; the if/else ternary (three-part) expression,\nwhich is essentially a limited if statement squeezed onto a single line; and the\ntry statement, which is a tool we’ll first use in Chapter 10 that catches and\nrecovers from errors altogether. Here are the first two in action:\n>>> D.get('a', 'missing')             # Like D['a'] but with a default\n1\n>>> D.get('e', 'missing')             # Default returned if absent\n'missing'\n>>> D['e'] if 'e' in D else 0         # if/else ternary expression form\n0\nWe’ll save the details on such alternatives until a later chapter. For now, let’s turn",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": 4,
      "content": "to other dictionary methods’ roles in another common use case.\nItem Iteration: for Loops\nDictionaries collect a lot of useful info, but what do we do if we need to process\ntheir items one at a time? As it turns out, dictionaries come with methods\ndesigned for the job:\n>>> D = dict(a=1, b=2, c=3)\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\n>>> list(D.keys())                     # Keys, values, and key/value pairs\n['a', 'b', 'c']\n>>> list(D.values())                   # list forces results genertion\n[1, 2, 3]\n>>> list(D.items())\n[('a', 1), ('b', 2), ('c', 3)]\nAs shown, a dictionary’s keys, values, and items methods return its keys,\nvalues, and key/value pairs (the latter is tuples, up next on this tour). Really,\nthough, these methods all return an object that produces results one at a time,\nwhich is why they’ve been wrapped in list calls, as we did for range earlier.\nThis reflects the iteration protocol in Python—a concept we’ll explore in full\nlater, but which boils down to an iterable object, which has an iterator object,\nwhich responds to next calls to produce one result at a time:\n>>> D.keys()                           # Get an iterable object\ndict_keys(['a', 'b', 'c'])\n>>> I = iter(D.keys())                 # Get an iterator from an iterable\n>>> next(I)                            # Get one result at a time from iterator\n'a'\n>>> next(I)                            # This is most of the iteration protocol\n'b'\nWe used the same next built-in to force results from a generator comprehension\nearlier, but without the iter step: because generators don’t support multiple\nscans, they are their own iterators, and iter is a no-op.\nTools that support this protocol can both save memory and minimize delays,",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": 4,
      "content": "because they don’t produce all their results at once. The iteration protocol works\non all sorts of objects in Python, but you can usually forget its details if you use\nthe Python for loop, which runs the iteration protocol automatically to step\nthrough items one at a time—both for physical collections like strings and lists,\nand virtual sequences like generators, range, and keys:\n>>> for key in D.keys():               # Auto run the iteration prototcol\n        print(key, '=>', D[key])       # Display multiple items, space between\na => 1\nb => 2\nc => 3\nTo code a for, provide a variable (e.g., key) and an iterable object (e.g.,\nD.keys()); for each item in the object, the for assigns the item to the variable\nand runs the nested (and usually indented) block of code—which uses the\nvariable to refer to the current item each time through. The for is one of the\nmain repetition tools in Python, together with the comprehensions you met\nearlier, and the more general-purpose while loop you’ll meet later in this book.\nBecause dictionary iteration is so common, the for, and similar iteration tools,\ncan also step through keys implicitly, as well as key/value pairs. The following\nloops, for example, produce the same output as the preceding example; the\nchoice between all these forms is partly a matter of personal preference, though\nexplicit is generally better:\n>>> for key in D:                      # Implicit keys() iteration\n        print(key, '=>', D[key])\n \n>>> for (key, value) in D.items():     # Key/value-pair tuples iteration\n        print(key, '=>', value)\nThe last of these uses something known as tuple assignment, which\nautomatically unpacks items into variables. But to fully understand the sorts of\nstuff we get back from the dictionary items method, we have to move ahead.\nTuples",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": 4,
      "content": "The tuple object (pronounced “toople” or “tuhple,” depending on whom you ask)\nis roughly like a list that cannot be changed—tuples are sequences, like lists, but\nthey are immutable, like strings. Functionally, they’re used to represent fixed\ncollections of items: the components of a specific calendar date, for instance.\nSyntactically, they are normally coded in parentheses instead of square brackets\nand support arbitrary object types, arbitrary nesting, and the usual sequence\noperations that we used on strings and lists earlier:\n>>> T = (1, 2, 3, 4)            # A 4-item tuple\n>>> len(T)                      # Length\n4\n>>> T + (5, 6)                  # Concatenation: a new tuple\n(1, 2, 3, 4, 5, 6)\n>>> T[0], T[1:]                 # Indexing, slicing, and more\n(1, (2, 3, 4))\nAs usual, tuples also have type-specific callable methods, but not nearly as many\nas lists:\n>>> T.index(4)                  # Tuple methods: 4 appears at offset 3\n3\n>>> T.count(4)                  # 4 appears once\n1\nThe primary distinction for tuples is that they cannot be changed once created.\nThat is, they are immutable sequences (quirk: one-item tuples like the one here\nrequire a trailing comma to distinguish them from simple expressions):\n>>> T[0] = 2                    # Tuples are immutable\nTypeError: 'tuple' object does not support item assignment\n>>> T = (2,) + T[1:]            # Make a new tuple for a new value\n>>> T\n(2, 2, 3, 4)\nLike lists and dictionaries, tuples support mixed types and nesting, but they don’t\ngrow and shrink like lists and dictionaries because they are immutable:\n>>> T = 'hack', 3.0, [11, 22, 33]",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": 4,
      "content": ">>> T\n('hack', 3.0, [11, 22, 33])\n>>> T[1]\n3.0\n>>> T[2][1]\n22\n>>> T.append(4)\nAttributeError: 'tuple' object has no attribute 'append'\nNotice the first line in this: the parentheses enclosing a tuple’s items can often be\nomitted, as done here. In contexts where commas don’t otherwise matter, the\ncommas are what actually builds a tuple. This also explains why REPLs show\nresults in parentheses when you enter multiple items separated by commas: the\ninput is really a tuple.\nWhy Tuples?\nSo, why have a kind of object that is like a list, but supports fewer operations?\nFrankly, tuples are not used as often as lists in practice, but their immutability is\nthe whole point. If you pass a collection of objects around your program as a list,\nit can be changed anywhere; if you use a tuple, it cannot. That is, tuples provide\na sort of integrity constraint that is convenient in programs larger than those\nhere. We’ll talk more about tuples later in the book, including an extension that\nbuilds upon them called named tuples. For now, though, let’s move on to this\ntour’s last major object.\nFiles\nFile objects are the main way your Python code will access the content of files\non your computer. They can be used to read and write text memos, audio clips,\nExcel documents, saved emails, and whatever else you have stored on your\ndevice. Files are a core object type, but they’re something of an oddball—there\nis no specific literal syntax for creating them. Rather, you create a file object by\ncalling the built-in open function with an external filename, and perhaps more\ndepending on your goals.\nFor example, to create a text output file, pass in its name and the 'w' processing",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": 4,
      "content": "mode string to write text data; Python automatically makes the newline character\n\\n portable across platforms when it’s transferred to and from files:\n>>> f = open('data.txt', 'w')      # Open a new file in text-output mode\n>>> f.write('Hello\\n')             # Write strings of characters to it\n6\n>>> f.write('world!\\n')            # Return number of items written\n7\n>>> f.close()                      # Close to flush output buffers to disk\nThis creates a file in the current directory and writes text to it (the filename can\nbe a full directory path if you need to access a file elsewhere on your device). To\nread back what you just wrote, reopen the file in 'r' processing mode, for\nreading text input—this is also the default if you omit the mode in the call. Then\nread the file’s content into a string and access it. A file’s content is always a\nstring in your script, regardless of the type of data the file contains:\n>>> f = open('data.txt')           # Open an existing file in text-input mode\n>>> text = f.read()                # Read entire file into a string\n>>> text\n'Hello\\nworld!\\n'\n>>> print(text)                    # print interprets control characters\nHello\nworld!\n>>> text.split()                   # File content is always a string\n['Hello', 'world!']\nOther file object methods support additional features we don’t have time to\ncover here. For instance, file objects provide more ways of reading and writing\n(read accepts an optional maximum byte/character size, readline reads one\nline at a time, and so on), as well as other tools (seek moves to a new file\nposition). As you’ll see later, though, the best way to read a text file is usually to\nnot read it at all—files support the iteration protocol with an iterator that\nautomatically reads line by line in for loops and other contexts:\n>>> for line in open('data.txt'):       # Display lines in a file\n        print(line.rstrip())            # Single spaced (sans \\n)",
      "content_length": 1941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": 4,
      "content": "You’ll meet the full set of file methods later in this book, but if you want a quick\npreview now, run a dir call on any open file and a help on any of the method\nnames that come back, as we learned earlier.\nUnicode and Byte Files\nThe prior section’s examples illustrate file basics that suffice for many roles.\nTechnically, though, they rely on the platform’s Unicode encoding default for the\nhost platform. Python text files always use a Unicode encoding to encode strings\non writes and decode them on reads. This is often irrelevant for simple ASCII\ntext data, which usually maps to and from file bytes unchanged. But for richer\nkinds of data, file interfaces can vary by content type.\nAs hinted when we studied Unicode strings earlier, Python draws a sharp\ndistinction between text and binary data in files: text files represent content as\nnormal str strings and perform Unicode encoding and decoding automatically\nas noted, while binary files represent content as the special bytes string and\nallow you to access file content unaltered.\nFor example, binary files are useful for processing media, accessing data created\nby C programs, and so on. What you send and receive is the literal content of the\nfile, whether it’s encoded text or a JPEG image. Add a b to the mode to invoke\nbinary (and expect a \\r\\n instead of \\n at the end on Windows, because its\nnewlines vary from Unix):\n>>> bf = open('data.bin', 'wb')\n>>> bf.write(b'h\\xFFa\\xEEc\\xDDk\\n')     # Write binary data in a bytes\n8\n>>> bf.close()\n>>> open('data.bin', 'rb').read()       # Read binary data to a bytes\nb'h\\xffa\\xeec\\xddk\\n'\nFor text files, we can’t really talk about content without also asking, “What\nkind?”—files may use any Unicode encoding, especially if they came from\nanother platform, or the internet at large. This applies to portable programs too:\nif you want your code to work across platforms, you should generally make\nencodings explicit to avoid unpleasant surprises. Luckily, this is easier than it\nmay sound—simply pass in an encoding name to open to force an encoding:",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": 4,
      "content": ">>> tf = open('unidata.txt', 'w', encoding='utf-8')\n>>> tf.write('\nh\\u00c4ck\n')                              # Encodes to UTF-8\n6\n>>> tf.close()\nIf you read with the same encoding (or one that’s compatible), you get back the\nsame text-character code points that you wrote. The encoded bytes on the file are\nin UTF-8 form, but your code usually doesn’t need to care:\n>>> open('unidata.txt', 'r', encoding='utf-8').read()      # Decodes from UTF-8\n'\nhÄck\n'\n>>> open('unidata.txt', 'rb').read()                       # Raw encoded text\nb'\\xf0\\x9f\\x90\\x8dh\\xc3\\x84ck\\xf0\\x9f\\x91\\x8f'\nWhile files automate most encodings, you can also encode and decode manually\nif your program gets Unicode data from another source—parsed from an email\nmessage or fetched over a network connection, for example:\n>>> 'hÄck'.encode('utf-8')\nb'h\\xc3\\x84ck'\n>>> b'h\\xc3\\x84ck'.decode('utf-8')\n'hÄck'\nPython also supports non-ASCII file names (not just content), but it’s largely\nautomatic. For the whole story on Unicode in Python, stay tuned for Chapter 37.\nOther File-Like Tools\nThe open function is the workhorse for most file processing you will do in\nPython. For more advanced tasks, though, Python comes with additional file-like\ntools: pipes, FIFOs, sockets, keyed-access files, persistent object shelves,\ndescriptor-based files, relational and object-oriented database interfaces, and\nmore. We won’t cover many of these topics in this language book, but you’ll find\nthem useful once you start programming Python in earnest.\nOther Object Types\nBeyond the core object types we’ve seen so far, there are others that get less",
      "content_length": 1604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": 4,
      "content": "publicity than their cohorts but are useful in their intended roles nonetheless.\nLet’s quickly run down some of the stragglers in this category.\nSets\nPython sets are neither mappings nor sequences; rather, they are unordered\ncollections of immutable (technically, “hashable”) objects, which store each\nobject just once. You create sets by calling the built-in set function with a\nsequence or other iterable, or by using a set literal expression, and sets support\nthe usual mathematical set operations:\n>>> X = set('hack')                    # Sequence => set\n>>> Y = {'a', 'p', 'p'}                # Set literal\n>>> X, Y\n({'c', 'k', 'a', 'h'}, {'p', 'a'})\n>>> X & Y, X | Y                       # Intersection, union\n({'a'}, {'p', 'c', 'k', 'h', 'a'})\n>>> X - Y, X > Y                       # Difference, superset\n({'c', 'k', 'h'}, False)\nEven less mathematically inclined programmers often find sets useful for\ncommon tasks such as filtering out duplicates, isolating differences, and\nperforming order-neutral equality tests without sorting:\n>>> list(set([3, 1, 2, 1, 3, 1]))      # Duplicates removal\n[1, 2, 3]\n>>> set('code') - set('hack')          # Collection difference\n{'d', 'o', 'e'}\n>>> set('code') == set('deoc')         # Order-neutral equality\nTrue\nAs you’ll see later in this part of the book, normal sets themselves are mutable\nand can be changed (with their remove and add methods, for example), though\nthe immutable items within them by definition cannot.\nBooleans and None\nPython also comes with Booleans, with predefined True and False objects that",
      "content_length": 1566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": 4,
      "content": "are essentially just the integers 1 and 0 with custom display logic; as well as a\nspecial placeholder object called None, commonly used to initialize names and\nobjects and designate an absence of a result in functions:\n>>> 1 > 2, 1 < 2                    # Booleans\n(False, True)\n>>> bool('hack')                    # All objects have a Boolean value\nTrue                                # Nonempty means True\n>>> X = None                        # None placeholder\n>>> print(X)                        # But None is a thing\nNone\n>>> L = [None] * 100                # Initialize a list of 100 Nones\n>>> L\n[None, None, None, None, None, None, None, None, None, None, None, None,\nNone, None, None, None, None, None, None, None, …etc: a list of 100 Nones…]\nTypes\nOne last core object merits a callout here. The type object, returned by the type\nbuilt-in function, is an object that gives the type of another object. We used it\nearlier when exploring the help function, but here’s its actual result:\n>>> L = [1, 2, 3]\n>>> type(L)                         # The type of a list object\n<class 'list'>\n>>> type(type(L))                   # Even types are objects!\n<class 'type'>\nBesides allowing you to explore your objects interactively, the type object in its\nmost practical application allows code to check the types of the objects it\nprocesses. In fact, there are at least three ways to do so in a Python script:\n>>> type(L) == type([])             # Type testing, if you must…\nTrue                                # Using a real object\n>>> type(L) == list                 # Using a type name\nTrue\n>>> isinstance(L, list)             # The object-oriented way\nTrue\nBut now that this book has shown you all these ways to do type testing, it’s",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": 4,
      "content": "required by Python law to tell you that doing so is almost always the wrong\nthing to do in a Python program (and often a sign of an ex-Java programmer first\nstarting to use Python!). The reason won’t become completely clear until later in\nthe book when we start writing larger code units like functions, but it’s a—and\nperhaps the—core Python concept. By checking for specific types in your code,\nyou effectively break its flexibility: you limit it to working on just one type.\nWithout such checks, your code may be able to work on a whole range of types\nautomatically.\nThis is part of the polymorphism mentioned earlier, and it stems from Python’s\nlack of type declarations. As you’ll learn more when we step up to coding\nfunctions and classes, in Python, we code to object interfaces (operations\nsupported), not to types. That is, we care what an object does, not what it is. Not\ncaring about specific types means that code can be applied to many of them: any\nobject with a compatible interface will work, regardless of its specific type.\nAlthough type checking is supported—and even required in some rare cases—\nyou’ll see that it’s not usually the “Pythonic” way of thinking. In fact, you’ll\nprobably find that polymorphism is the key to using Python well.\nType Hinting\nThat being said, Python has slowly accumulated a type-declaration facility\nknown as type hinting, based originally on its earlier function annotations and\ninspired by the TypeScript dialect of JavaScript. With these syntax and module\nextensions, it is possible to name expected object types of function arguments\nand results, attributes in class-based objects, and even simple variables in Python\ncode, and these hints may be used by external type checkers like mypy:\n>>> x: int = 1           # Optional hint: x might be an integer\n>>> x = 'anything'       # But it doesn't have to be!\nImportantly, though, Python type hinting is meant only for documentation and\nuse by third-party tools. The Python language does not itself mandate or use type\ndeclarations and has no plans to ever do so. Hence, type hinting by most\nmeasures is largely academic, no more useful than in-program comments, and\noddly contrary to Python’s core ideas. The fact that it was nevertheless elevated\nto language syntax and complex subdomain arguably does a disservice to Python",
      "content_length": 2326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": 4,
      "content": "learners and users alike. Especially for beginners, this is an optional and\nperipheral topic that’s best deferred until you master the flexibility of Python’s\ndynamic typing. We’ll study it only briefly in this book, in Chapter 6.\nTo be sure, type hinting does not mean that Python is no longer dynamically\ntyped. Indeed, a statically typed Python that requires type declarations would not\nbe a Python at all! Some programmers accustomed to restrictive languages may\nregrettably code Python type hints anyhow as a hard-to-break habit (or\nmisguided display of prowess), but good programmers focus instead on Python’s\npolymorphism. As you’ll find, it’s how to code Python in Python.\nUser-Defined Objects\nWe won’t study object-oriented programming (OOP) in Python and its class\nstatement until later in this book. In abstract terms, though, classes define new\ntypes of objects that extend the core set, so they merit a passing glance here.\nSuppose, for example, that you wish to have a kind of object that models\nworkers in a company. Although there is no such specific core object type in\nPython (it’s not an HR language, after all), a user-defined class might fit the bill:\n>>> class Worker:\n        …stay tuned for Part VI…\nMost of this code is omitted because it wouldn’t make much sense at this point\nin the book, but such a class might define attributes of workers like name and\npay, as well as behavior coded as custom methods. Calling the class would\ngenerate objects that are instances of our new type, and the class’s methods\nwould process them:\n>>> sue = Worker('Sue Jones', 60000)         # Make two new objects \n>>> bob = Worker('Bob Smith', 50000)         # Each has a name and pay\n>>> sue.lastName()                           # Call a method to process sue\n'Jones'\n>>> bob.lastName()                           # Call a method to process bob\n'Smith'\n>>> sue.giveRaise(.10)                       # Update sue's pay\n>>> sue.pay                                  # Display sue's pay\n66000.0",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": 4,
      "content": "This is called object-oriented, because there is always an implied subject in\nfunctions within a class. Class-based objects ultimately use built-in objects\ninternally, and we can always describe things like workers with Python’s built-in\nobjects instead, as we did with dictionaries and lists earlier. Classes, though,\nimplement operations with meaningful names, add structure to your code, and\ncome with inheritance mechanisms that lend themselves to customization by\nextension. In OOP, we strive to extend software by writing new classes, not by\nchanging what already works.\nAll of which is well beyond the bounds of this object preview, though, so we\nmust stop short here. For full disclosure on user-defined object types coded with\nclasses, you’ll have to read on. Because classes build upon other tools in Python,\nthey are one of the major destinations of this book’s journey.\nAnd Everything Else\nAs mentioned earlier, everything you can process in a Python script is a type of\nobject, so our object-type tour is necessarily incomplete. However, even though\neverything in Python is an “object,” not everything is considered a part of\nPython’s core toolset. Other object types in Python either are related to program\nexecution (like functions, modules, classes, and compiled code), or are\nimplemented by imported module functions, not language syntax. The latter of\nthese also tend to have application-specific roles—text patterns, database\ninterfaces, network connections, and so on.\nMoreover, keep in mind that the objects we’ve met here are objects, but not\nnecessarily object-oriented—a concept that usually requires the Python class\nstatement, which you’ll meet again later in this book. Still, Python’s core objects\nare the workhorses of all Python scripts you’re likely to meet and are often the\nbasis of larger noncore objects.",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": 4,
      "content": "Chapter Summary\nAnd that’s a wrap for our object tour. This chapter has previewed Python’s core\nobject types and the sorts of operations we can apply to them. We’ve studied\ngeneric operations that work on many object types (sequence operations such as\nindexing and slicing, for example), as well as type-specific operations available\nas method calls (string splits and list appends, for instance). We’ve also defined\nsome key terms, such as immutability, sequences, and polymorphism.\nAlong the way, we’ve learned that Python’s core object types are more flexible\nand powerful than what is available in lower-level languages. For instance,\nPython’s lists and dictionaries can nest, grow and shrink, and contain objects of\nany type, and their space is automatically created and cleaned up as you go.\nWe’ve also glimpsed the ways that strings and files work hand in hand to support\nbinary and text data, peeked at the iteration protocol and OOP, discussed the\nperils of type hinting, and introduced the if and for statements we’ll be using\nahead.\nThis chapter skipped many of the details in order to provide a first tour, so you\nshouldn’t expect all of this chapter to have made sense yet. In the next few\nchapters, we’ll start to dig deeper, taking a second pass over Python’s object\ntypes that will fill in details omitted here and give you a deeper understanding.\nWe’ll start off the next chapter with an in-depth look at Python numbers. First,\nthough, here is another quiz to review.\nTest Your Knowledge: Quiz\nWe’ll explore the concepts introduced in this chapter in more detail in upcoming\nchapters, so we’ll just cover the big ideas here:\n1. Name four of Python’s core object types.\n2. Why are they called “core” object types?\n3. What does “immutable” mean, and which three of Python’s object types\nare considered immutable?",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": 4,
      "content": "4. What does “sequence” mean, which objects fall into this category, and\nhow is “iterable” related?\n5. What does “mapping” mean, and which core object type is a mapping?\n6. What is “polymorphism,” and why should you care?\nTest Your Knowledge: Answers\n1. Numbers, strings, lists, dictionaries, tuples, files, and sets are generally\nconsidered to be the core object types. Booleans, None, and types\nthemselves are classified this way as well. Some of these types are\nreally categories: there are multiple number types (integer, floating\npoint, complex, fraction, and decimal) and multiple string types (text\nstrings, byte strings, and mutable byte strings).\n2. They are known as “core” object types because they are part of the\nPython language itself and are always available. To create other objects,\nyou generally must call functions in imported modules. Most of the core\nobjects have specific syntax for generating their objects: 'hack', for\nexample, is an expression that makes a string and determines the set of\noperations that can be applied to it. Because of this, core objects are\nhardwired into Python’s syntax. In contrast, you must call the built-in\nopen function to create a file object, even though this is usually\nconsidered a core object type too.\n3. An “immutable” object is an object that cannot be changed after it is\ncreated. Numbers, strings, and tuples in Python fall into this category.\nWhile you cannot change an immutable object in place, you can always\nmake a new one by running an expression. bytearrays offer mutability\nfor strings, but they only apply directly to text if it’s a simple 8-bit kind\n(e.g., ASCII).\n4. A “sequence” is a positionally ordered collection of objects. Strings,\nlists, and tuples are all sequences in Python. They share common\nsequence operations, such as indexing, concatenation, and slicing, but\nalso have type-specific method calls. The related term “iterable” means",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": 4,
      "content": "either a physical sequence, or a virtual one that produces its items on\nrequest. Sequences are iterable, but so are generators, files, results of\nfunctions like range, and the dictionary object (which produces its keys\nwhen iterated, just like its keys method).\n5. The term “mapping” denotes an object that maps keys to associated\nvalues. Python’s dictionary is the only mapping among its core object\ntypes. Mappings retain insertion order (as of Python 3.7), and support\naccess to data stored by key, plus type-specific method calls that enable\nkey tests, iteration, and more.\n6. “Polymorphism” means that the meaning of an operation (like a +)\ndepends on the objects being operated on. This turns out to be a key\nidea (and perhaps the largest) behind using Python well—not\nconstraining code to specific types makes that code automatically\napplicable to many types. This becomes more obvious when coding\nfunctions and classes in Python. While Python today has type hinting,\nit’s not used by Python itself and is meant only for documentation and\ntools.",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": 4,
      "content": "Chapter 5. Numbers and\nExpressions\nThis chapter begins our in-depth tour of the Python language. In Python, data\ntakes the form of objects—either built-in objects that Python provides or objects\nwe create using Python tools and other languages like C. In fact, objects are the\nbasis of every Python program you will ever write. Because they are the most\nfundamental notion in Python programming, objects are also our first focus in\nthis book.\nIn the preceding chapter, we took a quick first pass over Python’s core object\ntypes. Although essential terms were introduced in that chapter, we avoided\ncovering too many specifics in the interest of space. Here, we’ll begin a more\ncareful second look at object concepts, to fill in details we glossed over earlier.\nLet’s get started by exploring our first category: Python’s numeric objects and\noperations.\nNumeric Object Basics\nMost of Python’s numeric support is fairly typical and will probably seem\nfamiliar if you’ve used almost any other programming language in the past.\nThey can be used to keep track of your bank balance, the distance to Mars, the\nnumber of visitors to your website, and just about any other numeric quantity.\nIn Python, numbers are not really a single object type, but a category of similar\ntypes. Python supports the usual numeric types (integers and floating points), as\nwell as literals for creating numbers and expressions for processing them. In\naddition, Python provides more advanced numeric programming support and\nobjects for more advanced needs. A fairly complete inventory of Python’s\nnumeric toolbox includes:\nInteger and floating-point objects\nComplex number objects",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": 4,
      "content": "Decimal fixed-precision objects\nFraction rational number objects\nSet objects and operations\nBoolean and bitwise operations\nBuilt-in modules, such as math, cmath, random, and statistics\nThird-party add-ons, including vectors, visualization, plotting, and\nextended precision\nBecause the object types in this list’s first bullet item tend to see the most action\nin Python code, this chapter starts with basic numbers and fundamentals, then\nmoves on to explore other types on this list, which serve specialized roles. We’ll\nalso study sets here, which have both numeric and collection qualities, but are\ngenerally considered more the former than the latter. Before we jump into code,\nthough, the next few sections get us started with a brief overview of how we\nwrite and process numbers in our scripts.\nNumeric Literals\nAmong its basic object types, Python provides integers, which are positive and\nnegative whole numbers, and floating-point numbers, which are numbers with a\nfractional part (sometimes called floats for verbal economy). Python also allows\nus to write integers using hexadecimal, octal, and binary literals; offers a\ncomplex number type; and allows integers to have unlimited precision—they can\ngrow to have as many digits as your memory space allows. Table 5-1 shows\nwhat Python’s numeric types look like when written out in a program as literals\nor constructor-function calls.\nTable 5-1. Numeric literals and constructors\nLiteral\nInterpretation\n1234, −24, 0, 9_999_999_999_999\nIntegers (unlimited size)\n1.23, 1., 3.14e-10, 4E210, 4.0e+210\nFloating-point numbers",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": 4,
      "content": "0o177, 0x9ff, 0b101010\nOctal, hex, and binary literals\n3+4j, 3.0+4.0j, 3J\nComplex number literals\nset('hack'), {1, 2, 3, 4}\nSets: constructors and literals\nDecimal('1.0'), Fraction(1, 3)\nDecimal and fraction extension types\nbool(X), True, False\nBoolean type and constants\nIn general, Python’s numeric type literals are straightforward to write, but a few\ncoding concepts are worth highlighting up front:\nInteger and floating-point literals\nIntegers are written as strings of decimal digits. As noted, they have\nprecision (number of digits) limited only by your device’s available memory.\nYou can easily compute 2 raised to the power 1,000,000, though with 300K\ndigits, it may take some time to print (and can’t be converted to a print string\ntoday by default, per Chapter 4).\nFloating-point numbers have a decimal point and/or an optional signed\nexponent introduced by an e or E and followed by an optional sign. If you\nwrite a number with a decimal point or exponent, Python makes it a floating-\npoint object and uses floating-point (not integer) math when the object is\nused in an expression. As you’ll learn, mixing a floating-point number with\nan integer does floating-point math too, after converting the integer up.\nHexadecimal, octal, and binary literals\nIntegers may be coded in decimal (base 10), hexadecimal (base 16), octal\n(base 8), or binary (base 2), the last three of which are common in some\nprogramming domains. Hexadecimals start with a leading 0x or 0X, followed\nby a string of hexadecimal digits (0–9 and A–F). Hex digits may be coded in\nlowercase or uppercase. Octal literals start with a leading 0o or 0O (zero and\nlowercase or uppercase letter o), followed by a string of octal digits (0–7).\nBinary literals begin with a leading 0b or 0B, followed by binary digits (0–1).",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": 4,
      "content": "All of these literals produce integer objects in program code; they are just\nalternative syntaxes for specifying values. The built-in calls hex(I), oct(I),\nand bin(I) convert an integer to its representation string in these three\nbases, and int(str, base) converts a runtime string to an integer per a\ngiven base.\nComplex numbers\nThough used more rarely, Python complex literals are written as\nrealpart+imaginarypart, where the imaginarypart is terminated with a\nj or J. The realpart is technically optional, so the imaginarypart may\nappear on its own. Internally, complex numbers are implemented as pairs of\nfloating-point numbers, but all numeric operations perform complex math\nwhen applied to complex numbers. Complex numbers may also be created\nwith the complex(real, imag) built-in call.\nCoding other numeric types\nAs you’ll see later in this chapter, there are additional numeric types near the\nend of Table 5-1 that serve more advanced or specialized roles. You create\nsome of these by calling functions in imported modules (e.g., decimals and\nfractions), others have literal syntax all their own (e.g., sets), and Booleans\nare a kind of specialized integer.\nBuilt-in Numeric Tools\nBesides the built-in number literals and construction calls shown in Table 5-1,\nPython provides a set of tools for processing number objects:\nExpression operators\n+, -, *, /, >>, **, &, %, etc.\nBuilt-in mathematical functions",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": 4,
      "content": "pow, abs, round, int, hex, bin, etc.\nUtility modules\nrandom, math, statistics, etc.\nYou’ll meet all of these as we go along.\nAlthough numbers are primarily processed with expressions, built-ins, and\nmodules, they also have a handful of type-specific methods today, which you’ll\nmeet in this chapter. Floating-point numbers, for example, have an\nas_integer_ratio method useful for the fraction number type, and an\nis_integer method to test if the number is an integer. Integer attributes include\na bit_length method that gives the number of bits necessary to represent the\nobject’s value, and as part collection and part number, sets support both\nexpressions and methods too.\nSince expressions are the most essential tool for most number types, though, let’s\nturn to them next.\nPython Expression Operators\nThe most fundamental tool that processes numbers is the expression: a\ncombination of numbers (or other objects) and operators that computes a value\nwhen executed by Python. In Python, you write expressions using the usual\nmathematical notation and operator symbols. For instance, to add two numbers X\nand Y you would say X + Y, which tells Python to apply the + operator to the\nvalues named by X and Y. The result of the expression is the sum of X and Y,\nanother number object.\nTable 5-2 lists all the operator expressions available in Python, abstractly. Many\nare self-explanatory; for instance, the usual mathematical operators (+, −, *, /,\nand so on) are supported. A few will be familiar if you’ve used other languages\nin the past: % computes a division remainder, << performs a bitwise left-shift, &\ncomputes a bitwise AND result, and so on. Others are more Python specific, and\nnot all are numeric in nature: for example, the is operator tests object identity",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": 4,
      "content": "(i.e., same address in memory, a strict form of equality), and lambda creates\nunnamed functions.\nTable 5-2. Python expression operators, by increasing precedence (binding)\nOperators\nDescription\nyield x, yield from x\nGenerator function send protocol\nx := y\nAssignment expression\nlambda args: expression\nAnonymous function generation\nx if y else z\nTernary selection (x is evaluated only if y is true)\nx or y\nLogical OR (y is evaluated only if x is false)\nx and y\nLogical AND (y is evaluated only if x is true)\nnot x\nLogical negation\nx in y, x not in y\nx is y, x is not y\nx < y, x <= y, x > y, x >= y\nx == y, x != y\nMembership (iterables)\nObject identity tests\nMagnitude comparison, set subset and superset\nValue equality operators\nx | y\nBitwise OR, set union, dictionary merge\nx ^ y\nBitwise XOR, set symmetric difference\nx & y\nBitwise AND, set intersection\nx << y, x >> y\nShift x left or right by y bits\nx + y\nx – y\nAddition, concatenation\nSubtraction, set difference\nx * y\nx % y\nMultiplication, repetition\nRemainder, format",
      "content_length": 1022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": 4,
      "content": "x / y, x // y\nx @ y\nDivision: true and floor\nMatrix multiplication (unused by Python)\n−x, +x, ˜x\nNegation, identity\nBitwise NOT (inversion)\nx ** y\nPower (exponentiation)\nawait x\nAwait expression (async functions)\nx[i]\nx[i:j:k]\nx(...)\nx.attr\nIndexing (sequence, mapping, others)\nSlicing\nCall (function, method, class, other callable)\nAttribute reference\n(...)\n[...]\n{...}\nTuple, expression, generator expression\nList, list comprehension\nDictionary, set, dictionary and set comprehensions\nWhile Table 5-2 works as a reference, some of its operators won’t make sense\nuntil you’ve seen them in action, and some are more subtle than the table may\nimply. For instance:\nParentheses are required for yield if it’s not alone on the right side of\nan assignment statement, as well as the := named-assignment operator\nif it’s used in some contexts.\nComparison operators compare all parts of collections automatically\nand may be chained as a shorthand and potential optimization (e.g., X <\nY < Z produces the same result as X < Y and Y < Z).\nPython defines an @ operator meant for matrix multiplication but does\nnot provide an implementation for it; unless you code one in a class or\nuse a library that does, this operator does nothing.\nThe parentheses used for tuples, expressions, and generators may\nsometimes be omitted; when omitted for tuples, the comma separating",
      "content_length": 1356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": 4,
      "content": "its items acts like a lowest-precedence operator if not otherwise\nsignificant.\nSome operators, like yield, lambda, and await, have to do with larger\ntopics that have little to do with numbers and can safely be ignored at\nthis early point in your Python career.\nThis book will defer to Python’s manuals for other minutiae, but you’ll see most\nof the operators in Table 5-2 in action later. First, though, we need to take a\nquick look at the ways these operators may be combined in expressions.\nNOTE\nMeet the Python no-ops: The @ character is used by Python to introduce function decorators\n(covered later in this book), but not as an expression operator—despite its being specified as\nsuch. In the latter role, @ joins Ellipsis (...) and type hinting as tools defined but wholly\nunused by Python itself. As if you didn’t have enough to learn with the real stuff!\nMixed Operators: Precedence\nAs in most languages, in Python, you code more complex expressions by\nstringing together the operator expressions in Table 5-2. For instance, the sum of\ntwo multiplications might be written as a mix of variables and operators:\nA * B + C * D\nWhich raises the question: how does Python know which operation to perform\nfirst? The answer to this question lies in operator precedence. When you write\nan expression with more than one operator, Python groups its parts according to\nwhat are called precedence rules, and this grouping determines the order in\nwhich the expression’s parts are computed. To denote this, Table 5-2 is ordered\nby operator precedence:\nOperators lower in the table have higher precedence, and so bind more\ntightly in mixed expressions. Put another way, operators higher in the\ntable have lower precedence and bind less tightly than those below\nthem.",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": 4,
      "content": "Operators in the same row in the table generally group from left to right\nwhen combined (except for exponentiation, which groups right to left,\nand comparisons, which chain left to right).\nSo, for example, if you write X + Y * Z, Python evaluates the multiplication\nfirst (Y * Z) then adds that result to X, because * has higher precedence (is lower\nin the table) than +. Similarly, in this section’s original example, both\nmultiplications (A * B and C * D) will happen before their results are added\nbecause + is above *.\nParentheses Group Subexpressions\nYou can largely forget about precedence rules if you’re careful to group parts of\nexpressions with parentheses. When you enclose subexpressions in parentheses,\nyou override Python’s precedence rules; Python always evaluates expressions in\nparentheses first before using their results in the enclosing expressions.\nFor instance, instead of coding X + Y * Z, you could write one of the following\nto force Python to evaluate the expression in either desired order:\n(X + Y) * Z\nX + (Y * Z)\nIn the first case, + is applied to X and Y first, because this subexpression is\nwrapped in parentheses. In the second case, the * is performed first (just as if\nthere were no parentheses at all). Generally speaking, adding parentheses in\nlarge expressions is a good idea—it not only forces the evaluation order you\nwant, but also aids readability.\nMixed Types Are Converted Up\nBesides mixing operators in expressions, you can also mix numeric types. For\ninstance, you can add an integer to a floating-point number:\n40 + 3.14\nBut this leads to another question: what type is the result—integer or floating",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": 4,
      "content": "point? The answer is simple, especially if you’ve used almost any other language\nbefore: in mixed-type numeric expressions, operands (the parts of the expression\nthat aren’t operators) are first converted up to the type of the most complicated\noperand, and then the math is performed on same-type operands. The result is\nthat of the up-converted operands.\nFor this, Python ranks the complexity of numeric types like so: integers are\nsimpler than floating-point numbers, which are simpler than complex numbers.\nSo, when an integer is mixed with a floating point, as in the preceding example,\nthe integer is converted up to a floating-point value first, and floating-point math\nyields the floating-point result. See for yourself in your local Python REPL:\n>>> 40 + 3.14       # Integer to float, float math/result\n43.14\nSimilarly, any mixed-type expression where one operand is a complex number\nresults in the other operand being converted up to a complex number, and the\nexpression yields a complex result. Conversions also run in equality and\nmagnitude comparisons: 3 == 3.0 is true, but 3 > 3.0 is not.\nYou can force the issue by calling built-in functions to convert types manually:\n>>> int(3.1415)     # Truncates float to integer\n3\n>>> float(3)        # Converts integer to float\n3.0\nHowever, you won’t usually need to do this: because Python automatically\nconverts up to the more complex type within an expression, the results are\nnormally what you want.\nWhile automatic conversions are run for both numeric and comparison\noperators, keep in mind that they apply only when mixing numeric objects (e.g.,\nan integer and a float) in an expression. In general, Python does not convert\nacross any other type boundaries automatically. Adding a string of digits to an\ninteger, for example, results in an error, unless you manually convert one or the\nother; watch for an example and rationale when we explore strings in Chapter 7.\nEquality tests do work on mixed types (e.g., a string is never equal to any\ninteger), but magnitude comparisons do not.",
      "content_length": 2047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": 4,
      "content": "Preview: Operator Overloading and Polymorphism\nAlthough we’re focusing on built-in numbers right now, all Python operators\nmay be overloaded (i.e., implemented) by Python classes and C extension types\nto work on objects you create. For instance, you’ll see later that objects coded\nwith classes may be added or concatenated with x+y expressions, indexed with\nx[i] expressions, and so on.\nFurthermore, Python itself automatically overloads some operators, such that\nthey perform different actions depending on the type of built-in objects being\nprocessed. For example, the + operator performs addition when applied to\nnumbers but performs concatenation when applied to sequence objects like\nstrings and lists. In fact, + can mean anything at all when applied to objects you\ndefine with classes.\nAs we saw in the prior chapter, this property is usually called polymorphism—a\nterm indicating that the meaning of an operation depends on the type of the\nobjects being processed. We’ll revisit this concept when we explore functions in\nChapter 16, because it becomes a much more obvious feature in that context.\nNumbers in Action\nOn to the code! Probably the best way to understand numeric objects and\nexpressions is to see them in action, so with all the preceding basics in hand,\nlet’s start up the interactive command line and try some simple but illustrative\noperations (be sure to see Chapter 3 for pointers if you need help starting a\nREPL).\nVariables and Basic Expressions\nFirst of all, let’s do the math to demo some basics. In the following interaction,\nwe first assign two variables (a and b) to integers so we can use them later in a\nlarger expression. Variables are simply names—created by you or Python—that\nare used to keep track of information in your program. We’ll say more about this\nin the next chapter, but in Python:\nVariables are created when they are first assigned values.",
      "content_length": 1890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": 4,
      "content": "Variables are replaced with their values when used in expressions.\nVariables must be assigned before they can be used in expressions.\nVariables refer to objects and need not be declared ahead of time.\nIn other words, these assignments cause the variables a and b to spring into\nexistence automatically:\n$ python3                  # Fire up a REPL\n>>> a = 3                  # Name created: no need to declare ahead of time\n>>> b = 4\nThis code also uses comments. Recall from Chapter 3 that in Python code, text\nafter a # mark and continuing to the end of the line is considered to be a\ncomment and is ignored by Python. Comments are one way to write human-\nreadable documentation for your code, and an important part of programming.\nThey describe aspects of the code, salient or subtle, as an aid for others (and you,\nsix months down the road!). In the next part of the book, you’ll also meet a\nrelated feature—documentation strings—that attaches docs to objects so it’s\navailable after your code is loaded.\nAgain, though, because code you type interactively is temporary, you won’t\nnormally write comments in this context. If you’re working along, this means\nyou don’t need to type any of the comment text from the # through to the end of\nthe line in a REPL; it’s not a required part of the statements we’re running this\nway.\nNow, let’s use our new integer objects in expressions. At this point, the values of\na and b are still 3 and 4, respectively. Variables like these are replaced with their\nvalues whenever they’re used inside an expression, and the expression results are\nechoed back immediately and automatically when we’re working interactively:\n>>> a + 1, a − 1           # Addition (3 + 1), subtraction (3 − 1)\n(4, 2)\n>>> b * 3, b / 2           # Multiplication (4 * 3), division (4 / 2)\n(12, 2.0)                  \n>>> a % 2, b ** 2          # Modulus (remainder), power (4 ** 2)\n(1, 16)\n>>> 2 + 4.0, 2.0 ** b      # Mixed-type conversions\n(6.0, 16.0)",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": 4,
      "content": "Per Chapter 3, the results being echoed back here are tuples of two values\nbecause the lines typed at the prompt contain two expressions separated by\ncommas; that’s why the results are displayed in parentheses. More importantly,\nthese expressions work because the variables a and b within them have been\nassigned values. If you use a different variable that has not yet been assigned,\nPython reports an error rather than filling in some default value:\n>>> c * 2\nNameError: name 'c' is not defined\nAs also previewed in Chapter 3, you don’t need to predeclare variables in\nPython, but they must have been assigned at least once before you can use them.\nIn practice, this means you have to initialize counters to zero before you can add\nto them, initialize lists to an empty list before you can append to them, and so\non.\nHere are two slightly larger expressions to illustrate operator grouping and more\nabout conversions:\n>>> b / 2 + a              # Same as ((4 / 2) + 3)\n5.0\n>>> b / (2 + a)            # Same as (4 / (2 + 3))\n0.8\nIn the first expression, there are no parentheses, so Python automatically groups\nthe components according to its precedence rules—because / is lower in\nTable 5-2 than +, it binds more tightly and so is evaluated first. The result is as if\nthe expression had been organized with parentheses as shown in the comment to\nthe right of the code. In the second expression, parentheses are added around the\n+ part to force Python to evaluate it first (i.e., before the /).\nAlso, notice that all the numbers are integers in each of these examples. Python’s\n/ performs true division, which always retains fractional remainders and gives a\nfloating-point result—which is in turn reflected in the result of the whole\nexpression. You can force a fractional result by coding 2.0 instead of 2 but don’t\nhave to. You can also opt to use floor division by coding these examples with //\ninstead of /, and Python will discard decimal digits in the result; because results\nreflect the types of operands, you’ll get back truncated floating-point for floats:",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": 4,
      "content": ">>> a, b                   # Same, original values\n(3, 4)\n>>> b // 2 + a             # Floor division: integer\n5\n>>> b // (2 + a)           # Truncates fraction (for positives)\n0\n>>> b // 2.0 + a           # Auto-conversions: floating-point\n5.0\n>>> b // (2.0 + a)\n0.0\nYou’ll learn more about division later in this section.\nNumeric Display Formats\nOnce you start playing with Python numbers in earnest, the results of some\nexpressions may look a bit odd the first time you see them:\n>>> 1.1 + 2.2               # What's up with the 3 at the end?\n3.3000000000000003\n>>> print(1.1 + 2.2)        # Same for prints\n3.3000000000000003\nThe full story behind this odd result has to do with the limitations of floating-\npoint hardware and its inability to exactly represent some values in a limited\nnumber of bits. Python floating-point numbers map to the underling chips on\nyour device and are only as accurate as those chips allow—a physical constraint\nthat can be addressed with add-ons that extend floating-point precision, as well\nas techniques discussed in “Floating-point equality”.\nBecause computer architecture is well beyond this book’s scope, though, we’ll\nfinesse this by saying that your computer’s floating-point hardware is doing the\nbest it can, and neither it nor Python is in error here. In fact, this is partly a\ndisplay issue—Python’s floating-point display logic tries to be intelligent and\nusually shows fewer decimal digits, but occasionally cannot. Our earlier\nexamples gave fewer digits automatically, and you can always force the issue in\nprograms with string formatting:\n>>> num = 1.1 + 2.2",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": 4,
      "content": ">>> num                          # Auto-echoes (and prints)\n3.3000000000000003\n>>> '%e' % num                   # String-formatting expression\n'3.300000e+00'\n>>> '%.1f' % num                 # Alternative floating-point format\n'3.3'\n>>> f'{num:e}', f'{num:.1f}'     # F-strings (see also format method)\n('3.300000e+00', '3.3')\nThe last three tests here employ flexible string formatting, which we will explore\nin full in the upcoming chapter on strings (Chapter 7). Its results are strings that\nare typically, but not always, printed to displays or reports.\nDISPLAY FORMATS: STR AND REPR\nAlthough it’s not yet obvious in this chapter, the default output format of\ninteractive echoes and print technically correspond to the built-in repr and\nstr functions, respectively:\n>>> repr('hack')           # Used by echoes: as-code form\n\"'hack'\"\n>>> str('hack')            # Used by print: user-friendly form\n'hack'\nBoth of these convert arbitrary objects to their string representations: repr\n(and the default interactive echo) produces results that look as though they\nwere code; str (and the print operation) converts to a typically more user-\nfriendly format if available. Some objects have both—a str for general use,\nand a repr with extra details. This notion will resurface when we explore\nboth strings and operator overloading in classes.\nBesides providing print strings for arbitrary objects, the str built-in is also\nthe name of the string object type, which can be called with an encoding\nname to decode a Unicode string from a byte string as an alternative to the\nbytes.decode method introduced briefly in Chapter 4. You’ll learn more\nabout this advanced str role in Chapter 37.",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": 4,
      "content": "Comparison Operators\nSo far, we’ve been dealing with standard numeric operations (e.g., addition and\nmultiplication), but numbers, like all Python objects, can also be compared.\nNormal comparisons work for numbers exactly as you’d expect—they compare\nthe relative magnitudes of their operands and return a Boolean result, which we\nwould normally test and take action on in a larger statement and program (e.g.,\nsee the intro to if in Chapter 4):\n>>> 1 < 2                  # Less than (magnitude)\nTrue\n>>> 2.0 >= 1               # Greater than or equal: mixed-type 1 converted to 1.0\nTrue\n>>> 2.0 == 2.0             # Equal value\nTrue\n>>> 2.0 != 2.0             # Not equal value\nFalse\nNotice again how mixed types are allowed in numeric expressions (only); in the\nsecond test here, Python compares values in terms of the more complex type,\nfloat.\nChained comparisons\nInterestingly, Python also allows us to chain multiple comparisons together to\nperform range tests. Chained comparisons are a sort of shorthand for larger\nBoolean expressions. In short, Python lets us string together magnitude\ncomparison tests to code chained comparisons such as range tests. The\nexpression (A < B < C), for instance, tests whether B is between A and C,\nnoninclusively; it is equivalent to the Boolean test (A < B and B < C) but is\neasier on the eyes (and the keyboard). For example, assume the following\nassignments:\n>>> X = 2\n>>> Y = 4\n>>> Z = 6\nThe following two expressions have identical effects, but the first is shorter to\ntype, and it may run slightly faster since Python needs to evaluate Y only once—\nand it may matter if Y is a call to a complicated function:",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": 4,
      "content": ">>> X < Y < Z              # Chained comparisons: range tests\nTrue\n>>> X < Y and Y < Z\nTrue\nThe same equivalence holds for false results, and arbitrary chain lengths are\nallowed:\n>>> X < Y > Z\nFalse\n>>> X < Y and Y > Z\nFalse\n>>> 1 < 2 < 3.0 < 4\nTrue\n>>> 1 > 2 > 3.0 > 4\nFalse\nYou can use other comparisons in chained tests, but the resulting expressions can\nbecome nonintuitive unless you evaluate them the way Python does. The first of\nthe following, for instance, is false just because 1 is not equal to 2:\n>>> 1 == 2 < 3               # Same as: (1 == 2) and (2 < 3), but not: False < 3!\nFalse      \n>>> True is False is True    # Same as: (True is False) and (False is True)\nFalse\nPython does not compare the 1 == 2 expression’s False result to 3—this would\ntechnically mean the same as 0 < 3, which would be True (you’ll learn more\nabout the True and False objects later in this chapter and explore the rarely used\nis identity operator in the next).\nFloating-point equality\nOne last note here before we move on: chaining aside, numeric comparisons are\nbased on magnitudes, which are generally simple—though floating-point\nnumbers may not always work as you’d expect, and may require conversions or\nother massaging to be compared meaningfully:\n>>> 1.1 + 2.2 == 3.3              # Shouldn't this be True?\nFalse\n>>> 1.1 + 2.2                     # Close to 3.3, but not exactly: limited precision",
      "content_length": 1398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": 4,
      "content": "3.3000000000000003\nThis is related to the earlier coverage of numeric display formats and stems from\nthe fact that floating-point numbers cannot represent some values exactly due to\ntheir limited number of bits—a fundamental issue in numeric programming not\nunique to Python. To accommodate this imprecision in equality tests, either\ntruncate, round, use floors, or, as of Python 3.5, import and call the math\nstandard-library module’s isclose, which is true if values are within a tolerance\nof each other (there’s more on math and floors ahead, and more on isclose in\nPython’s manuals):\n>>> int(1.1 + 2.2) == int(3.3)        # OK if convert: see also floor, trunc ahead\nTrue\n>>> round(1.1 + 2.2, 1) == round(3.3, 1)\nTrue\n>>> import math                       # Import modules to call their functions\n>>> math.isclose(1.1 + 2.2, 3.3)      # Within default-but-passable tolerances\nTrue\nWe’ll revisit this later in this chapter when we meet decimals and fractions,\nwhich can also address such limitations. First, though, let’s continue our tour of\nPython’s core numeric operations, with a deeper look at division.\nDivision Operators\nPython has two division operators introduced earlier, as well as one that’s\nstrongly related. Here’s the whole gang:\nX / Y\nCalled true division, this always keeps remainders in floating-point results,\nregardless of types.\nX // Y\nCalled floor division, this always truncates fractional remainders down to\ntheir floor, regardless of types, and its result type depends on the types of its\noperands.",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": 4,
      "content": "X % Y\nCalled modulus, this returns a division’s remainder, with a result type that\nvaries per operand types. This also does formatting when used on strings, per\nChapter 7.\nThe following demos the two division operators at work:\n>>> 10 / 4              # True div: keeps remainder always\n2.5\n>>> 10 / 4.0            # Same for floats\n2.5\n>>> 10 // 4             # Floor div: drops remainder always\n2\n>>> 10 // 4.0           # Same for floats, but type varies\n2.0\nNotice that the object type of the result for // is dependent on its operand’s\ntypes: if either is a float, the result is a float; otherwise, it is an integer. If you\nwant to ensure an integer result, simply wrap the expression in int to convert:\n>>> int(10 // 4.0)\n2\nThe related modulus returns the remainder of division with a type to match\noperands (useful when your code needs to know how much is “left over” after a\n//), and the divmod built-in function gives both parts when needed:\n>>> 10 % 3, 10 % 3.0    # Remainder of division: (3 * 3) + 1\n(1, 1.0)\n>>> divmod(10, 3)       # Both parts of division in a tuple\n(3, 1)\nFloor versus truncation\nOne subtlety here: the // operator is informally called truncating division, but\nit’s more accurate to refer to it as floor division—it truncates the result down to\nits floor, which means the closest whole number below the true result. The net\neffect is to round down, not strictly truncate, and this matters for negatives. You",
      "content_length": 1439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": 4,
      "content": "can see the difference for yourself with the Python math module (as you’ve\nlearned, modules must be imported before you can use their contents):\n>>> import math\n>>> math.floor(2.5)           # Closest number below value\n2\n>>> math.floor(-2.5)          # But not truncation for negative!\n-3\n>>> math.trunc(2.5)           # Truncate fractional part (toward zero)\n2\n>>> math.trunc(-2.5)          # And is truncation for negative\n-2\nWhen running division operators, you only really truncate for positive results,\nsince truncation is then the same as floor; for negatives, it’s a floor result. Really,\nthey are both floor, but floor just happens to be the same as truncation for\npositives (cut-and-pasters: if minus signs morph to Unicode dashes in this book,\nreplace them with simple ASCII “-” hyphens to run; Python requires the latter\nfor the minus sign, and tools are notorious for botching this):\n>>> 5 / 2, 5 / -2             # True division keeps remainders\n(2.5, −2.5)\n>>> 5 // 2, 5 // -2           # Truncates to floor: rounds to first lower integer\n(2, −3)\n>>> 5 / 2.0, 5 / -2.0         # Ditto for floats\n(2.5, −2.5)\n>>> 5 // 2.0, 5 // -2.0       # Though result is float too\n(2.0, −3.0)\nIf you really want truncation toward zero regardless of sign, you can always run\na true division result through math.trunc (as demoed earlier, the round built-in\nhas related functionality and the int built-in has the same effect, and neither\nrequires an import):\n>>> import math\n>>> 5 / −2                    # Keep remainder\n−2.5\n>>> 5 // −2                   # Floor below result\n-3\n>>> math.trunc(5 / −2)        # Truncate instead of floor (same as int())",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": 4,
      "content": "−2\nSo why the fuss over truncation? This won’t be obvious until you graduate to\nwriting larger Python programs later in this book, but it’s an essential tool in\nsome use cases. Watch for a prime-number while loop example in Chapter 13\nand a corresponding exercise at the end of Part IV that wholly rely on the\ntruncating behavior of //.\nInteger Precision\nPython division may come in multiple flavors, but it’s still fairly standard as\nprogramming languages go. Here’s something a bit more unusual. As mentioned\nearlier, Python integers support unlimited size:\n>>> 999999999999999999999999999999 + 1\n1000000000000000000000000000000\nUnlimited-precision integers are a convenient built-in tool. For instance, you can\nuse them to count your country’s national debt in Python without numeric-value\noverflow (which is, of course, more impressive and resource intensive in some\nlocales than others). More universally, a 2 raised to the power 269 isn’t\nparticularly large, but nearly breaches this page’s width limits, and much larger\nnumbers work sans the safeguards against DOS attacks noted in Chapter 4:\n>>> 2 ** 269\n948568795032094272909893509191171341133987714380927500611236528192824358010355712\n>>> x = 2 ** 1000000\n>>> x\nValueError: Exceeds the limit (4300 digits) for integer string conversion; \nuse sys.set_int_max_str_digits() to increase the limit\nBecause Python must do extra work to support the extended precision, integer\nmath is usually substantially slower than normal when numbers grow large.\nHowever, if you need the precision, the fact that it’s built in for you to use will\nlikely outweigh its performance penalty.\nComplex Numbers",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": 4,
      "content": "Although less commonly used than the types we’ve been exploring thus far,\ncomplex numbers are a distinct core object type in Python. They are typically\nused in engineering and science applications. If you know what they are, you\nknow why they are useful; if not, consider this section optional reading (until\nthey appear in code you must reuse).\nComplex numbers are represented as two floating-point numbers—the real and\nimaginary parts—and you code them by adding a j or J suffix to the imaginary\npart. We can also write complex numbers with a nonzero real part by adding the\ntwo parts with a +. For example, the complex number with a real part of 2 and an\nimaginary part of −3 is written 2 + −3j. Here are some examples of complex\nmath at work:\n>>> 1j * 1J\n(-1+0j)\n>>> 2 + 1j * 3\n(2+3j)\n>>> (2 + 1j) * 3\n(6+3j)\nComplex numbers also allow us to extract their parts as attributes (via attributes\nreal and imag), support all the usual mathematical expressions, and may be\nprocessed with tools in the standard cmath module (the complex analogue of the\nstandard math module). Because complex numbers are rare in most\nprogramming domains, though, we’ll skip the rest of this story here. Check\nPython’s language reference manual for additional details.\nHex, Octal, and Binary\nAs previewed near the start of this chapter, Python integers can be coded in\nhexadecimal, octal, and binary notation, in addition to the normal base-10\ndecimal coding we’ve been using so far. The first three of these may at first seem\nforeign to 10-fingered beings, but some programmers find them convenient\nalternatives for specifying values, especially when their mapping to bytes and\nbits is important. We detailed coding rules before; let’s try these out live.\nAs noted earlier, these other-base literals are simply an alternative syntax for\nspecifying the value of an integer object. For example, the following literals",
      "content_length": 1895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": 4,
      "content": "produce normal integers with the specified values. In memory, an integer’s value\nis the same, regardless of the base we use to specify it in our code:\n>>> 0x01, 0x10, 0xFF            # Hex literals: base 16, digits 0-9/A-F\n(1, 16, 255)\n>>> 0o1, 0o20, 0o377            # Octal literals: base 8, digits 0-7\n(1, 16, 255)\n>>> 0b1, 0b10000, 0b11111111    # Binary literals: base 2, digits 0-1\n(1, 16, 255)\nHere, the hex value 0xFF, the octal value 0o377, and the binary value\n0b11111111 are all decimal 255. The F digits in the hex value, for example, each\nmean 15 in decimal and a 4-bit 1111 in binary, and reflect powers of 16. Thus,\nthe hex value 0xFF and others convert to decimal values as follows:\n>>> 0xFF, (15 * (16 ** 1)) + (15 * (16 ** 0))     # How hex/binary map to decimal\n(255, 255)\n>>> 0x2F, (2  * (16 ** 1)) + (15 * (16 ** 0))\n(47, 47)\n>>> 0xF, 0b1111, (1*(2**3) + 1*(2**2) + 1*(2**1) + 1*(2**0))\n(15, 15, 15)\nPython prints integer values in decimal (base 10) by default, but it also provides\nbuilt-in functions that convert integers to other bases’ digit strings formatted per\nPython-literal syntax—useful when programs or users expect to see values in a\ngiven base:\n>>> oct(64), hex(64), bin(64)               # Numbers=>digit strings\n('0o100', '0x40', '0b1000000')\nThe oct function converts decimal to octal, hex to hexadecimal, and bin to\nbinary—all as strings. To go the other way, the built-in int function converts a\nstring of digits to an integer, and an optional second argument lets you specify\nthe numeric base—useful for numbers read from files as strings instead of coded\nin scripts:\n>>> 64, 0o100, 0x40, 0b1000000              # Digits=>numbers in scripts and strings\n(64, 64, 64, 64)\n>>> int('64'), int('100', 8), int('40', 16), int('1000000', 2)",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": 4,
      "content": "(64, 64, 64, 64)\n>>> int('0x40', 16), int('0b1000000', 2)    # Literal forms supported too\n(64, 64)\nThe eval function can also be used to convert digit strings to numbers, because\nit treats strings as though they were Python code. Therefore, it has a similar\neffect, but usually runs more slowly—it actually compiles and runs the string as\na piece of a program, and it assumes the string being run comes from a trusted\nsource—a clever user might be able to submit a string that deletes files on your\nmachine. In other words, be sparing and careful with this call:\n>>> eval('64'), eval('0o100'), eval('0x40'), eval('0b1000000')\n(64, 64, 64, 64)\nFinally, you can also convert integers to base-specific strings with any of\nPython’s three string-formatting tools, though you’ll have to take this partly on\nfaith until we reach strings’ full coverage in Chapter 7:\n>>> '%o, %x, %#X' % (64, 255, 255)          # Numbers=>digits formatting*3\n'100, ff, 0XFF'\n>>> '{:o}, {:b}, {:x}, {:#X}'.format(64, 64, 255, 255)\n'100, 1000000, ff, 0XFF'\n \n>>> f'{64:o}, {64:b}, {255:x}, {255:#X}'    # The newest latest-and-greatest\n'100, 1000000, ff, 0XFF'\nIn this code, o, b, and x format as octal, binary, and hex, respectively, and #X\nadds a base prefix and uses uppercase. As an aside, you can avoid the repeated\ninputs in each of these three formatting tools (but you’re probably starting to see\nwhy picking just one is generally a good idea—and why feature redundancy is\ngenerally a bad idea!):\n>>> '%(i)o, %(j)x, %(j)#X' % dict(i=64, j=255)\n'100, ff, 0XFF'\n>>> '{0:o}, {0:b}, {1:x}, {1:#X}'.format(64, 255)\n'100, 1000000, ff, 0XFF'\n>>> f'{(i:=64):o}, {i:b}, {(i:=255):x}, {i:#X}'\n'100, 1000000, ff, 0XFF'",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": 4,
      "content": "Before we move on, keep in mind that other-base literals and converters support\narbitrarily large integers too. The following, for instance, creates an integer in\nhex and displays it in decimal and octal and binary with converters:\n>>> X = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n>>> X\n5192296858534827628530496329220095\n>>> oct(X)\n'0o17777777777777777777777777777777777777'\n>>> bin(X)\n'0b111111111111111111111111111111111111111111111111111111111 ...and so on... 11111'\nSpeaking of binary digits, the next section takes us on a tour of tools that process\nnumbers’ individual bits.\nBitwise Operations\nBesides the normal numeric operations (addition, subtraction, and so on), Python\nsupports most of the numeric expressions available in the C language. This\nincludes operators that treat integers as strings of binary bits and can come in\nhandy if your Python code must deal with things like network packets, serial\nports, or packed binary data produced by or intended for a C program.\nWe can’t dwell on the fundamentals of Boolean math here—again, those who\nmust use it probably already know how it works, and others can often postpone\nthe topic altogether—but the basics are straightforward. For instance, here are\nsome of Python’s bitwise expression operators at work performing bitwise shift\nand Boolean operations on integers:\n>>> x = 1                # 1 decimal is 0001 in bits\n>>> x << 2               # Shift left 2 bits: 0100\n4\n>>> x | 3                # Bitwise OR (either bit=1): 0001 | 0011\n3\n>>> x & 3                # Bitwise AND (both bits=1): 0001 & 0011\n1\nIn the first expression, a binary 1 (in base 2, 0001) is shifted left two slots to\ncreate a binary 4 (0100). The last two operations perform a binary OR to\ncombine bits (0001|0011 = 0011) and a binary AND to select common bits",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": 4,
      "content": "(0001&0011 = 0001). Such bit-masking operations allow us to encode and\nextract multiple flags and other values within a single integer.\nThis is one area where the binary and hexadecimal number support in Python\nbecome especially useful—they allow us to code and inspect numbers by bit-\nstrings:\n>>> X = 0b0001           # Binary literals\n>>> X << 2               # Shift left\n4\n>>> bin(X << 2)          # Binary digits string\n'0b100'\n>>> bin(X | 0b0011)      # Bitwise OR: either\n'0b11'\n>>> bin(X & 0b11)        # Bitwise AND: both\n'0b1'\nThis is also true for values that begin life as hex literals, or undergo base\nconversions:\n>>> X = 0xFF             # Hex literals\n>>> bin(X)\n'0b11111111'\n>>> X ^ 0b10101010       # Bitwise XOR: either but not both\n85\n>>> bin(X ^ 0b10101010)\n'0b1010101'\n>>> int('01010101', 2)   # Digits=>number: string to int per base\n85\n>>> hex(85)              # Number=>digits: Hex digit string\n'0x55'\nAlso in this department, Python integers come with a bit_length method,\nwhich allows you to query the number of bits required to represent the number’s\nvalue in binary. Sharp-eyed readers might point out that you can often achieve\nthe same effect by subtracting 2 from the length of the bin string using the len\nbuilt-in function we first used in Chapter 4 (to account for the leading “0b”),\nthough its temporary string result may make it less efficient:\n>>> X = 99\n>>> bin(X), X.bit_length(), len(bin(X)) - 2",
      "content_length": 1437,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": 4,
      "content": "('0b1100011', 7, 7)\n>>> bin(256), (256).bit_length(), len(bin(256)) - 2\n('0b100000000', 9, 9)\nWe won’t go into much more detail on such “bit twiddling” here. It’s supported\nif you need it, but bitwise operations are often not as important in a high-level\nlanguage such as Python as they are in a low-level language such as C. As a rule\nof thumb, if you find yourself wanting to flip bits in Python, you should think\nabout which language you’re really coding. As you’ll see in upcoming chapters,\nPython’s lists, dictionaries, and the like provide richer ways to encode\ninformation than bit strings, especially when your data’s audience includes\nreaders of the human variety.\nUnderscore Separators in Numbers\nIf you’re finding it hard to read the longer digit strings in this chapter, there’s\nsome good news: as of 3.6, numeric literals in Python can be coded with\nembedded underscores (“_”) to group digits for easier viewing. These\nunderscores don’t modify number values; Python simply discards them after\nreading your code. They do, however, work on all the numbers and bases we’ve\nmet, including complex-number parts and float-point decimal digits, and can\nenhance the readability of numeric literals in your scripts. Here’s how they look\n—with before on the left and after on the right:\n>>> 9999999999999 == 9_999_999_999_999                 # Decimal: thousands\nTrue\n>>> 0xFFFFFFFF == 0xFF_FF_FF_FF                        # Hex: 8-bit bytes\nTrue\n>>> 0o777777777777 == 0o777_777_777_777                # Octal: 9 bits each\nTrue \n>>> 0b1111111111111111 == 0b1111_1111_1111_1111        # Binary: 4-bit nibbles\nTrue\n>>> 3.141592653589793 == 3.141_592_653_589_793         # Float: decimal digits\nTrue\n>>> 123456789.123456789 == 123_456_789.123_456_789     # Float: both sides\nTrue\nWhile this is a useful feature, you should keep in mind that it’s just skin deep.\nFor example, numbers lose their underscores once read. You can add back\ncomma and underscore separators with the string-formatting method and f-string",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": 4,
      "content": "we’ll explore in Chapter 7, but this is mostly just for display, and the originals\nare lost:\n>>> x = 9_999_998              # Your number with \"_\"s for digit groupings\n>>> x                          # But dropped when read: not in displays\n9999998\n>>> x + 1                      # Ditto for derived computation results\n9999999\n>>> f'{x:,} and {x:_}'         # Formatting adds separators, but just for show\n'9,999,998 and 9_999_998'\nMoreover, Python doesn’t do any sort of sanity checks on underscores, except\nfor disallowing leading, trailing, and multiple-appearance uses. The underscores\nare really just digit “spacers” that can be used—and misused—arbitrarily:\n>>> 99_9                       # No position-error checking provided\n999\n>>> 1_23_456_7890              # Hmm…\n1234567890\n>>> _9\nNameError: name '_9' is not defined. Did you mean: '_'?\n>>> 9_\nSyntaxError: invalid decimal literal\n>>> 9_9__9\nSyntaxError: invalid decimal literal\n>>> 9_9_9                      # Syntax oddities checked, semantics not\n999\n>>> hex(0xf_ff_fff_f_f)        # And Python won't retain your \"_\"s\n'0xffffffff'\nAlso bear in mind that commas cannot be used in numeric literals you code—\ndespite their similarity, underscores are essentially ignored as documentation, but\ncommas are taken to be tuple item separators if erroneously used:\n>>> 12_345_678                 # Underscores are ignored \n12345678\n>>> 12,345,678                 # But commas mean a tuple!\n(12, 345, 678)\nUnderscores can enhance readability to be sure, but they are largely cosmetic",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": 4,
      "content": "and apply only to large numeric literals in your code—because typical Python\nprograms compute most numbers rather than hardcoding them, this seems likely\nto be uncommon in practice. A more promising use case is input—string-to-\nnumber converters allow underscores too:\n>>> int('1_234_567')           # Works in text read from data files too\n1234567\n>>> eval('1_234_567')          # But does raw-data readability matter?\n1234567\n>>> float('1_2_34.567_8_90')\n1234.56789\nBut it’s difficult to justify underscores on data alone, given that scripts could\nsimply strip underscores themselves. Like all such tools, use when it makes\nsense (and don’t be shocked if this crops up in unfair interview questions!).\nOther Built-in Numeric Tools\nIn addition to its core object types, Python also provides both built-in functions\nand standard-library modules for numeric processing. The pow and abs built-in\nfunctions, for instance, compute powers and absolute values, respectively. Here’s\na brief roundup of common tools in the built-in math module (which contains\nmost of the tools in the C language’s math library), along with a few numeric\nbuilt-in functions:\n>>> import math\n>>> math.pi, math.e                               # Common constants\n(3.141592653589793, 2.718281828459045)\n>>> math.sin(2 * math.pi / 180)                   # Sine, tangent, cosine\n0.03489949670250097\n>>> math.sqrt(144), math.sqrt(2)                  # Square root\n(12.0, 1.4142135623730951)\n>>> pow(2, 4), 2 ** 4, 2.0 ** 4.0                 # Exponentiation (power)\n(16, 16, 16.0)\n>>> abs(-62.0), sum((1, 2, 3, 4))                 # Absolute value, summation\n(62.0, 10)\n>>> min(3, 1, 2, 4), max(3, 1, 2, 4)              # Minimum, maximum",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": 4,
      "content": "(1, 4)\nThe sum function shown here works on a sequence (really, iterable) of numbers,\nand min and max accept either a collection or individual arguments. There are\nalso multiple ways to drop the decimal digits of floating-point numbers, both for\ncalculations and displays; some of these are richer than previously shown:\n>>> math.floor(2.567), math.floor(-2.567)            # Floor: next-lower integer\n(2, −3)\n>>> math.trunc(2.567), math.trunc(−2.567)            # Truncate: drop digits \n(2, −2)\n>>> int(2.567), int(−2.567)                          # Truncate: alternative\n(2, −2)\n>>> round(2.567), round(2.567, 2), round(2567, -3)   # Round to digits (+/-)\n(3, 2.57, 3000)\n>>> '%.1f' % 2.567, '{0:.2f}'.format(2.567)         # Format display (Chapter 7)\n('2.6', '2.57')\nAs shown earlier, the last of these produces strings that we would usually print\nand supports a variety of formatting options. String formatting is still subtly\ndifferent, though: round rounds and drops decimal digits but still produces a\nnumber in memory, whereas string formatting produces a string, not a number:\n>>> (1 / 3.0), round(1 / 3.0, 2), f'{(1 / 3.0):.2f}'\n(0.3333333333333333, 0.33, '0.33')\nInterestingly, there are three ways to compute square roots in Python: using a\nmodule function, an expression, or a built-in function (if you’re interested in\nperformance, we will revisit these in an exercise and its solution at the end of\nPart IV, to see which runs quicker):\n>>> import math\n>>> math.sqrt(144)              # Module\n12.0\n>>> 144 ** .5                   # Expression\n12.0\n>>> pow(144, .5)                # Built-in\n12.0",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": 4,
      "content": "Notice that standard-library modules such as math must be imported, but built-in\nfunctions such as abs and round are always available without imports. This is\nbecause modules are external components, but built-in functions live in an\nimplied namespace that Python automatically searches to find names used in\nyour program. This namespace simply corresponds to the standard-library\nmodule called builtins, and there is much more about name resolution in the\nfunction and module parts of this book; for now, when you hear “module,” think\n“import.”\nThe standard library’s statistics and random modules must be imported as\nwell. Both modules provide an array of tools; statistics supports operations\ncommonly found on calculators, and random enables tasks such as picking a\nrandom number between 0 and 1 and selecting a random integer between two\nnumbers:\n>>> import statistics\n>>> statistics.mean([1, 2, 4, 5, 7])        # Average, median\n3.8\n>>> statistics.median([1, 2, 4, 5, 7])      # And a whole lot more: see its docs\n4\n>>> import random\n>>> random.random()\n0.5566014960423105\n>>> random.random()              # Random floats, integers, choices, shuffles\n0.051308506597373515\n>>> random.randint(1, 10)\n5\n>>> random.randint(1, 10)\n9\nThe random module can also choose an item at random from a sequence, and\nshuffle a list of items randomly:\n>>> random.choice(['Pizza', 'Tacos', 'Tikka', 'Lasagna'])\n'Tikka'\n>>> random.choice(['Pizza', 'Tacos', 'Tikka', 'Lasagna'])\n'Lasagna'\n>>> suits = ['hearts', 'clubs', 'diamonds', 'spades']\n>>> random.shuffle(suits)",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": 4,
      "content": ">>> suits\n['spades', 'hearts', 'diamonds', 'clubs']\n>>> random.shuffle(suits)\n>>> suits\n['clubs', 'diamonds', 'hearts', 'spades']\nThough we’d need additional code to make this more tangible here, the random\nmodule can be useful for shuffling cards in games, picking images at random in\na slideshow GUI, performing statistical simulations, and much more. We’ll\ndeploy it again later in this book (e.g., in Chapter 20’s permutations case study),\nbut for more details, consult Python’s library manual.\nOther Numeric Objects\nSo far in this chapter, we’ve been using Python’s core numeric types—integer,\nfloating point, and complex. These will suffice for most of the number crunching\nthat many programmers will ever need to do. Python comes with a handful of\nmore exotic numeric types, though, that merit a brief look here.\nDecimal Objects\nFirst up, is Python’s special-purpose numeric object known formally as Decimal\n(and informally as decimal). Syntactically, decimals are created by calling a\nfunction within an imported standard-library module, rather than running a\nliteral expression. Functionally, decimals are like floating-point numbers, but\nthey have a fixed and configurable number of decimal digits. Hence, decimals\nare fixed-precision floating-point values.\nFor example, with decimals, we can have a floating-point value that always\nretains just two decimal digits. Furthermore, we can specify how to round or\ntruncate the extra decimal digits beyond the object’s cutoff. Although it\ngenerally incurs a performance penalty compared to normal floating point,\ndecimal is well suited to representing fixed-precision quantities like sums of\nmoney and can achieve better numeric accuracy in some contexts.\nDecimal basics\nAs we learned when we explored comparisons, floating-point math is less than",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": 4,
      "content": "exact because of the limited space used to store values. For instance, the\nfollowing should yield zero, but it does not. The result is close to zero, but there\nare not enough bits to be precise here:\n>>> 0.1 + 0.1 + 0.1 - 0.3                       # Almost zero, but not quite\n5.551115123125783e-17\nUsing print for the user-friendly display format doesn’t help here, because the\nhardware related to floating-point math is inherently limited in terms of accuracy\n(a.k.a. precision). With decimals, however, the result can be dead-on:\n>>> from decimal import Decimal\n>>> Decimal('0.1') + Decimal('0.1') + Decimal('0.1') - Decimal('0.3')\nDecimal('0.0')\nAs shown here, we can make decimal objects by calling the Decimal constructor\nfunction in the decimal module and passing in strings that have the desired\nnumber of decimal digits for the resulting object (using the str function to\nconvert floating-point values to strings if needed). When decimals of different\nprecision are mixed in expressions, Python converts up to the largest number of\ndecimal digits automatically:\n>>> Decimal('0.1') + Decimal('0.10') + Decimal('0.1000') - Decimal('0.30')\nDecimal('0.0000')\nIt’s also possible to create a decimal object from a floating-point object, with\neither a call to Decimal.from_float or by passing floating-point numbers\ndirectly:\n>>> Decimal(0.1) + Decimal(0.1) + Decimal(0.1) - Decimal(0.3)\nDecimal('2.775557561565156540423631668E-17')\nThe conversion is exact but can yield a large default number of digits, unless\nthey are fixed per the next section.\nSetting decimal precision\nOther tools in the decimal module can be used to set the precision of all decimal",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": 4,
      "content": "numbers, arrange error handling, and more. For instance, a context object in this\nmodule allows for specifying precision (number of decimal digits) and rounding\nmodes (down, ceiling, etc.). The precision is applied globally for all decimals\ncreated by the caller:\n>>> import decimal\n>>> decimal.Decimal(1) / decimal.Decimal(7)                     # Default precision\nDecimal('0.1428571428571428571428571429')\n>>> decimal.getcontext().prec = 4                               # Fixed precision\n>>> decimal.Decimal(1) / decimal.Decimal(7)\nDecimal('0.1429')\n>>> Decimal(0.1) + Decimal(0.1) + Decimal(0.1) - Decimal(0.3)   # Closer to 0…\nDecimal('1.110E-17')\nTechnically, significance is determined by digits input, and precision is applied\non math operations. Although more subtle than we can explore in this brief\noverview, this property can make decimals useful as the basis for some monetary\napplications and may sometimes serve as an alternative to manual rounding and\nstring formatting.\nBecause use of the decimal type is relatively rare in practice, though, this book\nwill defer to Python’s interactive help function and standard-library manuals for\nmore details. And because decimals address some of the same floating-point\naccuracy issues as the fraction type, let’s move on to the next section to see how\nthe two compare.\nFraction Objects\nPython’s standard-library fractions module implements a rational number\nobject. It essentially keeps both a numerator and a denominator explicitly, so as\nto avoid some of the inaccuracies and limitations of floating-point math. Like\ndecimals, fractions do not map as closely to computer hardware as floating-point\nnumbers. This means their performance may not be as good, but it also allows\nthem to provide extra utility in a standard tool where useful.\nFraction basics\nFraction is a functional cousin to the Decimal fixed-precision object of the",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": 4,
      "content": "prior section, as both can be used to address the floating-point object’s numerical\ninaccuracies. It’s also used in similar ways—like Decimal, Fraction resides in a\nmodule; import its constructor and pass in a numerator and a denominator to\nmake one (among other schemes). The following interaction shows how:\n>>> from fractions import Fraction\n>>> x = Fraction(1, 3)                    # Numerator, denominator\n>>> y = Fraction(4, 6)                    # Simplified to 2, 3 by gcd\n>>> x\nFraction(1, 3)\n>>> y\nFraction(2, 3)\n>>> print(y)\n2/3\nOnce created, Fractions can be used in mathematical expressions as usual:\n>>> x + y\nFraction(1, 1)\n>>> x − y                           # Results are exact: numerator, denominator\nFraction(−1, 3)\n>>> x * y\nFraction(2, 9)\nFraction objects can also be created from floating-point number strings, much\nlike decimals:\n>>> Fraction('.25')\nFraction(1, 4)\n>>> Fraction('1.25')\nFraction(5, 4)\n>>> Fraction('.25') + Fraction('1.25')\nFraction(3, 2)\nNumeric accuracy in fractions and decimals\nFraction math is different from floating-point-type math, which is constrained by\nthe underlying limitations of floating-point hardware. To compare, here are the\nsame operations run with floating-point objects, and notes on their limited\naccuracy—they may display fewer digits in recent Pythons than they used to, but",
      "content_length": 1339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": 4,
      "content": "they still aren’t exact values in memory:\n>>> a = 1 / 3                       # Only as accurate as floating-point hardware\n>>> b = 4 / 6                       # Can lose precision over many calculations\n>>> a\n0.3333333333333333\n>>> b\n0.6666666666666666\n>>> a + b\n1.0\n>>> a - b\n-0.3333333333333333\n>>> a * b\n0.2222222222222222\nThis floating-point limitation is especially apparent for values that cannot be\nrepresented accurately given their limited number of bits in memory. Both\nFraction and Decimal provide ways to get exact results, albeit at the cost of\nsome lost speed and added code verbosity. For instance, in the following\nexample (repeated from the prior section), floating-point numbers do not\naccurately give the zero answer expected, but both of the other types do:\n>>> 0.1 + 0.1 + 0.1 - 0.3           # This should be zero (close, but not exact)\n5.551115123125783e-17\n>>> from fractions import Fraction\n>>> Fraction(1, 10) + Fraction(1, 10) + Fraction(1, 10) - Fraction(3, 10)\nFraction(0, 1)\n>>> from decimal import Decimal\n>>> Decimal('0.1') + Decimal('0.1') + Decimal('0.1') - Decimal('0.3')\nDecimal('0.0')\nMoreover, fractions and decimals both allow more intuitive and accurate results\nthan floating points sometimes can, in different ways—by using rational\nrepresentation and by limiting precision:\n>>> 1 / 3                           # Normal floating-point\n0.3333333333333333\n>>> Fraction(1, 3)                  # Numeric accuracy, two ways\nFraction(1, 3)",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": 4,
      "content": ">>> import decimal\n>>> decimal.getcontext().prec = 2\n>>> Decimal(1) / Decimal(3)\nDecimal('0.33')\nIn fact, fractions both retain accuracy and automatically simplify results.\nContinuing the preceding interaction:\n>>> (1 / 3) + (6 / 12)\n0.8333333333333333\n>>> Fraction(1, 3) + Fraction(6, 12)\nFraction(5, 6)\n>>> decimal.Decimal(1 / 3) + decimal.Decimal(6 / 12)\nDecimal('0.83')\nTo support conversions, floating-point objects have an as_integer_ratio\nmethod noted earlier that yields numerator and denominator; fractions have a\nfrom_float method; and float accepts a Fraction as an argument. Because\nFraction is also a lesser-used utility, though, we’re going to stop short here too;\nfor more details on Fraction, experiment further on your own and consult\nPython’s documentation.\nSet Objects\nIn addition to all the numeric objects we’ve explored, Python has built-in\nsupport for sets—an unordered collection of unique and immutable objects that\nsupports operations corresponding to mathematical set theory. Sets straddle the\nfence between collections and math but lean far enough on the latter side to\nwarrant coverage in this chapter.\nBy definition, an item appears only once in a set, no matter how many times it is\nadded. Accordingly, sets have a variety of applications, especially in numeric\nand database-focused work. On the other hand, because sets are collections of\nother objects, they share some behavior with objects such as lists and\ndictionaries previewed in Chapter 4. For example, sets are iterable, can grow and\nshrink on demand, and may contain a variety of object types.\nStill, because sets are unordered and do not map keys to values, they are neither",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": 4,
      "content": "sequence nor mapping types; they are a type category unto themselves.\nMoreover, because sets also tend to be used much less often than pervasive\nobjects like lists and dictionaries, a brief look should suffice for most readers;\nlet’s get started here with the usual REPL tour.\nSets in action\nFirst off, there are two ways to make sets—by call and literal. The literal uses\nthe same “{}” braces as dictionaries but simply enumerates items (there is no\nkey) and allows you to initialize the set with individual objects. The call to set\naccepts an existing sequence (or other iterable) of items to add to the new set and\nis required to make an empty set ({} is reserved for an empty dictionary). When\nsets are printed, they prefer the literal form, except when empty:\n>>> x = set('abcde')                    # Make a set by calling its type/function\n>>> y = {99, 'b', 'y', 'd', 1.2}        # Make a set by literal\n>>> x \n{'d', 'c', 'e', 'a', 'b'}               # Order is scrambled, displays literal\n>>> y\n{1.2, 99, 'y', 'd', 'b'}\nNotice that sets don’t maintain insertion order, unlike the keys in a dictionary\n(see Chapter 4’s introduction). This is by definition—sets are just groups of\nitems—but the lack of positional ordering means that sequence operations won’t\nwork on sets. Per the following, empty sets require a call, a * in a literal unpacks\nitems, both call and * accept any iterable, and sets always filter out duplicate\nentries; again, this is just how sets work in Python and elsewhere:\n>>> z = set()                           # Make empty set: {} is empty dictionary\n>>> z\nset()\n>>> z = set([1.2, 'a', 3, 1.2, 'a'])    # Any sequence works, duplicates dropped\n>>> z\n{'a', 1.2, 3}\n>>> {1, *'abc', *[1, 2, 3]}             # Literal star unpacking (Python 3.5+)\n{1, 2, 3, 'c', 'b', 'a'}\nOnce you have sets, expression operators invoke set operations. Here are the",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": 4,
      "content": "most common in action; to run any of these on plain sequences like strings and\nlist, you must first create a set of their items:\n>>> x = set('abcd')\n>>> y = set('bdxy')\n>>> x – y                           # Difference: in x, not in y\n{'a', 'c'}\n \n>>> x | y                           # Union: in either\n{'y', 'd', 'x', 'c', 'a', 'b'}\n \n>>> x & y                           # Intersection: in both\n{'d', 'b'}\n \n>>> x ^ y                           # Symmetric difference: not in both\n{'y', 'x', 'c', 'a'}\n  \n>>> x < y, x > y                    # Superset, subset tests\n(False, False)\nAn exception: the in set membership test expression is also defined to work on\nall other collection types, where it also performs membership (or a search, if you\nprefer to think in procedural terms). Hence, we do not need to convert things like\nstrings and lists to sets to run this test:\n>>> 'd' in x                                      # Membership (sets)\nTrue\n>>> 'd' in 'code', 2 in [1, 2, 3]                 # But works on other types too\n(True, True)\nIn addition to expressions, the set object provides methods that correspond to\nthese operations and more, and that support set changes. For instance, the set add\nmethod inserts one item, update is an in-place union, and remove deletes an\nitem by value (per the prior chapter, run a dir call on any set instance or the set\ntype name to see all the available methods). Assuming x and y are still as they\nwere in the prior interaction:\n>>> z = x.intersection(y)                         # Same as x & y\n>>> z\n{'d', 'b'}",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": 4,
      "content": ">>> z.add('HACK')                                 # Insert one item\n>>> z\n{'HACK', 'd', 'b'}\n>>> z.update(set(['X', 'Y']))                     # Merge: in-place union\n>>> z\n{'X', 'HACK', 'd', 'b', 'Y'}\n>>> z.remove('b')                                 # Delete one item\n>>> z\n{'X', 'HACK', 'd', 'Y'}\nSets also are iterable (i.e., they support the iteration protocol introduced in the\nprior chapter) and hence can also be used in operations such as len, for loops,\nand list comprehensions. Because they are unordered, though, they don’t support\nsequence operations like indexing, slicing, or concatenation:\n>>> for item in set('abc'):                       # See Chapter 4 for \"for\"\n        print(item * 3)\naaa\nccc\nbbb\n>>> {'a', 'b', 'c'} + {'d'}\nTypeError: unsupported operand type(s) for +: 'set' and 'set'\nFinally, although the set expressions shown earlier generally require two sets,\ntheir method-based counterparts can often work with any iterable as well—and\nmay run faster because of it (though speed guesses are perilous in Python):\n>>> S = set([1, 2, 3])\n>>> S | set([3, 4])                # Expressions require both to be sets\n{1, 2, 3, 4}\n>>> S | [3, 4]\nTypeError: unsupported operand type(s) for |: 'set' and 'list'\n>>> S.union([3, 4])                # But their methods allow any iterable\n{1, 2, 3, 4}\n>>> S.intersection((1, 3, 5))\n{1, 3}\n>>> S.issubset(range(-5, 5))       # Subset of range -5...4 series generator\nTrue\n>>> S = set([1, 2, 3])\n>>> S.intersection_update((1, 2, 5))\n>>> S\n{1, 2}",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": 5,
      "content": ">>> \n>>> S |= {1, 2, 4}\n>>> S\n{1, 2, 4}\nFor more details on set operations, see Python’s library manual. Among topics\nskipped here, sets also support in-place changes with additional methods, as well\nas assignment operators we’ll study later (e.g., &= and |=). Although sets can be\ncoded manually in Python with other types like lists and dictionaries (and often\nwere in the past), Python’s built-in sets use efficient algorithms and\nimplementation techniques to provide quick and standard operations.\nImmutable constraints and frozen sets\nSets are powerful and flexible objects, but they do have one constraint that you\nshould keep in mind—largely because of their implementation, sets can contain\nonly immutable object types (Python refers to this as “hashable,” which is close\nenough to “immutable” to use the latter here). Hence, lists and dictionaries\ncannot be embedded in sets, but tuples of other immutables can if you need to\nstore compound values. Tuples compare by full values when used in sets:\n>>> S = {1.23}\n>>> S.add([1, 2, 3])                   # Only immutable objects work in a set\nTypeError: unhashable type: 'list'\n>>> S.add({'a':1})\nTypeError: unhashable type: 'dict'\n>>> S.add((1, 2, 3))\n>>> S                                  # No list or dict; tuple, str, numbers OK\n{1.23, (1, 2, 3)}\n>>> S | {(4, 5, 6), (1, 2, 3)}         # Union: same as S.union(...)\n{1.23, (4, 5, 6), (1, 2, 3)}\n>>> (1, 2, 3) in S                     # Membership: by complete values\nTrue\n>>> (1, 4, 3) in S\nFalse\nTuples in a set, for instance, might be used to represent dates, records, IP\naddresses, and so on (more on tuples later in this part of the book). Sets may also",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": 5,
      "content": "contain modules, type objects, and more. Sets themselves are mutable too, and\nso cannot be nested in other sets directly; if you need to store a set inside another\nset, the frozenset built-in call works just like set but creates an immutable set\nthat cannot change and thus can be embedded in other sets:\n>>> S.add(frozenset('app'))\n>>> S\n{1.23, (1, 2, 3), frozenset({'a', 'p'})}\nSet comprehensions\nIn addition to literals and calls, sets can also be made by running comprehension\nexpressions, previewed briefly in Chapter 4. Comprehensions also work for lists,\ndictionaries, and generators, and behave largely the same in all. For sets,\ncomprehensions are coded in curly braces. When run, they perform a loop that\ncollects the result of an expression on each iteration; a loop variable gives access\nto the current iteration value for use in the collection expression. The result is a\nnew set with all the normal set behavior. For example:\n>>> {x ** 2 for x in [1, 2, 3, 4]}         # Make a new set with a comprehension\n{16, 1, 4, 9}\nIn this expression, the loop is coded on the right, and the collection expression is\ncoded on the left (x ** 2). As for list comprehensions, we get back pretty much\nwhat this expression says: “Give me a new set containing X squared, for every X\nin a list.” Comprehensions can also iterate across other kinds of objects, such as\nstrings; the first of the following examples also illustrates the comprehension-\nbased way to make a set from an existing iterable:\n>>> {x for x in 'py3X'}                    # Same as: set('py3x')\n{'p', 'X', '3', 'y'}\n \n>>> {c * 4 for c in 'py3X'}                # Set of collected expression results\n{'yyyy', '3333', 'XXXX', 'pppp'}\n>>> {c * 4 for c in 'py3X' + 'py2X'}       # Expressions work on both sides\n{'yyyy', '3333', 'XXXX', '2222', 'pppp'}\n>>> S = {c * 4 for c in 'py3X'}            # All set ops work on results\n>>> S | {'zzzz', 'XXXX'}\n{'yyyy', '3333', 'XXXX', 'pppp', 'zzzz'}\n>>> S & {'zzzz', 'XXXX'}",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": 5,
      "content": "{'XXXX'}\nBecause the rest of the comprehensions story relies upon underlying concepts\nwe’re not yet prepared to tackle, we’ll postpone further details until later in this\nbook. In Chapter 8, you’ll meet first cousins, the list and dictionary\ncomprehension, and you’ll learn much more about all comprehensions—set, list,\ndictionary, and generator—later on, especially in Chapters 14 and 20. As you’ll\nfind, all comprehensions support additional syntax not shown here, including\nnested loops and if tests, which can be challenging before you’ve had a chance\nto study larger statements.\nWhy sets?\nSet operations have a variety of common uses, some more practical than\nmathematical. For example, because items are stored only once in a set, sets can\nbe used to filter duplicates out of other collections, albeit at the cost of original\nordering because sets are unordered in general. Simply convert the collection to\na set, and then convert it back:\n>>> L = [1, 2, 1, 3, 2, 4, 5]\n>>> set(L)\n{1, 2, 3, 4, 5}\n>>> L = list(set(L))                                  # Removing duplicates\n>>> L\n[1, 2, 3, 4, 5]\n>>> list(set(['yy', 'cc', 'aa', 'xx', 'dd', 'aa']))   # But order may change\n['xx', 'cc', 'yy', 'aa', 'dd']\nSets can be used to isolate differences in lists, strings, and other iterable objects\ntoo—simply convert to sets and take the difference—though again the unordered\nnature of sets means that the results may not match that of the originals:\n>>> set([1, 3, 5, 7]) - set([1, 2, 4, 5, 6])          # Find list differences\n{3, 7}\n>>> set('abcdefg') - set('abdghij')                   # Find string differences\n{'c', 'e', 'f'}\n>>> set('code') - set(['t', 'o', 'e'])                # Find differences, mixed\n{'c', 'd'}\nYou can also use sets to perform order-neutral equality tests by converting to a",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": 5,
      "content": "set before the test, because order doesn’t matter in a set. More formally, two sets\nare equal if and only if every element of each set is contained in the other—that\nis, each is a subset of the other, regardless of order. For instance, you might use\nthis to compare the outputs of programs that should work the same but may\ngenerate results in different order. Sorting with Python’s sorted built-in before\ntesting has the same effect for equality; sets don’t rely on an expensive sort, but\nalso don’t order items:\n>>> L1, L2 = [1, 3, 5, 2, 4], [2, 5, 3, 4, 1]\n>>> L1 == L2                                          # Order matters in sequences\nFalse\n>>> set(L1) == set(L2)                                # Order-neutral equality\nTrue\n>>> sorted(L1) == sorted(L2)                          # Similar but results ordered\nTrue\n>>> 'code' == 'edoc', set('code') == set('edoc'), sorted('code') == sorted('edoc')\n(False, True, True)\nSets can also be used to keep track of where you’ve already been when\ntraversing a graph or other cyclic structure. For example, the transitive module\nreloader and inheritance-tree lister examples we’ll code in Chapters 25 and 31,\nrespectively, must keep track of items visited to avoid loops, as Chapter 19\ndiscusses in the abstract. Using a list in this context is inefficient because\nsearches require linear scans. Although recording states visited as keys in a\ndictionary is efficient, sets offer an alternative that’s essentially equivalent and\nmay be more intuitive.\nFinally, sets are also convenient when you’re dealing with large data collections\nlike database query results—the intersection of two sets contains objects\ncommon to both categories, and the union contains all items in either set. To\nillustrate, here’s a more tangible example of set operations at work, applied to\npeople in a hypothetical company (like all examples in this book, any\nresemblance to the real world is purely coincidental!):\n>>> engineers = {'pat', 'ann', 'bob', 'sue'}\n>>> managers  = {'sue', 'tom'}\n>>> 'pat' in engineers                   # Is pat an engineer?\nTrue\n>>> engineers & managers                 # Who is both engineer and manager?",
      "content_length": 2159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": 5,
      "content": "{'sue'}\n>>> engineers | managers                 # All people in either category\n{'ann', 'sue', 'pat', 'tom', 'bob'} \n>>> engineers - managers                 # Engineers who are not managers\n{'pat', 'ann', 'bob'}\n>>> managers - engineers                 # Managers who are not engineers\n{'tom'}\n>>> engineers > managers                 # Are all managers engineers? (superset)\nFalse\n>>> {'sue', 'bob'} < engineers           # Are both engineers? (subset)\nTrue\n>>> (managers | engineers) > managers    # All people is a superset of managers\nTrue\n>>> managers ^ engineers                 # Who is in one group but not both?\n{'ann', 'pat', 'tom', 'bob'}\nYou can find more details on set operations in the Python library manual and\nsome mathematical and database texts. Also stay tuned for Chapter 8’s revival of\nset operations we’ve seen here, in the context of dictionary view objects. Here,\nwe have time for just one last numeric object type.\nBoolean Objects\nThough somewhat gray, the Python Boolean type, bool, is arguably numeric in\nnature because its two values, True and False, are just customized versions of\nthe integers 1 and 0 that print themselves differently. Python treats 1 and 0 as\ntrue and false like many programming languages, but its True and False makes\nBoolean roles more explicit. Although that’s all some programmers may need to\nknow, let’s briefly reveal this type’s forgery.\nTo represent truth values, Python has an explicit Boolean type called bool, from\nwhich the objects preassigned to built-in names True and False are made. That\nis, True and False are instances of bool, which is in turn just a subclass (in the\nobject-oriented sense) of the built-in integer type int. True and False behave\nexactly like the integers 1 and 0, except that they have customized printing logic",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": 5,
      "content": "—they print themselves as the words True and False, instead of the digits 1 and\n0. bool accomplishes this by redefining str and repr string formats (introduced\nearlier in this chapter) for its two objects, and all logical tests yield True or\nFalse for their results.\nBecause of this customization, Boolean expressions typed at the interactive\nprompt print results as the words True and False instead of the less obvious 1\nand 0. In addition, Booleans make truth values more apparent in your code. For\ninstance, an infinite loop can be coded as while True: instead of the less\nintuitive while 1:, and flags can be initialized more clearly with flag =\nFalse. We’ll discuss these statements further in Part III.\nAgain, though, for most practical purposes, you can treat True and False as\nthough they are predefined variables set to integers 1 and 0. This implementation\ncan lead to curious results, though; because True is just the integer 1 with a\ncustom display format, True + 4 yields integer 5 in Python:\n>>> type(True)               # True is a bool\n<class 'bool'>\n>>> isinstance(True, int)    # As well as an int\nTrue\n>>> True == 1                # Same value\nTrue\n>>> True is 1                # But a different object: see the next chapter!\nFalse\n>>> True or False            # Same as: 1 or 0\nTrue\n>>> True + 4                 # (Hmmm)\n5\nSince you probably won’t come across an expression like the last of these in real\nPython code, you can safely ignore any of its deeper metaphysical implications.\nWe’ll revisit Booleans in Chapter 9 to define Python’s notion of truth, and again\nin Chapter 12 to see how Boolean operators like and and or work.\nNumeric Extensions\nFinally, although Python’s core numeric objects offer plenty of power for most\napplications, a large catalog of third-party open source extensions is available to",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": 5,
      "content": "address more focused numeric needs.\nWe surveyed tools in this domain in Chapter 1’s section “What Can I Do with\nPython?” In short, there is a now-common stack of tools for advanced numeric\ncoding in Python today, including NumPy, SciPy, pandas, matplotlib, Jupyter,\nand more, and additional tools address subdomains like statistics, astronomy, and\nAI.\nThis toolkit is used by research organizations, financial entities, and aerospace\ngroups around the world, and performs the sort of tasks formerly coded in\nlanguages like C++ or Fortran. Many who work in this field liken the\ncombination of Python plus numeric extensions to a free, flexible, and powerful\nalternative to systems like MATLAB.\nThough a popular and exciting domain, Python numeric programming is just one\nway to use the language (Python web development, for example, is similarly\nsized) and is easily rich enough to fill entire books by itself. Hence, this book\ndoesn’t cover numeric extensions and focuses instead on teaching you the\nPython language that’s used in every domain. Once you’ve learned Python itself,\nyou’ll find copious resources for add-ons both on the web and at book outlets\nnear you when you’re ready to level up.",
      "content_length": 1197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": 5,
      "content": "Chapter Summary\nThis chapter has toured Python’s numeric object types and the operations we can\napply to them. Along the way, we met the trusty integer and floating-point\nobjects, as well as some more exotic and less commonly used types such as\ncomplex numbers, decimals, fractions, and sets. We also explored Python’s\nexpression syntax, type conversions, bitwise operations, and various literal forms\nfor coding numbers in scripts.\nLater in this part of the book, we’ll continue our in-depth object tour by filling in\ndetails about the next object type—the string. In the next chapter, however, we’ll\ntake some time to explore the mechanics of variable assignment in more detail\nthan we have here. This turns out to be perhaps the most fundamental idea in\nPython, so make sure you check out the next chapter before moving on. First,\nthough, it’s time to take the usual chapter quiz.\nTest Your Knowledge: Quiz\n1. What is the value of the expression 2 * (3 + 4) in Python, and why?\n2. What is the value of the expression 2 * 3 + 4 in Python, and why?\n3. What is the value of the expression 2 + 3 * 4 in Python, and why?\n4. What tools can you use to find a number’s square root, as well as its\nsquare?\n5. What is the type of the result of the expression 1 + 2.0 + 3, and why?\n6. How can you truncate and round a floating-point number?\n7. How can you convert an integer to a floating-point number?\n8. How would you display an integer in octal, hexadecimal, or binary\nnotation?\n9. How might you convert an octal, hexadecimal, or binary string to a\nplain integer?",
      "content_length": 1558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": 5,
      "content": "Test Your Knowledge: Answers\n1. The value will be 14, the result of 2 * 7, because the parentheses force\nthe addition to happen before the multiplication.\n2. The value will be 10, the result of 6 + 4. Python’s operator precedence\nrules are applied in the absence of parentheses, and multiplication has\nhigher precedence than (i.e., happens before) addition, per Table 5-2.\n3. This expression yields 14, the result of 2 + 12, for the same precedence\nreasons as in the prior question.\n4. Functions for obtaining the square root, as well as pi, tangents, and\nmore, are available in the imported math module. To find a number’s\nsquare root, import math and call math.sqrt(N). To get a number’s\nsquare, use either the exponent expression X ** 2 or the built-in\nfunction pow(X, 2). Either of these last two can also compute the\nsquare root when given a power of 0.5 (e.g., X ** .5).\n5. The result will be a floating-point number: the integers are converted up\nto floating point, the most complex type in the expression, and floating-\npoint math is used to evaluate it.\n6. The int(N) and math.trunc(N) functions truncate, and the round(N,\ndigits) function rounds. We can also compute the floor with\nmath.floor(N) and round for display with string-formatting\noperations.\n7. The float(I) function converts an integer to a floating point; mixing\nan integer with a floating point within an expression will result in a\nconversion as well. In some sense, Python / true division converts too\n—it always returns a floating-point result that includes the remainder,\neven if both operands are integers.\n8. The oct(I), hex(I), and bin(I) built-in functions return the octal,\nhexadecimal, and binary string forms for an integer. All three flavors of\nstring formatting (expression, method, and f-string) also provide targets\nfor some such conversions.",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": 5,
      "content": "9. The int(S, base) function can be used to convert from octal,\nhexadecimal, and binary digit strings to normal integers (pass in 8, 16,\nor 2 for the base). The eval(S) function can be used for this purpose\ntoo, but it’s more expensive to run and can have security risks. To some\nextent, other-base literals like 0xFFFF and 0b1111 in your code do this\nwork too when read by Python. Note that integers are always stored in\nbinary form in computer memory; these are just display string format\nconversions.",
      "content_length": 503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": 5,
      "content": "Chapter 6. The Dynamic Typing\nInterlude\nIn the prior chapter, we began exploring Python’s core object types in depth by\nstudying Python numbers and their operations. We’ll resume our object type tour\nin the next chapter, but before we move on, it’s important that you get a handle\non what may be the most fundamental idea in Python programming and is\ncertainly the basis of much of both the conciseness and flexibility of the Python\nlanguage: dynamic typing, and the polymorphism it implies.\nAs you’ll see here and throughout this book, in Python, we do not need to\ndeclare the specific types of the objects our scripts use. In fact, most programs\nshould not care about specific types—on purpose. By avoiding constraints this\nway, code naturally works in many contexts and often more than expected.\nBecause dynamic typing is the root of this flexibility, and is also a potential\nstumbling block for newcomers, let’s take a brief side trip to explore the model\nhere. At the end of the trip, we’ll also make a short stop at the paradox of type\nhinting, to learn why you should avoid it.\nThe Case of the Missing Declaration Statements\nIf you have a background in statically typed languages like C, C++, or Java, you\nmight find yourself a bit perplexed at this point in the book. So far, we’ve been\nusing variables without declaring their existence or their types, and it somehow\nworks. When we type a = 3 in an interactive session or program file, for\ninstance, how does Python know that a should stand for an integer? For that\nmatter, how does Python know what a is at all?\nOnce you start asking such questions, you’ve crossed over into the domain of\nPython’s dynamic typing model. In Python, types are determined automatically\nat runtime (“dynamically”), not in response to declarations added to code ahead\nof time (“statically”). This means that you never need to declare variables—a\nconcept that may be simpler to grasp if you keep in mind that it all boils down to",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": 5,
      "content": "variables, objects, and the links between them, as the next section explains.\nVariables, Objects, and References\nAs you’ve seen in many of the examples used so far in this book, when you run\nan assignment statement such as a = 3 in Python, it works even if you’ve never\ntold Python to use the name a as a variable, or that a should stand for an integer-\ntype object. In the Python language, this all pans out in a very natural way, as\nfollows:\nVariable creation\nA variable (also known in Python as a name), like a, is created when your\ncode first assigns it a value. Future assignments change the value of the\nalready created name. Technically, Python detects some names before your\ncode runs (e.g., locals in functions), but you can think of it as though initial\nassignments make variables.\nVariable types\nA variable itself never has any type information or constraints associated\nwith it. In Python, the notion of type lives with objects, not names. Variables\nare generic in nature; they always simply refer to a particular object at a\nparticular point in time.\nVariable use\nWhen a variable appears in an expression, it is immediately replaced with the\nobject that it currently refers to, whatever that may be. Further, all variables\nmust be explicitly assigned before they can be used; referencing unassigned\nvariables results in errors.\nIn sum, variables are created when assigned, can reference any type of object,\nand must be assigned before they are referenced. This means that you never need",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": 5,
      "content": "to declare names used by your script, but you must initialize names before you\ncan update them; counters, for example, must be initialized to zero before you\ncan add to them.\nThis dynamic typing model is strikingly different from the typing model of\ntraditional languages. When you are first starting out, the model is usually easier\nto understand if you keep clear the distinction between names and objects. For\nexample, when we say this to assign a variable a value in a Python REPL or\nscript:\n>>> a = 3                # Assign a name to an object\nat least conceptually, Python will perform three distinct steps to carry out the\nrequest. These steps reflect the operation of all assignments in the Python\nlanguage:\n1. Create an object to represent the value 3.\n2. Create the variable a, if it does not yet exist.\n3. Link the variable a to the new object 3.\nThe net result will be a structure inside Python that resembles Figure 6-1. As\nsketched, variables and objects are stored in different parts of memory and are\nassociated by links (the link is shown as a pointer in the figure). Variables\nalways link to objects and never to other variables, but larger objects may link to\nother objects (for instance, a list object has links to the objects it contains).",
      "content_length": 1261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": 5,
      "content": "Figure 6-1. Names (a.k.a. variables) and objects after running the assignment a = 3\nThese links from variables to objects are called references in Python—a kind of\nassociation, implemented as an object’s address in memory. Whenever variables\nare later used (i.e., referenced), Python automatically follows the variable-to-\nobject links. This is all simpler than the terminology may imply. In concrete\nterms:\nVariables are named entries in a system table, with spaces for links to\nobjects.\nObjects are pieces of allocated memory, with enough space to represent\nthe values for which they stand.\nReferences are automatically followed pointers from variables to\nobjects.\nAt least conceptually, each time you generate a new value in your script by\nrunning an expression, Python creates a new object (i.e., a chunk of memory) to\nrepresent that value. As an optimization, Python internally caches and reuses\ncertain kinds of unchangeable objects, such as small integers and strings (each 0\nis not really a new piece of memory—more on this caching behavior later). But\nfrom a logical perspective, it works as though each expression’s result value is a\ndistinct object and each object is a distinct piece of memory.\nTechnically speaking, objects have more structure than just enough space to\nrepresent their values. Each object also has two standard header fields: a type\ndesignator used to mark the type of the object, and a reference counter used to\ndetermine when it’s OK to reclaim the object. To understand how these two\nheader fields factor into the model, we need to move on.\nPYTHON REFERENCES FOR C PROGRAMMERS\nReaders with a background in C may find Python references similar to C\npointers (i.e., memory addresses). In fact, references are implemented as\npointers by CPython internally, and they often serve the same roles,\nespecially with objects that can be changed in place (more on this later).\nBecause references are always automatically dereferenced when used,",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": 5,
      "content": "though, you can never actually do anything useful with a reference itself. As\nnoted ahead, the referenced object’s address may be returned by the id built-\nin as a unique ID, but even this isn’t guaranteed: see Python’s manuals.\nThis lack of pointers avoids an entire category of notorious C bugs. But you\ncan think of Python references as C “void*” pointers that are automatically\nfollowed when used, without going too far off base.\nTypes Live with Objects, Not Variables\nTo see how object types come into play, watch what happens if we assign a\nvariable multiple times:\n>>> a = 3              # It's an integer\n>>> a = 'hack'         # Now it's a string\n>>> a = 1.23           # Now it's a floating point\nThis isn’t typical Python code, but it does work—a starts out as an integer, then\nbecomes a string, and finally becomes a floating-point number. This example\ntends to look especially odd to ex-C programmers, as it appears as though the\ntype of a changes from integer to string when we say a = 'hack'.\nHowever, that’s not really what’s happening. In Python, things work more\nsimply. Names have no types; as stated earlier, types live with objects, not\nnames. In the preceding listing, we’ve simply changed a to reference different\nobjects. Because variables have no type, we haven’t actually changed the type of\nthe variable a; we’ve simply made the variable reference a different type of\nobject. In fact, again, all we can ever say about a variable in Python is that it\nreferences a particular object at a particular point in time.\nObjects, on the other hand, know what type they are—each object contains a\nheader field that tags the object with its type. The integer object 3, for example,\nwill contain the value 3, plus a designator that tells Python that the object is an\ninteger (strictly speaking, a pointer to an object called int, the name of the\ninteger type). The type designator of the 'hack' string object points to the string\ntype (called str) instead, and 1.23 points to float. Because objects know their\ntypes, variables don’t have to.",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": 5,
      "content": "To recap, types are associated with objects in Python, not with variables. In\ntypical code, a given variable usually will reference just one kind of object.\nBecause this isn’t a requirement, though, you’ll find that Python code tends to be\nmuch more flexible than you may be accustomed to—if you use Python well,\nyour code might work on many types automatically.\nAs mentioned, objects have two header fields, a type designator and a reference\ncounter. To understand the latter of these, we need to move on and take a brief\nlook at what happens at the end of an object’s life.\nObjects Are Garbage-Collected\nIn the prior section’s listings, we assigned the variable a to different types of\nobjects in each assignment. But when we reassign a variable, what happens to\nthe value it was previously referencing? For example, after the following\nstatements, what happens to the object 3?\n>>> a = 3\n>>> a = 'text'\nThe answer is that in Python, whenever a name is assigned to a new object, the\nspace held by the prior object is reclaimed if it is not referenced by any other\nname or object. This automatic reclamation of objects’ space is known as\ngarbage collection and makes life much simpler for programmers of languages\nlike Python that support it.\nTo illustrate, consider the following example, which sets the name x to a\ndifferent object on each assignment:\n>>> x = 99\n>>> x = 'Python'               # Reclaim 99 now (unless referenced elsewhere)\n>>> x = 3.1415                 # Reclaim 'Python' now (ditto)\n>>> x = [1, 2, 3]              # Reclaim 3.1415 now (ditto)\nFirst, notice that x is set to a different type of object each time. Again, the effect\nis as though the type of x is changing over time, but this is not really the case.\nRemember, in Python types live with objects, not names. Because names are just\ngeneric references to objects, this sort of code works naturally.",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": 5,
      "content": "Second, notice that references to objects are discarded along the way. Each time\nx is assigned to a new object, Python reclaims the prior object’s space. For\ninstance, when it is assigned the string 'Python', the object 99 is immediately\nreclaimed (assuming it is not referenced anywhere else)—that is, the object’s\nspace is automatically thrown back into the free space pool, to be reused for a\nfuture object.\nInternally, Python accomplishes this feat by keeping a counter in every object\nthat keeps track of the number of references currently pointing to that object. As\nsoon as—and exactly when—this counter drops to zero, the object’s memory\nspace is automatically reclaimed. In the preceding listing, we’re assuming that\neach time x is assigned to a new object, the prior object’s reference counter\ndrops to zero, causing it to be reclaimed.\nThe most immediately tangible benefit of garbage collection is that it means you\ncan use objects liberally without ever needing to allocate or free up space in your\nscript. Python will make objects clean up their unused space for you as your\nprogram runs. In practice, this eliminates a substantial amount of bookkeeping\ncode required in lower-level languages such as C and C++.\nMORE ON PYTHON GARBAGE COLLECTION\nTechnically speaking, Python’s garbage collection is based mainly upon\nreference counters, as described here, but it also has a component that\ndetects and reclaims objects with cyclic references in time. This component\ncan be disabled if you’re sure that your code doesn’t create cycles, but it is\nenabled by default. Both references and this optional component do garbage\ncollection, but the latter may be what users of some other languages lacking\nreference counts think of as “garbage collection.”\nCircular references are a classic issue in reference-count garbage collectors.\nBecause references are implemented as pointers, it’s possible for an object to\nreference itself, or reference another object that does. For example, exercise\n6 in “Test Your Knowledge: Part I Exercises” and its solution in Appendix B\nshow how to create a cycle easily by embedding a reference to a list within\nitself (e.g., L.append(L)). The same phenomenon can occur for assignments\nto attributes of objects created from user-defined classes. Though relatively",
      "content_length": 2301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": 5,
      "content": "rare, because the reference counts for such objects never drop to zero, they\nmust be treated specially.\nFor more details on Python’s cycle detector and collector, see the\ndocumentation for the gc module in Python’s library manual. The best\ntakeaway here is that garbage-collection-based memory management is\nimplemented for you in Python, by people highly skilled at the task; it works\nwell, even for cycles.\nAlso note that this chapter’s description of Python’s garbage collector is not\npart of the language’s definition and applies to the standard implementation\nof Python (a.k.a. CPython) only. Chapter 2’s alternative implementations\nsuch as Jython, IronPython, and PyPy may use different schemes, though the\nnet effect in all is similar—unused space is reclaimed for you automatically,\nif not always as immediately.\nShared References\nSo far, we’ve explored what happens as a single variable is assigned references\nto objects. Now let’s introduce another variable into our interaction and watch\nwhat happens to its names and objects:\n>>> a = 3\n>>> b = a\nTyping these two statements generates the scene captured in Figure 6-2. The\nsecond command causes Python to create the variable b; the variable a is being\nused and not assigned here, so it is replaced with the object it references (3), and\nb is made to reference that object. The net effect is that the variables a and b\nwind up referencing the same object—that is, pointing to the same chunk of\nmemory.",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": 5,
      "content": "Figure 6-2. Names and objects after next running the assignment b = a\nThis scenario in Python—with multiple names referencing the same object—is\nusually called a shared reference (and sometimes and perhaps more accurately,\nshared object). Note that the names a and b are not linked to each other directly\nwhen this happens; in fact, there is no way to ever link a variable to another\nvariable in Python. Rather, both variables point to the same object via their\nreferences.\nNext, suppose we extend the session with one more statement:\n>>> a = 3\n>>> b = a\n>>> a = 'hack'\nAs with all Python assignments, this statement simply makes a new object to\nrepresent the string value 'hack' and sets a to reference this new object. It does\nnot, however, change the value of b; b still references the original object, the\ninteger 3. The resulting reference structure is shown in Figure 6-3.",
      "content_length": 878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": 5,
      "content": "Figure 6-3. Names and objects after finally running the assignment a = 'hack'\nThe same sort of thing would happen if we changed b to 'hack' instead—the\nassignment would change only b, not a. This behavior also occurs if there are no\ntype differences at all. For example, consider these three statements:\n>>> a = 3\n>>> b = a\n>>> a = a + 2\nIn this sequence, the same events transpire. Python makes the variable a\nreference the object 3 and makes b reference the same object as a, as in Figure 6-\n2; as before, the last assignment then sets a to a completely different object (in\nthis case, the integer 5, which is the result of the + expression). It does not\nchange b as a side effect. In fact, there is no way to ever overwrite the value of\nthe object 3—as introduced in Chapter 4, integers are immutable and thus can\nnever be changed in place (see why this stuff matters?). All we can ever do is\nmake a new integer object.\nOne way to think of this is that, unlike in some languages, variables in Python\nare always pointers to objects, not labels of changeable memory areas: setting a\nvariable to a new value does not alter the original object, but rather causes the\nvariable to reference an entirely different object. The net effect is that",
      "content_length": 1240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": 5,
      "content": "assignment to a variable itself can impact only the single variable being\nassigned. When mutable objects and in-place changes enter the equation, though,\nthe picture changes somewhat; to see how, let’s move on.\nShared References and In-Place Changes\nAs you’ll learn more in this part’s upcoming chapters, some operations do\nchange objects in place, but they’re only supported by Python’s mutable types—\nincluding lists, dictionaries, and sets. For instance, an assignment to an offset in\na list actually changes the list object itself in place, rather than generating a\nbrand-new list object.\nThough you must take it somewhat on faith at this point in the book, this\ndistinction can matter much in your programs. For objects that support such in-\nplace changes, you need to be more aware of shared references, since a change\nfrom one name may impact others. Otherwise, your objects may seem to change\nfor no apparent reason. Given that all assignments are based on references\n(including function argument passing), it’s a pervasive phenomenon.\nTo illustrate, let’s take another look at the list objects introduced in Chapter 4.\nRecall that lists, which do support in-place assignments to positions, are simply\ncollections of other objects, coded in square brackets:\n>>> L1 = [2, 3, 4]\n>>> L2 = L1\nL1 here is a list containing the objects 2, 3, and 4. Items inside a list are accessed\nby their positional offsets, so L1[0] refers to object 2, the first item in the list L1.\nOf course, lists are also objects in their own right, just like integers and strings.\nAfter running the two prior assignments, L1 and L2 reference the same shared\nobject, just like a and b in the prior example (see Figure 6-2). Now imagine that,\nas before, we extend this interaction to say the following:\n>>> L1 = 24\nThis assignment simply sets L1 to a different object; L2 still references the\noriginal list much as in the preceding section. If we change this statement’s",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": 5,
      "content": "syntax slightly, however, it changes its effect radically:\n>>> L1 = [2, 3, 4]         # A mutable object\n>>> L2 = L1                # Make a reference to the same object\n>>> L1[0] = 24             # An in-place change\n>>> L1                     # L1 is different\n[24, 3, 4]\n>>> L2                     # But so is L2!\n[24, 3, 4]\nReally, we haven’t changed L1 itself at line three here; we’ve changed a\ncomponent of the object that L1 references. This sort of change overwrites part\nof the list object’s value in place. Because the list object is shared by (referenced\nfrom) other variables, though, an in-place change like this doesn’t affect only L1\n—that is, you must be aware that when you make such changes, they can impact\nother parts of your program. In this example, the effect shows up in L2 as well\nbecause it references the same object as L1. Again, we haven’t actually changed\nL2, either, but its value will appear different because it refers to an object that has\nbeen overwritten in place.\nThis behavior occurs only for mutable objects that support in-place changes and\nis usually what you want, but you should be aware of how it works so that it’s\nexpected. It’s also just the default: if you don’t want such behavior, you can\nrequest that Python copy objects instead of making references. There are a\nvariety of ways to copy a list, including using the built-in list function, the list\ncopy method, and the standard-library copy module. Perhaps the most common\nway is to slice from start to finish (see Chapters 4 and 7 for more on slicing):\n>>> L1 = [2, 3, 4]\n>>> L2 = L1[:]             # Make a copy of L1 (or list(L1), L1.copy(), etc.)\n>>> L1[0] = 24\n>>> L1\n[24, 3, 4]\n>>> L2                     # L2 is not changed this time: different objects\n[2, 3, 4]\nHere, the change made through L1 is not reflected in L2 because L2 references a\ncopy of the object L1 references, not the original; that is, the two variables point",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": 5,
      "content": "to different objects and different pieces of memory.\nNote that this slicing technique won’t work on the other major mutable core\ntypes, dictionaries and sets, because they are not sequences—to copy a\ndictionary or set, instead use their X.copy() method call (lists have one too), or\npass the original object to their type names, dict and set. Also, note that the\nstandard-library copy module has a call for copying any object type generically,\nas well as a call for copying nested object structures—a dictionary with nested\nlists, for example:\nimport copy\nX = copy.copy(Y)          # Make top-level \"shallow\" copy of any object Y\nX = copy.deepcopy(Y)      # Make deep copy of any object Y: copy all nested parts\nWe’ll explore lists and dictionaries in more depth, and revisit the concept of\nshared references and copies, in Chapters 8 and 9. For now, keep in mind that\nobjects that can be changed in place—that is, mutable objects—are always open\nto these kinds of effects in any code they pass through. In Python, this includes\nlists, dictionaries, sets, and some objects defined with class statements. If this is\nnot desired behavior, simply copy your objects as needed.\nShared References and Equality\nIn the interest of full disclosure, it’s worth pointing out that the garbage-\ncollection behavior described earlier in this chapter may be more conceptual\nthan literal for certain types. Consider these statements:\n>>> x = 99\n>>> x = 'Python'          # Reclaim 99 now?\nBecause Python caches and reuses small integers and small strings, as mentioned\nearlier, the object 99 here is probably not literally reclaimed; instead, it will\nlikely remain in a system table to be reused the next time you generate a 99 in\nyour code. Most kinds of objects, though, are reclaimed immediately when they\nare no longer referenced; for those that are not, the caching mechanism is\nirrelevant to your code—unless you use atypical tools.\nFor instance, because of Python’s reference model, there are two different ways",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": 5,
      "content": "to check for equality in a Python program. Let’s create a shared reference to\ndemonstrate:\n>>> L = [1, 2, 3]\n>>> M = L                 # Make M and L reference the same object\n>>> L == M                # Same values\nTrue\n>>> L is M                # Same objects\nTrue\nThe first technique here, the == operator, tests whether the two referenced\nobjects have the same values; this is the method almost always used for equality\nchecks in Python. The second method, the is operator, instead tests for object\nidentity—it returns True only if both names point to the exact same object, so it\nis a much stronger form of equality testing and is rarely applied in most\nprograms (except for single-instance objects like None, True, and False, as in\nthe prior chapter).\nReally, is simply compares the pointers that implement references, and it serves\nas a way to detect shared references in your code if needed. It returns False if\nthe names point to equivalent but different objects, as is the case when we run\ntwo different literal expressions:\n>>> L = [1, 2, 3]\n>>> M = [1, 2, 3]         # Make M and L reference different objects\n>>> L == M                # Same values\nTrue\n>>> L is M                # Different objects\nFalse\nBut now watch what happens when we perform the same operations on an\nimmutable object like an integer:\n>>> X = 99\n>>> Y = 99                # Should be two different objects\n>>> X == Y\nTrue\n>>> X is Y                # Same object anyhow: caching at work!\nTrue\nIn this interaction, X and Y should be == (same value), but not is (same object)",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": 5,
      "content": "because we ran two different literal expressions (99). Because some integers and\nstrings are cached and reused, though, is tells us they reference the same single\nobject.\nIf you really want to look under the hood, the id built-in is another way to check\nobject identities (and may or may not return an object’s address in memory); and\nthe getrefcount function in the standard sys module returns the passed object’s\nreference count. As of Python 3.12, however, the latter is not as interesting as it\nonce was, because it returns a very high count for objects considered to be\nimmortal—which in Python just means cached for reuse:\n>>> import sys\n>>> sys.getrefcount(99)            # 99 is immortal (cached)\n4294967295\n>>> sys.getrefcount(2 ** 1000)     # But this is not\n1\n>>> id(99) == id(99)               # Same ID/same object (address?)\nTrue\nThis object caching and reuse is irrelevant to your code (unless you run the is\ncheck!). Because you cannot change immutable numbers or strings in place, it\ndoesn’t matter how many references there are to the same object—every\nreference will always see the same, unchanging value even if they all reference\nthe same cached object. Still, this behavior reflects one of the many ways Python\noptimizes its model for execution speed.\nDynamic Typing Is Everywhere\nOf course, you don’t really need to draw name/object diagrams with circles and\narrows to use Python. When you’re starting out, though, it sometimes helps you\nunderstand unusual cases if you can trace their reference structures as we’ve\ndone here. If a mutable object changes out from under you when passed around\nyour program, for example, chances are you are witnessing some of this\nchapter’s subject matter firsthand.\nMoreover, even if dynamic typing seems a little abstract at this point, you\nprobably will care about it eventually. Because everything seems to work by\nassignment and references in Python, a basic understanding of this model is",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": 5,
      "content": "useful in many different contexts. As you’ll see, it works the same in assignment\nstatements, function arguments, for loop variables, module imports, class\nattributes, and more. The good news is that there is just one assignment model in\nPython; once you get a handle on dynamic typing, you’ll find that it works the\nsame everywhere in the language.\nAt the most practical level, dynamic typing means there is less code for you to\nwrite. Just as importantly, though, dynamic typing is also the root of Python’s\npolymorphism, a concept introduced in Chapter 4 that we’ll revisit again later in\nthis book. Because we do not constrain types in Python code, it is both concise\nand highly flexible. As you’ll see, when used well, dynamic typing—and the\npolymorphism it implies—produces code that automatically adapts to new\nrequirements as your systems evolve.\nType Hinting: Optional, Unused, and Why?\nFinally, an implausible plot twist. If you’ve read Python code written in recent\nyears, you may have stumbled across some type declarations for variable names\nthat look like the following—and seem curious and out of place for a\ndynamically typed language like Python, and at first glance contradictory to\nsome of this chapter’s claims:\n>>> a: int\n>>> b: int = 0\n>>> c: list[int] = [1, 2, 3]\nAs previewed in Chapter 4, this is known as type hinting. Syntactically, it takes\nthe form of a colon and object type, between a variable and an optional\nassignment. The object type can be a name or an expression to denote collections\n(list[int] means a list of integers) and can use names predefined in the\nstandard-library typing module (e.g., Iterable, Union, and Any) to express\nricher types per elaborate theory. As of Python 3.12, a new type statement can\neven define type aliases to use in hints, though simple assignments that predated\nit can too:\n>>> type Data = list[float]\n>>> Data = list[float]",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": 5,
      "content": "As also noted in Chapter 4, though, type hints are optional, unused, and largely\nacademic. Python does not require them and does not use them in any way and\nhas no intentions of ever doing so. They are meant solely for use in third-party\ntools like type checkers, and as a form of documentation that’s an alternative to\ncode comments. You can say the same things more simply in both # comments\nand documentation strings you’ll meet later.\nEven when used, type hints do not constrain your code’s types in any way. The\npreceding type hint for a, for instance, does not create name a (only assignment\ndoes), and b’s and c’s hints are not enforced in the least:\n>>> a\nNameError: name 'a' is not defined\n>>> b = 'hack'\n>>> c = 'code'\n>>> b, c\n('hack', 'code')\nType hints can also appear in definitions of functions (and class methods) to\ndocument types of parameters and results, commandeering an earlier feature\nknown as function annotations. We haven’t covered these yet, but as a preview,\nthe following function hints that it accepts an integer and list of strings and\nreturns a float—extraneous info that shows up in __annotations__ dictionaries\nof hosting objects:\n>>> def func(a: int, b: list[str]) -> float:\n        return 'anything' + a + b\nYet as for simple variables, these hints are fully unused, and anything goes when\nthis function is actually run. Strings, for example, work fine for both inputs and\noutputs, despite the seemingly rigid hints:\n>>> func('You', 'Want')\n'anythingYouWant'\nThat is, type hinting is a conceptually heavy tool adopted by Python but\ncompletely unused by Python. It’s at best just another form of documentation in\nPython itself, albeit one that comes with complex rules. External tools might use\ntype hints to check for type mismatches (e.g., mypy) or boost performance, but",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": 5,
      "content": "such tools are also optional, uncommon, and wholly separate from the Python\nlanguage. Furthermore, programs require runtime testing in any language, and\noptimized Pythons introduced in Chapter 2 do not use type hints today, and in\nsome cases cannot (see PyPy).\nMore to the point, though, type hinting is also completely at odds with Python’s\ncore notion of dynamic typing. Type declarations in a dynamically typed\nlanguage are a pointless paradox that negates much of Python’s value\nproposition. Teaching this bizarre extension to Python learners would be a\ndisservice to both Python and learners.\nHence, this book recommends that beginners avoid type hinting at least until\nthey are comfortable with Python’s dynamic-typing paradigm. This book also\nwon’t be covering it further, because it’s far too much extra heft sans benefit for\nnewcomers struggling to master Python’s already sizable fundamentals. If and\nwhen you opt to delve into this inane yet convoluted corner of Python, consult\nits docs for more information.\nIn the end—and despite what you may see in Python code written by\nprogrammers coming from other languages—type hinting does not mean that\nPython is statically typed. Python still uses only dynamic typing, and hopefully\nalways will. After all, this is the root of most of its advantages over other tools.\nLet’s hope that Python developers of the future learn this well before bloating or\nbreaking a tool used and beloved by millions.",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": 5,
      "content": "Chapter Summary\nThis chapter took a deeper look at Python’s dynamic typing model—that is, the\nway that Python keeps track of object types for us automatically, rather than\nrequiring us to code declaration statements in our scripts.\nAlong the way, we learned how variables and objects are associated by\nreferences in Python that enable type flexibility. We also explored the topic of\ngarbage collection, learned how shared references to mutable objects can affect\nmultiple variables, and saw how references impact the notion of equality in\nPython. Lastly, we briefly glimpsed type hinting—a subdomain that weirdly\nadds unused type declarations to a dynamically typed language.\nBecause there is just one assignment model in Python, and because assignment\npops up everywhere in the language, it’s important that you have a handle on the\nmodel before moving on. The following quiz should help you review some of\nthis chapter’s ideas. After that, we’ll resume our core object tour in the next\nchapter, with strings.\nTest Your Knowledge: Quiz\n1. Consider the following three statements. Do they change the value\nprinted for A?\nA = 'code'\nB = A\nB = 'Python'\n2. Consider these three statements. Do they change the printed value of A?\nA = ['code']\nB = A\nB[0] = 'Python'\n3. How about these—is A changed now?",
      "content_length": 1297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": 5,
      "content": "A = ['code']\nB = A[:]\nB[0] = 'Python'\nTest Your Knowledge: Answers\n1. No: A still prints as 'code'. When B is assigned to the string 'Python',\nall that happens is that the variable B is reset to point to the new string\nobject. A and B initially share (i.e., reference/point to) the same single\nstring object 'code', but two names are never linked together in\nPython. Thus, setting B to a different object has no effect on A. The\nsame would be true if the last statement here were B = B + 'coding',\nby the way—the concatenation would make a new object for its result,\nwhich would then be assigned to B only. We can never overwrite a\nstring (or number, or tuple) in place, because strings are immutable.\n2. Yes: A now prints as ['Python']. Technically, we haven’t really\nchanged either A or B; instead, we’ve changed part of the object they\nboth reference (point to) by overwriting that object in place through the\nvariable B. Because A references the same object as B, the update is\nreflected in A, too.\n3. No: A still prints as ['code']. The in-place assignment through B has\nno effect this time because the slice expression made a copy of the list\nobject before it was assigned to B. After the second assignment\nstatement, there are two different list objects that have the same value—\nin Python, we say they are ==, but not is. The third statement changes\nthe value of the list object pointed to by B, but not that pointed to by A.\nWHEN REFERENCES ARE “WEAK”\nYou may occasionally see the term “weak reference” in the Python world.\nNo, this term isn’t a judgment about inferiority. It refers to a somewhat\nobscure and advanced tool, which is related to the reference model we’ve\nexplored here, and like the is operator, can’t really be understood without it.",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": 5,
      "content": "In short, a weak reference, implemented by the weakref standard-library\nmodule, is a reference to an object that does not by itself prevent the\nreferenced object from being garbage-collected. If the last remaining\nreferences to an object are all weak references, the object can be reclaimed.\nWhen this happens, the weak references to it will be notified that the object\nno longer exists and can respond as needed.\nAs an example of its utility, this can be useful in nonessential caches of large\nobjects primarily used elsewhere. If such a cache uses normal references, the\ncache’s references alone would keep the objects in memory indefinitely. By\nusing weak references, the object’s space may be reclaimed when it’s no\nlonger needed for its primary role, and the cache will be notified of its\ndemise, either on next fetch or by callback.\nNot all object types can be weakly referenced, though support can be added\nfor some with OOP techniques we won’t explore till later in this book. Still,\nthis is really just a special-case extension to the reference model we met\nhere. For more details on weak references, see Python’s library-manual\ncoverage of weakref, a useful—if unhappily named—tool.",
      "content_length": 1192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": 5,
      "content": "Chapter 7. String Fundamentals\nSo far, we’ve studied numbers and explored Python’s dynamic typing model.\nThe next major type on our in-depth object tour is the Python string—an ordered\ncollection of characters used to store and represent text- and bytes-based\ninformation. We looked briefly at strings in Chapter 4. Here, we will revisit them\nto fill in details we skipped earlier.\nBefore we get started, let’s get clear on what we won’t be covering here.\nChapter 4 also briefly previewed Unicode strings and files—tools for dealing\nwith non-ASCII text. Unicode is a key tool for programmers, especially those\nwho work in the internet domain. It can pop up, for example, in web pages,\nemails, GUI toolkits, file-processing tools, XML and JSON text, and more. At\nthe same time, Unicode can be a heavy topic for programmers just starting out,\nand a complete understanding of it relies on tools that we haven’t yet studied in\nfull, like files.\nIn light of that, this book splits its strings coverage between the essentials here,\nand their extension to Unicode and byte strings in Chapter 37 of its advanced\ntopics part. That is, this chapter tells only part of the string story in Python—the\npart that most scripts use, and most Python learners need to know up front.\nDespite this limited scope, everything we learn here will apply directly to\nUnicode and bytes processing, too, because Python text strings are Unicode,\neven if they’re simple ASCII text, and byte strings are simply strings constrained\nto byte values.\nAfter you’ve learned the basics here, Chapter 37 is recommended reading for the\nrest of the string saga, and most programmers will want to follow up with its\ncoverage eventually. Unicode is rarely optional in programming today, though\nit’s best deferred until you’ve had a chance to master strings in general. So let’s\nget started!\nString Object Basics\nFrom a functional perspective, strings can be used to represent just about",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": 5,
      "content": "anything that can be encoded as text or bytes. In the text department, this\nincludes symbols and words (e.g., your name), contents of text files loaded into\nmemory, internet addresses, Python source code, and so on. Strings can also be\nused to hold the raw bytes used for media files and network transfers, and both\nthe encoded and decoded forms of non-ASCII Unicode text.\nYou may have used strings in other languages, too. Python’s strings serve the\nsame role as character arrays in languages such as C, but they are a somewhat\nhigher-level tool than arrays. Unlike in C, strings in Python come with a\npowerful set of precoded processing tools. Also unlike languages such as C,\nPython has no distinct type for individual characters; instead, you just use one-\ncharacter strings for one-character info.\nStrictly speaking, Python strings are categorized as immutable sequences,\nmeaning that the characters they contain have a left-to-right positional order and\ncannot be changed in place. In fact, strings are the first representative of the\nlarger class of objects called sequences that we will explore here. Pay attention\nto the sequence operations covered in this chapter, because they will work the\nsame on other sequence types you’ll meet later, such as lists and tuples.\nAs a first step, Table 7-1 previews common string literals and operations\ndiscussed in this chapter, by abstract example (don’t expect its code snippets to\nrun!). As it shows, empty strings are written as a pair of quotation marks (single\nor double) with nothing in between, and there are a variety of ways to code\nstrings. For processing, strings support expression operations such as\nconcatenation (combining strings), slicing (extracting sections), indexing\n(fetching by offset), and so on. Besides expressions, Python also provides a set\nof string methods that implement common string-specific tasks, as well as\nmodules for more advanced text-processing tasks such as pattern matching.\nTable 7-1. Common string literals and operations\nOperation\nInterpretation\nS = ''\nEmpty string\nS = \"app's\"\nDouble quotes, same as single",
      "content_length": 2101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": 5,
      "content": "S = 'c\\no\\td\\x00e'\nEscape sequences\nS = \"\"\"…multiline…\"\"\"\nTriple-quoted block strings\nS = r'\\temp\\data.txt'\nRaw strings (ignore escapes)\nB = b'h\\xc4ck'\nByte strings (Chapter 4, Chapter 37)\nS = 'py\\U0001F40D'\nUnicode strings (Chapter 4, Chapter 37)\nS = u'py\\U0001F40D'\nPython 2.X compatibility (Chapter 37)\nS1 + S2\nS * 3\nConcatenate, repeat\nS[i]\nS[i:j]\nlen(S)\nIndex, slice, length\nS1 > S2, S1 == S2\nComparisons: magnitude, equality\n'a %s coder' % kind\nString-formatting expression\n'a {0} coder'.format(kind)\nString-formatting method\nf'a {kind} coder'\nString-formatting literal (3.6+)\nS.find('od')\nS.rstrip()\nS.replace('od', 'ood')\nS.split(',')\nS.isdigit()\nS.lower()\nS.endswith('thon')\nS.join(strlist)\nS.encode('utf8')\nB.decode('latin1')\nString methods (see ahead for all 43): search,\nremove whitespace,\nreplacement,\nsplit on delimiter,\ncontent test,\ncase conversion,\nend test,\ndelimiter join,\nUnicode encoding,\nUnicode decoding, etc. (see Table 7-3)\n'py' in S.lower()\nfor x in S: print(x)\n[c * 2 for c in S]\nMembership, iteration",
      "content_length": 1028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": 5,
      "content": "map(ord, S)\nre.match('Py(.*)on', line)\nPattern matching: library module\nBeyond the core set of string tools in Table 7-1, Python also supports more\nadvanced pattern-based string processing with the standard library’s re (for\n“regular expression”) module demoed in Chapter 37, and even higher-level text\nprocessing tools such as HTML, JSON, and XML parsers. This book and this\nchapter, though, are focused on the fundamentals represented by Table 7-1.\nThis chapter begins with an overview of string literal forms and string\nexpressions, then moves on to look at more advanced tools such as string\nmethods and formatting. Python comes with many string tools, and we won’t\ncover them all here; the complete story is chronicled in the Python library\nmanual. Our goal here is to explore enough commonly used tools to give you a\nrepresentative sample; methods we won’t see in action here are analogous to\nthose we will.\nString Literals\nBy and large, strings are fairly easy to use in Python. The first thing you need to\nknow about them, though, is that there are many ways to write them in your\ncode:\nSingle quotes: 'cod\"e'\nDouble quotes: \"cod'e\"\nTriple quotes: '''…code…''', \"\"\"…code…\"\"\"\nEscape sequences: \"c\\to\\nd\\0e\"\nRaw strings: r\"C:\\new\\test.bin\"\n(Chapter 37) Bytes literals: b'co\\x01de'\n(Chapter 37) Unicode literals: 'h\\u00c4ck'\nThe single- and double-quoted forms are by far the most common; the others",
      "content_length": 1404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": 5,
      "content": "serve specialized roles, and we’re postponing further discussion of the last two\nadvanced forms until Chapter 37. Let’s take a quick look at all the other options\nin turn.\nSingle and Double Quotes Are the Same\nAround Python strings, single- and double-quote characters are interchangeable,\nthough they must match, and must be straight quotes (beware tools that\nautocorrect to slanted quotes!). That is, string literals can be written enclosed in\neither two single or two double straight quotes—the two forms work the same\nand return the same type of object. For example, the following two strings,\ncoded at the usual REPL, are identical once they are read by Python:\n$ python3\n>>> 'python', \"python\"\n('python', 'python')\nThe reason for supporting both is that it allows you to embed a quote character\nof the other variety inside a string without escaping it with a backslash. You may\nembed a double-quote character in a string enclosed in single-quote characters,\nand vice versa, without having to use the escapes you’ll meet in a moment:\n>>> 'python\"s', \"python's\"                  # Mixed quotes sans escapes\n('python\"s', \"python's\")\nThis book generally prefers to use single quotes around strings just because they\nare marginally easier to read, except in cases where a single quote is embedded\nin the string. This is a purely subjective style choice, but Python displays strings\nthis way, too, and most Python programmers do the same today, so you probably\nshould too.\nNote that the comma is important in the preceding code. Without it, Python\nautomatically concatenates adjacent string literals of any kind. It is almost as\nsimple to add a + operator between them to invoke concatenation explicitly, but\nadjacent literals are concatenated early when your code is read (and as you’ll\nlearn ahead, wrapping this form in parentheses also allows it to span multiple\nlines for larger blocks of text that can’t use triple quotes):",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": 5,
      "content": ">>> title = \"Learning \" 'Python' \" 6E\"      # Implicit concatenation when read\n>>> title\n'Learning Python 6E'\nAdding commas between these strings would result in a tuple, not a string. Also\nnotice in all of these outputs that Python prints strings in single quotes unless\nthey embed one. If needed, you can also embed quote characters by escaping\nthem with backslashes:\n>>> 'python\\'s', \"python\\\"s\"\n(\"python's\", 'python\"s')\nTo understand why, you need to move on to learn how escapes work in general.\nEscape Sequences Are Special Characters\nThe prior example embedded a quote inside a string by preceding it with a\nbackslash (\\). This is representative of a general pattern in strings: backslashes\nare used to introduce special character codings known as escape sequences.\nEscape sequences let us embed characters in strings that cannot easily be\ninserted into a string literal or typed on a keyboard. The character \\, and one or\nmore characters following it in the string literal, are replaced with a single\ncharacter in the resulting string object, which has the value specified by the\nescape sequence. For example, here is a five-character string that embeds a\nnewline and a tab:\n>>> s = 'a\\nb\\tc'\nThe two characters \\n stand for a single character—the newline character\n(technically speaking, code-point value 10, which means newline in Unicode and\nits ASCII subset). Similarly, the sequence \\t is replaced with the tab character\n(code point 9). The way this string looks when printed depends on how you print\nit. The interactive echo shows the special characters as escapes, but print\ninterprets them instead:\n>>> s\n'a\\nb\\tc'\n>>> print(s)",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": 5,
      "content": "a\nb       c\nTo be completely sure how many actual characters are in this string, use the\nbuilt-in len function—it returns the actual number of characters in a string,\nregardless of how it is coded or displayed:\n>>> len(s)\n5\nThis string is five characters long: it contains an ASCII a, a newline character, an\nASCII b, and so on.\nNOTE\nBut length is not bytes: If you’re accustomed to all-ASCII text, it’s tempting to think of this\nlen result as meaning five bytes too, but you probably shouldn’t. Really, “bytes” in today’s\nUnicode world doesn’t hold the meaning it once did. For one thing, the Python string object\nincludes admin data that makes it larger in memory than its text alone.\nFor another, string content and length both reflect code points (identifying numbers) assigned\nby Unicode, and a single character’s code point does not necessarily map to a single byte—\neither when decoded in memory or encoded in files. Under encoding UTF-16, for example,\nASCII characters are multiple bytes in files and may be any size at all in memory depending\non how Python allocates their space. Moreover, code-point values of non-ASCII characters\nlike \n and \n are far too large to fit in an 8-bit byte in any form.\nIn fact, Python defines str strings formally as sequences of Unicode code points, not bytes, to\nmake this clear. There’s much more on how Unicode text obviates bytes in Chapter 37 when\nyou’re ready to take the plunge. For now, to avoid confusion, think characters instead of bytes\nin strings.\nNotice that the original backslash characters in the preceding examples are not\nreally stored with the string in memory; they are used only to describe special\ncharacter values to be stored in the string. For coding such special characters,\nPython recognizes a full set of escape code sequences, listed for reference in\nTable 7-2.\nTable 7-2. String backslash characters",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": 5,
      "content": "Escape\nMeaning\n\\\\\nBackslash (stores one \\)\n\\'\nSingle quote (stores ')\n\\\"\nDouble quote (stores \")\n\\n\nNewline (a.k.a. linefeed)\n\\r\nCarriage return (e.g., in Windows \\r\\n)\n\\t\nHorizontal tab\n\\v\nVertical tab\n\\a\nBell (where supported)\n\\b\nBackspace\n\\f\nFormfeed\n\\xhh\nHexadecimal code-point or byte value (exactly 2 hex digits)\n\\ooo\nOctal code-point or byte value (up to 3 digits, 377 ceiling)\n\\0\nNull: octal binary 0 character (doesn’t end string)\n\\uhhhh\nUnicode character code point, 16-bit value (exactly 4 hex\ndigits)\n\\Uhhhhhhhh\nUnicode character code point, 32-bit value (exactly 8 hex\ndigits)\n\\N{name}\nCharacter with ID name in Unicode database\n\\newline\nIgnored (precedes a continuation line)\n\\other\nRetained verbatim, but a warning in 3.12, an error in the\nfuture",
      "content_length": 761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": 5,
      "content": "utu e\nSome of Table 7-2’s escapes come with usage rules. Again for reference, here\nare the fine points:\nThe \\x, \\u, and \\U escape sequences require exactly two, four, and eight\nhexadecimal digits, respectively. Use digits 0–9 and A–F (uppercase or\nlowercase) for h.\nThe \\o escape accepts one to three octal digits and issues a warning for\nvalues over \\377 in Python 3.12 because values too big for a byte cause\nissues in byte strings. Use digits 0–7 for o.\nBoth \\u and \\U are recognized only in str text-string literals (e.g., '…'),\nwhere they give a character’s Unicode code-point value. This is the\ncode point’s decoded value.\n\\x and \\o escapes work in both bytes byte-string literals (b'…'), where\nthey give a byte’s absolute value; and in str text-string literals, where\nthey give a character’s Unicode code-point value.\nLettered escapes like \\n stand for their Unicode code points in text and\ntheir ASCII encodings in bytes, even if the two values agree (there is\nmore on Unicode escapes in Chapter 37).\nLet’s get back to running code. Some string escape sequences allow you to\nembed absolute values as characters of a string. When you do this, you’re really\ngiving the code-point value of the desired character. For instance, here’s a five-\ncharacter string that embeds two characters with zero values (coded as octal\nescapes of one digit):\n>>> s = 'a\\0b\\0c'\n>>> s\n'a\\x00b\\x00c'\n>>> len(s)\n5\nThe character associated with code point 0 is generally known as NULL (or",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": 6,
      "content": "NUL). Importantly, in Python, a character like this does not terminate a string\nthe way a “null byte” typically does in C. Instead, Python keeps both the string’s\nlength and text in memory. In fact, no character terminates a string in Python.\nHere’s a string that is all absolute escape codes—an absolute 1 and 2 (coded in\noctal), followed by an absolute 3 (coded in hexadecimal), and nonprintables all:\n>>> s = '\\001\\002\\x03'\n>>> s\n'\\x01\\x02\\x03'\n>>> len(s)\n3\nNotice that Python displays nonprintable characters in hex, regardless of how\nthey were specified. When needed, you can freely combine characters, absolute-\nvalue escapes, and the more symbolic escapes in Table 7-2. To demo, the\nfollowing string contains the characters “HACK”, a tab and newline, and an\nabsolute zero character coded in hex:\n>>> S = 'H\\tA\\nC\\x00K'\n>>> S\n'H\\tA\\nC\\x00K'\n>>> len(S)\n7\n>>> print(S)\nH   A\nCK\nThis becomes more important to know when you process binary data files in\nPython. Because their contents are represented as strings in your scripts, it’s OK\nto process binary files that contain any sorts of binary byte values. When opened\nin binary modes, files return raw bytes from the external file as bytes—a string\nvariant that supports most of the syntax and tools in this chapter (you’ll find\nmore on files and bytes in Chapters 4, 9, and 37).\nTwo limits in Table 7-2 merit callouts. First of all, octal escapes with values too\nlarge for a byte issue warnings as of Python 3.12 and will be errors soon—\ndespite the fact that these escapes in text strings denote code points, not bytes:\n>>> '\\400'\n<stdin>:1: SyntaxWarning: invalid octal escape sequence '\\400'",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": 6,
      "content": "'Ā'\nThis seems unlikely to break much code, but the last entry in Table 7-2 may: if\nPython does not recognize the character after a \\ as being a valid escape code, it\nsimply keeps the backslash in the resulting string—at least for the moment:\n>>> x = 'C:\\py\\code'    \n<stdin>:1: SyntaxWarning: invalid escape sequence '\\p'\n>>> x                         # Keeps \\ literally (and displays it as \\\\)\n'C:\\\\py\\\\code'\n>>> len(x)                    # But not for long: don't rely on this anymore!\n10\nDespite this behavior being expected (and even relied upon) for three decades, it\nhas recently been judged bad and has started issuing a warning when used. In\nPython 3.12, it invokes a syntax warning that doesn’t stop your program but\nclutters your output with nags. Worse, this will be treated as a programming-\nending error in a future Python, so you should not use this going forward—and\nare expected to change all the code you’ve written in the past that does!\nEven without this backward-incompatible Python change, though, strings that\nrely on this behavior seem as likely to lose backslashes in escapes as to retain\nthem elsewhere. Instead, code literal backslashes explicitly such that they are\nretained in your strings in all Pythons, by either doubling them with \\\\ (an\nescape for one \\), or using raw strings—the topic of the next section.\nRaw Strings Suppress Escapes\nAs we’ve already seen, escape sequences are handy for embedding special\ncharacters in strings. Sometimes, though, the special treatment of backslashes for\nintroducing escapes can lead to trouble. It’s surprisingly common, for instance,\nto see Python newcomers trying to open a file on Windows with a filename\nargument that looks something like this:\nmyfile = open('C:\\new\\text.dat', 'w')\nthinking that they will open a file called text.dat in the directory C:\\new. The\nproblem with this is that \\n is taken to stand for a newline character, and \\t is",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": 6,
      "content": "replaced with a tab. In effect, the call tries to open a file named C:\n(newline)ew(tab)ext.dat, with usually less-than-stellar results.\nThis is just the sort of thing that raw strings are meant to address. If the letter r\n(uppercase or lowercase, but usually the latter) appears just before the opening\nquote of any string literal covered in this chapter, it turns off the escape\nmechanism. The result is that Python retains your backslashes literally, exactly\nas they appear in the string. Hence, to fix the filename problem, just remember\nto add the letter r on Windows:\nmyfile = open(r'C:\\new\\text.dat', 'w')         # Works: disable \\ escapes\nAlternatively, because, as noted in the preceding section, two backslashes are\nreally an escape sequence for one backslash, you can keep your backslashes by\nsimply doubling them up when they should be taken verbatim:\nmyfile = open('C:\\\\new\\\\text.dat', 'w')        # Also works: \\\\ means \\\nIn fact, Python itself sometimes uses this doubling scheme when it prints strings\nwith embedded backslashes:\n>>> path = r'C:\\new\\text.dat'                  # Raw string: keep \\s\n>>> path                                       # Show as Python code\n'C:\\\\new\\\\text.dat'\n>>> print(path)                                # User-friendly format\nC:\\new\\text.dat\n>>> len(path)                                  # String length: \\s\n15\nAs covered in Chapter 5, the default format at the interactive prompt prints\nresults as if they were code, and therefore escapes backslashes in the output. The\nprint statement provides a more user-friendly format that shows that there is\nactually only one backslash in each spot. To verify this is the case, you can check\nthe result of the built-in len function, which returns the number of characters in\nthe string, independent of display formats. If you count the characters in the\nprint(path) output, you’ll see that there really is just 1 character per backslash,\nfor a total of 15.\nBesides directory paths on Windows, raw strings are also commonly used for",
      "content_length": 2020,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": 6,
      "content": "regular expressions in text pattern matching, supported with the re module\nintroduced in Chapter 37. Also note that Python scripts can usually use forward\nslashes in directory paths on both Windows and Unix because this slash is\ninterpreted portably (e.g., 'C:/new/text.dat' works when opening Windows\nfiles, too). Raw strings are useful for paths using native Windows backslashes,\nthough, and any other time you want to ensure that Python will leave your \\\nalone. They also work for triple-quoted strings to suppress escapes (and future\ninvalid-escape errors!) in text of the sort up next.\nNOTE\nRaw-string quirk: Despite its role, even a raw string cannot end in a single backslash, because\nthe backslash escapes the following quote character. That is, r'…\\' is not a valid string literal:\nyou still must escape the surrounding quote character to embed it in the string, and Python\nassumes that is your intent. The upshot is that a raw string cannot end in an odd number of\nbackslashes, including one. If you need to end a raw string with a single backslash, you can\nuse two and slice off the second (r'…\\\\'[:-1]), tack one on manually (r'…' + '\\\\'), or skip\nthe raw string syntax and just double up the backslashes in a normal string ('…\\\\'). All three\nof these forms create the same two-character string with a Unicode ellipsis and one backslash.\nTriple Quotes and Multiline Strings\nSo far, you’ve seen single quotes, double quotes, escapes, and raw strings in\naction. Python also has a triple-quoted string literal format, sometimes called a\nblock string, that is a syntactic convenience for coding multiline data. This form\nbegins with three quotes (of either the single or double variety), is followed by\nany number of lines of text, and is closed with the same triple-quote sequence\nthat opened it. Single and double quotes embedded in the string’s text may be,\nbut do not have to be, escaped—the string does not end until Python sees three\nunescaped quotes of the same kind used to start the literal. For example:\n>>> quip = \"\"\"Python strings\n...   sure have\n... a lot of options\"\"\"\n>>> \n>>> quip\n'Python strings\\n  sure have\\na lot of options'",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": 6,
      "content": "This string spans three lines. As you learned in Chapter 3, the interactive prompt\nchanges to ... on continuation lines like this in some interfaces, but not in\nothers. This book omits the dots in some examples to enable cut-and-paste, but\ndon’t type them yourself if they’re listed as they are here but absent in your\nREPL, and extrapolate as needed.\nPrompts aside, Python collects all the triple-quoted text in this example into a\nsingle multiline string, with embedded newline characters (\\n) at the places\nwhere your code has physical line breaks. Notice that, as in the literal, the\nsecond line in the result has leading spaces, but the third does not—what you\ntype is truly what you get. To see the string with the newlines interpreted, print it\ninstead of echoing:\n>>> print(quip)\nPython strings\n  sure have\na lot of options\nIn fact, triple-quoted strings will retain all the enclosed text, including any to the\nright of your code that you might intend as comments. So don’t do this—put\nyour comments above or below the quoted text, or use the automatic\nconcatenation of adjacent strings mentioned earlier, with explicit newlines if\nneeded, and surrounding parentheses to allow line spans (you’ll learn more about\nthis latter form when you study syntax rules in Chapters 10 and 12):\n>>> menu = \"\"\"\n... Open           # Comments here added to string!\n... Save           # Ditto\n... \"\"\"\n>>> menu\n'\\nOpen           # Comments here added to string!\\nSave           # Ditto\\n'\n>>> menu = (\n... 'Open\\n'       # Comments here ignored\n... 'Save\\n'       # But newlines not automatic\n... )\n>>> menu\n'Open\\nSave\\n'\nSo why use triple-quoted strings? For one thing, they are useful anytime you\nneed multiline text in your program—for example, to embed multiline error",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": 6,
      "content": "messages, or HTML, XML, JSON, or YAML code in your Python source code\nfiles. You can usually embed such blocks directly in your scripts by triple-\nquoting without resorting to external text files or concatenation and newline\ncharacters.\nTriple-quoted strings are also commonly used for docstrings (documentation\nstrings), which are string literals that are taken as comments when they appear at\nspecific points in your file (and are covered in full in Chapter 15). These don’t\nhave to be triple-quoted blocks, but they usually are to allow for multiline\ncomments and may need to be triple-quoted raw strings (e.g., r''') to avoid\ninvalid escape errors in the future (see the 3.12 syntax warnings noted\npreviously).\nFinally, triple-quoted strings are also sometimes used as a “horribly hackish”\nway to temporarily disable lines of code during development. Really, it’s not too\nhorrible, and it’s actually a fairly common practice today, but it wasn’t the\noriginal intent of the literal. If you wish to turn off a few lines of code and run\nyour script again, simply put three quotes above and below them, like this:\nX = 1\n\"\"\"\nimport os                            # Disable this code temporarily\nprint(os.getcwd())\n\"\"\"\nY = 2\nThis was tagged as “hackish” because Python really might make a string out of\nthe lines of code disabled this way, but this is probably not significant in terms of\nperformance. For large sections of code, it’s also easier than manually adding\nhash marks before each line and later removing them. This is especially true if\nyou are using a text editor that does not have support for editing Python code\nspecifically. In Python, practicality often beats aesthetics.\nStrings in Action\nOnce you’ve created a string with the literal expressions we just met, you will\nalmost certainly want to do things with it. This section and the next two\ndemonstrate string expressions, methods, and formatting—the first line of text-",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": 6,
      "content": "processing tools in the Python language.\nBasic Operations\nLet’s begin by interacting with the Python interpreter to illustrate the basic string\noperations listed earlier in Table 7-1. You can concatenate strings using the +\noperator and repeat them using the * operator:\n>>> len('abc')            # Length: number of items\n3\n>>> 'abc' + 'def'         # Concatenation: a new string\n'abcdef'\n>>> 'Py!' * 4             # Repetition: like 'Py!' + 'Py!' + 'Py!' + 'Py!'\n'Py!Py!Py!Py!'\nThe len built-in function here returns the length of a string (or any other object\nwith a length). Formally, adding two string objects with + creates a new string\nobject, with the contents of its operands joined, and repetition with * is like\nadding a string to itself a given number of times (minus one). In both cases,\nPython lets you create arbitrarily sized strings; there’s no need to predeclare\nanything in Python, including the sizes of data structures—you simply build\nstring objects as needed and let Python manage the underlying memory space\nautomatically, as we learned in Chapter 6.\nRepetition may seem a bit obscure at first, but it comes in handy in a surprising\nnumber of contexts. For example, to print a line of 80 dashes, you can count up\nto 80, or let Python count for you:\n>>> print('------ …more… ------')        # 80 dashes, the hard way\n>>> print('-' * 80)                      # 80 dashes, the easy way\nNotice that the operator overloading and polymorphism called out in Chapter 5\nand earlier is at work here already: we’re using the same + and * operators that\nperform addition and multiplication when using numbers. Python does the\ncorrect operation because it knows the types of the objects being added and\nmultiplied. But be careful: the rules aren’t quite as liberal as you might expect.\nFor instance, Python doesn’t allow you to mix numbers and strings in +\nexpressions: 'abc'+9 raises an error instead of automatically converting 9 to a",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": 6,
      "content": "string (we’ll fix this ahead).\nAs shown near the end of Table 7-1, you can also iterate over strings in loops\nusing for statements, which repeat actions, and test membership for both\ncharacters and substrings with the in expression operator, which is essentially a\nsearch. For substrings, in is much like the str.find() method covered later in\nthis chapter, but it returns a Boolean result instead of the substring’s position\n(don’t be alarmed if the following’s print indents your prompt; its end=' '\nchanges the default newline character at the end of the display to a space):\n>>> myjob = 'hacker'\n>>> for c in myjob:                     # Step through items, print each + ' '      \n...     print(c, end=' ')               # Suppress newlines after each item\n...\nh a c k e r\n>>> 'k' in myjob                        # Found\nTrue\n>>> 'z' in myjob                        # Not found\nFalse\n>>> 'HACK' in 'abcHACKdef'              # Substring search, no position returned\nTrue\nThe for loop, previewed in Chapter 4, assigns a variable to successive items in a\nsequence (here, a string) and executes one or more statements (normally\nindented) for each item. In effect, the variable c becomes a cursor stepping\nacross the string’s characters. Because iteration turns out to be a big idea in\nPython, we will discuss iteration tools like these and others listed in Table 7-1 in\nmore detail later in this book (see Chapters 14 and 20).\nIndexing and Slicing\nBecause strings are ordered collections (a.k.a. sequences) of characters, we can\naccess their components by position. As introduced in Chapter 4, characters in a\nstring are fetched by indexing—providing the numeric offset of the desired\ncomponent in square brackets after the string. You get back the one-character\nstring at the specified position.\nAs in most C-like languages, Python offsets start at 0 and end at one less than the\nlength of the string (and the “start at 0” part may be a short-lived hurdle if you’re\naccustomed to counting from 1). Unlike C, however, Python also lets you fetch",
      "content_length": 2044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": 6,
      "content": "items from sequences such as strings using negative offsets. Technically, a\nnegative offset is added to the length of a string to derive a positive offset, but\nyou can also think of negative offsets as counting backward from the end. The\nfollowing interaction demonstrates:\n>>> S = 'code'\n>>> S[0], S[-2]                         # Indexing from front or end\n('c', 'd')\n>>> S[1:3], S[1:], S[:-1]               # Slicing: extract a section\n('od', 'ode', 'cod')\nIn this code, the first line defines a four-character string and assigns it to the\nname S. The next line indexes it in two ways: S[0] fetches the item at offset 0\nfrom the left—the one-character string 'c' at the front; and S[−2] gets the item\nat offset 2 back from the end—or equivalently, at offset (4 + (−2)) from the front.\nThe last line in the foregoing example demonstrates slicing, a generalized form\nof indexing that returns an entire section, instead of a single item. It can be used\nto extract columns of data, chop off prefixes and suffixes, and more. Slicing can\nalso be viewed as a type of parsing (decomposing content), especially when\napplied to strings, because it’s an easy way to extract substrings. In fact, we’ll\nexplore slicing in the context of text parsing later in this chapter.\nSlicing works like this: when you index a sequence object such as a string on a\npair of offsets separated by a colon, Python returns a new object containing the\ncontiguous section identified by the offset pair. The left offset is taken to be the\nlower bound (inclusive), and the right is the upper bound (noninclusive). That is,\nPython fetches all items from the lower bound up to but not including the upper\nbound and returns a new object containing the fetched items. If omitted, the left\nand right bounds default to 0 and the length of the object you are slicing,\nrespectively.\nFor instance, in the example we just ran, S[1:3] extracts the items at offsets 1\nand 2—it grabs the second and third items and stops before the fourth item at\noffset 3. Next, S[1:] gets all items beyond the first—the upper bound, which is\nnot specified, defaults to the length of the string, which is off the end. Finally,\nS[:−1] fetches all but the last item—the lower bound defaults to 0, and −1\nrefers to the last item, noninclusive. In more graphic terms, indexes and slices",
      "content_length": 2321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": 6,
      "content": "map to cells as shown in Figure 7-1.\nFigure 7-1. Indexes and slices: positives start from the left (0) and negatives from the right (–1)\nAll of which may seem confusing at first glance, but indexing and slicing are\nsimple and powerful tools to use once you get the knack. Remember, if you’re\nunsure about the effects of a slice, try it out interactively. In the next chapter,\nyou’ll see that it’s even possible to change an entire section of another object in\none step by assigning to a slice (though not for immutables like the strings we’re\nstudying here). For now, here’s a cheat sheet of the details for reference:\nIndexing—S[I]—fetches components at offsets in sequences:\nThe first item is at offset 0.\nNegative indexes mean counting backward from the end or right.\nOut-of-bounds offsets are an error.\nS[0] fetches the first item.\nS[−2] fetches the second item from the end (like S[len(S)−2]).\nSlicing—S[I:J]—extracts contiguous sections of sequences:\nThe upper bound is noninclusive.\nSlice bounds default to 0 and the sequence length, if omitted.\nOut-of-bounds offsets are adjusted to be in bounds.\nS[1:3] fetches items at offsets 1 up to but not including 3.\nS[1:] fetches items at offset 1 through the end (the sequence length).",
      "content_length": 1236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": 6,
      "content": "S[:3] fetches items at offset 0 up to but not including 3.\nS[:−1] fetches items at offset 0 up to but not including the last item.\nS[:] fetches items at offsets 0 through the end—making a top-level\ncopy of S.\nExtended slicing—S[I:J:K]—accepts a step (or stride) K, which defaults to +1:\nAllows for skipping items and reversing order—see the next section.\nThe second-to-last bullet item listed here turns out to be a common technique:\nS[:] makes a full top-level copy of a sequence object—an object with the same\nvalue, but a distinct piece of memory (you’ll find more on copies in Chapter 9).\nThis isn’t very useful for immutable objects like strings, but it comes in handy\nfor objects that may be changed in place, such as lists: making a copy can avoid\nthe side effects of shared references shown in Chapter 6.\nAlso per the cheat sheet, slices differ from indexes in their policy on out-of-\nbounds (off the end) offsets: they’re always errors in indexing, because the\noffset does not exist, but scaled to be in bounds in slicing, because this can be\nuseful in programs that need to accommodate sizes flexibly:\n>>> S = 'code'\n>>> S[99]\nIndexError: string index out of range\n>>> S[1:99]\n'ode'\nBecause we’re going to explore this oddity in an end-of-part exercise, though,\nwe’ll cut the story short here.\nExtended slicing: The third limit and slice objects\nThough not commonly used, slice expressions also support an optional third\nindex, used as a step (sometimes called a stride). The step is added to the index\nof each item extracted. With it, the full-blown form of a slice is S[I:J:K], which\nmeans “extract all the items in S, from offset I through J−1, by K.” The third\nlimit, K, defaults to +1, which is why normally all items in a slice are extracted\nfrom left to right. If you specify an explicit value, however, you can use the third",
      "content_length": 1842,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": 6,
      "content": "limit to skip items or to reverse their order.\nFor instance, S[1:10:2] will fetch every other item in S from offsets 1–9; that\nis, it will collect the items at offsets 1, 3, 5, 7, and 9. As usual, the first and\nsecond limits default to 0 and the length of the sequence, respectively, so X[::2]\ngets every other item from the beginning to the end of the sequence:\n>>> S = 'abcdefghijklmnop'\n>>> S[1:10:2]                          # Skipping items\n'bdfhj'\n>>> S[::2]\n'acegikmo'\nYou can also use a negative stride to collect items in the opposite order. For\nexample, in the slicing expression S[::−1], the first two bounds default to\nsequence length–1 and –1 (they really default to None and None, but that’s\nunimportant here), and a stride of −1 indicates that the slice should go from right\nto left instead of the usual left to right. In much simpler terms, the effect is to\nreverse the sequence:\n>>> S = 'hello'\n>>> S[::−1]                            # Reversing items\n'olleh'\nWith a negative stride, the meanings of the first two bounds are essentially\nreversed. That is, the slice S[5:1:−1] fetches the items from 2 to 5, in reverse\norder (the result contains items from offsets 5, 4, 3, and 2):\n>>> S = 'abcedfg'\n>>> S[5:1:−1]                          # Bounds roles differ\n'fdec'\nSkipping and reversing like this are the most common use cases for three-limit\nslices, but see Python’s standard-library manual for more details (or run a few\nexperiments interactively). We’ll revisit three-limit slices again later in this\nbook, in conjunction with the for loop statement.\nLater in the book, you’ll also learn that slicing is equivalent to indexing with a\nslice object, a finding of importance to class writers seeking to support both",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": 6,
      "content": "operations:\n>>> 'code'[1:3]                       # Slicing syntax\n'od'\n>>> 'code'[slice(1, 3)]               # Slice objects with index syntax + object\n'od'\n>>> 'code'[::-1]\n'edoc'\n>>> 'code'[slice(None, None, -1)]\n'edoc'\nWHY YOU WILL CARE: SLICES\nThroughout this book, you’ll meet common use-case sidebars such as this\none that give you a peek at how some of the language features being\ndiscussed are typically used in real programs. Because you won’t be able to\nmake much sense of realistic use cases until you’ve seen more of the Python\npicture, these sidebars necessarily contain many references to topics not\nintroduced yet; at most, you should consider them previews of ways that you\nmay find these abstract language concepts useful for practical programming\ntasks.\nFor instance, you’ll see later that the argument words listed on a system\ncommand line used to launch a Python program are made available in the\nargv attribute of the built-in sys module:\n# File echo.py\nimport sys\nprint(sys.argv)\n$ python3 echo.py −a −b −c\n['echo.py', '−a', '−b', '−c']\nUsually, you’re interested only in inspecting the arguments that follow the\nprogram name. This leads to a typical application of slices: a single slice\nexpression can be used to return all but the first item of a list. Here,\nsys.argv[1:] returns the desired list, ['−a', '−b', '−c']. You can then\nprocess this list without having to accommodate the program name at the\nfront.",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": 6,
      "content": "Slices are also often used to clean up lines read from input files, of the sort\nwe’ll study in Chapter 9. If you know that a line will have a newline\ncharacter at the end (a \\n), you can get rid of it with a single expression such\nas line[:−1], which extracts all but the last character in the line. In both\ncases, slices do the job of logic that must be explicit in a lower-level\nlanguage.\nHaving said that, calling the line.rstrip method is often preferred for\nstripping newline characters because this call leaves the line intact if it has\nno newline character at the end—a common case for files created with some\ntext-editing tools. Slicing works only if you’re sure the line is properly\nterminated.\nString Conversion Tools\nOne of Python’s design mottos is that it refuses the temptation to guess. As a\nprime example, you cannot add a number and a string together in Python, even if\nthe string looks like a number (i.e., is all digits):\n>>> '62' + 1\nTypeError: can only concatenate str (not \"int\") to str\nThis is by design: because + can mean both addition and concatenation, the\nchoice of conversion would be ambiguous—do you want '621' or 63? Instead,\nPython treats this as an error. In Python, magic is generally omitted if it will\nmake your coding life more complex.\nWhat to do, then, if your script obtains a number as a text string from a file or\nuser interface? The trick is that you must simply employ conversion tools before\nyou can treat a string like a number, or vice versa. For instance:\n>>> int('62'), str(62)          # Convert from/to string\n(62, '62')\nThe int function converts a string to a number, and the str function converts a\nnumber to its string representation (essentially, what it looks like when printed).\nNow, although you can’t mix strings and number types around operators such as",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": 6,
      "content": "+, you can manually convert operands before that operation if needed:\n>>> S = '62'\n>>> I = 1\n>>> S + I\nTypeError: can only concatenate str (not \"int\") to str\n>>> int(S) + I            # Force addition\n63\n>>> S + str(I)            # Force concatenation\n'621'\nSimilar built-in functions handle floating-point-number conversions to and from\nstrings, if you need to mix the two in expressions:\n>>> float('1.5') + 2.8\n4.3\n>>> '1.5' + str(2.8)\n'1.52.8'\nThe built-in eval function introduced in Chapter 5 runs a string containing\nPython expression code, and so can also convert a string to any kind of object.\nThe functions int and float convert only to numbers, but this restriction means\nthey are usually faster (and more secure, because they do not accept arbitrary\nexpression code). As we also saw briefly in Chapter 5, string formatting provides\nother ways to convert numbers to strings; more on it ahead.\nCharacter-code conversions\nOn the subject of conversions, it is also possible to convert a single character to\nits underlying integer code by passing it to the built-in ord function—this returns\nthe numeric “ordinal” value used to represent the corresponding character in\nmemory (technically, its Unicode code point, as you’ll learn in Chapter 37, but\nthis isn’t crucial yet). The chr function performs the inverse operation, taking an\ninteger code and converting it to the corresponding character:\n>>> ord('h')               # Character => ID (code point)\n104\n>>> chr(104)               # ID => character (string)\n'h'",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": 6,
      "content": ">>> for c in 'hack':       # All code points in a string\n...     print(c, ord(c))\n... \nh 104\na 97\nc 99\nk 107\nYou can use a loop to apply ord to all characters in a string as shown, but these\ntools can also be used to perform a simple sort of string-based math. To advance\nto the next character, for example, convert and do the math in integer:\n>>> S = '5'\n>>> S = chr(ord(S) + 1)\n>>> S\n'6'\n>>> S = chr(ord(S) + 1)\n>>> S\n'7'\nAt least for single-character strings, this provides an alternative to using the\nbuilt-in int function to convert from string to integer (though this only makes\nsense if character ordinals are ordered as your code expects!):\n>>> int('5')\n5\n>>> ord('5') - ord('0')\n5\nString comparisons\nAnother reason for introducing ordinals here is that it helps us understand string\ncomparisons: when we compare two text strings, Python automatically compares\nthem left to right, character by character, and lexicographically—that is, by the\nsame character code-point values returned by ord—until the first mismatch or\nend of either string. In the following, for example, the code point of t is greater\nthan that of k, and the longer string at the end wins:\n>>> 'hack' == 'hack', 'hact' > 'hack', 'hacker' > 'hack'\n(True, True, True)",
      "content_length": 1242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": 6,
      "content": "The same holds true for the byte strings you’ll meet in Chapter 37 (they are\ncompared byte for byte until a result is known), and the next chapter’s richer\ncollections like lists do similar (Python compares all their parts for you).",
      "content_length": 232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": 6,
      "content": "“Changing” Strings Part 1: Sequence Operations\nRemember the term immutable sequence? As we’ve seen, being a sequence\nmeans that strings support operations like concatenation, repetition, indexing,\nand slicing. The immutable part means that you cannot change a string in place\n—for instance, by assigning to an index:\n>>> S = 'text'\n>>> S[0] = 'n'\nTypeError: 'str' object does not support item assignment\nHow to modify textual information in Python, then? To change a string, you\ngenerally need to build a new string using tools such as concatenation and\nslicing, and assign the result back to the string’s original name if desired:\n>>> S = 'text'\n>>> S = S + 'ual!'           # To change a string, make a new one\n>>> S\n'textual!'\n>>> S = S[:4] + ' processing' + S[-1]\n>>> S\n'text processing!'\nThe first example adds a substring at the end of S, by concatenation. Really, it\nmakes a new string and assigns it back to S to save it, but you can think of this as\n“changing” the original string. The second example replaces three characters\nwith many by slicing, indexing, and concatenating. As you’ll see in the next\nsection, you can achieve similar effects with string methods like replace:\n>>> S = 'text'\n>>> S = S.replace('ex', 'hough')\n>>> S\n'thought'\nLike every operation that yields a new string value, string methods generate new\nstring objects. If you want to retain those objects, you can assign them to\nvariable names. Whether by sequence operations or methods, generating a new\nstring object for each string change is not as inefficient as it may sound—\nremember, as discussed in the preceding chapter, Python automatically garbage-",
      "content_length": 1639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": 6,
      "content": "collects (reclaims the space of) old unused string objects as you go, so newer\nobjects reuse the space held by prior values. Python is usually more efficient\nthan you might expect.\nBut string methods can do much more, as the next section will explain.\nNOTE\nExcept for bytearray: As previewed in Chapter 4 and to be covered in Chapter 37, Python has\na string type known as bytearray, which is mutable and so may be changed in place.\nbytearray objects aren’t really text strings; they’re sequences of small, 8-bit integers.\nHowever, they support most of the same operations as normal strings and print as ASCII\ncharacters when displayed. Accordingly, they provide another option for large amounts of\nsimple 8-bit text that must be changed frequently. Richer Unicode text and str strings in\ngeneral, though, require techniques shown here.\nString Methods\nIn addition to all the string operations already introduced, strings provide a set of\nmethods that support more sophisticated text-processing goals. In Python,\nexpressions and built-in functions may work across a range of types, but\nmethods are generally specific to object types—string methods, for example,\nwork only on string objects. Some method names are used by multiple objects in\nPython for consistency (e.g., many have count and copy methods, and most\nmutables have a pop), but they are still more type specific than other tools.\nMethod Call Syntax\nAs introduced in Chapter 4, methods are simply functions that are associated\nwith and act upon particular objects. Technically, they are attributes attached to\nobjects that happen to reference callable functions which always have an implied\nsubject. In finer-grained detail, functions are packages of code, and method calls\ncombine two operations at once—an attribute fetch and a call:\nAttribute fetches\nAn expression of the form object.attribute means “fetch the value of",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": 6,
      "content": "attribute in object.”\nCall expressions\nAn expression of the form function(arguments) means “invoke the code\nof function, passing zero or more comma-separated argument objects to it,\nand return function’s result value.”\nPutting these two together allows us to call a method of an object. The method\ncall expression:\nobject.method(arguments)\nis evaluated from left to right—Python will first fetch the method of the object\nand then call it, passing in both object and the arguments. Or, in plain words,\nthe method call expression means this:\nCall method to process object with arguments.\nIf the method computes a result, it will also come back as the result of the entire\nmethod-call expression. As a more tangible example:\n>>> S = 'hack'\n>>> result = S.find('ac')     # Call the find method to look for 'ac' in string S\nThis mapping holds true for methods of both built-in types, as well as user-\ndefined classes we’ll study later. As you’ll see throughout this part of the book,\nmost objects have callable methods, and all are accessed using this same\nmethod-call syntax. To call an object method, as you’ll see in the following\nsections, you have to go through an existing object; methods cannot be run (and\nmake little sense) without a subject.\nAll String Methods (Today)\nTable 7-3 summarizes the methods and call patterns for built-in string objects in\nPython 3.12. These change over time, so be sure to check Python’s standard-",
      "content_length": 1431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": 6,
      "content": "library manual for the most up-to-date list, or run a dir or help call interactively\non any string or the str type name, as shown in Chapter 4.\nIn this table, S is a string object; optional arguments are enclosed in [] brackets;\nnested [] mean optional following an optional; *X and **X mean any number X;\nand the listed methods implement higher-level operations such as splitting and\njoining, case conversions, content tests, and substring searches and\nreplacements.\nTable 7-3. String method calls in Python 3.12\nS.capitalize()\nS.ljust(width [, fill])\nS.casefold()\nS.lower()\nS.center(width [, fill])\nS.lstrip([chars])\nS.count(sub [, start [, end]])\nS.maketrans(x [, y [, z]])\nS.encode([encoding [, errors]])\nS.partition(sep)\nS.endswith(suffix [, start [, end]])\nS.removeprefix(prefix)\nS.expandtabs([tabsize])\nS.removesuffix(suffix)\nS.find(sub [, start [, end]])\nS.replace(old, new [, count])\nS.format(fmtstr, *args, **kwargs)\nS.rfind(sub [, start [, end]])\nS.format_map(mapping)\nS.rindex(sub [, start [, end]])\nS.index(sub [, start [, end]])\nS.rjust(width [, fill])\nS.isalnum()\nS.rpartition(sep)\nS.isalpha()\nS.rsplit([sep [, maxsplit]])\nS.isascii()\nS.rstrip([chars])\nS.isdecimal()\nS.split([sep [, maxsplit]])\nS.isdigit()\nS.splitlines([keepends])",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": 6,
      "content": "S.isidentifier()\nS.startswith(prefix [, start [, end]])\nS.islower()\nS.strip([chars])\nS.isnumeric()\nS.swapcase()\nS.isprintable()\nS.title()\nS.isspace()\nS.translate(map)\nS.istitle()\nS.upper()\nS.isupper()\nS.zfill(width)\nS.join(iterable)\nAs you can see, strings have many methods, and we don’t have space to cover\nthem all here; omissions can be found in other resources when needed. To help\nyou get started, though, let’s work through some code that demonstrates some of\nthe most commonly used methods in action and illustrates Python text-\nprocessing basics along the way.",
      "content_length": 569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": 6,
      "content": "“Changing” Strings, Part 2: String Methods\nAs we’ve seen, most strings cannot be changed in place directly because they are\nimmutable. We explored changing strings with sequence operations in the\npreceding section, but let’s resume that story here in the context of methods.\nBy way of review, to make a new text value from an existing string, you can\nconstruct a new string with sequence operations such as slicing and\nconcatenation. For example, to replace two characters in the middle of a string,\nyou can use code like this, much as we did in the prior section:\n>>> S = 'textly!'\n>>> S[:4] + 'ful' + S[-1]             # Make a new string with sequence ops\n'textful!'\nBut, if you’re really just out to replace a substring, you can use the string\nreplace method instead:\n>>> S = 'textly!'\n>>> S.replace('ly', 'ful')            # Replace all 'ly' with 'ful' in S\n'textful!'\nThe replace method is more general than this code implies. It takes as\narguments the original substring (of any length) and a new substring (of any\nlength) to replace the original, and performs a global search and replace—\nsubject to an optional third argument that limits the number of replacements\nmade:\n>>> '--@--@--@--'.replace('@', 'PY', 2)\n'--PY--PY--@--'\nIn such a role, replace can be used as a tool to implement simple template\nreplacements (e.g., in form letters). If you need to replace one fixed-size string\nthat can occur at any offset, you can do a replacement again, or search for the\nsubstring with the string find method and then slice:\n>>> S = 'xxxxPYxxxxPYxxxx'\n>>> where = S.find('PY')              # Search for position",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": 6,
      "content": ">>> where                             # Occurs at offset 4\n4\n>>> S = S[:where] + 'CODE' + S[(where+2):]\n>>> S\n'xxxxCODExxxxPYxxxx'\nThe find method returns the offset where a substring appears, or −1 if it is not\nfound (it searches from the front by default, and its cousin rfind searches in\nreverse). As we saw earlier, this is a substring search operation just like the in\nexpression, but find returns the position of a located substring. In this context,\nreplace does the job easier, and can do more—in the following, replacing both\nmultiple occurrences and multiple targets:\n>>> S = 'xxxxPYxxxxPYxxxx'\n>>> S.replace('PY', 'CODE', 1)        # Replace one\n'xxxxCODExxxxPYxxxx'\n>>> S.replace('PY', 'CODE')           # Replace all\n'xxxxCODExxxxCODExxxx'\n>>> 'xxxxWHATxxxxHOWxxxx'.replace('WHAT', 'CODE').replace('HOW', 'PYTHON')\n'xxxxCODExxxxPYTHONxxxx'\nAs a reminder, replace returns a new string object each time (which is why two\ncalls can be strung together here). Because strings are immutable, methods, like\nsequence operations, never really change the subject string in place—even if\nthey are called “replace”! To save the new string object produced by a method\ncall, assign it to a name:\n>>> S = S.replace('PY', 'CODE')\n>>> S\n'xxxxCODExxxxCODExxxx'\nThe fact that concatenation operations and the replace method generate new\nstring objects each time they are run is a potential downside of using them to\nchange strings: each interim result must create a full-fledged object with a fresh\ncopy of its text. If you have to apply many changes to a very large string, you\nmight be able to improve your script’s performance by converting the string to\nan object that does support in-place changes:",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": 6,
      "content": ">>> S = 'text'\n>>> L = list(S)                       # Explode string into a list\n>>> L\n['t', 'e', 'x', 't']\nThe built-in list function (really, an object construction call) builds a new list\nout of the items in any sequence (or other iterable)—in this case, “exploding” the\ncharacters of a string into a list. Once the string is in this form, you can make\nmultiple changes to it without generating a new copy for each change:\n>>> L[0] = 'h'                        # Works for lists, not strings\n>>> L[3] = '!'\n>>> L\n['h', 'e', 'x', '!']\nAfter your changes, you can convert back to a string if needed (e.g., to write to a\nfile) by using the string join method to “implode” the list back into a string:\n>>> S = ''.join(L)                    # Implode back to a string\n>>> S\n'hex!'\nThe join method may look a bit backward on first encounter. Because it is a\nmethod of strings (not of lists), it is called through the desired delimiter string.\njoin puts the strings in a list (or other iterable) together, with the delimiter\nbetween list items; in this case, it uses an empty string delimiter to convert from\na list back to a string. More generally, any string delimiter and iterable of strings\nwill do:\n>>> 'PY'.join(['which', 'language', 'is', 'best', '?'])\n'whichPYlanguagePYisPYbestPY?'\nThough subject to Python implementation, joining substrings all at once might\nrun faster than concatenating them individually. The mutable bytearray string\nnoted earlier may help with efficiency too; because it can be changed in place, it\noffers an alternative to this list/join combo for simple kinds of byte-sized text\nlike ASCII.",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": 6,
      "content": "More String Methods: Parsing Text\nAnother common role for string methods is as a simple form of text parsing—\nthat is, analyzing structure and extracting substrings. To extract substrings at\nfixed offsets, we can employ slicing techniques:\n>>> line = 'aaa bbb ccc'\n>>> col1 = line[:3]\n>>> col2 = line[4:8]\n>>> col3 = line[-3:]\n>>> col1, col2, col3\n('aaa', 'bbb ', 'ccc')\nHere, the columns of data appear at fixed offsets and so may be sliced out of the\noriginal string. This technique passes for parsing, as long as the components of\nyour data have known positions. If instead some sort of delimiter separates the\ndata, you can pull out its components by splitting. This will work even if the data\nmay show up at arbitrary positions within the string:\n>>> line = 'aaa bbb     ccc'\n>>> cols = line.split()\n>>> cols\n['aaa', 'bbb', 'ccc']\nThe string split method chops up a string into a list of substrings, around a\ndelimiter string. We didn’t pass a delimiter in the prior example, so it defaults to\nwhitespace—the string is split at groups of one or more spaces, tabs, and\nnewlines, and we get back a list of the resulting substrings. In other applications,\nmore tangible delimiters may separate the data. To demo, the next example splits\n(and hence parses) the string at commas, a separator common in some database\nroles (string conversion tools covered earlier can change substrings here into\nnumbers):\n>>> line = 'Python,3.12,scripting,33'\n>>> line.split(',')\n['Python', '3.12', 'scripting', '33']\nDelimiters can be longer than a single character, too:",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": 6,
      "content": ">>> line = 'youPYarePYaPYstringPYcoder'\n>>> line.split('PY')\n['you', 'are', 'a', 'string', 'coder']\nAlthough there are limits to the parsing potential of slicing and splitting, both\nrun fast and can handle basic text-extraction chores. Comma-separated text data\nis also part of the CSV file format; for a more advanced tool on this front, see\nalso the csv module in Python’s standard library.\nOther Common String Methods\nOther string methods have more focused purposes—for example, to strip off\nwhitespace at the end of a line of text, perform case conversions, test content,\nand test for a substring at the end or front:\n>>> line = \"Python's strings are awesome!\\n\"\n>>> line.rstrip()                       # Drop whitespace (or other)\n\"Python's strings are awesome!\"\n>>> line.upper()                        # Case conversions\n\"PYTHON'S STRINGS ARE AWESOME!\\n\"\n>>> line.isalpha()                      # Content tests\nFalse\n>>> line.endswith('awesome!\\n')         # Suffix and prefix tests\nTrue\n>>> line.startswith('Python')\nTrue\nAlternative techniques can also sometimes be used to achieve the same results as\nstring methods—the in membership operator can be used to test for the presence\nof a substring, for instance, and length and slicing operations can be used to\nmimic endswith:\n>>> line.find('awesome') != -1          # Search via method call or expression\nTrue\n>>> 'awesome' in line\nTrue\n>>> sub = 'awesome!\\n'\n>>> line.endswith(sub)                  # End test via method call or slice\nTrue\n>>> line[-len(sub):] == sub\nTrue",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": 6,
      "content": "Note that none of the string methods accepts patterns—for pattern-based text\nprocessing, you must use the Python re standard-library module, an advanced\ntool that will be introduced briefly in Chapter 37 but is mostly outside the scope\nof this text. Because of their limitations, though, string methods may run more\nquickly than the re module’s tools.\nAgain, because there are so many methods available for strings, we won’t look at\nevery one here. You’ll see some additional string examples later in this book, but\nfor more details you can also turn to the Python library manual and other\nreference resources, or simply experiment interactively on your own. As noted in\nChapter 4, help(S.method) gives info for a method of any string object S; use\nhelp(str.method) if you have no S.\nAll that being said, one method is noticeably absent from this section’s coverage:\nformat performs string formatting, which combines many operations in a single\nstep. It’s also part of a larger topic in Python, which we turn to next.\nString Formatting: The Triathlon\nAlthough you can get a lot done with the string methods and sequence\noperations you’ve already met, Python also provides a more advanced way to\ncombine string processing tasks: string formatting allows us to perform multiple\ntype-specific substitutions on a string in a single step. It’s never strictly required,\nbut it can be convenient, especially when laying out text to be displayed to a\nprogram’s users.\nWe’ve used string formatting informally in this book already, but it’s finally time\nto dig into its details. As suggested in earlier examples, this story is regrettably\nconvoluted by Python’s history: there are today three different string-formatting\ntools that broadly overlap in functionality. This curious state of affairs reflects a\ncommon pattern in software development—new tools arise that boldly promise\nto be radical improvements over the past, only to be supplanted by even newer\ntools that boldly make the exact same claims.\nThe net effect handicaps languages with redundancy, and users with\nunnecessarily steep learning curves. While learning resources could present just\none of many options, that would both impose authors’ opinions and do a vast",
      "content_length": 2219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": 6,
      "content": "disservice to readers: even if you’re able to pick just one of the formatting tools\nfor your own work, the fact that the others have been available for decades and\nhave been used by millions of programmers virtually guarantees that you’ll be\nseeing them in the wild when you begin reusing other people’s code. Try as it\nmay, the new cannot erase the old.\nHence, this chapter presents all three formatting tools for the sake of\ninclusiveness. If you’re new to Python or programming in general, you probably\nshould focus on the current latest-and-greatest f-string option—not because it’s\nnecessarily “better,” but because it’s more likely to garner development attention\nand less likely to be deprecated in a backward-incompatible future (its\npredecessors have been spared this fate to date, but Python has a history here).\nBut that’s not to say that the others are out of the race: the expression and\nmethod alternatives may feel more comfortable to readers with backgrounds in\nsome other tools, and both are pervasive in the vast reams of Python code\nwritten over the last thirty-some years. Learning all three options hedges your\nbets best.\nString-Formatting Options\nAs a preview of what we’re going to explore in this section, here are today’s\nentries in the string-formatting race:\nFormatting expression: '…%s…%s…' % (value, value)\nThe original technique available since Python’s inception, this form is\nloosely based upon the C language’s printf model and sees widespread use\nin much existing code. Values on the right replace targets on the left.\nFormatting method: '…{}…{}…'.format(value, value)\nA newer technique added in Python 3.0, this form is derived in part from a\nsame-named tool in C#/.NET. It largely overlaps with the expression’s\nfunctionality but aims to address usage modes subjectively deemed subpar.\nFormatting literal: f'…{value}…{value}…'",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": 6,
      "content": "The very latest (so far) added in Python 3.6, this form is known as f-strings.\nIt shares much with the method but apes a host of languages that support\nstring interpolation—substituting inline expressions with their results.\nYou can also format strings manually with string methods, though it’s too\ncumbersome to count. And technically, an additional tool, string.Template,\npredates the method—and drives the formatting set’s length up to a whopping\nfour—but it’s so scantly used that it gets less billing than the three primary\noptions above and is relegated to a brief sidebar here (and you’d be excused for\npretending it doesn’t exist at all, given the heft of the formatting toolbox!).\nThe following sections present all three formatting options above in turn. While\nit may be tempting to jump straight to whatever the blogosphere may be\nrecommending as you read these words, these sections partly build on each other\n(e.g., f-strings use the method’s format specifier and assume its earlier\ncoverage), so a linear read is suggested.\nThe String-Formatting Expression\nSince string-formatting expressions are the original in this department, we’ll start\nwith them. Python defines the % binary operator to work on strings. You may\nrecall that this is also the remainder of division, or modulus, operator for\nnumbers. When applied to strings, the % operator provides a simple way to\nformat values as strings according to a format definition. It’s a much more\nconcise way to code multiple substitutions than processing parts individually.\nFormatting expression basics\nTo format strings with an expression:\n1. On the left of the % operator, provide a format string containing one or\nmore embedded conversion targets, each of which starts with a % (e.g.,\n%d).\n2. On the right of the % operator, provide the object that you want Python\nto insert into the format string on the left in place of the conversion",
      "content_length": 1903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": 6,
      "content": "target; for multiple targets, provide multiple objects in a tuple.\nFor instance, in the following formatting example, the integer 3 replaces the %d\nin the format string on the left, and the string 'format' replaces the %s. The\nresult is a new string that reflects these two substitutions, which may be printed\nor saved for use in other roles:\n>>> 'There are %d ways to %s!' % (3, 'format')      # Formatting expression\n'There are 3 ways to format!'\nTechnically speaking, string formatting in any flavor is usually optional—you\ncan generally do similar work with multiple concatenations and conversions.\nHowever, formatting allows us to combine many steps into a single operation.\nIt’s powerful enough to warrant a few more introductory examples:\n>>> option = 'expression'\n>>> 'Meet the formatting %s!' % option              # String substitution\n'Meet the formatting expression!'\n>>> '%d %s %g you' % (1, 'formatter', 4.0)          # Type-specific substitutions\n'1 formatter 4 you'\n \n>>> '%s -- %s -- %s' % (42, 3.14159, [1, 2, 3])     # All types match a %s target\n'42 -- 3.14159 -- [1, 2, 3]'\nThe first example here plugs a string into the target on the left, replacing the %s\nmarker. In the second example, three values are inserted into the target string.\nNotice that when you’re inserting more than one value, you need to group the\nvalues on the right in parentheses—that is, put them in a tuple. The % operator’s\nright side generally expects a tuple of one or more items (or a dictionary of items\nfor key references, covered ahead), but allows a single nontuple item if there is\njust one substitution target. You’ll know which form to use when coding the\nexpression, of course, but this difference was nevertheless deemed a quirk\nsufficient to justify other formatting options over time.\nThe third example again inserts three values—an integer, a floating-point\nnumber, and a list—but notice that all of the targets on the left are %s, which\nstands for conversion to string. As every type of object can be converted to a\nstring (the one used when printing), every object type works with the %s",
      "content_length": 2099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": 6,
      "content": "conversion code. Because of this, unless you need to do special formatting, %s is\noften the only code you need to remember for the formatting expression.\nAgain, keep in mind that formatting always makes a new string, rather than\nchanging the string on the left; because strings are immutable, it must work this\nway. As before, assign the result to a variable name if you need to retain it.\nFormatting expression custom formats\nFor more advanced type-specific formatting, you can use any of the conversion\ntype codes listed in Table 7-4 in formatting expressions; they appear after the %\ncharacter in substitution targets. C programmers will recognize most of these\nbecause Python string formatting supports all the usual C printf format codes\n(but returns the result, instead of displaying it like printf). Some of the format\ncodes in the table provide alternative ways to format the same type; for instance,\n%e, %f, and %g provide alternative ways to format floating-point numbers.\nTable 7-4. Formatting-expression type codes\nCode\nMeaning\ns\nString (or any object’s str(X) string)\nr\nSame as s, but uses repr, not str\na\nSame as s, but uses ascii, not str\nc\nCharacter (integer code or string)\nd\nDecimal (signed base-10 integer)\ni\nInteger (see d)\nu\nSame as d (obsolete: no longer unsigned)\no\nOctal integer (base 8)\nx\nHex integer (base 16)",
      "content_length": 1335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": 6,
      "content": "X\nSame as x, but with uppercase letters\ne\nFloating point with exponent, lowercase\nE\nSame as e, but uses uppercase letters\nf\nFloating-point decimal\nF\nSame as f, but uses uppercase letters\ng\nFloating-point e or f\nG\nFloating-point E or F\n%\nLiteral % (coded as %%)\nAll told, conversion targets in the format string on the expression’s left side\nsupport a variety of conversion operations with a fairly sophisticated syntax all\ntheir own. In formal terms, the general structure of conversion targets looks like\nthe following, where […] denotes an optional part (its two square-bracket\ncharacters are not included in the expression’s code), and no spaces are allowed\nbetween parts (though some parts may contain spaces):\n%[(keyname)][flags][width][.precision]typecode\nOne of the type code characters in the first column of Table 7-4 shows up at the\nend of this target string’s format, at typecode. Between the % and this type code\ncharacter, you can do any (or none) of the following:\nProvide a key name for indexing the dictionary used on the right side of\nthe expression.\nList flags that specify zero padding (0), left justification (−), numeric\nsign (+), or a blank before positive numbers and a – for negatives (a\nspace), where − overrides 0, and + overrides a space.\nGive a total but minimum field width for the substituted text.",
      "content_length": 1328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": 6,
      "content": "Set the number of digits (precision) to display after a decimal point for\nfloating-point numbers.\nBoth the width and precision parts can also be coded as a * to specify that\nthey should take their values dynamically from the next item in the input values\non the expression’s right side (useful when this isn’t known until your code is\nrun, but unavailable when keynames are used). And if you don’t need any of\nthese extra tools, a simple %s in the format string will be replaced by the\ncorresponding value’s default print string, regardless of its type.\nAdvanced formatting expression examples\nFormatting target syntax is documented in full in the Python standard manuals\nand other reference resources, but to demonstrate common usage, let’s explore a\nfew examples. The first formats integers using the default, and then in a six-\ncharacter field with left justification and zero padding:\n>>> x = 1234\n>>> res = 'integers: ...%d...%-6d...%06d' % (x, x, x)\n>>> res\n'integers: ...1234...1234  ...001234'\nThe %e, %f, and %g formats display floating-point numbers in different ways, as\nthe following interaction demonstrates—%E is the same as %e but the exponent is\nuppercase, and g chooses formats by number content (it’s formally defined to\nuse exponential format e if the exponent is less than −4 or not less than\nprecision, and decimal format f otherwise, with a default minimum total-digits\nprecision of 6; no, really!):\n>>> x = 1.23456789\n>>> x                                   # Default REPL display\n1.23456789\n>>> '%e | %f | %g' % (x, x, x)\n'1.234568e+00 | 1.234568 | 1.23457'\n>>> '%E' % x\n'1.234568E+00'\nFor floating-point numbers, you can achieve a variety of additional formatting",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": 6,
      "content": "effects by specifying left justification, zero padding, numeric signs, total field\nwidth, and digits after the decimal point. For simpler tasks, you might get by\nwith simply converting to strings with a %s type code or the str built-in function\nwe used earlier:\n>>> '%-6.2f | %05.2f | %+06.1f' % (x, x, x)\n'1.23   | 01.23 | +001.2'\n>>> '%s' % x, str(x)\n('1.23456789', '1.23456789')\nWhen sizes are not known until runtime, you can use a dynamically computed\nwidth and precision by specifying them with a * in the format string to force\ntheir values to be taken from the next item in the inputs to the right of the %\noperator—the 4 in the tuple here gives precision:\n>>> '%f, %.2f, %.*f' % (1/3.0, 1/3.0, 4, 1/3.0)\n'0.333333, 0.33, 0.3333'\nAs usual, experiment with some of these examples and operations on your own\nfor more insight.\nDictionary-based formatting expressions\nAs a more advanced extension, % string formatting also allows conversion targets\non the left to refer to the keys in a dictionary coded on the right and use the\ncorresponding values. Syntactically, this form requires () key references on the\nleft of %, and a single dictionary (or other mapping) on the right. Functionally, it\nallows formatting to be used as a basic template tool. You’ve met dictionaries\nonly briefly thus far in Chapter 4, but the following demos the idea:\n>>> '%(qty)s more %(tool)s' % {'qty': 1, 'tool': 'formatter'}\n'1 more formatter'\nHere, the (qty) and (tool) in the format string on the left refer to keys in the\ndictionary literal on the right and fetch their associated values. Programs that\ngenerate text such as HTML or XML often use this form: they build up a\ndictionary of values and substitute them all at once with a single formatting",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": 6,
      "content": "expression, using key references and a template string either loaded from a file\nor coded in the script. The following demos the idea (notice that its first\ncomment is above the triple quote to keep it out of the string, and “…” prompts\nmay not appear in your REPL):\n>>>                                           # Template with substitution targets\n>>> reply = \"\"\"\n... Hello %(name)s!\n... Welcome to %(year)s\n... \"\"\"\n \n>>> values = {'name': 'Pat', 'year': 2024}    # Build up values to substitute\n>>> print(reply % values)                     # Perform substitutions\nHello Pat!\nWelcome to 2024\nThis trick is also sometimes used in conjunction with the vars built-in function,\nwhich returns a dictionary containing all the variables that exist in the place it is\ncalled:\n>>> name = 'Pat'\n>>> year = 2024\n>>> vars()\n{'name: 'Pat', 'year': 2024, …plus built-in names set by Python…}\nWhen used on the right side of a format operation, this allows the format string\nto refer to variables by name—using dictionary-key syntax:\n>>> '%(name)s from %(year)s' % vars()         # Variables are keys in vars()\n'Pat from 2024'\nAlthough formatting expressions are positional by nature, dictionaries also allow\nthem to reuse values more than once (see Chapter 5 for another demo of this in\naction):\n>>> '%(value)f, %(value).2f, %(value).f' % ({'value': 1 / 3.0})\n'0.333333, 0.33, 0'\nWe’ll study dictionaries in more depth in Chapter 8. See also “Hex, Octal, and\nBinary” for examples that convert to hexadecimal and octal number strings with",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": 6,
      "content": "the %x and %o formatting expression target codes, which we won’t repeat here.\nAdditional formatting expression examples also appear ahead as comparisons to\nthe formatting method and f-string—the first of which is this chapter’s next\ntopic.\nThe String-Formatting Method\nAs noted, Python 3.0 added a second way to format strings that some see as\nmore Python specific. Unlike formatting expressions, the formatting method is\nnot as closely based upon the C language’s “printf” model; is sometimes more\nexplicit in intent; and avoids the value-or-tuple quirk of %: substituted values are\njust arguments, whether one or many.\nOn the other hand, the new technique still relies on “printf” concepts like type\ncodes and formatting specifications. Moreover, it largely overlaps with\nformatting expressions; often yields more verbose code; and in practice can be\njust as complex in most roles. And if we’re all being honest, the value-or-tuple\nquirk of the % expression is much more of a concern in theory than practice.\nLuckily, the two are similar enough that many core concepts overlap.\nFormatting method basics\nThe string object’s format method at the heart of this option is based on function\ncall syntax, instead of an expression. Specifically, it uses the call’s subject string\nas a template and takes any number of arguments that represent values to be\nsubstituted according to the template.\nThis option requires knowledge of functions and calls but is mostly\nstraightforward. Within the subject string, curly braces designate substitution\ntargets and arguments to be inserted—either by relative position ({}), absolute\nposition (e.g., {1}), or keyword (e.g., {name}). As you’ll learn when we explore\nargument passing in depth in Chapter 18, arguments to functions and methods\nmay be passed by position (e.g., value) or keyword (e.g., name=value), and\nPython’s ability to collect arbitrarily many arguments allows for general method-\ncall patterns. As initial examples:\n>>> template = '{}, {}, and {}'                               # Relative position\n>>> template.format('expr', 'method', 'fstring')",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": 6,
      "content": "'expr, method, and fstring'\n>>> template = '{0}, {1}, and {2}'                            # Absolute position\n>>> template.format('expr', 'method', 'fstring')\n'expr, method, and fstring'\n>>> template = '{first}, {second}, and {third}'               # Keyword name\n>>> template.format(first='expr', second='method', third='fstring')\n'expr, method, and fstring'\n>>> template = '{first}, {0}, and {third}'                    # Combos ({0} or {})\n>>> template.format('method', first='expr', third='fstring')\n'expr, method, and fstring'\nBy comparison, the last section’s formatting expression can be a bit more\nconcise, but uses dictionaries instead of keyword arguments for named\nreferences, and as you’ll see in a moment doesn’t allow quite as much flexibility\nfor value sources in the template string itself (which may be an asset or liability,\ndepending on your perspective):\n>>> template = '%s, %s, and %s'                               # Equivalent %s \n>>> template % ('expr', 'method', 'fstring') \n'expr, method, and fstring'\n>>> template = '%(first)s, %(second)s, and %(third)s'\n>>> template % dict(first='expr', second='method', third='fstring') \n'expr, method, and fstring'\nNote the use of dict() to make a dictionary from keyword arguments here,\nintroduced in Chapter 4 and covered in full in Chapter 8; it’s an often less\ncluttered alternative to the {…} literal. Naturally, the subject string in the format\nmethod call can also be a literal that creates a temporary string, and arbitrary\nobject types can be substituted at targets much like the expression’s %s code:\n>>> '{pi}, {} and {years}'.format(62, pi=3.14, years=[1995, 2024])\n'3.14, 62 and [1995, 2024]'\nJust as with the % expression and other string methods, format creates and\nreturns a new string object, which can be printed immediately or saved for\nfurther work (as another reminder, strings are immutable, so format really must\nmake a new object). String formatting in any of its forms is not just for display:",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": 6,
      "content": ">>> X = '{pi}, {} and {years}'.format(62, pi=3.14, years=[1995, 2024])\n>>> X\n'3.14, 62 and [1995, 2024]'\n>>> X.split(' and ')\n['3.14, 62', '[1995, 2024]'] \n>>> Y = X.replace('and', 'but under no circumstances')\n>>> Y\n'3.14, 62 but under no circumstances [1995, 2024]'\nAdding keys, attributes, and offsets\nLike % formatting expressions, format calls can become more complex to\nsupport more advanced usage. For instance, format strings can name object\nattributes and dictionary keys—as in normal Python syntax, square brackets\nname dictionary keys and dots denote object attributes of an item referenced by\nposition or keyword. The first of the following examples indexes a dictionary on\nthe key kind and then fetches the attribute platform from the already imported\nsys module object. The second does the same, but names the objects by\nkeyword instead of position:\n>>> import sys    # Standard-library module\n>>> 'This {1[kind]} runs {0.platform}'.format(sys, {'kind': 'laptop'})\n'This laptop runs darwin'\n>>> 'This {map[kind]} runs {sys.platform}'.format(sys=sys, map={'kind': 'phone'})\n'This phone runs linux'\nSquare brackets in format strings can also name offsets to index lists (and other\nsequences), but only a single positive offset works syntactically within each [],\nso this feature is not as general as you might think. To reference negative offsets\nor slices, or to use arbitrary expression results in general, you must run\nexpressions outside the format string itself, just as you would for % expressions\n(note the use of *parts here to unpack a tuple’s items into individual function\narguments; you’ll learn more about this form when we study function arguments\nin Chapter 18):\n>>> somelist = list('HACK')\n>>> somelist",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": 6,
      "content": "['H', 'A', 'C', 'K'] \n>>> 'zero={0[0]}, two={0[2]}'.format(somelist)\n'zero=H, two=C'\n>>> 'first={}, last={}'.format(somelist[0], somelist[-1])    # [-1] fails in fmt\n'first=H, last=K'\n>>> parts = (somelist[0], somelist[-1], somelist[1:3])       # [1:3] fails in fmt\n>>> 'first={}, last={}, middle={}'.format(*parts)            # Or {0}, {1}, {2}\n\"first=H, last=K, middle=['A', 'C']\"\nIf you simply cannot do without full generality inside format strings, stay tuned\nfor the f-string and its arbitrary nested expressions (albeit in a code literal instead\nof a method object, and at the cost of more redundancy and less utility).\nFormatting method custom formats\nAnother similarity with % expressions is that format lets you can achieve more\nspecific layouts with extra format-string syntax. For the formatting method, we\nuse a colon after the possibly empty substitution target’s identification, followed\nby a format specifier that can name the field size, justification, and a specific\ntype code. Here’s the formal structure of what can appear as a substitution target\nin a format string—its four parts are all optional (denoted by surrounding []\nhere, which aren’t coded in the format string) and must appear without\nintervening spaces:\n{[fieldname][component][!conversionflag][:formatspec]}\nText outside a {} substitution target is taken literally and may use doubled {{\nand }} to escape braces (each is replaced with a single brace). Within a {}\nsubstitution target:\nfieldname\nIs an optional number or keyword identifying an argument, which may be\nomitted to reference arguments by relative position\ncomponent\nIs a string of zero or more .name or [index] (brackets required!) references,",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": 7,
      "content": "which are used to fetch attributes and indexed values of an argument, and\nmay be omitted to use the whole argument value\nconversionflag\nStarts with a ! if present, which is followed by s, r, or a to call str, repr, or\nascii built-in functions on the value, respectively (this may bypass the\nvalue’s normal formatting)\nformatspec\nStarts with a : if present, followed by text that specifies how the value\nshould be presented, including details such as field width, alignment,\npadding, decimal precision, and so on, and ends with an optional type code\nThe formatspec component after the colon character has a rich format all its\nown and is formally described as follows. As you’ll see later, this part is reused\nby f-strings (again, […] in this denotes an optional component whose square\nbrackets are not coded literally, and spaces aren’t allowed between parts but may\nappear within some):\n[[fill]align][sign][z][#][0][width][grouping][.precision][typecode]\nWithin this formatspec part of the {} substitution target, the salient parts are\nthese:\nfill\nCan be any fill character other than { or }\nalign\nMay be <, >, =, or ^, for left alignment, right alignment, padding after a sign\ncharacter, or centered alignment\nsign",
      "content_length": 1216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": 7,
      "content": "Can be + (to sign all numbers), − (to sign only negatives), or a space (to use a\nspace for positives)\ngrouping\nMay be , or _ to request a comma or underscore separator, added for\nthousands in decimal number type codes, and four-digit groups in\nnondecimal number type codes (which support only _)\nwidth and precision\nSimilar to those in the % expression of the preceding section, as shown by\nexamples ahead\ntypecode\nSimilar to those in the % expression, with the exceptions described below this\nlist\nOthers\nFor numbers, a 0 before width enables sign-aware zero-padding\n(redundantly with some fill usage), and # invokes an alternate form (e.g.,\nadding 0b and 0X prefixes for binary and hex type codes b and X).\nThe formatspec may also contain nested {} substitution targets for any of its\nparts, to use argument-list values dynamically (much like the * in formatting\nexpressions). These nested {} use fieldname to identify arguments from which\nvalues are pulled, and their formatted results are used in place of the nested {}.\nNesting may be only one level deep, and f-strings (ahead) use an arbitrary\nexpression instead of fieldname in a nested {}.\nThe method’s typecode options largely overlap with those used in % expressions\nand listed earlier in Table 7-4, but the formatting method adds a b to display\nintegers in binary format (much like using the bin built-in), adds a % to display",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": 7,
      "content": "percentages, uses only d for base-10 integers (i and u are unused), uses !\nconversion flags for some cases, and requires a string object for s (to flexibly\nallow any type like the expression’s %s, either omit the type code or formatspec\nin full, or use a !s conversion flag as described earlier).\nSee Python’s library manual for more on substitution syntax that we’ll omit here.\nIn addition to the string’s format method, a single object may also be formatted\nwith the format(object, formatspec) built-in function (which the method\nuses internally), and may be customized in user-defined classes with the\n__format__ operator-overloading method (see Part VI). Different objects may\nuse different format specifiers, but most follow norms.\nAdvanced formatting method examples\nAs you can tell, the syntax in formatting methods can be complex. Because your\nbest ally in such cases is often the interactive prompt, let’s turn to some\nexamples. In the following, {0:10} means the first positional argument in a field\n10 characters wide; {1:<10} means the second positional argument left-justified\nin a 10-character-wide field; ^10 center aligns in 10; and {0.platform:>10}\nmeans the platform attribute of the first argument, right-justified in a 10-\ncharacter-wide field (notice again the use of dict() to make a dictionary from\nkeyword arguments):\n>>> '{0:10} = {1:10}'.format('text', 123.4567)\n'text       =   123.4567'\n>>> '{0:>10} = {1:<10}'.format('text', 123.4567)\n'      text = 123.4567  '\n>>> '{1[kind]:^10} = {0.platform:^10}'.format(sys, dict(kind='laptop'))\n'  laptop   =   darwin  '\nAs demoed earlier, you can omit the argument number if you’re selecting them\nfrom left to right—though this may make your code less explicit, thereby\nnegating one of the purported pluses of format versus %. Code readers must\ncount to match {}s to arguments off to the right (something the f-string’s inline\nexpressions wholly avoid):",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": 7,
      "content": ">>> '{:10} = {:10}'.format('text', 123.4567)\n'text       =   123.4567'\nFloating-point numbers support the same type codes and formatting specificity in\nformatting method calls as in % expressions. For instance, in the following {2:g}\nmeans the third argument formatted by default according to the “g” floating-\npoint representation, {:.2f} designates the “f” floating-point format with just\ntwo decimal digits (and rounding), and {:06.2f} denotes a field with a width of\nsix characters and zero padding on the left:\n>>> '{0:e}, {1:.3e}, {2:g}'.format(3.14159, 3.14159, 3.14159)\n'3.141590e+00, 3.142e+00, 3.14159'\n>>> '{:f}, {:.2f}, {:06.2f}'.format(3.14159, 3.14159, 3.14159)\n'3.141590, 3.14, 003.14'\nHex, octal, and binary formats are supported by the formatting method as well\n(% has all these except binary). In fact, string formatting is an alternative to some\nof the built-in functions that format integers to a given base:\n>>> '{:X}, {:o}, {:b}'.format(255, 255, 255)         # Hex, octal, binary\n'FF, 377, 11111111'\n>>> hex(255), int('FF', 16), 0xFF                    # Other to/from hex\n('0xff', 255, 255)\n>>> oct(255), int('377', 8), 0o377                   # Other to/from octal\n('0o377', 255, 255)\n>>> bin(255), int('11111111', 2), 0b11111111         # Other to/from binary\n('0b11111111', 255, 255)\nThe formatting method also supports separator insertions (but % currently does\nnot): you can add commas and underscores between thousands groups in\nnumbers, and underscores between four-digit groups in hex, octal, and binary\nformats, and absolute argument numbers let you reuse values passed in (% uses\ndictionaries to do the same):\n>>> '{:,.2f}'.format(12345.678)\n'12,345.68'",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": 7,
      "content": ">>> '{0:,}  {0:_}  {1:_x}  {1:_b}'.format(2 ** 32, 0x1FFFF)\n'4,294,967,296  4_294_967_296  1_ffff  1_1111_1111_1111_1111'\nFormatting parameters can either be hardcoded in format strings or taken from\nthe arguments list dynamically by nested format syntax—much like the * syntax\nin formatting expressions’ width and precision:\n>>> '{:.4f}'.format(1 / 3.0)                         # Parameters hardcoded\n'0.3333'\n>>> '%.4f' % (1 / 3.0)                               # Ditto for expression\n'0.3333'\n>>> '{0:.{1}f}'.format(1 / 3.0, 4)                   # Take value from arguments\n'0.3333'\n>>> '%.*f' % (4, 1 / 3.0)                            # Ditto for expression\n'0.3333'\nIn fact, format allows any component of a formatspec string to be taken from\narguments at runtime, rather than hardcoded at programming time (this is more\ngeneral than %). In the following, a number is zero filled, left-justified, signed,\ntwelve wide, comma separated, with two decimal digits—both statically and\ndynamically (and with and without argument numbering that nearly breaches\nthis page’s width limits!):\n>>> '{0:0<+12,.2f}!'.format(1234.564)\n'+1,234.56000!'\n>>> '{0:{1}{2}{3}{4}{5}.{6}{7}}!'.format(1234.564, 0, '<', '+', 12, ',', 2, 'f')\n'+1,234.56000!'\n>>> '{:{}{}{}{}{}{}{}{}}!'.format(1234.564, 0, '<', '+', 12, ',', '.', 2, 'f')\n'+1,234.56000!'\nFinally, Python’s built-in format function noted earlier can also be used to\nformat a single item. It may be simpler than using the format method in this\ncase, and is roughly similar to formatting one item with the % expression:\n>>> '{:.2f}'.format(1.2345)                          # String method\n'1.23'\n>>> format(1.2345, '.2f')                            # Built-in function\n'1.23'\n>>> '%.2f' % 1.2345                                  # Expression",
      "content_length": 1782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": 7,
      "content": "'1.23'\n>>> f'{1.2345:.2f}'                                  # Preview: f-string\n'1.23'\nTechnically, the format built-in runs the subject object’s __format__ method,\nwhich the str.format method does internally for each formatted item. It’s still\nmore verbose than the original % expression’s equivalent here, though, and the f-\nstring alternative may best both when the value is a variable’s name—which\nleads us to the next section.\nThe F-String Formatting Literal\nIf you’ve survived the formatting story this far, there’s some good news: the\nthird and last variant is mostly just a takeoff on the second. The f-string is a text-\nstring literal that embeds substitution values in the format string itself, rather\nthan listing them separately. The code it uses to specify custom formatting,\nthough, is the same as that used for the formatting method. Hence, much of what\nyou just learned for the method applies here in full.\nF-strings, added in Python 3.6, perform what’s generally called string\ninterpolation—replacing the text of an expression with the result of running it\nlive, when the f-string itself is run. These expressions are coded inside the f-\nstring and can be arbitrary Python expression code. The effect isn’t functionally\ndifferent from listing replacement values after a % in the expression, or as\narguments in the format method. Because they embed values where they are to\nbe substituted, though, f-strings are often shorter and may seem easier to read to\nsome observers.\nSyntactically, f-strings begin with the letter f (uppercase or lowercase, but\nusually the latter) before any string-literal form (single, double, or triple quotes).\nThe f prefix may be combined with r in any order to code formatted raw strings\nthat ignore backslashes (e.g., rf), and f-strings concatenate implicitly with an\nadjacent text-string literal of any kind (but use the f prefix on other concatenated\nliterals if you want them to be f-strings too—even on continuation lines).\nAs a preview of Chapter 37, the f cannot be combined with byte-string prefix b\n(which means that the f-string, like the format method but unlike the %\nexpression, works only for text, not bytes); and cannot be mixed with the",
      "content_length": 2198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": 7,
      "content": "backward-compatible and seldom-used u (which would yield prefixes\ninappropriate for this family-oriented text).\nF-string formatting basics\nWithin the f-string literal, curly braces are used to denote substitutions just like\nthe formatting method. Unlike the method, though, {}s contain inline Python\nexpressions whose formatted runtime results replace the bracketed parts:\n>>> what = 'coding'\n>>> tool = 'Python'\n>>> f'Learning {what} in {tool}'\n'Learning coding in Python'\nAs usual, the f-string result is a new string that we’re letting the REPL display,\nbut it can also be assigned to a name to be used elsewhere in our code. F-strings\nalso frequently appear in print calls to display formatted text, though\nsometimes more often than they should: there’s no reason to use an f-string for\nsimple space-separated prints.\nIn its simplest form like this, an f-string’s expressions enclosed in {} are\nevaluated and then formatted per their print-string defaults. This isn’t much\ndifferent from the equivalent expression or method, but is slightly easier on your\nkeyboard and may be slightly easier on your eyes:\n>>> 'Learning %s in %s' % (what, tool)           # Expression equivalent\n'Learning coding in Python'\n>>> 'Learning {} in {}'.format(what, tool)       # Method equivalent\n'Learning coding in Python'\nImportantly, any expression can be used in the curly braces and works as it\nwould outside the f-string:\n>>> task = f'Learning {what.upper() + '!'} in {tool + str(3.12)}'\n>>> task\n'Learning CODING! in Python3.12'\nMoreover, {} expressions are evaluated both where they appear (subject to the",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": 7,
      "content": "name-scoping details you’ll meet later in this book) and when the f-string is run\nto make a string (not when your code is first read by Python). Like the\nformatting expression and method, it’s a runtime operation that uses the current\nvalues of any variables it names:\n>>> what = 'f-strings'\n>>> task                                 # F-strings built when run (only)\n'Learning CODING! in Python3.12'\n>>> task = f'Learning {what.upper() + '!'} in {tool + str(3.12)}'\n>>> task\n'Learning F-STRINGS! in Python3.12'\nAs a preview, this runtime nature also means that f-strings, unlike all other text-\nstring literals, don’t work as Chapter 15’s docstrings. This makes sense if you\nkeep in mind that f-strings are runtime code, more like the % expression and\nformat method calls. When coded in a function, for example, an f-string won’t\nbe run until that function is called.\nOne syntax quirk here: as this example demos, you can embed quotes within an\nf-string’s {} even if they are the same as the quotes used for the f-string at large\n—but this is new as of Python 3.12. In earlier Pythons, backslash escapes didn’t\nhelp for nested quotes, though other enclosing-quote tricks did, and none of this\napplies outside a {}:\nf'Learning {what + '!'}'         # OK as of Python 3.12\nf'Learning {what + '!'}'         # An error before Python 3.12\nf'Learning {what + \\'!\\'}'       # And this doesn't make it work\nf\"Learning {what + '!'}\"         # OK before (and after) Python 3.12\nf'''Learning {what + '!'}'''     # Ditto\nf'Learning '{what + '!'}''       # An error in 3.12+\nf'Learning \\'{what + '!'}\\''     # OK if escape quotes outside {}\nAlso new as of Python 3.12, a backslash can be used in an f-string’s {} part and\nworks just like it does outside the {}, as do both comments and newlines (the\nfollowing may stretch the limits of f-strings, but prove these points):\n>>> f'{'\\n'.join([what] * 3) + '\\x21'}'",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": 7,
      "content": "'f-strings\\nf-strings\\nf-strings!'\n>>> f'Learning {                 # Your comment here\n...      what.upper() + '!'\n...      } in {tool + str(3.12)}'\n'Learning F-STRINGS! in Python3.12'\nIn other words, if you like f-strings, you’ll like them best in Python 3.12+. As\nthis book is based on 3.12 (and 3.13 is right around the corner), it will generally\nuse 3.12’s f-string rules; mod examples’ quotes for older Pythons if needed.\nF-string custom formats\nAs noted, f-strings use the same custom-format syntax as string methods, so\nthere’s not much new to learn here. In the abstract, f-strings are coded with a\nformat like this (as usual, […] means an optional part here and its square\nbrackets are not part of the f-string’s code, but spaces are generally allowed\nbetween the parts here):\nf'…literaltext… {expression [=] [!s, !r, or !a] [:formatspec]} …literaltext…'\nAs in the method, text outside a {} is taken literally and uses {{ and }} to escape\nbraces, and the {} part can be repeated to embed multiple values. In each, the\nexpression part is any Python expression code (including nested f-strings) and\nis run to produce the substitution value before formatting it. As in the formatting\nmethod, the optional !s, !r, or !a render the expression in user-friendly, as-\ncode, or ASCII-with-escapes form—which is the same as calling str, repr, or\nascii for the entire expression enclosed by {}.\nWithin a {}, the formatspec that is coded after a : is (nearly) identical to that\nused in the string method and presented earlier, so we won’t repeat its syntax\nhere; see “Formatting method custom formats” for the options that f-strings\nshare with the method. As a minor convenience for developers, f-strings also\nallow an = character after expression to add the expression’s text and an “=” as\na label before its value formatted with a repr default.\nAdvanced f-string examples\nAll of which is easier to explain by example than narrative, so let’s get back to",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": 7,
      "content": "running code. Numbers can be formatted in a variety of ways spelled out for the\nformatting method earlier—including defaults, fixed decimal digits, comma and\nunderscore separators, exponents, signs, and leading zeroes:\n>>> a = 3.14156\n>>> b = 1_234_567\n>>> f'{a} and {b}'                           # Defaults\n'3.14156 and 1234567'\n>>> f'{a:.2f} and {b:09}'                    # Decimals, padding        \n'3.14 and 001234567'\n \n>>> f'{a * 1000:,.2f} and {b:,} and {b:_}'   # Comma and underscore separators\n'3,141.56 and 1,234,567 and 1_234_567'\n>>> f'{a * 1000:e} and {b:+012,}'            # Exponents, signs, and padding\n'3.141560e+03 and +001,234,567'\n>>> f'{b:_X} and {b:_o} and {b // 64:_b}'    # Hex, octal, binary, underscores\n'12_D687 and 455_3207 and 100_1011_0101_1010'\nAdding an = after the expression may be useful when you’re debugging code, as\nit labels data automatically (though this may be more readable for simple\nvariable names than larger expressions):\n>>> f'{a=:e} and {b=:+012,}'                 # Labeled\n'a=3.141560e+00 and b=+001,234,567'\n \n>>> f'{a + 1=:e} and {b * 2=:+012,}'\n'a + 1=4.141560e+00 and b * 2=+002,469,134'\nString formats can vary according to the s/r/a flag also coded before the format\nspecifier (but after an =). To demo, the following uses a non-ASCII character (an\n“A” with either an umlaut mark or diaeresis):\n>>> c = 'h\\xc4ck'                            # \\xc4 (a.k.a \\u00c4) is non-ASCII Ä\n>>> f'{c} and {c} and {c}'                   # Defaults\n'hÄck and hÄck and hÄck'\n \n>>> f'{c!s} and {c!r} and {c!a}'             # Display mode, two ways\n\"hÄck and 'hÄck' and 'h\\\\xc4ck'\"",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": 7,
      "content": ">>> f'{str(c)} and {repr(c)} and {ascii(c)}'\n\"hÄck and 'hÄck' and 'h\\\\xc4ck'\"\n \n>>> f'{c=!s} and {c=!r} and {c=!a}'          # Labeled\n\"c=hÄck and c='hÄck' and c='h\\\\xc4ck'\"\n \n>>> f'{c=!s:8} and {repr(c)} and {c:0>8}'    # Width, fill, alignment\n\"c=hÄck     and 'hÄck' and 0000hÄck\"\nAdvanced tip: as in the formatting method, parts of the format specifier can be\nfetched dynamically at runtime instead of being hardcoded, by using nested {}\nexpressions. The nested expression’s formatted result is used where it appears:\n>>> width = 8\n>>> f'{a:.8f} and {c:0>8}'                   # Hardcoded parameters\n'3.14156000 and 0000hÄck'\n>>> f'{a:.{width}f} and {c:0>{width}}'       # Dynamic parameters\n'3.14156000 and 0000hÄck'\n>>> f'{a=:.{width}f} and {c * 2:0>{width * 3}}'\n'a=3.14156000 and 0000000000000000hÄckhÄck'\nLike the format method, any part of a formatspec in a {} can be a nested {}—\nthough they contain expressions in the f-string, not argument identifiers. This is\nwhy f-string formats are just “(nearly)” identical. Other parts of f-strings don’t\nallow {}s (forward reference: for economy, the following assigns many names\npositionally with sequence-assignment syntax covered formally in Chapter 11—\nit’s like a=0, b='<', and so on):\n>>> what = 1_234.564\n>>> a, b, c, d, e, f, g, h = 0, '<', '+', 12, ',', '.', 2, 'f'\n>>> f'{what:0<+12,.2f}!'\n'+1,234.56000!'\n>>> f'{what:{a}{b}{c}{d}{e}{f}{g}{h}}!'      # But don't try this at home?\n'+1,234.56000!'\nUsage tip: because f-strings run expressions that reference variables, they may\nnot be easy to use as templates when substitution values are collected in a",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": 7,
      "content": "container at runtime—as they often would be. In the following, a ** converts\ndictionary keys to keyword arguments, as you’ll learn later in this book:\n>>> values = dict(tool='Python', role='scripting')     # Collected values\n>>> 'Use %(tool)s for %(role)s.' % values              # Expression: keys\n'Use Python for scripting.'\n>>> 'Use {tool} for {role}.'.format(**values)          # Method: keywords\n'Use Python for scripting.'\n>>> 'Use {0[tool]} for {0[role]}.'.format(values)      # Method: reused-arg keys\n'Use Python for scripting.'\n>>> f'Use {values['tool']} for {values['role']}.'      # F-string: expressions\n'Use Python for scripting.'\nFor more impressive f-string results, assign substitution values to same-scope\nvariables when possible:\n>>> tool = 'Python'\n>>> role = 'scripting'\n>>> f'Use {tool} for {role}.'\n'Use Python for scripting.'\nBut also bear in mind that f-strings may not be easy to use when templates are\nloaded from external files at runtime—as they often would be. While the format\nexpression and method can treat such templates as simple text data, f-strings are\nPython program code, and hence may have to be run post load with the eval\nfunction of Chapter 5—and trusted to not contain code that will do damage (e.g.,\nerasing files is fairly easy in a Python expression):\n>>> fs = \"\"\"f'Use {tool} for {role}.'\"\"\"               # As if loaded from a file\n>>> eval(fs)                                           # Run as code – and trust!\n'Use Python for scripting.'\nF-strings are a powerful tool, but their nested expressions make them geared\nmore toward in-program formatting than data-based roles. For more details on\nthe f-string, see its full disclosure in Python’s language reference manual.\nAnd the Winner Is…",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": 7,
      "content": "The previous edition of this book went to considerable lengths (about seven\npages) to show how the formatting method, newest at the time, was functionally\nredundant with the expression, in order to underscore the downsides of feature\nbloat in programming languages. Given that the number of primary formatting\ntools in Python has climbed from two to three since then, that message may not\nhave entirely hit its mark. Consequently, this edition has dropped most of the\nrhetoric, and opted to leave this race’s call up to you.\nBut if you’re looking for a guideline, there is no killer argument for dismissing\nany formatting option out of hand for all use cases. As stated at the start of this\nsection, f-strings may be best in most new code, on logistical grounds alone:\nnewer is less likely to be culled by Python sooner. In fact, we’ll be using them\nregularly in the rest of this book where warranted, so expect more examples\nahead. Even so, you will also see the expression and method often in existing\ncode and may have to use them in roles that f-strings don’t address (e.g., for\nsubstitutions in text loaded from files).\nMore fundamentally, this book is not in the business of telling you what to do,\nand you are welcome to use any formatting option you prefer. Imposing new\ntools on programmers is exclusive, divisive, and probably rude. Despite norms in\nthe software field today, the choice of development options should be yours—\nand yours alone—to make.\nAs for the bloat: change is not always bad, but it can be when it creates\nredundancy or incompatibility. In fact, you’ve just had a front-row seat to one of\nits worst consequences: N functionally equivalent options can multiply\nnewcomers’ learning requirements by N. We’ll return to this and other perils of\nego-fueled churn and convolution in software development at the end of this\nbook. For now, we’ll close by simply noting that this stuff still matters. With any\nluck, a future edition won’t have yet another formatting tool to doc, and future\nlearners won’t have yet another one to grok.\nPLUS ONE MORE: STRING.TEMPLATE\nTechnically speaking, there are four (not three) formatting tools built into\nPython today, if we include the obscure string module’s Template tool\nmentioned earlier. Now that you’ve seen the other three, you can tell how it",
      "content_length": 2310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": 7,
      "content": "compares. The expression, method, and f-string can all be used as templating\ntools, referring to substitution values by name using dictionary keys,\nkeyword arguments, or variables (the “;” in the following separates multiple\nstatements needed to give the f-string variables to reference):\n>>> 'The %(num)s %(tool)ss' % dict(num=4, tool='formatter')\n'The 4 formatters'\n>>> 'The {num} {tool}s'.format(num=4, tool='formatter')\n'The 4 formatters'\n>>> num=4; tool='formatter'; f'The {num} {tool}s'\n'The 4 formatters'\nThe module’s templating system allows values to be referenced by name too,\nprefixed by a $, as either dictionary keys or keywords, but does not support\nall the utilities of the other two methods—a limitation that yields simplicity,\nthe prime motivation for this tool:\n>>> import string\n>>> t = string.Template('The $num ${tool}s')\n>>> t.substitute(num=4, tool='formatter')\n'The 4 formatters'\n>>> t.substitute(dict(num=4, tool='formatter'))\n'The 4 formatters'\nSee Python’s manuals for more details. It’s possible that you may see this\nalternative (as well as additional tools in the third-party domain) in Python\ncode too; thankfully this technique is simple and is used rarely enough to\nwarrant its limited coverage here.\nGeneral Type Categories\nNow that we’ve explored the first of Python’s collection objects, the string, let’s\nclose this chapter by defining a few general type concepts that will apply to most\nof the types we’ll look at from here on. With regard to built-in types, it turns out\nthat operations work the same for all the types in the same category, so we’ll\nonly need to define most of these ideas once. We’ve examined only numbers and\nstrings so far, but because they are representative of two of the three major type\ncategories in Python, you already know more about several other types than you",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": 7,
      "content": "might think.\nTypes Share Operation Sets by Categories\nAs you’ve learned, strings are immutable sequences: they cannot be changed in\nplace (the immutable part), and they are positionally ordered collections that are\naccessed by offset (the sequence part). It so happens that all the sequences we’ll\nstudy in this part of the book respond to the same sequence operations shown in\nthis chapter at work on strings—concatenation, indexing, iteration, and so on.\nMore formally, there are three major type (and hence operation) categories in\nPython that have this generic nature:\nNumbers (integer, floating-point, decimal, fraction, others)\nSupport addition, multiplication, etc.\nSequences (strings, lists, tuples)\nSupport indexing, slicing, concatenation, etc.\nMappings (dictionaries)\nSupport indexing by key, etc.\nPython’s byte strings mentioned at the start of this chapter fall under the general\n“strings” label here; sets are something of a category unto themselves (they don’t\nmap keys to values and are not positionally ordered sequences); and we haven’t\nyet explored mappings on our in-depth tour (we will in the next chapter).\nHowever, many of the other types we will encounter will be similar to numbers\nand strings. For example, for any sequence objects X and Y:\nX + Y makes a new sequence object with the contents of both operands\njoined.\nX * N makes a new sequence object with N copies of the sequence\noperand X.\nIn other words, these operations work the same way on any kind of sequence,\nincluding strings, lists, tuples, and some user-defined object types. The only",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": 7,
      "content": "difference is that the new result object you get back is of the same type as the\noperands X and Y—if you concatenate lists, you get back a new list, not a string.\nIndexing, slicing, and other sequence operations work the same on all sequences,\ntoo; the type of the objects being processed tells Python which flavor of the task\nto perform (and if that sounds like polymorphism again, it should).\nMutable Types Can Be Changed in Place\nThe string’s immutable classification is an important constraint to be aware of,\nyet it tends to trip up new users. If an object type is immutable, you cannot\nchange its value in place; Python raises an error if you try. Instead, you must run\ncode to make a new object containing the new value. The major core types in\nPython break down as follows:\nImmutables (numbers, strings, tuples, frozensets)\nNone of the object types in the immutable category support in-place changes,\nthough we can always run expressions to make new objects and assign their\nresults to variables as needed.\nMutables (lists, dictionaries, sets, bytearray)\nConversely, the mutable object types can always be changed in place with\noperations that do not create new objects. Although such objects can be\ncopied manually, in-place changes support direct modification.\nGenerally, immutable types give some degree of integrity by guaranteeing that\nan object won’t be changed by another part of a program. For a refresher on why\nthis matters, see the discussion of shared object references in Chapter 6. To see\nhow lists, dictionaries, and tuples participate in type categories, we need to move\nahead to the next chapter.",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": 7,
      "content": "Chapter Summary\nIn this chapter, we took an in-depth, second-pass tour of the string object type.\nWe learned about coding string literals, and we explored string operations,\nincluding sequence expressions, string method calls, and string formatting in its\nexpression, method, and literal flavors. Along the way, we studied a variety of\nconcepts in depth, such as slicing, method call syntax, and triple-quoted block\nstrings. We also defined some core ideas common to a variety of types:\nsequences, for example, share an entire set of operations demoed here for\nstrings.\nIn the next chapter, we’ll continue our types tour with a look at the most general\nobject collections in Python—lists and dictionaries. As you’ll find, much of what\nyou’ve learned here will apply to those types as well. And as mentioned earlier,\nin the final part of this book we’ll return to Python’s string model to flesh out the\ndetails of Unicode text and binary data, which are of interest to some, but not all,\nPython programmers, and depend on tools we haven’t yet studied in full. Before\nmoving on, though, here’s another chapter quiz to review the material covered\nhere.\nTest Your Knowledge: Quiz\n1. Can the string find method be used to search a list?\n2. Can a string slice expression be used on a list?\n3. How would you convert a character to its ASCII integer code? How\nwould you convert the other way, from an integer code to a character?\n4. How might you go about changing a string in Python?\n5. Given a string S with the value 'c,od,e', name two ways to extract the\ntwo characters in the middle.\n6. How many characters are there in the string \"a\\nb\\x1f\\000d\"?\n7. Write an expression, method call, and f-string to format 'Python' and",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": 7,
      "content": "3.12 at a string’s beginning and end.\nTest Your Knowledge: Answers\n1. No, because methods are always type specific; that is, they only work\non a single object type. Expressions like X+Y and built-in functions like\nlen(X) are generic, though, and may work on a variety of types. In this\ncase, for instance, the in membership expression has a similar effect as\nthe string find, but it can be used to search both strings and lists.\nPython makes some attempt to name similar methods consistently\n(many objects have a copy method, for example, and mutable objects\nmay share method names like pop), but methods are still more type\nspecific than other operation sets.\n2. Yes. Unlike methods, expressions are generic and apply to many types.\nIn this case, the slice expression is really a sequence operation—it\nworks on any type of sequence object, including strings, lists, and\ntuples. The only difference is that when you slice a list, you get back a\nnew list.\n3. The built-in ord(S) function converts from a one-character string to an\ninteger character code; chr(I) converts from the integer code back to a\nstring. Keep in mind, though, that these integers are only ASCII codes\nfor text whose characters are drawn only from the ASCII character set.\nIn the Unicode model, text strings are really sequences of Unicode code\npoint identifying integers, which may fall outside the 7-bit range of\nnumbers reserved by ASCII (we previewed Unicode in Chapter 4 and\nwill revisit it in Chapter 37).\n4. Strings cannot be changed; they are immutable. However, you can\nachieve a similar effect by creating a new string—by concatenating,\nslicing, using a method call like replace, or running formatting\noperations—and then assigning the result back to the original variable\nname.\n5. You can slice the string using S[2:4] or split on the comma and index",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": 7,
      "content": "the string using S.split(',')[1]. Try these interactively to see for\nyourself.\n6. Six. The string \"a\\nb\\x1f\\000d\" contains the characters a, newline\n(\\n), b, literal value 31 (as hex escape \\x1f, which is a code point that\nstands for the nonprintable control character US), literal value 0 (an\noctal escape \\000), and d. Pass the string to the built-in len function to\nverify this and print each of its characters’ ord results to see the actual\ncode point (identifying number) values. See Table 7-2 for more details\non escapes.\n7. There’s no right answer for what goes in the middle of the result string,\nbut as examples: '%s is %s' % ('Python', 3.12) works for the\nexpression, and '{} is {}'.format('Python', 3.12) suffices for\nthe method call. There’s almost no reason to use an f-string if all parts\nare known and formatted per defaults (f'{'Python'} is {3.12}'\nseems silly), but f-strings are more useful if values are first assigned to\nvariables: x='Python'; y=3.12; f'{x} is {y}' or similar garners\nfull points.",
      "content_length": 1017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": 7,
      "content": "Chapter 8. Lists and Dictionaries\nNow that we’ve explored numbers and strings, this chapter moves on to give the\nfull story on Python’s list and dictionary objects—collections of other objects,\nand the main workhorses in almost all Python scripts. As you’ll find, both are\nremarkably flexible: they can be changed in place, can grow and shrink on\ndemand, and may contain and be nested in any other kind of object. By\nleveraging these built-in object types, you can create and process rich\ninformation structures in your scripts without having to define new object types\nof your own.\nLists\nThe first stop on this chapter’s tour is the Python list. Lists are Python’s most\nflexible ordered collection object type. Unlike strings, lists can contain any sort\nof object: numbers, strings, and even other lists. Also, unlike strings, lists may be\nchanged in place by assignment to offsets and slices, list method calls, deletion\nstatements, and more—they are mutable objects.\nPython lists do the work of many of the collection data structures you might\nhave to implement manually in lower-level languages such as C. Here is a quick\nlook at their main properties. Python lists are:\nOrdered collections of arbitrary objects\nFrom a functional view, lists are just places to collect other objects so you\ncan treat them as groups. Lists also maintain a left-to-right positional\nordering among the items they contain.\nAccessed by offset\nJust as with strings, you can fetch a component object from a list by indexing\nthe list on the object’s offset. Because items in lists are ordered by their\npositions, you can also do tasks such as slicing and concatenation.",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": 7,
      "content": "Variable-length, heterogeneous, and arbitrarily nestable\nUnlike strings, lists can grow and shrink in place (their lengths can vary),\nand they can contain any sort of object, not just one-character strings (they’re\nheterogeneous). Because lists can both contain and be contained by other\ncollection objects, they also support arbitrary nesting; you can create lists of\nlists of lists, and so on.\nOf the category “mutable sequence”\nIn terms of our type category qualifiers, lists are mutable (i.e., can be\nchanged in place) and can respond to all the sequence operations used with\nstrings, such as indexing, slicing, and concatenation. In fact, sequence\noperations work the same on lists as they do on strings; the only difference is\nthat sequence operations such as concatenation and slicing return new lists\ninstead of new strings when applied to lists. Because lists are mutable,\nhowever, they also support other operations that strings don’t, such as\ndeletion, expansion, and index assignment operations, which change the lists\nin place.\nArrays of object references\nTechnically, Python lists contain zero or more references to other objects.\nLists might remind you of arrays of pointers (addresses) if you have a\nbackground in some other languages, and fetching an item from a Python list\nis about as fast as indexing a C array. In fact, lists really are arrays inside the\nstandard CPython interpreter, not linked structures. As we learned in\nChapter 6, though, Python always follows a reference to an object whenever\nthe reference is used, so your program deals only with objects. Whenever\nyou assign an object to a data structure component or variable name, Python",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": 7,
      "content": "always stores a reference to that same object, not a copy of it (though the\nobject stored may be a copy of another, if you requested one before the\nstore).\nAs a preview and reference, Table 8-1 summarizes common and representative\nlist object operations. It is fairly complete, but for the full story, consult the\nPython standard-library manual, or run a help(list) or dir(list) call\ninteractively for a complete list of list methods—you can pass in a real list, or\nthe word list, which is the name of the list data type. The set of methods here is\nespecially prone to change, so be sure to cross-check in the future.\nTable 8-1. Common list literals and operations\nOperation\nInterpretation\nL = []\nAn empty list\nL = [123, 'abc', 1.23, {}]\nFour items: indexes 0..3\nL = ['Pat', 40.0, ['dev', 'mgr\n']]\nNested sublists\nL = list('code')\nL = list(range(-4, 4))\nList of an iterable’s items, list of successive\nintegers\nL[i]\nL[i][j]\nL[i:j]\nlen(L)\nIndex, index of index, slice, length\nL1 + L2\nL * 3\nConcatenate, repeat\nL1 > L1, L1 == L2\nComparisons: magnitude, equality\n3 in L\nfor x in L: print(x)\nMembership, iteration\nL.append(4)\nMethods: growing",
      "content_length": 1138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": 7,
      "content": "L.extend([5, 6, 7])\nL.insert(i, X)\nL.index(X)\nL.count(X)\nMethods: searching\nL.sort()\nL.reverse()\nL.copy()\nL.clear()\nMethods: sorting, reversing, copying, clearing\nL.pop(i)\nL.remove(X)\ndel L[i]\ndel L[i:j]\nL[i:j] = []\nMethods, statements: shrinking\nL[i] = 3\nL[i:j] = [4, 5, 6]\nIndex assignment, slice assignment\nL = [*x, 0, *y, *z]\nIterable unpacking\nL = [x**2 for x in range(5)]\nlist(map(ord, 'python'))\nList comprehensions and maps\nWhen written down as a literal expression, a list is coded as a series of objects\n(really, expressions that return objects) in square brackets, separated by commas.\nFor instance, the second row in Table 8-1 assigns the variable L to a four-item\nlist. A nested list is coded as a nested square-bracketed series (row 3), and the\nempty list is just a square-bracket pair with nothing inside (row 1).1\nMany of the operations in Table 8-1 should look familiar, as they are the same\nsequence operations we put to work on strings earlier—indexing, concatenation,\niteration, and so on. Lists also respond to list-specific method calls (which\nprovide utilities such as sorting, reversing, adding items to the end, etc.), as well\nas in-place change operations (deleting items, assignment to indexes and slices,\nand so forth). Again, lists have these tools for change operations because they\nare a mutable object type.\nLists in Action",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": 7,
      "content": "Probably the best way to understand lists is to see them at work. Let’s once again\nturn to some simple interpreter interactions to illustrate the operations in Table 8-\n1.\nBasic List Operations\nBecause they are sequences, lists support many of the same operations as strings,\nwhich means we don’t have to repeat all the operation details again here. In\nshort, though, lists respond to the + and * operators much like strings—they\nmean concatenation and repetition here too, except that the result is a new list,\nnot a string:\n$ python3                                    # Launch a REPL\n>>> len([1, 2, 3])                           # Length\n3\n>>> [1, 2, 3] + [4, 5, 6]                    # Concatenation\n[1, 2, 3, 4, 5, 6]\n>>> ['Py!'] * 4                              # Repetition\n['Py!', 'Py!', 'Py!', 'Py!']\nAlthough the + operator works the same for lists and strings, it’s important to\nknow that it expects the same sort of sequence on both sides—otherwise, you get\na type error when the code runs. For instance, you cannot concatenate a list and a\nstring unless you first convert the list to a string (using tools such as str or\nformatting) or convert the string to a list (the list built-in function does the\ntrick):\n>>> str([1, 2]) + '34'                       # Same as '[1, 2]' + '34'\n'[1, 2]34'\n>>> [1, 2] + list('34')                      # Same as [1, 2] + ['3', '4']\n[1, 2, '3', '4']\nAs suggested in prior chapters, lists also support comparisons, which\nautomatically compare all parts from left to right until a result is known. In the\nfollowing, for instance, a nested 3 is greater than a nested 2, and the one-item list\n[1] at the end is considered less because it’s shorter (though it broadly prefers\nthe term horizontally challenged):\n>>> L = [1, [2, 3], 4]",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": 7,
      "content": ">>> (L == [1, [2, 3], 4]), (L > [1, [2, 2], 4]), (L > [1])\n(True, True, True)\nIndexing and Slicing\nBecause lists are sequences, indexing and slicing also work the same way for\nlists as they do for strings. For lists, though, the result of indexing is whatever\ntype of object lives at the offset you specify (not a one-character string), and\nslicing a list always returns a new list (not a string):\n>>> L = ['hack', 'Hack', 'HACK!']\n>>> L[2]                              # Offsets start at zero\n'HACK!'\n>>> L[-2]                             # Negative: count from the right\n'Hack'\n>>> L[1:]                             # Slicing fetches sections\n['Hack', 'HACK!']\nNew here: because you can nest lists and other object types within lists, you will\nsometimes need to string together index operations to go deeper into a data\nstructure. For example, one of the simplest ways to represent matrixes\n(multidimensional arrays) in Python is as lists with nested sublists. Here’s a\nbasic 3 × 3 two-dimensional list-based array—a reprise from Chapter 4:\n>>> matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nWith one index, you get an entire row (really, a nested sublist), and with two,\nyou get an item within the row:\n>>> matrix[1]\n[4, 5, 6]\n>>> matrix[1][1]\n5\n>>> matrix[2][0]\n7\nAs demoed earlier, lists can naturally span multiple lines if you want them to\nbecause they are contained by a pair of brackets; watch for more on syntax like\nthis in the next part of the book (and ignore the “…” if absent in your REPL):\n>>> matrix = [[1, 2, 3],",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": 7,
      "content": "...           [4, 5, 6],\n...           [7, 8, 9]]\n>>> matrix[1][1]\n5\nFor more on matrixes, also watch for a dictionary-based alternative later in this\nchapter, which can be more efficient when matrixes are largely empty. We’ll also\ncontinue this thread in Chapter 20 where we’ll write additional matrix code,\nespecially with list comprehensions. And for high-powered numeric work, the\nNumPy extension mentioned in Chapters 1, 4, and 5 provides other ways to\nhandle matrixes.\nChanging Lists in Place\nBecause lists are mutable, they support operations that change a list object in\nplace. That is, list operations in this section and others all modify the list object\ndirectly—overwriting its former value—without requiring that you make a new\ncopy, as you had to for strings. Because Python deals only in object references,\nthis distinction between changing an object in place and creating a new object\nmatters; as discussed in Chapter 6, if you change an object in place, you might\nimpact more than one reference to it at the same time.\nIndex and slice assignments\nFirst up in this category is a twist on the indexing and slicing we’ve already\nexplored. When using a list, you can change its contents by assigning to either a\nparticular item (offset) or an entire section (slice):\n>>> L = ['code', 'Code', 'CODE!']\n>>> L[1] = 'Hack'                     # Index assignment\n>>> L                                 # Replaces item 1\n['code', 'Hack', 'CODE!']\n>>> L[0:2] = ['write', 'Python']      # Slice assignment: \"delete+insert\"\n>>> L                                 # Replaces items 0,1\n['write', 'Python', 'CODE!']\nBoth index and slice assignments are in-place changes—they modify the subject\nlist directly, rather than generating a new list object for the result. Index\nassignment in Python works much as it does in most other languages: Python",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": 7,
      "content": "replaces the single object reference at the designated offset with a new one; the\noffset’s reference is changed.\nSlice assignment, the last operation in the preceding example, replaces an entire\nsection of a list in a single step. Because it can be a bit complex, it is perhaps\nbest thought of as a combination of two steps:\n1. Deletion: The slice you specify to the left of the = is deleted.\n2. Insertion: The new items contained in the iterable object to the right of\nthe = are inserted into the list on the left, at the place where the old slice\nwas deleted.2\nThis isn’t what really happens, but it can help clarify why the number of items\ninserted doesn’t have to match the number of items deleted. For instance, given a\nlist L of two or more items, an assignment L[1:2]=[4,5] replaces one item with\ntwo—it’s as though Python first deletes the one-item slice at [1:2] (from offset\n1, up to but not including offset 2), then inserts both 4 and 5 where the deleted\nslice used to be. The net effect makes the list larger.\nThis also explains why the second slice assignment in the following is really an\ninsert—Python replaces an empty slice at [1:1] with two items; and why the\nthird is really a deletion—Python deletes the slice (the item at offset 1), and then\ninserts nothing:\n>>> L = [1, 2, 3]\n>>> L[1:2] = [4, 5]                   # Replacement/insertion\n>>> L\n[1, 4, 5, 3]\n>>> L[1:1] = [6, 7]                   # Insertion (replace nothing)\n>>> L\n[1, 6, 7, 4, 5, 3]\n>>> L[1:2] = []                       # Deletion (insert nothing)\n>>> L\n[1, 7, 4, 5, 3]\nIn effect, slice assignment replaces an entire section, or “column,” all at once—\neven if the column or its replacement is empty. Because the length of the\nsequence being assigned does not have to match the length of the slice being\nassigned to, slice assignment can be used to replace (by overwriting), expand",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": 7,
      "content": "(by inserting), or shrink (by deleting) the subject list. It’s a powerful operation,\nbut frankly, one that you may not see very often in practice. There are often more\nstraightforward and mnemonic ways to replace, insert, and delete (including\nconcatenation expressions, and the insert, pop, and remove list methods\ncoming up soon), which Python programmers tend to prefer in practice.\nOn the other hand, this operation can be used as a sort of in-place concatenation\nat the front of the list—per the next section’s method coverage, something the\nlist’s extend does more mnemonically but at list end:\n>>> L = [1]\n>>> L[:0] = [2, 3, 4]          # Insert all at :0, an empty slice at front\n>>> L\n[2, 3, 4, 1]\n>>> L[len(L):] = [5, 6, 7]     # Insert all at len(L):, an empty slice at end\n>>> L\n[2, 3, 4, 1, 5, 6, 7]\n>>> L.extend([8, 9, 10])       # Insert all at end, named method (see also +=)\n>>> L\n[2, 3, 4, 1, 5, 6, 7, 8, 9, 10]\nList method calls\nLike strings, Python list objects also support type-specific method calls, most of\nwhich change the subject list itself:\n>>> L = ['write']\n>>> L.append('Python')                     # Append: add one item at the end\n>>> L\n['write', 'Python']\n>>> L.extend(['code', 'goodly'])           # Extend: add many items at the end\n>>> L\n['write', 'Python', 'code', 'goodly']\n>>> L.sort()                               # Sort: order list items ('P' < 'c')\n>>> L\n['Python', 'code', 'goodly', 'write']\nMethods were introduced in Chapter 7. In brief, they are functions (really, object\nattributes that reference functions) that are associated with and act upon\nparticular subject objects (the objects through which they are called). Methods\nprovide type-specific tools; the list methods presented here, for instance, are\ngenerally available only for lists.",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": 7,
      "content": "Demoed in the preceding code, append is perhaps the most commonly used list\nmethod. It simply tacks a single item (really, an object reference) onto the end of\nthe subject list in place, and the list expands to make room for the addition.\nUnlike concatenation, append expects you to pass in one object, not a list, and\nuses it literally. In fact, the effect of L.append(X) is similar to L+[X], but the\nformer changes L in place, while the latter makes a new list.3\nBy contrast, the extend method adds multiple items at the end of the list, again\nin place. Technically, extend always iterates through and adds each item in the\npassed iterable object, whereas append simply adds a single item as is without\niterating—a distinction that will be more meaningful in Chapter 14. For now, it’s\nenough to know that extend adds many items, and append adds one. Also\nahead: Chapter 11 covers the += statement, which does in-place assignment for\nlists too, and is yet another way to add many items that redundantly mirrors\nextend.\nThe sort method orders the list’s items but merits a section of its own.\nSorting lists\nAs we’ve just witnessed, sort orders a list in place. It’s a common tool that uses\nPython standard comparison tests (string comparisons in most examples here,\nbut other objects work in sorts too), and by default sorts in ascending order. You\ncan modify sort behavior by passing in keyword arguments—a special\nname=value syntax in function calls that we’ve used in earlier chapters, gives\nvalues by name, and is often used for configuration options.\nIn sorts, the reverse argument allows sorts to be made in descending instead of\nascending order, and the key argument gives a one-argument function that\nreturns the value to be used in sorting—the string object’s standard lower case\nconverter in the following (though its newer casefold may handle some types\nof Unicode text better):\n>>> L = ['abc', 'ABD', 'aBe']\n>>> L.sort()                                # Sort with mixed case\n>>> L\n['ABD', 'aBe', 'abc']",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": 7,
      "content": ">>> L = ['abc', 'ABD', 'aBe']\n>>> L.sort(key=str.lower)                   # Normalize to lowercase\n>>> L\n['abc', 'ABD', 'aBe']\n>>> L = ['abc', 'ABD', 'aBe']\n>>> L.sort(key=str.lower, reverse=True)     # And change sort order\n>>> L\n['aBe', 'ABD', 'abc']\nThe sort key argument can also be useful when sorting lists of dictionaries, to\nselect a sort value by indexing each dictionary on a field along the way. We’ll\nstudy dictionaries later in this chapter, and you’ll learn more about keyword\nfunction arguments in Part IV.\nTwo notes of caution here: first, sorts won’t work by default with mixed types. In\nPython, magnitude comparison of mixed types is an error, as it’s generally\nambiguous. Because sorting uses these comparisons internally, though, sorting\nmixed-type lists fails by proxy. To work around this limitation, use the key\nargument to code value transformations during the sort. The following simply\nconverts all items to strings with the str built-in, but key can be an arbitrary\nfunction of your making (and is often coded inline with the lambda expression\nyou’ll meet later in this book):\n>>> L = [1, 'hack', 2]            # Mixed-type sorts fail by default\n>>> L.sort()\nTypeError: '<' not supported between instances of 'str' and 'int'\n>>> L.sort(key=str)\n>>> L                             # Enable mixed-type sorts: all str\n[1, 2, 'hack']\nSecond, beware that append, extend, and sort change the associated list object\nin place, but don’t return the modified list as a result (technically, they return the\nNone placeholder object introduced in Chapter 4). If you run L=L.append(X),\nyou won’t get the modified value of L; in fact, you’ll lose the reference to the list\naltogether! When you use methods like these, objects are changed as a side\neffect, so there’s no reason to reassign.\nPartly because of such constraints, sorting is also available in Python as the\nsorted built-in function, which sorts any collection (not just lists) and returns a",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": 7,
      "content": "new list for the result (instead of changing the collection in place):\n>>> L = ['abc', 'ABD', 'aBe']\n>>> sorted(L)                                      # Sorting built-in function\n['ABD', 'aBe', 'abc']\n>>> L                                              # L is not modified\n['abc', 'ABD', 'aBe']\n>>> sorted(L, key=str.lower, reverse=True)         # Same arguments as list.sort\n['aBe', 'ABD', 'abc']\n>>> L = ['abc', 'ABD', 'aBe']\n>>> sorted([x.lower() for x in L], reverse=True)   # Pretransform items: differs!\n['abe', 'abd', 'abc']\nNotice the last example here—we can convert to lowercase prior to the sort with\na list comprehension (introduced in Chapter 4 and expanded shortly), but the\nresult does not contain the original list’s values as it does with the key argument.\nThe latter is applied temporarily during the sort, instead of changing the values\nto be sorted altogether. As we move along, you’ll see roles in which the sorted\nbuilt-in can sometimes be more useful than the sort method.\nMore List Methods\nLike strings, lists have other methods that perform other specialized operations.\nFor instance, reverse reverses the list in place, and pop deletes one item at the\nend (by default). Just like sorting, there is also a reversed built-in function that\ndoes not change the list in place. Confusingly, though, reversed does not return\na new list like sorted; it instead returns an iterable object that produces results\non demand, and must be wrapped in a list call to collect its results if a real list\nis needed (e.g., for indexing, or display at the REPL here):\n>>> L = [1, 2, 3, 4, 5]\n>>> L\n[1, 2, 3, 4, 5]\n>>> L.pop()                          # Delete and return an item: @ -1 by default\n5\n>>> L\n[1, 2, 3, 4]\n>>> L.reverse()                      # In-place reversal\n>>> L",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": 7,
      "content": "[4, 3, 2, 1]\n>>> list(reversed(L))                # Reversal built-in with a result (iterable)\n[1, 2, 3, 4]\n>>> L                                # L was unchanged\n[4, 3, 2, 1]\nIn some types of programs, the list pop method is used in conjunction with\nappend to implement a quick last-in-first-out (LIFO) stack structure. The end of\nthe list serves as the “top” of the stack:\n>>> L = []\n>>> L.append(1)                      # Push onto stack\n>>> L.append(2)\n>>> L\n[1, 2]\n>>> L.pop()                          # Pop off stack\n2\n>>> L\n[1]\nThe pop method also accepts an optional offset of the item to be deleted and\nreturned (the default is the last item at offset −1). Other list methods remove an\nitem by value (remove), insert an item at an offset (insert), count the number of\noccurrences (count), and search for an item’s offset (index—a search for the\nindex of an item, not to be confused with indexing itself, despite the name!):\n>>> L = ['hack', 'Py', 'code']\n>>> L.index('Py')                    # Index _of_ an object (search/find)\n1\n>>> L.insert(1, 'more')              # Insert at offset/position\n>>> L\n['hack', 'more', 'Py', 'code']\n>>> L.remove('code')                 # Delete by value\n>>> L\n['hack', 'more', 'Py'] \n>>> L.pop(1)                         # Delete by offset/position\n'more'\n>>> L\n['hack', 'Py'] \n>>> L.count('Py')                    # Number of occurrences\n1\nNote that unlike other list methods, count and index do not change the list",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": 7,
      "content": "itself, but return information about its content. Run a help(list) or dir(list)\nin your REPL, see Python reference resources, or experiment with these calls\ninteractively on your own to learn more about list methods.\nIteration, Comprehensions, and Unpacking\nLists also respond to other sequence operations we used on strings in the prior\nchapter, including iteration tools, and are regularly used in conjunction with the\nrange built-in previewed in Chapter 4:\n>>> 3 in [1, 2, 3]                           # Membership\nTrue\n>>> for x in [1, 2, 3]:\n...     print(x, end=' ')                    # Iteration\n...\n1 2 3\n>>> list(range(5))                           # Counter generators (0…N-1)\n[0, 1, 2, 3, 4]\nWe will talk more formally about for iteration and the range built-in in\nChapter 13, because they are related to statement syntax. Per Chapter 4\npreviews, though, for loops step through items in any sequence (or other\niterable) from left to right, executing one or more statements for each item; and\nrange makes a series of integers, for use in a variety of roles, including counter\nloops (and coerced by list to surrender its values for display here).\nList comprehensions and maps\nThe last items in Table 8-1, list comprehensions and map calls, were also\nintroduced in Chapter 4 and are covered in full in Chapters 14 and 20. Their\nbasic usage is straightforward, though—list comprehensions are close kin to for\nloops, and build a new list by applying an expression to each item in a sequence\n(really, any iterable):\n>>> res = [x * 4 for x in 'code']            # List comprehension\n>>> res\n['cccc', 'oooo', 'dddd', 'eeee']\nAs a preview, this expression is equivalent to a for loop that builds up a list of",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": 7,
      "content": "results manually with the append we met earlier, but as you’ll learn in later\nchapters, list comprehensions are simpler to code and may run faster:\n>>> res = []                                 # Equivalent loop code\n>>> for x in 'code':                         # Append expression results \n...     res.append(x * 4)                    \n... \n>>> res\n['cccc', 'oooo', 'dddd', 'eeee']\nComprehension also support extensions like if filters and nested for loops, and\nthe map built-in function does similar work, but applies a function instead of an\nexpression to items in a sequence (or other iterable) and collects all the results in\na new list—if we wrap it in list to force it to produce all its results:\n>>> [x * 4 for x in 'program' if x >= 'p']   # Filter items with if clauses\n['pppp', 'rrrr', 'rrrr']\n>>> [x + y for x in 'py' for y in '312']     # Nested loops: see Chapter 14!\n['p3', 'p1', 'p2', 'y3', 'y1', 'y2']\n>>> list(map(abs, [−1, −2, 0, 1, 2]))        # Map a function across a sequence\n[1, 2, 0, 1, 2]\nWhile these are powerful tools, their loop equivalents also make them optional.\nStay tuned for the related dictionary comprehension later in this chapter, and\nmuch more on comprehensions and maps later in this book.\nList-literal unpacking\nAs of Python 3.5, list literals also support a * syntax that unpacks the contents of\nany iterable (including sequences like lists and strings) at the top level of the list\nbeing created. The effect flattens the starred item in the new list:\n>>> L = ['code', 'hack']\n>>> [L, 2, 3, L]\n[['code', 'hack'], 2, 3, ['code', 'hack']]   # Normal assignments nest items\n>>> [*L, 2, 3, *L]                           # Iterable unpacking in list literal\n['code', 'hack', 2, 3, 'code', 'hack']\n>>> S = 'code'                               # Any sequence (or other iterable)",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": 7,
      "content": ">>> [*'Py', *S, *range(3)]\n['P', 'y', 'c', 'o', 'd', 'e', 0, 1, 2]\nImportantly, and as is so often true in Python today, * unpacking is never\nrequired. The following simple concatenation, for example, has the same effect\nas the preceding’s first * unpacking—which is redundant in this case:\n>>> L + [2, 3] + L                           # What * unpacking does, for lists     \n['code', 'hack', 2, 3, 'code', 'hack']\nFor nonlist iterables, unpacking without * requires conversions because +\nexpects lists on both sides, but it’s not much extra work, especially given the\nrarity of such code:\n>>> list('Py') + list(S) + list(range(3))    # What * unpacking does, for nonlists\n['P', 'y', 'c', 'o', 'd', 'e', 0, 1, 2]\nLoops can achieve the same goals as unpacking too; they may be more verbose,\nbut are also more general purpose:\n>>> M = []\n>>> for x in ('Py', S, range(3)): M.extend(x)        # Loops v special-case hacks\n... \n>>> M\n['P', 'y', 'c', 'o', 'd', 'e', 0, 1, 2]\nNotice two things about this example. First, it codes a tuple of three items as the\nsequence that the for loop steps through (the tuple’s outer parentheses are\noptional but explicit). Second, the for loop in this code avoids a line by moving\nits body up to its header, using a syntax rule we’ll define formally in the next\npart of this book (tl;dr: this works only for simple statements, like calls to print\nand extend).\nBecause we’re not quite ready for the full iteration story, we’ll postpone further\ndetails for now, but watch for similar unpacking expressions for dictionaries later\nin this chapter. Here, though, you should already be able to weigh for yourself\nwhether the alternatives to * unpacking were sufficiently subpar to warrant the\nconvolution.",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": 7,
      "content": "Other List Operations\nBecause lists are mutable, you can also use the del statement to delete an item or\nsection in place:\n>>> L = ['hack', 'more', 'Py', 'code'] \n>>> del L[0]                             # Delete one item (in place)\n>>> L\n['more', 'Py', 'code'] \n>>> del L[1:]                            # Delete an entire section\n>>> L                                    # Same as L[1:] = []\n['more']\nAs covered earlier, because slice assignment is a deletion plus an insertion, you\ncan also delete a section of a list by assigning an empty list to a slice (L[i:j]=\n[]); Python deletes the slice named on the left, and then inserts nothing.\nAssigning an empty list to an index, on the other hand, just stores a reference to\nthe empty list object in the specified slot, rather than deleting an item:\n>>> L = ['hack', 'more', 'Py', 'code']\n>>> L[0:1] = []\n>>> L\n['more', 'Py', 'code']\n>>> L[1:] = []\n>>> L\n['more']\n>>> L[0] = []\n>>> L\n[[]]\nAlthough all the operations just discussed are typical, there may be additional\nlist methods and operations not illustrated here. The method toolbox may also\nchange over time, and in fact has: the newer L.copy() method makes a top-level\ncopy of the list, much like L[:] and list(L), but is symmetric with copy\nmethods in sets and dictionaries. For a comprehensive and up-to-date list of type\ntools, you should always consult Python’s manuals.\nAnd because it’s such a common hurdle, this book is compelled to remind you\nonce again that all the in-place change operations discussed here work only for\nmutable objects: they won’t work on strings (or tuples, coming up in Chapter 9),\nno matter how hard you try. Mutability is an inherent yes/no property of each",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": 7,
      "content": "object type—including the subject of this chapter’s next section.\nDictionaries\nAlong with lists, dictionaries are one of the most flexible built-in object types in\nPython. If you think of lists as order-based collections of objects, you can think\nof dictionaries as key-based collections; the chief distinction is that in\ndictionaries, items are stored and fetched by key, instead of by positional offset.\nWhile lists can serve roles similar to arrays in other languages, dictionaries can\ntake the place of records, search tables, and any other sort of aggregation where\nitem names are more meaningful than item positions.\nFor example, dictionaries can replace many of the searching algorithms and data\nstructures you might have to implement manually in lower-level languages—as a\nhighly optimized built-in tool, indexing a dictionary is a very fast search\noperation. Dictionaries also sometimes do the work of records and symbol tables\nused in other languages, can be used to represent sparse (mostly empty) data\nstructures, and much more. As a rundown of their main properties, Python\ndictionaries are:\nAccessed by key, not offset position\nDictionaries are sometimes called associative arrays or hashes (especially by\nusers of other scripting languages). They associate a set of values with\ncorresponding keys, so you can fetch an item out of a dictionary using the\nkey under which you originally stored it. You use the same indexing\noperation to get components in a dictionary as you do in a list, but the index\ntakes the form of a key, not a relative offset.\nInsertion-ordered collections of arbitrary objects\nUnlike in a list, keys in a dictionary are ordered only by the order in which\nthey are inserted. This is not the same as the positional ordering of sequences\nlike lists: items added to dictionaries later always show up at the end of the\nkeys list (even if they appeared earlier in the past), and there is no way to",
      "content_length": 1928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": 7,
      "content": "insert a key into the middle of the keys list. Moreover, this ordering is new as\nof Python 3.7, before which keys’ left-to-right order was pseudo random.\nUnder either ordering regime, keys provide the symbolic (not physical)\nlocations of items in a dictionary.\nVariable-length, heterogeneous, and arbitrarily nestable\nLike lists, dictionaries can grow and shrink in place (without new copies\nbeing made), they can contain objects of any type, and they support nesting\nto any depth (they can be freely mixed with lists, other dictionaries, and so\non). Each key can have just one associated value, but that value can be a\ncollection of multiple objects if needed, and a given value can be stored\nunder any number of keys.\nOf the category “mutable mapping”\nYou can change dictionaries in place by assigning to indexes (they are\nmutable), but they don’t support the sequence operations that work on strings\nand lists. Because dictionaries are key-based collections, operations that\ndepend on a fixed positional order (e.g., concatenation, slicing) don’t make\nsense. Instead, dictionaries are the only built-in, core-type representatives of\nthe mapping category—objects that map keys to values. Other mappings in\nPython are created by imported modules, not language syntax.\nTables of object references (hash tables)\nIf lists are arrays of object references that support access by position,\ndictionaries are tables of object references that support access by key.\nInternally, dictionaries are implemented as hash tables (data structures that\nsupport very fast retrieval), which start small and grow on demand.\nMoreover, Python employs optimized hashing algorithms to find keys, so",
      "content_length": 1674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": 7,
      "content": "retrieval is quick. Like lists, though, dictionaries store object references (not\ncopies, unless you make them explicitly before storing).\nFor reference and preview, Table 8-2 summarizes some of the most common and\nrepresentative dictionary operations and is relatively complete at this writing. As\nusual, though, you should consult Python’s library manual or run a dir(dict) or\nhelp(dict) call for a complete list (dict is the built-in name of the dictionary\ntype).\nTable 8-2. Common dictionary literals and operations\nOperation\nInterpretation\nD = {}\nEmpty dictionary\nD = {'name': 'Pat', 'a\nge': 40.0}\nTwo-item dictionary\nE = {'cto': {'name': '\nSue', 'age': 40}}\nNesting\nD = dict(name='Bob', a\nge=40)\nD = dict([('name', 'Bo\nb'), ('age', 40)])\nD = dict(zip(keyslist,\nvalueslist))\nD = dict.fromkeys(['na\nme', 'age'])\nAlternative construction techniques: keywords,\nkey/value pairs, zipped key/value lists, key lists\nD['name']\nE['cto']['age']\nIndexing by key, nested indexes\n'age' in D\nfor key in D: print(D[\nkey])\nKey membership and iteration\nD.keys()\nD.values()\nD.items()\nD.copy()\nD.clear()\nMethods: all keys,\nall values,\nall key+value tuples,\ncopy (top-level),",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": 7,
      "content": "D.update(D2)\nD.get(key, default?)\nD.pop(key, default?)\nD.setdefault(key, defa\nult?)\nD.popitem()\nclear (remove all items),\nmerge by keys,\nfetch by key, if absent default (or None),\nremove by key, if absent default (or error)\nfetch by key, if absent set default (or None),\nremove/return any (key, value) pair; etc.\nlen(D)\nLength: number of stored entries\nD[key] = 62\nAdding keys, changing key values\ndel D[key]\nDeleting entries by key\nD1 == D2\nComparisons: equality (only)\nlist(D.keys())\nD1.keys() & D2.keys()\nDictionary views\nD = {**x, 'a': 1, **y,\n**z}\nDictionary unpacking\nD = {x: x*2 for x in r\nange(10)}\nDictionary comprehensions\nD | E\nDictionary merge expression: copy + update\nPer Table 8-2, when coded as a literal expression, a dictionary is written as a\nseries of key:value pairs separated by commas, and enclosed in curly braces.4\nAn empty dictionary is an empty set of curly braces, and you can nest\ndictionaries by simply coding one as a value inside another dictionary, or within\na list or tuple literal. There’s a lot more to dictionaries than their literals, though,\nas the next section’s tutorial will begin to reveal.\nDictionaries in Action\nAs Table 8-2 summarizes, dictionaries are indexed by key, and nested dictionary\nentries are referenced by a series of indexes (keys in square brackets). When\nPython creates a dictionary, it stores its items in a way that associates values with",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": 7,
      "content": "keys; to fetch a value back, you supply the key with which it is associated, not its\nrelative position. Let’s go back to the interactive prompt to see what this all\nlooks like in code.\nBasic Dictionary Operations\nIn normal operation, you create dictionaries with literals and store and access\nitems by key with indexing:\n$ python3\n>>> D = {'hack': 1, 'Py': 2, 'code': 3}       # Make a dictionary\n>>> D['Py']                                   # Fetch a value by key\n2\n>>> D                                         # Insertion ordered\n{'hack': 1, 'Py': 2, 'code': 3}\nHere, the dictionary is assigned to the variable D; the value of the key 'Py' is the\ninteger 2, and so on. We use the same square bracket syntax to index dictionaries\nby key as we did to index lists by offset, but here it means access by key, not by\nposition. Notice the end of this example: unlike the randomly ordered sets we\nstudied in Chapter 5, dictionary keys retain their insertion order today, but we\nneed to cover some more basics before formalizing this.\nThe built-in len function works on dictionaries, too—it returns the number of\nvalues stored in the dictionary or, equivalently, the number of its keys. The\ndictionary in membership operator allows you to test for key existence, and the\nkeys method returns all the keys in the dictionary. The latter of these can be\nuseful for processing dictionaries in full, by fetching corresponding values in\nloops. Because the keys result can be treated like a normal list, it can also be\nsorted if order matters and the automatic insertion order doesn’t do the job (more\non sorting and dictionaries later):\n>>> len(D)                                    # Number of entries in dictionary\n3\n>>> 'code' in D                               # Key membership test\nTrue\n>>> list(D.keys())                            # Create a new list of D's keys\n['hack', 'Py', 'code']",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": 7,
      "content": "Notice the second step in this listing. As noted, the in membership test used for\nstrings and lists also works on dictionaries, where it checks whether a key is\nstored. Technically, this works because dictionaries define a protocol run by in\nto lookup a key quickly. Other objects provide in protocols that reflect their\ncommon uses; files, for example, use iterators that read line by line. This will\nmatter later in this book, when we reach iterators and classes.\nAlso note the syntax of the last step in this listing. It uses list for similar\nreasons—the dictionary keys method returns an iterable “view” object that\nproduces results on demand, instead of a physical list. The list call forces it to\nserve up all its values at once for display at the interactive REPL, though this\ncall isn’t required and may even be subpar in some other contexts. More on this,\nas well as other dictionary basics like comparisons, later in this chapter.\nChanging Dictionaries in Place\nLet’s continue with our interactive session. Dictionaries, like lists, are mutable,\nso you can change, expand, and shrink them in place without making new\ndictionaries: simply assign a value to a key to change or create an entry. The del\nstatement works here, too; it deletes the entry associated with the key specified\nas an index. Notice also the nesting of a list inside a dictionary in this example\n(the value of the key 'Py'); all collection data types in Python can nest inside\neach other arbitrarily, and can be changed independently of their containing\nobjects:\n>>> D\n{'hack': 1, 'Py': 2, 'code': 3}\n>>> D['Py'] = ['app', 'dev']                      # Change entry (value=list)\n>>> D\n{'hack': 1, 'Py': ['app', 'dev'], 'code': 3}\n>>> del D['code']                                 # Delete entry (also D.pop(k))\n>>> D\n{'hack': 1, 'Py': ['app', 'dev']}\n>>> D['years'] = 32                               # Add new entry\n>>> D\n{ {'hack': 1, 'Py': ['app', 'dev'], 'years': 32}",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": 7,
      "content": ">>> D['Py'][0] = 'program'                        # Change a nested list in place\n>>> D\n{'hack': 1, 'Py': ['program', 'dev'], 'years': 32}\nLike lists, assigning to an existing index in a dictionary changes its associated\nvalue. Unlike lists, however, whenever you assign a new dictionary key (one that\nisn’t already present) you create a new entry in the dictionary, as was done in the\nprevious example for the key 'years'. This doesn’t work for lists because you\ncan only assign to existing list offsets—Python considers an offset beyond the\nend of a list out of bounds and raises an error. As you learned earlier, to expand a\nlist, you need to use tools such as the append method or slice assignment\ninstead.\nMore Dictionary Methods\nDictionary methods provide a variety of type-specific tools. For instance, the\ndictionary values and items methods return all of the dictionary’s values and\n(key,value) pair tuples, respectively; along with keys, these are useful in loops\nthat need to step through dictionary entries one by one (we’ll start coding such\nloops later in this chapter). As with keys, these two methods also return iterable\nobjects, and wrapping them in a list call collects their values all at once for\ndisplay:\n>>> D = {'program': 1, 'script': 2, 'app': 3}\n>>> list(D.keys())                                 # All keys\n['program', 'script', 'app']\n>>> list(D.values())                               # All values\n[1, 2, 3]\n>>> list(D.items())                                # All (key, value) tuples \n[('program', 1), ('script', 2), ('app', 3)]\nIn realistic programs that gather data as they run, you often won’t be able to\npredict what will be in a dictionary before the program is launched, much less\nwhen it’s coded. Fetching a nonexistent key is normally an error, but the get\nmethod returns a default value—None, or a passed-in default—if the key doesn’t\nexist. It’s an easy way to fill in a default for a key that isn’t present and avoid a\nmissing-key error when your program can’t anticipate contents ahead of time:",
      "content_length": 2037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": 7,
      "content": ">>> D.get('script')                        # A key that is present\n2\n>>> print(D.get('code'))                   # A key that is missing\nNone\n>>> D.get('code', 4)\n4\nThe update method provides something similar to concatenation for dictionaries\n(though it works in the realm of keys and values, not just values). It merges the\nkeys and values of one dictionary into another, both adding new entries for new\nkeys, and blindly overwriting values of the same key if there’s a clash:\n>>> D\n{'program': 1, 'script': 2, 'app': 3}\n>>> D2 = {'code':4, 'hack':5, 'app': 6}    # New keys added, app:6 wins        \n>>> D.update(D2)\n>>> D\n{'program': 1, 'script': 2, 'app': 6, 'code': 4, 'hack': 5}\nFinally, the dictionary pop method deletes a key from a dictionary and returns\nthe value it had. It’s similar to the list pop method, but it takes a key instead of\nan optional position:\n>>> D.pop('app')                           # Pop a dictionary key\n6\n>>> D.pop('hack')                          # Delete key and return its value\n5\n>>> D\n{'program': 1, 'script': 2, 'code': 4} \n>>> L = ['aa', 'bb', 'cc', 'dd']           # Pop a list by position\n>>> L.pop()                                # Delete and return from the end\n'dd'\n>>> L.pop(1)                               # Delete from a specific position\n'bb'\n>>> L\n['aa', 'cc']\nDictionaries also provide a copy method that, as you might guess, makes a copy;\nwe’ll revisit this in Chapter 9, as it’s a way to avoid the potential side effects of\nshared references to the same dictionary. In fact, dictionaries come with more\nmethods than the common ones demoed here, and over time may gain others",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": 7,
      "content": "beyond those listed in Table 8-2; again, see the Python library manual, dir and\nhelp, or other reference resources for a comprehensive list.\nOther Dictionary Makers\nBecause dictionaries are so useful, multiple ways to build them have emerged\nover time. The following summarizes the most common alternatives; its last two\ncalls to the dict constructor (really, type name) have the same effect as the\nliteral and key-assignment forms listed above them:\n{'name': 'Pat', 'age': 40}              # 1) Traditional literal expression\nD = {}                                  # 2) Assign by keys dynamically\nD['name'] = 'Pat'\nD['age']  = 40\ndict(name='Pat', age=40)                # 3) dict keyword-argument form\ndict([('name', 'Pat'), ('age', 40)])    # 4) dict key/value tuples form\nAll four of these forms create the same two-key dictionary, but they are useful in\ndiffering circumstances:\nThe first is handy if you can spell out the entire dictionary ahead of\ntime.\nThe second is of use if you need to create the dictionary one field at a\ntime on the fly.\nThe third involves less typing than the first, but it requires all keys to be\nstrings.\nThe last is useful if you need to build up keys and values as sequences\nat runtime.\nWe met name=value keyword arguments earlier when sorting lists; the dict\nform in this summary that uses them is newer than literals and common in\nPython code, because it has less syntax (and hence less room for mistakes). The\nlast form in the summary is also commonly used in conjunction with the zip\nfunction, to combine separate lists of keys and values obtained dynamically at",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": 7,
      "content": "runtime (parsed out of a data file’s columns, for instance):\ndict(zip(keyslist, valueslist))        # Zipped key/value tuples form (ahead)\nThere is more on zipping dictionary keys in the next section. Provided all the\nkey’s values are the same initially, you can also create a dictionary with the\nfollowing special form—simply pass in a list of keys and an initial value for all\nof the values (the default is None). Notice this is run from the dict type name,\nnot an actual dictionary:\n>>> dict.fromkeys(['a', 'b'], 0)\n{'a': 0, 'b': 0}\nAlthough you could get by with just literals and key assignments at this point in\nyour Python career, you’ll probably find uses for all of these dictionary-creation\nforms as you start applying them in realistic Python programs.\nDictionary-literal unpacking\nAs of Python 3.5, dictionary literals also support a special ** syntax that\nunpacks the contents of another dictionary in its top level, similar to the * in list\nliterals we met earlier. Any number of ** can appear in a literal to unpack any\nnumber of dictionaries, and the rightmost’s value wins when keys collide:\n>>> D = dict(a=4, c=3)\n>>> {'a': 1, 'b': 2, **D}                     # Dictionary-literal unpacking\n{'a': 4, 'b': 2, 'c': 3}\n>>> dict(a=1, b=2) | D                        # Same, but with union operator\n{'a': 4, 'b': 2, 'c': 3}\nAs shown, the effect of ** is similar to both the update method we met earlier\nand the dictionary | union covered ahead and might avoid a follow-up update\ncall in some contexts. It’s also related to the extended-unpacking assignment\nstatement you’ll meet in Chapter 11.\nThe ** also works in dict, but simply because ** unpacks keyword arguments\nin any function call (as usual in Python, this relies on larger concepts we’ll reach\nlater in this book). Unlike in {} literals though, ** in dict fails if any key",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": 7,
      "content": "appears twice:\n>>> dict(a=1, **{'b': 2}, **dict(c=3))        # Works in dict(), as in all calls\n{'a': 1, 'b': 2, 'c': 3}                      # As long as no keys are repeated!\n \n>>> dict(a=1, b=2, **dict(a=4, c=3))\nTypeError: dict() got multiple values for keyword argument 'a'\nThe examples so far demo the many basic ways to create dictionaries, but there\nis yet another that’s even more powerful, as the next section will explain.\nDictionary Comprehensions\nLike the set and list comprehensions we met in previous coverage, dictionary\ncomprehensions run an implied loop, collecting the key/value results of\nexpressions on each iteration and using them to fill out a new dictionary. A loop\nvariable allows the comprehension to use loop iteration values along the way.\nThe net effect lets us create new dictionaries with small bits of code that are\nsimpler than the full-blown statements we’ll study later in this book.\nAbstractly, dictionary comprehensions map to for loops as follows, where both\nk and v can use loop variable x:\n{k: v for x in iterable}        # Dictionary comprehension\nnew = {}                        # Equivalent loop code\nfor x in iterable:\n    new[k] = v\nTo illustrate, a standard way to initialize a dictionary dynamically is to combine\nits keys and values with zip, and pass the result to the dict call, per the last\nsection. The zip built-in function is the hook that allows us to construct a\ndictionary from key and value lists this way—if you cannot predict the set of\nkeys and values in your code, you may be able to build them up as lists and zip\nthem together. We’ll study zip in detail in the next part of this book; it’s an\niterable, so we must wrap it in a list call to show its results there, but its basic\nusage is otherwise straightforward:\n>>> list(zip(['a', 'b', 'c'], [1, 2, 3]))        # Zip together keys and values",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": 7,
      "content": "[('a', 1), ('b', 2), ('c', 3)]\n>>> D = dict(zip(['a', 'b', 'c'], [1, 2, 3]))    # Make a dict from zip result\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\nYou can achieve the same effect, though, with a dictionary comprehension\nexpression. The following uses tuple assignment to unpack items into variables\n(per its Chapter 4 debut), as it scans the list of zipped pairs from left to right. The\nnet effect builds a new dictionary with a key/value pair for every such pair in the\nzip result (the Python code reads almost the same as this natural-language\ndescription, but with a bit more formality):\n>>> D = {k: v for (k, v) in zip(['a', 'b', 'c'], [1, 2, 3])}\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\nComprehensions are longer to code in this case, but they’re also more general\nthan this example implies—we can use them to map a single stream of values to\ndictionaries and apply them to any kind of sequence (or other iterable), and\ncollected keys can be computed with expressions just like collected values:\n>>> D = {x: x ** 2 for x in [1, 2, 3, 4]}        # Or: in range(1, 5)\n>>> D\n{1: 1, 2: 4, 3: 9, 4: 16}\n>>> D = {c: c * 4 for c in 'HACK'}               # Loop over any iterable\n>>> D\n{'H': 'HHHH', 'A': 'AAAA', 'C': 'CCCC', 'K': 'KKKK'}\n>>> D = {c.lower(): (c + '!') for c in ['HACK', 'PY', 'CODE']}    # Expr: expr\n>>> D\n{'hack': 'HACK!', 'py': 'PY!', 'code': 'CODE!'}\nDictionary comprehensions are also useful for initializing dictionaries from keys\nlists, in much the same way as the fromkeys method we met at the end of the\npreceding section:\n>>> D = dict.fromkeys(['a', 'b', 'c'], 0)        # Initialize dict from keys\n>>> D\n{'a': 0, 'b': 0, 'c': 0}",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": 7,
      "content": ">>> D = {k: 0 for k in ['a', 'b', 'c']}          # Same, but with a comprehension\n>>> D\n{'a': 0, 'b': 0, 'c': 0}\n>>> D = dict.fromkeys('code')                    # Other iterables, default value\n>>> D\n{'c': None, 'o': None, 'd': None, 'e': None}\n>>> D = {k: None for k in 'code'}\n>>> D\n{'c': None, 'o': None, 'd': None, 'e': None}\nLike its list and set relatives, dictionary comprehensions support additional\nsyntax not shown here, including if clauses to filter values out of results, and\nnested for loops. Unfortunately, to truly understand dictionary comprehensions,\nyou need to also know more about iteration statements and concepts in Python,\nand this book hasn’t yet disclosed enough information to tell that story well.\nYou’ll learn much more about all flavors of comprehensions—list, set,\ndictionary, and generator—in Chapters 14 and 20, so we’ll defer further details\nuntil later.\nKey Insertion Ordering\nBy now, you’ve probably noticed that dictionaries remember their keys’ order—\nno matter what sort of syntax we use to make them. As noted both earlier here\nand in Chapter 4, as of Python 3.7 (and CPython 3.6), dictionaries have shed\ntheir former random key order and adopted insertion order instead: keys are now\nordered left to right from oldest to newest additions. This order is maintained for\nliterals, dict calls, comprehensions, and new keys added on the fly.\nAmong other things, insertion order makes dictionary outputs arguably more\ncoherent and readable and may avoid key sorts in some programs. This isn’t the\nsame as a list (e.g., there’s no way to add a key in the “middle”) and doesn’t\nmake dictionaries sequences. The effect, however, is close. Here’s a brief\nillustration:\n>>> D = dict(a=1, b=2)              # Literal keys stored left-to-right\n>>> D['c'] = 3                      # New keys always added at the end\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\n>>> D.pop('b')                      # Method pop removes any key by name",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": 7,
      "content": "2\n>>> D                               # Other keys still in insertion order\n{'a': 1, 'c': 3}\n>>> D['b'] = 2                      # Earlier key goes at the end too\n>>> D\n{'a': 1, 'c': 3, 'b': 2}\n>>> D['c'] = 0                      # Changing values doesn't impact order\n>>> D['d'] = 1                      # And new arrivals always go at the end\n>>> D\n{'a': 1, 'c': 0, 'b': 2, 'd': 1}\nIf you stare at this long enough, you’ll probably notice that the insertion order of\ndictionaries is much like the LIFO stack ordering we coded with list append and\npop methods earlier. In fact, the dictionary popitem is defined to return the\nkey/value pair (really, tuple) for the key added most recently (i.e., at the end):\n>>> D.popitem()\n('d', 1)\n>>> D.popitem()\n('b', 2)\n>>> D\n{'a': 1, 'c': 0}\nWhile useful in some contexts, keep in mind that you may still need to sort\ndictionary keys manually, using techniques we’ll study ahead. Ordering\nfilenames for display, for example, will often warrant string-value sorts instead\nof a program’s internal insertion order.\nDictionary “Union” Operator\nLast up in the dictionary toolbox is a bit of an oddball: as of Python 3.9,\ndictionaries have sprouted a “union” operation that’s kicked off with the |\noperator—the same syntax used for union in the set objects you met in\nChapter 5.\nPer the quotes, though, this operation isn’t really mathematical set union at all.\nIt’s a positionally sensitive, key-based merge of dictionaries, that works the same\nas the dictionary update method but returns its result as a new dictionary instead\nof updating a subject in place. Hence, dictionary | is like the combination of\nrunning the dictionary’s copy and update methods in turn, and really just a",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": 7,
      "content": "minor convenience for narrow roles. Here it is at work:\n>>> D = dict(a=1, b=2)\n>>> D\n{'a': 1, 'b': 2}\n \n>>> D | {'b': 3, 'c': 4}             # Update, return, rightmost wins\n{'a': 1, 'b': 3, 'c': 4}\n>>> D | {'b': 3, 'c': 4} | dict(a=5, d=6)\n{'a': 5, 'b': 3, 'c': 4, 'd': 6}\nYou get roughly the same mileage with the dictionary’s longstanding update\nmethod, though it requires a manual copy to avoid in-place changes in programs\nthat don’t want them (as a spoiler, the |= in-place assignment form of dictionary\nunion that you’ll meet later in this book is identical to calling update with a\nsingle argument sans copy):\n>>> D\n{'a': 1, 'b': 2}\n>>> C = D.copy()\n>>> C.update({'b': 3, 'c': 4})       # \"Union\" sans the | expression\n>>> C.update(dict(a=5, d=6))\n>>> C\n{'a': 5, 'b': 3, 'c': 4, 'd': 6}\nThe Python 3.5 dictionary-literal unpacking we explored earlier can have the\nsame effect too, though, at least when used with gnarly embedded literals as in\nthe following, perhaps less readably:\n>>> D\n{'a': 1, 'b': 2}\n>>> {**D, **{'b': 3, 'c': 4}, **dict(a=5, d=6)}\n{'a': 5, 'b': 3, 'c': 4, 'd': 6}\nWhich—true sports fans will note—raises the number of dedicated dictionary\nmerge operations from one to three since this book’s prior edition. This bloat is\nespecially grievous, given that it takes just one line of simple code to accomplish\nwhat the update method, the newer ** unpacking, and the newest | union\noperator all redundantly do in their main use cases:",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": 7,
      "content": ">>> D1 = dict(a=1, b=1)\n>>> D2 = dict(a=2, c=2)\n>>> for k in D2: D1[k] = D2[k]        # What update(), **, and | redundantly do\n>>> D1                                # Did this really justify 3 alternatives?\n{'a': 2, 'b': 1, 'c': 2}\nDictionary | “union” may be handy in limited contexts. But whether this\njustifies convoluting dictionaries with just one binary operator, just one of many\nset operations, and an operation that’s almost entirely redundant with two earlier\ntools that were already almost entirely redundant with very simple code, is a\nriddle left to the reader to solve.\nIntermission: Books Database\nLet’s take a break from the fine points and code a more concrete example of\ndictionaries at work. In honor of this book’s quarter-century milestone, the\nfollowing builds a simple in-memory editions database that maps edition years\n(the keys) to titles (the values). As coded, you fetch edition names by indexing\non year strings:\n>>> table = {'2024': 'Learning Python, 6th Edition',       # Year => title\n             '2013': 'Learning Python, 5th Edition',\n             '1999': 'Learning Python'}\n>>> table['2024']                                          # Key => Value\n'Learning Python, 6th Edition' \n>>> for year in table:                                     # Keys iteration\n        print(year + '\\t' + table[year])\n2024     Learning Python, 6th Edition\n2013     Learning Python, 5th Edition\n1999     Learning Python\nThe last command uses a for loop, which we’ve used several times since its\nChapter 4 preview. The full tale of the for isn’t told until Chapter 13, but this\nparticular loop simply iterates through each key in the table to print a tab-\nseparated list of keys and their values (recall from Chapter 7 that \\t in a Python\nstring means vertical tab).",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": 7,
      "content": "Dictionaries aren’t sequences like lists and strings, but if you need to step\nthrough the items in a dictionary, it’s easy—either call the dictionary keys\nmethod or use the dictionary itself. Given a dictionary D, saying for key in D\nworks the same as saying for key in D.keys(), and both use the iteration\nprotocol that we’ll expand on later in this book. Either way, you can index from\nkey to value inside the for loop as you go, as this code does.\nMapping values to keys\nNotice how the prior table maps years to titles, but not vice versa. If you want to\nmap the other way—titles to years—you can either code the dictionary\ndifferently:\n>>> table = {'Learning Python, 6th Edition': 2024,         # Title => year\n             'Learning Python, 5th Edition': 2013,\n             'Learning Python':              1999}\n>>> table['Learning Python']                               # Key => value\n1999\nOr use other dictionary methods like items that give searchable sequences (the\nlist call is required in the following because items returns an iterable view, a\ntopic coming up shortly):\n>>> list(table.items())[:2]\n[('Learning Python, 6th Edition', 2024), ('Learning Python, 5th Edition', 2013)]\n>>> [title for (title, year) in table.items() if year == 1999]     # Value => key\n['Learning Python']\nThe last command here uses the list comprehension we explored earlier, as well\nas tuple assignment mentioned earlier and covered in this book’s next part\n(synopsis: it unpacks items into variables). The combo scans the (key, value)\npairs returned by the dictionary’s items method, selecting keys for values\nmatching a search target (1999).\nThe net effect of all this is to index backward—from value to key, instead of key\nto value. This is useful if you want to store data just once and map backward\nfrom values only rarely (searching through sequences like this is generally much",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": 7,
      "content": "slower than a direct key-to-value index, though not all programs need to care).\nIn fact, although dictionaries by nature map keys to values unidirectionally, there\nare multiple ways to map values back to keys with a bit of extra generalizable\ncode:\n>>> K = 'Learning Python'\n>>> V = 1999\n>>> table[K]              # Key = >Value (normal usage)\n1999\n>>> [key for (key, value) in table.items() if value == V]         # Value => Key\n['Learning Python'] \n>>> [key for key in table.keys() if table[key] == V]              # Same: keys()\n['Learning Python']\nNote that both of the last two commands return a list of titles: in dictionaries,\nthere’s just one value per key, but there may be many keys per value if a given\nvalue is be stored under multiple keys, and a value might be a collection itself to\nrepresent many values per key. It’s also possible to invert a dictionary for\nindexing values by zipping its values and keys—but only if its values are all\nimmutable and don’t appear in multiple keys:\n>>> dict(zip(table.values(), table.keys()))[1999]      # Key/value inversion?\n'Learning Python'\nSharp-eyed readers may notice that this yields a dictionary with integer keys,\nwhich works naturally per a tip in the next section. For more on this front, watch\nfor a less lossy dictionary inversion function in the mapattrs.py example from\n“Example: Mapping Attributes to Inheritance Sources”—code that would surely\nstretch this preview past its breaking point if included here. For this chapter’s\npurposes, let’s wrap up by adding in a few pragmatic pieces to the dictionary\npuzzle.\nDictionary Usage Tips\nDictionaries are fairly straightforward tools once you get the hang of them, but\nhere are a few additional pointers and reminders you should be aware of when\nusing them:",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": 7,
      "content": "Sequence operations don’t work\nDictionaries are mappings, not sequences. Because they deal with keys and\nvalues and are ordered by insertion-time only, things like concatenation (an\nordered joining of values) and slicing (extracting a contiguous section of\nvalues) simply don’t apply—and Python raises an error if your code tries to\nuse them on dictionaries.\nAssigning to new indexes adds entries\nKeys can be created when you write a dictionary literal (embedded in the\ncode of the literal itself), or when you assign values to new keys of an\nexisting dictionary object individually. The end result is the same.\nKeys need not always be strings\nOur examples so far have used strings as keys, but any other immutable\nobjects work just as well. For instance, you can use integers as keys, which\nmakes the dictionary look much like a list (when indexing, at least). Tuples\nmay be used as dictionary keys too, allowing compound key values—such as\ndates and IP addresses—to have associated values. User-defined class\ninstance objects (discussed in Part VI) can also be used as keys, as long as\nthey define the proper methods; roughly, they need to tell Python that their\nvalues are “hashable” and thus won’t change, as otherwise they would be\nuseless as fixed keys. Mutable objects such as lists, sets, and other\ndictionaries don’t work as keys because they may change, but are allowed as\nvalues.\nThe following sections delve deeper into these and other mysteries of dictionary-\nprocessing code.\nUsing dictionaries to simulate flexible lists: Integer keys",
      "content_length": 1549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": 7,
      "content": "The last point in the preceding list is important enough to demonstrate with a\nfew examples. When you use lists, it is illegal to assign to an offset that is off the\nend of the list:\n>>> L = []\n>>> L[99] = 'hack'\nIndexError: list assignment index out of range\nAlthough you can use repetition to preallocate as big a list as you’ll need (e.g.,\n[0]*100), you can also do something that looks similar with dictionaries that\ndoes not require such space allocations—and potential waste. By using integer\nkeys, dictionaries can emulate lists that seem to grow on offset assignment:\n>>> D = {}\n>>> D[99] = 'hack'\n>>> D[99]\n'hack'\n>>> D\n{99: 'hack'}\nHere, it looks as if D is a 100-item list, but it’s really a dictionary with a single\nentry; the value of the key 99 is the string 'hack'. You can access this structure\nwith offsets much like a list, catching nonexistent keys with get or in tests if\nrequired, but you don’t have to allocate space for all the positions to which you\nmight need to assign values in the future. When used like this, dictionaries are\nlike more flexible equivalents of lists:\n>>> D[62] = 'code'\n>>> D[30] = 'write'\n>>> D[62]\n'code'\n>>> D\n{99: 'hack', 62: 'code', 30: 'write'}\nAs another example, we might also employ integer keys in the first book book-\ndatabase code we wrote earlier to avoid quoting the year, albeit at the expense of\nexpressiveness (integer keys cannot contain nondigit characters):\n>>> table = {2024: 'Learning Python, 6th Edition',      # Integers work as keys\n             …etc…",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": 7,
      "content": ">>> table[2024]\n'Learning Python, 6th Edition'\nUsing dictionaries for sparse data structures: Tuple keys\nIn a similar way, dictionary keys are also commonly leveraged to implement\nsparse—mostly empty—data structures, such as multidimensional arrays where\nonly a few positions have values stored in them:\n>>> Matrix = {}\n>>> Matrix[(2, 3, 4)] = 88\n>>> Matrix[(7, 8, 9)] = 99\n>>> \n>>> X = 2; Y = 3; Z = 4               # A ; separates statements: see Chapter 10\n>>> Matrix[(X, Y, Z)]\n88\n>>> Matrix\n{(2, 3, 4): 88, (7, 8, 9): 99}\nHere, we’ve used a dictionary to represent a three-dimensional array that is\nempty except for the two positions (2,3,4) and (7,8,9). The keys are tuples\nthat record the coordinates of nonempty slots. Rather than allocating a large and\nmostly empty three-dimensional matrix to hold these values, we can use a simple\ntwo-item dictionary. In this scheme, accessing an empty slot triggers a\nnonexistent key exception, as these slots are not physically stored:\n>>> Matrix[(2,3,6)]\nKeyError: (2, 3, 6)\nAvoiding missing-key errors\nErrors for nonexistent key fetches like the foregoing are common in sparse\nmatrixes, but you probably won’t want them to shut down your program. There\nare at least three ways to fill in a default value instead of getting such an error\nmessage—you can use the dictionary get method shown earlier to provide a\ndefault for keys that do not exist, test for keys ahead of time in if statements, or\nuse a try statement to catch and recover from the error. Though straightforward,\nthe last of these are previews of statement syntax we’ll begin studying in\nChapter 10:",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": 7,
      "content": ">>> Matrix.get((2, 3, 4), 0)           # Exists: fetch and return\n88\n>>> Matrix.get((2, 3, 6), 0)           # Doesn't exist: use passed default\n0\n>>> if (2, 3, 6) in Matrix:            # Check for key before fetch\n...     print(Matrix[(2, 3, 6)])       # See Chapters 10 and 12 for if/else\n... else:\n...     print(0)\n...\n0\n>>> try:\n...     print(Matrix[(2, 3, 6)])       # Try to index\n... except KeyError:                   # Catch and recover\n...     print(0)                       # See Chapters 10 and 34 for try/except\n...\n0\nOf these, the get method is the most concise in terms of coding requirements,\nbut the if and try statements are much more general in scope—as you’ll start\nseeing for yourself soon in Chapter 10.\nNesting in dictionaries\nAs you can tell, dictionaries can play many roles in Python. In general, they can\nreplace search data structures (because indexing by key is a search operation)\nand can represent many types of structured information. For example,\ndictionaries are one of many ways to describe the properties of an item in your\nprogram’s domain; that is, they can serve the same role as “records” or “structs”\nin other language, and JSON content in language-neutral roles. The following,\nfor example, fills out a dictionary describing a book, by assigning to new keys:\n>>> rec = {}\n>>> rec['title'] = 'Learning Python, 5th Edition'\n>>> rec['year']  = 2013\n>>> rec['isbn']  = '9781449355739'\n>>>\n>>> rec['year'], rec['isbn']\n(2013, '9781449355739')\nEspecially when nested, though, Python’s built-in data types allow us to easily\nrepresent structured information. The following again uses a dictionary to",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": 7,
      "content": "capture object properties, but it codes it all at once rather than assigning to each\nkey separately, and nests a dictionary and list to represent structured property\nvalues:\n>>> rec = {'title':  'Learning Python, 5th Edition',\n           'date':   {'year': 2013, 'month': 'July'},\n           'isbns':  ['1449355730', '9781449355739']}\nTo fetch components of nested objects, simply string together indexing\noperations:\n>>> rec['title']\n'Learning Python, 5th Edition'\n>>> rec['isbns']\n['1449355730', '9781449355739']\n>>> rec['isbns'][1]\n'9781449355739'\n>>> rec['date']['year']\n2013\nAlthough you’ll learn in Part VI that classes (which group both data and logic)\ncan sometimes be better in this record role, dictionaries are an easy-to-use tool\nfor simpler requirements. For more on record representation choices, see also the\nupcoming sidebar “Why You Will Care: List Versus Dictionary Versus Set”.\nAlso notice that while we’ve focused on a single “record” with nested data here,\nthere’s no reason we couldn’t nest the record itself in a larger, enclosing\ndatabase collection coded as a list or dictionary, though an external file or\nformal database interface often plays the role of top-level container in realistic\nprograms. The following abstract snippets would both print a record’s two-item\nisbns list if run live and provided with an other record omitted here:\ndb = []\ndb.append(rec)             # A list \"database\"\ndb.append(other)\ndb[0]['isbns']\ndb = {}\ndb['lp5e'] = rec            # A dictionary \"database\"\ndb['lp6e'] = other\ndb['lp5e']['isbns']",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": 7,
      "content": "Later in the book you’ll meet tools such as Python’s shelve, which works much\nthe same way, but automatically maps objects to and from files to make them\npermanent. Python objects really can be database records.\nDictionary key/value/item view objects\nWhen we explored the dictionary keys, values, and items methods earlier, we\nwrapped their results in list calls for display at the REPL prompt. Technically,\nthis is because these methods return view objects that produce results on\ndemand, instead of physical lists. Displaying their raw values suggests as much,\nbut collects their values anyhow:\n>>> D = dict(program=1, script=2, app=3)\n>>> D.keys()\ndict_keys(['program', 'script', 'app'])\nView objects are iterables, which we’ve seen simply means objects that generate\nresult items one at a time, instead of producing the result list all at once in\nmemory. Besides being iterable, dictionary views are insertion ordered, reflect\nfuture changes to the dictionary, and support set operations. On the other hand,\nbecause they are not lists, they do not directly support operations like indexing\nor the list sort method, and do not display as a normal list when printed.\nWe’ll discuss the notion of iterables more formally in Chapter 14, but for our\npurposes here it’s enough to know that we have to run the results of these three\nmethods through the list built-in if we want to apply list operations or display\ntheir values as lists. For example:\n>>> D = dict(a=1, b=2, c=3)\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\n \n>>> K = D.keys()                   # Makes a view object, not a list\n>>> K\ndict_keys(['a', 'b', 'c'])\n>>> list(K)                        # Force a real list when needed\n['a', 'b', 'c']\n \n>>> V = D.values()                 # Ditto for values and items views\n>>> V\ndict_values([1, 2, 3])",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": 8,
      "content": ">>> list(V)\n[1, 2, 3]\n \n>>> D.items()\ndict_items([('a', 1), ('b', 2), ('c', 3)])\n>>> list(D.items())\n[('a', 1), ('b', 2), ('c', 3)]\n \n>>> K[0]                           # List operations fail unless converted\nTypeError: 'dict_keys' object is not subscriptable\n>>> K.sort()\nAttributeError: 'dict_keys' object has no attribute 'sort'\n>>> list(K)[0], list(K).sort()\n('a', None)\nUnlike lists, though, dictionary views don’t take up space for their full results,\nand are not carved in stone when created—they dynamically reflect future\nchanges made to the dictionary after the view object has been created:\n>>> D = {'a': 1, 'b': 2, 'c': 3}\n>>> D\n{'a': 1, 'b': 2, 'c': 3}\n>>> K = D.keys()\n>>> V = D.values()\n>>> list(K), list(V)              # Views maintain same order as dictionary\n(['a', 'b', 'c'], [1, 2, 3])\n>>> del D['b']                    # Change the dictionary in place\n>>> D\n{'a': 1, 'c': 3}\n>>> list(K), list(V)              # Reflected in any current view objects\n(['a', 'c'], [1, 3])\nThat said, use cases for dynamic view morph are likely rare. In fact, apart from\nresult displays at the interactive prompt, you probably won’t even notice views\nvery often in practice, because looping constructs in Python automatically force\nthem to produce one result on each:\n>>> for k in D.keys(): print(k)    # View iterators used automatically in loops\n...                                # But no need to call keys() to iterate on D\na\nb\nc",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": 8,
      "content": "As we’ve seen, it’s also often unnecessary to call keys directly in such cases\nbecause dictionaries themselves provide implicit key iterators; for better or\nworse, for k in D is usually as much code as you need to type.\nDictionary views and sets\nThough perhaps even more obscure than view objects in general, some\ndictionary views are also set-like and support set operations such as union and\nintersection that we used on true sets in Chapter 5. Specifically, views returned\nby the keys method are set-like, values views are not, and items views are if\ntheir (key, value) pairs are unique and hashable (immutable). This reflects\nlogical symmetry: set items are unique and immutable just like dictionary keys,\nand sets themselves behave like unordered and valueless dictionaries (and are\neven coded in curly braces).\nHere is what keys views look like when used in set operations (continuing the\nprior section’s session); as also shown, dictionary value views are never set-like,\nbecause their items are not necessarily unique or immutable:\n>>> K, V\n(dict_keys(['a', 'c']), dict_values([1, 3])) \n>>> K | {'x': 4}                   # Keys (and some items) views are set-like\n{'x', 'a', 'c'}                    # Results are unordered sets, not views\n>>> V | {'x': 4}\nTypeError: unsupported operand type(s) for |: 'dict_values' and 'dict'\n>>> V | {'x': 4}.values()\nTypeError: unsupported operand type(s) for |: 'dict_values' and 'dict_values'\nBe careful not to confuse the | here with the dictionary union operation we met\nearlier in “Dictionary “Union” Operator”; when run on a view, | is not a full\nkey-based dictionary merge. In set operations, views may be mixed with other\nviews, sets, and dictionaries, and dictionaries are treated the same as their keys\nviews:\n>>> D = {'a': 1, 'b': 2, 'c': 3}\n>>> D.keys() & D.keys()            # Intersect views\n{'b', 'a', 'c'}\n>>> D.keys() & {'b'}               # Intersect view and set\n{'b'}",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": 8,
      "content": ">>> D.keys() & {'b': 1}            # Intersect view and dict\n{'b'}\n>>> D.keys() | {'b', 'c', 'd'}     # Union view and set\n{'b', 'a', 'd', 'c'}\nItems views are set-like too, but only if they are hashable—that is, if they\ncontain only immutable objects:\n>>> {'a': [1, 2]}.items() | {0, 1}     # Immutable value: no set ops\nTypeError: unhashable type: 'list'\n>>> D = {'a': 1}\n>>> D.items()                          # Items set-like if and only if hashable\ndict_items([('a', 1)])\n>>> D.items() | D.keys()               # Union view and view\n{('a', 1), 'a'}\n>>> D.items() | D                      # Dictionary treated same as its keys\n{('a', 1), 'a'}\n \n>>> D.items() | {('c', 3), ('d', 4)}           # Set of key/value pairs\n{('a', 1), ('c', 3), ('d', 4)}\n>>> dict(D.items() | {('c', 3), ('d', 4)})     # dict accepts iterable sets too\n{'a': 1, 'c': 3, 'd': 4}\nSee Chapter 5’s coverage of sets if you need a refresher on these operations.\nTheir role in dictionary views is probably uncommon and may even be\nacademic; but they work when helpful (and at least you now have a fighting\nchance when they crop up in sadistic final exams!). Here, let’s wrap up with two\nmore quick coding notes for dictionaries.\nSorting dictionary keys\nFirst of all, keys lists must be sorted when the inherent insertion order doesn’t\nmeet your goals. Beware, though, that a common coding pattern for scanning a\ncollection in sorted order won’t work, because keys does not return a list:\n>>> D = {'c': 3, 'b': 2, 'a': 1}\n>>> D\n{'c': 3, 'b': 2, 'a': 1}\n \n>>> Ks = D.keys()                            # Sorting a view object doesn't work!\n>>> Ks.sort()\nAttributeError: 'dict_keys' object has no attribute 'sort'",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": 8,
      "content": "To work around this, you can either convert keys to a list manually, or use the\nsorted call, applied to lists earlier in this chapter, on either a keys view or the\ndictionary itself:\n>>> Ks = list(D.keys())                      # Convert keys to a list\n>>> Ks.sort()                                # And then sort with list.sort()\n>>> for k in Ks: print(k, D[k])\n...\na 1\nb 2\nc 3\n>>> Ks = D.keys()                            # Or use sorted() on the keys view\n>>> for k in sorted(Ks): print(k, D[k])      # sorted() accepts any iterable\n...                                          # sorted() returns its result\na 1\nb 2\nc 3\nOf these, using the dictionary’s keys iterator is simplest and common, though\nalso arguably implicit:\n>>> for k in sorted(D): print(k, D[k])       # Or use sorted() on the dict\n...                                          # dict iterators produce keys \na 1\nb 2\nc 3\nDictionary magnitude comparisons\nFinally, dictionaries can be compared for equality directly with ==, which\nautomatically compares each key/value pair and ignores insertion order:\n>>> D1 = dict(a=1, b=2, c=3)\n>>> D2 = dict(c=3, b=2, a=1)\n>>> D1, D2\n({'a': 1, 'b': 2, 'c': 3}, {'c': 3, 'b': 2, 'a': 1})\n>>> D1 == D2, D1 != D2\n(True, False)\nMagnitude comparisons like < and > do not work on dictionaries themselves,\nthough you can implement them by comparing key/value items views manually,\nif you also sort them to discount any insert-order difference (subtlety: a > on the",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": 8,
      "content": "items results directly runs the set superset test because the views are set-like—\nnot greater-than!):\n>>> D1 = dict(a=1, b=2, c=4)\n>>> D2 = dict(c=3, b=2, a=1)                   # D1 > D2: D1['c'] > D2['c']\n>>> D1 > D2\nTypeError: '>' not supported between instances of 'dict' and 'dict'\n>>> list(D1.items()), list(D2.items())\n([('a', 1), ('b', 2), ('c', 4)], [('c', 3), ('b', 2), ('a', 1)])\n \n>>> list(D1.items()) > list(D2.items())        # Fails: insertion order\nFalse\n>>> D1.items() > D2.items()                    # Fails: superset, not magnitude\nFalse\n>>> sorted(D1.items()) > sorted(D2.items())    # OK: sorted to neutralize order\nTrue\nBecause we’ll revisit this near the end of the next chapter to reinforce them in\nthe context of comparisons at large, we’ll postpone further coverage here.",
      "content_length": 797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": 8,
      "content": "Chapter Summary\nIn this chapter, we explored the list and dictionary types—probably the two most\ncommon, flexible, and powerful collection types you will see and use in Python\ncode. We learned that the list type supports positionally ordered collections of\narbitrary objects, and that it may be freely nested and grown and shrunk on\ndemand. The dictionary type is similar, but it stores items by key instead of by\nposition and orders keys by insertion time only. Both lists and dictionaries are\nmutable, and so support a variety of in-place change operations not available for\nstrings: for example, lists can be grown by slice assignment and append calls,\nand dictionaries by key assignment and update.\nIn the next chapter, we will wrap up our in-depth core object-type tour by\nlooking at tuples and files. After that, we’ll move on to statements that code the\nlogic that processes our objects, taking us another step toward writing complete\nprograms. Before we tackle those topics, though, here are some chapter quiz\nquestions to review what you’ve learned.\nTest Your Knowledge: Quiz\n1. Name two ways to build a list containing five integer zeros.\n2. Name two ways to build a dictionary with two keys, 'a' and 'b', each\nhaving an associated value of 0.\n3. Name four operations that change a list object in place.\n4. Name four operations that change a dictionary object in place.\n5. Why might you use a dictionary instead of a list?\nTest Your Knowledge: Answers\n1. A literal expression like [0, 0, 0, 0, 0] and a repetition expression\nlike [0] * 5 will each create a list of five zeros. In practice, you might",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": 8,
      "content": "also build one up with a loop that starts with an empty list and appends\n0 to it in each iteration, with L.append(0). A list comprehension like\n[0 for i in range(5)] could work here, too, but this is more work\nthan you need to do for this answer.\n2. A literal expression such as {'a': 0, 'b': 0} or a series of\nassignments like D = {}, D['a'] = 0, and D['b'] = 0 would create\nthe desired dictionary. You can also use the newer and simpler-to-code\ndict(a=0, b=0) keyword form, or the more flexible dict([('a',\n0), ('b', 0)]) key/value sequences form. Because all the values are\nthe same, you can also use the special form dict.fromkeys('ab', 0),\nand dictionary comprehension like {k: 0 for k in 'ab'} suffices\ntoo, though again, this may be overkill here.\n3. The append and extend methods grow a list in place, the sort and\nreverse methods order and reverse lists, the insert method inserts an\nitem at an offset, the remove and pop methods delete from a list by\nvalue and by position, the del statement deletes an item or slice, and\nindex and slice assignment statements replace an item or entire section.\nPick any four of these for the quiz.\n4. Dictionaries are primarily changed by assignment to a new or existing\nkey, which creates or changes the key’s associated value in the table.\nAlso, the del statement deletes a key’s entry, the dictionary update\nmethod merges one dictionary into another in place, and D.pop(key)\nremoves a key and returns the value it had. Dictionaries also have other,\nmore exotic in-place change methods presented tersely or not at all in\nthis chapter, such as popitem and setdefault; see reference resources\nfor more details.\n5. This question is a bit unfair, given that the following sidebar gives the\nanswer, but you may have already figured this out on your own.\nDictionaries are generally better when the data is labeled (a record with\nfield names, for example); lists are best suited to collections of\nunlabeled items (such as all the files in a directory). Dictionary lookup\nis also usually quicker than searching a list, though this might vary per",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": 8,
      "content": "program and Python.\nWHY YOU WILL CARE: LIST VERSUS DICTIONARY\nVERSUS SET\nWith all the objects in Python’s core types arsenal, some readers may be\npuzzled over the choice between lists and dictionaries. In short, although\nboth are flexible collections of other objects, lists assign items to positions,\nand dictionaries assign them to more mnemonic keys. Because of this,\ndictionary data often carries more meaning to human readers. For example, a\nnested list structure can always be used to record info:\n>>> L = ['Pat', 40.5, ['dev', 'mgr']]  # List-based \"record\"\n>>> L[0]\n'Pat'\n>>> L[1]                               # Positions/numbers for fields\n40.5\n>>> L[2][1]\n'mgr'\nFor some types of data, the list’s access-by-position makes sense—a list of\nemployees in a company, the files in a directory, or numeric matrixes, for\nexample. But a more symbolic record like this may be more meaningfully\ncoded as a dictionary, with labeled fields replacing field positions:\n>>> D = {'name': 'Pat', 'age': 40.5, 'jobs': ['dev', 'mgr']}\n>>> D['name']\n'Pat'\n>>> D['age']                           # Dictionary-based \"record\"\n40.5\n>>> D['jobs'][1]                       # Names mean more than numbers\n'mgr'\nFor variety, here is the same record recoded with dict and keywords, which\nmay seem even more readable to some human readers:\n>>> D = dict(name='Pat', age=40.5, jobs=['dev', 'mgr'])\n>>> D['name']\n'Pat'\n>>> D['jobs'].remove('mgr')\n>>> D",
      "content_length": 1429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": 8,
      "content": "{'name': 'Pat', 'age': 40.5, 'jobs': ['dev']}\nIn practice, dictionaries tend to be best for data with labeled components, as\nwell as structures that can benefit from quick, direct lookups by name,\ninstead of slower linear (left-to-right) searches. As we’ve seen, they also\nmay be better for sparse collections and collections that grow flexibly.\nPython programmers also have access to the sets we studied in Chapter 5,\nwhich are much like the keys of a valueless dictionary; they don’t map keys\nto values, but can often be used like dictionaries for fast lookups when there\nis no associated value, especially in search routines:\n>>> D = {}\n>>> D['state1'] = True                 # A visited-state dictionary\n>>> 'state1' in D\nTrue\n>>> S = set()\n>>> S.add('state1')                    # Same, but with sets\n>>> 'state1' in S\nTrue\nWatch for a rehash of this record representation thread in the next chapter,\nwhere you’ll see how tuples and named tuples compare to dictionaries in this\nrole, as well as in Chapter 27, where you’ll learn how user-defined classes\nfactor into this picture, combining both data and logic to process it.\n1  In practice, you won’t see many lists written out like this in list-processing programs. It’s more\ncommon to see code that processes lists constructed dynamically (at runtime), from user inputs, file\ncontents, and so on. In fact, although it’s important to master literal syntax, many data structures in\nPython are built by running program code at runtime.\n2  This description requires elaboration when the value and the slice being assigned overlap:\nL[2:5]=L[3:6], for instance, works fine because the value to be inserted is fetched before the\ndeletion happens on the left. Hence, slice assignment is really a fetch + delete + insert, but the fetch\npart matters too rarely to make the marquee.\n3  Unlike + concatenation, append doesn’t have to generate new objects, so it’s usually faster than +\ntoo. You can also mimic append with the clever slice assignments of the prior section: L[len(L):]=\n[X] is like L.append(X), and L[:0]=[X] is like appending at the front of a list. Both delete an empty\nslice and insert X, changing L in place quickly, like append. Both are arguably more complex than list\nmethods, though. For instance, L.insert(0, X) can also append an item to the front of a list, and",
      "content_length": 2332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": 8,
      "content": "seems noticeably more mnemonic. L.insert(len(L), X) inserts one object at the end too, but\nunless you like typing, you might as well use L.append(X)!\n4  As for lists, you might not see dictionaries coded in full using literals very often—programs rarely\nknow all their data before they are run, and more typically extract it dynamically from users, files,\nand so on. Lists and dictionaries are grown in different ways, though. In the next section you’ll see\nthat you often build up dictionaries by assigning to new keys at runtime; this approach fails for lists,\nwhich are commonly grown with append or extend instead.",
      "content_length": 618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": 8,
      "content": "Chapter 9. Tuples, Files, and\nEverything Else\nThis chapter rounds out our in-depth tour of the core object types in Python by\nexploring the tuple, a collection of other objects that cannot be changed, and the\nfile, an interface to external files on your computer. As you’ll see, the tuple is a\nrelatively simple object that largely performs operations you’ve already learned\nabout for strings and lists. The file object is a commonly used and full-featured\ntool for processing files on a host of devices. Because files are so pervasive in\nprogramming, the basic overview of files here is supplemented by larger\nexamples in later chapters.\nThis chapter also concludes this part of the book by summarizing properties\ncommon to all the core object types we’ve met—the notions of equality,\ncomparisons, object copies, and so on. We’ll also briefly explore other object\ntypes in Python’s toolbox, including the None placeholder and the namedtuple\nhybrid; as you’ll see, although we’ve covered all the primary built-in types, the\nobject story in Python is broader than implied thus far. Finally, we’ll close this\npart of the book by taking a look at a set of common object type pitfalls and\nexploring some exercises that will allow you to experiment with and cement the\nideas you’ve learned.\nOne logistics note up front: as for strings in Chapter 7, our exploration of files\nhere will be limited to fundamentals that most Python programmers—and\nespecially Python newcomers—need to know. In particular, Unicode text files\nwere previewed in Chapter 4, but we’re going to postpone full coverage of them\nuntil Chapter 37, as optional or deferred reading. For this chapter’s purpose,\nwe’ll assume that the contents of any text files will be encoded and decoded per\nyour platform’s default Unicode encoding (and you won’t yet need to know what\nthat means). The basics you’ll learn here, though, will apply both to the simpler\nfiles in this chapter as well as their extensions in Chapter 37.",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": 8,
      "content": "Tuples\nThe last collection type in our survey is the Python tuple. Tuples construct\nsimple groups of objects. They work much like lists, except that tuples can’t be\nchanged in place (they’re immutable) and are usually written as a series of items\nin parentheses, not square brackets. Although they don’t support as many\nmethods, tuples share most of their properties with lists. Here’s a quick look at\nthe basics. Tuples are:\nOrdered collections of arbitrary objects\nLike strings and lists, tuples are positionally ordered collections of objects\n(i.e., they maintain a left-to-right order among their contents). Like lists and\ndictionaries, they can embed any kind of object.\nAccessed by offset\nLike strings and lists, items in a tuple are accessed by offset (not by key);\nthey support all the offset-based access operations, such as indexing and\nslicing.\nOf the category “immutable sequence”\nLike strings and lists, tuples are sequences; they support many of the same\noperations. However, like strings, tuples are immutable; they don’t support\nany of the in-place change operations applied to lists.\nFixed-length, heterogeneous, and arbitrarily nestable\nBecause tuples are immutable, you cannot change the size of a tuple without\nmaking a copy. On the other hand, tuples may contain any type of object,\nincluding other collection objects (e.g., lists, dictionaries, and other tuples),\nand so support arbitrary nesting.\nArrays of object references\nLike lists, tuples are best thought of as object reference arrays: tuples store",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": 8,
      "content": "access points to other objects (references), and indexing a tuple is relatively\nquick.\nTable 9-1 highlights common tuple operations. In it, T means a tuple. As shown,\na tuple is written as a series of objects (technically, expressions that generate\nobjects), separated by commas and normally enclosed in parentheses. An empty\ntuple is just a parentheses pair with nothing inside.\nTable 9-1. Common tuple literals and operations\nOperation\nInterpretation\n()\nAn empty tuple\nT = (0,)\nA one-item tuple (not an expression)\nT = (0, 'Py', 1.2, 3)\nA four-item tuple\nT = 0, 'Py', 1.2, 3\nAnother four-item tuple (same as prior\nline)\nT = ('Pat', ('dev', 'mgr'))\nNested tuples\nT = tuple('hack')\nTuple of items in an iterable\nT[i]\nT[i][j]\nT[i:j]\nlen(T)\nIndex, index of index, slice, length\nT1 + T2\nT * 3\nConcatenate, repeat\nT1 > T2, T1 == T2\nComparisons: magnitude, equality\n'code' in T \nfor x in T: print(x)\n[x ** 2 for x in T]\nMembership, iteration\nT = (*x, 0, *y, *x)",
      "content_length": 956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": 8,
      "content": "Iterable unpacking\nT.index('Py')\nT.count('Py')\nMethods: search, count\nnamedtuple('Emp', ['name', 'jobs']\n)\nNamed-tuple extension type\nTuples in Action\nAs usual, let’s start an interactive session to explore tuples at work. Notice in\nTable 9-1 that tuples do not have most of the methods that lists have (e.g., an\nappend call won’t work here). They do, however, support the usual sequence\noperations that we explored for both strings and lists, compare recursively as\nusual, and support the same * iterable-unpacking syntax in their literals that we\nused for lists in the preceding chapter:\n$ python3                      # Start your REPL\n>>> (1, 2) + (3, 4)            # Concatenation\n(1, 2, 3, 4)\n>>> (1, 2) * 4                 # Repetition\n(1, 2, 1, 2, 1, 2, 1, 2)\n>>> T = (1, 2, 3, 4)\n>>> T[0], T[1:3]               # Indexing, slicing\n(1, (2, 3))\n>>> T == (1, 2, 3, 4), T > (1, 2, 3, 3), T > (1, 2, 3)\n(True, True, True)\n>>> L = ['code', 'hack']\n>>> (*L, 1, 2, *(3, 4))        # Iterable unpacking\n('code', 'hack', 1, 2, 3, 4)\nTuple syntax peculiarities: Commas and parentheses\nThe second and fourth entries in Table 9-1 merit a bit more explanation. Because\nparentheses can also enclose expressions (see Chapter 5), you need to do\nsomething special to tell Python when a single object in parentheses is a tuple\nobject and not a simple expression. If you really want a single-item tuple, simply\nadd a trailing comma after the single item, before the closing parenthesis:",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": 8,
      "content": ">>> x = (40)                   # An integer!\n>>> x\n40\n>>> y = (40,)                  # A tuple containing an integer\n>>> y\n(40,)\nAs a special case, Python also allows you to omit the opening and closing\nparentheses for a tuple in contexts where it isn’t syntactically ambiguous to do\nso. For instance, the fourth line of Table 9-1 simply lists four items separated by\ncommas. In the context of an assignment statement, Python recognizes this as a\ntuple, even though it doesn’t have parentheses. That’s why all the comma-\nseparated items we’ve typed at the REPL print with parentheses—it’s a tuple:\n>>> 1, 2, 3, 4                 # Tuple sans parentheses, in REPL and elsewhere\n(1, 2, 3, 4)\nThis syntactic trick is also commonly leveraged by the sequence assignment\nshorthand we used briefly in Chapter 7 and will study in earnest in Chapter 11—\nnames on the left are paired with values on the right and assigned by position,\nbut both sides are really tuples without parentheses:\n>>> a, b, c = 1, 2, 3          # Sequence assignment: tuples on both sides\n>>> a, b, c\n(1, 2, 3)\nNow, some people will tell you to always use parentheses in your tuples, and\nsome will tell you to never use parentheses in tuples (and still others have lives\nand won’t tell you what to do with your tuples!). The most common places\nwhere the parentheses are required for tuple literals are those where:\nParentheses matter—within a function call, or nested in a larger\nexpression\nCommas matter—within a function call, or embedded in the literal of a\nlarger object like a list or dictionary\nIn most other contexts, the enclosing parentheses are optional. For beginners, the\nbest advice is that it’s probably easier to use the parentheses than it is to\nremember when they are optional or required. Many programmers also find that",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": 8,
      "content": "parentheses tend to aid script readability by making the tuples more explicit and\nobvious.\nAnd for language lawyers in the audience, bear in mind that the comma is really\na sort of lowest precedence operator, though only in contexts where it’s not\notherwise significant. In such contexts, it’s the comma that builds tuples, not the\nparentheses. This makes the latter optional, but can also lead to odd, unexpected\nsyntax errors if parentheses are omitted (e.g., in lambda covered in Part IV).\nAdding parentheses to your tuples as a habit avoids the oddities.\nConversions, methods, and immutability\nApart from literal-syntax differences, tuple operations (the middle rows in\nTable 9-1) are identical to string and list operations. The only differences worth\nnoting are that the +, *, and slicing operations return new tuples when applied to\ntuples, and that tuples don’t provide the same methods you saw for strings, lists,\nand dictionaries. If you want to sort a tuple, for example, you’ll usually have to\neither first convert it to a list to gain access to a sorting method call and make it a\nmutable object, or use the newer sorted built-in that accepts any sequence\nobject (and other iterables—a term introduced in Chapter 4 that we’ll be more\nformal about in the next part of this book):\n>>> T = ('cc', 'aa', 'dd', 'bb')\n>>> tmp = list(T)                  # Make a list from a tuple's items\n>>> tmp.sort()                     # Sort the list\n>>> tmp\n['aa', 'bb', 'cc', 'dd']\n>>> T = tuple(tmp)                 # Make a tuple from the list's items\n>>> T\n('aa', 'bb', 'cc', 'dd')\n>>> sorted(T, reverse=True)        # Or use the sorted built-in, and save steps\n['dd', 'cc', 'bb', 'aa']\nHere, the list and tuple built-in functions are used to convert the object to a\nlist and then back to a tuple. Really, both calls make new objects from any sort\nof iterables, but the net effect is like a conversion.\nIn some sense, list comprehensions can also be used to convert tuples. The\nfollowing, for example, makes a list from a tuple, adding 20 to each item along",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": 8,
      "content": "the way:\n>>> T = (1, 2, 3, 4, 5)\n>>> L = [x + 20 for x in T]        # Like list(T) + expression logic\n>>> L\n[21, 22, 23, 24, 25]\nList comprehensions are really sequence operations—they always build new\nlists, but they may be used to iterate over any sequence objects, including tuples,\nstrings, and other lists. As you’ll see later in the book, they even work on some\nthings that are not physically stored sequences—any iterable objects will do,\nincluding files, which are automatically read line by line. Given this, they may\nbe better called iteration tools.\nNotice that you’d have to convert the prior example’s list result back to a tuple if\nyour code must care. There is no tuple comprehension in Python (as explored\nlater in this book, parenthesized comprehensions make generators), though you\nsimulate one by using tuple to force a generator to give up its values:\n>>> tuple(x + 20 for x in T)       # Tuple \"comprehension\" = generator + builder\n(21, 22, 23, 24, 25)\nAlthough tuples don’t have the same methods as lists and strings, they do have\ntwo of their own—index and count work as they do for lists, but they are\ndefined for tuple objects:\n>>> T = (1, 2, 3, 2, 4, 2)         # Tuple methods\n>>> T.index(2)                     # Offset of first appearance of 2: index _of_!\n1\n>>> T.index(2, 2)                  # Offset of appearance after offset 2\n3\n>>> T.count(2)                     # How many 2s are there?\n3\nAlso, note that the rule about tuple immutability applies only to the top level of\nthe tuple itself, not to its contents. A list inside a tuple, for instance, can be\nchanged as usual:\n>>> T = (1, [2, 3], 4)\n>>> T[1] = 'mod'                   # This fails: can't change tuple itself",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": 8,
      "content": "TypeError: 'tuple' object does not support item assignment\n>>> T[1][0] = 'mod'                # This works: can change mutables inside\n>>> T\n(1, ['mod', 3], 4)\nFor most programs, this one-level-deep immutability is sufficient for common\ntuple roles. Which, coincidentally, brings us to the next section.\nWhy Lists and Tuples?\nThis seems to always be the first question that comes up when teaching\nbeginners about tuples: why do we need tuples if we have lists? Some of the\nreason is philosophical: a tuple is meant to be a simple association of objects,\nwhile a list is intended to be a data structure that changes over time. In fact, this\nmeaning of “tuple” derives from mathematics, as well its frequent use for a row\nin a relational database table.\nThe best answer, however, seems to be that the immutability of tuples provides\nsome integrity—you can be sure a tuple won’t be changed through another\nreference elsewhere in a program, but there’s no such guarantee for lists. Tuples\nand other immutables, therefore, serve a similar role to “constant” declarations\nin other languages, though the notion of constantness is associated with objects\nin Python, not variables.\nTuples can also be used in places that lists cannot—for example, as dictionary\nkeys (see the sparse matrix example in Chapter 8). Some built-in operations may\nalso require or imply tuples instead of lists (e.g., the substitution values in the %\nstring formatting expression of Chapter 7), though some operations have often\nbeen generalized to be more flexible. As a rule of thumb, lists are the tool of\nchoice for ordered collections that might need to change; tuples can handle the\nother cases of fixed associations.\nRecords Revisited: Named Tuples\nIn fact, the choice of data types is even richer than the prior section may have\nimplied—Python programmers can choose from an assortment of both built-in\ncore types, and extension types built on top of them. For example, in the prior\nchapter’s sidebar “Why You Will Care: List Versus Dictionary Versus Set”, we",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": 8,
      "content": "saw how to represent record-like information with both a list and a dictionary\nand noted that dictionaries offer the advantage of more mnemonic keys that label\ndata. As long as we don’t require mutability, tuples can serve similar roles, with\npositions for record fields like lists:\n>>> pat = ('Pat', 40.5, ['dev', 'mgr'])                    # Tuple record\n>>> pat\n('Pat', 40.5, ['dev', 'mgr']) \n>>> pat[0], pat[2]                                         # Access by position\n('Pat', ['dev', 'mgr'])\nAs for lists, though, field numbers in tuples generally carry less information than\nthe names of keys in a dictionary. To review, here’s the same record recoded as a\ndictionary with named fields:\n>>> pat = dict(name='Pat', age=40.5, jobs=['dev', 'mgr'])  # Dictionary record\n>>> pat['name'], pat['jobs']                               # Access by key\n('Pat', ['dev', 'mgr'])\nIn fact, we can convert parts of the dictionary to tuples if needed:\n>>> tuple(pat.values())                                    # Values to tuple\n('Pat', 40.5, ['dev', 'mgr']) \n>>> list(pat.items())                                      # Items to tuple list\n[('name', 'Pat'), ('age', 40.5), ('jobs', ['dev', 'mgr'])]\nBut really, this is a false dichotomy: with extra code, we can implement objects\nthat offer both positional and named access to record fields. For example, the\nnamedtuple utility, noncore but always available in the standard library’s\ncollections module, implements an extension type that adds logic to tuples\nthat allows components to be accessed by both position and attribute name, and\ncan be converted to dictionary-like form for access by key if desired. Attribute\nnames come from classes and are not exactly dictionary keys, but they are\nsimilarly mnemonic:\n>>> from collections import namedtuple                     # Import extension type\n>>> Rec = namedtuple('Rec', ['name', 'age', 'jobs'])       # Make a generated class\n>>> pat = Rec('Pat', age=40.5, jobs=['dev', 'mgr'])        # A named-tuple record\n>>> pat",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": 8,
      "content": "Rec(name='Pat', age=40.5, jobs=['dev', 'mgr'])\n>>> pat[0], pat[2]                                         # Access by position\n('Pat', ['dev', 'mgr']) \n>>> pat.name, pat.jobs                                     # Access by attribute\n('Pat', ['dev', 'mgr'])\nConverting to a dictionary also supports key-based behavior when needed:\n>>> D = pat._asdict()                                      # Dictionary-like form\n>>> D['name'], D['jobs']                                   # Access by key too\n(('Pat', ['dev', 'mgr'])\n>>> D\n{'name': 'Pat', 'age': 40.5, 'jobs': ['dev', 'mgr']}\nAs you can see, named tuples are a tuple/class/dictionary hybrid. They also\nrepresent a classic trade-off. In exchange for their extra utility, they require extra\ncode to use (the two startup lines in the preceding examples that import the type\nand make the class) and incur some performance costs to work this magic. Still,\nthey are an example of the kind of custom data types that we can build on top of\nbuilt-in types like tuples when extra utility is desired. They are also extensions,\nnot core types—they live in the standard library and fall into the same category\nas Chapter 5’s Fraction and Decimal—so we’ll delegate to the Python library\nmanual for more details.\nWatch for a final rehash of this record representation thread when we explore\nhow user-defined classes compare in Chapter 27. As you’ll find there, classes\nlabel fields with names too, but can also provide program logic to process the\nrecord’s data in the same code package.\nFiles\nYou may already be familiar with the notion of files, which are named storage\ncompartments on your PC, phone, or other computer that are managed by your\noperating system. The last major built-in object type that we’ll examine on our\nobject-types tour provides a way to access those files inside Python programs.\nIn short, the built-in open function creates a Python file object, which serves as a\nlink to a file residing on your device. After calling open, you can transfer strings",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": 8,
      "content": "of data to and from the associated external file by calling the returned file\nobject’s methods.\nCompared to the types you’ve seen so far, file objects are outliers. They are\nconsidered a core type because they are created by a built-in function, but\nthey’re not numbers, sequences, or mappings, and they don’t respond to\nexpression operators; they export only methods for common file-processing\ntasks. Most file methods are concerned with performing input from and output to\nthe external file associated with a file object, but other file methods allow us to\nseek to a new position in the file, flush output buffers, and so on. Table 9-2\nsummarizes common file operations.\nTable 9-2. Common file operations\nOperation\nInterpretation\noutput = open(r'C:\\data', 'w')\nCreate output file (Windows path, 'w' = write)\ninput = open('/home/me/data', '\nr')\nCreate input file (Unix path, 'r'= read)\ninput = open('data')\nCreate input file (current directory, 'r' is\ndefault)\naString = input.read()\nRead entire file into a single string\naString = input.read(N)\nRead up to next N characters (or bytes) into a\nstring\naString = input.readline()\nRead next line (including \\n newline) into a\nstring\naList = input.readlines()\nRead entire file into a list of line strings (with \n\\n)\noutput.write(aString)\nWrite a string of characters (or bytes) into a\nfile\noutput.writelines(aList)\nWrite all line strings in a list into a file",
      "content_length": 1405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": 8,
      "content": "(verbatim)\noutput.close()\nManual close (done for you when file is\ncollected)\noutput.flush()\nFlush output buffer to disk without closing\nanyFile.seek(N)\nChange file position to offset N for next\noperation\nfor line in open('data'): use li\nne\nFile iterators read line by line\nopen('f.txt', encoding='utf-8')\nUnicode text files (using str strings)\nopen('f.bin', 'rb')\nBytes files (using bytes strings)\ncodecs.open('f.txt',…)\nAlternative Unicode text-file interface\nOpening Files\nTo open a file, a program calls the built-in open function, with the external\nfilename first, followed by a processing mode. Both arguments are strings. The\ncall returns a file object, which in turn has methods for data transfer:\nafile = open(filename, mode)\nafile.method()\nThe first argument to open, the external filename, may include a platform-\nspecific and absolute (complete) or relative (partial) directory-path prefix that\nidentifies a file’s location in the host device’s filesystem. The filesystem is just a\nhierarchy of folders that stores files and nested folders. A filename a/b/file.txt,\nfor example, includes the path prefix a/b that leads to a file on Unix (e.g.,\nmacOS, Linux, or Android), and a\\b\\file.txt does the same on Windows.\nIf filename has no directory-path prefix at all, the file is assumed to exist in,\nand hence is relative to, the current working directory (CWD). The CWD is the\nfolder from which a script is run (e.g., where you are in a console when you\nlaunch a Python command). In a REPL, the CWD is wherever you are working",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": 8,
      "content": "at the time; to see what the CWD is, run a pwd in most system shells, or Python’s\nos.getcwd() after import os in a REPL.\nAs you’ll see in Chapter 37’s expanded file coverage, the filename may also\ncontain non-ASCII Unicode characters that Python automatically translates to\nand from the underlying host’s encoding. These characters may be provided in\nthe filename literally, or in a pre-encoded byte string that you’ll learn about later\nin this book.\nThe second argument to open, processing mode, is typically the string 'r' to\nopen for text input (the default), 'w' to create and open for text output, or 'a' to\nopen for appending text to the end (e.g., for adding to logfiles). The processing\nmode argument can specify additional options:\nAdding a b to the mode string (e.g., 'wb') allows for processing binary\nfile content. End-of-line translations and Unicode encodings used for\ntext are turned off.\nAdding a + to the mode (e.g., 'r+') opens the file for both input and\noutput. You can read and write to the same file object, often in\nconjunction with seek operations to reposition in the file.\nBoth of the first two arguments to open must be Python strings. An optional\nthird argument takes an integer to control output buffering—passing a zero\nmeans that output is unbuffered (it’s transferred to the external file immediately\non a write method call), and additional arguments may be provided for special\ntypes of files (e.g., the string name of a Unicode encoding for text files). You can\nalso use name=value keywords to pass open arguments (e.g., file=name,\nmode=mode), though this is somewhat above our pay grade in this chapter.\nWe’ll cover file fundamentals and explore some basic examples here, but we\nwon’t go into all file-processing mode options; run help(open) in a REPL or\nconsult the Python library manual for additional details.\nUsing Files\nOnce you make a file object with open, you can call its methods to read from or\nwrite to the associated external file. In all cases, file content takes the form of",
      "content_length": 2023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": 8,
      "content": "strings in Python programs; reading a file returns its content in strings, and\ncontent is passed to the write methods as strings. Reading and writing methods\ncome in multiple flavors; Table 9-2 lists the most common. Here are a few points\nof orientation up front:\nFile iterators may be best for reading text lines\nThough the reading and writing methods in the table are common, keep in\nmind that probably the best way to read lines from a text file today is to not\nread the file at all—as you’ll see in Chapter 14, files also have an iterator\nthat automatically reads one line at a time in a for loop, list comprehension,\nor other iteration context.\nContent is strings, not objects\nNotice in Table 9-2 that content read from a file always comes back to your\nscript as a string, so you’ll have to convert it to a different type of object if a\nstring is not what you need. Similarly, file write operations do not add any\nsort of formatting and do not convert objects to strings automatically, so you\nmust convert if needed and format as desired. Because of this, the tools we\nhave already met to convert objects from and to strings (e.g., int, float,\nstr, and string formatting) come in handy when dealing with files. Also note\nthat newlines, added by print but not file writes, may have to be skipped if\ntext from file reads is sent to print to avoid double spacing.\nPython also includes advanced standard-library tools for handling generic\nobject storage (the pickle module), for dealing with packed binary data in\nfiles (the struct module), and for processing special types of content such\nas JSON and CSV text. We’ll demo these later in this chapter, but Python’s\nmanuals document them in full.\nFiles are buffered and seekable\nBy default, output files are always buffered, which means that text you write\nmay not be transferred from memory to disk immediately—closing a file, or\nrunning its flush method, forces the buffered data to disk. You can avoid",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": 8,
      "content": "buffering with extra open arguments, but it may impede performance. Python\nfiles are also random-access on a byte-offset basis—their seek method\nallows your scripts to jump around to read and write at specific locations.\nclose may be optional: auto-close on collection\nCalling the file close method terminates your connection to the external file,\nreleases its system resources, and flushes its buffered output to disk if any is\nstill in memory. As discussed in Chapter 6, an object’s memory space is\nautomatically reclaimed as soon as the object is no longer referenced\nanywhere in the program. When file objects are reclaimed, Python also\nautomatically closes them if they are still open (this also happens to open\nfiles when a program shuts down). This means you don’t always need to\nmanually close your files in Python, especially those in simple scripts with\nshort runtimes, and temporary files used by a single line or expression.\nOn the other hand, manual close calls don’t hurt and are a good habit to\nform, especially in long-running systems and code run at the REPL. Strictly\nspeaking, this auto-close of files is an implementation artifact of the standard\nCPython, and not part of the language definition—it may change over time,\nmay not happen when you expect it to in interactive REPLs, and may not\nwork the same in Python implementations whose garbage collectors reclaim\nspace differently than CPython. In fact, when many files are opened within\nloops, some Pythons may require close calls to free up system resources\nimmediately, before garbage collection can get around to freeing objects.\nClose calls may sometimes also be required to flush buffered output of file\nobjects not yet reclaimed. For an alternative and automatic way to ensure\ncloses, watch for the file object’s context manager ahead.\nFiles in Action\nLet’s work through an example that demonstrates file-processing basics. The\nfollowing code begins by opening a new text file for output, writing two lines\n(strings terminated with a newline marker, \\n), and closing the file. Later, the\nexample opens the same file again in input mode and reads the lines back one at\na time with readline:",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": 8,
      "content": ">>> myfile = open('myfile.txt', 'w')        # Open for text output: create/empty\n>>> myfile.write('hello text file\\n')       # Write a line of text: string\n16\n>>> myfile.write('goodbye text file\\n')\n18\n>>> myfile.close()                          # Ensure output is flushed to disk\n>>> myfile = open('myfile.txt')             # Open for text input: 'r' is default\n>>> myfile.readline()                       # Read the lines back\n'hello text file\\n'\n>>> myfile.readline()\n'goodbye text file\\n'\n>>> myfile.readline()                       # Empty string: end-of-file\n''\nNotice that the third readline call returns an empty string—this is how most\nfile read methods tell you that you’ve reached the end of the file (and as you’ll\nsee ahead, the empty string is inherently false in logical tests). Empty lines in the\nfile instead come back as strings containing just a newline character ('\\n'), not\nas empty strings.\nAlso notice that file write calls return the number of characters written; this is\nnormally superfluous apart from error checks but is echoed in a REPL like this.\nThis example writes each line of text, including its newline terminator, \\n, as a\nstring. Write methods don’t add the newline character for us, so we must include\nit to properly terminate our lines; without this, the next write will simply extend\nthe current line in the file.\nIf you want to display the file’s content with newline characters interpreted, read\nthe entire file into a string all at once with the file object’s read method and\nprint it (stringing together the open and read like this runs left to right and\ndoesn’t allow for an explicit close, but it doesn’t matter for simple input at the\nREPL):\n>>> open('myfile.txt').read()               # Read all at once into string\n'hello text file\\ngoodbye text file\\n'\n>>> print(open('myfile.txt').read())        # User-friendly display\nhello text file\ngoodbye text file\nAnd if you want to scan a text file line by line, file iterators are often your best",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": 8,
      "content": "option:\n>>> for line in open('myfile.txt'):         # Use file iterators, not reads\n...     print(line, end='')                 # Don't add another \\n: line has one!\n...\nhello text file\ngoodbye text file\nWhen coded this way, the temporary file object created by open will\nautomatically read and return one line on each loop iteration. This form is\nusually easiest to code, light on memory use, and may be faster than some other\noptions (depending on many variables, of course). Since we haven’t reached\nstatements or iterators yet, though, you’ll have to wait until Chapter 14 for a\nmore complete explanation of this code.\nAs noted, files content is always strings, so other kinds of objects must be\nconverted for write. The str call and concatenating separators and newlines\nsuffice and emulate what print does automatically (but if you like the\nsugarcoating provided by print, stay tuned for its coverage in the next part of\nthis book, where you’ll learn how to route its display to a file you make first\nwith open):\n>>> myfile = open('myfile2.txt', 'w')       # Nonstring objects fail\n>>> myfile.write(3.14)\nTypeError: write() argument must be str, not float\n>>> myfile.write(str(3.14) + '\\n')          # Convert (and emulate print)\n5\n>>> myfile.close()\n>>> open('myfile2.txt').read()              # Can reconvert with float()\n'3.14\\n'\nIncidentally, the files we’ve made here show up in the CWD, because we didn’t\nprovide a path prefix in their filenames. In Python, you can check what the CWD\nis and get a listing of the files there, with the os standard-library module:\n>>> import os\n>>> os.getcwd()                   # Show the current working directory\n'/Users/me/code/Chapter09'\n>>> os.listdir()                  # List files here (or in a passed path)\n['myfile2.txt', 'myfile.txt']",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": 8,
      "content": "This was run on macOS and mirrors shell commands, but such tools are useful in\nmany programs. The takeaway is that filename myfile.txt in this CWD is\nequivalent to /Users/me/code/Chapter09/myfile.txt in open and other tools.\nNOTE\nCoding Windows paths: If you opt to provide full directory paths for files on Windows, they\nmay require special handling because the Windows \\ path separator is also used for string\nescapes in Python per Chapter 7. As noted there, open accepts Unix-style forward slashes in\nplace of backward slashes on Windows, so any of the following forms work for directory paths\non Windows:\nopen(r'C:\\Users\\me\\code\\newata.txt')        # Raw strings\nopen('C:/Users/me/code/newdata.txt')        # Forward slashes\nopen(‘C:\\\\Users\\\\me\\\\code\\\\newdata.txt')    # Doubled-up escapes\nThe raw string form in the first command is useful to turn off unintended escapes (e.g., \\n),\nthough the other two options make the escapes issue moot. On Unix, you’ll simply use forward\nslashes, of course (and drop the Windows drive letter: your drives are mounted, not\nsegregated).\nText and Binary Files: The Short Story\nStrictly speaking, the examples in the prior section use text files. More generally,\nfile type is determined by the second argument to open, the mode string—\nincluding a “b” in it means binary, which is sharply distinguished from text:\nText files represent content as a normal str string, perform Unicode\nencoding and decoding automatically, and perform newline translation\nby default. This mode is useful for processing text of all kinds.\nBinary files represent content as a special bytes string and allow\nprograms to access file content unaltered. This mode is useful for\nprocessing nontext content like media.\nPrograms that deal only with simple text like ASCII can get by with the basic\ntext-file interface used in the prior examples, and normal strings. All text strings\nare technically Unicode in Python, but ASCII users will not generally notice\nbecause it’s a subset of Unicode (every ASCII file is a Unicode file, even if its",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": 8,
      "content": "character range is limited).\nIf you need to handle non-ASCII text or byte-oriented data, though, you’ll need\nto match object types to file modes—bytes strings for binary files, and normal\nstr strings for text files. Because text files implement Unicode encodings, you\nalso should not open a binary data file in text mode: decoding its content to\nUnicode text will likely fail.\nLet’s turn to a brief example. When you write and read a binary file, you send\nand receive a bytes object—a sequence of small integers that represent absolute\nbyte values (which may or may not correspond to characters), and which is\ncoded with a leading b but looks and feels almost exactly like a normal text\nstring:\n>>> myfile = open('myfile3.bin', 'wb')        # Make binary file: wb=write binary\n>>> myfile.write(b'\\x00\\x01hack\\x02\\x03').    # Bytes string holds binary data\n8\n>>> myfile.close()\n>>> data = open('myfile3.bin', 'rb').read()   # Read binary file: rb=read binary\n>>> data                                      # Raw, unaltered bytes returned\nb'\\x00\\x01hack\\x02\\x03'\n>>> data[2:6]                                 # Bytes act like text strings\nb'hack'\n>>> byte = data[2:6][0]                       # But really small 8-bit integers\n>>> byte, chr(byte), bin(byte)\n(104, 'h', '0b1101000')\nIn addition, binary files do not perform any newline translation on content, but\ntext files by default map all forms to and from \\n when read and written. Text\nfiles also implement Unicode encodings on transfers, using an optional encoding\nname passed to the encoding argument of open. If encoding is not passed (as in\nearlier examples), files fall back on the underlying platform’s default, which may\nnot be interoperable with other hosts or files.\nPer the start of this chapter, though, that’s as much as we’re going to say about\nUnicode text and binary data files here, and just enough to understand upcoming\nexamples in this chapter. If you’re anxious to dive into this topic further, see\neither the preview in Chapter 4 or wait for the full story in Chapter 37.\nFor this chapter, let’s move on to a handful of more substantial file examples that",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": 8,
      "content": "demonstrate common ways to store Python object values in files.\nStoring Objects with Conversions\nOur next example writes a variety of Python objects to a text file on multiple\nlines. We wrote a single number to a file earlier, but are kicking it up a notch\nhere to demo more conversions. Again, file content is strings in our code, and\nwrite methods do not do any to-string formatting (for space, this chapter omits\nwrite return values from here on):\n>>> X, Y, Z = 62, 63, 64                       # Native Python objects\n>>> S = 'Text'                                 # Must be strings to store in file\n>>> D = {'a': 1, 'b': 2}\n>>> L = [1, 2, 3]\n>>> F = open('datafile.txt', 'w')              # Create output text file\n>>> F.write(S + '\\n')                          # Terminate lines with \\n\n>>> F.write(f'{X},{Y},{Z}\\n')                  # Convert numbers to strings\n>>> F.write(str(L) + '$' + str(D) + '\\n')      # Convert and separate with $\n>>> F.close()\nOnce we have created our file, we can inspect its contents by opening it and\nreading it into a string (strung together as a single operation here). Notice that\nthe interactive echo gives the exact character contents, while the print operation\ninterprets embedded newline characters to render a more user-friendly display:\n>>> chars = open('datafile.txt').read()        # Raw string display\n>>> chars\n\"Text\\n62,63,64\\n[1, 2, 3]${'a': 1, 'b': 2}\\n\"\n>>> print(chars)                               # User-friendly display\nText\n62,63,64\n[1, 2, 3]${'a': 1, 'b': 2}\nWe now have to use other conversion tools to translate from the strings in the\ntext file to real Python objects. As Python never converts strings to numbers (or\nother types of objects) automatically, this is required if we need to gain access to\nnormal object tools like indexing, addition, and so on:\n>>> F = open('datafile.txt')                   # Open again\n>>> line = F.readline()                        # Read one line",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": 8,
      "content": ">>> line\n'Text\\n' \n>>> line.rstrip()                              # Remove newline\n'Text'\nFor this first line, we used the string rstrip method to get rid of the trailing\nnewline character; a line[:−1] slice would work, too, but only if we can be\nsure all lines end in the \\n character (the last line in a file sometimes does not).\nSo far, we’ve read the line containing the string. Now let’s grab the next line,\nwhich contains numbers, and parse out (that is, extract) the objects on that line:\n>>> line = F.readline()                       # Next line from file\n>>> line                                      # It's a string here\n'62,63,64\\n' \n>>> parts = line.rstrip().split(',')          # Split (parse) on commas\n>>> parts\n['62', '63', '64']\nWe used the string split method here to chop up the line on its comma\ndelimiters (after removing the trailing \\n with rstrip as before—its result is a\nnew string on which we run split). The result is a list of substrings containing\nthe individual numbers. We still must convert from strings to integers, though, if\nwe wish to perform math on these:\n>>> int(parts[1])                              # Convert from string to int\n63\n>>> numbers = [int(P) for P in parts]          # Convert all in list at once\n>>> numbers\n[62, 63, 64]\nAs we have learned, int translates a string of digits into an integer object, and\nthe list comprehension expression introduced in Chapters 4 and 8 can apply the\ncall to each item in our list all at once (again, you’ll find more on list\ncomprehensions later in this book). Nit: we didn’t have to use rstrip to delete\nthe \\n at the end of the line, because int and some other converters quietly\nignore whitespace around digits; still, being explicit is often best.\nFinally, to convert the stored list and dictionary in the third line of the file, we\ncan run them through eval, a built-in function we first met in Chapter 5, that",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": 8,
      "content": "treats a string as a piece of executable program code (technically, a string\ncontaining a Python expression, with trade-offs discussed in the next section):\n>>> line = F.readline()                        # Next line from file\n>>> line\n\"[1, 2, 3]${'a': 1, 'b': 2}\\n\"\n>>> parts = line.split('$')                    # Split (parse) on $\n>>> parts\n['[1, 2, 3]', \"{'a': 1, 'b': 2}\\n\"]\n>>> eval(parts[0])                             # Convert to any object type\n[1, 2, 3]\n>>> objects = [eval(P) for P in parts]         # Do same for all in list\n>>> objects\n[[1, 2, 3], {'a': 1, 'b': 2}]\nBecause the end result of all this parsing and converting is a list of normal\nPython objects instead of strings, we can now apply list and dictionary\noperations to them in our script.\nStoring Objects with pickle\nUsing eval to convert from strings to objects, as demonstrated in the preceding\ncode, is a powerful tool. In fact, sometimes it’s too powerful. eval will happily\nrun any Python expression—even one that might delete all the files on your\ncomputer, given the necessary permissions. If you really want to store native\nPython objects, but you don’t want to run file content as program code, Python’s\nstandard-library pickle module can help.\nThe pickle module is a more advanced tool that allows us to store almost any\nPython object in a file directly, with no to- or from-string conversion\nrequirement on our part. It’s like a super-general data formatting and parsing\nutility. To store a dictionary in a file, for instance, we pickle it directly after\nusing binary mode to open the file:\n>>> D = {'a': 1, 'b': 2}\n>>> F = open('datafile.pkl', 'wb')\n>>> import pickle\n>>> pickle.dump(D, F)                          # Pickle almost any object to file\n>>> F.close()\nThen, to get the dictionary back later, we simply use pickle again to re-create it,",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": 8,
      "content": "again using a binary-mode file:\n>>> F = open('datafile.pkl', 'rb')\n>>> E = pickle.load(F)                         # Load almost any object from file\n>>> E\n{'a': 1, 'b': 2}\nWe get back an equivalent dictionary object, with no manual splitting or\nconverting required. The pickle module performs what is known as object\nserialization—converting objects to and from strings of bytes—but requires very\nlittle work on our part. In fact, pickle internally translates our dictionary to a\nbytes-string form; it’s not much to look at, and may vary for protocol options\nand future morph, but mandates binary-mode files:\n>>> open('datafile.pkl', 'rb').read()          # Format is prone to change!\nb'\\x80\\x04\\x95\\x11\\x00\\x00\\x00\\x00\\ …etc… \\x8c\\x01a\\x94K\\x01\\x8c\\x01b\\x94K\\x02u.'\nBecause pickle can reconstruct the object from this format, we don’t have to\ndeal with it ourselves. For more on the pickle module, see the Python standard-\nlibrary manual, or import pickle and pass it to help interactively. While you’re\nexploring, also take a look at the shelve module. shelve is a tool that uses\npickle to store Python objects in an access-by-key filesystem, which is beyond\nour scope here (though you will get to see an example of shelve in action in\nChapter 28, and other pickle examples in Chapters 31 and 37).\nStoring Objects with JSON\nThe prior section’s pickle module translates nearly arbitrary Python objects to a\nproprietary format developed specifically for Python, and honed for performance\nover many years. JSON is a newer data interchange format, which is both\nprogramming-language-neutral and supported by a variety of systems.\nMongoDB, for instance, stores data in a JSON document database (using a\nbinary JSON format), and JSON is common in configuration roles.\nJSON does not support as broad a range of Python object types as pickle, but\nits portability is an advantage in some contexts, and it represents another way to\nserialize a specific category of Python objects for storage and transmission.",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": 8,
      "content": "Moreover, because JSON is so close to Python dictionaries and lists in syntax,\nthe translation to and from Python objects is trivial, and is automated by the\njson standard-library module.\nFor example, a Python dictionary with nested structures is very similar to JSON\ndata, though Python’s variables and expressions support richer structuring\noptions (any part of the following can be an arbitrary expression in Python):\n>>> who = dict(first='Pat', last='Smith')\n>>> rec = dict(name=who, job=['dev', 'mgr'], age=40.5)\n>>> rec\n{'name': {'first': 'Pat', 'last': 'Smith'}, 'job': ['dev', 'mgr'], 'age': 40.5}\nThe final dictionary format displayed here is a valid literal in Python code, and\nalmost passes for JSON when printed as is, but the json module makes the\ntranslation official—here translating Python objects to and from a JSON\nserialized string representation in memory:\n>>> import json\n>>> json.dumps(rec)\n'{\"name\": {\"first\": \"Pat\", \"last\": \"Smith\"}, \"job\": [\"dev\", \"mgr\"], \"age\": 40.5}'\n>>> S = json.dumps(rec)       # Python => JSON\n>>> O = json.loads(S)         # JSON => Python\n>>> O\n{'name': {'first': 'Pat', 'last': 'Smith'}, 'job': ['dev', 'mgr'], 'age': 40.5}\n>>> O == rec\nTrue\nIt’s similarly straightforward to translate Python objects to and from JSON data\nstrings in files. Prior to being stored in a file, your data is simply Python objects;\nthe JSON module re-creates them from the JSON textual representation when it\nloads it from the file. In both cases, we use text files, because JSON is text:\n>>> file = open('testjson.txt', 'w')\n>>> json.dump(rec, fp=file, indent=4)         # Python => JSON\n>>> file.close()\n>>> print(open('testjson.txt').read())        # Human-readable file content\n{\n    \"name\": {\n        \"first\": \"Pat\",\n        \"last\": \"Smith\"",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": 8,
      "content": "},\n    \"job\": [\n        \"dev\",\n        \"mgr\"\n    ],\n    \"age\": 40.5\n}\n>>> P = json.load(open('testjson.txt'))       # JSON => Python\n>>> P\n{'name': {'first': 'Pat', 'last': 'Smith'}, 'job': ['dev', 'mgr'], 'age': 40.5}\n>>> P == rec\nTrue\nOnce you’ve translated from JSON text, you process the data using normal\nPython object operations in your script. For more details on JSON-related topics,\nsee Python’s library manuals and the web at large. You could, of course, store\nreal Python dictionaries and lists in an imported Python .py module file, and the\ndata would be just as readable and editable; JSON, however, is not run as code,\nand may be perceived by some as more interoperable today.\nNote that strings are all Unicode in JSON to support richer forms of text. Since\nPython strings in memory simply are Unicode, the distinction matters most\nwhen transferring text to and from files. You’ll learn how to apply Unicode\nencodings to JSON (and other) files and data in Chapter 37.\nStoring Objects with Other Tools\nFor other common ways to deal with formatted data files, see the standard\nlibrary’s struct and csv modules. Very briefly, the struct module can both\ncreate and parse packed binary data of the sort often shared with C programs:\n>>> import struct\n>>> data = struct.pack('i6s', 62, b'Python')    # Pack an int and str in bytes\n>>> data\nb'>\\x00\\x00\\x00Python'\n>>> file = open('data.bin', 'wb')               # Write/read file, and unpack\n>>> file.write(data)                            # Binary data in binary files\n>>> file.close() \n>>> struct.unpack('i6s', open('data.bin', 'rb').read())\n(62, b'Python')",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": 8,
      "content": "And the csv module parses and creates comma-separated value (CSV) data in\nfiles and strings; it doesn’t map as directly to Python objects (and requires post-\nparse conversions), but is another way to map value to and from files:\n>>> import csv\n>>> rdr = csv.reader(open('csvdata.txt'))\n>>> for row in rdr: print(row)\n...\n['a', 'bbb', 'cc', 'dddd']\n['11', '22', '33', '44']\nFor additional data storage ideas like YAML and SQLite, see the overview of\ndatabase tools in Chapter 1.\nFile Context Managers\nYou’ll also want to watch for Chapter 34’s in-depth discussion of the file’s\ncontext manager support. Though more a feature of exception processing than\nfiles themselves, it allows us to wrap file-processing code in a logic layer that\nensures that the file will be closed (and if needed, have its output flushed to disk)\nautomatically on statement exit, instead of relying on the auto-close during\ngarbage collection or manual close calls. As a preview:\nwith open('data.txt') as myfile:              # File closed on \"with\" exit \n    for line in myfile:                       # See Chapter 34 for details\n        …use line here…\nThe with statement closes the temporary file on exit, whether an error occurs or\nnot. The try/finally statement that we’ll also study in Chapter 34 can provide\nsimilar functionality, but at some cost in extra code—three extra lines, to be\nprecise (though we can often avoid both options and let Python close files for us\nautomatically):\nmyfile = open('data.txt')\ntry:                                          # General termination handler\n    for line in myfile:                       # See Chapter 34 for details\n        …use line here…\nfinally:\n    myfile.close()",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": 8,
      "content": "The with context manager scheme ensures release of system resources in all\nPythons and may be more useful for output files to guarantee buffer flushes;\nunlike the more general try, though, it is also limited to objects that support its\nprotocol. Since both these options require more information than we have yet\nobtained, however, we’ll postpone the rest of their stories until later in this book.\nOther File Tools\nThere are additional, more specialized file methods shown in Table 9-2, and\neven more that are not in the table. For instance, as mentioned earlier, seek\nresets your current position in a file (the next read or write happens at that\nposition); flush forces buffered output to be written out to disk without closing\nthe connection (by default, files are always buffered); and readlines and\nwritelines process file content in line lists.\nThe Python standard-library manual and other reference resources provide\ncomplete details on file methods, but for a quick look, run a dir or help call\ninteractively, passing in open or a file object made with it. And for more file-\nprocessing examples, watch for Chapter 37’s extended coverage, as well as the\nsidebar “Why You Will Care: File Scanners”, which sketches common file-\nscanning patterns with statements we have not yet covered here.\nAlso, note that although the open function and the file objects it returns are your\nmain interface to external files in a Python script, there are additional file-related\ntools in the Python toolset. Prominent among these are:\nStandard streams\nPreopened file objects in the sys module, such as sys.stdout, connected by\ndefault to the UI where a script is run (see “Print Operations” for details)\nDescriptor files in the os module\nInteger file handles that support lower-level tools such as read-only access\n(see also the “x” mode modifier in open for exclusive creation)\nSockets, pipes, and FIFOs",
      "content_length": 1895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": 8,
      "content": "File-like objects used to synchronize processes or communicate over\nnetworks\nAccess-by-key files known as shelves\nUsed to store unaltered and pickled Python objects directly, by key (see\nChapter 28 for an example)\nShell-command streams\nTools such as os.popen and subprocess.Popen that support spawning shell\ncommands and reading and writing to their standard streams (see Chapter 21\nfor an os.popen example)\nThe third-party open source domain offers even more file-like tools, including\nsupport for communicating with serial ports in the PySerial extension and\ninteractive programs in the pexpect system. Consult the web at large for\nadditional information on file-like tools.\nFor code spelunkers, it’s also worth noting that Python’s open function is really\njust an interface to tools in its standard-library io module, which adds logic on\ntop of the underlying system’s file tools to make them portable and efficient. If\nyou’re looking for docs, implementation details, or customization hooks, look\nfor this module in all the usual places. io is really a folder called a module\npackage—a structure for larger code that we’ll study later.\nCore Types Review and Summary\nNow that we’ve seen all of Python’s built-in objects in action, let’s wrap up our\nobject-types tour by reviewing some of the properties they share. Table 9-3\nclassifies all the major types we’ve studied so far according to the type\ncategories introduced earlier. Here are some points to remember:\nObjects share operations according to their category. For instance,\nsequence objects—strings, lists, and tuples—all share sequence",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": 8,
      "content": "operations such as concatenation, length, and indexing.\nOnly mutable objects—lists, dictionaries, and sets—may be changed in\nplace. You cannot change numbers, strings, or tuples in place, but can\nmake a new one with a different value.\nFiles export only methods, so mutability doesn’t really apply to them—\ntheir state may be changed when they are processed, but this isn’t quite\nthe same as Python object mutability.\n“Numbers” in Table 9-3 includes all number types: integer, floating\npoint, complex, decimal, and fraction.\n“Strings” in Table 9-3 includes all string types: str for text, as well as\nbytes for binary data. Exception: the bytearray string type convolutes\ncategorization because it is a mutable string.\nSets are something like the keys of a valueless dictionary, but they don’t\nmap to values and are not ordered, so sets are neither a mapping nor a\nsequence type. Exception: frozenset is an immutable variant of set.\nIn addition to type category operations, all the objects types in Table 9-3\nhave callable methods in Python today, which are generally specific to\ntheir type.\nTable 9-3. Object classifications\nObject type\nCategory\nMutable?\nNumbers\nNumeric\nNo\nStrings\nSequence\nNo\nLists\nSequence\nYes\nDictionaries\nMapping\nYes\nTuples\nSequence\nNo\nFiles\nExtension\nN/A",
      "content_length": 1275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": 8,
      "content": "Sets\nSet\nYes\nFrozenset\nSet\nNo\nbytearray\nSequence\nYes\nWHY YOU WILL CARE: OPERATOR OVERLOADING\nIn Part VI of this book, you’ll learn that objects implemented with classes\ncan pick and choose from these categories arbitrarily. For instance, if we\nwant to provide a new kind of specialized sequence object that is consistent\nwith built-in sequences, we can code a class that overloads things like\nindexing and concatenation:\nclass MySequence:\n    def __getitem__(self, index):\n        # Called on self[index], others\n    def __add__(self, other):\n        # Called on self + other\n    def __iter__(self):\n        # Preferred in iterations\nand so on. We can also make the new object mutable or not by selectively\nimplementing methods called for in-place change operations (e.g.,\n__setitem__ is called on self[index]=value assignments). Although it’s\nbeyond this book’s scope, it’s also possible to implement new objects in an\nexternal language like C as extension types. For these, we fill in C function\npointer slots to choose between number, sequence, and mapping operation\nsets, and similarly choose immutability constraints.\nObject Flexibility\nThis part of the book introduced a number of compound object types—\ncollections with components. In general:\nLists, dictionaries, and tuples can hold any kind of object.",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": 8,
      "content": "Sets can contain any type of immutable object.\nLists, dictionaries, and tuples can be arbitrarily nested.\nLists, dictionaries, and sets can dynamically grow and shrink.\nBecause they support arbitrary structures, Python’s compound object types are\ngood at representing complex information in programs. For example, values in\ndictionaries may be lists, which may contain tuples, which may contain\ndictionaries, and so on. The nesting can be as deep as needed to model the data\nto be processed.\nIn code, the following interaction defines a tree of nested compound sequence\nobjects, sketched in Figure 9-1. To access its components, you may include as\nmany index operations as required. Python evaluates the indexes from left to\nright and fetches a reference to a more deeply nested object at each step.\nFigure 9-1 may seem a pathologically complicated data structure, but it\nillustrates the syntax used to access nested objects in general:\n>>> L = ['abc', [(1, 2), ([3], 4)], 5]\n>>> L[1]\n[(1, 2), ([3], 4)]\n>>> L[1][1]\n([3], 4)\n>>> L[1][1][0]\n[3]\n>>> L[1][1][0][0]\n3",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": 8,
      "content": "Figure 9-1. A nested object tree with the offsets of its components\nReferences Versus Copies\nChapter 6 mentioned that assignments always store references to objects, not\ncopies of those objects. In practice, this is usually what you want. Because\nassignments can generate multiple references to the same object, though, it’s",
      "content_length": 324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": 8,
      "content": "important to be aware that changing a mutable object in place may affect other\nreferences to the same object elsewhere in your program. If you don’t want such\nbehavior, you’ll need to tell Python to copy the object explicitly.\nWe studied this phenomenon in Chapter 6, but it can become more subtle when\nlarger objects of the sort we’ve explored since then come into play. For instance,\nthe following example creates a list assigned to X, and another list assigned to L\nthat embeds a reference back to list X. It also creates a dictionary D that contains\nanother reference back to list X:\n>>> X = [1, 2, 3]\n>>> L = ['a', X, 'b']            # Embed references to X's object in two others\n>>> D = {'x':X, 'y':2}\nAt this point, there are three references to the first list created: from the name X,\nfrom inside the list assigned to L, and from inside the dictionary assigned to D.\nThe situation is illustrated in Figure 9-2.\nFigure 9-2. Shared objects: changing from X makes it look different from L and D too\nBecause lists are mutable, changing the shared list object from any of the three\nreferences also changes what the other two reference:\n>>> X[1] = 'surprise'             # Changes all three references!\n>>> L\n['a', [1, 'surprise', 3], 'b']\n>>> D\n{'x': [1, 'surprise', 3], 'y': 2}\nReferences are a higher-level analogue of pointers in other languages that are",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": 8,
      "content": "always followed when used. Although you can’t grab hold of the reference itself,\nit’s possible to store the same reference in more than one place (variables, lists,\nand so on). This is a feature—you can pass a large object around a program\nwithout generating expensive copies of it along the way. If you really want to\navoid the potential side effects of shared references, however, you can request\ncopies, in one of a number of ways:\nSlice expressions with empty limits (L[:]) copy sequences.\nThe dictionary, set, and list copy method (X.copy()) copies a\ndictionary, set, or list.\nSome built-in functions, such as list and dict make copies (list(L),\ndict(D), set(S)).\nThe copy standard-library module makes full (“recursive”) copies when\nneeded.\nFor example, say you have a list and a dictionary, and you don’t want their\nvalues to be changed through other variables:\n>>> L = [1,2,3]\n>>> D = {'a':1, 'b':2}\nTo prevent this, simply assign copies to the other variables, not references to the\nsame objects:\n>>> A = L[:]                      # Instead of A = L (or list(L), L.copy())\n>>> B = D.copy()                  # Instead of B = D (ditto for sets)\nThis way, changes made from the other variables will change the copies, not the\noriginals:\n>>> A[1] = 'Py'\n>>> B['c'] = 'code'               # Changes copies, not originals\n>>>\n>>> L, D\n([1, 2, 3], {'a': 1, 'b': 2})\n>>> A, B\n([1, 'Py', 3], {'a': 1, 'b': 2, 'c': 'code'})",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": 8,
      "content": "In terms of our original example, you can avoid the reference side effects by\nslicing the original list instead of simply naming it:\n>>> X = [1, 2, 3]\n>>> L = ['a', X[:], 'b']           # Embed copies of X's object\n>>> D = {'x':X[:], 'y':2}\nThis changes the picture in Figure 9-2—L and D will now point to different lists\nthan X. The net effect is that changes made through X will impact only X, not L\nand D; similarly, changes to L or D will not impact X.\nOne final note on copies: empty-limit slices and the dictionary copy method only\nmake top-level copies; that is, they do not copy nested data structures, if any are\npresent. If you need a complete, fully independent copy of a deeply nested data\nstructure (like the various record structures we’ve coded in recent chapters), use\nthe standard copy module, introduced in Chapter 6:\nimport copy\nX = copy.deepcopy(Y)               # Fully copy an arbitrarily nested object Y\nThis call traverses objects to copy all their parts, no matter how deep they may\nbe. This is a much rarer case, though, which is why you have to say more to use\nthis scheme. References are usually what you will want; when they are not,\nslices and copy methods are usually as much copying as you’ll need to do.\nComparisons, Equality, and Truth\nAll Python objects also respond to comparisons: tests for equality, relative\nmagnitude, and so on. We’ve seen comparison at work on specific objects in\nearlier chapters but can finally summarize the general rules.\nIn short, Python comparisons always inspect all parts of compound objects until\na result can be determined. When nested objects are present, Python\nautomatically traverses data structures to apply comparisons from left to right,\nand as deeply as needed. The first difference found along the way determines the\ncomparison result.\nThis is sometimes called a recursive comparison—the same comparison\nrequested on the top-level objects is applied to each of the nested objects, and to",
      "content_length": 1964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": 8,
      "content": "each of their nested objects, and so on, until a result is found. Later in this book\n(Chapter 19) you’ll learn how to write recursive functions of your own that work\nsimilarly on nested structures. For now, think about comparing all the linked\npages at two websites if you want a metaphor for such structures, and a reason\nfor writing recursive functions to process them.\nIn terms of core objects, the recursion is automatic. For instance, a comparison\nof list objects compares all their components automatically until a mismatch is\nfound or the end is reached:\n>>> L1 = [1, ('a', 3)]           # Same value, but different objects\n>>> L2 = [1, ('a', 3)]\n>>> L1 == L2, L1 is L2           # Same value? Same object?\n(True, False)\nHere, L1 and L2 are assigned lists that are equivalent but distinct objects. As a\nreview of Chapter 6’s coverage, because of the nature of Python references, there\nare two ways to test for equality:\nThe == operator tests value equivalence. Python performs an\nequivalence test, comparing all nested objects recursively.\nThe is operator tests object identity. Python tests whether the two are\nreally the same object (i.e., live at the same address in memory).\nIn the preceding example, L1 and L2 pass the == test (they have equivalent\nvalues because all their components are equivalent) but fail the is check (they\nreference two different objects, and hence two different pieces of memory).\nNotice what happens for short strings, though:\n>>> S1 = 'text'\n>>> S2 = 'text'\n>>> S1 == S2, S1 is S2\n(True, True)\nHere, we should again have two distinct objects that happen to have the same\nvalue: == should be true, and is should be false. But because Python internally\ncaches and reuses some objects as an optimization, there really is just a single\nstring 'text' in memory, shared by S1 and S2. Hence, the is identity test",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": 8,
      "content": "reports a true result. To trigger the normal behavior, we need to use strings that\nare longer (or otherwise defeat caching rules prone to change over time):\n>>> S1 = 'a longer string'\n>>> S2 = 'a longer string'\n>>> S1 == S2, S1 is S2\n(True, False)\nOf course, because strings are immutable, the object caching mechanism is\nirrelevant to your code—strings can’t be changed in place, regardless of how\nmany variables refer to them. If identity tests seem confusing, see Chapter 6 for\na refresher on object reference concepts. As a rule of thumb, the == operator is\nwhat you will want to use for almost all equality checks; is is reserved for\nhighly specialized roles. You’ll see use cases for both later.\nAs demoed along the way, relative magnitude comparisons are also applied\nrecursively to nested data structures:\n>>> L1 = [1, ('a', 3)]                # Nested 3 > nested 2\n>>> L2 = [1, ('a', 2)]\n>>> L1 < L2, L1 == L2, L1 > L2        # Less, equal, greater: tuple of results\n(False, False, True)\nHere, L1 is greater than L2 because the nested 3 is greater than the nested 2.\nMore broadly, Python compares its core object types as follows:\nNumbers are compared by relative magnitude, after conversion to the\ncommon highest type if needed (e.g., 1 < 1.1 after 1 is replaced with\n1.0).\nStrings are compared lexicographically (by the character code-point\nvalues returned by ord), and character by character until the end or first\nmismatch (e.g., 'abc' < 'ac').\nLists and tuples are compared by comparing each component from left\nto right, and recursively for nested structures, until the end or first\nmismatch (e.g., [1, 3] > [1, 2]).\nSets are equal if both contain the same items (formally, if each is a\nsubset of the other), and magnitude comparison operators for sets apply",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": 8,
      "content": "subset and superset tests.\nDictionaries compare as equal if their sorted (key, value) lists are\nequal. Magnitude comparisons are not supported for dictionaries but can\nbe coded by comparing manually sorted items results.\nNonnumeric mixed-type magnitude comparisons (e.g., 1 < 'text') are\nerrors. By proxy, this also applies to sorts, which use comparisons\ninternally: nonnumeric mixed-type collections cannot be sorted sans\nconversions.\nIn general, comparisons of structured objects proceed as though you had written\nthe objects as literals and compared all their parts one at a time from left to right.\nIn later chapters, you’ll also see that class-based objects can change the way they\nare compared. Here, the following sections provide a few more details on\nPython’s built-in comparisons.\nMixed-type comparisons and sorts\nThe preceding section’s last bullet point applies only to nonnumeric mixed-type\nmagnitude tests, not equality, but it also applies by proxy to sorting, which does\nmagnitude testing internally. Python disallows mixed-type magnitude testing,\nexcept for numeric types and manually converted types:\n>>> 11 == '11'                          # Equality works but magnitude does not\nFalse\n>>> 11 >= '11'\nTypeError: '>=' not supported between instances of 'int' and 'str'\n>>> ['11', '22'].sort()                 # Ditto for sorts\n>>> [11, '11'].sort()\nTypeError: '<' not supported between instances of 'str' and 'int'\n>>> 11 > 9.123                          # Mixed numbers convert to highest type\nTrue\n>>> str(11) >= '11', 11 >= int('11')    # Manual conversions force the issue\n(True, True)\n>>> [11, '11'].sort(key=str)            # Ditto for sorts: see Chapter 8\nDictionary comparisons",
      "content_length": 1704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": 8,
      "content": "As noted in Chapter 8, magnitude comparisons don’t work for dictionaries\ndirectly. Though subject to implementation morph, this purportedly reflects that\nfact that magnitude comparison would incur too much overhead and may\nhamper the more common equality test, which may use an optimized scheme\nthat doesn’t compare sorted key/value lists:\n>>> D1 = {'b':3, 'a':1}\n>>> D2 = {'a':1, 'b':3}\n>>> D1 == D2                                     # Equality, not magnitude\nTrue\n>>> D1 < D2\nTypeError: '<' not supported between instances of 'dict' and 'dict'\nTo work around this limitation, either write loops to compare values by key, or,\nas also shown in Chapter 8, simply compare sorted key/value lists manually by\ncombining the items dictionary method and sorted built-in:\n>>> list(D1.items())\n[('b', 3), ('a', 1)] \n>>> sorted(D1.items())\n[('a', 1), ('b', 3)] \n>>> sorted(D1.items()) < sorted(D2.items())      # Dictionary magnitude tests\nFalse\n>>> sorted(D1.items()) >= sorted(D2.items())\nTrue\nThis takes more code than a simple < or > and may run relatively slowly; but in\npractice, most programs requiring this behavior either will develop more\nefficient ways to compare data in dictionaries or won’t care about the sloth.\nThe Meaning of True and False in Python\nNotice that the test results returned in the last two examples represent true and\nfalse values. They print as the words True and False, but now that we’re using\nlogical tests like these in earnest, it’s time to be a bit more formal about what\nthese names really mean.\nIn Python, as in most programming languages, an integer 0 represents false, and\nan integer 1 represents true (a heritage rooted in the digital nature of computer",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": 8,
      "content": "hardware). In addition, though, Python recognizes any empty data structure as\nfalse and any nonempty data structure as true. More generally, the notions of true\nand false are intrinsic properties of every object in Python—each object is either\ntrue or false, as follows:\nNumbers are false if zero, and true otherwise.\nCollection objects are false if empty, and true otherwise.\nThe None placeholder object is always false.\nTrue and False are preset to true and false, respectively.\nTable 9-4 gives examples of true and false values of various objects in Python.\nTable 9-4. Example object truth\nvalues\nObject\nValue\n'text'\nTrue\n''\nFalse\n[1, 2]\nTrue\n[]\nFalse\n{'a': 1}\nTrue\n{}\nFalse\n1\nTrue\n0.0\nFalse\nNone\nFalse\nAs one application of this, because objects are true or false themselves, it’s\ncommon to see Python programmers code tests like if X:, which, assuming X is\na string, is the same as if X != '':. In other words, you can test the object",
      "content_length": 939,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": 8,
      "content": "itself to see if it contains anything, instead of comparing it to an empty—and\ntherefore false—object of the same type.\nThe None object\nAs shown in the last row in Table 9-4, Python also provides a special object\ncalled None, which is always considered to be false. None was introduced briefly\nin Chapter 4; it is the only value of a special data type in Python and typically\nserves as an empty placeholder (much like a NULL pointer in C).\nFor example, recall that for lists you cannot assign to an offset unless that offset\nalready exists—the list does not magically grow if you attempt an out-of-bounds\nassignment. To preallocate a list such that you can store values in any of its\noffsets, you can fill it with None objects:\n>>> size = 50\n>>> L = []\n>>> L[size - 1] = 'NO'\nIndexError: list assignment index out of range\n>>> L = [None] * size\n>>> L[size - 1] = 'OK'\n>>> L[-10:]\n[None, None, None, None, None, None, None, None, None, 'OK']\nThis doesn’t limit the size of the list (it can still grow and shrink later), but\nsimply presets an initial size to allow for future index assignments. You could\ninitialize a list with zeros the same way, of course, but best practice suggests\nusing None if the type of the list’s contents is variable or not yet known.\nKeep in mind that None does not mean “undefined.” That is, None is something,\nnot nothing (despite its name!)—it is a real object and a real piece of memory\nthat is created and given a built-in name by Python itself. Watch for other uses of\nthis special object later in the book; as you’ll learn in Part IV, it is also the\ndefault return value of functions that don’t exit by running into a return\nstatement with a result value.\nThe bool type\nWhile we’re on the topic of truth, also keep in mind that the Python Boolean",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": 8,
      "content": "type bool, introduced in Chapter 5, simply augments the notions of true and\nfalse in Python. As we learned earlier, the built-in words True and False are just\ncustomized versions of the integers 1 and 0—it’s as if these two words have been\npreassigned to 1 and 0 everywhere in Python. Because of the way this new type\nis implemented, this is really just a minor extension to the notions of true and\nfalse already described, designed to make truth values more explicit:\nWhen used explicitly in truth test code, the words True and False are\nequivalent to 1 and 0, respectively, but they make the programmer’s\nintent clearer.\nResults of Boolean tests run interactively print as the words True and\nFalse, instead of as 1 and 0, to make the type of result clearer.\nYou are not required to use only Boolean types in logical statements such as if;\nall objects are still inherently true or false, and all the Boolean concepts\nmentioned in this chapter still work as described if you use other types. Python\nalso provides a bool built-in function that can be used to extract the Boolean\nvalue of an object. You can use this to explicitly check if an object is true—that\nis, nonzero or nonempty:\n>>> bool(1)\nTrue\n>>> bool('text')\nTrue\n>>> bool({})\nFalse\nIn practice, though, you’ll rarely notice the Boolean type produced by logic tests,\nbecause Boolean results are used automatically by if statements and other\nselection tools. We’ll explore Booleans further when we study logical statements\nin Chapter 12.\nPython’s Type Hierarchies\nAs a summary and reference, Figure 9-3 sketches all the major built-in object\ntypes available in Python and their relationships. We’ve explored the most\nprominent of these in this part of the book. Other objects in Figure 9-3 are",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": 8,
      "content": "program units (e.g., functions and modules) or interpreter internals (e.g., stack\nframes and compiled code).\nThe main point to notice here is that everything processed in a Python program is\nan object type. This is sometimes called a “first class” object model, because all\nobjects are on equal footing with respect to your code. For instance, you can pass\na class to a function, assign it to a variable, stuff it in a list or dictionary, and so\non.\nType Objects\nIn fact, even types themselves are an object type in Python: the type of an object\nis an object of type type (and not just because it’s a decent tongue twister!).\nSeriously, a call to the built-in function type(X) returns the type object of object\nX. The practical application of this is that type objects can be used for manual\ntype comparisons in Python if statements. However, for reasons introduced in\nChapter 4 that we won’t rehash here, manual type testing is usually not the right\nthing to do in Python, since it limits your code’s flexibility. Python is about\nflexibility, not constraints.\nOne note on type names: each core type has a built-in name that supports various\nroles, including type customization through object-oriented subclassing: dict,\nlist, str, tuple, int, float, complex, bytes, type, set, and more.\nTechnically speaking, these names reference classes, and calls to these names are\nreally object constructor calls, not simply conversion functions, though you can\ntreat them as simple functions for basic usage.\nIn addition, the types standard-library module provides additional type names\nfor types that are not available as built-ins (e.g., types.FunctionType is the is\nthe type of functions), and the isinstance built-in function checks types with\nconsideration of inheritance in OOP—a topic we’ll reach later on our Python\njourney. Because types can be customized with OOP in Python, though, the\nisinstance technique is generally recommended in the very rare cases where\ncode must know about specific types. There’s more on type customizations in\nChapter 32, and an example in which isinstance is useful and warranted in\nChapter 19.",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": 8,
      "content": "Figure 9-3. Python’s major built-in object types, organized by categories",
      "content_length": 73,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": 8,
      "content": "Other Types in Python\nBesides the core objects studied in this part of the book, and the program-unit\nobjects such as functions, modules, and classes that you’ll meet later, a typical\nPython installation has dozens of additional object types available as linked-in C\nextensions or imported Python classes—regular expression objects, GUI\nwidgets, network sockets, and so on. Depending on whom you ask, the named\ntuple you met earlier in this chapter may fall in this category too, along with\nDecimal and Fraction of Chapter 5.\nThe main difference between these extra tools and the built-in types you’ve seen\nso far is that the built-ins have language-defined syntax for creating their objects\n(e.g., 4 for an integer, [1,2] for a list, the open function for files, and def and\nlambda for functions). Other tools are made available in standard-library\nmodules that you import to use. For instance, to make a regular-expression\nobject in pattern matching, you import re and call re.compile().\nBecause most objects in this noncore category are application-level tools that are\nbeyond the scope of this language tutorial, be sure to browse Python’s library\nreference early and often in your coding career for a comprehensive chronicle of\nall the supplemental tools available to Python programs.\nNOTE\nWhat about range?: Python’s documentation reclassified the built-in range function as a\nsequence type, along with lists and tuples, but this is an academic sleight of hand that we won’t\nadhere to in this book. As you’ll see later, range returns an iterable object that produces\nresults on demand, not a physically stored sequence. It supports some—but not all—sequence\noperations (e.g., indexing works but concatenation does not), but even this is an\nimplementation trick, and hardly enough to constitute a new type on the same level as real\nsequences. Labeling range an “immutable sequence of integers” conflates tool categories and\nconfuses Python learners.\nBuilt-in Type Gotchas\nThat’s the end of our look at core data types. We’ll wrap up this part of the book\nwith a discussion of common problems that seem to trap new users (and the",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": 8,
      "content": "occasional expert), along with their solutions. Some of this is a review of ideas\nwe’ve already covered, but these issues are important enough to warrant callouts\nagain here.\nAssignment Creates References, Not Copies\nYes, this is redundant, but it’s such a common pitfall that it’s worth underscoring\none more time: shared references to mutable objects can matter. In the following,\nfor instance, the list object assigned to the name L is referenced both from L and\nfrom inside the list assigned to the name M. Changing L in place changes what M\nreferences, too:\n>>> L = [1, 2, 3]\n>>> M = ['X', L, 'Y']           # Embed a reference to L\n>>> M\n['X', [1, 2, 3], 'Y']\n>>> L[1] = 0                    # Changes M (what M references) too\n>>> M\n['X', [1, 0, 3], 'Y']\nThis effect usually becomes important only in larger programs, and shared\nreferences are often exactly what you want. If objects change out from under you\nin ways unexpected and unwanted, though, you can avoid sharing objects easily\nby copying them explicitly. For lists, you can always make a top-level copy by\nusing an empty-limits slice, among other techniques described earlier in this\nchapter:\n>>> L = [1, 2, 3]\n>>> M = ['X', L[:], 'Y']        # Embed a copy of L\n>>> L[1] = 0                    # Changes only L, not M\n>>> L\n[1, 0, 3]\n>>> M\n['X', [1, 2, 3], 'Y']\nRemember, slice limits default to 0 and the length of the sequence being sliced;\nif both are omitted, the slice extracts every item in the sequence and so makes a\ntop-level copy (a new, unshared object).",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": 8,
      "content": "Repetition Adds One Level Deep\nAs we’ve learned, repeating a sequence is like adding it to itself a number of\ntimes. However, when mutable sequences are nested, the effect might not always\nbe what you expect. For instance, in the following example X is assigned to L\nrepeated four times, whereas Y is assigned to a list containing L repeated four\ntimes:\n>>> L = [4, 5, 6]\n>>> X = L * 4                   # Like [4, 5, 6] + [4, 5, 6] + …\n>>> Y = [L] * 4                 # [L] + [L] + … = [L, L, …]\n>>> X\n[4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6]\n>>> Y\n[[4, 5, 6], [4, 5, 6], [4, 5, 6], [4, 5, 6]]\nSubtly, because L was nested in the second repetition, Y winds up embedding\nreferences back to the original list assigned to L, and so is open to the same sorts\nof side effects noted in the preceding section:\n>>> L[1] = 0                    # Impacts Y but not X\n>>> X\n[4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6]\n>>> Y\n[[4, 0, 6], [4, 0, 6], [4, 0, 6], [4, 0, 6]]\nThis may seem artificial and academic—until it happens unexpectedly in your\ncode! The same solutions to this problem apply here as in the previous section,\nas this is really just another way to create the shared mutable object reference\ncase—make copies when you don’t want shared references:\n>>> L = [4, 5, 6]\n>>> Y = [list(L)] * 4           # Embed a (shared) copy of L\n>>> L[1] = 0\n>>> Y\n[[4, 5, 6], [4, 5, 6], [4, 5, 6], [4, 5, 6]]\nEven more subtly, although Y doesn’t share an object with L anymore, it still\nembeds four references to the same copy of it. If you must avoid that sharing too,\nyou’ll want to make sure each embedded copy is unique:",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": 8,
      "content": ">>> Y[0][1] = 99                # All four copies are still the same\n>>> Y\n[[4, 99, 6], [4, 99, 6], [4, 99, 6], [4, 99, 6]]\n>>> L = [4, 5, 6]\n>>> Y = [list(L) for i in range(4)]\n>>> Y\n[[4, 5, 6], [4, 5, 6], [4, 5, 6], [4, 5, 6]]\n>>> Y[0][1] = 99                # And now they're not!\n>>> Y\n[[4, 99, 6], [4, 5, 6], [4, 5, 6], [4, 5, 6]]\nIf you remember that repetition, concatenation, and slicing copy only the top\nlevel of their operand objects, these sorts of cases make much more sense.\nBeware of Cyclic Data Structures\nWe encountered this concept in a prior exercise but didn’t explore it much: if a\ncollection object contains a reference to itself, it’s called a cyclic object. Python\nprints a [...] whenever it detects a cycle in the object back to itself, rather than\ngetting stuck in an infinite loop (as it once did long ago, when dinosaurs roamed\nthe planet):\n>>> L = ['stuff']                # Append reference to same object\n>>> L.append(L)                  # Makes a cycle back to the same object: [...]\n>>> L\n['stuff', [...]]\nBesides understanding that the three dots in square brackets represent a cycle in\nthe object (should they ever crop up in your outputs or job interviews!), this case\nis worth knowing about because it can lead to real gotchas—cyclic structures\nmay cause code of your own to fall into unexpected loops if you don’t anticipate\nthem.\nFor instance, some programs that walk through structured data must keep a list,\ndictionary, or set of already visited items, and check it when they’re about to step\ninto a cycle that could cause an unwanted loop. See the Part I exercise solutions\nin “Part I, Getting Started” in Appendix B for more on this problem. Also watch\nfor general discussion of recursion in Chapter 19, as well as the reloadall.py\nprogram in Chapter 25 and the ListTree class in Chapter 31 for concrete",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": 8,
      "content": "examples of programs where cycle detection can matter.\nAs is so often the case in programming, the solution is knowledge: don’t use\ncyclic references unless you really need to, and make sure you anticipate them in\nprograms that must care. There are good reasons to create cycles, but unless you\nhave code that knows how to handle them, objects that reference themselves\nmay be more liability than asset. They need not, however, also be a surprise.\nImmutable Types Can’t Be Changed in Place\nAnd just once more for completeness: you cannot change an immutable object in\nplace. Instead, you construct a new object with slicing, concatenation, and so on,\nand assign it back to the original reference, if needed:\nT = (1, 2, 3)\nT[2] = 4              # Error!\nT = T[:2] + (4,)      # OK: (1, 2, 4)\nThat might seem like extra coding work (and it is), but the upside is that most of\nthe previous gotchas in this section can’t happen when you’re using immutable\nobjects like tuples and strings; because they can’t be changed in place, they are\nnot generally open to the sorts of side effects that can imperil mutable objects\nlike lists and dictionaries.",
      "content_length": 1143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": 8,
      "content": "Chapter Summary\nThis chapter explored the last two major core object types—the tuple and the\nfile. We learned that tuples support all the usual sequence operations, have just a\nfew methods, do not allow any in-place changes because they are immutable,\nand are generalized by the named-tuple extension type. We also learned that files\nare returned by the built-in open function and provide methods for reading and\nwriting content of both the text and binary kind.\nAlong the way we explored how to translate Python objects to and from strings\nfor storing in files, and we looked at pickle, json, and other modules for\nadvanced roles (object serialization and binary data). Finally, we wrapped up by\nreviewing some properties common to all object types (e.g., shared references)\nand went through a list of common mistakes (“gotchas”) in the object-type\ndomain.\nIn the next part of this book, we’ll shift gears, turning to the topic of statement\nsyntax—the way we code processing steps and logic in our scripts. Along the\nway, this next part explores all of Python’s basic procedural statements. The next\nchapter kicks off this topic with an introduction to Python’s general syntax\nmodel, which is applicable to all statement types. Before moving on, though,\ntake the chapter quiz, and then work through the end-of-part lab exercises to\nreview type concepts. The next part’s statements largely just create and process\nobjects, so make sure you’ve mastered this domain by working through all the\nexercises before reading on.\nTest Your Knowledge: Quiz\n1. How can you determine how large a tuple is? Why is this tool located\nwhere it is?\n2. Write an expression that changes the first item in a tuple. (4, 5, 6)\nshould become (1, 5, 6) in the process.\n3. What is the default for the processing mode argument in a file open",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": 9,
      "content": "call?\n4. What module might you use to store Python objects in a file without\nconverting them to strings yourself?\n5. How might you go about copying all parts of a nested structure at once?\n6. When does Python consider an object to be true?\nTest Your Knowledge: Answers\n1. The built-in len function returns the length (number of contained items)\nfor any container object in Python, including tuples. It is a built-in\nfunction instead of a type method because it applies to many different\ntypes of objects. In general, built-in functions and expressions may span\nmany object types; methods are specific to a single object type, though\nsome method names may be available on more than one type (index,\nfor example, works on lists and tuples).\n2. Because they are immutable, you can’t really change tuples in place, but\nyou can generate a new tuple with the desired value. Given T = (4, 5,\n6), you can change the first item by making a new tuple from its parts\nby slicing and concatenating: T = (1,) + T[1:]. (Recall that single-\nitem tuples require a trailing comma.) You could also convert the tuple\nto a list, change it in place, and convert it back to a tuple, but this is\nmore expensive and is rarely required in practice—simply use a list if\nyou know that the object will require in-place changes.\n3. The default for the processing mode argument in a file open call is 'r',\nfor reading text input. For input text files, simply pass in the external\nfile’s name or path (unless you also need to customize things like\nbuffering policies or provide a Unicode text encoding to override your\nplatform’s default—as fleshed out in Chapter 37).\n4. The pickle module can be used to store Python objects in a file without\nexplicitly converting them to strings. json similarly converts a limited\nset of Python objects to and from strings per the JSON format. The",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": 9,
      "content": "struct module is related, but it assumes the data is to be in packed\nbinary format in the file.\n5. Import the copy module, and call copy.deepcopy(X) if you need to\ncopy all parts of a nested structure X. This is also rarely needed in\npractice; references are usually the desired behavior, and shallow copies\n(e.g., aList[:], aDict.copy(), set(aSet)) usually suffice for most\ncopies.\n6. An object is considered true if it is either a nonzero number or a\nnonempty collection object. The built-in words True and False are\nessentially predefined to have the same meanings as integer 1 and 0,\nrespectively.",
      "content_length": 601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": 9,
      "content": "Test Your Knowledge: Part II Exercises\nThis session asks you to get your feet wet with coding built-in object\nfundamentals. As before, a few new ideas may pop up along the way, so be sure\nto flip to the answers in “Part II, Objects and Operations” in Appendix B when\nyou’re done (or even when you’re not). If you have limited time, consider\nstarting with exercises 10 and 11 (the most practical of the bunch) and then\nworking from first to last as time allows. This is all fundamental material,\nthough, so try to do as many of these as you can; programming is a hands-on\nactivity, and there is no substitute for practicing what you’ve read to make ideas\ngel.\n1. The basics: Experiment interactively with the common type operations\nfound in the various operation tables in this part of the book. To get\nstarted, bring up the Python interactive interpreter (a REPL of your\nchoosing), type each of the following expressions, and try to explain\nwhat’s happening in each case. Note that the semicolon in some of these\nis being used as a statement separator, to squeeze multiple statements\nonto a single line: for example, X=1;X assigns and then prints a variable\n(more on statement syntax in the next part of the book). Also remember\nthat a comma between expressions usually builds a tuple, even if there\nare no enclosing parentheses: X,Y,Z is a three-item tuple, which Python\nprints back to you in parentheses.\n2 ** 16\n2 / 5, 2 / 5.0\n'hack' + 'code'\nS = 'Python'\n'grok ' + S\nS * 5\nS[0], S[:0], S[1:]\nhow = 'fun'",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": 9,
      "content": "'coding %s is %s!' % (S, how)\n'coding {} is {}!'.format(S, how)\nf'coding {S} is {how}!'\n('x',)[0]\n('x', 'y')[1]\nL = [1, 2, 3] + [4, 5, 6]\nL, L[:], L[:0], L[−2], L[−2:]\n([1, 2, 3] + [4, 5, 6])[2:4]\n[L[2], L[3]]\nL.reverse(); L\nL.sort(); L\nL.index(4)\n{'a': 1, 'b': 2}['b']\nD = {'x': 1, 'y': 2, 'z': 3}\nD['w'] = 0\nD['x'] + D['w']\nD[(1, 2, 3)] = 4\nlist(D.keys()), list(D.values()), (1, 2, 3) in D\n[[]], [\"\", [], (), {}, None]\n2. Indexing and slicing: At the interactive prompt, define a list named L\nthat contains four strings or numbers (e.g., L=[0, 1, 2, 3]). Then,\nexperiment with the following boundary cases. You may never see these\ncases in real programs (especially not in the bizarre ways they may\nappear here!), but they are intended to make you think about the\nunderlying model, and some may be useful in less artificial forms—\nslicing out of bounds can help, for example, if a sequence is not as long\nas you expect:\na. What happens when you try to index out of bounds (e.g.,\nL[4])?",
      "content_length": 987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": 9,
      "content": "b. What about slicing out of bounds (e.g., L[-1000:100])?\nc. Finally, how does Python handle it if you try to extract a\nsequence in reverse, with the lower bound greater than the\nhigher bound (e.g., L[3:1])? Hint: try assigning to this slice\n(L[3:1]=['?']), and see where the value is put. Do you think\nthis may be the same phenomenon you saw when slicing out of\nbounds?\n3. Indexing, slicing, and del: Define another list L with four items, and\nassign an empty list to one of its offsets (e.g., L[2]=[]). What happens?\nThen, assign an empty list to a slice (L[2:3]=[]). What happens now?\nRecall that slice assignment deletes the slice and inserts the new value\nwhere it used to be.\nThe del statement deletes offsets, keys, attributes, and names. Use it on\nyour list to delete an item (e.g., del L[0]). What happens if you delete\nan entire slice (del L[1:])? What happens when you assign a\nnonsequence to a slice (L[1:2]=1)?\n4. Tuple assignment: Type the following lines:\n>>> X = 'code'\n>>> Y = 'hack'\n>>> X, Y = Y, X\nWhat do you think is happening to X and Y when you type this\nsequence?\n5. Dictionary keys: Consider the following code fragments:\n>>> D = {}\n>>> D[1] = 'a'\n>>> D[2] = 'b'\nYou’ve learned that dictionaries aren’t accessed by offsets, so what’s\ngoing on here? Does the following shed any light on the subject? (Hint:",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": 9,
      "content": "strings, integers, and tuples share which type category?)\n>>> D[(1, 2, 3)] = 'c'\n>>> D\n{1: 'a', 2: 'b', (1, 2, 3): 'c'}\n6. Dictionary indexing: Create a dictionary named D with three entries, for\nkeys 'a', 'b', and 'c'. What happens if you try to index a nonexistent\nkey (D['d'])? What does Python do if you try to assign to a nonexistent\nkey 'd' (e.g., D['d']='hack')? How does this compare to out-of-\nbounds assignments and references for lists? Does this sound like the\nrule for variable names?\n7. Generic operations: Run interactive tests to answer the following\nquestions:\na. What happens when you try to use the + operator on\ndifferent/mixed types (e.g., string + list, list + tuple)?\nb. Does + work when one of the operands is a dictionary?\nc. Does the append method work for both lists and strings? How\nabout using the keys method on lists? (Hint: what does append\nassume about its subject object?)\nd. Finally, what type of object do you get back when you slice or\nconcatenate two lists or two strings?\n8. String indexing: Define a string S of four characters: S = 'hack'. Then\ntype the following expression: S[0][0][0][0][0]. Any clue as to\nwhat’s happening this time? (Hint: recall that a string is a collection of\ncharacters, but Python characters are one-character strings.) Does this\nindexing expression still work if you apply it to a list such as ['h',\n'a', 'c', 'k']? Why?\n9. Immutable types: Define a string S of four characters again: S =\n'hack'. Write an assignment that changes the string to 'heck', using\nonly slicing and concatenation. Could you perform the same operation",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": 9,
      "content": "using just indexing and concatenation? How about index assignment?\n10. Nesting: Write a data structure that represents your personal\ninformation: name (first, middle, last), age, job, address, email address,\nand phone number. You may build the data structure with any\ncombination of built-in object types you like (lists, tuples, dictionaries,\nstrings, numbers). Then, access the individual components of your data\nstructures by indexing. Do some structures make more sense than others\nfor this object?\n11. Files: Write a script that creates a new output file called myfile.txt and\nwrites the string 'Hello file world!' into it. Then write another\nscript that opens myfile.txt and reads and prints its contents. Run your\ntwo scripts from the system command line (or other script-launcher tool\navailable to you). Does the new file show up in the directory where you\nran your scripts? What if you add a different directory path to the\nfilename passed to open? Note: file write methods do not add newline\ncharacters to your strings; add an explicit \\n at the end of the string if\nyou want to fully terminate the line in the file.",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": 9,
      "content": "Part Ill. Statements and Syntax",
      "content_length": 31,
      "extraction_method": "OCR"
    },
    {
      "page_number": 389,
      "chapter": 9,
      "content": "Chapter 10. Introducing Python\nStatements\nNow that you’re familiar with Python’s built-in objects, this chapter begins our\nexploration of its fundamental statement forms. Much like the previous part’s\napproach, we’ll begin here with a general introduction to statement syntax and\nfollow up with more details about specific statements in the next few chapters.\nIn simple terms, statements are the code we write to tell Python what our\nprograms should do. If, as suggested in Chapter 4, programs “do things with\nstuff,” then statements are the way we specify what sort of things a program\ndoes. Less informally, Python is a procedural, statement-based language; by\ncombining statements, we specify a procedure that Python performs to satisfy a\nprogram’s goals.\nThe Python Conceptual Hierarchy Revisited\nAnother way to understand the role of statements is to revisit the concept\nhierarchy introduced in Chapter 4, which talked about built-in objects and the\nexpressions used to manipulate them. This chapter climbs the hierarchy to the\nnext level of Python program structure:\n1. Programs are composed of modules.\n2. Modules contain statements.\n3. Statements contain expressions.\n4. Expressions create and process objects.\nAt their base, programs written in the Python language are composed of\nstatements and expressions. Expressions process objects and are embedded in\nstatements. Statements code the larger logic of a program’s operation—they use\nand direct expressions to process the objects we studied in the preceding\nchapters. Moreover, statements are where objects spring into existence (e.g., in",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": 9,
      "content": "expressions within assignment statements), and some statements create entirely\nnew kinds of objects (functions, classes, and so on). At the top, statements\nalways exist in modules, which themselves are managed with statements.\nPython’s Statements\nTable 10-1 summarizes Python’s statement set. Each statement in Python has its\nown specific purpose and its own specific syntax—the rules that define its\nstructure—though, as you’ll see, many share common syntax patterns, and some\nstatements’ roles overlap. Table 10-1 also gives examples of each statement,\nwhen coded according to its syntax rules. In your programs, these units of code\ncan perform actions, repeat tasks, make choices, build larger program structures,\nand so on.\nThis part of the book deals with entries in the table from the top through break\nand continue. You’ve informally been introduced to a few of the statements in\nTable 10-1 already; this part of the book will fill in details that were skipped\nearlier, introduce the rest of Python’s procedural statement set, and cover the\noverall syntax model. Statements lower in Table 10-1 that have to do with larger\nprogram units—functions, classes, modules, and exceptions—lead to larger\nprogramming ideas, so they will each have a section of their own. More focused\nstatements (like del, which deletes various components) are covered elsewhere\nin this book, as well as in Python’s standard manuals.\nTable 10-1. Python statements\nStatement\nRole\nExample\nAssignment\nCreating references\na, b = 'python', 3.12\na = (b := next()) + more\nCalls and other\nexpressions\nRunning functions\nlog.write('app crash')\nprint\nPrinting objects\nprint(hack, code, file=log)\nif/elif/else\nSelecting actions\nif 'python' in text: read(text)",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": 9,
      "content": "match/case\nMultiway selections\nmatch edition: case 6: print(2024)\nfor/else\nIteration\nfor x in myiterable: print(x)\nwhile/else\nGeneral loops\nwhile x := file.readline(): print(x)\npass\nEmpty placeholder\nif 'python' not in text: pass\nbreak\nLoop exit\nwhile True: if exittest(): break\ncontinue\nLoop continue\nwhile True: if skiptest(): continue\ndef\nFunctions and methods\ndef f(a, b, c=2, *more): print(a + b \n+ c)\nreturn\nFunctions results\ndef f(a, b, c=2, *more): return a + b\n+ c\nyield\nGenerator functions\ndef gen(n): for i in n: yield i*2\nglobal\nNamespaces\nx = 1 def function(): global x; x = 2\nnonlocal\nNamespaces\ndef outer(): x = 1 def inner(): nonlo\ncal x; x = 2\nasync\nCoroutine designator\nasync def consumer(a, b): await produ\ncer()\nawait\nCoroutine transfer\nawait asyncio.sleep(1)\nimport\nModule access\nimport sys\nfrom\nAttribute access\nfrom sys import stdin as f\nclass\nBuilding objects\nclass Subclass(Superclass): classAttr\n= [] def method(self): pass\ntry/except/final\nly\nCatching exceptions,\ntermination actions\ntry: action() except: print('action e\nrror')\nraise\nTriggering exceptions\nraise EndSearch(location)",
      "content_length": 1109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": 9,
      "content": "assert\nDebugging checks\nassert X > Y, 'X too small'\nwith\nContext managers\nwith open('data') as file: process(fi\nle)\ndel\nDeleting references\ndel data[k]\ndel data[i:j]\ndel obj.attr\ndel variable\ntype\nType hinting alias\ntype vector = list[float]\nTechnically, Table 10-1 is sufficient as a quick preview and reference, but it’s not\nquite complete as is. Here are a few fine points about its content:\nAssignment statements come in a wide variety of syntax flavors,\ndescribed in Chapter 11: basic, sequence, augmented, and more; and\nnamed assignment (:=) is used as an expression, not a statement.\nprint is really a built-in function call, and neither a reserved word nor a\nstatement. Because it will nearly always be run as an expression\nstatement, though, and usually on a line by itself, it’s generally thought\nof as a statement type, and will be treated separately in Chapter 11.\nyield and await are also expressions instead of statements. Like\nprint, they’re often used as expression statements and so are included\nin this table, but scripts may also assign or otherwise use their result, as\nyou’ll see in Chapter 20. As expressions, yield and await are also\nreserved words, unlike print.\nMost of the words used in statements and expressions are reserved and\ncannot be used as variables in your code; this includes and, in, if, for,\nwhile, and others. Newer statements use “soft” reserved words that are\nreserved only when used in the statements to which they belong; this\nincludes match, case, and type (though not async and await). We’ll\nformalize the full list of reserved words in Chapter 11.",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": 9,
      "content": "A Tale of Two ifs\nBefore we delve into the details of any of the concrete statements in Table 10-1,\nthis book wishes to begin our look at Python statement syntax by showing you\nwhat you are not going to type in Python code.\nConsider the following if statement, coded in a C-like language:\nif (x > y) {\n    x = 1;\n    y = 2;\n}\nThis might be a statement in C, C++, Java, JavaScript, or similar. Now, look at\nthe equivalent statement in the Python language:\nif x > y:\n    x = 1\n    y = 2\nThe first thing that may pop out at you is that the equivalent Python statement is\nless cluttered—that is, there are fewer syntactic components. This is by design;\nas a scripting language, one of Python’s goals is to make programmers’ lives\neasier by requiring less typing. And less typing also means less room for\nmistakes.\nMore specifically, when you compare the two syntax models, you’ll notice that\nPython adds one new thing to the mix but removes three things that are required\nin C-like languages.\nWhat Python Adds\nThe one new syntax component in Python is the colon character (:). All Python\ncompound statements—statements that have other statements nested inside them\n—follow the same general pattern of a header line terminated in a colon,\nfollowed by a nested block of code usually indented underneath the header line,\nlike this:\nHeader line:\n    Nested statement block",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": 9,
      "content": "The colon is required, and omitting it is probably the most common coding\nmistake among new Python programmers (it’s certainly one witnessed thousands\nof times live in Python training classes). In fact, if you are new to Python, you’ll\nalmost certainly forget the colon character very soon, if you haven’t already.\nYou’ll get an error message if you do, and most Python-friendly editors make\nthis mistake easy to spot:\n>>> if x\n    if x\n        ^\nSyntaxError: expected ':'\nIncluding the colon eventually becomes an unconscious habit—so much so that\nyou may start typing colons in your C-like language code, too (generating reams\nof entertaining error messages from that language’s compiler!).\nWhat Python Removes\nAlthough Python requires the extra colon character, there are three things\nprogrammers in C-like languages must include that you don’t generally have to\ncode in Python.\nParentheses are optional\nThe first of these is the set of parentheses around the tests at the top of some\nstatements:\nif (x < y)\nThe parentheses here are required by the syntax of many C-like languages. In\nPython, though, they are not—we simply omit the parentheses, and the statement\nworks the same way:\nif x < y\nTechnically speaking, because every expression can be enclosed in parentheses,\nincluding them will not hurt in this Python code, and they are not treated as an\nerror if present.",
      "content_length": 1373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": 9,
      "content": "But don’t do that. You’ll be wearing out your keyboard needlessly, and\nbroadcasting to the world that you’re a programmer of a C-like language still\nlearning Python (it happens). The “Python way” is to simply omit the\nparentheses in these kinds of statements altogether.\nEnd-of-line is end of statement\nThe second and more significant syntax component you won’t find in Python\ncode is the semicolon. You don’t need to terminate statements with semicolons\nin Python the way you do in C-like languages:\nx = 1;\nIn Python, the general rule is that the end of a line automatically terminates the\nstatement that appears on that line. In other words, you can leave off the\nsemicolons, and it works the same way:\nx = 1\nThere are some ways to work around this rule, as you’ll see in a moment (for\ninstance, wrapping code in a bracketed structure allows it to span lines). But, in\ngeneral, you write one statement per line for the vast majority of Python code,\nand no semicolon is required.\nHere, too, if you are pining for your C programming days (and why would you?)\nyou can continue to use semicolons at the end of each statement—the Python\nlanguage lets you get away with them if they are present, because the semicolon\nis also a separator when statements are combined.\nBut don’t do that either (really!). Again, doing so tells the world that you’re a\nprogrammer of a C-like language who still hasn’t quite made the switch to\nPython coding. The Pythonic style is to leave off the semicolons altogether.\nJudging from students in classes, this seems a tough habit for some veteran\nprogrammers to break. But you’ll get there; semicolons are useless noise in this\nrole in Python.\nEnd of indentation is end of block\nThe third and final syntax component that Python removes…and the one that",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": 9,
      "content": "may seem the most unusual to soon-to-be-ex-programmers of C-like languages,\nuntil they’ve used it for 10 minutes and realize it’s actually a feature...is that you\ndo not type anything explicit in your code to syntactically mark the beginning\nand end of a nested block of code. You don’t need to include begin/end,\nthen/endif, or {/} around the nested block, as you do in C-like languages:\nif (x > y) {\n    x = 1;\n    y = 2;\n}\nInstead, in Python, we consistently indent all the statements in a given single\nnested block the same distance to the right, and Python uses the statements’\nphysical indentation to determine where the block starts and stops:\nif x > y:\n    x = 1\n    y = 2\nThis indentation means the blank whitespace all the way to the left of the two\nnested statements here. Python doesn’t care how you indent (you may use either\nspaces or tabs), or how much you indent (you may use any number of spaces or\ntabs). In fact, the indentation of one nested block can be totally different from\nthat of another. The syntax rule is only that for a given single nested block, all of\nits statements must be indented the same distance to the right. If this is not the\ncase, you will get a syntax error, and your code will not run until you repair its\nindentation to be consistent:\n>>> if True:\n...     a = 1\n...   b = 2\n    b = 2\n         ^\nIndentationError: unindent does not match any outer indentation level\nWhy Indentation Syntax?\nThe indentation rule may seem unusual at first glance to programmers\naccustomed to C-like languages, but it is an intentional feature of Python: it’s",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": 9,
      "content": "one of the main ways that Python almost forces programmers to produce\nuniform, regular, and readable code. It essentially means that you must line up\nyour code vertically, in columns, according to its logical structure. The net effect\nis to make your code more consistent and readable—unlike much of the code\nwritten in C-like languages.\nTo put that more strongly, aligning your code according to its logical structure is\na major part of making it readable, and thus reusable and maintainable, by\nyourself and others. In fact, even if you never use Python after reading this book,\nyou should get into the habit of aligning your code for readability in any block-\nstructured language. Python underscores the issue by making this a part of its\nsyntax, but it’s an important thing to do in any programming language, and it has\na huge impact on the usefulness of your code.\nYour experience may vary, but there’s a common phenomenon in large, old C++\nprograms that have been worked on by many programmers over the years.\nAlmost invariably, each programmer has his or her own style for indenting code.\nFor example, a while loop coded in the C++ language may have begun its\ntenure like this:\nwhile (x > 0) {\nBefore we even get into indentation, there are three or four ways that\nprogrammers can arrange the {} braces in a C-like language, and organizations\noften suffer political battles and standards docs to address the options (which\nseems more than a little off-topic for the problem to be solved by programming).\nBe that as it may, here’s the scenario often encountered in C++ code. The first\nperson who worked on the code indented the loop four spaces:\nwhile (x > 0) {\n    --------;\n    --------;\nThat person eventually moved on to other projects (or, sadly, management), only\nto be replaced by someone who liked to indent further to the right:\nwhile (x > 0) {\n    --------;\n    --------;",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": 9,
      "content": "--------;\n           --------;\nThat person later moved on to other opportunities (ending their reign of coding\nterror), and someone else picked up the code who liked to indent less:\nwhile (x > 0) {\n    --------;\n    --------;\n           --------;\n           --------;\n--------;\n--------;\n}\nAnd so on. Eventually, the block is terminated by a closing brace (}), which of\ncourse makes this “block-structured code” (yes, sarcasm). No: in any block-\nstructured language, Python or otherwise, if nested blocks are not indented\nconsistently, they become very difficult for the reader to interpret, change, or\nreuse, because the code no longer visually reflects its logical meaning.\nReadability matters, and indentation is a major component of readability.\nHere is another example that may have burned you in the past if you’ve done\nmuch programming in a C-like language. Consider the following statement in C:\nif (x)\n     if (y)\n          statement1;\nelse\n     statement2;\nWhich if does the else here go with? Surprisingly, the else is paired with the\nnested if statement (if (y)) in C, even though it looks visually as though it is\nassociated with the outer test (if (x)). This is a classic pitfall in the C language,\nand it can lead to the reader completely misinterpreting the code and changing it\nincorrectly in ways that might not be uncovered until the Mars rover crashes into\na giant rock!\nThis cannot happen in Python—because indentation is significant, the way the\ncode looks is the way it will work. Consider an equivalent Python statement:",
      "content_length": 1544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": 9,
      "content": "if x:\n     if y:\n          statement1\nelse:\n     statement2\nIn this example, the if that the else lines up with vertically and visually is the\none it is associated with logically and behaviorally (the outer if x). In a sense,\nPython is a WYSIWYG language—what you see is what you get—because the\nway code looks is the way it runs, regardless of who coded it.\nIf this still isn’t enough to underscore the benefits of Python’s syntax, here’s\nanother anecdote. Some companies that develop systems software in the C\nlanguage, where consistent indentation is not required, enforce it anyhow. It’s\nnot too uncommon for such groups to run automated tools that analyze the\nindentation used in code, when it is checked into source control systems at the\nend of the day. If the tools notice that you’ve indented your code inconsistently,\nyou might just receive an automated email about it the next morning—and so\nmight your manager!\nThe point is that even when a language doesn’t require it, good programmers\nknow that consistent use of indentation has a huge impact on code readability\nand quality. The fact that Python promotes this to the level of syntax is seen by\nmost as a feature of the language.\nAlso keep in mind that nearly every programmer-friendly text editor has built-in\nsupport for Python’s syntax model. In the IDLE GUI, for example, lines of code\nare automatically indented when you are typing a nested block; pressing the\nBackspace key backs up one level of indentation, and you can customize how far\nto the right IDLE indents statements in a nested block. There is no requirement\non this: four spaces is very common, but it’s up to you to decide how and how\nmuch you wish to indent (unless your company has endured politics and docs to\nstandardize this too). Indent further to the right for further nested blocks, and\nless to close the prior block.\nAs a caution, though, you probably shouldn’t mix tabs and spaces in the same\nblock in Python, unless you do so consistently; use tabs or spaces in a given\nblock, but not both (in fact, Python issues an error for inconsistent use of tabs\nand spaces, as you’ll see in Chapter 12). Then again, you probably shouldn’t mix",
      "content_length": 2175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": 9,
      "content": "tabs or spaces in indentation in any structured language—such code can cause\nmajor readability issues if the next programmer has her or his editor set to\ndisplay tabs differently than yours. C-like languages might let coders get away\nwith this, but they really shouldn’t: the result can be a mangled mess.\nRegardless of which language you code in, you should be indenting consistently\nfor readability. In fact, if you weren’t taught to do this earlier in your career, your\nteachers did you a disservice. Most programmers—especially those who must\nread others’ code—consider it an asset that Python elevates this to the level of\nsyntax. Moreover, generating tabs instead of braces is no more difficult in\npractice for tools that must output Python code, and the page breaks that can\nobscure code nesting in the print versions of books (including this one!) will not\nbe present in the real world of coding.\nIn sum, if you do what you should be doing in a C-like language anyhow, but get\nrid of the braces, your code will satisfy Python’s syntax rules.\nA Few Special Cases\nAs mentioned previously, in Python’s syntax model:\nThe end of a line terminates the statement on that line (without\nsemicolons).\nNested statements are blocked and associated by their physical\nindentation (without braces).\nThose rules cover almost all Python code you’ll write or see in practice.\nHowever, Python also provides some special-purpose rules that allow for\nflexibility in both statements and nested statement blocks. They’re not required\nand should be used sparingly, but programmers have found them useful in\npractice.\nStatement rule special cases\nThere are three special rules for statements, two of which have already been\nintroduced and used in this book. First of all, although statements normally\nappear one per line, it is possible to squeeze more than one statement onto a\nsingle line in Python by separating them with semicolons:",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": 9,
      "content": "a = 1; b = 2; print(a + b)               # Three statements on one line\nThis is the only place in Python where semicolons are required: as statement\nseparators. This only works, though, if the statements thus combined are not\nthemselves compound statements. In other words, you can chain together only\nsimple statements, like assignments, and calls to print and other functions and\nmethods. Compound statements like if tests and while loops must still appear\non lines of their own (otherwise, you could squeeze an entire program onto one\nline, which probably would not make you very popular among your coworkers!).\nThe other special rule for statements is essentially the inverse: you can make a\nsingle statement span across multiple lines. To make this work, you simply have\nto enclose part of your statement in a bracketed pair—parentheses (()), square\nbrackets ([]), or curly braces ({}). Any code enclosed in these constructs can\ncross multiple lines: your statement doesn’t end until Python reaches the line\ncontaining the closing part of the pair. For instance, to continue a list literal:\nmylist = [1111,           # Continuation lines\n          2222,           # Any code in (), [], {} \n          3333]\nBecause this code is enclosed in a square brackets pair, Python simply keeps\nreading on the next line until it encounters the closing bracket. The curly braces\nsurrounding dictionaries (as well as set literals and dictionary and set\ncomprehensions) allow them to span lines this way, too, and parentheses handle\ntuples, function calls, and expressions. The indentation of the continuation lines\ndoes not matter, though common sense dictates that the lines should be aligned\nsomehow for readability. Any # comments are ignored as usual in continuation\nlines too, and nested brackets must all be closed before the continuation-line run\nends.\nParentheses are the catchall device—because any expression can be wrapped in\nthem, simply inserting a left parenthesis allows you to drop down to the next line\nand continue your statement:\nX = (A + B +\n     C + D)",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": 9,
      "content": "This technique works within compound statements, too, by the way. Anywhere\nyou need to code a large expression, simply wrap it in parentheses to continue it\non the next line:\nif (A == 1 and\n    B == 2 and\n    C == 3):\n        print('hack' * 3)\nAn older rule also allows for continuation lines when the prior line ends in a\nbackslash:\nX = A + B + \\\n      C + D               # An error-prone older alternative\nThis alternative technique is somewhat discouraged today because it’s difficult\nto notice and maintain the backslashes. It’s also fairly brittle and error-prone—\nthere can be no spaces or # comments after the backslash, and accidentally\nomitting it can have unexpected effects if the next line is mistaken to be a new\nstatement (in this example, “C + D” is a valid statement by itself if it’s not\nindented and would silently be run as such). This rule is also a throwback to the\nC language, where it is commonly used in “#define” macros. While \\ may be\noccasionally useful, when in Pythonland, do as Pythoneers do: use bracketed\npairs instead of \\ as a rule.\nBlock rule special case\nAs mentioned previously, statements in a nested block of code are normally\nassociated by being indented the same amount to the right. As one special case\nhere, and another we met in earlier chapters, the body of a compound statement\ncan instead appear on the same line as the header in Python, after the colon:\nif x > y: print(x)\nThis allows us to code single-line if statements, single-line while and for\nloops, and so on. Much like ; separators, though, this will work only if the body\nof the compound statement itself does not contain any compound statements.\nThat is, only simple statements—assignments, calls to print and others, and the",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": 9,
      "content": "like—are allowed after the colon. Larger statements must still appear on lines by\nthemselves. Extra parts of compound statements (such as the else part of an if,\nwhich you’ll meet in the next section) must also be on separate lines of their\nown. Compound statement bodies can also consist of multiple simple statements\nseparated by semicolons, but this tends to be frowned upon.\nIn general, even though it’s not always required, if you keep most of your\nstatements on individual lines and indent your nested blocks as a norm, your\ncode will be easier to read and change in the future. Moreover, some code\nprofiling and coverage tools may not be able to distinguish between multiple\nstatements squeezed onto a single line, or the header and body of a one-line\ncompound statement. It is almost always to your advantage to keep things simple\nin Python. You can use the special-case exceptions to write Python code that’s\nhard to read, but it takes a lot of work, and there are probably better ways to\nspend your time.\nTo see a prime and common exception to one of these rules in action, however\n(the use of a single-line if statement to break out of a loop), and to introduce\nmore of Python’s syntax, let’s move on to the next section and write some real\ncode.\nA Quick Example: Interactive Loops\nYou’ll see all these syntax rules in action when we tour Python’s specific\ncompound statements in the next few chapters, but they work the same\neverywhere in the Python language. To get started, let’s work through a brief but\nrealistic example that demos the way that statement syntax and nesting come\ntogether and introduces a few statements along the way. To work along, either\ncopy and paste this section’s examples from emedia into your REPL or run them\nfrom the file interact.py located in this book’s examples package using any of\nthe launch tools we studied in Chapter 3.\nA Simple Interactive Loop\nSuppose you’re asked to write a Python program that interacts with a user in a\nconsole window. Maybe you’re accepting inputs to send to a database or reading\nnumbers to be used in a calculation. Regardless of the purpose, you need to code",
      "content_length": 2136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": 9,
      "content": "a loop that reads one or more inputs from a user typing on a keyboard and prints\nback a result for each. In other words, you need to write a classic\nread/evaluate/print loop program, similar to Python’s standard REPL.\nIn Python, typical boilerplate code for such an interactive loop might look like\nthis:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop': break\n    print(reply.upper())\nThis code makes use of a few new ideas and some we’ve already explored:\nThe code leverages the Python while loop, Python’s most general\nlooping statement. We’ll study the while statement in more detail later,\nbut in short, it consists of the word while, followed by an expression\nthat is interpreted as a true or false result, followed by a nested block of\ncode that is repeated while the test at the top is true. The word True\nhere is considered always true, so this loop continues forever, unless\nsomehow stopped.\nThe input built-in function we met earlier in the book (to keep\nwindows open after clicks in Chapter 3 and the Appendix A coverage it\nreferenced) is used here for general console input—it prints its optional\nargument string as a prompt and returns the user’s typed reply as a\nstring.\nA single-line if statement that makes use of the special rule for nested\nblocks also appears here: the body of the if appears on the header line\nafter the colon instead of being indented on a new line underneath it.\nThis would work either way, but as it’s coded, we’ve saved an extra\nline.\nFinally, the Python break statement is used to exit the loop immediately\n—it simply jumps out of the loop statement altogether, and the program\ncontinues after the loop. Without this exit statement, the while would\nloop forever, as its test is always true.",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": 9,
      "content": "In effect, this combination of statements essentially means “read a line from the\nuser and print it in uppercase until the user enters the word ‘stop.’” There are\nother ways to code such a loop (e.g., see the note ahead), but the form used here\nis very common in Python code and serves to illustrate syntax basics.\nNotice that all three lines nested under the while header line are indented the\nsame amount: because they line up vertically in a column this way, they are the\nblock of code that is associated with the while test and repeated. Either a lesser-\nindented statement or the end of the source file (as here) will suffice to terminate\nthe loop body block.\nWhen this code is run, either interactively or as a script file, here is the sort of\ninteraction we get:\nEnter text:python\nPYTHON\nEnter text:312\n312\nEnter text:stop\nNOTE\nOr crunch code with the := operator: Spoiler alert—this section’s code is traditional and\nsimple and works as a syntax demo, but it’s possible to reduce it from four lines to two with\nthe := named-assignment expression added in Python 3.8 and covered in the next chapter. This\nexpression assigns a name to another expression’s result, but also returns the assigned value as\nits overall result. The net effect lets us fetch, assign, and test input on the same line—and all in\nthe loop’s header:\nwhile (reply := input('Enter text:')) != 'stop':\n    print(reply.upper())\nWhile useful in narrow roles, this expression also requires nested parentheses in this context\nand is arguably more implicit than traditional forms (though ex-C programmers’ mileage may\nvary!).\nDoing Math on User Inputs\nOur script works, but now suppose that instead of converting a text string to\nuppercase, we want to do some math with numeric input—squaring it, for",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": 9,
      "content": "example (perhaps in some misguided effort of an age-input program to tease its\nusers). We might try statements like these to achieve the desired effect:\n>>> reply = '40'\n>>> reply ** 2\nTypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\nThis won’t quite work in our script, though: input from a user is always returned\nas a string, and as discussed in the prior part of this book, Python won’t convert\nobject types in expressions unless they are all numeric. We cannot raise a string\nof digits to a power unless we convert it manually to an integer:\n>>> int(reply) ** 2\n1600\nArmed with this information, we can now recode our loop to perform the\nnecessary math. Type the following in a file to run it with command line, IDLE\nmenu options, or any other technique we met in Chapter 3 (the REPL both\nrequires a blank line after the while, and runs just one statement at a time—the\nfinal print here wouldn’t make sense):\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop': break\n    print(int(reply) ** 2)\nprint('Bye')\nThis script uses a single-line if statement to exit on “stop” as before, but it also\nconverts inputs to perform the required math. This version also adds an exit\nmessage at the bottom. Because the print statement in the last line is not\nindented as much as the nested block of code, it is not considered part of the\nloop body and will run only once, after the loop is exited:\nEnter text:2\n4\nEnter text:40\n1600\nEnter text:stop\nBye",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": 9,
      "content": "Handling Errors by Testing Inputs\nSo far so good, but notice what happens when the input is invalid:\nEnter text:xxx\nValueError: invalid literal for int() with base 10: 'xxx'\nThe built-in int function raises an exception (i.e., flags an error) here in the face\nof a nonnumber. If we want our script to be robust, we can check the string’s\ncontent ahead of time with the string object’s isdigit method:\n>>> S = '40'\n>>> T = 'xxx'\n>>> S.isdigit(), T.isdigit()\n(True, False)\nThis also gives us an excuse to further nest the statements in our example. The\nfollowing new version of our interactive script uses a full-blown if statement to\nwork around the exception on conversion errors:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop':\n        break\n    elif not reply.isdigit():\n        print('Bad!' * 8)\n    else:\n        print(int(reply) ** 2)\nprint('Bye')\nWe’ll study the if statement in more detail in Chapter 12, but it’s a fairly\nlightweight tool for coding logic in scripts. In its full form, it consists of the\nword if followed by a test and an associated block of code; one or more\noptional elif (“else if”) tests and code blocks; and an optional else part with a\nblock of code at the bottom to serve as a default. Python runs the block of code\nassociated with the first test that is true, working from top to bottom, or the else\npart if all tests are false.\nThe if, elif, and else parts in the preceding example are associated as part of\nthe same statement because their opening words all line up vertically (i.e., share",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": 9,
      "content": "the same level of indentation). The if statement spans from the word if to just\nbefore the print statement on the last line of the script. In turn, the entire if\nblock is part of the while loop because all of it is indented under the loop’s\nheader line. Statement nesting like this is natural once you start using it.\nWhen we run our new script, its code catches errors before they occur and prints\nan error message before continuing (which you’ll probably want to improve\nbefore this code is handed over to the quality-assurance team), but “stop” still\ngets us out, and valid numbers are still squared:\nEnter text:5\n25\nEnter text:xxx\nBad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!\nEnter text:10\n100\nEnter text:stop\nBye\nHandling Errors with try Statements\nThe preceding solution works, but as you’ll see later in the book, the most\ngeneral way to handle errors in Python is to catch and recover from them\ncompletely using the Python try statement. We’ll explore this statement in depth\nin Part VII of this book, but as a preview, using a try here can lead to code that\nsome might see as simpler than the prior version:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop': break\n    try:\n        num = int(reply)\n    except:\n        print('Bad!' * 8)\n    else:\n        print(num ** 2)\nprint('Bye')\nThis version works exactly like the previous one, but we’ve replaced the explicit\nerror check with code that assumes the conversion will work and wraps it in an\nexception handler for cases when it doesn’t. In other words, rather than detecting",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": 9,
      "content": "an error, we simply respond if one occurs.\nThis try statement is another compound statement and follows the same pattern\nas if and while. It’s composed of the word try, followed by the main block of\ncode (the action we are trying to run), followed by an except part that gives the\nexception handler code and an else part to be run if no exception is raised in the\ntry part. Python first runs the try part, then runs either the except part (if an\nexception occurs) or the else part (if no exception occurs).\nIn terms of statement nesting, because the words try, except, and else are all\nindented to the same level, they are all considered part of the same single try\nstatement. Notice that the else part is associated with the try here, not the if.\nAs we’ve seen, else can appear in if statements in Python, but it can also\nappear in try statements and loops—its indentation tells you what statement it is\na part of. In this case, the try statement spans from the word try through the\ncode indented under the word else, because the else is indented the same as\ntry. The if statement in this code is a one-liner and ends after the break, so the\nelse cannot apply to it.\nSupporting Floating-Point Numbers\nAgain, we’ll come back to the try statement later in this book. For now, be\naware that because try can be used to intercept any error, it reduces the amount\nof error-checking code you have to write, and it’s a very general approach to\ndealing with unusual cases. If we’re sure that print won’t fail, for instance, this\nexample could be even more concise:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop': break\n    try:\n        print(int(reply) ** 2)\n    except:\n        print('Bad!' * 8)\nprint('Bye')\nAnd if we wanted to support input of floating-point numbers instead of just\nintegers, for example, using try would be much easier than manual error testing",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": 9,
      "content": "—we could simply run a float call and catch its exceptions:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop': break\n    try:\n        print(float(reply) ** 2)\n    except:\n        print('Bad!' * 8)\nprint('Bye')\nThere is no isfloat method for strings today, so this exception-based approach\nspares us from having to accommodate all possible floating-point syntax in an\nup-front error check (a nontrivial task, given the many faces of floats!). When\ncoded this way, we can enter a wider variety of numbers, but errors and exits still\nwork as before:\nEnter text:50\n2500.0\nEnter text:40.5\n1640.25\nEnter text:1.23E-100\n1.5129e-200\nEnter text:hack\nBad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!\nEnter text:stop\nBye\nNOTE\nOr run anything with eval and exec: Python’s built-in eval call, which we used in Chapters 5\nand 9 to convert data in strings and files, would work in place of float here, too, and would\nallow input of arbitrary expressions (“2 ** 100” would be a legal, if curious, input, especially\nif we’re assuming the program is processing ages!). This is a powerful concept that is open to\nthe same security issues mentioned in the prior chapters. If you can’t trust the source of a code\nstring, use more focused and restrictive conversion tools like int and float.\nPython’s exec, used in Chapter 3 to run code read from a file, is similar to eval (but assumes\nthe string is a statement instead of an expression and has no result), and its compile call\nprecompiles frequently used code strings to bytecode objects for speed. Run a help on any of\nthese for more details. We’ll also use exec to import modules by name string in Chapter 25—\nan example of its more dynamic roles.",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": 9,
      "content": "Nesting Code Three Levels Deep\nLet’s look at one last mutation of our code. Nesting can take us even further if\nwe need it to—we could, for example, extend our prior integer-only script to\nbranch to one of a set of alternatives based on the relative magnitude of a valid\ninput:\nwhile True:\n    reply = input('Enter text:')\n    if reply == 'stop':\n        break\n    elif not reply.isdigit():\n        print('Bad!' * 8)\n    else:\n        num = int(reply)\n        if num < 20:\n            print('low')\n        else:\n            print(num ** 2)\nprint('Bye')\nThis version adds an if statement nested in the else clause of another if\nstatement, which is in turn nested in the while loop. When code is conditional or\nrepeated like this, we simply indent it further to the right. The net effect is like\nthat of prior versions, but we’ll now print “low” for numbers less than 20:\nEnter text:19\nlow\nEnter text:20\n400\nEnter text:hack\nBad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!\nEnter text:stop\nBye",
      "content_length": 974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": 9,
      "content": "Chapter Summary\nThat concludes our first look at Python statement syntax. This chapter\nintroduced the general rules for coding statements and blocks of code. As you’ve\nlearned, in Python we normally code one statement per line and indent all the\nstatements in a nested block the same amount (indentation is part of Python’s\nsyntax). However, we also looked at a few exceptions to these rules, including\ncontinuation lines and single-line tests and loops. Finally, we put these ideas to\nwork in an interactive script that demonstrated a handful of statements and\nshowed statement syntax in action.\nIn the next chapter, we’ll start to dig deeper by going over each of Python’s basic\nprocedural statements in depth. As you’ll see, though, all statements follow the\nsame general rules introduced here.\nTest Your Knowledge: Quiz\n1. What three things are required in a C-like language but omitted in\nPython?\n2. How is a statement normally terminated in Python?\n3. How are the statements in a nested block of code normally associated in\nPython?\n4. How can you make a single statement span multiple lines?\n5. How can you code a compound statement on a single line?\n6. Is there any valid reason to type a semicolon at the end of a statement in\nPython?\n7. What is a try statement for?\n8. What is the most common coding mistake among Python beginners?\nTest Your Knowledge: Answers",
      "content_length": 1369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": 9,
      "content": "1. C-like languages require parentheses around the tests in some\nstatements, semicolons at the end of each statement, and braces around\na nested block of code. Python requires none of these (but adds a :).\n2. The end of a line terminates the statement that appears on that line.\nAlternatively, if more than one statement appears on the same line, they\ncan be separated with semicolons; similarly, if a statement spans many\nlines, you must terminate it by closing a bracketed syntactic pair.\n3. The statements (code lines) in a nested block are all indented the same\nnumber of tabs or spaces.\n4. You can make a statement span many lines by enclosing part of it in\nparentheses, square brackets, or curly braces; the statement ends when\nPython sees a line that contains the closing part of the pair.\n5. The body of a compound statement can be moved to the header line\nafter the colon, but only if the body consists of only noncompound\nstatements.\n6. Only when you need to squeeze more than one statement onto a single\nline of code. Even then, this works only if all the statements are\nnoncompound, and it’s discouraged because it can lead to code that is\ndifficult to read.\n7. The try statement is used to catch and recover from exceptions (errors)\nin a Python script. It’s often an alternative to manually checking for\nerrors in code.\n8. Forgetting to type the colon character at the end of the header line in a\ncompound statement is the most common beginner’s mistake. If you’re\nnew to Python and haven’t made it yet, you probably will soon!",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": 9,
      "content": "Chapter 11. Assignments,\nExpressions, and Prints\nNow that we’ve had a first introduction to Python statement syntax, this chapter\nbegins our in-depth tour of specific Python statements. We’ll begin with the\nbasics: assignment statements, expression statements, and print operations.\nWe’ve already seen all of these in action, but here we’ll fill in important details\nwe’ve skipped so far. Although they’re relatively simple, as you’ll see, there are\noptional variations for each of these statement types that will come in handy\nonce you begin writing realistic Python programs.\nAssignments\nWe’ve been using the Python assignment statement for a while to retain objects\nin examples. In its basic form, you write the target of an assignment on the left\nof an equals sign, and the object to be assigned on the right. The target on the left\nmay be a name or object component, and the object on the right can be an\narbitrary expression that creates an object. For the most part, assignments are\nstraightforward, but here are a few key properties to note up front:\nAssignments create object references. As discussed in Chapter 6,\nPython assignments store references to objects in names or data\nstructure components. They always create references to objects instead\nof copying the objects. Because of that, Python variables are more like\npointers than data storage areas.\nNames are created when first assigned. Python creates a variable\nname the first time you assign it a value (i.e., an object reference), so\nthere’s no need to predeclare names ahead of time. Some (but not all)\ndata structure slots are created when assigned, too (e.g., dictionary\nentries, some object attributes). Once assigned, a name is replaced with\nthe value it references whenever it appears in an expression.",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": 9,
      "content": "Names must be assigned before being referenced. It’s an error to use\na name to which you haven’t yet assigned a value. Python raises an\nexception if you try, rather than returning some sort of ambiguous\ndefault value. This turns out to be crucial in Python because names are\nnot predeclared—if Python provided default values for unassigned\nnames used in your program instead of treating them as errors, it would\nbe much more difficult for you to spot name typos in your code.\nSome operations perform assignments implicitly. In this section,\nwe’re concerned with the = statement and its := expression relative, but\nassignment occurs in many contexts in Python. For instance, you’ll see\nlater that module imports, function and class definitions, for loop\nvariables, and function arguments are all implicit assignments. Because\nassignment works the same everywhere it pops up, all these contexts\nsimply bind (i.e., assign) names and object components to object\nreferences at runtime.\nWith those preliminaries in hand, let’s move on to the code.\nAssignment Syntax Forms\nAlthough assignment is a general and pervasive concept in Python, in this\nchapter we are primarily interested in assignment statements, plus one limited\nexpression. Table 11-1 illustrates the different syntax forms available for coding\nassignments in Python.\nTable 11-1. Assignment statement and expression forms\nOperation\nInterpretation\ntarget = 'Hack'\nBasic assignment\ncode, hack = 'py', 'PY'\nTuple assignment\n[code, hack] = ['py', 'PY']\nList assignment\na, b, c, d = 'hack'\nSequence assignment",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": 9,
      "content": "a, *b = 'hack'\nExtended-unpacking assignment\ncode = hack = 'python'\nMultiple-target assignment\ncode += 1, hack *= 2\nAugmented assignments\n(python := 3.12) + 0.01\nNamed assignment expression\nThe basic form atop Table 11-1 is by far the most common: binding a single\ntarget (a name or data structure component) to a single object (an expression\nresult). In fact, you could get all your work done with this form alone. The other\ntable entries represent forms that are all optional, but that programmers often\nfind convenient in practice:\nTuple and list assignments\nThe second and third forms in the table are related. When you code a tuple or\nlist on the left side of the =, Python pairs objects on the right side with\ntargets on the left by position and assigns them from left to right. For\nexample, in the second line of Table 11-1, both sides are tuples (sans\nparentheses), and the names code and hack are assigned 'py' and 'PY',\nrespectively. The left of the = is special syntax but the right is a real object,\nwhich is why this is called “unpacking” assignment—components on the\nright are unpacked into targets on the left.\nSequence assignment\nTuple and list assignments were later generalized into instances of what we\nnow call sequence assignment—any sequence of targets can be assigned to\nany sequence (really, iterable) of values, and Python assigns the items one at\na time by position. We can even mix and match the types of the sequences\ninvolved. The fourth line in Table 11-1, for example, pairs a tuple of names\nwith a string of characters: a is assigned 'h', b is assigned 'a', and so on.",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": 9,
      "content": "Despite this flexibility, the item on the left of the = is still a tuple or list of\nassignment targets.\nExtended-unpacking assignment\nAn even later assignment form allows more flexibility in how we assign\nportions of a sequence—or other iterable—to a sequence of targets. The fifth\nline in Table 11-1, for example, matches a with the first character in the\nstring on the right, and the starred name b with the rest: a is assigned 'h',\nand b is assigned ['a', 'c', 'k']. This provides an alternative to\nassigning the results of slicing operations. Starred collectors like this have\nalso somewhat usurped the term unpacking, though this is an artifact more\nhistorical than technical.\nMultiple-target assignment\nThe sixth line in Table 11-1 shows the multiple-target form of assignment. In\nthis form, Python assigns a reference to the same object (the object farthest\nto the right) to all the targets on the left. In the table, the names code and\nhack are both assigned references to the same string object, 'python'. The\neffect is the same as if we had run hack = 'python' followed by code =\nhack, as hack evaluates to the original string object (i.e., not a separate copy\nof that object).\nAugmented assignments\nThe second-to-last line in Table 11-1 is an example of augmented assignment\n—a shorthand that combines an expression and an assignment in a concise\nway. Saying code += 1, for example, has the same effect as code = code +\n1, but the augmented form requires less typing and is generally quicker to\nrun. In addition, if the target of the assignment is mutable, an augmented",
      "content_length": 1580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": 9,
      "content": "assignment may run even quicker by choosing an in-place update operation\ninstead of an object copy. As you’ll see, there is one augmented assignment\nstatement for most binary expression operators in Python (even the unused\n@!).\nNamed assignment expression\nNew in Python 3.8, the := operator allows you to code assignment as an\nexpression, which returns the value it assigns to a name. This expression can\nbe nested in places where assignment statements don’t work syntactically,\nand in common roles allows you to both assign a name and use its value in\nthe same place in your code.\nBasic Assignments\nLet’s turn to examples at the REPL prompt as usual. We’ve already used\nTable 11-1’s basic assignment in this book, so you should be familiar with its\nbasics. In short, it assigns a single target to a single value, where the target may\nbe a name, index, slice, or attribute, and the value is any expression:\n$ python3\n>>> L = [1, 2]             # Name target\n>>> L[0] = 3               # Index target\n>>> L[-1:] = [4, 5]        # Slice target\n>>> L\n[3, 4, 5]\nAttribute targets crop up for classes; we haven’t studied these yet, but the\nassignment is straightforward:\nobject.attr = L            # Attribute target (see Part VI)\nThough names are common and dominate examples here, any of these four types\nof assignment targets can be used in any form of assignment, except where noted\nahead (named assignment expressions, for example, allow only names).",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": 9,
      "content": "NOTE\nName annotations: Basic assignment statements can also have an annotation expression\nintroduced by a colon immediately after the target on the left. This statement form allows only\na single target, and is used for type hinting, described near the end of Chapter 6. As noted\nthere, because this is optional, convoluted, completely unused by Python, and fundamentally at\nodds with the language’s core idiom, we’re skipping it in this book. See Python’s docs for\nmore details if you ever stumble in the wild onto the curious case of type declarations in a\ndynamically typed language.\nSequence Assignments\nNext up, here are a few simple and comparable examples of tuple and list\nassignment (a.k.a. sequence assignment) in action, unpacking items into\nindividual variables:\n>>> first  = 1                      # Basic assignment\n>>> second = 2\n>>> A, B = first, second            # Tuple assignment\n>>> A, B                            # Similar to A = first; B = second\n(1, 2)\n>>> [C, D] = [first, second]        # List assignment\n>>> C, D\n(1, 2)\nNotice that we really are coding two tuples in the third line in this interaction—\nwe’ve just omitted their enclosing parentheses. Python pairs the values in the\ntuple on the right side of the assignment operator with the variables in the tuple\non the left side and assigns the values one at a time. The same goes when = is\nsurrounded by lists.\nTuple assignment leads to a common coding trick in Python that was introduced\nin a solution to the exercises at the end of Part II. Because Python creates a\ntemporary tuple that saves the original values of the variables on the right while\nthe statement runs, unpacking assignments are also an easy way to swap two\nvariables’ values without creating a temporary variable of your own—the tuple\non the right remembers the prior values of the variables automatically:\n>>> first  = 1",
      "content_length": 1871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": 9,
      "content": ">>> second = 2\n>>> first, second = second, first    # Tuples: swaps values\n>>> first, second                    # Like T = first; first = second; second = T\n(2, 1)\nAs already noted, the original tuple and list assignment forms in Python were\neventually generalized to accept any type of sequence (really, iterable) on the\nright as long as it is of the same length as the sequence on the left. You can\nassign a tuple of values to a list of variables, a string of characters to a tuple of\nvariables, and so on. In all cases, Python assigns items in the sequence on the\nright to targets in the sequence on the left by position—from left to right:\n>>> [a, b, c] = (1, 2, 3)          # Assign tuple of values to list of names\n>>> a, c\n(1, 3)\n>>> (a, b, c) = 'ABC'              # Assign string of characters to tuple names\n>>> a, c\n('A', 'C')\nMore broadly, while the left side of a sequence assignment is still a sequence (a\ntuple or list of targets), the right side may be any iterable object, not just any\nsequence. This is a more general category that includes collections both physical\n(e.g., lists) and virtual (e.g., a file’s lines), which was first defined in Chapter 4\nand has popped up in passing ever since. We’ll firm up this term when we\nexplore iterables in Chapters 14 and 20, and apply it to unpack a range iterable\nin the next section. For now, the “sequence” in assignment is best associated\nwith what’s on the left of the =.\nAdvanced sequence-assignment patterns\nAlthough we can mix and match sequence types around the = symbol, we must\ngenerally have the same number of items on the right as we have targets on the\nleft, or we’ll get an error. As you’ll see in the next section, Python allows us to\nbe more general with extended-unpacking * syntax, but the number of items in\nthe assignment target and subject must normally match:\n>>> string = 'TEXT'\n>>> a, b, c, d = string                            # Same number on both sides\n>>> a, b, c, d\n('T', 'E', 'X', 'T')",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": 9,
      "content": ">>> a, b, c = string                               # Error if not\nValueError: too many values to unpack (expected 3)\nTo be more flexible, we can always slice. There are a variety of ways to employ\nslicing to make this last case work:\n>>> a, b, c = string[0], string[1], string[2:]     # Index and slice\n>>> a, b, c                                        # a, b, c = ('T', 'E', 'XT')\n('T', 'E', 'XT')\n>>> a, b, c = list(string[:2]) + [string[2:]]      # Slice and concatenate\n>>> a, b, c                                        # a, b, c = ['T', 'E', 'XT']\n('T', 'E', 'XT')\n>>> a, b = string[:2]                              # Slice and unpack directly\n>>> c = string[2:]                                 # a, b = 'TE'; c = 'XT'\n>>> a, b, c\n('T', 'E', 'XT')\n>>> (a, b), c = string[:2], string[2:]             # Nested sequences\n>>> a, b, c                                        # (a, b), c = 'TE', 'XT'\n('T', 'E', 'XT')\nAs the last example in this interaction demonstrates, we can even assign nested\nsequences, and Python unpacks their parts according to their shape, as expected.\nIn this case, we are assigning a tuple of two items, where the first item is a\nnested sequence (a string), exactly as though we had coded it this way:\n>>> ((a, b), c) = ('TE', 'XT')                     # Paired by shape and position\n>>> a, b, c\n('T', 'E', 'XT')\nPython pairs the first string on the right ('TE') with the first tuple on the left\n((a, b)) and assigns one character at a time, before assigning the entire second\nstring ('XT') to the variable c all at once. In this event, the sequence-nesting\nshape of the object on the left must match that of the object on the right. Nested\nsequence assignment like this is somewhat rare to see, but it can be convenient\nfor picking out the parts of data structures with known shapes.\nFor example, you’ll see in Chapter 13 that this technique also works in for\nloops, because loop items are assigned to the target given in the loop header just",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": 9,
      "content": "as if an = statement had been run:\nfor (a, b, c) in [(1, 2, 3), (4, 5, 6)]: …          # Simple tuple assignment\nfor ((a, b), c) in [((1, 2), 3), ((4, 5), 6)]: …    # Nested tuple assignment\nSuch nested sequence assignments can also be achieved with the match\nstatement of the next chapter, but we’ll hold back the details until then.\nSequence-unpacking assignments also give rise to another common coding\nidiom in Python—assigning an integer series to a set of variables:\n>>> red, green, blue = range(3)\n>>> red, blue\n(0, 2)\nThis code initializes the three names on the left of = to the integers 0, 1, and 2,\nrespectively. Because each name gets a unique value, it’s Python’s simplest\nequivalent to the enumerated data types you may have seen in other languages.\nTo make sense of this, you need to recall that the range built-in function returns\nan iterable, which generates a list of successive integers (wrap it in a list if you\nwish to display its values at the REPL all at once like this):\n>>> list(range(3))                       # list() required for display\n[0, 1, 2]\nThis call was previewed briefly in Chapter 4; because range is commonly used\nin for loops, we’ll say more about it in Chapter 13. Another place you may see a\ntuple assignment at work is for splitting a sequence into its front and the rest, in\nloops like the following’s while, introduced in the prior chapter:\n>>> L = [1, 2, 3, 4]\n>>> while L:                             # Reapeat until L becomes empty (false)\n...     front, L = L[0], L[1:]           # See next section for * alternative\n...     print(front, L)\n...\n1 [2, 3, 4]\n2 [3, 4]\n3 [4]\n4 []",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": 9,
      "content": "The tuple assignment in the loop here could be coded as the following two lines\ninstead, but it’s often more convenient to string them together:\n...     front = L[0]\n...     L = L[1:]\nNotice that this code is using the list as a sort of stack data structure, which can\noften also be achieved with the append and pop methods of list objects; here,\nfront = L.pop(0) would have much the same effect as the tuple assignment\nstatement, but it would be an in-place change. You’ll learn more about while\nloops, and other (and often better) ways to step through a sequence with for\nloops, in Chapter 13.\nExtended-Unpacking Assignments\nThe prior section demonstrated how to use manual slicing to make sequence\nassignments more general. In later Pythons, sequence assignment was further\nextended to make this easier. In short, a starred target, *X, can be used on the left\nof = in order to specify a more general matching against the iterable on the right\n—the starred target is assigned a list, which collects all items in the iterable not\nassigned to other targets. This is especially handy for common coding patterns\nsuch as splitting a sequence into its “front” and “rest,” as in the preceding\nexample.\nExtended unpacking in action\nLet’s jump right into an example to demo how this works. As already shown,\nsequence assignments normally require exactly as many targets on the left as\nthere are items in the object on the right. We get an error if the lengths disagree,\nunless we manually slice on the right, as shown in the prior section:\n>>> seq = [1, 2, 3, 4]\n>>> a, b, c, d = seq\n>>> a, d\n(1, 4)\n>>> a, b = seq\nValueError: too many values to unpack (expected 2)",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": 9,
      "content": "We can, however, use a single starred target in this example to match more\ngenerally. In the following continuation of our interactive session, a matches the\nfirst item in the sequence, and b matches the rest:\n>>> a, *b = seq\n>>> a\n1\n>>> b\n[2, 3, 4]\nWhen a starred target is used (like name b here), the number of items on the left\nneed not match the length of the iterable on the right (like sequence seq here). In\nfact, the starred target can appear anywhere on the left. For instance, in the next\ninteraction b matches the last item in the sequence, and a matches everything\nbefore the last:\n>>> *a, b = seq\n>>> a\n[1, 2, 3]\n>>> b\n4\nWhen the starred target appears in the middle, it collects everything between the\nother targets listed. Thus, in the following interaction, a and c are assigned the\nfirst and last items, and b gets everything in between them:\n>>> a, *b, c = seq\n>>> a\n1\n>>> b\n[2, 3]\n>>> c\n4\nMore generally, wherever the starred target shows up, it will be assigned a list\nthat collects every unassigned item at that position:\n>>> a, b, *c = seq\n>>> a\n1\n>>> b",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": 9,
      "content": "2\n>>> c\n[3, 4]\nNaturally, like normal sequence assignment, extended-unpacking syntax works\nfor any sequence types (really, again, any iterable), not just lists. Here it is\nunpacking characters in a string and an iterable range—which generates values\n0…3 on demand:\n>>> a, *b = 'hack'\n>>> a, b\n('h', ['a', 'c', 'k']) \n>>> a, *b, c = 'hack'\n>>> a, b, c\n(('h', ['a', 'c'], 'k')\n>>> a, *b, c = range(4)\n>>> a, b, c\n(0, [1, 2], 3)\nThis is similar in spirit to slicing, but not exactly the same—a sequence\nunpacking assignment always returns a list for matched items, whereas slicing\nreturns a sequence of the same type as the object sliced:\n>>> S = 'hack' \n>>> a, b, c = S[0], S[1:-1], S[-1]      # Slices are type specific\n>>> a, b, c\n('h', 'ac', 'k')\n>>> a, *b, c = S                        # But * always returns a list\n>>> a, b, c\n('h', ['a', 'c'], 'k')\nFinally, the closing example of the prior section becomes even simpler with this\nextension, since we don’t have to manually slice to get the first and rest of the\nitems (though the “rest” L always becomes a list after the first *):\n>>> L = [1, 2, 3, 4]\n>>> while L:\n...     front, *L = L                   # Get first, rest without slicing\n...     print(front, L)",
      "content_length": 1216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": 9,
      "content": "...\n1 [2, 3, 4]\n2 [3, 4]\n3 [4]\n4 []\nBoundary cases\nAlthough extended unpacking is flexible, some boundary (atypical) cases are\nworth noting. First, the starred target may match just a single item, but is always\nassigned a list:\n>>> seq = [1, 2, 3, 4]\n>>> a, b, c, *d = seq\n>>> print(a, b, c, d)\n1 2 3 [4]\nSecond, if there is nothing left to match the starred target, it is assigned an empty\nlist, regardless of where it appears. In the following, names a, b, c, and d have\nmatched every item in the sequence, but Python assigns e an empty list instead\nof treating this as an error case:\n>>> a, b, c, d, *e = seq\n>>> print(a, b, c, d, e)\n1 2 3 4 []\n>>> a, b, *e, c, d = seq\n>>> print(a, b, c, d, e)\n1 2 3 4 []\nErrors can still be triggered, though, if there is more than one starred target, if\nthere are too few values and no star (as before), and if the starred target is not\nitself coded inside a sequence:\n>>> a, *b, c, *d = seq\nSyntaxError: multiple starred expressions in assignment\n>>> a, b = seq\nValueError: too many values to unpack (expected 2)\n>>> *a = seq\nSyntaxError: starred assignment target must be in a list or tuple",
      "content_length": 1131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": 9,
      "content": ">>> *a, = seq         # A one-item tuple sans parentheses\n>>> a                 # Same as a = seq, but makes a new copy!\n[1, 2, 3, 4]\nTechnically, the single-appearance rule for starred targets on the left of = really\napplies only to each sequence when there is nesting on both sides—though you\nstill can’t have more than one per nested sequence:\n>>> [(a, *b), (c, *d)] = [(1, 2, 3, 4), (5, 6, 7, 8)]\n>>> a, b, c, d\n(1, [2, 3, 4], 5, [6, 7, 8])\n \n>>> [(a, *b, *x), (c, *d)] = [(1, 2, 3, 4), (5, 6, 7, 8)]\nSyntaxError: multiple starred expressions in assignment\nAnd finally, any assignment target can be starred—though you may be hard-\npressed to find some in real code:\n>>> a, *L = [1, 2, 3]          # Name target \n>>> a, L\n(1, [2, 3])\n>>> a, *L[0] = [4, 5, 6]       # Index target\n>>> a, L\n(4, [[5, 6], 3])\n>>> a, *L[1:] = [7, 8, 9]      # Slice target\n>>> a, L\n(7, [[5, 6], 8, 9])\n>>> class C: …code…            # See Part VI\n>>> a, *C.attr = 'yikes'       # Attribute target\n>>> a, C.attr\n('y', ['i', 'k', 'e', 's'])\nA useful convenience\nKeep in mind that extended-unpacking assignment is just a convenience (well, in\naddition to a mouthful). We can usually achieve the same effects with explicit\nindexing and slicing, but extended unpacking is simpler to code. The common\n“first, rest” splitting coding pattern, for example, can be coded either way, but\nslicing is extra work:\n>>> seq",
      "content_length": 1389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": 9,
      "content": "[1, 2, 3, 4]\n>>> a, *b = seq                        # First, rest\n>>> a, b\n(1, [2, 3, 4])\n>>> a, b = seq[0], seq[1:]             # First, rest sans *\n>>> a, b\n(1, [2, 3, 4])\nThe also-common “rest, last” splitting pattern can similarly be coded either way,\nbut extended-unpacking syntax again requires noticeably fewer keystrokes (and\nnotably fewer neurons):\n>>> *a, b = seq                        # Rest, last\n>>> a, b\n([1, 2, 3], 4)\n>>> a, b = seq[:-1], seq[-1]           # Rest, last sans *\n>>> a, b\n([1, 2, 3], 4)\nBeing both simpler and arguably more natural, extended-unpacking syntax is\nalso common in Python code.\nApplication to for loops\nBecause the loop variable in the for loop statement can be any assignment\ntarget, extended unpacking works here too. We used the for loop iteration tool\nbriefly in Chapter 4 and will study it formally in Chapter 13, but its tie-in here is\nstraightforward: extended-unpacking assignments may show up after the word\nfor, where a simple variable name is more commonly used:\nfor (a, *b, c) in [(1, 2, 3, 4), (5, 6, 7, 8)]: …\nWhen used in this context, on each iteration Python simply assigns the next tuple\nof values to the tuple of targets. On the first loop, for example, it’s as if we’d run\nthe following assignment statement:\na, *b, c = (1, 2, 3, 4)                            # b gets [2, 3]",
      "content_length": 1337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": 9,
      "content": "The names a, b, and c can be used within the loop’s code to reference the\nextracted components. In fact, this is really not a special case at all, but just an\ninstance of general assignment at work. As we saw earlier in this chapter, for\nexample, we can do similar in loops with simple sequence (tuple) assignment:\nfor (a, b, c) in [(1, 2, 3), (4, 5, 6)]: …          # a, b, c = (1, 2, 3), …\nAnd we can always emulate extended-unpacking assignment behavior by\nmanually slicing:\nfor all in [(1, 2, 3, 4), (5, 6, 7, 8)]: …\n    a, b, c = all[0], all[1:-1], all[-1]\nSince we haven’t learned enough to get more detailed about the syntax of for\nloops, we’ll put this topic on the back burner until its reprise in Chapter 13.\nTHE MANY STARS OF PYTHON\nNo, this sidebar is not about personalities (which already permeate software\nculture more than they should). It’s a reference to all the special-case\nappearances of * that have cropped up in Python over the years, many of\nwhich you’ve already seen. Prior to Python 3.5, the special starred-item\nsyntax forms can appear in:\nAssignments, where a single *X starred assignment target of any\nkind (name, index, slice, attribute), and repeatable in nested parts,\ncollects unmatched items in a new list, as covered in this chapter\nFunction headers, where single *X and **X starred names collect\nunmatched positional and keyword arguments in a tuple and\ndictionary, respectively\nFunction calls, where single *X and **X starred expressions unpack\niterables and mappings into individual positional and keyword\narguments, respectively\nAs of Python 3.5, function calls (the latter listed item) support any number of\n*X and **X unpacking expressions, and any number of *X and **X starred",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": 9,
      "content": "expressions can also appear in object literals, where they unpack collections\nof types implied by the literal in which they appear:\n[x, *iter]       # List:  unpack items in iterables\n(x, *iter, y)    # Tuple: ditto, parentheses or not\n{*iter, x}       # Set:   ditto, though values are unordered and unique\n{x: y, **map}    # Dict:  unpack keys/values in mappings, rightmost dup wins\nAs of Python 3.10, both *X and **X starred names can also show up in the\npatterns of the match statement covered in the next chapter, where they serve\nroles similar to that in sequence assignment, though only *X overlaps\nbetween the two, and its assignment in match is really a side effect of a true-\nor-false test.\nAnd as of Python 3.11, an except* can appear in try statements, where it\nallows multiple handlers to be run to process multiple exceptions wrapped in\nan exception group—an addition that bifurcates the already-convoluted try\nfor narrow roles, which we won’t meet until Chapter 35.\nAll that being said, the avenue of stars has been a meandering walk in\nPython, and it’s not impossible that other parts of the language will rise to\nstardom in the future. Consult the stars for more info.\nMultiple-Target Assignments\nSimpler than sequence assignment on first glance, a multiple-target assignment\nsimply assigns all the given targets to the same object all the way to the right.\nThe following, for example, assigns the three names (variables) a, b, and c to the\nstring 'code':\n>>> a = b = c = 'code'\n>>> a, b, c\n('code', 'code', 'code')\nThis form is equivalent to—but easier to code and lighter on line count than—\nthese three assignments:\n>>> c = 'code'",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": 9,
      "content": ">>> b = c\n>>> a = b\nMultiple-target assignment and shared references\nWhile this seems simple, keep in mind that there is just one object here, shared\nby all three variables: they all wind up referencing the same object in memory.\nThis behavior is worry-free for immutable types—for example, when initializing\na set of counters to zero (recall that variables must be assigned before they can\nbe used in Python, so you must initialize counters to zero before you can start\nadding to them):\n>>> a = b = 0\n>>> b = b + 1\n>>> a, b\n(0, 1)\nHere, changing b changes only b because numbers do not support in-place\nchanges. As long as the object assigned is immutable, it’s irrelevant if more than\none name references it.\nAs usual, though, we have to be more cautious when initializing variables to an\nempty mutable object such as a list or dictionary:\n>>> a = b = []\n>>> b.append(42)\n>>> a, b\n([42], [42])\nThis time, because a and b reference the same object, appending to it in place\nthrough b will impact what we see through a as well. This is really just another\nexample of the shared reference phenomenon we first met in Chapter 6. To avoid\nthe issue, initialize mutable objects in separate statements instead, so that each\ncreates a distinct empty object, by running a distinct literal expression:\n>>> a = []\n>>> b = []                 # a and b do not share the same object\n>>> b.append(42)\n>>> a, b\n([], [42])",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": 9,
      "content": "A tuple assignment like the following has the same effect with more brevity—by\nrunning two list-literal expressions, it creates two distinct objects:\n>>> a, b = [], []          # a and b do not share the same object\nAugmented Assignments\nIn addition to the basic, sequence (original and starred), and multiple assignment\nforms already covered, Python gained all the assignment statement formats listed\nin Table 11-2 relatively early in its career. Known as augmented assignments,\nand borrowed from the C language, these formats are mostly just shorthand: they\nimply the combination of a binary (two-operand) expression and an assignment.\nFor instance, the following two formats are functionally equivalent, though the\nlatter may use in-place options to change X directly, as you’ll see in a moment:\nX = X + Y                       # Basic form\nX += Y                          # Augmented form\nTable 11-2. Augmented assignment statements\nX += Y\nX -= Y\nX *= Y\nX /= Y\nX @= Y\nX //= Y\nX %= Y\nX **= Y\nX >>= Y\nX <<= Y\nX &= Y\nX |= Y\nX ^= Y\nAugmented assignment works on any type that supports the implied binary\nexpression. For example, here are two ways to add 1 to a name; really, they both\nchange a name to reference different values:\n>>> x = 1\n>>> x = x + 1                   # Basic\n>>> x\n2\n>>> x += 1                      # Augmented\n>>> x\n3\nWhen applied to a sequence such as a string, the augmented form performs",
      "content_length": 1412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": 9,
      "content": "concatenation instead, because that’s what + means for such objects. Thus, the\nsecond line here is equivalent to typing the longer S = S + 'HACK':\n>>> S = 'hack'\n>>> S += 'HACK'                 # Implied concatenation\n>>> S\n'hackHACK'\nAs shown in Table 11-2, there are analogous augmented assignment forms for\nother Python binary expression operators (i.e., operators with values on their left\nand right sides). For instance, X *= Y multiplies and assigns, X >>= Y shifts\nright and assigns, and so on (though per Chapter 5, the form X @= Y is unused\nand unimplemented by Python itself today). All told, augmented assignments\nhave three noteworthy advantages:1\nThere’s less for you to type. (Need this book say more?)\nThe left side has to be evaluated only once. In X += Y, X may be a\ncomplicated object expression. In the augmented form, its code must be\nrun only once. However, in the long form, X = X + Y, X appears twice\nand must be run twice. Because of this, augmented assignments usually\nrun faster.\nThe optimal technique is automatically chosen. That is, for objects that\nsupport in-place changes, the augmented forms automatically perform\nin-place change operations instead of slower copies.\nThe last point here requires a bit more explanation. For augmented assignments,\nin-place operations may be applied for mutable objects as an optimization.\nRecall that lists can be extended in a variety of ways. To add a single item to the\nend of a list, we can concatenate or call append:\n>>> L = [1, 2]\n>>> L = L + [3]                 # Concatenate one: slower\n>>> L\n[1, 2, 3]\n>>> L.append(4)                 # Faster, but in place\n>>> L\n[1, 2, 3, 4]",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": 9,
      "content": "And to add a set of items to the end, we can either concatenate again or call the\nlist extend method:\n>>> L = L + [5, 6]              # Concatenate many: slower\n>>> L\n[1, 2, 3, 4, 5, 6]\n>>> L.extend([7, 8])            # Faster, but in place\n>>> L\n[1, 2, 3, 4, 5, 6, 7, 8]\nIn both cases, concatenation is less prone to the side effects of shared object\nreferences but will generally run slower than the in-place equivalent.\nConcatenation operations must create a new object, copy in the list on the left,\nand then copy in the list on the right. By contrast, in-place method calls simply\nadd items at the end of a memory block (it can be a bit more complicated than\nthat internally, but this description suffices).\nWhen we use augmented assignment to extend a list, we can largely forget these\ndetails—Python automatically calls the quicker extend method (or its\nequivalent) instead of using the slower concatenation operation implied by +:\n>>> L += [9, 10]                # Mapped to L.extend([9, 10])\n>>> L\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nAs suggested in Chapter 6, we can also use slice assignment (e.g., L[len(L):]\n= [11,12,13]), but this works roughly the same as the simpler and more\nmnemonic list extend method or the += statement. Note, however, that because\nof this equivalence += for a list is not exactly the same as a + and = in all cases—\nfor lists += allows arbitrary sequences (just like extend), but concatenation\nnormally does not:\n>>> L = []\n>>> L += 'hack'                 # += and extend allow any sequence, but + does not!\n>>> L\n['h', 'a', 'c', 'k'] \n>>> L = L + 'code'\nTypeError: can only concatenate list (not \"str\") to list\nMoreover, using index and slice targets in augmented assignment may change",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": 9,
      "content": "mutables in other ways:\n>>> L = [1, 2]\n>>> L[0] += 10                 # Change an item of a list, in place\n>>> L\n[11, 2]\n>>> L[-1:] += [3, 4]           # Change a section of a list, in place\n>>> L\n[11, 2, 3, 4]\nAugmented assignment and shared references\nMore subtly, the in-place change implicit in += for lists is very different from the\nnew object created by + concatenation. As for all shared-reference cases, this\ndifference may matter when objects are shared by many names:\n>>> L = [1, 2]\n>>> M = L                       # L and M reference the same object\n>>> L = L + [3, 4]              # Concatenation makes a new object\n>>> L, M                        # Changes L but not M\n([1, 2, 3, 4], [1, 2])\n>>> L = [1, 2]\n>>> M = L\n>>> L += [3, 4]                 # But += really means extend, not +\n>>> L, M                        # M sees the in-place change too!\n([1, 2, 3, 4], [1, 2, 3, 4])\nThis only matters for mutables like lists and dictionaries, and it is a fairly\nobscure case (at least, until it impacts your code!). As always, make copies of\nyour mutable objects if you need to break the shared reference structure.\nNamed Assignment Expressions\nFor most of Python’s three-decade tenure, it resisted emulating the assignment-\nas-expression idiom of the C language, on the grounds that it was too subtle and\nerror-prone, and fostered code that was hard to read. While these concerns still\napply, Python 3.8 gained a flavor of this, known as named assignment.\nImportantly, this flavor does not make normal = assignment statements nestable\nexpressions, as they are in C. Instead, it adds a new expression operator to",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": 9,
      "content": "Python, :=, on the grounds that its different and limited syntax may neutralize\npitfalls of other languages. You still can’t accidentally type = when you mean ==,\nand := is visually distinct.\nWe explored this expression briefly in a spoiler note in the previous chapter. In\nshort, the expression name := value first evaluates expression value, and then\nboth:\nAssigns the result to the provided variable name\nReturns the result as the value of the overall := expression\nThere’s no reason to use this syntax for the first part alone, because all of\nPython’s = assignment statements already assign values to names (in fact, you’ll\nget a syntax error if you try using := as a statement sans parentheses). Because\n:= is an expression that returns the value assigned, though, it can be nested in\ncontexts where statements aren’t allowed and may be used in roles that both test\nand then use the assigned name.\nAs an artificial first example, the following nested := both assigns 2 to b, and\nreturns it to be used in the * string-repetition result assigned to a:\n>>> a = 'hack' * (b := 2) \n>>> a\n'hackhack'\n>>> b\n2\nAmong its deliberate limitations, named assignment allows only a simple, single\nname (variable) on the left, so neither other assignment-statement forms nor\nreferences to data-structure components work here:\n>>> (python := 3.12) + 0.01         # Just a name on the left\n3.13\n>>> python\n3.12\n>>> (python, Python := 3.12, 3.13) + 0.01\nTypeError: can only concatenate tuple (not \"float\") to tuple\n>>> (python[0] := 3.12) + 0.01\nSyntaxError: cannot use assignment expressions with subscript",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": 9,
      "content": ">>> (python.attr := 3.12) + 0.01\nSyntaxError: cannot use assignment expressions with attribute\nThe first of these failers, for example, is taken to be a three-item tuple with := in\nthe middle, not a two-target named sequence assignment. Moreover, named\nassignment does not support any of the augmented assignment forms we met\nearlier: there is no :+=, for instance, and coding statement X += 1 may actually\ntake less work than expression (X := X + 1) (subtly, a :=+ runs, but simply\napplies the + identity operator to the expression on the right).\nWhen to use named assignment\nWhile more useful contexts for := are limited, its most common use cases arise\nin concert with the if and while statements, which were both introduced in the\npreceding chapter and earlier. Without :=, it was—and still is—common to\nassign a variable before the statement, test it in the statement header, and then\nuse it in the statement body.\nFor example, code that reads lines from files and must detect the end of the file\n(which, you’ll recall from Chapter 9, means an empty string that is logically\nfalse) may look like these partial snippets (to run code in this section live, open a\ntext file for input and assign it to file before each snippet):\nline = file.readline()             # Sans the := expression \nif line:\n    print(line)\nline = file.readline()             # Ditto, in while loops\nwhile line:\n    print(line)\n    line = file.readline()\nTo avoid redundant calls, it’s also common to code the latter like this in Python\n—as in the preceding chapter:\nwhile True:                        # Sans both := and redundancy\n   line = file.readline()\n   if not line: break\n   print(line)",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": 9,
      "content": "These forms still work well, and the := is never required. With :=, however, we\ncan sometimes collapse a fetch, assignment, and test into a one-liner within\nstatement headers themselves:\nif line := file.readline():        # The := alternatives\n    print(line)\nwhile line := file.readline():     # Read all lines (vs: for line in file)\n    print(line)\nIn both cases, because := returns the results of the readline call, its logical\nvalue can be tested in the statement header itself. And because := also assigns\nthe result to line, it can be used in the statement’s body if it is run. The upshot is\nthat := provides a sort of shortcut that allows multiple operations to be coded in\na compact way.\nIf you opt to use this expression, bear in mind that it often requires enclosing\nparentheses to avoid interacting with surrounding code. Even comparing its\nresult to an explicit value, for instance, mandates parentheses that are not\nrequired in the equivalent statements:\nif (line := file.readline()) != ignore:          # Parentheses required\n    print(line)\nwhile (line := file.readline()) != stop:         # And not a bad idea \n    print(line)\nAs a rule of thumb, if you opt to use :=, wrapping it in parentheses both sets it\noff visually and avoids sticky issues that may arise if the code surrounding it\nchanges its meaning unexpectedly. In the preceding, for example, names are\nassigned the results of the != tests if parentheses are omitted, because it binds\ntighter than := (see Chapter 9). Moreover, := has special syntax rules that we’ll\nomit here, but they encourage parenthesized usage by design.\nBesides file reads and similar roles, the := can be leveraged to reuse results in\nliterals and get last results in comprehensions and other iteration tools, and can\neven be nested in f-strings and itself—with requisite parentheses:\n>>> [val := 'Py!', val * 2, val * 3]               # Reusing a value",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": 9,
      "content": "['Py!', 'Py!Py!', 'Py!Py!Py!']\n>>> list(pow := 2 ** num for num in [2, 4, 8])     # Capturing a result\n[4, 16, 256]\n>>> pow\n256\n>>> f'Hello {(name := input('Who are you? '))}'    # Nesting in f-strings (hmm)\nWho are you? Pat\n'Hello Pat'\n>>> name\n'Pat'\n>>> (x := (y := (z := 1) + 1) + 1)                 # Nesting in itself (hmm * 2)\n3\n>>> x, y, z\n(3, 2, 1)\nAs another rule of thumb, bear in mind that a deeply nested and obscure :=\nexpression might seem clever but will be a lot harder to read—and even notice—\nthan a simple standalone assignment. You’ll be able to judge this for yourself in\nexamples in Chapter 20. Generally speaking, though, for the sake of others\n(including your future self!), use :=, like so many nestable tools, sparingly and\nwisely. Clarity is usually worth the extra line or two.\nVariable Name Rules\nNow that we’ve explored assignment statements and expressions, it’s time to get\nmore formal about the use of variable names. In Python, names come into\nexistence when you assign values to them, but there are a few rules to follow\nwhen choosing names for the subjects of your programs:\nSyntax: (underscore or letter) + (any number of letters, digits, or underscores)\nVariable names must start with an underscore or letter, which can be\nfollowed by any number of letters, digits, or underscores. _hack, hack, and\nHack_1 are legal names, but 1_hack, hack$, and @#! are not.\nPython also allows non-ASCII Unicode characters to appear in variable\nnames, but such characters may make your code difficult to use in some\ncontexts and are subject to a handful of rules that require deep Unicode\nknowledge and are too arcane to cover here. See Chapter 37’s note “Unicode",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": 9,
      "content": "in variable names” for expanded coverage of this topic, and Python’s\nlanguage manuals for full details.\nCase matters: HACK is not the same as hack\nPython always pays attention to case in programs, both in names you create\nand in reserved words. For instance, the names X and x refer to two different\nvariables. For portability, case also matters in the names of imported module\nfiles, even on platforms where the filesystems are case-insensitive (as is\ncommon in Windows). That way, your imports still work after programs are\ncopied to differing platforms.\nReserved words are off-limits\nNames you define cannot be the same as words that mean special things in\nthe Python language. For instance, Python will raise a syntax error if you try\nto use class as a variable name, but klass and Class work fine. Table 11-3\nlists the words that are currently reserved—and hence off-limits for names of\nyour own—in Python. Note: Python’s docs call these “keywords” today,\ndespite the longstanding and differing use of “keywords” for pass-by-name\nfunction arguments; to avoid confusion, this book uses “reserved words” for,\nwell, reserved words.\nTable 11-3. Python reserved words\nFalse\nawait\nelse\nimport\npass\nNone\nbreak\nexcept\nin\nraise\nTrue\nclass\nfinally\nis\nreturn\nand\ncontinue\nfor\nlambda\ntry\nas\ndef\nfrom\nnonlocal\nwhile",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": 10,
      "content": "assert\ndel\nglobal\nnot\nwith\nasync\nelif\nif\nor\nyield\nIn addition to Table 11-3 (and as partly leaked in Chapter 10), the words match,\ncase, _, and type are soft reserved words: they are reserved only in the context\nof the statement to which they belong and can be used as variables anywhere\nelse. As you’ll learn in the next chapter, the first three are part of a multiple-\nchoice match statement; the latter is used by a type statement that creates type\naliases, used for the type hinting discussed at the end of Chapter 6.\nAs you can see, most of Python’s reserved words are all lowercase. They are also\nall truly reserved—unlike names in the built-in scope that you will meet in the\nnext part of this book, you cannot redefine reserved words by assignment. The\nstatement and = 1, for instance, results in a syntax error.\nBesides being of mixed case, the first three entries in Table 11-3—False, None,\nand True—are somewhat unusual in meaning: they also appear in the built-in\nscope of Python described in Chapter 17, and they are technically names\nassigned to objects. They are truly reserved in all other senses, though, and\ncannot be used for any other purpose in your script other than that of the objects\nthey represent. All the other reserved words are hardwired into Python’s syntax\nand can appear only in the specific contexts for which they are intended.2\nFurthermore, because module names in import statements become variables in\nyour scripts, variable name constraints extend to your module filenames too. For\ninstance, you can code files called and.py and my-code.py and can run them as\ntop-level scripts, but you cannot import them: their names without their .py\nextensions become variables in your code on imports, and so must follow all the\nvariable rules just outlined. Hence, reserved words are off-limits, and dashes\nwon’t work, though underscores will. This module idea will be revisited in\nPart V of this book, where you’ll find that this constraint also applies to names\nof “package” folders by extension.\nPYTHON’S DEPRECATION POLICIES\nBecause making new words reserved has the potential to break code widely,",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": 10,
      "content": "such changes are generally phased into the language gradually. When a\nchange might impact existing code, Python often makes it an option and\nbegins issuing deprecation warnings one or more releases before the mod is\nofficially enabled. The idea is that you should have time to notice the\nwarnings and update your code before upgrading your Python. More\nrecently, the “soft” reserved-word category has arisen to allow new reserved\nwords to be added with neither warnings nor breakages. This seems a tacit\nrecognition that deprecations only go so far.\nDeprecation policies are not always followed, especially for major releases\nlike 3.0 (which broke existing code wantonly), but also for newer changes\ndeemed to be justifiable or innocuous by their promoters. In theory, changes\nare overseen by a steering committee charged with enforcing deprecation\npolicies. While this helps in some cases, it is largely ineffectual in stemming\nthe flood of opinionated morph and bloat in Python, which makes\nengineering reliable and durable software more challenging than it should\nbe.\nEven when deprecation warnings are issued, though, programs that cannot\nbe easily changed will break if they have been distributed in source code\nform to users who innocently upgrade their local Python. Those on the\nreceiving end of such flux may legitimately see deprecation warnings less as\na courtesy than a bandage on a gaping wound. Warning that you’re going to\nbe rude doesn’t make it OK to be rude!\nNaming conventions\nBesides these rules, there is also a set of naming conventions—rules that are not\nrequired but are followed in normal practice. For instance, because names with\ntwo leading and trailing underscores (e.g., __name__) generally have special\nmeaning to the Python interpreter, you should avoid this pattern for your own\nnames except in contexts where it is expected. Here is a list of the conventions\nPython follows:\nNames that begin with a single underscore (_X) are not imported by a\nfrom module import * statement, described in Chapter 23.",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": 10,
      "content": "Names that have two leading and trailing underscores (__X__) are\nsystem-defined names that have special meaning to the interpreter and\nprovide implementation details in the user-defined OOP classes of\nPart VI.\nNames that begin with two underscores and do not end with two more\n(__X) are localized (“mangled”) to enclosing classes, per the discussion\nof pseudoprivate attributes in Chapter 31.\nThe name that is just a single underscore (_) retains the result of the last\nexpression when you are working interactively at some REPLs and is a\nsoft keyword for a wildcard in match, as covered in Chapter 12.\nIn addition to these Python interpreter conventions, there are various other\nconventions that Python programmers usually follow. For instance, some\nprogrammers distinguish parts of long names using “camelCase” (aLongName),\nand others use underscores (a_Long_name); either is completely valid according\nto both Python and this book.\nLater in the book, you’ll also see that class names commonly start with an\nuppercase letter and module names with a lowercase letter, and that the name\nself, though not reserved, usually has a special role in classes. Moreover, in\nChapter 17 we’ll study another, larger category of names known as the built-ins,\nwhich are predefined but not reserved (and so can be reassigned: open = 99\nsilently works, though you might occasionally wish it didn’t!).\nNames have no type, but objects do\nThis is mostly review, but remember that it’s crucial to keep Python’s distinction\nbetween names and objects clear. As described in Chapter 6, objects have a type\n(e.g., integer, list) and may be mutable or not. Names (a.k.a. variables), on the\nother hand, are always just references to objects; they have no notion of\nmutability and have no associated type information, apart from the type of the\nobject they happen to reference at a given point in time.\nThus, it’s OK to assign the same name to different kinds of objects at different\ntimes:\n>>> x = 0               # x bound to an integer object",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": 10,
      "content": ">>> x = 'Hello'         # Now it's a string\n>>> x = [1, 2, 3]       # And now it's a list\nEspecially when we step up to functions and classes later in this book, you’ll see\nthat this generic nature of names can be a decided advantage in Python\nprogramming. In Chapter 17, you’ll also learn that names live in something\ncalled a scope, which defines where they can be used; the place where you\nassign a name determines where it is visible.3\nExpression Statements\nIn Python, you can use any expression as a statement, too—which usually means\non a line by itself. Because the result of the expression won’t be saved, though, it\nusually makes sense to do so in scripts only if the expression does something\nuseful as a side effect. Expressions are commonly used as statements in two\nsituations:\nFor calls to functions and methods\nSome functions do their work without returning a value. Such functions are\nsometimes called procedures in other languages. Because they don’t return\nvalues that you might be interested in retaining, you can call these functions\nwith expression statements. This also applies to methods, which are just\nfunctions with an implied subject.\nFor printing values at the interactive prompt\nAs you certainly know by now, Python echoes back the results of\nexpressions typed at the interactive command line. Technically, these are\nexpression statements, too; they serve as a shorthand for typing print\nstatements.\nTable 11-4 lists some common expression statement forms in Python. Calls to\nfunctions and methods are coded with zero or more argument objects (really,\nexpressions that evaluate to objects) in parentheses, after the function or method",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": 10,
      "content": "name.\nTable 11-4. Common Python expression statements\nOperation\nInterpretation\nhack('Py', 3.12)\nFunction calls\ncode.hack('Py')\nMethod calls\nhack\nPrinting results in the interactive interpreter (REPL)\nprint(a, b, c, sep='')\nPrinting operations (a special function call)\nyield x ** 2\nYielding expression statements (generators)\nawait producer()\nPausing for steps to finish (coroutines)\nThe last three entries in Table 11-4 are somewhat special cases. As you’ll see\nlater in this chapter, printing in Python is a function call usually coded as a\nstatement by itself, and important enough to callout here. The yield and await\noperations on generator and coroutine functions (discussed in Chapter 20) are\nregularly coded as statements as well. All three, though, are really just instances\nof expressions masquerading as statements.\nFor instance, though you normally run a print call on a line by itself as an\nexpression statement, it actually returns a value like any other function call—the\nreturn value is None, the default return value for functions that don’t return\nanything meaningful (it requires another print to reveal in output):\n>>> x = print('code')         # print is a function-call expression\ncode\n>>> print(x)                  # But is usually coded as an expression statement\nNone\nAlso keep in mind that although expressions can appear as statements in Python,\nthe converse is not true: statements cannot be used as expressions. A statement\nthat is not also an expression must generally appear on a line all by itself, not",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": 10,
      "content": "nested in a larger syntactic structure.\nFor example, Python doesn’t allow you to embed basic assignment statements\n(=) in other expressions. The rationale for this is that it avoids common coding\nmistakes; you can’t accidentally change a variable by typing = when you really\nmean to use the == equality test. If you really miss this coding pattern from other\nlanguages, though, the newer and visually distinct := named-assignment\nexpression we met earlier works the same way with less chance of being\nconfused with ==, and a variety of coding alternatives achieve similar goals (e.g.,\nsee while coverage in Chapter 13).\nExpression Statements and In-Place Changes\nThis brings up a common mistake in Python work, which we’ve encountered\nbefore, but is so pervasive that it merits another quick nag here. Expression\nstatements are often used to run list methods that change a list in place:\n>>> L = [1, 2]\n>>> L.append(3)               # Append is an in-place change\n>>> L\n[1, 2, 3]\nIt’s not unusual, though, for Python newcomers to code this as an assignment\nstatement instead:\n>>> L = L.append(4)           # But append returns None, not L\n>>> print(L)                  # So we lose our list!\nNone\nBut this doesn’t work: in-place change methods like append, sort, and reverse\nalways change the list in place, but do not return the list they have changed;\ninstead, they return the None object. Assigning such an operation’s result back to\nthe variable name loses your reference to the list (and it’s probably garbage-\ncollected in the process).\nSo don’t do that—call in-place change operations without assigning their results.\nWe’ll revisit this phenomenon in “Common Coding Gotchas”, because it can\nalso appear in the context of some looping statements we’ll explore in the",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": 10,
      "content": "chapters ahead.\nPrint Operations\nIn Python, print prints things—it’s simply a programmer-friendly interface to\nthe standard output stream. It’s a function-call expression that we’re calling out\nhere because it’s so pervasive and is usually coded as a statement.\nTechnically, printing converts one or more objects to their textual\nrepresentations, adds some minor formatting, and sends the resulting text to\neither standard output or another file-like stream. In a bit more detail, print is\nstrongly bound up with the notions of files and streams in Python:\nFile object methods\nIn Chapter 9, we explored file object methods that write text (e.g.,\nfile.write(str)). Printing operations are similar, but more focused—\nwhereas file write methods write strings to arbitrary files, print writes\nobjects to the stdout stream by default, with some automatic conversion and\nformatting added. Unlike with file methods, there is no need to convert\nobjects to strings when using print operations.\nStandard output stream\nThe standard output stream (often known as stdout) is simply a default\nplace to send a program’s text output. Along with the standard input and\nerror streams, it’s one of three data connections created when your script\nstarts. The standard output stream is usually mapped to the window where\nyou started your Python program or REPL, unless it’s been redirected to a\nfile or pipe in your operating system’s shell or rerouted by your coding GUI.\nBecause the standard output stream is available in Python as the stdout file\nobject in the built-in sys module (i.e., sys.stdout), it’s possible to emulate\nprint with file write method calls. However, print is noticeably simpler to\nuse in most roles and makes it easy to print text to other files and streams.",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": 10,
      "content": "NOTE\nBlast from the past: Printing is also one of the most visible places where Python 3.X and 2.X\ndiverged: it morphed from statement in 2.X to built-in function in 3.X—and broke nearly every\nexisting Python program in the process. This book no longer covers 2.X’s statement, but the\nlegacy of 3.X’s backward incompatibilities lives on as a natural brake on language changes\n(and in the muscle memory of Python coders who still type print x unconsciously!).\nThe print Function\nStrictly speaking, printing is not a separate statement form. Instead, it is simply\nan instance of the expression statement we studied in the preceding section. The\nprint built-in function is normally called on a line of its own, because it doesn’t\nreturn any value we care about (technically, it returns None, per the preceding\nsection). Because it is a normal function, though, it can use standard function-\ncall syntax (including keyword arguments for special operation modes) and may\nbe both passed around as an object and reassigned to a different implementation.\nCall format\nSyntactically, calls to the print function have the following form:\nprint([object, …][, sep=' '][, end='\\n'][, file=sys.stdout][, flush=False])\nIn this notation, items in square brackets are optional and may be omitted in a\ngiven call, and values after = give keyword-argument defaults. In English, this\nbuilt-in function prints the textual representation of one or more objects,\nseparated by the string sep and followed by the string end, to the stream file,\nflushing buffered output or not per flush.\nThe sep, end, file, and flush parts, if present, must be given as keyword\narguments—that is, you must use a name=value syntax to pass the arguments by\nname instead of position. Keyword arguments are covered in depth in\nChapter 18, but they’re straightforward to use. The keyword arguments sent to\nthis call may appear in any left-to-right order following the objects to be printed,\nand they control the print operation:\nsep is a string inserted between each object’s text, which defaults to a",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": 10,
      "content": "single space if not passed. Passing an empty string suppresses\nseparators altogether.\nend is a string added at the end of the printed text, which defaults to a\n\\n newline character if not passed. Passing an empty string avoids\ndropping down to the next output line at the end of the printed text—the\nnext print will keep adding to the end of the current output line.\nfile specifies the file, standard stream, or other file-like object to\nwhich the text will be sent. It defaults to the sys.stdout standard\noutput stream if not passed, but any object with a file-like\nwrite(string) method may be passed, including real file objects that\nhave already been opened for output to external files.\nflush defaults to False. It allows prints to mandate that their text be\nflushed through the output stream immediately to any waiting\nrecipients. Normally, whether printed output is buffered in memory or\nnot is determined by the file object; passing a true value to flush\nforcibly flushes the stream.\nThe textual representation of each object to be printed is obtained by passing\nthe object to the str built-in call (or its equivalent inside Python); as we’ve seen,\nthis built-in returns a “user friendly” display string for any object.4 With no\narguments at all, the print function simply prints a newline character to the\nstandard output stream, which usually displays a blank line.\nThe print function in action\nPrinting is probably simpler than its full details may imply. To illustrate, let’s run\nsome quick examples in the REPL. The following prints a variety of object types\nto the default standard output stream, with the default separator and end-of-line\nformatting added (these are the defaults because they are the most common use\ncase):\n>>> print()                                      # Display a blank line\n>>> x = 'python'\n>>> y = 3.12\n>>> z = ['lp6e']",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": 10,
      "content": ">>> print(x, y, z)                               # Print objects per defaults\npython 3.12 ['lp6e']\nThere’s no need to convert objects to strings here, as would be required for file\nwrite methods. By default, print calls add a space between the objects printed.\nTo suppress this, send an empty string to the sep keyword argument, or send an\nalternative separator of your choosing:\n>>> print(x, y, z, sep='')                       # Suppress separator\npython3.12['lp6e']\n>>>\n>>> print(x, y, z, sep=', ')                     # Custom separator\npython, 3.12, ['lp6e']\nAlso by default, print adds an end-of-line character to terminate the output line.\nYou can suppress this and avoid the line break altogether by passing an empty\nstring to the end keyword argument, or you can pass a different terminator of\nyour own including a \\n character to break the line manually if desired (the\nsecond of the following is two statements on one line, separated by a semicolon\nto demo the effect of custom terminators in the REPL):\n>>> print(x, y, z, end='')                        # Suppress line break (see >>>)\npython 3.12 ['lp6e']>>>\n>>>\n>>> print(x, y, z, end=''); print(x, y, z)        # Two prints, same output line\npython 3.12 ['lp6e']python 3.12 ['lp6e']\n>>> print(x, y, z, end='...\\n')                   # Custom line end\npython 3.12 ['lp6e']...\n>>>\nYou can also combine keyword arguments to specify both separators and end-of-\nline strings—they may appear in any order but must appear after all the objects\nbeing printed:\n>>> print(x, y, z, sep='...', end='!\\n')          # Multiple keywords\npython...3.12...['lp6e']!\n>>> print(x, y, z, end='!\\n', sep='...')          # Order doesn't matter\npython...3.12...['lp6e']!\nHere is how the file keyword argument is used—it directs the printed text to an",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": 10,
      "content": "open output file or other compatible object for the duration of the single print\n(this is really a form of stream redirection, a topic we will revisit later in this\nsection):\n>>> print(x, y, z, sep='...', file=open('data.txt', 'w'))      # Print to a file\n>>> print(x, y, z)                                             # Back to stdout\npython 3.12 ['lp6e']\n>>> print(open('data.txt').read())                             # Display file text\npython...3.12...['lp6e']\nFinally, keep in mind that the separator and end-of-line options provided by print\noperations are just conveniences. If you need to display more specific\nformatting, don’t print this way. Instead, build up a more custom string either\nahead of time or within the print itself, using the string tools we mastered in\nChapter 7—and print the string all at once. String formatting expressions and f-\nstrings, for example, were designed for this sort of job:\n>>> text = '%s: %-.4f, %05d' % (x, y, int(z[0][-2]))\n>>> print(text)\npython: 3.1200, 00006\n>>> print(f'{x}: {y:-0.4f}, {int(z[0][-2]):05d}')\npython: 3.1200, 00006\nOn the other hand, there seems to be a misconception that f-strings are somehow\nrequired for print, and they crop up needlessly. In truth, f-strings are overkill if\nyou’re simply trying to print objects separated by spaces:\n>>> a, b, c, = 11, 3.14, 'hack'\n>>> print(f'{a} {b} {c}')\n11 3.14 hack\n>>> print(a, b, c)\n11 3.14 hack\nAs usual, don’t use a sledgehammer to drive every nail!\nPrint Stream Redirection\nAs we’ve seen, print sends text to the standard output stream by default.\nHowever, it’s often useful to send it elsewhere—to a text file, for example, to",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": 10,
      "content": "save results for later use or testing purposes. Although such redirection can\nusually be accomplished in system shells outside Python itself (with syntax like\n> file, per “Command-Line Usage Variations”), this is not a Python tool, and\nit’s just as easy to redirect a script’s streams from Python code.\nThe Python “hello world” program\nLet’s start off with the usual (and largely pointless) language benchmark—the\n“hello world” program. To print a “hello world” message in Python, simply print\nthe string with print:\n>>> print('hello world')               # Print a string object\nhello world\nReally, because expression results are echoed on the interactive command line,\nyou often don’t even need to use a print statement there—simply type the\nexpressions you’d like to have printed, and their results are echoed back:\n>>> 'hello world'                      # Interactive echoes\n'hello world'\nThis code isn’t exactly an earth-shattering feat of software mastery, but it serves\nto illustrate printing behavior. Technically, though, the print operation is just an\nergonomic feature of Python—it provides a simple interface to the sys.stdout\nobject, with a bit of default formatting. In fact, if you enjoy working harder than\nyou must (or are pining for your Java coding days) you can also code print\noperations this way, which omits the write call’s return value for space, as in\nChapter 9:\n>>> import sys                         # Printing the hard way\n>>> sys.stdout.write('hello world\\n')\nhello world\nThis code explicitly calls the write method of sys.stdout—an attribute preset\nwhen Python starts up to an open file object connected to the output stream. The\nprint operation hides most of those details, providing a simple tool for simple\nprinting tasks.",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": 10,
      "content": "Manual stream redirection\nSo, why bother learning the hard way to print? The sys.stdout equivalent to\nprint turns out to be the basis of a common technique in Python. In general,\nprint and sys.stdout are directly related as follows. This statement:\nprint(X, Y)\nis equivalent to the longer:\nimport sys\nsys.stdout.write(str(X) + ' ' + str(Y) + '\\n')\nwhich manually performs a string conversion with str, adds a separator and\nnewline with +, and calls the output stream’s write method. That is, this is what\nthe print does. Which would you rather code?\nObviously, the long form isn’t all that useful for printing by itself. However, it is\nuseful to know that this is exactly what print operations do because it is\npossible to reassign sys.stdout to something different from the standard output\nstream. In other words, this equivalence provides a way of making your print\noperations send their text to other places. For example:\nimport sys\nsys.stdout = open('log.txt', 'a')       # Redirects prints to a file\n…\nprint(x, y, x)                          # Now printed text shows up in log.txt\nHere, we reset sys.stdout to a manually opened file named log.txt, located in\nthe script’s working directory (CWD) and opened in 'a' append mode (so we\nadd to its current content). After the reset, every print operation anywhere in\nthe program will write its text to the end of the file log.txt instead of to the\noriginal output stream. The print operations are happy to keep calling\nsys.stdout’s write method, no matter what sys.stdout happens to refer to.\nAnd because there is just one sys module in your program (technically, process),\nassigning sys.stdout this way will redirect every print anywhere in your\nprogram.",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": 10,
      "content": "In fact, as the sidebar “Why You Will Care: print and stdout” will explain, you\ncan even reset sys.stdout to an object that isn’t a file at all, as long as it has the\nexpected interface: a method named write to receive the printed text-string\nargument. When that object is a class, printed text can be routed and processed\narbitrarily per a write method you code yourself.\nThis trick of resetting the output stream might be more useful for programs\noriginally coded with print statements. If you know that output should go to a\nfile to begin with, you can always call file write methods instead. To redirect the\noutput of a print-based program, though, resetting sys.stdout provides a\nconvenient alternative to changing every print statement or using system shell-\nbased redirection syntax, which may be above users’ pay grades.\nIn other roles, streams may be reset to objects that display them in pop-up\nwindows in GUIs, colorize them in IDEs like IDLE, and so on. It’s a general\ntechnique.\nAutomatic stream redirection\nAlthough redirecting printed text by assigning sys.stdout is a useful tool, a\npotential problem with the last section’s code is that there is no direct way to\nrestore the original output stream should you need to switch back after printing\nto a file. Because sys.stdout is just a normal file object, though, you can\nalways save it and restore it if needed:5\n>>> import sys\n>>> temp = sys.stdout                   # Save for restoring later\n>>> sys.stdout = open('log.txt', 'a')   # Redirect prints to a file\n>>> print('lp6e was here')              # Prints go to file, not here\n>>> print(1, 2, 3)\n>>> sys.stdout.close()                  # Flush output to disk\n>>> sys.stdout = temp                   # Restore original stream\n>>> print('back in the REPL')           # Prints show up here again\nback in the REPL\n>>> print(open('log.txt').read())       # Result of earlier prints\nlp6e was here\n1 2 3\nAs you can see, though, manual saving and restoring of the original output",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": 10,
      "content": "stream like this involves extra juggling work. Because this crops up fairly often,\na print extension is available to make it unnecessary.\nAs introduced earlier, the file keyword allows a single print call to send its\ntext to the write method of a file (or file-like object), without actually resetting\nsys.stdout. Because the redirection is temporary, normal print calls keep\nprinting to the original output stream. For example, the following abstract\nsnippet again sends printed text to a file named log.txt:\nlog = open('log.txt', 'a')\nprint(x, y, z, file=log)                # Print to a file-like object\nprint(a, b, c)                          # Print to original stdout\nThese redirected forms of print are handy if you need to print to both files and\nthe standard output stream in the same program. If you use these forms,\nhowever, be sure to give them a file object (or an object that has the same write\nmethod as a file object), not a file’s name string. Here is the technique in action\nlive:\n>>> log = open('log2.txt', 'w')\n>>> print(1, 2, 3, file=log)\n>>> print(4, 5, 6, file=log)\n>>> log.close()\n>>> print(7, 8, 9)\n7 8 9\n>>> print(open('log2.txt').read())\n1 2 3\n4 5 6\nThese extended forms of print are also commonly used to print error messages\nto the standard error stream, available to your script as the preopened file object\nsys.stderr. You can either use its file write methods and format the output\nmanually, or print with redirection syntax:\n>>> import sys\n>>> sys.stderr.write(('Bad!' * 8) + '\\n')\nBad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!\n>>> print('Bad!' * 8, file=sys.stderr)\nBad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": 10,
      "content": "Now that you know all about print redirections, the equivalence between printing\nand file write methods should be clear. The following demo prints both ways,\nthen redirects the output to an external file to verify that the same text is printed\n(on Unix, you won’t get the \\r added to newlines here on Windows, and write\nresults are back here):\n>>> import sys\n>>> X, Y = 1, 2\n>>> print(X, Y)                                            # Print: the easy way\n1 2\n>>> sys.stdout.write(str(X) + ' ' + str(Y) + '\\n')         # Print: the hard way\n1 2\n4\n>>> print(X, Y, file=open('temp1', 'w'))                   # Redirect text to file\n>>> open('temp2', 'w').write(str(X) + ' ' + str(Y) + '\\n') # Send to file manually\n4\n>>> print(open('temp1', 'rb').read())                      # Binary mode for bytes\nb'1 2\\r\\n'\n>>> print(open('temp2', 'rb').read())\nb'1 2\\r\\n'\nAs you can see, unless you happen to enjoy typing, print operations are often the\nbest option for displaying text. For another example of the equivalence between\nprints and file writes, watch for a print function emulation example in\nChapter 18; it uses the coding patterns here to provide a print equivalent that\ncan be customized.",
      "content_length": 1190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": 10,
      "content": "Chapter Summary\nIn this chapter, we began our in-depth look at Python statements by exploring\nassignments, expressions, and print operations. Although these are generally\nsimple to use, they have some alternative forms that, while optional, are often\nconvenient in practice—for example, augmented and named assignments, as\nwell as the redirection form of print operations, allow us to avoid some manual\ncoding work. Along the way, we also studied the syntax of variable names,\nstream redirection techniques, and a variety of common mistakes to avoid, such\nas assigning the result of an append method call back to a variable.\nIn the next chapter, we’ll continue our statement tour by filling in details about\nthe if statement, Python’s main selection tool; there, we’ll also revisit Python’s\nsyntax model in more depth and look at the behavior of Boolean expressions, as\nwell as the match multiple-choice statement. Before we move on, though, the\nend-of-chapter quiz will test your knowledge of what you’ve learned here.\nTest Your Knowledge: Quiz\n1. Name three ways that you can assign three variables to the same value.\n2. What’s dangerous about assigning three variables to a mutable object?\n3. What’s wrong with saying L = L.sort()?\n4. How might you use the print operation to send text to an external file?\nTest Your Knowledge: Answers\n1. You can use multiple-target assignments (A = B = C = 0), sequence\nassignment (A, B, C = 0, 0, 0), or multiple assignment statements\non three separate lines (A = 0, B = 0, and C = 0). With the latter\ntechnique, as introduced in Chapter 10, you can also string the three\nseparate statements together on the same line by separating them with",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": 10,
      "content": "semicolons (A = 0; B = 0; C = 0).\n2. If you assign them this way: A = B = C = [], then all three names\nreference the same object, so changing it in place from one (e.g.,\nA.append(99)) will affect the others. This is true only for in-place\nchanges to mutable objects like lists and dictionaries; for immutable\nobjects such as numbers and strings, this issue is irrelevant because they\ncan never be changed in place.\n3. The list sort method is like append in that it makes an in-place change\nto the subject list—it returns None, not the list it changes. The\nassignment back to L sets L to None, not to the sorted list. As discussed\nboth earlier and later in this book (e.g., Chapter 8), a newer built-in\nfunction, sorted, sorts any sequence and returns a new list with the\nsorting result; because this is not an in-place change, its result can be\nsafely and meaningfully assigned to a name.\n4. To print to a file for a single print operation, you can use the print(X,\nfile=F) call form, or assign sys.stdout to a manually opened file\nbefore the print and restore the original after if needed. You can also\nredirect all of a program’s printed text to a file with special syntax in the\nsystem shell like > file, but this is outside Python’s scope.\nWHY YOU WILL CARE: PRINT AND STDOUT\nThe equivalence between the print operation and writing to sys.stdout\nmakes it possible to reassign sys.stdout to any user-defined object that\nprovides the same write method as files. Because the print statement just\nsends text to the sys.stdout.write method, you can capture printed text in\nyour programs by assigning sys.stdout to an object whose write method\nprocesses the text in arbitrary ways.\nFor instance, you can send printed text to a GUI window, or tee it off to\nmultiple destinations, by defining an object with a write method that does\nthe required routing. You’ll see an example of this trick when we study\nclasses in Part VI of this book, but as an abstract preview, it looks like this:",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": 10,
      "content": "class FileFaker:\n    def write(self, string):\n        # Do something with the printed text in string\nimport sys\nsys.stdout = FileFaker()\nprint(someObjects)              # Sends text to class write method\nThis works because print is what we will call in the next part of this book a\npolymorphic operation—it doesn’t care what sys.stdout is, only that it has\na method (i.e., interface) called write. This redirection to objects is made\neven simpler with the file keyword argument of print, because we don’t\nneed to reset sys.stdout explicitly—normal prints will still be routed to the\nstdout stream:\nmyobj = FileFaker()             # Redirect to object for one print\nprint(someObjects, file=myobj)  # Does not reset sys.stdout\nPython’s built-in input function reads from the sys.stdin file, so you can\nintercept read requests in a similar way, using classes that implement file-\nlike read methods instead. See the input and while loop example in\nChapter 10 for more background on this function.\nNotice that because printed text goes to the stdout stream, it’s also the way\nto print HTML reply pages in server-side scripts used on the web, and\nenables you to redirect Python script input and output at the operating\nsystem’s shell command line as usual:\npython script.py < inputfile > outputfile\npython script.py | filterProgram\nPython’s print operation redirection tools are essentially pure-Python\nalternatives to some of these shell syntax forms. See other resources for\nmore on web scripts and shell syntax.\n1  C/C++ programmers also take note: although Python now supports statements like X += Y, it still\ndoes not have C’s auto-increment/decrement operators (e.g., X++, −−X). These don’t quite map to the",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": 10,
      "content": "Python object model because Python has no notion of in-place changes to immutable objects like\nnumbers. As a preview of the next section, there also are no augmented-named expression forms\ntoday; :+= 1 would be close to ++ but thankfully is fiction. That said, we eventually got += and :=,\nso…\n2  Exception: Alternative implementations of Python such as Jython might allow reserved words to\nappear as identifiers in some contexts. See Chapter 2 for more on alternative Pythons.\n3  If you’ve used a more restrictive language like C++, you may be interested to know that there is no\nnotion of C++’s const declaration in Python; certain objects may be immutable, but names can\nalways be assigned. Python also has ways to hide names in classes and modules, but they’re not the\nsame as C++’s declarations (if hiding attributes matters to you, see the coverage of _X module names\nin Chapter 25, __X class names in Chapter 31, and the Private and Public class decorators example\nin Chapter 39).\n4  Technically, printing uses the equivalent of str in the internal implementation of Python, but the\neffect is the same. Besides this to-string conversion role, str is also the name of the string data type\nand can be used to decode Unicode strings from raw bytes with an extra encoding argument, as you’ll\nlearn in Chapter 37; this latter role is an advanced usage that you can safely ignore here.\n5  You may also be able to use the __stdout__ attribute in the sys module, which refers to the original\nvalue sys.stdout had at program startup time. You still need to restore sys.stdout to\nsys.__stdout__ to go back to this original stream value, though. See the sys module documentation\nfor more details. Also note that sys.stdout and its cohorts may be None in some GUI programs with\nno console on which to display tests; be sure to check this where it matters.",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": 10,
      "content": "Chapter 12. if and match\nSelections\nThis chapter presents Python’s two statements used for selecting from alternative\nactions based on test results:\nif/elif/else\nThe main selection workhorse in most programs, capable of coding arbitrary\nlogic\nmatch/case\nA tool for narrower multiple-choice selection, with advanced matching\noperations\nBecause this is our first in-depth look at compound statements—statements that\nembed other statements—we will also explore the general concepts behind the\nPython statement syntax model here in more detail than we did in the\nintroduction in Chapter 10. Because the if statement introduces the notion of\ntests, this chapter will also deal with Boolean expressions, cover the “ternary” if\nexpression, and fill in some details on truth tests in general.\nif Statements\nIn simple terms, the Python if statement selects actions to perform. Along with\nits if expression counterpart, it’s the primary selection tool in Python and\nrepresents much of the logic a Python program possesses. It’s also our first\ncompound statement. Like all such statements, the if statement may contain\nother statements, including other ifs. In fact, Python lets you combine\nstatements in a program both sequentially (so that they execute one after\nanother), and in an arbitrarily nested fashion (so that they execute only under",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": 10,
      "content": "certain conditions, such as selections and loops).\nGeneral Format\nThe Python if is typical of if statements in most procedural languages. It takes\nthe form of an if test, followed by one or more optional elif (for “else if”)\ntests, and a final and optional else block. The tests and the else part each have\nan associated block of nested statements, indented under a header line. When the\nif statement runs, Python executes the block of code associated with the first\ntest that evaluates to true, or the else block if all tests prove false. The general\nform of an if statement looks like this:\nif test1:                 # Main if test\n    statements1           #     Associated block\nelif test2:               # Optional elif test(s)\n    statements2           #     Associated block\nelse:                     # Optional else default\n    statements3           #     Associated block\nBasic Examples\nTo demonstrate, let’s turn to a few simple examples of the if statement at work,\nin the REPL as usual. All parts are optional, except the initial if test and its\nassociated statements. Thus, in the simplest case, the other parts are omitted:\n$ python3\n>>> if 1:\n...     print('true')\n...\ntrue\nAs you’ve seen before, the prompt changes to ... for continuation lines in many\nREPLs; in IDLE, you’ll simply drop down to an indented line instead (and tap\nBackspace to back up when needed). A blank line, input by pressing Enter twice,\nterminates and runs the entire statement in most interactive interfaces.\nRemember that 1 is Boolean true (as we’ll review later, the word True is its\nequivalent), so this statement’s test always succeeds. To handle a false result\nhere, code the else to be run when the if test is not true:",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": 10,
      "content": ">>> if not 1:\n...     print('true')\n... else:\n...     print('false')\n...\nfalse\nIf you’re working along: this chapter would like to omit all the ... prompts for\neasier emedia copy and paste, but this would throw off indentation of associated\nlines like else. Instead, prompts are left off where possible, and code of some\nlonger examples is listed without prompts, before its output. The book’s\nexamples package also has code sans prompts.\nHere’s a more complex if statement with all its optional parts present; it aims to\ndisplay the roots of today’s mobile operating systems (though it’s not fully\ninclusive, and is prone to grow dated in a discriminating future near you):\nif os in ['iOS', 'iPhoneOS']:\n    print('macOS')\nelif mode == 'mobile' and os != 'Windows':\n    print('Linux')\nelse:\n    print('unknown?')\n>>> os, mode = 'Windows', 'mobile'\n>>> …insert the code above here…\nunknown?\nThis multiline statement extends from the if line through the block nested under\nthe else. When it’s run, Python executes the statements nested under the first\ntest that is true, or the else part if all tests are false (in this example, they are). In\npractice, both the elif and else parts may be omitted, and there may be more\nthan one statement nested in each section. Note that the words if, elif, and\nelse are associated by the fact that they line up vertically, with the same\nindentation (ignoring the REPL prompts you may see if you paste and run this\nlive).\nThe and expression in the preceding example is true if the expressions on its left\nand right sides are both true (more on such logical tests ahead). The if statement\ncan also be nested to code choices that depend on others, and arbitrary logic in",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": 10,
      "content": "general. The following, for example, first ensures that it’s dealing with a mobile\nbefore checking specific system names—logically speaking, there is an implied\nand between the enclosing and nested ifs:\nif mode == 'mobile':\n    if os == 'Android':\n        print('Linux')\n    elif os != 'Windows':\n        print('macOS')\n>>> os, mode = 'Android', 'mobile'\n>>> …insert the code above here…\nLinux\nMultiple-Choice Selections\nUntil Python’s version 3.10, it had no multiple-choice selection statement similar\nto a “switch” in some other languages that selects an action based on a variable’s\nvalue. As you’ll learn ahead, Python today has sprouted a match statement that\nachieves the same goals (and substantially more!). Even so, multiple-choice\nlogic can usually be coded just as easily by a series of if/elif tests and\noccasionally by indexing dictionaries or searching lists. Because dictionaries and\nlists can be built at runtime dynamically, they are often more flexible than\nhardcoded if (or match) logic in your script. The following, for instance, picks\nan operating system’s release year, more or less, from its name:\n>>> choice = 'Windows'\n>>> print({'macos':    2001,             # A dictionary-based 'switch'\n           'Linux':    1991,             # Use get() for a default (ahead)\n           'Windows':  1985}[choice])\n1985\nAlthough it may take a few moments for this to sink in the first time you see it,\nthis dictionary is a multiple-choice branch—indexing on the key choice\nbranches to one of a set of values, much like a “switch” statement in other\nlanguages. An almost equivalent but more verbose Python if statement might\nlook like the following:\nif choice == 'macos':                    # The equivalent if statement\n    print(2001)",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": 10,
      "content": "elif choice == 'Linux':\n    print(1991)\nelif choice == 'Windows':\n    print(1985)\nelse:\n    print('Bad choice')\n>>> …insert the code above here…\n1985\nThough it’s perhaps more readable, the potential downside of an if like this is\nthat, short of constructing it as a string and running it with tools like the eval or\nexec tools noted in Chapter 10, it cannot handle choices unknown until the\nprogram runs as easily as a dictionary. In more dynamic programs, data\nstructures offer added flexibility.\nHandling switch defaults\nNotice the else clause on the if here to handle the default case when no key\nmatches. As demoed in Chapter 8, dictionary defaults can be coded with in\nexpressions, get method calls, or exception catching with the try statement\nintroduced in the preceding chapter. All of these same techniques can be used\nhere to code a default action in a dictionary-based multiple choice. As a review\nin the context of this role, here’s the get scheme at work with defaults (which\nalso uses the more compact dict call to make the same dictionary):\n>>> branch = dict(macos=2001, Linux=1991, Windows=1985)\n>>> print(branch.get('Windows', 'Bad choice'))\n1985\n>>> print(branch.get('Solaris', 'Bad choice'))\nBad choice\nAn in membership test in an if statement can have the same default effect:\n>>> choice = 'AmigaOS'\n>>> if choice in branch:\n...     print(branch[choice])\n... else:\n...     print('Bad choice')\n...\nBad choice",
      "content_length": 1427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": 10,
      "content": "And the try statement is a general way to handle dictionary-based defaults by\ncatching and handling the errors they’d otherwise trigger (for more on\nexceptions, see Chapter 11’s overview and Part VII’s full treatment):\n>>> choice = 'GEM'\n>>> try:\n...     print(branch[choice])\n... except KeyError:\n...     print('Bad choice')\n...\nBad choice\nHandling larger actions\nAs you can tell, dictionaries are good for associating values with keys, but what\nabout the more complicated actions you can code in the statement blocks\nassociated with if statements? In Part IV, you’ll learn that dictionaries can also\ncontain functions to represent more complex actions and implement general\n“jump tables.” Such functions appear as dictionary values, may be coded as\nfunction names or inline lambdas, and are run by simply adding parentheses to\ntrigger their actions.\nHere’s an abstract sample of this technique, but stay tuned for a rehash in\nChapter 19 after you’ve learned more about function definition:\ndef action():  …\ndef default(): …\nbranch = {'Android':    lambda: …,       # A table of callable function objects\n          'iOS':        action,          # Via inline lambdas, or def elsewhere\n          'Symbian OS': lambda: …}\nchoice = 'Android'\nbranch.get(choice, default)()            # Fetch and run associated action\nAlthough dictionary-based multiple-choice branching is useful in programs that\ndeal with more dynamic data, most programmers will probably find that coding\nan if statement is a straightforward way to perform this task. As a rule of thumb\nin coding, when in doubt, err on the side of simplicity and readability; it’s the\n“Pythonic” way.",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": 10,
      "content": "And the punch line here, of course, is that the match statement may handle basic\nmultiple-choice selections better today—though it’s doesn’t do general logic like\nif, can’t handle dynamic data like dictionary indexing, and comes with\nsubstantial extra convolution for other roles. You’ll have to move on to the next\nsection to see all this for yourself.\nmatch Statements\nFor most of Python’s three-decade career, it resisted adding a multiple-choice\nselection statement, in large part because of all the options for coding such logic\nwith existing tools that we just explored. Those options permeate vast amounts\nof Python code and remain relatively simple techniques that are perfectly valid\nto use in Python code written today.\nNevertheless, programming languages have a tendency to get caught up in arms\nraces with each other—copying other languages’ features and eroding their own\ndistinctions in the process, and often for no better reason than familiarity with\nother tools. One of the fruits of this process is the match statement, new as of\nPython 3.10.\nAt its basic level, match is a potentially useful tool that adds a multiple-choice\nstatement to Python. At this level, it works very much like “switch” statements\nin other languages and can be used instead of both if/elif/else combos and\ndictionary indexing in some contexts. In its full-blown form, however, match\nimplements something known as structural pattern matching, which quickly\nfalls off a complexity cliff, and seems a highly convoluted answer to a question\nthat most Python programmers never asked. Especially for newcomers, it’s a lot\nto justify.\nBecause of all that, this section is going to focus on the basic roles of match, and\ntouch on its advanced pattern-matching roles only briefly, with delegation to\nPython’s manuals for more of the story.\nBasic match Usage\nThe good news is that match’s basic usage is straightforward. In its first-level\nform, match works the same as both an if statement and a dictionary index—",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": 10,
      "content": "like the following abstract snippets that emulate traffic lights in some locales (to\nrun most examples here live, first assign variables to one of the options listed in\ntheir opening comments):\n# state = 'go' or 'stop' or other\nif state == 'go':\n   print('green')\nelif state == 'stop':                        # if-based multiple choice\n    print('red')\nelse:\n    print('yellow')\ncolors = dict(go='green', stop='red')        # Dictionary-based mltiple choice\nprint(colors.get(state, 'yellow'))\nThe equivalent match statement provides explicit syntax for such multiple-\nchoice logic, at the cost of extra lines and extra indentation:\nmatch state:\n    case 'go':\n        print('green')                       # match-based multiple choice: 3.10+\n    case 'stop':\n        print('red')\n    case _:\n        print('yellow')\nThis statement works like this: match first evaluates the expression given in its\nheader line (e.g., state) and then compares its result to values given in case\nheader lines indented below it (e.g., 'go')—one after another, and top to bottom.\nAs soon as a first match is found, the block of code nested under the matching\ncase is run, and the entire match is exited. If no case values match, the match\nstatement either runs the block under the case with value _ (which provides a\nfallback default, and must appear last), or simply exits silently if no _ case is\npresent.\nAs usual, case blocks can contain multiple statements and nesting, and match\nitself can be nested in other statements. Moreover, case headers can also\ndesignate multiple values separated by | (“or”) which are all checked for a\nmatch, name a variable to be assigned to the matched value with as, and give a",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": 10,
      "content": "simple variable that is assigned the match expression’s result and always\nmatches (and hence ends the statement, much like the anonymous _):\n# state = 'go' or 'proceed' or 'start', or 'stop' or 'halt', or any other\nmatch state:\n    case 'go' | 'proceed' | 'start':      # Match any one of these 3\n        print('green')                    # First left-to-right match wins\n        print('means go')\n    case 'stop' | 'halt' as what:         # Match any, and assign it to what\n        print('red')                      # what outlives match if assigned\n        print('means', what)\n    case other:                           # Set other to state, and match\n        print('catchall', other)          # other outlives match if assigned\nIn general, variables (like this example’s what and other) embedded in case\nheaders and assigned during a successful match outlive the match statement\nitself: they can be used in code after the match exits, as long as that code is part\nof the same scope—which roughly means the same module or function, per\ncoverage later in this book.\nMatch versus if live\nAs a live match example, the following maps statements to categories and\nassigns some variables along the way for later use; we’ll get formal about the\nfor loop used here in the next chapter:\n>>> for stmt in ['if', 'while', 'try']:\n        match stmt:\n            case 'if' | 'match':\n                print('logic')\n            case 'for' | 'while' as which:\n                print('loop')\n            case other:\n                print('tbd')\n \nlogic\nloop\ntbd\n>>> which, other\n('while', 'try')\nThe equivalent if must assign the variables explicitly—though it requires less",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": 10,
      "content": "indentation, this example is artificial, and the if can handle more complex logic\nthat the match cannot; it’s not just a multiple-choice tool:\n>>> for stmt in ['if', 'while', 'try']:\n        if stmt in ['if', 'match']:\n            print('logic')\n        elif stmt in ['for', 'while']:\n            which = stmt\n            print('loop')\n        else:\n            other = stmt\n            print('tbd')\n…same results…\nWatch for more basic match statements to show up later in this book (e.g., in\nChapters 25, 30, and 38). Though it cannot be used to code general logic like if,\nmatch can make multiple-choice selection explicit in its basic form. Its extension\nto the structural pattern matching of the next section, however, is more difficult\nto rationalize—but you’ll have to read on to judge for yourself.\nAdvanced match Usage\nAs noted, beyond its basic level shown so far, match becomes too complex to\ncover usefully here. In this guise, it goes well beyond multiple-choice logic, to\ndefine a language of its own for extracting components of structured objects. As\na very brief survey, this over-caffeinated match treats case values as patterns,\nwhich may be:\nLiteral patterns: a literal X matches the same value, by equality or\nidentity\nWildcard patterns: _ matches anything, but the value is not assigned to _\nCapture patterns: variable X matches anything, and will be assigned to it\nOr patterns: X | Y | Z matches patterns X or Y or Z, stopping at the\nfirst match\nAs patterns: X as Y matches pattern X, and assigns the matched value to",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": 10,
      "content": "Y\nAdditional patterns that match sequences, mappings, attributes, and\ninstances by structure\nAs an artificial (if frightening) example, Example 12-1 demos literal, sequence,\nand mapping patterns. Its […] and (…) patterns both match any sequence and are\ninterchangeable; its {…} matches a mapping; and its single * or ** names in\npatterns collect unmatched parts of a sequence or mapping, respectively. Using a\n* in this context is similar to Chapter 11’s extended-unpacking assignments,\nthough here ** is unique, not all parts of a pattern must be variables, and\nassignment to starred names is really a side effect of a Boolean test for a match\n(pasters beware: blank lines added for readability here won’t work in a REPL,\nand […] patterns preclude (…)s; run from a file and experiment freely).\nExample 12-1. matchdemo.py\n# state = 1 or \n#   [1, 2, 3] or [0, 2, 3] or (1, 2, 3) or (0, 2, 3) or \n#   dict(a=1, b=2, c=3) or dict(a=0, b=2, c=3) or other\nmatch state:\n   case 1 | 2 | 3 as what:              # Match integer literals, what = 1\n       print('or', what)\n   case [1, 2, what]:                   # Match sequence (1), what = 3\n       print('list', what)\n   case [0, *what]:                     # Match sequence (0), what = [2, 3]\n       print('list', what)\n   case {'a': 1, 'b': 2, 'c': what}:    # Match mapping, what = 3\n       print('dict', what)\n   case {'a': 0, **what}:               # Match mapping, what = {'b': 2, 'c': 3}\n       print('dict', what)\n   case (1, 2, what):                   # Match sequence: same as [1, 2, what] \n       print('tuple', what)\n   case (0, *what):                     # Match sequence: same as [0, *what]\n       print('tuple', what)\n   case _ as what:                      # Match all other, what = other\n       print('other', what)\nSubtly, the […], {…}, and (…) patterns in this code’s case headers are not normal\nobject literals. They’re really special-case syntax forms that contain nested",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": 10,
      "content": "patterns, many of which just happen to be literal patterns here. Nested patterns\ncan also be capture patterns (possibly after at most one * or **), and may use |\nors, or _ wildcards. The case header [*a, 2 | 3, _], for example, is a valid\nsequence pattern, but has little to do with list literals.\nAttribute and instance patterns require knowledge you haven’t yet gained (a\nrecurring theme in Python today), but they check for attribute values and\ninheritance-tree membership, and some of their components are nested patterns\nagain, which may be literals, captures, and more. As a preview—which you can\nrevisit after reading Part VI:\nclass Emp:\n    def __init__(self, name): self.name = name\npat = Emp('Pat')                         # pat.name becomes 'Pat': see Part VI!\n# state = 'Pat' or pat\nmatch state:\n    case pat.name as what:               # Match object's attribute, what = 'Pat'\n        print('attr', what)\n    case Emp(name=what):                 # Match an Emp instance, what = 'Pat'\n        print('instance', what)\nAnd if that’s not already overkill for your multiple-choice logic needs,\nparentheses may be used around any pattern for grouping (as in general\nexpressions); nested structures match recursively as they do in sequence\nassignment; and each case header can also end with an optional guard\nexpression introduced by an if (after the optional as capture), which must be\ntrue for the case to be selected and its code block run:\nstate = ((1, 2), 3)\nguard1 = True                            # a=1, b=3 if True; a=(1, 2) if False\nmatch state:\n    case ((a, 2), b) if guard1:          # Match+run only if guard1 is True\n        print(f'case1 {a=} {b=}')        \n    case (a, 3) as what:                 # Reached only if guard1 is False \n        print(f'case2 {a=} {what=}')\n    case [a, (3 | 4)] as what if guard1:\n        print(f'case3 {a=} {what=}')",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": 10,
      "content": "All of which seems a blizzard of functionality to address usage that’s\noverwhelmingly straightforward. Hence, this is where this book’s match\ncoverage must stop short for space (and mercy). For more on advanced roles of\nmatch, including all the gory details of its special-case patterns, consult Python’s\nonline resources.\nBefore you do, though, ponder just for a moment on the fact that Python was\nused successfully for three decades and rose to the top of the programming-\nlanguages heap without a match statement. Saddling the language with yet\nanother convoluted subdomain that obviates a trivial amount of code might owe\nat least as much to hubris as user need.\nThat said, match may be useful in simpler roles, though you’ll still need to\nchoose between if, dictionaries fetches, and match when coding multiple-choice\nlogic. While you should generally strive to avoid doing something just because\nyou’ve done it in other languages (you’re now using Python, after all), such\nchoices are yours to make.\nPython Syntax Revisited\nPython’s syntax model was introduced in Chapter 10. Now that we’re stepping\nup to larger statements like if and match, this section reviews and expands on\nthe syntax ideas introduced earlier. In general, Python has a simple, statement-\nbased syntax. Among its highlights:\nStatements execute one after another, until you say otherwise.\nPython normally runs statements in a file or nested block in order from\nfirst to last, but statements like if (as well as loops and exceptions)\ncause the interpreter to jump around in your code. Because Python’s\npath through a program is called the control flow, statements such as if\nthat affect it are often called control-flow statements.\nBlock and statement boundaries are detected automatically. As\nwe’ve seen, there are no braces or “begin/end” delimiters around blocks\nof code in Python; instead, Python uses the indentation of statements\nunder a header to group the statements in a nested block. Similarly,\nPython statements are not normally terminated with semicolons; rather,",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": 10,
      "content": "the end of a line usually marks the end of the statement coded on that\nline. As special cases you’ll meet later, statements can both span lines\nand be combined on a line when it’s useful.\nCompound statements = header + “:” + indented statements. All\nPython compound statements—those with nested statements—follow\nthe same pattern: a header line terminated with a colon, followed by one\nor more nested statements, usually indented under the header. The\nindented statements are called a block (or sometimes, a suite). In the if\nstatement, the elif and else are part of the if, but they are also header\nlines with nested blocks of their own. As a special case, blocks can be\non the same line as the header if they are not compound.\nBlank lines, spaces, and comments are usually ignored. Blank lines\nare both optional and ignored in files (but not at the interactive prompt,\nwhen they terminate compound statements). Spaces inside statements\nand expressions are almost always ignored (except in string literals, and\nwhen used for indentation). Comments are always ignored: they start\nwith a # character (not inside a string literal) and extend to the end of\nthe current line.\nDocstrings are ignored but are saved and displayed by tools. Python\nsupports an additional comment form called documentation strings\n(docstrings for short), which, unlike # comments, are retained at\nruntime for inspection. Docstrings are simply strings that show up at the\ntop of program files and some statements. Their content is ignored, but\nthey are attached to objects and may be displayed with tools covered\nlater in this book.\nFor most Python newcomers, the lack of the braces and semicolons used to mark\nblocks and statements in many other languages seems to be the most novel\nsyntactic feature of Python, so let’s explore what this means in more depth.\nBlock Delimiters: Indentation Rules\nAs introduced in Chapter 10, Python detects block boundaries automatically, by\nline indentation—that is, the empty space to the left of your code. This section is",
      "content_length": 2032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": 10,
      "content": "a rehash of the rules, with a few more details sprinkled in along the way.\nIn short, all statements indented the same distance to the right belong to the same\nblock of code. In other words, the statements within a block line up vertically, as\nin a column. The block ends when a lesser-indented line or the end of the file is\nencountered (or you enter a blank like in a REPL), and more deeply nested\nblocks are simply indented further to the right than the statements in the\nenclosing block.\nFor instance, Figure 12-1 demonstrates the block structure of the following code:\nx = 1\nif x:\n    y = 2\n    if y:\n        print('block2')\n    print('block1')\nprint('block0')\nThis code contains three blocks: the first (the top-level code of the file) is not\nindented at all, the second (within the outer if statement) is indented four\nspaces, and the third (the print statement under the nested if) is indented eight\nspaces.\nTop-level (unnested) code must start in column 1, but nested blocks can start in\nany column; indentation may consist of any number of spaces and tabs, as long\nas it’s the same for all the statements in a given single block. That is, Python\ndoesn’t care how you indent your code; it only cares that it’s done consistently.\nFour spaces or one tab per indentation level are common conventions, but there\nis no absolute standard or rule in the Python world.",
      "content_length": 1368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": 10,
      "content": "Figure 12-1. Nested blocks of code denoted by their indentation\nIndenting code is quite natural in practice. For example, the following fully\nfrivolous code snippet demonstrates common indentation errors in Python code,\nwhich are easy to spot because they’re visually askew:\n  x = 'Hack'                        # Error: first line indented\nif 'tho' in 'python':\n    print(x * 8)\n        x += 'More!'                # Error: unexpected indentation\n        if x.endswith('re!'):\n                x *= 2\n            print(x)                # Error: inconsistent indentation\nThe properly indented version of this code looks like the following—even for an",
      "content_length": 649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": 10,
      "content": "artificial example like this, proper indentation makes the code’s intent much\nmore apparent:\nx = 'Hack'\nif 'tho' in 'python':\n    print(x * 8)                    # Prints 8 Hack\n    x += 'More!'\n    if x.endswith('re!'):\n        x *= 2\n        print(x)                    # Prints HackMore!HackMore!\nIt’s important to know that the only major place in Python where whitespace\nmatters is where it’s used to the left of your code, for indentation; in most other\ncontexts, space can be coded or not. However, indentation is really part of\nPython syntax, not just a stylistic suggestion: all the statements within any given\nsingle block must be indented to the same level, or Python reports a syntax error.\nThis is intentional—because you don’t need to explicitly mark the start and end\nof a nested block of code, some of the syntactic clutter found in other languages\nis unnecessary in Python.\nAs described in Chapter 10, making indentation part of the syntax model also\nenforces consistency, a crucial component of readability in structured\nprogramming languages like Python. In Python’s syntax, the indentation of each\nline of code unambiguously tells readers what it is associated with. This uniform\nand consistent appearance in turn makes Python code easier to maintain and\nreuse.\nIn the end, indentation is easier than you may think. Consistently indented code\nalways satisfies Python’s rules, and most text editors (including IDLE) make it\neasy to follow Python’s model by automatically indenting as you type.\nAvoid mixing tabs and spaces\nThat said, there’s one rule of thumb you should know: although you can use\nspaces or tabs to indent, it’s usually not a good idea to mix the two within a block\n—use one or the other. Technically, tabs count for enough spaces to move the\ncurrent column number up to a multiple of 8, and your code will work if you mix\ntabs and spaces consistently. However, mixing tabs and spaces makes code\ndifficult to read and change completely apart from Python’s syntax rules—tabs\nmay look very different in the next programmer’s editor than they do in yours.",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": 10,
      "content": "For these reasons, Python issues an error when a script mixes tabs and spaces for\nindentation inconsistently within a block (that is, in a way that makes it\ndependent on a tab’s equivalent in spaces). So don’t do that: when in Python do\nas Pythoneers do and use consistent indentation instead of block delimiters.\nStatement Delimiters: Lines and Continuations\nWhile blocks are indented, a statement in Python normally ends at the end of the\nline on which it appears. When a statement is too long to fit on a single line,\nthough, a few special rules may be used to make it span multiple lines:\nStatements may span multiple lines if you’re continuing an “open\npair.” The code of a statement can always be continued on the next line\nif it’s enclosed in a (), {}, or [] pair. For instance, expressions in\nparentheses and dictionary and list literals can span any number of\nlines; the statement doesn’t end until the end of the line containing the\nclosing part of the pair (a ), }, or ]). Continuation lines—lines 2 and\nbeyond of the statement—can start at any indentation level, but it’s best\nto align vertically for readability in some fashion.\nStatements may span multiple lines if they end in a backslash.\nThough best used as a fallback option, if a statement needs to span\nmultiple lines, you can also add a backslash—a \\ not embedded in a\nstring literal or comment—at the end of the prior line to indicate you’re\ncontinuing on the next line. Because you can also continue by adding\nparentheses around most constructs, backslashes are rarely used today.\nThis approach is also error-prone: accidentally forgetting a \\ may\ngenerate a syntax error or cause the next line to run independently.\nStatements may be combined if separated with a semicolon. Though\nuncommon, you can terminate a statement with a semicolon. This is\nsometimes used to squeeze more than one statement onto a single line\nby separating them with semicolons, but this works only when the\ncombined statements are not compound.\nStatements may contain multiline strings. As we learned in\nChapter 7, triple-quoted string blocks are designed to span multiple",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": 10,
      "content": "lines normally. We also learned in Chapter 7 that adjacent string literals\nare implicitly concatenated; when this is used in conjunction with the\nopen-pairs rule mentioned earlier, wrapping this construct in\nparentheses allows it to span multiple lines.\nSpecial Syntax Cases in Action\nHere’s what a continuation line looks like using the open-pairs rule just\ndescribed. Delimited constructs, such as lists in square brackets, can span across\nany number of lines:\nL = ['app',\n     'script',\n     'program']                  # Open pairs may span lines\nThis also works for list comprehensions enclosed in []; anything in () (tuples,\nexpressions, function argument and headers, and generators expressions); and\nanything in {} (dictionary and set literals and comprehensions). Some of these\nare tools we’ll study in later chapters, but this rule naturally covers most\nconstructs that span lines in practice.\nIf you’re accustomed to using backslashes to continue lines, you can in Python,\ntoo, but it’s not common practice:\nif a == b and c == d and   \\\n   d == e and f == g:\n   print('old school')           # Backslashes allow continuations...\nBecause any expression can be enclosed in parentheses, you can usually use the\nopen-pairs technique instead if you need your code to span multiple lines—\nsimply wrap a part of your statement in parentheses:\nif (a == b and c == d and\n    d == e and e == f):\n    print('new school')          # But parentheses usually do too, and are obvious\nIn fact, backslashes are generally discouraged, because they’re too easy to not\nnotice and too easy to omit altogether. In the following, x is assigned 10 with the\nbackslash, as intended; if the backslash is accidentally omitted, though, x is",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": 10,
      "content": "assigned 6 instead, and no error is reported (the +4 is a valid expression\nstatement by itself). In a real program with a more complex assignment, this\ncould be the source of a very obscure bug:\nx = 1 + 2 + 3 \\                  # Omitting the \\ makes this very different!\n+4\nAs another special case, Python allows you to write more than one\nnoncompound statement (i.e., statements without nested statements) on the same\nline, separated by semicolons. Some coders use this form to save program file\nreal estate, but it usually makes for more readable code if you stick to one\nstatement per line for most of your work:\nx = 1; y = 2; print(x)           # More than one simple statement\nAs covered in Chapter 7, triple-quoted string literals span lines too. In addition,\nif two string literals appear next to each other, they are concatenated as if a + had\nbeen added between them—when used in conjunction with the open-pairs rule,\nwrapping in parentheses allows this form to span multiple lines. For example,\nthe first of the following inserts newline characters at line breaks and assigns S\nto '\\naaaa\\nbbbb\\ncccc', and the other two implicitly concatenate and assign\nS to 'aaaabbbbcccc'; as we also saw in Chapter 7, # comments are ignored in\nthe second form but included in the string in the first, and f-strings require f\nprefixes even on continuations lines:\nS = \"\"\"\naaaa\nbbbb\ncccc\"\"\"\nS = ('aaaa' \n     'bbbb'              # Comments here are ignored, add \\n if needed\n     'cccc')\nS = (f'{'a' * 4}'        # Also makes 'aaaabbbbcccc'\n     f'{'b' * 4}'        # Use f'' on each f-string part\n     r'cccc')            # And ditto for r'' raw-string parts\nFinally, and also as a review, Python lets you move a compound statement’s",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "body up to the header line, provided the body contains just simple\n(noncompound) statements. You’ll most often see this used for simple if\nstatements with a single test and action, as in the interactive loops we coded in\nChapter 10:\nif 1: print('hello')             # Simple statement on header line\nWith a little effort, you can combine some of these special cases to write Python\ncode that is difficult to read, but it’s not generally recommended. As a rule of\nthumb, try to keep each statement on a line of its own, and indent all but the\nsimplest of blocks. Six months down the road, you’ll be happy you did.\nTruth Values Revisited\nThe notions of comparison, equality, and truth values were introduced in\nChapter 9. Like syntax, though, the if is the first statement we’ve studied that\nactually uses these tools, so we’ll rehash these ideas with additional info. All\ntold, Python’s Boolean operators are a bit different from their counterparts in\nsome other languages. In Python:\nAll objects have an inherent Boolean true or false value.\nAny nonzero number or nonempty object is true.\nZero numbers, empty objects, and the special object None are\nconsidered false.\nComparisons and equality tests are applied recursively to data\nstructures.\nComparisons and equality tests return True or False (custom versions\nof 1 and 0).\nBoolean and and or operators return a true or false operand object.\nBoolean operators stop evaluating (short-circuit) as soon as a result is\nknown.\nThe if statement takes action on truth values, but Boolean operators are used to",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "combine the results of other tests in richer ways to produce new truth values.\nMore formally, there are three Boolean expression operators in Python:\nX and Y\nIs true if both X and Y are true—and returns either X or Y\nX or Y\nIs true if either X or Y is true—and returns either X or Y\nnot X\nIs true if X is false—and returns either True or False\nHere, X and Y may be any truth value, or any expression that returns a truth value\n(e.g., an == equality test, an in membership check, and so on). Boolean\noperators are typed out as words in Python (instead of C’s &&, ||, and !), and\nshouldn’t be confused with Python’s &, |, and ^ operators that work on numbers\nand sets (and dictionaries).\nMost uniquely, Boolean and and or operators return a true or false object in\nPython, not the values True or False. Let’s turn to a few examples to see how\nthis works:\n>>> 2 < 3, 3 < 2        # Less than: return True or False (1 or 0)\n(True, False)\nMagnitude comparisons such as these return True or False as their truth results,\nwhich, as we learned in Chapters 5 and 9, are really just custom versions of the\nintegers 1 and 0 (they print themselves differently but are otherwise the same).\nBoolean operators and and or, on the other hand, always return an object—\neither the object on the left side of the operator or the object on the right. If we\ntest their results in if or other statements, they will be as expected (remember,\nevery object is inherently true or false), but we won’t get back a simple True or\nFalse.\nFor or tests, Python evaluates the operand objects from left to right and returns",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "the first one that is true. Moreover, Python stops at the first true operand it finds.\nThis is usually called short-circuit evaluation, as determining a result short-\ncircuits (terminates) the rest of the expression as soon as the result is known:\n>>> 2 or 3, 3 or 2      # Return left operand if true\n(2, 3)                  # Else, return right operand (true or false)\n>>> [] or 3\n3\n>>> [] or {}\n{}\nIn the first line of the preceding example, both operands (2 and 3) are true (i.e.,\nare nonzero), so Python always stops and returns the one on the left—which\ndetermines the result because true or anything is always true. In the other two\ntests, the left operand is false (an empty object), so Python simply evaluates and\nreturns the object on the right—which may have either a true or a false value\nwhen tested, but determines the result of the or at large.\nPython and operations also stop (short-circuit) as soon as the result is known.\nHowever, in this case Python evaluates the operands from left to right and stops\nif the left operand is a false object because it determines the result—false and\nanything is always false:\n>>> 2 and 3, 3 and 2    # Return left operand if false\n(3, 2)                  # Else, return right operand (true or false)\n>>> [] and {}\n[]\n>>> 3 and []\n[]\nHere, both operands are true in the first line, so Python evaluates both sides and\nreturns the object on the right—which determines the result of the and. In the\nsecond test, the left operand is false ([]), so Python stops and returns it as the\nand result without ever running the code on the right side. In the last test, the left\nside is true (3), so Python evaluates and returns the object on the right—which\nhappens to be a false [].\nThe net effect of all this apparent nonsense is the same as in most other\nlanguages—you get a value that is logically true or false if tested in an if or",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "while according to the normal definitions of or and and. However, in Python,\nBooleans return either the left or the right object, not a simple integer flag.\nThis behavior of and and or may seem esoteric at first glance, but see this\nchapter’s sidebar, “Why You Will Care: Booleans”, for examples of how it is\nsometimes used to advantage in Python coding. The next section also shows a\ncommon way to leverage this behavior, and its more mnemonic alternative.\nThe if/else Ternary Expression\nOne common role for the prior section’s Boolean operators is to code an\nexpression that runs the same as an if statement. To get started, consider the\nfollowing very common code, which sets A to either Y or Z, based on the truth\nvalue of X:\nif X:\n    A = Y\nelse:\n    A = Z\nSometimes, though, the items involved in such a statement are so simple that it\nseems like overkill to spread them across four lines. At other times, we may\nwant to nest such a construct in a larger statement instead of assigning its result\nto a variable separately. For such reasons (and possibly to appease ex-C\nprogrammers), Python includes a “ternary” (three-part) expression that allows us\nto say the same thing in just one line of code:\nA = Y if X else Z\nThis expression has the exact same effect as the preceding four-line if\nstatement, but it’s simpler to code. In some sense, it is to if statements what the\nprior chapter’s := is to assignment statements: an expression equivalent with\nmore limited syntax and narrower roles, which is nevertheless useful in some\ncode. For example, you can’t code full statements in the parts of this expression,\nbut you can embed it anywhere that Python allows an expression.\nAs in the statement equivalent, the ternary expression runs expression Y only if X",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "turns out to be true and runs expression Z only if X turns out to be false. That is,\nit short-circuits, just like the Boolean operators described in the prior section,\nrunning just Y or Z but not both. Here are some examples of it in action:\n>>> tone = 'formal'\n>>> a = 'code' if tone == 'formal' else 'hack'\n>>> a\n'code'\n>>> tone = 'informal'\n>>> a = 'code' if tone == 'formal' else 'hack'\n>>> a\n'hack'\nThe same effect can often be achieved by a careful combination of and and or\noperators, because they return either the object on the left side or the object on\nthe right as the preceding section described. The ternary expression in the\nfollowing works the same as the Boolean expression below it:\nA = Y if X else Z           # Ternary if/else\nA = ((X and Y) or Z)        # and+or equivalent\nThis works, but there is a catch—you have to be able to assume that Y will be\nBoolean true. If that is the case, the effect is the same: the and runs first and\nreturns Y if X is true; if X is false the and skips Y and returns false X, and the or\nsimply returns Z. In other words, we get “if X then Y else Z”—which is exactly\nwhat the ternary expression says, albeit in a different order.\nThe and/or combination form also seems to require a “moment of great clarity”\nto understand the first time you see it, which qualifies as an argument against its\ndeployment. As a guideline: use the equivalent and more robust and mnemonic\nif/else expression when you need this structure, or use a full if statement\nwhen the parts are nontrivial.\nAs a side note (and just in case this section hasn’t made your head explode yet),\nusing the following expression is similar to the prior Boolean and ternary\nexpressions, because the bool function will translate any X into the equivalent of\ninteger 1 or 0 (i.e., True or False), which can then be used as an offset to pick",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "true and false values from a list:\nA = [Z, Y][bool(X)]\nTruth be told, the bool is not required when X already yields a truth value, but is\nwhen X is an object like a string:\n>>> ['hack', 'code'][tone == 'formal']\n'hack'\n>>> ['hack', 'code'][bool('formal')]\n'code'\nBut alas, this isn’t exactly the same, because Python will not short-circuit—it\nwill always run both Z and Y, regardless of the value of X. Because of such\ncomplexities, you’re better off using the simpler and more easily digested\nif/else expression. Even then, common sense goes a long way here as always.\nLike most nestable tools, the ternary expression is naturally prone to yield code\nthat’s tough to read. If you find yourself working hard at packing logic into one,\nconsider taking a moment to think about how hard it will be to unpack later.\nYour coworkers will be glad you did.",
      "content_length": 849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we studied the Python if statement. Additionally, because this\nwas our first compound and logical statement, we reviewed Python’s general\nsyntax rules and explored the operation of truth values and tests in more depth\nthan we were able to previously. Along the way, we also looked at how to code\nmultiple-choice logic in Python with both dictionaries and match, learned about\nthe if/else ternary expression, and explored Boolean operators.\nThe next chapter continues our look at procedural statements by expanding the\ncoverage of while and for loops. There, you’ll learn about alternative ways to\ncode loops in Python, some of which may be better than others. Before that,\nthough, here is the usual chapter quiz to review before moving ahead.\nTest Your Knowledge: Quiz\n1. How might you code a multiple-choice branch in Python?\n2. How can you code an if/else statement as an expression in Python?\n3. How can you make a single statement span many lines?\n4. What do the words True and False mean?\n5. What does “short-circuiting” mean, and where does it crop up?\nTest Your Knowledge: Answers\n1. An if statement with multiple elif clauses is often the most\nstraightforward way to code a multiple-choice branch, though not\nnecessarily the most concise or flexible. Dictionary indexing can often\nachieve the same result, especially if the dictionary contains callable\nfunctions coded with def statements or lambda expressions. As of\nPython 3.10, the match statement provides explicit syntax for multiple-\nchoice selections; it works well in basic roles but can’t code logic as",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "general as that in the if statement and comes with substantial heft in\nsupport of structural pattern matching, a very different tool.\n2. The expression form Y if X else Z returns Y if X is true, or Z\notherwise; it’s the same as a four-line if statement and works well in\nlimited contexts, but can’t code actions as rich as those in the full if\nstatement, and has the potential to produce code that’s hard to read. The\nand/or combination (((X and Y) or Z)) can work the same way, but\nit’s more obscure and requires that the Y part be true.\n3. Wrap up the statement in an open syntactic pair ((), [], or {}), and it\ncan span as many lines as you like; the statement ends when Python\nsees the closing (right) half of the pair, and lines 2 and beyond of the\nstatement can begin at any indentation level. Backslash continuations\nwork, too, but are broadly discouraged in the Python world.\n4. This is partly a review from Chapter 9, but is reinforced in the sidebar at\nthe end of this chapter. True and False are just custom versions of the\nintegers 1 and 0, respectively: they always stand for Boolean true and\nfalse values in Python. They’re available for use in truth tests and\nvariable initialization and are printed for expression results at the\ninteractive prompt. In all these roles, they serve as a more mnemonic\nand hence readable alternative to 1 and 0.\n5. Short-circuiting happens when Python stops evaluating an expression\nearly because its result can already be determined from the expression\nso far. It comes up in and and or expressions, which run their right side\nonly if their left sides don’t determine their results. It also comes up in\nthe if/else ternary expression, which runs either its true or false parts,\ndepending on its test part’s logical result.\nWHY YOU WILL CARE: BOOLEANS\nOne common way to use the somewhat unusual behavior of Python Boolean\noperators is to select from a set of objects with an or. A statement such as\nthis:",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "X = A or B or C or None\nassigns X to the first nonempty (that is, true) object among A, B, and C, or to\nNone if all of them are empty. This works because the or operator returns\none of its two objects, and it turns out to be a fairly common coding\nparadigm in Python: to select a nonempty object from among a fixed-size\nset, simply string them together in an or expression. In simpler form, this is\nalso commonly used to designate a default—the following sets X to A if A is\ntrue (nonzero or nonempty), and to default otherwise:\nX = A or default\nIt’s also important to understand the short-circuit evaluation of Boolean\noperators and the if/else, because it may prevent actions from running.\nExpressions on the right of a Boolean operator, for example, might call\nfunctions that perform substantial or important work, or have side effects\nthat won’t happen if the short-circuit rule takes effect:\nif f1() or f2(): …\nHere, if f1 returns a true (or nonempty) value, Python will never run f2. To\nguarantee that both functions will be run, call them before the or:\ntmp1, tmp2 = f1(), f2()\nif tmp1 or tmp2: …\nYou’ve already seen another application of this behavior in this chapter:\nbecause of the way Booleans work, the expression ((A and B) or C) can\nbe used to emulate an if statement—almost (see this chapter’s discussion of\nthis form for details).\nWe met additional Boolean use cases in prior chapters. As we saw in\nChapter 9, because all objects are inherently true or false, it’s common and\neasier in Python to test an object directly (if X:) than to compare it to an\nempty value (if X != '':). For a string, the two tests are equivalent. As we\nalso saw in Chapter 5, the preset Boolean values True and False are the",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "same as the integers 1 and 0 and are useful for initializing variables (X =\nFalse), for loop tests (while True:), and for displaying results at the\ninteractive prompt.\nAlso watch for related discussion in operator overloading in Part VI: when\nwe define new object types with classes, we can specify their Boolean nature\nwith either the __bool__ or __len__ methods. The latter of these is tried if\nthe former is absent and designates false by returning a length of zero—\nbecause an empty object is considered false.\nFinally, and as a preview, other tools in Python have roles similar to the or\nchains at the start of this sidebar: the filter call and list comprehensions\nyou’ll explore later can be used to select true values when the set of\ncandidates isn’t known until runtime (though they evaluate all values and\nreturn all that are true), and the any and all built-ins can be used to test if\nany or all items in a collection are true (they short-circuit their checks like\nand, or, and if/else, but don’t select an item per se):\n>>> L = [1, 0, 2, 0, 'hack', '', 'py', []]\n>>> list(filter(bool, L))                    # Get true values\n[1, 2, 'hack', 'py']\n>>> [x for x in L if x]                      # Comprehensions\n[1, 2, 'hack', 'py']\n>>> any(L), all(L)                           # Aggregate truth\n(True, False)\nAs we’ve learned, the bool function here simply returns its argument’s true\nor false value, as though it were tested in an if. Watch for more on these\nrelated tools in Chapters 14, 19, and 20.",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "Chapter 13. while and for Loops\nThis chapter concludes our tour of Python procedural statements by presenting\nthe language’s two main looping constructs—statements that repeat an action\nover and over:\nwhile/else\nThe most general looping statement, which can handle repetitive tasks of all\nkinds\nfor/else\nA specialized loop designed for stepping through the items in any “iterable”\nobject easily\nWe’ve met and used both of these informally already, but we’ll fill in additional\nusage details here. While we’re at it, we’ll also study a few less prominent\nstatements used within loops, such as break and continue, the loop else, and\ncover some built-ins commonly used with loops, such as range, zip, and\nenumerate.\nAlthough the while and for statements covered here are the primary syntax\nprovided for coding repeated actions, there are additional looping operations and\nconcepts in Python. Because of that, the iteration story is continued in the next\nchapter, where we’ll explore the related ideas of Python’s iteration protocol\n(used by the for loop) and list comprehensions (a close cousin to the for loop).\nLater chapters explore even more exotic iteration tools such as generators and\nfunctional tools like map, filter, and reduce. For now, though, let’s keep things\nsimple.\nwhile Loops",
      "content_length": 1290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "Python’s while statement is the most general iteration construct in the language.\nIn simple terms, it repeatedly executes an associated block of statements as long\nas a test at the top keeps evaluating to a true value. It is called a “loop” because\ncontrol keeps looping back to the start of the statement until the test becomes\nfalse. When the test does become false, control passes to the statement that\nfollows the while block. The net effect is that the loop’s body is executed\nrepeatedly while the test at the top is true. If the test is false to begin with, the\nbody never runs and the while statement is skipped.\nGeneral Format\nIn its most complex form, the while statement consists of a header line with a\ntest expression, a body of one or more normally indented statements, and an\noptional else part that is executed if control exits the loop without a break\nstatement being encountered. Python keeps evaluating the test at the top and\nexecuting the statements nested in the loop body until the test returns a false\nvalue:\nwhile test:                  # Loop test\n    statements               #     Repeated loop body\nelse:                        # Optional else\n    statements               #     Run if didn't exit loop body with break\nExamples\nTo illustrate, let’s look at a few simple while loops in action. The first, which\nconsists of a print statement nested in a while loop, just prints a message\nforever. Recall that True is just a custom version of the integer 1 and always\nstands for a Boolean true value; because the test is always true, Python keeps\nexecuting the body forever or until you stop its execution. This sort of behavior\nis usually called an infinite loop—it’s not really immortal, but you may need a\nCtrl+C key combination to forcibly terminate it:\n>>> while True:\n...    print('Type Ctrl+C to stop me!')\nThe next example keeps slicing off the first character of a string until the string",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "is empty and hence false (and begins omitting the REPL’s ... prompts for easier\nemedia copy and paste where possible). It’s typical to test an object directly like\nthis instead of using the more verbose equivalent (while x != '':), though\nlater in this chapter you’ll see other ways to step through the items in a string\nmore easily with a for loop:\n>>> x = 'code'\n>>> while x:                  # While x is not empty\n        print(x, end=' ')     # Print next character\n        x = x[1:]             # Strip first character off x\n   \ncode ode de e\nNote the end=' ' keyword argument used here to place all outputs on the same\nline separated by a space; see Chapter 11 if you’ve forgotten why this works as it\ndoes. This will probably leave your REPL’s input prompt at the end of the\noutput’s line; type Enter (or your keyboard’s or app’s equivalent) to reset if\ndesired.\nThe following code counts from the value of a up to, but not including, b. Loops\nlike this are often used to generate object indexes. You’ll also see an easier way\nto do this with a Python for loop and the built-in range function later:\n>>> a=0; b=10\n>>> while a < b:              # One way to code counter loops\n        print(a, end=' ')\n        a += 1                # Or, a = a + 1\n   \n0 1 2 3 4 5 6 7 8 9\nFinally, notice that Python doesn’t have what some languages call a “do until”\nloop statement. However, we can simulate one with a test and break at the\nbottom of the loop body, so that the loop’s body is always run at least once:\nwhile True:\n    …loop body…               # Always run loop body at least once\n    if test: break            # Test for loop exit at the bottom\nTo fully understand how this structure works, we need to move on to the next\nsection’s coverage of break.",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "break, continue, pass, and the Loop else\nNow that we’ve seen a few Python loops in action, it’s time to take a look at two\nsimple statements that have a purpose only when nested inside loops—the break\nand continue statements. While we’re looking at oddballs, we will also study\nthe loop else clause here because it is intertwined with break, as well as\nPython’s empty placeholder statement pass, which is not tied to loops but falls\ninto the category of simple one-word statements. In Python:\nbreak\nJumps out of the closest enclosing loop (past the entire loop statement)\ncontinue\nJumps to the top of the closest enclosing loop (to the loop’s header line)\npass\nDoes nothing at all: it’s an empty statement placeholder\nLoop else block\nRuns if and only if the loop is exited normally (i.e., without hitting a break)\nGeneral Loop Format\nFactoring in break and continue statements, the general format of the while\nloop looks like this:\nwhile test:\n    statements\n    if test: break              # Exit loop now, skip else if present\n    if test: continue           # Go to test at top of loop now\nelse:\n    statements                  # Run on exit if didn't hit a 'break'\nbreak and continue statements can appear anywhere inside the while (or per\nahead, for) loop’s body, but they are usually coded further nested in an if test to\ntake action in response to some condition.",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "Let’s turn to a few simple examples to see how these statements come together\nin practice.\npass\nSimple things first: the pass statement is a no-operation placeholder that is\ncoded when the syntax requires a statement, but you have nothing useful to say.\nIt is often used to code an empty body for a compound statement. For instance, if\nyou want to code an infinite loop that does nothing each time through, do it with\na pass:\nwhile True: pass                   # Type Ctrl+C to stop me!\nBecause the body is just an empty statement, Python gets stuck in this loop. pass\nis roughly to statements as None is to objects—an explicit nothing. Notice that\nhere the while loop’s body is on the same line as the header, after the colon; as\nwith if statements, this works only if the body isn’t a compound statement (and\ndoesn’t contain one).\nThis example does nothing forever. It probably isn’t the most useful Python\nprogram ever written (unless you want to warm up your laptop or phone on a\ncold winter’s day), but it’s tough to come up with a better pass example at this\npoint in the book; it’s not a commonly used tool.\nYou’ll see other places where pass makes a bit more sense later—for instance,\nto ignore exceptions caught by try statements and to define empty class\nobjects with attributes that behave like “structs” and “records” in other\nlanguages. Though partly a preview, a pass is also sometime coded to mean “to\nbe filled in later,” to stub out the bodies of functions temporarily:\ndef func1():\n    pass                 # Add real code here later\ndef func2():\n    pass\nWe can’t leave the body empty without getting a syntax error, so we say pass\ninstead.",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "The ellipsis-literal alternative\nDespite the limited roles, there’s a similar, if more obscure, way to achieve the\nsame effect as pass. Python allows an ellipsis, coded as ... (literally, three\nconsecutive dots, not the Unicode character), to appear any place an expression\ncan. Because ellipses do nothing by themselves, they can serve as an alternative\nto the pass statement, especially for code to be filled in later—a sort of Python\nTBD:\ndef func1():\n    ...                   # Alternative to pass\nfunc1()                   # Does nothing if called\nThis works because any expression can appear as a statement (as we learned in\nChapter 11), and the ... literal qualifies as an expression. Ellipses can also\nappear on a statement header by itself, and may be used to initialize variable\nnames if no specific type is required—which also makes it an alternative to\nNone:\ndef func1(): ...          # Works on same line too\n>>> tbd = ...             # Alternative to both pass and None\n>>> tbd                   # Ellipsis is a real (if oddball!) thing\nEllipsis\nThis goes well beyond the original intent of ... in slicing extensions (which,\nlike the @ operator and type hinting, is unused by Python itself), so time will tell\nif it rises to challenge pass and None in these inane and vacuous roles.\ncontinue\nThe continue statement causes an immediate jump to the top of a loop. It’s\noften used to avoid statement nesting, as in the next example that uses continue\nto skip odd numbers. This code prints all even numbers less than 10 and greater\nthan or equal to 0. Recall that 0 means false and % is the remainder-of-division\n(modulus) operator, so this loop counts down to 0, skipping numbers that aren’t\nmultiples of 2—and prints 8 6 4 2 0:",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "x = 10\nwhile x:\n    x -= 1                         # Or, x = x - 1\n    if x % 2 != 0: continue        # Odd? -- skip print\n    print(x, end=' ')\nBecause continue jumps to the top of the loop, you don’t need to nest the print\nstatement here inside an if test; the print is only reached if the continue is not\nrun.\nThe nested-code alternative\nIf all this sounds similar to a “go to” in other languages, it should. Python has no\n“go to” statement, but because continue lets you jump about in a program,\nmany of the warnings about readability and maintainability you may have heard\nabout “go to” apply. continue should probably be used sparingly, especially\nwhen you’re first getting started with coding. For instance, the last example\nmight be clearer if the print were nested under the if:\nx = 10\nwhile x:\n    x -= 1\n    if x % 2 == 0:                 # Even? -- print\n        print(x, end=' ')\nLater in this book, you’ll also learn that raised and caught exceptions can also\nemulate “go to” statements in limited and structured ways; stay tuned for more\non this technique in Chapter 36 where you will learn how to use it to break out\nof multiple nested loops, a feat not possible with the next section’s topic alone.\nbreak\nThe break statement causes an immediate exit from a loop—technically, from\nthe closest enclosing loop, when loops are nested. Because the code that follows\nit in the loop is not executed if the break is reached, it can sometimes avoid\nnesting much like continue. For example, here is a simple interactive loop (a\ntakeoff on code we studied in Chapter 10) that inputs data with input and exits\nwhen the user enters a “stop” line:",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": ">>> num = 1\n>>> while True:\n        tool = input(f'{num}) What\\'s your favorite language? ')\n        if tool == 'stop': break\n        print('Bravo!' if tool == 'Python' else 'Try again...')\n        num += 1\n \n1) What's your favorite language? Java\nTry again...\n2) What's your favorite language? Python\nBravo!\n3) What's your favorite language? stop\nBecause the break in this terminates the while immediately, there’s no reason to\nnest code below it in an else.\nThe named-assignment alternative\nThat said, it’s also possible to use the newer := expression we met in “Named\nAssignment Expressions” to crunch this example—albeit, at the expense of its\nrole as a break demo. While you should judge for yourself, the net effect is\nconcise but may at least flirt with unreadability:\n>>> num = 1\n>>> while (tool := input(f'{num}) What\\'s your favorite language? ')) != 'stop':\n        print('Bravo!' if tool == 'Python' else 'Try again...')\n        num += 1\n    \n1) What's your favorite language? Python\nBravo!\nNesting := within := as in the following, however, could easily incite pitchforks\nand torches (in fact, the full one-liner here is too wide for this book!). Unless\nyou can defend this in a court of your code-reuse peers, just say no:\nnum = 0\nwhile (tool := input(f'{(num := num + 1)}) What\\'s your favorite language? ')) != \n'stop':\nPreview: in Chapter 36, you’ll see that input also raises an exception at end-of-\nfile (e.g., if the user enters Ctrl+Z on Windows or Ctrl+D on Unix); wrapping\ninput in try statements allows users to respond this way too.",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "Loop else\nWhen combined with the loop else clause, the break statement can often\neliminate the need for the search status flags used in other languages. In abstract\nterms the break in the following skips the else on the way out of the loop:\nwhile continuing:\n    if found: \n        found code\n        break\n    else advance\nelse:\n    not-found code\nAs a more concrete example, the following piece of code determines whether a\npositive integer num is prime—has no factors other than 1 and itself—by\nsearching for factors greater than 1 (to run live, assign num before pasting):\nx = num // 2                              # For some num > 1, start at half\nwhile x > 1:\n    if num % x == 0:                      # Remainder 0? Factor found\n        print(num, 'has factor', x)\n        break                             # Exit now and skip else\n    x -= 1\nelse:                                     # Normal exit, when x reaches 1\n    print(num, 'is prime')\nRather than setting a flag to be tested when the loop is exited, it inserts a break\nwhere a factor is found. This way, the loop else clause can assume that it will be\nexecuted only if no factor is found; if this code never hits the break, the number\nis prime. Trace through this code to see how this works.\nThe loop else clause is also run if the body of the loop is never executed, as you\ndon’t run a break in that event either; in a while loop, this happens if the test in\nthe header is false to begin with. Thus, in the preceding example you still get the\n“is prime” message if x is initially less than or equal to 1 (for instance, if num is\n2).\nNOTE",
      "content_length": 1604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "Subprime code: This example determines primes, but only informally so. Numbers less than 2\nare not considered prime by the strict mathematical definition, but 1 and 0 are classified as\nsuch here. To be really picky, this code also fails for negative numbers and succeeds for\nfloating-point numbers with all-zero decimal digits. Also note that its code must use // instead\nof / because we need the initial division to truncate remainders, not retain them. If you want to\nexperiment with this code further, watch for its associated exercise at the end of Part IV, which\nwraps it in a function for reuse.\nWhy the loop else?\nBecause the loop else clause is unique to Python, it tends to perplex some\nnewcomers (and even some veterans; in fact, a few either pointlessly code the\nloop else without a break or don’t know that the loop else exists at all!). In\ngeneral terms, the loop else simply provides explicit syntax for a common\ncoding scenario—it is a coding structure that lets us catch the “other” way out of\na loop, without setting and checking flags or conditions.\nSuppose, for instance, that we are writing a loop to search a list for a value and\nneed to know whether the value was found after you exit the loop. We might\ncode such a task this way (this code is intentionally abstract and incomplete; x is\na sequence and match is a tester function to be defined):\nfound = False\nwhile x and not found:\n    if match(x[0]):                  # Value at front?\n        print('Found')\n        found = True\n    else:\n        x = x[1:]                    # Slice off front and repeat\nif not found:\n    print('Not found')\nHere, we initialize, set, and later test a found flag to determine whether the\nsearch succeeded or not. This is valid Python code, and it does work; however,\nthis is exactly the sort of structure that the loop else clause is meant to handle.\nHere’s an else equivalent:\nwhile x:                             # Exit when x empty\n    if match(x[0]):\n        print('Found')\n        break                        # Exit, go around else",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "x = x[1:]\nelse:\n    print('Not found')               # Only here if exhausted x\nThis version is more concise. The flag is gone, and we’ve replaced the if test at\nthe loop end with an else (lined up vertically with the word while). Because the\nbreak inside the main part of the while exits the loop and goes around the else,\nthis serves as a more structured way to catch the search-failure case.\nSome readers might have noticed that the prior example’s else clause could be\nreplaced with a test for an empty x after the loop (e.g., if not x:). Although\nthat’s true in this example, the else provides explicit syntax for this coding\npattern (it’s more obviously a search-failure clause here), and such an explicit\nempty test may not apply in some cases. The loop else becomes even more\nuseful when used in conjunction with the for loop—the topic of the next section\n—because sequence iteration is not under your control.\nfor Loops\nThe for loop is a generic iterator in Python: it can step through the items in any\nordered sequence or other iterable object. All told, the for statement works on\nstrings, lists, tuples, sets, dictionaries, and all other built-in iterables, as well as\nnew user-defined objects that you’ll learn how to create later with classes. We\nmet for briefly in Chapter 4 and have used it in conjunction with sequence\nobject types; let’s expand on its usage more formally here.\nGeneral Format\nThe Python for loop begins with a header line that specifies an assignment\ntarget (or targets), along with the object you want to step through. The header is\nfollowed by a block of (normally indented) statements that you want to repeat:\nfor target in object:           # Assign object items to target\n    statements                  #     Repeated loop body: use target\nelse:                           # Optional else\n    statements                  #     Run if didn't exit loop body with break",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "When Python runs a for loop, it assigns the items in the iterable object to the\ntarget one by one and executes the loop body for each. The loop body typically\nuses the assignment target to refer to the current item in the sequence as though\nit were a cursor stepping through the sequence.\nWhile target can be any assignment target we met in Chapter 11, it’s often just\na simple name. This name is a possibly new variable that lives in the scope\nwhere the for statement itself is coded. There’s not much unique about this\nname; it can even be changed inside the loop’s body, but it will automatically be\nset to the next item in object when control returns to the top of the loop again.\nAfter the loop this variable normally still refers to the last item visited, which is\nthe last item in the sequence unless the loop exits early with a break statement.\nThe for statement also supports an optional else block, which works exactly as\nit does in a while loop—it’s executed if the loop exits without running into a\nbreak statement (i.e., if all items in the sequence have been visited). The break\nand continue statements introduced earlier also work the same in a for loop as\nthey do in a while. Given all that, the for loop’s complete format can be\ndescribed this way:\nfor target in object:           # Assign object items to target\n    statements\n    if test: break              # Exit loop now, skip else\n    if test: continue           # Go to top of loop now\nelse:\n    statements                  # Run on exit if didn't hit a 'break'\nExamples\nLet’s type a few for loops interactively now, so you can see how they are used\nin practice.\nBasic usage\nAs mentioned earlier, a for loop can step across any kind of sequence object. In\nour first example, for instance, we’ll assign the name x to each of the three items\nin a list in turn, from left to right, and the print statement will be executed for\neach. Inside the print statement (the loop body), the name x refers to the current",
      "content_length": 1980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "item in the list:\n>>> for x in ['app', 'script', 'program']:\n        print(x, end=' ')\n \napp script program\nThe next two examples compute the sum and product of all the items in a list.\nLater in this chapter and later in this book you’ll meet tools that apply operations\nsuch as + and * to items in a list automatically, but it’s often just as easy to use a\nfor:\n>>> sum = 0\n>>> for x in [1, 2, 3, 4]:\n        sum = sum + x\n   \n>>> sum\n10\n>>> prod = 1\n>>> for item in [1, 2, 3, 4]: prod *= item\n   \n>>> prod\n24\nOther data types\nAny sequence works in a for, as it’s a generic tool. For example, for loops also\nwork on strings and tuples:\n>>> S = 'Python'\n>>> T = ('web', 'num', 'app')\n>>> for x in S: print(x, end=' ')      # Iterate over a string\n   \nP y t h o n\n>>> for x in T: print(x, end=' ')      # Iterate over a tuple\n   \nweb num app\nIn fact, as we’ll explore in the next chapter when we formalize the notion of\niterables, for loops can even work on some objects that are not sequences—\nincluding files.",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "Tuple (sequence) assignment in for loops\nIf you’re iterating through a sequence of tuples, the loop target itself can actually\nbe a tuple of targets. This is just another case of the tuple-unpacking assignment\nwe studied in Chapter 11 at work. Remember, the for loop assigns items in the\nsequence object to the target, and assignment works the same everywhere:\n>>> T = [(1, 2), (3, 4), (5, 6)]\n>>> for (a, b) in T:                   # Tuple assignment at work\n        print(a, b)\n   \n1 2\n3 4\n5 6\nHere, the first time through the loop is like running (a,b) = (1,2), the second\ntime is like (a,b) = (3,4), and so on. The net effect is to automatically unpack\nthe current tuple on each iteration. List syntax works as a for target too because\ntuple and list assignment are both sequence assignment, and tuple parentheses\nare optional:\n>>> for [a, b] in T:                   # List assignment: same effect\n>>> for a, b in T:                     # Tuple sans parentheses: same effect\nThis list-of-tuples data format is commonly used in conjunction with the zip call\nyou’ll meet later in this chapter, to implement parallel traversals. It also crops up\nin conjunction with SQL databases in Python, where query result tables are\nreturned as sequences of sequences like the list used here—the outer list is the\ndatabase table, the nested tuples are the rows within the table, and tuple (i.e.,\nsequence) assignment extracts columns.\nAs we’ve seen in earlier chapters, tuples in for loops also come in handy to\niterate through both keys and values in dictionaries using the items method,\nrather than looping through the keys and indexing to fetch the values manually:\n>>> D = {'a': 1, 'b': 2}\n>>> for key in D:\n       print(key, '=>', D[key])        # Use dict keys iterator and index\n   \na => 1",
      "content_length": 1785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "b => 2\n>>> list(D.items())\n[('a', 1), ('b', 2)] \n>>> for (key, value) in D.items():\n       print(key, '=>', value)         # Iterate over both keys and values\n   \na => 1\nb => 2\nIt’s important to note that tuple assignment in for loops isn’t a special case; any\nassignment target works syntactically after the word for. For example, we can\nalways assign manually within the loop to unpack:\n>>> T\n[(1, 2), (3, 4), (5, 6)]\n>>> for both in T:\n        a, b = both                    # Manual assignment equivalent\n        print(a, b)\n   \n1 2\n3 4\n5 6\nBut tuples in the loop header save us an extra step when iterating through\nsequences of sequences. As suggested in Chapter 11, even nested structures may\nbe automatically unpacked this way in a for:\n>>> ((a, b), c) = ((1, 2), 3)          # Nested sequences work too\n>>> a, b, c\n(1, 2, 3)\n>>> for ((a, b), c) in [((1, 2), 3), ((4, 5), 6)]: print(a, b, c)\n   \n1 2 3\n4 5 6\nEven this is not a special case, though—the for loop simply runs the sort of\nassignment we ran just before it, on each iteration. Any nested sequence\nstructure may be unpacked this way, simply because sequence assignment is so\ngeneric:",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": ">>> for ((a, b), c) in [([1, 2], 3), ['XY', 6]]: print(a, b, c)\n   \n1 2 3\nX Y 6\nExtended-unpacking assignment in for loops\nIn fact, because the loop variable in a for loop can be any assignment target, we\ncan also use the starred names and other targets of extended-unpacking\nassignment here to extract both items and sections of sequences within\nsequences. Because this works in assignment statements, it automatically works\nin for loops too.\nThis topic was introduced in Chapter 11, but here’s a quick refresher to reinforce\nthe technique. Consider the tuple assignment form introduced in the prior\nsection. A tuple of values is assigned to a tuple of names on each iteration,\nexactly like a simple assignment statement:\n>>> a, b, c = (1, 2, 3)                               # Tuple assignment\n>>> a, b, c\n(1, 2, 3)\n>>> for (a, b, c) in [(1, 2, 3), (4, 5, 6)]:          # Used in for loop\n        print(a, b, c)\n   \n1 2 3\n4 5 6\nBecause sequence assignment supports a more general set of names with a\nstarred target to collect multiple items, we can use the same syntax to extract\nparts of nested sequences in the for loop:\n>>> a, *b, c = (1, 2, 3, 4)                           # Extended-unpacking \nassignment\n>>> a, b, c\n(1, [2, 3], 4)\n>>> for (a, *b, c) in [(1, 2, 3, 4), (5, 6, 7, 8)]:\n        print(a, b, c)\n   \n1 [2, 3] 4\n5 [6, 7] 8",
      "content_length": 1339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "In practice, this approach might be used to pick out multiple columns from rows\nof data represented as nested sequences. As usual in Python, you can achieve\nsimilar effects with more basic tools—in this case by slicing. The only difference\nis that slicing returns a type-specific result, whereas starred targets always\nreceive lists:\n>>> for all in [(1, 2, 3, 4), (5, 6, 7, 8)]:          # Manual slicing version\n        a, b, c = all[0], all[1:-1], all[-1]\n        print(a, b, c)\n   \n1 (2, 3) 4\n5 (6, 7) 8\nFinally, all the starred-target forms work in for as in = assignment statements,\nincluding nested sequences, indexes, and slices (though practical roles for code\nlike the following are probably much more rare than common!):\n>>> L, M = [1, 2], [3, 4]\n>>> pairs = [[(5, 6), (7, 8), (9, 10)]] * 2\n \n>>> for [(a, *X), (b, *L[0]), (c, *M[:0])] in pairs:\n        print(f'<{a=} {X=}>  <{b=} {L=}>  <{c=} {M=}>')\n \n<a=5 X=[6]>  <b=7 L=[[8], 2]>  <c=9 M=[10, 3, 4]>\n<a=5 X=[6]>  <b=7 L=[[8], 2]>  <c=9 M=[10, 10, 3, 4]>\nSee Chapter 11 for more on the extended-unpacking form of assignment.\nNested for loops\nNow let’s look at some for loops that are a bit more sophisticated than those\ndemoed so far. The first shows what happens when for loops are nested—the\ninner loop is run for every iteration of the outer loop, and the + within the inner\nloop combines their items by using each loop’s variable:\n>>> for x in 'abc':                         # For each item in one string\n        for y in '123':                     # And for each item in another string\n            print(x + y, end=' ')           # Concatenate current items from both\n \na1 a2 a3 b1 b2 b3 c1 c2 c3",
      "content_length": 1664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "The next example kicks the nesting up a notch, illustrating both three-level\nstatement nesting and the loop else clause in a for. Given a list of objects\n(items) and a list of keys (tests), this code searches for each key in the objects\nlist and reports on the search’s outcome:\n>>> items = ['aaa', 111, (4, 5), 2.01]      # A list of objects\n>>> tests = [(4, 5), 3.14]                  # Keys to search for\n>>>\n>>> for key in tests:                       # For all keys\n        for item in items:                  # For all items\n            if item == key:                 # Check for match\n                print(key, 'was found')\n                break\n        else:\n            print(key, 'not found!')\n   \n(4, 5) was found\n3.14 not found!\nBecause the nested if runs a break when a match is found, the inner loop’s\nelse clause can assume that if it is reached, the search has failed. Notice the\nnesting here. When this code runs, there are two loops going at the same time:\nthe outer loop scans the keys list, and the inner loop scans the items list for each\nkey. The nesting of the loop else clause is critical; it’s indented to the same\nlevel as the header line of the inner for loop, so it’s associated with the inner\nloop, not the if or the outer for.\nThe preceding example is illustrative, but it may be easier to code if we employ\nthe in operator to test membership. Because in implicitly scans an object\nlooking for a match (at least logically), it replaces the inner loop:\n>>> for key in tests:                       # For all keys\n        if key in items:                    # Let Python check for a match\n            print(key, 'was found')\n        else:\n            print(key, 'not found!')\n   \n(4, 5) was found\n3.14 not found!\nIn general, it’s a good idea to let Python do as much of the work as possible (as",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "in this solution) for the sake of both brevity and performance.\nOur final example is similar, but builds a list as it goes for later use instead of\nprinting. It performs a typical data-structure task with a for—collecting\ncommon items in two sequences (it’s nearly intersection, unless there are\nduplicate values). After the loop runs, res refers to a list that contains all the\nitems found in seq1 and seq2:\n>>> seq1 = 'trippy'\n>>> seq2 = 'python'\n>>>\n>>> res = []                                # Start empty\n>>> for x in seq1:                          # Scan first sequence\n        if x in seq2:                       # Common item?\n            res.append(x)                   # Add to result end\n   \n>>> res\n['t', 'p', 'p', 'y']\nUnfortunately, this code is equipped to work only on two specific variables:\nseq1 and seq2. It would be nice if this loop could somehow be generalized into\na tool you could use more than once. As you’ll see, that simple idea leads us to\nfunctions, the topic of the next part of the book.\nOf course, if you read Chapter 4 or Chapter 5, you know that Python has sets\nthat provide true intersection with the & operator—but the result’s order is\nscrambled, duplicates are dropped, and multiple conversions are required to\nmatch:\n>>> list(set(seq1) & set(seq2))             # Real intersection with sets\n['p', 'y', 't']\nMore usefully, this code also exhibits the classic list comprehension pattern—\ncollecting a results list with an iteration and optional filter test—and could be\ncoded much more concisely with this tool:\n>>> [x for x in seq1 if x in seq2]          # Let Python collect results\n['t', 'p', 'p', 'y']\nBut you’ll have to read on to the next chapter for the rest of this story.",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "Loop Coding Techniques\nThe for loop we just studied subsumes most counter-style loops. It’s generally\nsimpler to code and often quicker to run than a while, so it’s the first tool you\nshould reach for whenever you need to step through a sequence or other iterable.\nIn fact, as a general rule, you should resist the temptation to count things in\nPython—its iteration tools automate much of the work you do to loop over\ncollections in lower-level languages like C.\nStill, there are situations where you will need to iterate in more specialized ways.\nFor example, what if you need to visit every second or third item in a list, or\nchange the list along the way? How about traversing more than one sequence in\nparallel, in the same for loop? What if you need indexes too?\nYou can always code such unique iterations with a while loop and manual\nindexing, but Python provides a set of built-ins that allow you to specialize the\niteration in a for:\nThe built-in range function produces a series of successively higher\nintegers, which can be used as indexes in a for.\nThe built-in zip function returns a series of parallel-item tuples, which\ncan be used to traverse multiple sequences in a for.\nThe built-in enumerate function generates both the values and indexes\nof items in an iterable, so we don’t need to count manually.\nBecause for loops may run quicker than while-based counter loops, it’s to your\nadvantage to use tools like these that allow you to use for whenever possible.\nLet’s look at each of these built-ins in turn, in the context of common roles. As\nyou’ll see, some loop-coding alternatives are more valid than others.\nCounter Loops: range\nOur first loop-related function, range, is a general tool that can be used in a\nvariety of contexts. We met it briefly in Chapter 4 and have used it occasionally\nalong the way. Although it’s used often to generate indexes in a loop, you can\ncall it anywhere you need a series of integers (see Chapter 11’s enumerated-",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "names trick for a prime example).\nAs we’ve seen, range is an iterable that generates items on demand, so we need\nto wrap it in a list call to display all its results at once in a REPL. Surprisingly,\nrange’s results support some sequence operations, but not all; per Chapter 9, it\nwas reclassified in Python’s docs as a sort of sequence object type, though one\nwith less functionality than lists and tuples—and much less basis in reality:\n>>> list(range(5)), list(range(2, 5)), list(range(0, 10, 2))\n([0, 1, 2, 3, 4], [2, 3, 4], [0, 2, 4, 6, 8])\n>>> range(5)[2], range(5)[1:3], list(range(5)) + [6, 7]\n(2, range(1, 3), [0, 1, 2, 3, 4, 6, 7])\n>>> range(5) + [6, 7]\nTypeError: unsupported operand type(s) for +: 'range' and 'list'\nCategorization aside, range usage is straightforward, With one argument, range\ngenerates a series of integers from zero up to but not including the argument’s\nvalue. If you pass in two arguments, the first is taken as the lower bound. And an\noptional third argument can give a step; if it is used, Python adds the step to each\nsuccessive integer in the result (the step defaults to +1). Ranges can also be\nnonpositive and nonascending, if you need them to be:\n>>> list(range(-5, 5))\n[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n>>> list(range(5, -5, -1))\n[5, 4, 3, 2, 1, 0, -1, -2, -3, -4]\nWe’ll take a deeper look at iterables in Chapter 14. In this case, Python 2.X had\nan optimized built-in named xrange, which was like its range but didn’t build a\nresult list in memory all at once; which was later superseded in 3.X by the\ngenerator behavior of its range; which was later rebranded a sequence by 3.X\ndocs (confusingly!). The upshot of this long walk is that today’s range doesn’t\nconsume much space, because it produces numbers only on demand.\nAlthough the preceding range results may be useful all by themselves, they tend\nto come in most handy within for loops. For one thing, they provide a simple\nway to repeat an action a specific number of times. To print three lines, for",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "example, use a range to generate the appropriate number of integers:\n>>> for i in range(3):\n        print(i, 'Pythons')\n   \n0 Pythons\n1 Pythons\n2 Pythons\nNote that for loops force results from range automatically, so we don’t need to\nuse a list wrapper here. In fact, we shouldn’t: letting range produce its results\none at a time uses much less memory than forcing them all at once.\nSequence Scans: while, range, and for\nThe range call is also sometimes used to iterate over a sequence indirectly,\nthough it’s often not the best approach in this role. The easiest and fastest way to\nstep through a sequence exhaustively is almost always with a simple for,\nbecause Python handles most of the details for you in quick, internal code:\n>>> X = 'hack'\n>>> for item in X: print(item, end=' ')           # Automatic iteration with for\n   \nh a c k\nInternally, the for loop handles the details of the iteration automatically when\nused this way. If you really need to take over the indexing logic explicitly (and\nsometimes you may), you can do it with a while loop:\n>>> i = 0\n>>> while i < len(X):                             # Manual iteration with while\n        print(X[i], end=' ')\n        i += 1\n   \nh a c k\n>>> i = -1\n>>> while (i := i + 1) < len(X):                  # Manual, but with := operator\n        print(X[i], end=' ')\nh a c k",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "You can also do manual indexing with a for, though, if you use range to\ngenerate indexes to iterate through. It’s a multistep process—you must ask for\nthe range of the subject’s len—but it’s sufficient to generate offsets, rather than\nthe items at those offsets:\n>>> X\n'hack'\n>>> len(X)                                        # Length of string\n4\n>>> list(range(len(X)))                           # All legal offsets into X\n[0, 1, 2, 3]\n>>>\n>>> for i in range(len(X)): print(X[i], end=' ')  # Manual range/len iteration\n   \nh a c k\nImportantly, because this example is stepping over a list of offsets into X, not the\nactual items of X, we need to index back into X within the loop to fetch each\nitem. If this seems like overkill, though, it’s because it is: there’s really no reason\nto work this hard in this example.\nAlthough the range/len combination is useful in some roles, it’s probably not\nthe best option in most. It may run slower, and it’s also more code than we need\nto write. Unless you have a special indexing requirement, you’re better off using\nthe simple for loop form in Python:\n>>> for item in X: print(item, end=' ')           # Use auto iteration if you can\nAs guidelines, use for instead of while whenever possible, and don’t use range\ncalls in for loops except as a last resort. This simpler solution is almost always\nbetter. Like every good guideline, though, there are plenty of exceptions—as the\nnext section demonstrates.\nSequence Shufflers: range and len\nThough not ideal for simple sequence scans, the range/len coding pattern used\nin the prior example does allow us to do more specialized sorts of traversals\nwhen required. For example, some algorithms can make use of sequence\nreordering—to generate alternatives in searches, to test the effect of different",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "value orderings, and so on. Such cases may require offsets in order to pull\nsequences apart and put them back together, as in the following; its range’s\nintegers provide a repeat count in the first, and a position for slicing in the\nsecond:\n>>> S = 'hack'\n>>> for i in range(len(S)):       # For repeat counts 0..3\n        S = S[1:] + S[:1]         # Move front item to end\n        print(S, end=' ')\n   \nackh ckha khac hack\n>>> S\n'hack'\n>>> for i in range(len(S)):       # For positions 0..3\n        X = S[i:] + S[:i]         # Rear part + front part\n        print(X, end=' ')\n   \nhack ackh ckha khac\nTrace through these one iteration at a time if they seem confusing. The second\ncreates the same results as the first, though in a different order, and doesn’t\nchange the original variable as it goes. Because both slice to obtain parts to\nconcatenate, they also work on any type of sequence, and return sequences of the\nsame type as that being shuffled—if you shuffle a list, you create reordered lists:\n>>> L = [1, 2, 3, 4]\n>>> for i in range(len(L)):\n        X = L[i:] + L[:i]         # Works on any sequence type\n        print(X, end=' ')\n \n[1, 2, 3, 4] [2, 3, 4, 1] [3, 4, 1, 2] [4, 1, 2, 3]\nThe results of range itself, however, don’t make the grade in either coding\n(they’re not true sequences!):\n>>> L = range(4)\n>>> …same code as prior example…\nTypeError: unsupported operand type(s) for +: 'range' and 'range'\nWe’ll make use of code like this to test functions with different argument\norderings in Chapter 18, and will extend it to functions, generators, and more",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "complete permutations in Chapter 20—it’s a widely useful tool.\nSkipping Items: range and Slices\nThe prior section showed one valid applications for the range/len combination.\nWe might also use this technique to skip items as we go:\n>>> S = 'abcdefghijk'\n>>> list(range(0, len(S), 2))\n[0, 2, 4, 6, 8, 10]\n>>> for i in range(0, len(S), 2): print(S[i], end=' ')\n   \na c e g i k\nHere, we visit every second item in the string S by stepping over the generated\nrange list. To visit every third item, change the third range argument to be 3,\nand so on. In effect, using range this way lets you skip items in loops while still\nretaining the simplicity of the for statement.\nIn many or most cases, though, this is also probably not the “best practice”\ntechnique in Python today. If you really mean to skip items in a sequence, the\nextended three-limit form of the slice expression, presented in Chapter 7,\nprovides a simpler route to the same goal. To visit every second character in S,\nfor example, slice with a stride of 2:\n>>> S = 'abcdefghijk'\n>>> for c in S[::2]: print(c, end=' ')\n   \na c e g i k\nThe result is the same, but substantially easier for you to write and for others to\nread. The potential advantage to using range here instead is space: slicing makes\na copy of the string, while range does not—and hence may save significant\nmemory for very large strings. Naturally, whether your program needs to care\ndepends on what it does.\nChanging Lists: range and Comprehensions",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "Another common place where you may use the range/len combination with for\nis in loops that change a list as it is being traversed. Suppose, for example, that\nyou need to add 1 to every item in a list (maybe you’re updating ages at year\nend). You can try this with a simple for loop, but the result may not be what you\nwant or expect:\n>>> L = [10, 20, 30, 40, 50]\n>>> for x in L:\n        x += 1                       # Changes x, not L!\n   \n>>> L                                # L's objects unchanged\n[10, 20, 30, 40, 50] \n>>> x                                # x is not a cursor into L\n51\nThis doesn’t quite work—it changes the loop variable x, not the list L. The\nreason is somewhat subtle. Each time through the loop, x refers to the next\ninteger already pulled out of the list. In the first iteration, for example, x is\ninteger 10, taken from L, When we then add to x in the loop body with +=, it sets\nx to a different object, integer 11, but it does not update the list where 10\noriginally came from; the new 11 is a piece of memory separate from the list.\nTo really change the list as we march across it, we need to use indexes so we can\nassign an updated value to each position as we go. The range/len combination\ncan produce the required indexes for us:\n>>> L = [10, 20, 30, 40, 50]\n>>> for i in range(len(L)):          # Add one to each item in L\n        L[i] += 1                    # Or L[i] = L[i] + 1\n   \n>>> L\n[11, 21, 31, 41, 51]\nWhen coded this way, the list is changed as we proceed through the loop. There\nis no way to do the same with a simple for x in L, because such a loop iterates\nthrough actual items, not their positions. But what about the equivalent while\nloop? Such a loop requires a bit more work on our part, and might run more\nslowly depending on your Python, your host device, and perhaps the alignment",
      "content_length": 1834,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "of planets (you’ll see how to check such claims in Chapter 21):\n>>> i = 0\n>>> while i < len(L):                # And similar with := assignment\n        L[i] += 1\n        i += 1\n   \n>>> L\n[12, 22, 32, 42, 52]\nHere again, though, the range solution may not be ideal either. A list\ncomprehension expression of the form:\n>>> [x + 1 for x in L]\n[13, 23, 33, 43, 53]\nlikely runs faster today and would do similar work, albeit without changing the\noriginal list in place (we could assign the expression’s new list object result back\nto L, but this would not update any other references to the original list). Because\nthis is such a central looping concept, we’ll save a complete exploration of list\ncomprehensions for the next chapter, and tell the rest of the statements story of\nloops there.\nParallel Traversals: zip\nOur next loop coding technique adds to its bag of tricks. As we’ve seen, the\nrange built-in allows us to traverse sequences with for in a nonexhaustive\nfashion. In a similar spirit, the built-in zip function allows us to use for loops to\nvisit multiple sequences in parallel—not overlapping in time, but during the\nsame loop. In basic operation, zip takes one or more arguments (sequences or\nother iterables) and returns a series of tuples that pair up parallel items taken\nfrom those arguments. For example, suppose we’re working with two lists of\ndata paired by position:\n>>> L1 = [1, 2, 3, 4]\n>>> L2 = [5, 6, 7, 8]\nTo combine the items in these lists, we can use zip to create a list of tuple pairs.",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "Like range, zip is an iterable object, so we must wrap it in a list call to collect\nand display all its results at once (again, the next chapter will be more formal\nabout iterables like this):\n>>> zip(L1, L2)                             # An iterable that generates pairs\n<zip object at 0x026523C8>\n>>> list(zip(L1, L2))                       # list() required to see all results\n[(1, 5), (2, 6), (3, 7), (4, 8)]\nSuch a result may be useful in other contexts as well, but when wedded with the\nfor loop, it supports parallel iterations:\n>>> for (x, y) in zip(L1, L2):\n        print(f'{x} + {y} => {x + y}')\n    \n1 + 5 => 6\n2 + 6 => 8\n3 + 7 => 10\n4 + 8 => 12\nHere, we step over the result of the zip call—that is, the pairs of items pulled\nfrom the two lists. Notice that this for loop again uses the tuple (a.k.a. sequence)\nassignment form we met earlier to unpack each tuple in the zip result. The first\ntime through, it’s as though we ran the assignment statement (x, y) = (1, 5) ;\nand so on.\nThe net effect is that we scan both L1 and L2 in our loop. To be sure, we could\nachieve a similar effect with a while loop that handles indexing manually—like\nthe following that produces the same output as the preceding:\n>>> i = -1\n>>> while (i := i + 1) < len(L1):\n       print(f'{L1[i]} + {L2[i]} => {L1[i] + L2[i]}')\nBut this requires noticeably more code, and hence would likely run slower than\nthe for/zip approach. Moreover, it’s no better on space: being an iterable, zip\nmakes just one pair per loop, and so does not consume memory needlessly. The\nclincher, though, is that this is not really equivalent to zip—for reasons\ndisclosed in the next section.",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "More on zip: size and truncation\nFor the record, the zip function is more general than the prior example suggests.\nFor instance, it both is an iterable and accepts any type of iterable object,\nincluding range results, input files, and more:\n>>> list(zip(range(4), 'hack'))\n[(0, 'h'), (1, 'a'), (2, 'c'), (3, 'k')]\nIn addition, zip is not just for two-item pairs: it accepts any number of\narguments, of any size. The following, for example, builds a list of three-item\ntuples for three arguments, with items from each sequence—essentially\nprojecting by columns:\n>>> T1, T2, T3 = (1, 2, 3), (4, 5, 6), (7, 8, 9)\n>>> T3\n(7, 8, 9)\n>>> list(zip(T1, T2, T3))                   # 3 args of 3 vals => 3 3-item tuples\n[(1, 4, 7), (2, 5, 8), (3, 6, 9)]\nAnd formally speaking, for N arguments that contain M items, zip gives us an\nM-long series of N-ary tuples:\n>>> list(zip(T1, T2))                       # 2 args of 3 vals => 3 2-item tuples\n[(1, 4), (2, 5), (3, 6)]\nWhen argument lengths differ, zip truncates the series of result tuples at the\nlength of the shortest sequence. To demo, the following zips two strings to pick\nout characters in parallel, but the result has only as many tuples as the length of\nthe shortest sequence (formally again, M in the prior definition is really the\nminimum of arguments’ lengths):\n>>> S1 = 'abc'\n>>> S2 = 'xyz123'\n>>>\n>>> list(zip(S1, S2))                       # Truncates at len(shortest)\n[('a', 'x'), ('b', 'y'), ('c', 'z')]\nTo pad instead of truncating, you can write loop code to pad results yourself—as\nwe will in Chapter 20, after we’ve had a chance to study some additional",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "iteration concepts that make it a fair fight.\nMore zip roles: dictionaries\nFine points aside, parallel traversals with zip are also useful in dictionary\nconstruction. We met this technique in Chapter 8, but here’s a quick refresher in\nthe context of looping statements. As we learned earlier, you can always create a\ndictionary by calling dict, assigning to keys over time, or coding a dictionary\nliteral like the following:\n>>> D1 = {'app': 1, 'script': 3, 'program':5}          # Or dict(key=value,…)\n>>> D1\n{'app': 1, 'script': 3, 'program': 5}\nWhat to do, though, if your program obtains dictionary keys and values at\nruntime, after you’ve coded your script? For example, the following may be\ncollected from a user, a file, or any other dynamic source:\n>>> keys = ['app', 'script', 'program']\n>>> vals = [1, 3, 5]\nOne way to turn these into a dictionary is to zip the lists and step through them\nin parallel with a for loop:\n>>> list(zip(keys, vals))\n[('app', 1), ('script', 3), ('program', 5)] \n>>> D2 = {}\n>>> for (k, v) in zip(keys, vals): D2[k] = v\n   \n>>> D2\n{'app': 1, 'script': 3, 'program': 5}\nAs suggested earlier in this book, though, you can skip the for loop altogether in\nthis context, and simply pass the zipped keys/values lists to the built-in dict\nconstructor call:\n>>> D3 = dict(zip(keys, vals))\n>>> D3\n{'app': 1, 'script': 3, 'program': 5}",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "The built-in name dict is really a type name (you’ll learn about type names, and\nsubclassing them, in Chapter 32). Calls to it are object construction requests, but\nalso perform a to-dictionary conversion here. In the next chapter, you’ll also\nlearn more about related but richer concepts—list comprehensions, which build\nlists in expressions, and their dictionary comprehensions kin, which are an\nalternative to both for statements and dict for zipped key/value pairs:\n>>> {k: v for (k, v) in zip(keys, vals)}\n{'app': 1, 'script': 3, 'program': 5}\nOffsets and Items: enumerate\nOur final loop-helper function is designed to support dual usage modes. Earlier,\nwe discussed using range to generate the offsets of items in a string, rather than\nthe items at those offsets. In some programs, though, we need both: the item to\nuse, plus an offset as we go. This might be coded with a for loop that also keeps\na counter of the current offset:\n>>> S = 'hack'\n>>> offset = 0\n>>> for item in S:\n        print(item, 'appears at offset', offset)\n        offset += 1\n   \nh appears at offset 0\na appears at offset 1\nc appears at offset 2\nk appears at offset 3\nThis works, but Python has a built-in function named enumerate that does the\njob for us—its net effect is to give loops a counter “for free,” without sacrificing\nthe simplicity of automatic iteration:\n>>> S = 'hack'\n>>> for (offset, item) in enumerate(S):\n        print(item, 'appears at offset', offset)\nh appears at offset 0\na appears at offset 1\nc appears at offset 2\nk appears at offset 3",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "As for range and zip, the enumerate function’s result is an iterable—a kind of\nobject that supports the iteration protocol that we will dive into in the next\nchapter. In short, it has a method called by the next built-in function, which\nreturns an (index, value) tuple each time through the loop. The for steps\nthrough these tuples automatically, which allows us to unpack their values with\ntuple assignment, much as we did for zip:\n>>> E = enumerate(S)\n>>> E\n<enumerate object at 0x10ebd7880>\n>>> next(E)\n(0, 'h')\n>>> next(E)\n(1, 'a')\n>>> next(E)\n(2, 'c')\nWe don’t normally see this machinery because all iteration contexts—including\nlist comprehensions, the main subject of Chapter 14—run the iteration protocol\nautomatically:\n>>> [c * i for (i, c) in enumerate(S)]\n['', 'a', 'cc', 'kkk']\n>>> for (ix, line) in enumerate(open('data.txt')):\n        print(f'{ix}) {line.rstrip()}')\n   \n0) Testing file IO\n1) Learning Python, 6E\n2) Python 3.12\nTo fully understand iteration concepts like enumerate and list comprehensions,\nthough, we need to move on to the next chapter for a deeper dissection.",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we explored Python’s looping statements and their related tools.\nWe looked at the while and for loop statements in depth, and we learned about\ntheir associated else clauses. We also studied the break and continue\nstatements, which have meaning only inside loops, and met several built-ins\ncommonly used in for loops, including range, zip, and enumerate, although\nsome of the details regarding their roles as iterables were intentionally cut short.\nIn the next chapter, we continue the iteration story by discussing list\ncomprehensions and the iteration protocol in Python—concepts strongly related\nto for loops. There, we’ll also fill in the rest of the picture behind the iterable\ntools we met here, such as range and zip, and study some of the subtleties of\ntheir operation. As always, though, before moving on let’s exercise the\nknowledge you’ve picked up here with a quiz.\nTest Your Knowledge: Quiz\n1. What are the main functional differences between while and for loops?\n2. What’s the difference between break and continue?\n3. When is a loop’s else clause executed?\n4. How can you code a counter-based loop in Python?\n5. What can a range be used for in a for loop?\nTest Your Knowledge: Answers\n1. The while loop is a general looping statement, but the for is designed\nto automatically iterate across items in a sequence or other iterable.\nAlthough the while can imitate the for with counter loops, it takes\nmore code and might run slower.",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "2. The break statement exits a loop immediately (control flow winds up\nbelow the entire while or for loop statement), and continue jumps\nback to the top of the loop (control flow winds up positioned just before\nthe test in while or the next item fetch in for).\n3. The else clause in a while or for loop will be run once as the loop is\nexiting, if and only if the loop exits normally (i.e., by a false test in\nwhile or an empty object in for), without running into a break\nstatement. A break exits the loop immediately, skipping the else part\non the way out (if there is one).\n4. Counter loops can be coded with a while statement that keeps track of\nthe index manually, or with a for loop that uses the range built-in\nfunction to generate successive integer offsets. Neither is the preferred\nway to code in Python, if you need to simply step across all the items in\na sequence. Instead, use a simple for loop without range or counters,\nwhenever possible; it will be easier to code and usually quicker to run.\n5. The range built-in can be used in a for loop to implement a fixed\nnumber of repetitions, to scan by offsets instead of items at offsets, to\nskip successive items as you go, and to change a list while stepping\nacross it. None of these roles requires range, and most have alternatives\n—scanning actual items, three-limit slices, and list comprehensions are\noften better solutions today (despite the natural inclinations of ex–C\nprogrammers to want to count things!).\nWHY YOU WILL CARE: FILE SCANNERS\nLoops come in handy anywhere you need to repeat an operation or process\nsomething more than once. Because text files contain multiple characters and\nlines, they are a typical role for loops. Assuming a file’s contents can fit in\nmemory, you can load it all at once with the file object’s read method of\nChapter 9:\nfile = open('data.txt')          # Read contents into a string all at once\nprint(file.read())",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "For more granular access, you can scan by characters instead with either of\nthe following—the second of which doesn’t load the whole file and has\ngrown terser with the := named assignment of Chapter 11:\nfor char in open('data.txt').read():\n    print(char, end='')\nfile = open('data.txt')\nwhile char := file.read(1):      # Read by character, empty means end-of-file\n    print(char, end='')          # Don't add a \\n after each character\nTo read by lines or blocks instead, you can use while loops like the\nfollowing; binary data is often read by blocks using binary file mode 'rb',\nbut text should use text mode to avoid splitting character bytes:\nfile = open('data.txt')\nwhile line := file.readline():   # Read line by line\n    print(line.rstrip())         # Line already has a \\n newline\nfile = open('data.txt')\nwhile chunk := file.read(10):    # Read block by block: up to 10 characters\n    print(chunk, end='')         # Keep but don't add newlines\nTo read text files by lines, though, the for loop tends to be easiest to code\nand may be quickest to run:\nfor line in open('data.txt').readlines():\n    print(line.rstrip())\nfor line in open('data.txt'):   # Use iterators: best for text input (maybe)\n    print(line.rstrip())\nThe first version here uses the file readlines method to load a file all at\nonce into a line-string list, but the second example relies on file iterators to\nautomatically read one line on each loop iteration.\nThe second example is also generally best for text files—besides its\nsimplicity, it works for arbitrarily large files because it doesn’t load the\nentire file into memory all at once. The iterator version may also be the\nquickest, though speed can vary per Python release (we’ll study ways to time\ncode later in this book).",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "File readlines calls can still be useful, though—to reverse a file’s lines, for\nexample, assuming its content can fit in memory. The reversed built-in\nworks on sequences, but does not accept iterables that generate values;\nsorted, by contrast, does, so it can order all the lines in a file without\nloading it in full:\nfor line in reversed(open('data.txt').readlines()): \n    print(line.rstrip())\nfor line in sorted(open('data.txt')): \n    print(line.rstrip())\nSee Python’s documentation for more on the file-object calls used here—as\nwell as its coverage of tools like os.popen that returns a file object\nconnected to a shell command’s output (by default), and hence supports the\nsame sort of loops. There’s also an os.popen example in Chapter 21, and\nmore on the distinctions of text and binary files in Chapter 37 when we dive\ninto Unicode more deeply.",
      "content_length": 854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "Chapter 14. Iterations and\nComprehensions\nIn the prior chapter, we met Python’s two looping statements, while and for.\nAlthough they can handle most repetitive tasks programs need to perform,\niterating over collections is so common and pervasive that Python provides\nadditional tools to make it simpler and more efficient. This chapter begins our\nexploration of these tools. Specifically, it presents Python’s iteration protocol, a\nmethod-call model used by the for loop, and fills in some details on\ncomprehensions, which are a close cousin to the for loop that apply an\nexpression to each item in a collection.\nBecause these tools are related to both the for loop and functions, we’ll take a\ntwo-pass approach to covering them in this book—along with a postscript:\nThis chapter introduces their basics in the context of looping-based\ntools, serving as something of a continuation of the prior chapter.\nChapter 20 revisits them in the context of function-based tools, and\nextends the topic to include built-in and user-defined generators.\nChapter 30 provides a shorter final installment in this story, which will\nshow us how to code user-defined iterable objects with classes.\nOne note up front: some of the concepts presented in these chapters may seem\nadvanced at first glance. With practice, though, you’ll find that these tools are\nuseful and natural. Although never strictly required, they’ve also become\ncommonplace in Python code, so a basic understanding can help if you must\nread programs written by others.\nIterations\nIn the preceding chapter, we learned that the for loop can work on any sequence\ntype in Python, including lists, tuples, and strings, like this (with blank lines",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "required by REPLs after compound statements omitted for brevity):\n>>> for x in [1, 2, 3, 4]: print(x ** 2, end=' ')\n1 4 9 16\n>>> for x in (1, 2, 3, 4): print(x ** 3, end=' ')\n1 8 27 64\n>>> for x in 'text': print(x * 2, end=' ')\ntt ee xx tt\nAs we’ve also learned, the for loop is even more generic than this—it works on\nany iterable object. In fact, this is true of all iteration tools that scan objects from\nleft to right in Python, including for loops; comprehensions of all stripes; some\nin membership tests; the zip, enumerate, and map built-in functions; and more.\nAny iterable object will do, even nonsequences like dictionaries:\n>>> for k in dict(a=1, b=2, c=3): print(k, end=' ')\na b c\nThe concept of iterable objects was added to Python after its inception, but it has\ncome to permeate the language’s toolset. It’s essentially a generalization of the\nnotion of sequences—an object is considered iterable if it is either a physically\nstored sequence or an object that produces one result at a time in the context of\nan iteration tool like for.\nIn a sense, iterable objects include both real sequences and virtual sequences\ncomputed on demand. The virtual sequences both save memory and avoid delays\nby producing results one at a time, instead of all at once. These are not true\nsequences, however: virtual iterables do not support the full range of operations\ndefined for lists and tuples. Rather, they simply materialize a series of values\nover time and on request.\nWhether an iterable is physical or virtual, it announces its support for iterations\nby implementing the iteration protocol—a set of callable methods used by all\niteration tools, and the subject of the next section.\nNOTE",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "Terminology moment: The Python world sometimes uses the terms “iterable” and “iterator”\ninterchangeably (and confusingly!) to refer to an object that supports iteration in general. For\nclarity, this book uses the term iterable to refer to an object that has the iter call at the top of\nthe protocol we’re about to meet, and iterator to refer to an object that has the next call to\nproduce results.\nThat is, an iterable returns an iterator that advances on next. This book also uses the phrase\niteration tool for language tools that run an iteration, like for loops and zip calls. Chapter 20\nwill muddle this jargon with the term generator—which refers to objects that automatically\nsupport the iteration protocol, and hence are iterable—even though all iterables generate\nresults!\nThe Iteration Protocol\nOne of the easiest ways to understand the iteration protocol is to see how it\nworks with a built-in type such as the file object we first explored in Chapter 9.\nIn this chapter, we’ll be using the following three-line input file as a demo:\n>>> print(open('data.txt').read())\nTesting file IO\nLearning Python, 6E\nPython 3.12\n>>> open('data.txt').read()\n'Testing file IO\\nLearning Python, 6E\\nPython 3.12\\n'\nRecall from Chapter 9 that open file objects have a method called readline,\nwhich reads one line of text from a file at a time—each time we call the\nreadline method, we advance to the next line. At the end of the file, an empty\nstring is returned, which we can detect to break out of a line-reading loop\n(remember, empty means false):\n>>> f = open('data.txt')       # Read a three-line file in this directory\n>>> f.readline()               # readline loads one line on each call\n'Testing file IO\\n'\n>>> f.readline()               # Newlines are \\n everywhere in text mode\n'Learning Python, 6E\\n'\n>>> f.readline()               # Last lines may have a \\n or not\n'Python 3.12\\n'\n>>> f.readline()               # Returns empty string at end-of-file\n''",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "Files, however, also have a method named __next__ that has a nearly identical\neffect—it returns the next line from a file each time it is called. The only\nnoticeable difference is that __next__ raises a built-in StopIteration\nexception (that is, invokes a signaling event) at end-of-file instead of returning\nan empty string:\n>>> f = open('data.txt')       # __next__ loads one line on each call too\n>>> f.__next__()               # But raises an exception at end-of-file\n'Testing file IO\\n'\n>>> f.__next__()\n'Learning Python, 6E\\n'\n>>> f.__next__()\n'Python 3.12\\n'\n>>> f.__next__()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\nAnd this interface is most of what we call the iteration protocol in Python. Any\nobject with a __next__ method to advance to a next result, which raises\nStopIteration at the end of the series of results, is considered an iterator in\nPython. Any such object may also be stepped through with a for loop or any\nother iteration tool, because all iteration tools normally work internally by\ncalling __next__ on each iteration and catching the StopIteration exception\nto know when to exit. As you’ll see in a moment, for some objects the full\nprotocol includes an additional first step to call iter, but this isn’t required for\nfiles.\nThe upshot of all this magic is that, as mentioned in Chapters 9 and 13, the\ngenerally best way to read a text file line by line today is to not read it at all—\ninstead, allow the for loop to automatically call __next__ to advance to the next\nline on each iteration. The file object’s iterator will do the work of automatically\nloading lines as you go, both one at a time and efficiently. The following, for\nexample, reads line by line, printing the uppercase version of each line along the\nway—without ever explicitly reading from the file at all:\n>>> for line in open('data.txt'):         # Use file iterators to read by lines\n        print(line.upper(), end='')       # Calls __next__, catches StopIteration\n  \nTESTING FILE IO",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "LEARNING PYTHON, 6E\nPYTHON 3.12\nNotice that the print uses end='' here to suppress adding a \\n, because line\nstrings already have one (without this, our output would be double-spaced). This\nfor coding pattern is usually the best way to read text files line by line, for three\nreasons: it’s the simplest to code, might be the quickest to run, and uses memory\nspace sparingly. Prior to the advent of the iteration protocol in Python,\nprogrammers achieved the same effect with a for loop by calling the file\nreadlines method to load the file’s content into memory as a list of line strings:\n>>> for line in open('data.txt').readlines():\n        print(line.upper(), end='')\n   \nTESTING FILE IO\nLEARNING PYTHON, 6E\nPYTHON 3.12\nThis readlines technique still works, but is not considered the best practice\ntoday because it performs poorly in terms of memory usage. In fact, because this\nversion really does load the entire file into memory all at once, it will not even\nwork for files too big to fit into the memory space available on your device. By\ncontrast, the iterator-based version is immune to such memory-explosion issues\nbecause it reads just one line at a time. The iterator version might run quicker\ntoo, though this can vary per Python release (but see the upcoming note for a few\nspecs).\nAs mentioned in the prior chapter’s closing sidebar, “Why You Will Care: File\nScanners”, it’s also possible to read a file line by line with a while loop:\n>>> f = open('data.txt')\n>>> while line := f.readline():\n        print(line.upper(), end='')\nTESTING FILE IO\nLEARNING PYTHON, 6E\nPYTHON 3.12\nHowever, this may run slower than the iterator-based for loop version because\nfile iterators run at C-language speed inside the standard CPython, whereas the",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "while version must run Python bytecode through the Python virtual machine.\nAnytime we trade Python code for C code, speed tends to increase. This is not an\nabsolute truth, though; again, we’ll explore timing techniques in Chapter 21 for\nmeasuring the relative speed of alternatives like these, though the following note\nruins some of the surprise for the impatient.\nNOTE\nSpoiler alert: Per calls to min(timeit.repeat(code, repeat=50, number=10)) in CPython\n3.12 on macOS, the file iterator is still slightly faster than readlines, which is faster than the\nwhile loop. With a 9k-line file and this chapter’s code (using pass for loop bodies), the\niterator, readlines, and while alternatives check in at 0.0073, 0.0077, and 0.0102 seconds,\nrespectively. The while is slowest and using := doesn’t help much (it’s 0.0104 sans :=). For\nmore info, see the examples package and Chapter 21. Caveats: your test variables may vary,\nmemory matters too, and 0.0029 seconds may not be enough to get excited about in some\nprograms.\nThe iter and next built-ins\nTo simplify manual iteration code, Python also provides a built-in function,\nnext, that has the same net effect as calling an object’s __next__ method. That\nis, given an iterator object X, the call next(X) is the same as X.__next__(), but\nis noticeably simpler to type and read (and actually runs slightly faster in\nCPython 3.12 for tested cases). With files, for instance, either form may be used:\n>>> f = open('data.txt')\n>>> f.__next__()                   # Call iteration method directly\n'Testing file IO\\n'\n>>> next(f)                        # next(f) is the same as f.__next__()\n'Learning Python, 6E\\n'\n>>> next(f)\n'Python 3.12\\n'\n>>> next(f)\n…exception text omitted from here on…\nStopIteration\nTechnically, there is one more piece to the iteration protocol alluded to earlier.\nWhen the for loop begins, it first obtains an iterator from an iterable object, by\ncalling the iterable’s __iter__ method. The object returned by this call in turn",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "has the required __next__ method to advance. For convenience again, the iter\nbuilt-in function internally runs the equivalent of the __iter__ method, much as\nnext runs the equivalent of __next__.\nHence, for loops run the internal equivalent of the following, though the iter\nstep is moot and optional for files—they are their own iterators, because files\ndon’t support multiple scans per open:\n>>> f = open('data.txt')\n>>> I = iter(f)                    # Fetch an iterator from an iterable\n>>> next(I)                        # Fetch the next result from the iterator\n'Testing file IO\\n'\n>>> next(I)                        # Files iterables are iterators themselves\n'Learning Python, 6E\\n'\n…etc…\nThe full iteration protocol\nWith all these pieces in place, Figure 14-1 sketches this full iteration protocol,\nused by every iteration tool in Python and supported by a wide variety of object\ntypes. It’s based on two objects used in two distinct steps by iteration tools:\nThe iterable object for which iteration is requested. Calling this object’s\n__iter__ returns an iterator, and is the same as calling iter.\nThe iterator object returned by the iterable. Calling this object’s\n__next__ produces results during the iteration and raises\nStopIteration when no more results remain, and is the same as\ncalling next.",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "Figure 14-1. The iteration protocol, used by for loops, comprehensions, maps, and more\nThese steps are orchestrated automatically by iteration tools in most cases, but it\nhelps to understand these two objects’ roles. For example, in some cases these\ntwo objects are the same when only a single scan is supported (e.g., files), and\nthe iterator object is often a temporary, used internally by the iteration tool.\nMoreover, some objects are both an iteration tool (they iterate) and an iterable\nobject (their results are iterable)—including the enumerate and zip built-ins,\nand Chapter 20’s generator expressions. Such iterables already avoid\nconstructing result lists in memory themselves, but applying them to other\niterables saves even more space. When such tools are combined, no work is done\nuntil an iteration tool requests results.\nIn code, the protocol’s first step becomes obvious if we look at how for loops\ninternally process built-in sequence types such as lists (for uses the internal\nequivalents of the “__” methods, but you use either in your code):\n>>> L = [1, 2]\n>>> I = iter(L)                 # Obtain an iterator object from an iterable\n>>> next(I)                     # Call next(iterator) to advance to next item",
      "content_length": 1232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "1\n>>> next(I)                     # Or use L.__iter__() and I.__next__() calls\n2\n>>> next(I)\nStopIteration\nAs we saw earlier, the initial iter step is not required for files, because a file\nobject supports just one scan and hence is its own iterator. You can see this\nyourself with is (recall from Chapter 9 that this means object identity—the same\nexact piece of object memory, not just the same value):\n>>> f = open('data.txt')\n>>> iter(f) is f                # Files are iterators themselves: iter() optional\nTrue\n>>> iter(f) is f.__iter__()     # Both calls return file object f itself\nTrue\n>>> next(f)                     # Which responds to next requests directly\n'Testing file IO\\n'\nLists and many other built-in objects, though, are not their own iterators because\nthey do support multiple open iterations—there may be any number of iterations\nactive in nested loops, and all may be at different positions at a given point in\ntime. For such objects, we must call iter to start iterating manually:\n>>> L = [1, 2, 3]\n>>> iter(L) is L                # Lists are not their own iterators: use iter()\nFalse\n>>> next(L)\nTypeError: 'list' object is not an iterator\n>>> I = iter(L)                 # Same as L.__iter__()\n>>> next(I)                     # Same as I.__next__()\n1\nManual iteration\nAlthough Python iteration tools call these functions automatically, we can also\nuse them to apply the iteration protocol manually when needed. The following\ndemonstrates the equivalence between automatic and manual iteration (again,\nfor runs the internal equivalent of I.__next__ instead of the next(I) used\nhere, but the effect is the same):",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": ">>> L = [1, 2, 3]\n>>>\n>>> for X in L:                   # Automatic iteration\n        print(X ** 2, end=' ')    # Obtain iter, call __next__, catch exceptions\n1 4 9\n>>> I = iter(L)                   # Manual iteration: what for loops usually do\n>>> while True:                   # Use try statements to catch stop exception\n        try:\n            X = next(I)\n        except StopIteration:\n            break\n        print(X ** 2, end=' ')\n   \n1 4 9\nTo understand this code, you need to know that try statements run an action and\ncatch exceptions that occur while the action runs (we met exceptions briefly in\nChapter 10 but will explore them in depth in Part VII).\nMore on iter and next\nFor full fidelity, it should also be noted that for loops and other iteration tools\ncan sometimes work differently for user-defined classes—repeatedly indexing an\nobject instead of running the iteration protocol—but they prefer the iteration\nprotocol when supported (more on this story when we study operator\noverloading in Chapter 30).\nThough not commonly used, it’s also worth noting that next accepts an optional\nsecond default argument for an exit value; if passed, it’s returned at the end\ninstead of raising a stop exception:\n>>> L = [1]\n>>> I = iter(L)                                # Result instead of exception\n>>> next(I, 'end of list')\n1\n>>> next(I, 'end of list')\n'end of list'\nCombined with the := named-assignment expression, this can shave multiple\nlines off the preceding manual-iteration code—but will also fail if the passed\ndefault can appear as a valid result:",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": ">>> I = iter(L)\n>>> while (X := next(I, None)) != None:        # Same effect, less code\n        print(X ** 2, end=' ')                 # Assuming None is safe!\nThough also uncommon, iter accepts a second sentinel argument to signal stop\nfrom a callable. This very different mode provides an arguably tricky way to use\nfor to read files by blocks—but requires info on functions or lambda, which we\ndon’t yet have:\n>>> f = open('data.txt')\n>>> I = iter(lambda: f.read(5), '')            # Callables and sentinels\n>>> for block in I: print(block, end='')       # Assuming you know lambda!\nWatch for lambda in the next part of this book, and see Python’s docs for more\non next and iter modes.\nOther Built-in Iterables\nBesides files and physical sequences like lists, many other objects in Python\nhave useful iterators as well. Now that we have a better handle on how the\niteration protocol works, this section revisits some tools we’ve already seen in\nthis context, and introduces a handful of additional iterables along the way.\nReprise: Dictionaries, range, enumerate, and zip\nAs we saw in the last chapter, the usual way to step through the keys of a\ndictionary is with a for loop:\n>>> D = dict(a=1, b=2)\n>>> for key in D:                # Dictionaries are implicitly iterable\n        print(key, D[key])\n   \na 1\nb 2\nThis works simply because dictionaries are iterables with an iterator that\nautomatically returns one key at a time in an iteration tool like for:\n>>> I = iter(D)                  # Which just means they support the protocol\n>>> next(I)",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "'a'\n>>> next(I)\n'b'\n>>> next(I)\nStopIteration\nThe iteration protocol is also the reason that we’ve had to wrap range results in\na list call to see their values all at once in a REPL. Objects that are\nnonsequence iterables return results one at a time, not in a physical list:\n>>> R = range(5)\n>>> R\nrange(0, 5)\n>>> I = iter(R)                  # Use iteration protocol to produce results\n>>> next(I)\n0\n>>> next(I)\n1\n>>> list(range(5))               # Or use list() to collect all results at once\n[0, 1, 2, 3, 4]\nNote that the list call here is not needed and shouldn’t be used in contexts\nwhere iteration happens automatically—such as within for loops. It is needed\nfor displaying values all at once, though, and may also be required when list-like\nbehavior or multiple scans are required for objects that normally produce results\non demand (more on this later).\nNow that you have a better understanding of this protocol, you should also be\nable to see how it explains why the enumerate tool introduced in the prior\nchapter works the way it does:\n>>> E = enumerate('text')        # enumerate is an iterable too\n>>> E\n<enumerate object at 0x1010ab880>\n>>> I = iter(E)\n>>> next(I)                      # Generate results with iteration protocol\n(0, 't')\n>>> next(I)                      # Or use list() to force generation to run\n(1, 'e')\n>>> list(enumerate('text'))\n[(0, 't'), (1, 'e'), (2, 'x'), (3, 't')]\nUnlike range, the enumerate built-in’s result is its own iterator, much like files.",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "This means it supports just one scan per call, and iter is optional (more on this\nahead too):\n>>> R = range(5)\n>>> iter(R) is R\nFalse\n>>> E = enumerate('text')\n>>> iter(E) is E\nTrue\n>>> next(E)\n(0, 't')\nThe zip built-in, covered in the prior chapter, works the same way in iteration\ntools. Like enumerate, zip is also its own iterator, so we have to call it again to\niterate again:\n>>> Z = zip((1, 2, 3), (10, 20, 30))\n>>> Z\n<zip object at 0x101236240>\n>>> I = iter(Z)\n>>> next(I)\n(1, 10)\n>>> next(I)\n(2, 20)\n>>> I is Z\nTrue\n>>> list(Z)\n[(3, 30)]\n>>> list(zip((1, 2, 3), (10, 20, 30)))\n[(1, 10), (2, 20), (3, 30)]\nIterator nesting\nMore interestingly, zip is both iteration tool and iterable: it iterates over its\narguments’ results, but also returns an iterable object with an iterator for its own\nresults. In the following, for example, it’s a tool that receives results from two\nrange iterables, but its own results are produced on demand as well. In other\nwords, there are three iterables and two levels of iteration at work here:\n>>> Z = zip(range(1, 4), range(10, 40))\n>>> next(Z)",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "(1, 10)\n>>> next(Z)\n(2, 11)\n>>> next(Z)\n(3, 12)\n>>> list(zip(range(1, 4), range(10, 40)))\n[(1, 10), (2, 11), (3, 12)]\nIn fact, iterators can be nested arbitrarily. Because enumerate is both iteration\ntool and object too, in the following, range, enumerate, and zip all produce\ntheir results on demand, and list makes all the dances run:\n>>> list(enumerate(range(1, 4)))\n[(0, 1), (1, 2), (2, 3)]\n>>> list(zip(enumerate(range(1, 4)), enumerate(range(5, 8))))\n[((0, 1), (0, 5)), ((1, 2), (1, 6)), ((2, 3), (2, 7))]\nSimilarly, the following enumerates the zipped results of ranges—but only when\nrequested by for:\n>>> for x in enumerate(zip(range(1, 4), range(5, 8))): print(x)\n... \n(0, (1, 5))\n(1, (2, 6))\n(2, (3, 7))\nThere are three levels of iterables in this, all deferring their results until they are\nactivated. In practice, this works naturally, but nothing happens in such code\nuntil a tool like list or for asks for results at the top. In response, all the actors\nhere return just one result at a time, to minimize memory requirements and avoid\ndelays.\nFunctional iterables: map and filter\nLike range, enumerate, and zip, the map and filter built-ins produce their\nresults individually to conserve space and avoid pauses. Like enumerate and\nzip, these tools also are both iterable tools and iterable objects themselves: they\nscan other iterables, and produce their own results on demand.\nUnlike other iterables we’ve met, though, map and filter apply function calls",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "instead of expressions, so their complete story requires function-coding skills we\nwon’t gain until the next part of the book. Still, we can preview their\nfundamentals here using built-in functions without having to code new functions\nof our own.\nFor example, the map built-in, which made a brief cameo appearance in\nChapter 8 (and has nothing directly to do with mappings like dictionaries!), calls\na provided function for each item in a provided iterable, and returns the collected\nresults as another iterable. In the following, it applies the ord built-in to collect\ncharacter code points:\n>>> ord('p')                            # Return a single character's code point\n112\n>>> M = map(ord, 'py3')                 # map returns an iterable, not a list\n>>> M                                   # Runs ord(x) for every x in iterable\n<map object at 0x101227550>\n>>> next(M)                             # Iterating manually: exhausts results\n112                                     # map supports no sequence ops like [i]\n>>> next(M)\n121\n>>> next(M)\n51\n>>> next(M)\nStopIteration\nAs usual, you can force results with list if you must treat them as a list, and\nfor automates iterations:\n>>> list(map(ord, 'py3'))               # Force a real list - only if needed\n[112, 121, 51]\n>>> M = map(ord, 'py3')                 # Must call again to scan again\n>>> for x in M: print(x, end=' ')       # Iteration tools auto call next()\n112 121 51\nThe filter built-in, which we met momentarily in Chapter 12 and will study\nmore fully in the next part of this book, is analogous. It returns items in an\niterable for which a passed-in function returns True. In the following, we’re\nleveraging concepts we’ve already learned—True includes nonempty and\nnonzero objects, the bool built-in returns a single object’s truth value, and the\nstr string’s isdigit method is true for all-digit text:",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": ">>> filter(bool, ['lp6e', '', 2024])\n<filter object at 0x101227850>\n>>> list(filter(bool, ['lp6e', '', 2024]))           # Collect \"true\" items\n['lp6e', 2024]\n>>> list(filter(str.isdigit, ['lp6e', '2024']))      # Collect all-digit strings\n['2024']\nLike most of the tools discussed in this section, filter both accepts an iterable\nto process and returns an iterable to generate results. It doesn’t do any work until\ncode like a for loop asks it to. As a preview, both map and filter can be\nemulated roughly with list comprehensions and more closely with Chapter 20’s\ngenerator expressions; but to grok the following code in full, we have to await\nthis chapter’s presentation of comprehensions coming up soon:\n>>> [ord(x) for x in 'py3']\n[112, 121, 51]\n>>> [x for x in ['lp6e', '2024'] if x.isdigit()]\n['2024']\nMultiple-pass versus single-pass iterables\nWe’ve noted a few times that some iterables that don’t allow multiple scans are\ntheir own iterables. Since this is a subtle difference that can impact the way\nyou’ll use them, it’s worth a separate callout here.\nIn particular, the range built-in’s result, along with objects like dictionaries and\nlists, differs from other built-ins described in this section. They are not their own\niterators (you must make one with iter when iterating manually), and they\nsupport multiple iterators (each remembers its position independently):\n>>> R = range(3)                           # range allows multiple iterators\n>>> next(R)\nTypeError: range object is not an iterator\n>>> I1 = iter(R)\n>>> next(I1)\n0\n>>> next(I1)\n1\n>>> I2 = iter(R)                           # Two iterators on one range",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": ">>> next(I2)\n0\n>>> next(I1)                               # I1 is at a different spot than I2\n2\nBy contrast, the results of enumerate, zip, map, filter, as well as open for\nfiles, are their own iterators, because none of these tools support multiple active\niterations on the same call result. Because of this, the iter call is optional for\nstepping through such objects’ results (though harmless: their iter is\nthemselves), and we must call these tools again to begin a fresh iteration from\nthe start:\n>>> Z = zip((1, 2, 3), (10, 11, 12))\n>>> I1 = iter(Z)\n>>> I2 = iter(Z)                           # Two iterators on one zip\n>>> next(I1)\n(1, 10)\n>>> next(I1)\n(2, 11)\n>>> next(I2)                               # But I2 is at same spot as I1!\n(3, 12)\n>>> M = map(ord, 'py3')                    # Ditto for map (and others)\n>>> I1, I2 = iter(M), iter(M)\n>>> next(I1), next(I1)\n(112, 121)\n>>> next(I2)\n51\n>>> L = [0, 1, 2]                          # But lists (and others) do many scans\n>>> I1, I2 = iter(L), iter(L)\n>>> next(I1), next(I1)\n(0, 1)\n>>> next(I2)                               # Multiple active scans, like range\n0\nWhen we code our own iterable objects with classes later in the book\n(Chapter 30), you’ll see that multiple iterators are usually supported by returning\nnew objects for the iter call; a single iterator generally means an object returns\nitself and supports next directly. In Chapter 20, you’ll also find that generator\nfunctions and expressions behave like map and zip instead of range in this\nregard, supporting just a single active iteration scan. Also in that chapter, you’ll\nsee some subtle implications of single-pass iterators in loops that attempt to scan",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "multiple times—code that treats these as lists may fail without manual list\nconversions.\nStandard-library iterables in Python\nFinally, while out of scope here, and technically part of its standard library\ninstead of its language, Python provides additional tools that support the iteration\nprotocol and thus may also be used in for loops and other iteration tools.\nFor instance, shelves (an access-by-key filesystem for Python objects), as well as\nthe results of os.popen (a tool for reading the output of shell commands), are\niterables that can be processed with the full set of iteration tools. The standard\ndirectory (a.k.a. folder) walker in Python, os.walk, is iterable too, but we’ll save\ndetails and an example until Chapter 20’s coverage of this tool’s basis—\ngenerators and yield.\nUltimately, all such tools implement the iter/next interface defined by the\niteration protocol. We don’t normally see this machinery because for and its kin\nrun it for us automatically to step through results. In fact, everything that scans\nleft to right in Python employs the iteration protocol in the same way—including\nthe topic of the next section.\nComprehensions\nNow that we’ve seen how the iteration protocol works, let’s turn to one of its\nmost common use cases. Together with for loops, list comprehensions are one\nof the most prominent contexts in which the iteration protocol is applied.\nIn the previous chapter, we learned how to use range to change a list as we step\nacross it:\n>>> L = [1, 2, 3, 4, 5]\n>>> for i in range(len(L)):\n        L[i] += 10\n   \n>>> L\n[11, 12, 13, 14, 15]\nThis works, but as mentioned there, it may not be the optimal “best practice”",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "approach in Python. Today, the list comprehension expression makes many such\nprior coding patterns obsolete. Here, for example, we can replace the loop with a\nsingle expression that produces the desired result list:\n>>> L = [x + 10 for x in L]\n>>> L\n[21, 22, 23, 24, 25]\nThe net result is similar, but it requires less coding on our part and is likely to\nrun substantially faster—in fact, it’s often twice as fast as tested in CPython 3.12.\nThe list comprehension isn’t exactly the same as the for loop statement version\nbecause it makes a new list object, which might matter if there are multiple\nreferences to the original list, but it’s close enough for most applications and is a\ncommon and convenient enough approach to merit a closer look here.\nList Comprehension Basics\nThe list comprehension was introduced in Chapter 4, and it’s been demoed often.\nSyntactically, its syntax is derived from a construct in set theory notation that\napplies an operation to each item in a set, but you don’t have to know set theory\nto use this tool. In Python, most people find that a list comprehension simply\nlooks like a backward for loop.\nTo get a handle on the syntax, let’s dissect the prior section’s example in more\ndetail:\nL = [x + 10 for x in L]\nList comprehensions are written in square brackets because they are ultimately a\nway to construct a new list. They begin with an arbitrary expression that we\nmake up, which uses a loop variable that we make up (x + 10). That is followed\nby what you should now recognize as the header of a for loop, which names the\nloop variable, and an iterable object (for x in L).\nTo run the expression, Python executes an iteration across L inside the\ninterpreter, assigning x to each item in turn, and collects the results of running\nthe items through the expression on the left side. The result list we get back is\nexactly what the list comprehension says—a new list containing x + 10, for",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "every x in L.\nTechnically speaking, list comprehensions are never really required because we\ncan always build up a list of expression results manually with for loops that\nappend results as we go:\n>>> res = []\n>>> for x in L:\n        res.append(x + 10)\n   \n>>> res\n[31, 32, 33, 34, 35]\nIn fact, this is exactly what the list comprehension does internally (using internal\nequivalents, of course).\nHowever, list comprehensions are more concise to write, and widely useful in\nPython programs because building result lists is such a common task. Moreover,\ndepending on your Python and code, list comprehensions might run much faster\nthan manual for loop statements (often 2X as stated earlier) because their\niterations are performed at the speed of optimized (and usually compiled) code\ninside the interpreter, rather than with manual Python code. Especially for larger\ndata sets, there is often a major performance advantage to using this expression.\nList Comprehensions and Files\nLet’s work through another common application of list comprehensions to\nexplore them in more detail. Recall that the file object has a readlines method\nthat loads the file into a list of line strings all at once:\n>>> f = open('data.txt')\n>>> lines = f.readlines()\n>>> lines\n['Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n']\nThis works as we saw earlier, but the lines in the result all include the newline\ncharacter (\\n) at the end. For many programs, the newline character gets in the\nway—we have to be careful to avoid double-spacing when printing, and so on. It\nwould be nice if we could get rid of these newlines all at once, wouldn’t it?",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "Anytime we start thinking about performing an operation on each item in a\nsequence, we’re in the realm of list comprehensions. For example, assuming the\nvariable lines is as it was in the prior interaction, the following code does the\njob by running each line in the list through the string rstrip method to remove\nwhitespace on the right side (a line[:−1] slice would work, too, but only if we\ncan be sure all lines are properly \\n terminated, and this may not always be the\ncase for the last line in a file):\n>>> lines = [line.rstrip() for line in lines]\n>>> lines\n['Testing file IO', 'Learning Python, 6E', 'Python 3.12']\nThis works as planned. Because list comprehensions are an iteration tool just\nlike for loop statements, though, we don’t even have to open the file ahead of\ntime. If we open it inside the expression, the list comprehension will\nautomatically use the iteration protocol we met earlier in this chapter. That is, it\nwill read one line from the file at a time by calling the file’s next handler\nmethod, run the line through the rstrip expression, and add it to the result list.\nAgain, we get what we ask for—the rstrip result of a line, for every line in the\nfile:\n>>> lines = [line.rstrip() for line in open('data.txt')]\n>>> lines\n['Testing file IO', 'Learning Python, 6E', 'Python 3.12']\nThis expression does a lot implicitly, but we’re getting a lot of work for free here\n—Python scans the file line by line and builds a list of operation results\nautomatically. It’s also an efficient way to code this operation: because most of\nthis work is done inside the Python interpreter, it’s likely faster than an\nequivalent for statement. Just as importantly, its use of file iterators means that\nit won’t load the file into memory all at once, like readlines does. Again,\nespecially for large files, the advantages of list comprehensions can be\nsignificant.\nBesides their efficiency, list comprehensions are also remarkably expressive. In\nour example, we can run any string operation on a file’s lines as we iterate. To\nillustrate, here’s the list comprehension equivalent to the file iterator uppercase",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "example we met earlier, along with a few other representative operations to\nsample the possibilities:\n>>> [line.upper() for line in open('data.txt')]\n['TESTING FILE IO\\n', 'LEARNING PYTHON, 6E\\n', 'PYTHON 3.12\\n'] \n>>> [line.rstrip().upper() for line in open('data.txt')]\n['TESTING FILE IO', 'LEARNING PYTHON, 6E', 'PYTHON 3.12'] \n>>> [line.split() for line in open('data.txt')]\n[['Testing', 'file', 'IO'], ['Learning', 'Python,', '6E'], ['Python', '3.12']]\n>>> [line.replace('\\n', '!') for line in open('data.txt')]\n['Testing file IO!', 'Learning Python, 6E!', 'Python 3.12!'] \n>>> [('Py' in line, line.split()[0]) for line in open('data.txt')]\n[(False, 'Testing'), (True, 'Learning'), (True, 'Python')]\nRecall that the method chaining in the second of these examples works because\nstring methods return a new string, to which we can apply another string method.\nThe last of these shows how we can also collect multiple results, as long as\nthey’re wrapped in a collection like a tuple or list.\nOne fine point here: recall from Chapter 9 that file objects close themselves\nautomatically in CPython when garbage-collected if still open. Hence, these list\ncomprehensions will also automatically close the file when their temporary file\nobject is garbage-collected after the expression runs. Outside CPython, though,\nyou may want to code these to close manually if this is run in a loop, to ensure\nthat file resources are freed immediately: open before the comprehension, and\nclose after. See Chapter 9 for more on file close calls if you need a refresher on\nthis.\nExtended List Comprehension Syntax\nHandy as they may already seem, list comprehensions can be even richer in\npractice, and even constitute a sort of iteration mini-language in their fullest\nforms. Let’s take a quick look at their extra syntax tools here.\nFilter clauses: if\nAs one particularly useful extension, the for loop nested in a comprehension",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "expression can have an associated if clause to filter out of the result items for\nwhich the test is not true. (It’s really a filter in, but it works out the same.)\nFor example, suppose we want to repeat the prior section’s file-scanning\nexample, but we need to collect only lines that begin with the letters L or P\n(perhaps the first character on each line is an action code of some sort). Adding a\nsimple if filter clause to our expression does the trick:\n>>> lines = [line.rstrip() for line in open('data.txt') if line[0] in 'LP']\n>>> lines\n['Learning Python, 6E', 'Python 3.12']\nHere, the if clause checks each line read from the file to see whether its first\ncharacter matches; if not, the line is omitted from the result list, and the iteration\ncontinues. This is a fairly big expression, but it’s easy to understand if we\ntranslate it to its simple for loop statement equivalent:\n>>> res = []\n>>> for line in open('data.txt'):\n        if line[0] in 'LP':\n            res.append(line.rstrip())\n   \n>>> res\n['Learning Python, 6E', 'Python 3.12']\nIn general, we can always translate a list comprehension to a for statement by\nappending as we go and further indenting each successive part. The converse\nisn’t true: for statements are more general and can address additional roles out\nof scope for comprehensions, but the latter’s results collection is a very common\ntask.\nThis for statement equivalent works, but it takes up four lines instead of one and\nmay run slower. In fact, you can squeeze a substantial amount of logic into a list\ncomprehension when you need to—the following works like the prior but selects\nonly lines that end in a digit (before the newline at the end), by filtering with a\nmore sophisticated expression on the right side (which uses [-1:] instead of\n[-1] to handle files with blank lines empty after rstrip):\n>>> [line.rstrip() for line in open('data.txt') if line.rstrip()[-1:].isdigit()]",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "['Python 3.12']\nAs another if filter example, the first result in the following gives the total lines\nin a text file, and the second strips whitespace on both ends to omit blank lines in\nthe tally in just one line of code:\n>>> fname = 'data-blank-lines.txt'\n>>> len(open(fname).readlines())                                 # All lines\n5\n>>> len([line for line in open(fname) if line.strip() != ''])    # Nonblank lines\n3\nNested loops: for\nList comprehensions can become even more complex if we need them to—for\ninstance, they may contain nested loops, coded as a series of for clauses. In fact,\ntheir full syntax allows for any number of for clauses, each of which can have\nan optional associated if clause.\nFor example, the following builds a list of the concatenation of x + y for every\nx in one string and every y in another. It effectively collects all the ordered\ncombinations of the characters in two strings:\n>>> [x + y for x in 'abc' for y in '123']\n['a1', 'a2', 'a3', 'b1', 'b2', 'b3', 'c1', 'c2', 'c3']\nAgain, one way to understand this expression is to convert it to statement form\nby indenting its parts. The following is an equivalent, but likely slower,\nalternative way to achieve the same effect (it’s the same as the first nested for\nexample in the prior chapter, but builds a list of results instead of simply printing\nthem):\n>>> res = []\n>>> for x in 'abc':\n        for y in '123':\n            res.append(x + y)\n   \n>>> res\n['a1', 'a2', 'a3', 'b1', 'b2', 'b3', 'c1', 'c2', 'c3']",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "Beyond this complexity level, though, list comprehension expressions can\nsometimes become too compact for their own good. In general, they are intended\nfor simple types of iterations; for more involved work, a simpler for statement\nstructure will probably be easier to understand and modify in the future. As usual\nin programming, if something is difficult for you to understand, it’s probably not\nthe best idea.\nComprehensions Cliff-Hanger\nBecause comprehensions are generally better taken in multiple doses, we’ll cut\nthis story short here for now. We’ll revisit list comprehensions in Chapter 20 in\nthe context of functional programming tools, and will define their syntax more\nformally and explore additional examples there. As you’ll find, comprehensions\nturn out to be just as related to functions as they are to looping statements.\nList comprehensions are also related to—and predate—the set and dictionary\ncomprehensions introduced in this book’s prior part, as well as the generator\nexpression you’ll meet later that produces items on request instead of building a\nlist. All share the same syntax, but are coded slightly differently and produce\ndifferent sorts of stuff:\n[x + 10 for x in L if x > 0]         # List comprehension\n{x + 10 for x in L if x > 0}         # Set comprehension\n{x: x + 10 for x in L if x > 0}      # Dictionary comprehension\n(x + 10 for x in L if x > 0)         # Generator expression\nWe’ll put the last three of these to work on files briefly in the next section. To\nbetter expand this plotline to generators, though, we have to move on to this\nbook’s next part.\nNOTE\nSpeed disclaimer: As a blanket qualification for all performance claims in this book, the\nrelative speed of code depends much on the exact code tested and version of Python used, and\nis prone to vary and change. For example, list comprehensions have been consistently twice as\nfast as corresponding for loops on most tests for all CPythons through 3.12, but may be just\nmarginally quicker on some tests, and perhaps even slower when if filter clauses are used.\nYou’ll learn how to time your own code and Python in Chapter 21. For now, keep in mind that\nabsolutes in performance benchmarks are as elusive as consensus in open source projects.",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "Iteration Tools\nLater in this book, you’ll learn how user-defined classes can implement the\niteration protocol too. Because of this, it’s sometimes important to know which\nbuilt-in tools make use of it—any tool that employs the iteration protocol will\nautomatically work on any built-in type or user-defined class that provides it.\nThis section closes out this chapter with a summary and sort of “grand finale” of\ntools in this domain.\nSo far, we’ve mostly seen iterators at work in the context of the for loop\nstatement, because this part of the book is focused on statements. Keep in mind,\nthough, that every built-in tool that scans from left to right across collection\nobjects uses the iteration protocol. This includes the for loops we’ve seen:\n>>> for line in open('data.txt'): \n        print(line.upper(), end='')\n   \nTESTING FILE IO\nLEARNING PYTHON, 6E\nPYTHON 3.12\nBut also much more. For instance, the prior section’s list comprehensions and\nthe map built-in function we met earlier use the same protocol as their for loop\ncousin. When applied to a file, they both leverage the file object’s iterator\nautomatically to scan line by line, fetching an iterator with __iter__ and calling\n__next__ each time through:\n>>> uppers = [line.upper() for line in open('data.txt')]\n>>> uppers\n['TESTING FILE IO\\n', 'LEARNING PYTHON, 6E\\n', 'PYTHON 3.12\\n'] \n>>> list(map(str.upper, open('data.txt')))\n['TESTING FILE IO\\n', 'LEARNING PYTHON, 6E\\n', 'PYTHON 3.12\\n']\nAs we saw earlier, the map built-in applies a function call to each item in an\niterable object. map is similar to a list comprehension but is more limited because\nit requires a function instead of an arbitrary expression. It also returns an iterable",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "object, so we must wrap it in a list call to force it to give us all its values at\nonce. Because map, like the list comprehension, is related to both for loops and\nfunctions, watch for its revival in Chapter 20.\nMany of Python’s other built-ins process iterables, too. We’ve seen how zip\ncombines items from iterables, enumerate pairs items in an iterable with relative\npositions, and filter selects items for which a function is true. In addition,\nsorted sorts items in an iterable, and reduce (now oddly relegated to a module)\nruns pairs of items in an iterable through a function. All of these accept iterables,\nand zip, enumerate, and filter also return an iterable like map. Here they are\nin action running the file’s iterator automatically to read line by line:\n>>> sorted(open('data.txt'))\n['Learning Python, 6E\\n', 'Python 3.12\\n', 'Testing file IO\\n']\n>>> list(zip(range(99), open('data.txt')))\n[(0, 'Testing file IO\\n'), (1, 'Learning Python, 6E\\n'), (2, 'Python 3.12\\n')]\n>>> list(enumerate(open('data.txt')))\n[(0, 'Testing file IO\\n'), (1, 'Learning Python, 6E\\n'), (2, 'Python 3.12\\n')] \n>>> list(filter(bool, open('data.txt')))\n['Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n'] \n>>> import functools, operator\n>>> functools.reduce(operator.add, open('data.txt'))\n'Testing file IO\\nLearning Python, 6E\\nPython 3.12\\n'\nAll of these are iteration tools, but they have unique roles. We met zip and\nenumerate in this chapter; filter and reduce are in Chapter 19’s functional\nprogramming domain, so we’ll defer their details for now; the point to notice\nhere is their use of the iteration protocol for files and other iterables.\nWe first saw the sorted function used here at work in Chapter 4, and we used it\nin Chapter 8. sorted is a built-in that employs the iteration protocol—it’s like\nthe original list sort method, but it returns the new sorted list as a result and\nruns on any iterable object. Notice that, unlike map and others, sorted returns an\nactual list instead of an iterable. Per the prior chapter’s closer, its reversed\ncohort returns an iterable but does not run the iteration protocol.",
      "content_length": 2125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "In general, though, everything in Python’s built-in toolset that scans object is\ndefined to use the iteration protocol on their subject. This even includes tools\nsuch as the list and tuple built-in functions (which build new objects from\niterables), and Chapter 7’s string join method (which makes a new string by\nputting a substring between strings in an iterable). Hence, these will also work\non an open file and automatically read one line at a time:\n>>> list(open('data.txt'))\n['Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n'] \n>>> tuple(open('data.txt'))\n('Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n') \n>>> '&&'.join(open('data.txt'))\n'Testing file IO\\n&&Learning Python, 6E\\n&&Python 3.12\\n'\nEven some tools you might not expect fall into this category. For example,\nChapter 11’s sequence assignment (original and starred), the in membership test,\nslice assignment, the list’s extend method, and single-star literal unpacking also\nleverage the iteration protocol to scan, and thus read a file by lines\nautomatically:\n>>> a, b, c = open('data.txt')              # Sequence assignment\n>>> b, c\n('Learning Python, 6E\\n', 'Python 3.12\\n') \n>>> a, *b = open('data.txt')                # Extended-unpacking assignment\n>>> a, b\n('Testing file IO\\n', ['Learning Python, 6E\\n', 'Python 3.12\\n'])\n>>> 'Python 2.7\\n' in open('data.txt')      # Membership test\nFalse\n>>> 'Python 3.12\\n' in open('data.txt')\nTrue\n>>> L = [11, 22, 33, 44]                    # Slice assignment\n>>> L[1:3] = open('data.txt')\n>>> L\n[11, 'Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n', 44]\n>>> L = [11]\n>>> L.extend(open('data.txt'))              # list.extend method\n>>> L\n[11, 'Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n']",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": ">>> L = [11, *open('data.txt'), 44]         # List-literal unpacking\n>>> L\n[11, 'Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n', 44]\nRemember that extend iterates automatically, but append does not—though you\ncan use the latter to add an iterable to a list without iterating, and iterate across it\nlater:\n>>> L = [11]\n>>> L.append(open('data.txt'))\n>>> list(L[-1])\n['Testing file IO\\n', 'Learning Python, 6E\\n', 'Python 3.12\\n']\nNor does the iteration grand finale end here. In the prior chapter, we saw that the\nbuilt-in dict call accepts an iterable zip result too. For that matter, so does the\nset call, as well as the set and dictionary comprehension expressions we met\nearlier and will revisit later:\n>>> set(open('data.txt'))\n{'Python 3.12\\n', 'Learning Python, 6E\\n', 'Testing file IO\\n'}\n>>> {line for line in open('data.txt')}\n{'Python 3.12\\n', 'Learning Python, 6E\\n', 'Testing file IO\\n'}\n>>> {ix: line for ix, line in enumerate(open('data.txt'))}\n{0: 'Testing file IO\\n', 1: 'Learning Python, 6E\\n', 2: 'Python 3.12\\n'}\nAs noted, both set and dictionary comprehensions support the extended syntax of\nlist comprehensions we met earlier in this chapter, including if tests:\n>>> {line for line in open('data.txt') if line[0] in 'LP'}\n{'Python 3.12\\n', 'Learning Python, 6E\\n'}\n>>> {ix: line for (ix, line) in enumerate(open('data.txt')) if line[0] in 'LP'}\n{1: 'Learning Python, 6E\\n', 2: 'Python 3.12\\n'}\nLike the list comprehension, both of these scan the file line by line and pick out\nlines that begin with the specific letters. They also happen to build sets and\ndictionaries in the end, but we get a lot of work “for free” by combining file\niteration and comprehension syntax. In the next part of this book, you’ll meet a",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "relative of comprehensions—generator expressions—that deploys the same\nsyntax and works on iterables too, but is also iterable itself:\n>>> list(line.upper() for line in open('data.txt'))\n['TESTING FILE IO\\n', 'LEARNING PYTHON, 6E\\n', 'PYTHON 3.12\\n']\nOther built-in functions support the iteration protocol as well, but frankly, some\nare harder to cast in interesting examples related to files. For example, the sum\ncall computes the sum of all the numbers in any iterable; the any and all built-\nins return True if any or all items in an iterable are True, respectively; and max\nand min return the largest and smallest item in an iterable, respectively. Like\nreduce, all of the tools in the following examples accept any iterable as an\nargument and use the iteration protocol to scan it, but return a single result:\n>>> sum(range(5))                # Punt (requires numbers)\n10\n>>> any(open('data.txt'))        # Any/all lines true (nonempty)\nTrue\n>>> all(open('data.txt'))        # Mostly pointless for files\nTrue\n>>> max(open('data.txt'))        # Line with highest string value\n'Testing file IO\\n'\n>>> min(open('data.txt'))        # Use cases wanted!\n'Learning Python, 6E\\n'\nThere’s one last iteration tool worth mentioning, although it’s a preview of this\nbook’s next part: in Chapter 18, you’ll learn that a special * form can be used in\nfunction calls to unpack a collection of values into individual arguments, much\nas it does in list (and other) literals. As you can probably predict by now, this\naccepts any iterable too:\n>>> def f(a, b, c):                    # See Part IV \n        print(a, b, c, sep='&')   \n   \n>>> f(*open('data.txt'))               # Iterates by lines too!\nTesting file IO\n&Learning Python, 6E\n&Python 3.12\nIn fact, because this argument-unpacking syntax in calls accepts iterables, it’s",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "also possible to use the zip built-in to unzip zipped tuples, by making prior or\nnested zip results arguments for another zip call (warning: you probably\nshouldn’t read the following example if you plan to operate heavy machinery\nanytime soon!):\n>>> X, Y = (1, 2), (3, 4)\n>>> list(zip(X, Y))                    # Zip tuples: returns an iterable\n[(1, 3), (2, 4)]\n>>> A, B = zip(*zip(X, Y))             # Unzip a zip, really!\n>>> A, B\n((1, 2), (3, 4))\nAnd that concludes our iteration-tools finale. It’s probably not complete, but you\nprobably get the point.\nOther Iteration Topics\nAs mentioned in this chapter’s introduction, there is more coverage of both list\ncomprehensions and iterables in Chapter 20, in conjunction with functions, and\nagain in Chapter 30 when we study classes. As you’ll see later:\nUser-defined functions can be turned into iterable generator functions,\nwith yield statements.\nList comprehensions morph into iterable generator expressions when\ncoded in parentheses.\nUser-defined classes are made iterable with __iter__ or __getitem__\nin operator overloading.\nIn particular, user-defined iterables defined with classes allow arbitrary objects\nand operations to be used in any of the iteration tools we’ve met in this chapter.\nBy supporting just a single operation—iteration—objects may be used in a wide\nvariety of contexts and tools.",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we explored concepts related to looping in Python. We took our\nfirst substantial look at the iteration protocol in Python—a way for nonsequence\nobjects to take part in iteration loops—and at list comprehensions. As we saw, a\nlist comprehension is an expression similar to a for loop that applies another\nexpression to all the items in any iterable object. Along the way, we also saw\nmany of the other built-in iteration tools in Python’s arsenal.\nThis wraps up our tour of specific procedural statements and related tools. The\nnext chapter closes out this part of the book by discussing documentation options\nfor Python code. Though a bit of a diversion from the more detailed aspects of\ncoding, documentation is also part of the general syntax model, and it’s an\nimportant component of well-written programs. In the next chapter, we’ll also\ndig into a set of exercises for this part of the book before we turn our attention to\nlarger structures such as functions. As usual, though, let’s first exercise what\nwe’ve learned here with a quiz.\nTest Your Knowledge: Quiz\n1. How are for loops and iterable objects related?\n2. How are for loops and list comprehensions related?\n3. Name four iteration tools in the Python language.\n4. What is the best way to read line by line from a text file today?\nTest Your Knowledge: Answers\n1. The for loop normally uses the iteration protocol to step through items\nin the iterable object across which it is iterating. It first fetches an\niterator from the iterable by calling the iterable’s __iter__, and then\ncalls this iterator object’s __next__ method on each iteration to advance\nand catches its StopIteration exception to determine when to stop",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "looping. Any object that supports this model works in a for loop and in\nall other iteration tools. The protocol’s methods can also be run\nmanually with built-ins iter and next. For some objects that are their\nown iterator, the initial iter call is extraneous but harmless.\n2. Both are iteration tools. List comprehensions are a concise and often\nefficient way to perform a common for loop task: collecting the results\nof applying an expression to all items in an iterable object. It’s always\npossible to translate a list comprehension to a for loop, and part of the\nlist comprehension expression looks like the header of a for loop\nsyntactically. The for loop, however, can be used in additional looping\nroles that comprehensions do not address.\n3. Iteration tools in Python include the for loop; list comprehensions; the\nmap built-in function; the in membership test expression; and the built-\nin functions sorted, sum, any, and all. This category also includes the\nlist and tuple built-ins, string join methods, and sequence\nassignments (starred or not), all of which use the iteration protocol (see\nanswer #1) to step across iterable objects one item at a time.\n4. The “best” way to read lines from a text file today is to not read it\nexplicitly at all: instead, open the file within an iteration tool such as a\nfor loop or list comprehension, and let the iteration tool automatically\nscan one line at a time by running the file’s next handler method on\neach iteration. This approach is generally best in terms of coding\nsimplicity, memory space, and possibly execution speed requirements.",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "Chapter 15. The Documentation\nInterlude\nThis part of the book concludes with a brief look at techniques and tools used for\ndocumenting Python code. Although Python code is comparatively readable by\nitself, a few well-placed and human-accessible comments can do much to help\nothers understand the workings of your programs—especially when code grows\nlarger than most in this book. As you’ll see, Python includes both syntax and\ntools to make documentation easier. In particular, the Pydoc system covered here\ncan render a code file’s documentation as either plain text in an interactive\nREPL, or HTML in a browser.\nWhile this topic is partly tools related, it’s presented here because it both\ninvolves Python’s syntax model and provides a resource for readers struggling to\nunderstand Python’s toolset. For the latter purpose, this chapter also expands on\ndocumentation pointers first given in Chapter 4. As usual, because this chapter\ncloses out its part, it also ends with some warnings about common pitfalls and a\nset of exercises for this part of the text, in addition to its chapter quiz.\nPython Documentation Sources\nBy this point in the book, you’re probably starting to realize that Python comes\nwith an amazing amount of prebuilt functionality—built-in functions and\nexceptions, predefined object attributes and methods, standard-library modules,\nand more. And we’ve really only scratched the surface of each of these\ncategories.\nOne of the first questions that bewildered beginners often ask is, How do I find\ninformation on all the built-in tools? This section provides tips on the various\ndocumentation sources available in Python. It also presents documentation\nstrings (docstrings, for short), and the Pydoc system shipped with Python that\nmakes use of them. These topics are somewhat peripheral to the core language\nitself, but they become essential knowledge as soon as your code becomes large",
      "content_length": 1908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "or complex enough to challenge its readers—including yourself, six months\ndown the road.\nAs summarized in Table 15-1, there are a variety of places to look for\ninformation about Python, with generally increasing verbosity. Because\ndocumentation is such a crucial tool in practical programming, we’ll explore\neach of these categories in the sections that follow.\nTable 15-1. Python documentation sources\nForm\nRole\n# comments\nIn-file documentation\nThe dir function\nLists of attributes available in objects\nDocstrings: __doc__\nIn-file documentation attached to objects\nPydoc: the help function\nInteractive help for objects\nPydoc: HTML reports\nModule documentation in a browser\nSphinx third-party tool\nRicher documentation for larger projects\nThe standard manuals\nOfficial language and library descriptions\nWeb resources\nOnline tutorials, examples, and so on\n# Comments\nAs we’ve learned—and used in most example listings so far—hash-mark\ncomments are the most basic way to document your code. Python simply ignores\nall the text following a # (as long as it’s not inside a string literal), so you can\nfollow this character with any words and descriptions meaningful to\nprogrammers. Such comments are accessible only in your source files, though;\nto code comments that are more widely available, you’ll use docstrings (ahead).\nIn fact, current best practice (generally accepted convention) dictates that",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "docstrings are best for larger functional documentation (e.g., “my script does\nthis”), and # comments are best limited to smaller code documentation (e.g.,\n“this strange expression does that”) and are best limited in scope to a statement\nor small group of statements within a script or function. Docstrings are a broader\ntopic we’ll get to in a moment; first, let’s see how to explore objects.\nThe dir Function\nAs we’ve also seen, the built-in dir function is an easy and automatic way to list\nall of the attributes available inside an object (i.e., its methods and simpler data\nitems). It can be called with no arguments to list variables in the caller’s scope,\nsomething the next part of this book will define in full. More usefully, it can also\nbe called with any object that has attributes, including imported modules and\nobjects of built-in types, as well as the name of a data type itself. For example, to\nfind out what’s available in a module, such as the standard library’s sys, import\nit and pass it to dir:\n$ python3\n>>> import sys\n>>> dir(sys)\n[…many names here…]\nThe list of attribute name strings you’ll get back is omitted here because it’s not\nsmall and is prone to vary per Python version; run this on your own for a better\nlook. In fact, there are currently 110 attributes in sys, though we generally care\nonly about the 97 that do not have leading double underscores (two underscores\nusually means interpreter-related), or the 83 that have no leading underscore at\nall (one underscore usually means implementation private, informally). Rooting\nout such numbers is a prime example of the preceding chapter’s list\ncomprehension at work:\n>>> len(dir(sys))                                           # Number names in sys\n110\n>>> len([x for x in dir(sys) if not x.startswith('__')])    # Non __X names only\n97\n>>> len([x for x in dir(sys) if not x[0] == '_'])           # Ignore _X names too\n83",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "To find out what attributes are provided in objects of built-in types, run dir on a\nliteral, an existing instance, or the name of the desired type. For example, to see\nall text-string attributes, you can pass an empty or the type name str:\n>>> dir('')\n['__add__', …more names here…, 'zfill']\n>>> dir(str) == dir('')\nTrue\nThe dir result for any built-in type includes a set of attributes that are related to\nthe implementation of that type (mostly, for expression operators). Much as in\nmodules, they all begin and end with double underscores to make them distinct,\nand you can safely ignore them at this point in the book (they’ll resurface later\nfor OOP). For instance, there are 81 text-string attributes, but only 47 that\ncorrespond to named methods or data; lists have even fewer:\n>>> len(dir('')), len([x for x in dir('') if not x.startswith('__')])\n(81, 47)\n>>> len(dir([])), len([x for x in dir([]) if not x.startswith('__')])\n(48, 11)\nTo list names, filter out double-underscored items that are not of common\nprogram interest by running the same list comprehensions, but print the\nattributes. To demo, here are the named attributes in lists and dictionaries:\n>>> [a for a in dir(list) if not a.startswith('__')]\n['append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop',\n'remove', 'reverse', 'sort']\n>>> [a for a in dir(dict) if not a.startswith('__')]\n['clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', \n'setdefault', 'update', 'values']\nThis may seem like a lot to type to get an attribute list, but beginning in the next\nchapter, you’ll learn how to wrap such code in a simple, importable, and reusable\nfunction so you don’t need to type it again.\nAside: type names like str, list, and dict work in dir because they are\nactually names of types in Python today, not just type-converter functions;",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "calling one of these invokes its constructor to generate an instance of that type.\nPart VI will have more to say about constructors and operator overloading\nmethods when we discuss classes.\nThe dir function is mostly a memory jogger—it provides a list of attribute\nnames, but it does not tell you anything about what those names mean. For such\nextra information, we need to move on to the next documentation source.\nNOTE\nAttributes in IDEs: Some GUIs for Python work, including IDLE, have features that list\nattributes on objects automatically, which can be viewed as alternatives to dir. IDLE, for\nexample, will list an object’s attributes in a pop-up selection window when you type a period\nafter the object’s name and pause or press Tab. This is mostly meant as an autocomplete\nfeature, though, not an information source. Chapter 3 and Appendix A have introductory info\non IDLE.\nDocstrings and __doc__\nBesides # comments, Python supports documentation that is automatically\nattached to objects and retained at runtime for inspection. Syntactically, such\ncomments are coded as string literals of any type, except bytes and f-strings.\nThey are located at the tops of code components (module files and function and\nclass statements), and before any other executable code (# comments, including\nUnix-style #! lines, are OK before them). When present, the text of such strings,\nknown as docstrings, is automatically stuffed into the __doc__ attributes of the\ncorresponding objects.\nUser-defined docstrings\nFor example, consider the file docstrings.py in Example 15-1. You can safely\nignore most of its code here (we’ll study functions, modules, and classes in the\nnext three parts of this book), but notice its three string literals: because they are\ncoded at the beginning of the file, and at the start of a function (def) and class\n(class) within it, they are taken as docstrings and are saved in the associated\nobject’s __doc__ when this file is loaded by a run or import.\nExample 15-1. docstrings.py",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "\"\"\"\nModule documentation\nThis module defines a name, function and class.\n\"\"\"\nedition = 6\ndef square(x):\n   r'''\n   Function documentation\n   Returns the \\square\\ of its \\numeric\\ argument.\n   '''\n   return x ** 2   # power operator\nclass PartVI:\n   \"Class documentation for \\U0001F40D \n\"\n   pass\n# Top-level code\nprint(square(edition))\nprint(square.__doc__)\nprint(PartVI.__doc__)\nSome fine points about this example’s docstrings:\nLiterals: the first of this file’s docstrings uses a triple-quoted block at\nthe top of the file. This is common for docstrings as it allows for\nmultiline comments, but any sort of text string (except f-strings) will\nwork. Both single- or double-quoted one-liners like that in the class are\nfine, but don’t readily support multiline text.\nRaw strings: as the function demos, r'…' strings work too—and may\neven be required to suppress unwanted backslash escapes and avoid a\nsyntax warning (and eventual error!) for unrecognized escapes each\ntime code is run or recompiled to bytecode (see Chapter 7’s coverage of\nnew \\ deprecations in Python 3.12).\nContent: docstrings can contain any sort of text, including the class’s\nUnicode escape and raw emoji (per the Unicode intro in Chapter 4 and\nthe whole story in Chapter 37). Notice that the \\U Unicode escape\nrequires backslashes to be used, so it precludes using raw strings to\navoid syntax errors in the future (use \\\\).",
      "content_length": 1397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "The whole point of this documentation protocol is that your comments are\nretained for inspection in __doc__ attributes after the file is loaded. Thus, to\ndisplay the docstrings associated with the module and the two code components\nit defines, we simply import the file and print their __doc__ attributes, where\nPython has saved the text. Assuming this module is located in the directory in\nwhich we’re currently working (and deferring to Chapter 3’s note about imports\nand directories for background info on why that matters):\n$ python3\n>>> import docstrings\n36\n    Function documentation\n    Returns the \\square\\ of its \\numeric\\ argument.\n    \nClass documentation for \n \n>>> print(docstrings.__doc__)\nModule documentation\nThis module defines a name, function and class.\n>>> print(docstrings.square.__doc__)\n    Function documentation\n    Returns the \\square\\ of its \\numeric\\ argument.\n    \n>>> print(docstrings.PartVI.__doc__)\nClass documentation for \n \nYou will usually want to use print like this to view docstrings; otherwise, you’ll\nget a single string with embedded \\n newline characters. Also note that import\nruns the file’s top-level prints (hence the output immediately following the\nimport), as does launching it as a top-level script. Within the file, square is just\nsquare, a simple variable:\n$ python3 docstrings.py \n36\n    Function documentation\n    Returns the \\square\\ of its \\numeric\\ argument.\n    \nClass documentation for",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "You can also attach docstrings to methods of classes (covered in Part VI too), but\nbecause, as you’ll learn, these are just def statements nested in class\nstatements, they’re not a special case. For instance, to fetch the docstring of a\nmethod function inside a class within a module, you would simply extend the\npath to go through the class: module.class.method.__doc__. We’ll code an\nexample of method docstrings in Chapter 29.\nNOTE\nDocstrings futurism: If you look closely, you’ll notice that multiline docstrings, like that of\nsquare, retain their leading indentation as it was in the code. Python 3.13, still on the drawing\nboard as this note is being written, plans to remove leading indentation from docstrings, mostly\nto save space in bytecode files and memory. Even before this mod, though, you won’t normally\nsee the indents in docs, because tools like Pydoc already strip and reformat, as you’ll learn\nshortly.\nDocstring standards\nAs mentioned earlier, common practice today recommends hash-mark comments\nfor only smaller-scale documentation about an expression, statement, or small\ngroup of statements. Docstrings are better used for higher-level and broader\nfunctional documentation for a file, function, or class, and have become an\nexpected part of Python software. Beyond these guidelines, though, you still\nmust decide what to write.\nAlthough some companies have internal standards, there is no broad consensus\nabout what should go into the text of a docstring. There have been various\nmarkup language and template proposals, including HTML, but they don’t seem\nto have caught on in the Python world. Frankly, convincing programmers to\ndocument their work using handcoded HTML probably won’t happen in our\nlifetimes, but this shouldn’t apply to documentation in general.\nDocumentation tends to have a lower priority than it should. Too often, if you\nget any docs in a file at all, you count yourself lucky (and even better if they’re\naccurate and up to date). While technically optional, documentation is a crucial\npart of well-written programs, and you’re encouraged to comment your code\nliberally. When you do, though, there is no standard docstring format; if you\nwant to use them, anything goes today. Just as for writing code itself, it’s up to",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "you to create docstring content and keep it up to date, but common sense is\nprobably your best ally in both tasks.\nBuilt-in docstrings\nWhile it’s important to document our own code, it turns out that built-in modules\nand objects in Python use similar techniques to attach documentation above and\nbeyond the attribute lists returned by dir. For example, to view a human-\nreadable description of a built-in module, import it and print its __doc__ string\n—as we did for our own code:\n>>> import sys\n>>> print(sys.__doc__)\nThis module provides access to some objects used or maintained by the\ninterpreter and to functions that interact strongly with the interpreter.\nDynamic objects:\nargv -- command line arguments; argv[0] is the script pathname if known\npath -- module search path; path[0] is the script directory, else ''\nmodules -- dictionary of loaded modules \n…more text omitted…\nFor finer-grained details, functions, classes, and methods within built-in modules\nhave attached descriptions in their __doc__ attributes as well:\n>>> print(sys.getrefcount.__doc__)\nReturn the reference count of object.\nThe count returned is generally one higher than you might expect,\nbecause it includes the (temporary) reference as an argument to\ngetrefcount().\nYou can also read about built-in functions via their docstrings:\n>>> print(map.__doc__)\nmap(func, *iterables) --> map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\nIn fact, you can get a wealth of information about built-in tools by inspecting\ntheir docstrings this way—but you don’t have to. The Python help function\nlargely automates this for you, as the next section will explain.",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "Pydoc: The help Function\nThe docstring technique proved to be so useful that Python eventually added a\ntool that makes docstrings even easier to display. Namely, Pydoc is Python code\nthat knows how to extract docstrings and associated structural information and\nformat them into nicely arranged reports of various types. Additional tools for\nextracting and formatting docstrings are available in the open source domain\n(including third-party tools that may support structured text—search the Web for\npointers), but Python ships with Pydoc in its always-present standard library.\nThere are a variety of ways to launch Pydoc, but the two most prominent are the\nbuilt-in help function and the Pydoc browser-based interface. Of these, help\nmay be the most straightforward to use and is always available in a REPL.\nWe met help briefly in Chapter 4. It invokes Pydoc to generate a simple plain-\ntext report for any Python object. In this mode, help text looks much like a\n“manpage” on Unix-like systems. In fact, outside GUIs like IDLE, help text\nworks the same way as a Unix “more” when there are multiple pages of text—\npress the space bar to move to the next page, Enter (or your keyboard’s\nequivalent) to go to the next line, and Q to quit the display. GUIs generally scroll\nhelp text instead. Let’s turn to a few examples to see how this works.\nRunning help on built-in tools\nBecause built-in tools come with the docstrings we saw earlier, help gives us a\nlevel of reference docs for Python, midway between dir lists and the full\nmanuals. Here it is live, reporting on a standard-library module’s function:\n>>> import sys\n>>> help(sys.getrefcount)\nHelp on built-in function getrefcount in module sys:\ngetrefcount(object, /)\n    Return the reference count of object.\n    The count returned is generally one higher than you might expect,\n    because it includes the (temporary) reference as an argument to\n    getrefcount().\nYou do not have to import sys in order to call help, but you generally have to",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "import sys to get help on sys this way, because help expects an object reference\nto be passed in. Alternatively, you can also get help for a module you have not\nimported by quoting the module’s name as a string. For example, help('sys')\nand help('sys.getrefcount') both work too, without the import step.\nFor larger objects such as modules and classes, the help display is broken down\ninto multiple sections, the preambles of which are shown here. Run this\ninteractively to see the full report:\n>>> help(sys)\nHelp on built-in module sys:\nNAME\n    sys\nMODULE REFERENCE\n    https://docs.python.org/3.12/library/sys.html\n    …more omitted…\nDESCRIPTION\n    This module provides access to some objects used or maintained by the\n    …more omitted…\nSUBMODULES\n    monitoring\nFUNCTIONS\n    __breakpointhook__ = breakpointhook(...)\n    …more omitted…\nDATA\n    __stderr__ = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf...\n    …more omitted…\nFILE\n    (built-in)\nSome of the information in this report is docstrings, and some of it (e.g., function\ncall patterns) is structural information that Pydoc gleans automatically by\ninspecting objects’ internals using tools available to your code too.\nBesides modules, you can also use help on built-in functions, methods, and\ntypes. To get help for a built-in type, try either the type name (e.g., dict for\ndictionary or str for string); an actual object of the type (e.g., {} or ''); or a",
      "content_length": 1433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "method of an actual object or type name (e.g., 's'.join or str.join ).1 For an\nentire type, you’ll get a large display that describes all the methods available for\nthat type; for a method, the help is more focused:\n>>> help(dict)\nHelp on class dict in module builtins:\nclass dict(object)\n |  dict() -> new empty dictionary\n |  dict(mapping) -> new dictionary initialized from a mapping object's\n |  …more omitted…\n>>> help(str.replace)\nHelp on method_descriptor:\nreplace(self, old, new, count=-1, /)\n    Return a copy with all occurrences of substring old replaced by new.\n    …more omitted…\n>>> help(''.replace)\nHelp on built-in function replace:\nreplace(old, new, count=-1, /) method of builtins.str instance\n    Return a copy with all occurrences of substring old replaced by new.\n    …more omitted…\n>>> help(ord)\nHelp on built-in function ord in module builtins:\nord(c, /)\n    Return the Unicode code point for a one-character string.\nThe “/” in function docs means that arguments before it must be passed by\nposition only (not by name), but this is substantially above this chapter’s pay\ngrade. See the next part of this book for details on this function syntax.\nRunning help on your own code\nFinally, the help function works just as well on your modules as it does on built-\nins. Here it is reporting on components defined in the docstrings.py file we\ncoded earlier in Example 15-1. Again, some of this is our docstrings, and some\nis information automatically extracted by inspecting objects’ structures:\n>>> import docstrings\n>>> help(docstrings.square)\nHelp on function square in module docstrings:\nsquare(x)",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "Function documentation\n    Returns the \\square\\ of its \\numeric\\ argument.\n>>> help(docstrings.PartVI)\nHelp on class PartVI in module docstrings:\nclass PartVI(builtins.object)\n |  Class documentation for \n \n |\n |  Data descriptors defined here:\n |  …more omitted…\nAnd asking for help on the entire module collects all parts at once, with your\ndocstrings in the mix:\n>>> help(docstrings)\nHelp on module docstrings:\nNAME\n    docstrings\nDESCRIPTION\n    Module documentation\n    This module defines a name, function and class.\nCLASSES\n    builtins.object\n        PartVI\n    class PartVI(builtins.object)\n     |  Class documentation for \n \n     |\n     |  Data descriptors defined here:\n     |  …more omitted…\nFUNCTIONS\n    square(x)\n        Function documentation\n        Returns the \\square\\ of its \\numeric\\ argument.\nDATA\n    edition = 6\nFILE\n    /Users/me/MY-STUFF/Books/Dev/6E/LP6E/LP6E/Examples/Chapter15/docstrings.py\nIf you look closely, you’ll notice that we didn’t get our file’s printed output on\nimport this time, just because it was imported earlier; as noted in Chapter 3,",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "imports run just once per file per session (more in Part V).\nPydoc: HTML Reports\nThe text displays of the help function are adequate in many contexts, especially\nat the interactive prompt. To readers who’ve grown accustomed to richer\npresentation mediums, though, they may seem a bit rudimentary. This section\npresents the HTML-based flavor of Pydoc, which renders module documentation\nmore graphically for viewing in a web browser, and can even open one\nautomatically for you. This browser interface scheme combines both search and\ndisplay in a web page that communicates with an automatically started local\nserver.\nUsing Pydoc’s browser interface\nTo view docs in a browser, you’ll usually launch Pydoc’s main script with a\npydoc -b command line. This spawns two components running locally on your\ndevice: a documentation server, and a web-browser client that provides both\npage display and search input for content fetched from the server. A command\nline like the following starts the show:\n$ python3 -m pydoc -b\nServer ready at http://localhost:53965/\nServer commands: [b]rowser, [q]uit\nserver> q\nThis command works on all PCs; it’s known to also work on Android devices in\nthe Pydroid 3 app’s Terminal, but is spotty on phones in general. It uses the\nPython –m argument to locate Pydoc’s module file on the module-import search\npath (covered in Part V of this book), and run it as a top-level script. Per\nAppendix A and Chapter 3, use py instead of python3 in this command on\nWindows. On macOS and Linux, a pydoc3 -b command has the same effect,\nand on Windows the “Module Docs” entry in Python’s Start menu automatically\nruns Pydoc’s -b mode.\nHowever you start it, Pydoc’s browser mode opens with its index page, captured\nin Figure 15-1. The index page lists all the modules available for imports and\nincludes search boxes (e.g., Get loads a named module’s docs), as well as topic",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "guides and reserved-word info. You’ll see links to the docs of every module on\nyour module search path, including the directory where Pydoc is launched.\nPydoc’s server runs on the local host and on a dedicated but by default arbitrary\nunused port, but this can be tailored; launch Pydoc without -b for more options.\nFigure 15-1. The top-level index start page of the Pydoc browser interface\nWhen you click or tap on a module’s entry in Pydoc’s module-index page (or\nenter its name in Get), you get a page of docs for that module alone—which is\nessentially the same info that help provides in an interactive REPL, but\nformatted for display in your browser. Figure 15-2, for example, shows the page\nwe get for the docstrings module we coded earlier in Example 15-1. It’s the",
      "content_length": 772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "same report created for help(docstrings) with a simple HTML layout.\nFigure 15-2. Pydoc browser interface displaying a user-defined module’s docs\nExperiment with Pydoc on your own for more insight. It’s an optional tool, but\nprovides extra docs for both built-in tools provided by Python, as well as\ndocstring-loaded code files that you write yourself.\nCustomizing Pydoc\nIf you run this live, your Pydoc will probably look different than the screenshots\nin the book, because colors were customized on the machine used to capture the\nimages. Though this is entirely optional and requires some web-development",
      "content_length": 606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "knowledge, you can customize Pydoc’s appearance, too, by editing the simple\nCSS file it uses to render its pages. To modify colors, edit file _pydoc.css stored\nin the pydoc_data folder alongside Pydoc’s module—which you can locate on\nany platform by importing Pydoc’s module and inspecting its __file__\nattribute:\n$ python3 -c \"import pydoc; print(pydoc.__file__)\"\n…foldepath…/pydoc.py\n$ ls …foldepath…/pydoc_data          # Pydoc's CSS customization file is in here\n__init__.py        topics.py\n__pycache__        _pydoc.css\nOn Windows, use py and dir instead of python3 and ls. This uses the Python -\nc command-line argument to submit code to be run as a string, which is handy\nfor short bits of one-off code like this; here, the two statements separated by a\nsemicolon run the same as they would if they were typed one at a time at a\nREPL’s >>> prompt.\nYou can also find the path to Pydoc by running help('pydoc'): its path shows\nup at the bottom of the display. Once you’ve located the CSS file, edit it to\ncustomize as you like (but save the original first as a fallback). As an example,\nthis book’s captures use RGB color string #173166 for index-decor. All of\nwhich assumes CSS skills beyond this book’s scope (plus an undocumented\nconfig file that’s prone to change or vanish in the future!), but is probably\nenough to get you started if you want to have a go.\nMore Pydoc tips\nPydoc works by importing selected files to extract their documentation. This has\ntwo usage implications. First, Pydoc can access only files in folders included on\nthe module search path used by imports. This path automatically includes\nPython’s standard library, so you can view docs for all built-in modules. It also\nincludes the current working directory—the folder from which Pydoc was\nstarted—but that may be meaningless when started from a Windows Start button.\nChange your PYTHONPATH setting to include other code folders as needed, per\nAppendix A.\nSecond, although Pydoc can render docs for both importable modules and",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "runnable scripts in accessible folders, script docs comes with a catch: when a file\nis selected for help, Pydoc must import it in order to collect its documentation,\nand as we learned in Chapter 3, importing runs a file’s top-level code. Hence,\nscript help implies a script run.\nImported modules normally just define tools when run, so this is usually\nirrelevant. If you ask for the documentation of a top-level script file, though, the\nscript will be run, and the console window where you launched Pydoc serves as\nthe script’s standard input and output for any user interaction. This may work\nbetter for some scripts and modes than others; script IO may appear before or\nafter help is dismissed in console mode, before help is scrolled in a GUI like\nIDLE, or interleaved oddly with Pydoc’s own server-command prompts in\nbrowser mode.\nLater in the book you’ll learn ways to code top-level logic that’s kicked off only\nwhen a file is run, not when it is imported (e.g., by nesting it under a test for\nvariable __name__ being '__main__'). If scripts use these protocols, they’re\nsafe to view in Pydoc, because it won’t run any top-level code as an inadvertent\nside effect of viewing docs.\nFinally, Pydoc has additional tools we’ll skip here for space, including a -w\nswitch for saving HTML docs to a file for later viewing, and a plain-text mode\nrun from a command line with just a topic name that works the same as help in\na REPL. Again, run Pydoc without any command-line arguments for its full set\nof options.\nNOTE\nBlast from the past: Pydoc once had a simple GUI mode, invoked by flag -g, that launched a\ntkinter GUI client that communicated with the server. This was available until Python 3.2,\nwhen the current browser-only mode was deemed sufficient to warrant dropping the\nlongstanding GUI mode in full. Browsers are GUIs too, of course, and the CSS configuration\nscheme for Pydoc that arose with the browser mode assumes extra web-development\nknowledge that many Python users won’t have. In open source, those with time and desire to\nchange things define the future for everyone.\nBeyond Docstrings: Sphinx",
      "content_length": 2112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "If you’re looking for a way to document your Python programs in a more\nsophisticated way, you may wish to check out Sphinx—a documentation\ngenerator system used to create Python’s standard manuals described in the next\nsection, as well as docs for many other software projects. It uses simple\nreStructuredText as its markup language and inherits much from the Docutils\nsuite of parsing and translating tools.\nAmong other things, Sphinx supports a variety of output formats, automatic\ncross-references and indexes, and automatic code highlighting (colorization)\nusing Pygments, which is itself a noteworthy Python tool. This is probably\noverkill for smaller programs where docstrings and help may suffice, but can\nyield higher-grade documentation for larger projects. See the web for more\ndetails on Sphinx, its related tools, and other options in the docs domain.\nThe Standard Manuals\nAs you know by now, this book is a tutorial that teaches by example. While its\nindex and table of contents can be used to hunt for random topics after the fact,\nit’s mainly designed to be read, not to serve as a reference resource. Given that,\nyou’ll probably want to supplement this book with reference tools once you\nmove on to real projects.\nAmong these, Python’s standard manuals may provide the most complete and\nup-to-date reference to the language and its toolset. You can easily view these\nmanuals online with the Documentation link at Python’s website. They can also\nbe viewed via the “Manuals” entry in Python’s entry in the Start menu on\nWindows, and can be opened from “Python Docs” in the Help menu within the\nIDLE coding GUI. See your toolset for other access options.\nWhen first opened, the manuals display a root page with a search box, like that\ncaptured in Figure 15-3. The two most important entries here are most likely the\n“Library reference” (which documents built-in types, functions, exceptions, and\nstandard-library modules) and the “Language reference” (which provides a\nformal description of language-level details). Both will probably be regular\ncompanions once you start coding Python in earnest.\nAlso of notable interest, the “What’s new” documents in this standard manual set\nchronicle Python changes made in each release beginning with Python 2.0,",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "which came out in late 2000—useful for those porting older Python code, or\nolder Python skills. These documents are also useful for uncovering additional\ndetails on the differences in the Python 2.X and 3.X language lines, as well as\nrecent Python 3.X changes covered in this book.\nFor both better and worse, change has been a constant in Python since its 0.X\ndays. While the future is impossible to predict, the standard manuals’ “What’s\nnew” docs will also cover Python mods almost certain to arise after this book’s\nrelease. It’s nearly required reading for anyone working downstream of a\nperpetually morphing sandbox.",
      "content_length": 621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "Figure 15-3. Python’s standard manuals, available on the web and elsewhere\nWeb Resources\nFinally, besides the standard manuals, Python’s website also hosts additional\nresources, some of which cover special topics or domains, including non-English\nPython resources and introductions scaled to different target audiences.\nToday you will also find a multitude of Python blogs, websites, wikis, and other\nresources on the web at large. Given the explosion of the web in recent decades,\nsome online resources are naturally more authoritative and reliable than others,\nand some are sadly geared more toward monetization than education. This word\nof caution includes AI chatbots, which can only do as well as the data they’re fed\nand paraphrase, and are not a replacement for the deep learning needed to work\nin software.\nThat said, there’s a wealth of Python material out there to be had—for those\nwilling to exercise the prudence and skepticism that today’s web demands.\nCommon Coding Gotchas\nBefore the programming exercises for this part of the book, let’s run through\nsome of the most common mistakes beginners make when coding Python\nstatements and programs. Many of these are warnings issued earlier in this part\nof the book, collected here for ease of reference. You’ll learn to avoid these\npitfalls once you’ve gained a bit of Python coding experience, but a few words\nnow might help you avoid falling into some of these traps initially:\nDon’t forget the colons. Always remember to type a : at the end of\ncompound statement headers—the first line of an if, match, while,\nfor, etc. You’ll probably forget at first (as have thousands of Python\nstudents over the years), but you can take some comfort from the fact\nthat it will soon become an unconscious habit.\nStart in column 1. Be sure to start top-level (unnested) code in column\n1. That includes unnested code typed into module files, as well as\nunnested code typed at the interactive prompt (a.k.a. REPL). Indented",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "means nested in Python, so it doesn’t work at the top.\nBlank lines matter at the interactive prompt. Blank lines in\ncompound statements are always irrelevant and ignored in module files,\nbut when you’re typing code at the interactive prompt, they end the\nstatement. In other words, blank lines tell the interactive REPL that\nyou’ve finished a compound statement; if you want to continue, don’t\nhit the Enter key at the ... prompt (if shown) until you’re really done.\nThis also means you can’t paste multiline code at this prompt; it must\nrun one full statement at a time, with blank lines as needed.\nIndent consistently. Avoid mixing tabs and spaces in the indentation of\na nested block. Otherwise, what you see in your editor may not be what\nPython sees when it counts tabs as a number of spaces. This is true in\nany block-structured language, not just Python—if the next programmer\nhas tabs set differently, it will be difficult or impossible to understand\nthe structure of your code. It’s safer to use all tabs or all spaces for each\nblock.\nDon’t code C in Python. A reminder for C/C++ programmers: you\ndon’t need to type parentheses around tests in if and while headers\n(e.g., if (X==1)). You can if you like (any expression can be enclosed\nin parentheses), but they are fully superfluous in this context. Also, do\nnot terminate all your statements with semicolons; it’s technically legal\nto do this in Python as well, but it’s totally useless unless you’re placing\nmore than one statement on a single line (the end of a line normally\nterminates a statement). And remember, don’t use {} around blocks\n(indent your nested code blocks consistently instead), and avoid\nembedding assignment statements in while loop tests unless they’re\nsimple (even though Python now enables this with :=).\nUse simple for loops instead of while or range. Another reminder: a\nsimple for loop (e.g., for x in y) is almost always simpler to code\nand often quicker to run than a while- or range-based counter loop.\nAvoid the temptation to count things in Python; though occasionally\nrequired, it’s usually subpar.",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "Beware of mutables in assignments. As cautioned before, be careful\nabout using mutables in a multiple-target assignment (a = b = []), as\nwell as in an augmented assignment (a += [1, 2]). In both cases, in-\nplace changes may impact other variables. See Chapter 11 for details if\nyou’ve forgotten why this is true.\nDon’t expect results from functions that change objects in place. We\nencountered this nag earlier, too: in-place change operations like the\nlist.append and list.sort methods presented in Chapter 8 do not\nreturn values (other than None), so you should call them without\nassigning the result. It’s not uncommon for beginners to say something\nlike mylist = mylist.append(X) to try to get the result of an append,\nbut this assigns mylist to None, not to the modified list (in fact, you’ll\nlose your reference to the list altogether).\nAlways use parentheses to call a function. You must add parentheses\nafter a function name to call it, whether it takes arguments or not (e.g.,\nuse function(), not function). In the next part of this book, you’ll\nlearn that functions are simply objects that have a special operation—a\ncall that you trigger with the parentheses—but they can be referenced\nlike any other object without triggering a call. This often crops up with\nfiles; it’s common for beginners to type file.close to close a file,\nrather than file.close(). Because it’s legal to reference a function\nwithout calling it, the first version succeeds silently, but it does not\nclose the file.\nDon’t use extensions or paths in imports and reloads. Omit directory\npaths and file extensions in import statements—say import mod, not\nimport mod.py. We discussed module basics in Chapter 3 and will\ncontinue studying modules in Part V. Because modules may have other\nextensions besides .py (e.g., .pyc for bytecode), hardcoding a particular\nextension is not only invalid syntax; it doesn’t make sense. Python picks\nan extension automatically, and any platform-specific directory path\nsyntax comes from module search path settings, not the import\nstatement. Until we explore this in more depth, use just a simple name\nin imports.",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "And other pitfalls in other parts. Be sure to also see the built-in type\nwarnings at the end of Part II, as they may qualify as coding issues too.\nThere are additional “gotchas” that crop up commonly in Python coding\n—losing a built-in function by reassigning its name, hiding a library\nmodule by using its name for one of your own, changing mutable\nargument defaults, and so on—but we don’t have enough background to\ncover them yet. To learn more about both what you should and\nshouldn’t do in Python, you’ll have to read on; later parts extend the set\nof “gotchas” and fixes we’ve enumerated here.",
      "content_length": 599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter toured documentation—both documentation we write ourselves in\nour own programs, and documentation available for tools we use. We met\ndocstrings, explored reference resources for Python, and learned how Pydoc’s\nhelp function and browser-based interfaces provide extra sources of\ndocumentation. Because this is the last chapter in this part of the book, we also\nreviewed common coding mistakes to help you avoid them.\nIn the next part of this book, we’ll start applying what we already know to larger\nprogram constructs. Specifically, the next part takes up the topic of functions—a\ntool used to group statements for reuse. Before moving on, however, be sure to\nwork through the set of lab exercises for this part of the book that appear at the\nend of this chapter. And even before that, let’s run through this chapter’s quiz to\nreview.\nTest Your Knowledge: Quiz\n1. When should you use documentation strings instead of hash-mark\ncomments?\n2. Name three ways you can view documentation strings.\n3. How can you obtain a list of the available attributes in an object?\n4. How can you get a list of all importable modules on your computer?\nTest Your Knowledge: Answers\n1. Documentation strings (docstrings) are considered best for larger,\nfunctional documentation, describing the use of modules, functions,\nclasses, and methods in your code. Hash-mark comments are better\nlimited to smaller-scale documentation about arcane expressions or\nstatements at strategic points on your code. This is partly because\ndocstrings are easier to find in a source file (they have specific",
      "content_length": 1595,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "locations), but also because they can be extracted and displayed by the\nPydoc system.\n2. You can see docstrings by printing an object’s __doc__ attribute, by\npassing it to Pydoc’s help function, and by selecting modules in\nPydoc’s HTML-based browser interface. The latter is launched with a -\nb command-line switch, and runs a client/server system that displays\ndocumentation in a popped-up web browser. Pydoc can also be run to\nsave a module’s documentation in an HTML file for later viewing or\nprinting.\n3. The built-in dir(X) function returns a list of all the attributes attached\nto any object. A list comprehension like [a for a in dir(X) if not\na.startswith('__')] can be used to filter out internals’ names with\nunderscores (you’ll learn how to wrap this in a function in the next part\nof the book to make it reusable).\n4. Because the index page opened for Pydoc’s browser-based -b mode\ndisplays every module on your import search path, this shows all the\nmodules available for imports in your programs. This relies on\nconfigurable search-path settings that we’ll cover more fully later in this\nbook, but it includes both the current directory and Python’s standard\nlibrary by default. You can also write code of your own to achieve this\nusing the same tools the Pydoc uses (it’s just Python code, after all), but\nit’s a significant manual task.",
      "content_length": 1352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "Test Your Knowledge: Part III Exercises\nNow that you know how to code basic program logic, the following exercises\nwill ask you to implement some simple tasks with statements. Much of the work\nis in exercise 4, which lets you explore coding alternatives. There are always\nmany ways to arrange statements, and part of learning Python is learning which\narrangements work better than others. You’ll eventually gravitate naturally\ntoward what experienced Python programmers call “best practice,” but best\npractice takes practice.\nSee “Part III, Statements and Syntax” in Appendix B for solutions to the\nfollowing exercises:\n1. Coding basic loops: This exercise asks you to experiment with for\nloops.\na. Write a for loop that prints the ASCII code point of each\ncharacter in a string named S. Use the built-in function\nord(character) to convert each character to its integer code\npoint. This function technically returns a Unicode code point\nthat may not fall in ASCII’s range, but if you restrict its content\nto ASCII characters, you’ll get back ASCII codes. (Test it\ninteractively to see how it works, if needed.)\nb. Next, change your loop to compute the sum of the ASCII code\npoints of all the characters in a string.\nc. Finally, modify your loop again to return a new list that\ncontains the ASCII code points of each character in the string.\nDoes the expression map(ord, S) have a similar effect? How\nabout [ord(c) for c in S]? Why? (Hint: see Chapter 14.)\n2. Coding basic selections: Write Python if and match statements that\nprint the first three month names of the year, given their relative\nnumbers. For example, given 1, the output should be January, and for 3,\nit should be March (or whatever months are named in your locale).",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "Then do the same by coding the choice with both a dictionary-key index\nand a list-offset index. How would you handle out-of-range month\nnumbers?\n3. Backslash characters: What happens on your machine when you type\nthe following code interactively?\nfor i in range(50):\n    print(f'hello {i}\\a')\nBeware that if it’s run outside of some interfaces like IDLE, this\nexample may beep at you, so you may not want to run it in a crowded\nroom! IDLE ignores odd characters instead of beeping—spoiling much\nof the joke (see the backslash escape characters in Table 7-2).\n4. Sorting dictionaries: In Chapter 8, we saw that dictionaries are\ncollections that store keys by insertion order only. Write a for loop that\nprints a dictionary’s items in sorted (ascending) key/value order. (Hint:\nstore keys in unordered fashion, and use the dictionary keys and list\nsort methods, or the sorted built-in function.)\n5. Program logic alternatives: Consider the following code, which uses a\nwhile loop and found flag to search a list of powers of 2 for the value\nof 2 raised to the fifth power (32). It’s stored in a module file called\npower.py:\nL = [1, 2, 4, 8, 16, 32, 64]\nX = 5\nfound = False\ni = 0\nwhile not found and i < len(L):\n   if 2 ** X == L[i]:\n       found = True\n   else:\n       i = i+1",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "if found:\n    print('at index', i)\nelse:\n    print(X, 'not found')\n$ python3 power.py\nat index 5\nAs is, the example doesn’t follow normal Python coding techniques.\nFollow the steps outlined here to improve it (for all the transformations,\nyou may either type your code interactively or store it in a script file run\nfrom the system command line or other interface—using a file makes\nthis exercise much easier):\na. First, rewrite this code with a while loop else clause to\neliminate the found flag and final if statement.\nb. Next, rewrite the example to use a for loop with an else\nclause, to eliminate the explicit list-indexing logic. (Hint: to get\nthe index of an item, use the list index method—L.index(X)\nreturns the offset of the first X in list L.)\nc. Next, remove the loop completely by rewriting the example\nwith a simple in operator membership expression. (See\nChapter 8 for more details, or type this to test: 2 in [1, 2,\n3].)\nd. Finally, use a for loop and the list append method to generate\nthe powers-of-2 list (L) instead of hardcoding a list literal.\nDeeper thoughts:\na. Do you think it would improve performance to move the 2 **\nX expression outside the loops? How would you code that?\nb. As we saw in exercise 1, Python includes a map(function,\niterable) tool that can generate a powers-of-2 list, too:",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "map(lambda x: 2 ** x, range(7)). Try typing this code\ninteractively; you’ll meet lambda more formally in the next\npart of this book, especially in Chapter 19. Would a list\ncomprehension help here (see Chapter 14)? How about a :=\nexpression (see Chapter 11)?\n1  Note that asking for help on an actual string object directly (e.g., help('xyz')) doesn’t work as you\nmay expect: you usually get no help, because strings are interpreted specially—as a request for help\non a topic or tool by name, as described earlier. You must use the str type name in this context,\nthough both an empty string (help('')) and string method names referenced through actual objects\n(help(''.join)) work fine. An interactive help mode, started by typing just help(), avoids manual\nhelp calls, and may avoid some of their drama.",
      "content_length": 803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "Part IV. Functions and Generators",
      "content_length": 33,
      "extraction_method": "OCR"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "Chapter 16. Function Basics\nIn Part III, we studied basic procedural statements in Python. Here, we’ll move\non to explore a set of additional statements and expressions that we can use to\ncreate functions of our own.\nIn simple terms, a function is a package of code invoked by name. It labels and\ngroups a set of statements so they can be run more than once in a program. A\nfunction also can compute a result value, and lets us specify parameters that\nserve as inputs and may differ each time the function’s code is run. Wrapping an\noperation in a function makes it a generally useful tool, which we can apply in a\nvariety of contexts.\nOn a more pragmatic level, functions are the alternative to programming by\ncutting and pasting—rather than having multiple redundant copies of an\noperation’s code, we can factor it into a single function. In so doing, we reduce\nour future work radically: if the operation must be changed later, we have only\none copy to update in the function, not many scattered throughout the program.\nFunctions are also the most basic program structure Python provides for\nmaximizing code reuse, and lead us to the larger notions of program design. As\nyou’ll see, functions let us split complex systems into manageable parts. By\nimplementing each part as a function, we make it both reusable and easier to\ncode.\nTable 16-1 abstractly previews the function-related tools we’ll study in this part\nof the book—a set that includes call expressions, two ways to make functions\n(def and lambda), two ways to manage scope visibility (global and nonlocal),\ntwo ways to send results back to callers (return and yield), and tools to pause\nfor results (async and await). While this comprises a feature-rich topic, you’ll\nfind that its commonly used core is straightforward.\nTable 16-1. Function-related statements and expressions\nStatement or\nexpression\nExamples",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "Call expressions\nmyfunc('hack', tool=python, *versions)\ndef\ndef printer(message): print('Hello', message)\nreturn\ndef adder(a, b=1, *c): return a + b + c[0]\nlambda\nfuncs = [lambda x: x**2, lambda x: x**3]\nglobal\nx = 'old' def changer(): global x; x = 'new'\nnonlocal\ndef outer(): x = 'old' def changer(): nonlocal x; x =\n'new'\nyield\ndef squares(x): for i in range(x): yield i ** 2\nGenerator expressions\n(i ** 2 for i in range(x))\nasync/await\nasync def consumer(a, b): await producer(b)\nDecorators and\nannotations\n@tracer def func(a: 'hack' = None) -> None\nWhy Use Functions?\nBefore we get into the details, let’s establish a clearer picture of what functions\nare all about. Functions are a nearly universal program-structuring device. You\nmay have come across them before in other languages, where they may have\nbeen called subroutines or procedures. As a brief introduction, functions serve\ntwo primary development roles, and serve as the basis of other tools:\nMaximizing reuse and minimizing redundancy\nAs in most programming languages, Python functions are the simplest way\nto package logic you may wish to use in more than one place and more than\none time. Up until now, almost all the code we’ve been writing has run\nimmediately. Functions allow us to defer and generalize code to be used\narbitrarily many times later. Because they allow us to code an operation in a",
      "content_length": 1369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "single place and use it in many others, functions are also a factoring tool:\nthey enable us to reduce code redundancy in our programs, and thereby\nreduce maintenance effort.\nDividing and conquering\nFunctions also provide a tool for splitting systems into pieces that have well-\ndefined roles and manageable scopes. For instance, to make a pizza from\nscratch, you would start by mixing the dough, rolling it out, adding toppings,\nbaking it, and so on. If you were programming a pizza-making robot,\nfunctions would help you divide the overall “make pizza” task into smaller\nparts—one function for each subtask in the process. Because it’s easier to\nimplement the smaller tasks in isolation than it is to implement the entire\nprocess at once, this makes larger tasks more practical.\nImplementing object methods\nIn general, functions are about procedure—how to do something, rather than\nwhat you’re doing it to. Nevertheless, when paired with an implied subject,\nthey can also be used to code object-specific behavior known as methods.\nYou’ll see why this distinction matters in Part VI, when we start making new\nobjects with classes. As you’ll find, classes are largely just packages of the\nfunctions you’ll learn to code here.\nIn this part of the book, we’ll explore the tools used to code functions in Python:\nfunction basics, scope rules, and argument passing, along with a few related\nconcepts such as generators, coroutines, and functional tools. Because its\nimportance begins to become more apparent at this level of coding, we’ll also\nrevisit the notion of polymorphism, which was introduced earlier in the book. As\nyou’ll see, functions don’t imply much new syntax, but they do lead us to some\nbigger programming ideas.",
      "content_length": 1724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": "Function Coding Overview\nAlthough it wasn’t made very formal, we’ve already used functions in earlier\nchapters. For instance, we called the built-in open function to make a file object,\ninvoked the len built-in function to ask for the number of items in a collection\nobject, and employed tools like zip and range for iteration tasks.\nIn this chapter, we will begin exploring how to write new functions in Python.\nFunctions we write behave the same way as the built-ins we’ve already seen:\nthey are called in expressions, are passed values, and return results. But writing\nnew functions requires the application of a few additional tools and ideas that\nhaven’t yet been introduced. Moreover, functions behave very differently in\nPython than they do in compiled languages like C.\nBasic Function Tools\nAs a road map and brief rundown on what we’ll study in this part of the book,\nhere are the basic components and concepts in Python’s function toolbox:\ndef creates a function and assigns it to a name. Python functions are\ncoded with def statements. When Python reaches and runs a def, it\ngenerates a new function object and assigns it to the function’s name.\nAs with all assignments, the function name becomes a reference to the\nfunction object. There’s nothing magical about the name of a function—\nas you’ll see, the function object can be assigned to other names, stored\nin a list, and so on. Function objects may also have arbitrary user-\ndefined attributes attached to them to record data.\ndef is executable code. Unlike functions in compiled languages, def is\nan executable statement—your function does not exist until Python runs\nits def. In fact, it’s legal (and even occasionally useful) to nest def\nstatements inside if statements, for loops, and even other defs. In\ntypical usage, def statements are coded at the top level of module files,\nand are automatically run to make functions when their modules are\nimported.\nreturn sends a result object back to the caller. When a function is",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": "called, the caller normally stops until the function finishes its work and\nreturns control to the caller. Functions that compute a value send it back\nto the caller with a return statement, and the returned value becomes\nthe result of the function call. A return without a value simply returns\nto the caller (and sends back None, the default result).\nlambda creates a function but returns it as a result. Function objects\nmay also be created with the lambda expression, a feature that allows us\nto code inline function definitions in places where a def statement\nwon’t work syntactically. Like def, functions made this way are created\nwhen the lambda is reached and run. Because lambda is not typically\ncoded at the top level of a module, though, such functions are made\nduring program runs, not imports. lambda is an optional convenience\nbest seen as a sidebar to the def statement.\nAdvanced Function Tools\nIn addition, Python functions can leverage more advanced tools like the\nfollowing, which we’ll take up after we’ve introduced the basics:\nglobal declares module-level variables that are to be assigned. By\ndefault, all names assigned in a function are local to that function and\nexist only while the function runs. To assign a name in the enclosing\nmodule, functions need to list it in a global statement. More generally,\nnames are always looked up in scopes—places where variables are\nstored—and names are bound to scopes per the location of assignments\nin your code.\nnonlocal declares enclosing-function variables that are to be\nassigned. In the same category as global, the nonlocal statement\nallows a function to assign a name that exists in the scope of a\nsyntactically enclosing def statement. This allows enclosing functions\nto serve as a place to retain state—information remembered between\nfunction calls—without using shared global names.\nyield sends a result object back to the caller, but remembers where",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": null,
      "content": "it left off. Functions known as generators may also use the yield\nstatement to send back a value and suspend their state such that they\nmay be resumed later, to produce a series of results over time. Along\nwith their generator expression kin, such functions save space and\navoid delays just like the built-in iterables we met in the prior part of\nthis book.\nawait/async pause a waiting function so that other tasks may run.\nFunctions known as coroutines can suspend their execution until a\nrequired result is available. This is an advanced applications-level topic,\nbut allows code to use language tools to get other work done while\nwaiting on a blocking event like slow IO.\nGeneral Function Concepts\nFinally, along the way we’ll explore a handful of core concepts that span\nfunctions of all kinds:\nArguments are passed by assignment (object reference). In Python,\narguments are passed to functions by assignment—which, as we’ve\nlearned, means by object reference. As you’ll see, in Python’s model the\ncaller and function share objects by references, but there is no name\naliasing. Changing an argument name within a function does not also\nchange the corresponding name in the caller, but changing passed-in\nmutable objects in place can change objects shared by the caller, and\nserve as a function result.\nArguments are passed by position, unless you say otherwise. Values\nyou pass in a function call match argument names in a function’s\ndefinition from left to right by default. For flexibility, function calls can\nalso pass arguments by name with name=value keyword syntax and\nunpack arbitrarily many arguments to send with *args and **args\nstarred-value notation. Function definitions use the same syntax forms\nto specify argument defaults and collect arbitrarily many arguments\nreceived.\nArguments, return values, and variables are not declared. As with",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": null,
      "content": "everything in Python, there are no type constraints on functions. In fact,\nnothing about a function needs to be declared ahead of time: you can\npass in arguments of any type, return any kind of object, and so on. As a\nconsequence, a single function can often be applied to a variety of\nobject types—any objects that have a compatible interface (support for\nexpected methods and expressions) will do, regardless of their specific\ntypes. This makes code flexible by design.\nAttributes, annotations, and decorators support advanced roles.\nFunctions can optionally be augmented with both user-defined attributes\nand other tools that extend their roles. Annotations of arguments and\nresults, for example, can serve a variety of goals. Among these, type\nhinting (noted in Chapter 6) gives suggested object types but is unused\nby Python itself and simply serves as a weighty form of documentation,\nwhich we won’t cover further in this book. More usefully, functions can\nalso be prefixed with @ decorators that add extra layers of logic but are\nused in ways that merit largely deferring until Part VI when we’ve also\nlearned about classes.\nIf some of the preceding words didn’t sink in, don’t worry—we’ll explore most\nof these concepts with real code in this part of the book. Let’s get started by\nexpanding on some of the basics by coding a few abstract examples.\ndef Statements\nThe def statement creates a function object and assigns it to a name. Its general\nformat and usage is as follows:\ndef name(arg1, arg2,… argN):        # Define a function\n    statements                      # Function body\nname(val1, val2,… valN)             # Call it later in an expression\nAs with all compound statements, def consists of a header line followed by a\nblock of other statements, usually indented (though a simple statement after the\ncolon works as usual). The statement block becomes the function’s body—that\nis, the code Python executes each time the function is later called.",
      "content_length": 1965,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": null,
      "content": "The def header line specifies a function name that is assigned the new function\nobject, along with a list of zero or more arguments (sometimes called\nparameters) enclosed in parentheses (even for zero arguments). The argument\nnames in the header are assigned to the objects passed in parentheses at the point\nof call elsewhere in your code.\nTo call the function later in your program and run its body, you can use the\nfunction name assigned by def, passing in zero or more values (objects) to\nmatch the arguments listed in the def header. A call expression can also denote\nthe function with other types of object references; it need not necessarily be the\nname coded in the def header.\nreturn Statements\nFunction bodies often contain one or more return statements that make sense\nonly inside a def:\ndef name(arg1, arg2,… argN):\n    …\n    return result                   # The result of the call expression\nThe Python return statement can show up anywhere in a function body. When\nreached, it ends the function call and sends a result back to the caller to serve as\nthe result of the call expression. The return statement consists of an optional\nobject-value expression that gives the result. If the value is omitted, return\nsends back a None by default.\nThe return statement itself is optional too; if it’s not present, the function exits\nwhen the control flow falls off the end of the function body. Technically, a\nfunction without a return statement also returns the None object automatically,\nbut this return value is usually ignored at the call, by using a call-expression\nstatement of Chapter 11.\nFunctions may also contain yield statements used to produce a series of values\nover time, as well as await and async for/with statements used to suspend the\nfunction’s execution, but we’ll defer discussion of these more advanced tools\nuntil we survey generator and coroutine topics in Chapter 20.",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": null,
      "content": "def Executes at Runtime\nThe Python def is a true executable statement: when it runs, it creates a new\nfunction object and assigns it to a name. (Remember, all we have in standard\nPython is runtime; there is no such thing as a separate compile time.) Because\nit’s a statement, a def can appear anywhere a statement can—even nested in\nother statements. For instance, although defs are normally top-level code run\nwhen the module enclosing them is imported, it’s also completely legal to nest a\nfunction def inside an if statement to select between alternative definitions:\nif test:\n    def func():            # Define func this way\n        …\nelse:\n    def func():            # Or else this way\n        …\n…\nfunc()                     # Call the version selected and built\nOne way to understand this code is to realize that the def is much like an =\nstatement: it simply assigns a name (like func here) at runtime. Unlike in\ncompiled languages such as C, Python functions do not need to be fully defined\nbefore the program runs. More generally, defs are not run until they are reached,\nand the code inside defs is not run until the functions are later called.\nBecause function definition happens at runtime, there’s nothing special about the\nfunction name. What’s important is the object to which it refers. For example:\nothername = func           # Assign function object\nothername()                # Call func again\nHere, the function was assigned to a different name and called through the new\nname. Like everything else in Python, functions are just objects; they are\nrecorded explicitly in memory at program execution time. In fact, besides calls,\nfunctions allow arbitrary attributes to be attached to record information for later\nuse:\ndef func(): …              # Create + assign function object\nfunc()                     # Call object via its name\nfunc.attr = value          # Attach attributes",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": null,
      "content": "In other words, functions are first-class objects, to borrow a term introduced in\nChapter 9. While they don’t support some operations that other objects do,\nfunctions inhabit the same category as every object. As you’ll see, passing them\nabout and storing them in other objects is both syntactically legal and\nsurprisingly useful.\nlambda Makes Anonymous Functions\nIn addition to def, you can make a new function with the lambda expression. It’s\ncoded and might be used like this:\nname = lambda arg1, arg2,… argN: expression\nname(val1, val2,… valN)\nLike def, lambda makes a new function object to be called later. It begins with\nthe word lambda, followed by an arguments-list header that works the same as\nin def but is coded without parentheses. Unlike def, the code after the : in\nlambda is a single expression—it cannot contain statements and is the implied\nbody and return value of the function that lambda makes. We don’t need to say\nreturn in a lambda (and we can’t).\nAlso unlike def, lambda does not itself assign the new function to a name, but\nsimply returns it as the result of the whole lambda expression. This snippet\nmanually assigns the function to a name through which it is called, but that’s\noptional: the result might also be saved in another object or passed for use\nelsewhere. Because of this, lambda is usually called an “anonymous” function—\none that’s unnamed.\nYou’ll learn more about lambda later and see it in action along the way. But your\nfirst question may be, Why have a version of def whose code is limited to just\none expression? In short, lambda can be used in places that def cannot,\nincluding in calls and object literals where we want to embed small bits of\ndeferred and runnable code. While def handles larger tasks and also supports\nembedding functions by name, lambda is an optional amenity in simpler roles.\nAnd if you’re keeping track, we’ve now seen four expression equivalents for",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": null,
      "content": "more general statements—the lambda for def, the := named assignment for =,\nthe if/else ternary for if, and the comprehension for for. Although lambdas\nare limited to expressions, they can use any of these expression equivalents to\ncode assignments, logic, and loops. All these expression forms were\naccumulated over time, but are convenient alternatives in practical code.\nA First Example: Definitions and Calls\nApart from their runtime flavor (which tends to seem most novel to\nprogrammers with backgrounds in compiled languages), Python functions are\nstraightforward to use. Let’s code a first real example to demonstrate the\nfundamentals, from both sides of the function equation: definition (the def or\nlambda that creates a function) and calls (the expressions that tell Python to run\nthe function’s body).\nDefinition\nHere’s a definition typed interactively that defines a function named times,\nwhich returns the product of its two arguments (it’s not much, but it demos the\nbasic bits). As usual “…” prompts are omitted here for copy and paste:\n>>> def times(x, y):       # Create and assign function\n        return x * y       # Body executed when called\nWhen Python reaches and runs this def, it creates a new function object that\npackages the function’s code, and assigns this object to the name times.\nTypically, such a statement is coded in a module file and runs when the\nenclosing file is imported; for something this small, though, the interactive\nREPL suffices.\nAs an aside, a lambda can have the same effect if we assign its result to a name,\nthough it’s typically used in more focused roles than this, and often not as top-\nlevel code. Try the def, lambda, or both if you’re working along—they both\nmake function objects that work the same:\n>>> times = lambda x, y: x * y",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": null,
      "content": "Calls\nBoth def and lambda make a function but do not call it. After either has run, you\ncan call (i.e., run) the function in your program by adding parentheses after the\nfunction’s name. The parentheses may optionally contain one or more object\narguments, to be passed (i.e., assigned) to the names in the function’s header:\n>>> times(2, 4)            # Arguments in parentheses\n8\nThis expression passes two arguments to times. As mentioned previously,\narguments are passed by assignment, so in this case the name x in the function\nheader is assigned the object 2, y is assigned 4, and the function’s body is run.\nFor this function, the body is just a def’s return statement or a lambda’s\nexpression, that sends back the result as the value of the call expression. The\nreturned object was printed here interactively (as in most languages, 2 * 4 is 8\nin Python), but if we needed to use it later we could instead assign it to a\nvariable. For example:\n>>> x = times(3.14, 4)     # Save the result object\n>>> x\n12.56\nNow, watch what happens when the function is called a third time, with very\ndifferent kinds of objects passed in:\n>>> times('Py', 4)         # Functions are \"typeless\"\n'PyPyPyPy'\nThis time, our function means something wholly different. In this third call, a\nstring and an integer are passed to x and y, instead of two numbers. Recall that *\nworks on both numbers and sequences; because we don’t constrain the types of\nvariables, arguments, or return values in Python, we can use times to either\nmultiply numbers or repeat sequences.\nIn other words, what our times function means and does depends on what we\npass into it. This is a core idea in Python (and perhaps the key to using the",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": null,
      "content": "language well), which merits a separate callout here.\nPolymorphism in Python\nAs we just saw, the very meaning of the expression x * y in our simple times\nfunction depends completely upon the kinds of objects that x and y are—thus,\nthe same function can perform multiplication in one instance and repetition in\nanother. Python leaves it up to the objects to do something reasonable for the\nsyntax. Really, * is just a dispatch mechanism that routes control to the objects\nbeing processed.\nThis sort of type-dependent behavior is known as polymorphism, a term we first\nmet in Chapter 4 that essentially means that the meaning of an operation depends\non the objects being operated upon. Because it’s a dynamically typed language,\npolymorphism runs rampant in Python. In fact, every operation is a polymorphic\noperation in Python: printing, indexing, the * operator, and much more.\nThis is deliberate, and it accounts for much of the language’s conciseness and\nflexibility. A single function, for instance, can generally be applied to a whole\ncategory of object types automatically. As long as those objects support the\nexpected interface (a.k.a. protocol), the function can process them. That is, if the\nobjects passed into a function have the expected methods and expression\noperators, they are plug-and-play compatible with the function’s logic.\nEven in our simple times function, this means that any two objects that support\na * will work, no matter what they may be, and no matter when they are coded.\nThis function will work on two numbers (performing multiplication), or a string\nand a number (performing repetition), or any other combination of objects\nsupporting the expected interface—even class-based objects we have not even\nimagined yet.\nMoreover, if the objects passed in do not support this expected interface, Python\nwill detect the error when the * expression is run and raise an exception\nautomatically. It’s therefore usually pointless to code error checking in such code\nourselves. In fact, doing so would limit our function’s utility, as it would be\nrestricted to work only on objects whose types we test for:\n>>> times('not', 'quite')",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": null,
      "content": "TypeError: can't multiply sequence by non-int of type 'str'\nThis turns out to be a crucial philosophical difference between Python and\nstatically typed languages like C++ and Java: in Python, your code is not\nsupposed to care about specific data types. If it does, it will be limited to\nworking on just the types you anticipated when you wrote it, and it will not\nsupport other compatible object types coded now or in the future. Although it is\npossible to test for types with tools like the type built-in function, doing so\nbreaks your code’s flexibility. By and large, we code to object interfaces in\nPython, not data types.\nOf course, some programs have unique requirements, and this polymorphic\nmodel of programming means we have to test our code to detect some errors that\nstatically typed compiled languages might be able to detect earlier. In exchange\nfor an initial bit of extra testing, though, we radically reduce the amount of code\nwe have to write, and radically increase our code’s flexibility. As you’ll come to\nfind with time, it’s a resounding net win in practice.\nA Second Example: Intersecting Sequences\nNext, let’s explore a second function example that does something a bit more\nuseful than multiplying arguments and further illustrates function basics.\nIn Chapter 13, we coded a for loop that collected items held in common in two\nstrings (and called it nearly intersection, except for duplicates). We noted there\nthat the code wasn’t as useful as it could be because it was set up to work only\non specific variables and could not be rerun later. Of course, we could copy the\ncode and paste it into each place where it needs to be run, but this solution is\nneither good nor general—we’d still have to edit each copy to support different\nsequence names, and changing the algorithm would then require changing\nmultiple copies.\nDefinition\nBy now, you can probably guess that the solution to this dilemma is to package\nthe for loop inside a function. Doing so offers a number of advantages:",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": null,
      "content": "Putting the code in a function makes it a tool that you can run as many\ntimes as you like.\nBecause callers can pass in arbitrary arguments, functions are general\nenough to work on any two sequences (or other iterables) you wish to\nintersect.\nWhen the logic is packaged in a function, you have to change code in\nonly one place if you ever need to change the way the intersection\nworks.\nCoding the function in a module file means it can be imported and\nreused by any program run on your machine.\nIn effect, wrapping the code in a function makes it a general utility, as captured\nby Example 16-1.\nExample 16-1. inter1.py\ndef intersect(seq1, seq2):\n   res = []                     # Start empty\n   for x in seq1:               # Scan seq1\n       if x in seq2:            # Common item?\n           res.append(x)        # Add to end\n   return res\nThe transformation from the simple code of Chapter 13 to this function is\nstraightforward; we’ve just nested the original logic under a def header and\nmade the objects on which it operates passed-in parameter names. Because this\nfunction computes a result, we’ve also added a return statement to send a result\nobject back to the caller.\nCalls\nBefore you can call a function, you have to make it. To do this, run its def\nstatement, either by typing it interactively or by coding it in a module file and\nimporting the file. You can paste it at a REPL as a whole, but we’ll import it\nfrom a file here (per Chapter 3, make sure you can see it in your REPL by\nworking in the file’s folder if needed). Once you’ve run the def by such means,\nyou can call the function by passing any two sequence objects in parentheses:",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": null,
      "content": ">>> from inter1 import intersect     # Get function from module\n>>> s1 = 'HACK'\n>>> s2 = 'CHOK'\n>>> intersect(s1, s2)                # Pass two strings\n['H', 'C', 'K']\nHere, we’ve passed in two strings, and we get back a list containing the\ncharacters in common. The algorithm the function uses is simple: “For every\nitem in the first argument, if that item is also in the second argument, append the\nitem to the result.” It’s a little shorter to say that in Python than in English, but it\nworks out the same.\nTo be fair, our intersect function could probably be quicker (it executes naive\nnested loops), isn’t really mathematical intersection (there may be duplicates in\nthe result), and isn’t required at all (as we’ve seen, Python’s set objects provide a\nbuilt-in intersection operation kicked off with &). Indeed, the function could be\nreplaced with a single list comprehension expression, as it exhibits the classic\ncollector-loop code pattern:\n>>> [x for x in s1 if x in s2]\n['H', 'C', 'K']\nWhich also works as a loop inside a lambda body expression—despite being five\nlines in the def. This suffices as a demo, but doesn’t make much sense in a\nsimple role like this (again, we’ll put lambdas to better use later):\n>>> intersect = lambda seq1, seq2: [x for x in seq1 if x in seq2]\n>>> intersect(s1, s2)\n['H', 'C', 'K']\nHowever it’s coded, though, this function does the job: its single piece of code\ncan apply to an entire range of object types, as the next section explains. In fact,\nwe’ll improve and extend this to support arbitrarily many operands in\nChapter 18, after we uncover more about argument passing modes.\nPolymorphism Revisited\nLike all good functions in Python, intersect is polymorphic. That is, it works\non arbitrary types, as long as they support the expected object interface:",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": null,
      "content": ">>> x = intersect([1, 2, 3], (1, 4))      # Pass mixed types\n>>> x                                     # Saved result object\n[1]\nThis time, we passed in different types of objects to our function—a list and a\ntuple (mixed types)—and it still picked out the common items. Because you\ndon’t have to specify the types of arguments ahead of time, the intersect\nfunction happily iterates through any kind of objects you send it, as long as they\nsupport the expected interfaces.\nFor intersect, this means that the first argument has to support the for loop,\nand the second has to support the in membership test, in both def and lambda\nflavors. Any two such objects will work, regardless of their specific types—that\nincludes physically stored sequences like strings and lists; all the iterable objects\nwe met in Chapter 14, including files and dictionaries; and even any class-based\nobjects we code that apply operator overloading techniques we’ll discuss later in\nthe book.1\nAnd here again, if we pass in objects that do not support these interfaces (e.g.,\nnumbers), Python will automatically detect the mismatch and raise an exception\nfor us—which is exactly what we want, and the best we could do on our own if\nwe coded explicit type tests:\n>>> intersect([1, 2, 3], 1)\nTypeError: argument of type 'int' is not iterable\nBy not coding type tests and allowing Python to detect the mismatches for us,\nwe both reduce the amount of code we need to write, and avoid artificially\nlimiting our code’s scope.\nSegue: Local Variables\nPerhaps the most interesting part of this example, though, is its names. It turns\nout that the variable res inside the def version of intersect is what in Python\nis called a local variable—a name that is visible only to code inside the function\ndef and that exists only while the function call runs. In fact, because all names\nassigned in any way inside a function are classified as local variables by default,\nnearly all the names in the def qualify:",
      "content_length": 1970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": null,
      "content": "res is obviously assigned, so it is a local variable.\nArguments are passed by assignment, so seq1 and seq2 are, too.\nThe for loop assigns items to a variable, so the name x is also local.\nAll these local variables appear when the function is called and disappear when\nthe function exits—the return statement at the end of intersect sends back\nthe result object, but the name res goes away. The same goes for argument\nnames in the lambda, though its compression hides its loop variable (sans a\nnested := to export it).\nBecause of this, a function’s variables don’t clash with names elsewhere, but\nalso won’t remember values between calls. Although the object returned by a\nfunction lives on, retaining other state information requires other techniques. To\nfully explore the notion of locals and state, though, we need to move on to the\nscopes coverage of the next chapter.",
      "content_length": 871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter introduced the core ideas behind function definition—the syntax\nand operation of the def and return statements and lambda expression, the\nbehavior of function call expressions, and the notion and benefits of\npolymorphism in Python functions. As we saw, a def and lambda are executable\ncode that create a function object at runtime; when the function is later called,\nobjects are passed into it by assignment (which means object reference in\nPython), and computed values are sent back by return in def and implicitly in\nlambda. We also began exploring the concepts of local variables and scopes, but\nsaved the details for Chapter 17. First, though, a quick quiz.\nTest Your Knowledge: Quiz\n1. What is the point of coding functions?\n2. At what time does Python create a function?\n3. What does a def function return if it has no return statement in it?\n4. When does the code nested inside the function definition run?\n5. What’s wrong with checking the types of objects passed into a function?\nTest Your Knowledge: Answers\n1. Functions are the most basic way of avoiding code redundancy in\nPython—factoring code into functions means that we have only one\ncopy of an operation’s code to update in the future. Functions are also\nthe basic unit of code reuse in Python—wrapping code in functions\nmakes it a reusable tool, callable in a variety of programs. Finally,\nfunctions allow us to divide a complex system into manageable parts,\neach of which may be developed individually. Functions are also used\nfor object methods, but we’re postponing this role until we study\nclasses.",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": null,
      "content": "2. A function is created when Python reaches and runs a def statement,\nwhich creates a function object and assigns it to the function’s name.\nThis normally happens when the enclosing module file is imported by\nanother module (recall that imports run the code in a file from top to\nbottom, including any defs), but it can also occur when a def is typed\ninteractively or nested in other statements, such as ifs. Functions are\nalso created when a lambda is reached and run, though this doesn’t\nnormally happen during an import operation.\n3. A def function returns the None object by default if the control flow\nfalls off the end of the function body without running into a return\nstatement. Such functions are usually called with expression statements,\nas assigning their None results to variables is generally pointless. A\nreturn statement with no expression in it also returns None. A lambda\nimplicitly returns its expression’s result, so it has no default.\n4. The function body (the code nested inside the function def statement or\nfollowing the colon in lambda) is run when the function is later invoked\nwith a call expression. The body runs anew each time the function is\ncalled.\n5. Checking the types of objects passed into a function effectively breaks\nthe function’s flexibility, constraining the function to work on specific\ntypes only. Without such checks, the function would likely be able to\nprocess an entire array of object types—any objects that support the\ninterface expected by the function will work. (The term interface means\nthe set of methods and expression operators the function’s code runs.)\n1  This code will always work if we intersect files’ contents obtained with file.readlines(). It may\nnot work to intersect lines in open input files directly, though, depending on the file object’s\nimplementation of the in operator or general iteration. Files must generally be rewound (e.g., with a\nfile.seek(0) or another open) after they have been read to end-of-file once, and so are single-pass\niterators. As you’ll see in Chapter 30 when we study operator overloading, objects implement the in\noperator either by providing the specific __contains__ method or by supporting the general iteration\nprotocol with the __iter__ or older __getitem__ methods; classes can code these methods\narbitrarily to define what iteration means for their data.",
      "content_length": 2360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": null,
      "content": "Chapter 17. Scopes\nThe preceding chapter introduced basic function definitions and calls. As we\nsaw, Python’s core function model is simple to use, but even simple function\nexamples quickly lead to questions about the meaning of variables in code. This\nchapter moves on to present the details behind Python’s scopes—the places\nwhere variables are defined and looked up. Like module files, scopes help\nprevent same-name clashes across a program’s code: names defined in one\nprogram unit don’t interfere with names in another.\nAs you’ll learn here, the place where a name is assigned in your code is crucial\nto determining what the name means. You’ll also find that scope usage can have\na major impact on program maintenance effort; overuse of globals, for example,\nis a generally bad idea. On the plus side, you’ll also learn that scopes can\nprovide a way to retain state information between function calls, and they offer\nan alternative to classes in some roles.\nPython Scopes Basics\nNow that we’re starting to write our own functions, we need to get more formal\nabout what names mean in Python. When we use a name in a program, Python\ncreates, changes, or looks up the name in what is known as a namespace—a\nplace where names live. And when we talk about names in relation to code,\nnamespaces correspond to scopes: the location of a name’s assignment in your\nsource code determines the scope of the name’s visibility in your program.\nJust about everything related to names, including scope classification, happens at\nassignment time in Python. As we’ve seen, names in Python spring into\nexistence when they are first assigned values, and they must be assigned before\nthey are used. Because names are not declared ahead of time, Python uses the\nlocation of the assignment of a name to associate it with (i.e., bind it to) a\nparticular namespace and scope. In other words, the place where you assign a\nname in your source code determines the namespace it will live in, and hence its\nscope of visibility.",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": null,
      "content": "Besides packaging code for reuse, functions add an extra namespace layer to\nyour programs to minimize the potential for collisions among variables of the\nsame name—by default, all names assigned inside a function are associated with\nthat function’s namespace, and no other. This rule means that:\nNames assigned inside a def can be seen only by the code within that\ndef. You cannot even refer to such names from outside the function.\nNames assigned inside a def do not clash with variables outside the\ndef, even if the same names are used elsewhere. A name X assigned\noutside a given def (i.e., in a different def or at the top level of a\nmodule file) is a completely different variable from a name X assigned\ninside that def.\nNames assigned in a lambda work the same way, though assignment in\ntheir expressions can happen only for arguments and := named\nassignments, and is much less common than in def.\nIn all cases, the scope of a variable (where it can be used) is wholly determined\nby where it is assigned in your source code, and has nothing to do with which\nfunctions call which. In fact, as you’ll learn in this chapter, variables may be\nassigned in three different places, corresponding to three different scopes:\nIf a variable is assigned inside a def or lambda, it is local to that\nfunction.\nIf a variable is assigned in an enclosing def or lambda, it is nonlocal to\nnested functions.\nIf a variable is assigned outside all defs and lambdas, it is global to the\nentire file.\nWe call this lexical scoping because variable scopes are determined entirely by\nthe locations of the variables in the source code of your program files, not by\nfunction calls. Hence, a simple visual inspection is enough to make the call.\nFor example, in the following module file, the X = 99 assignment creates a\nglobal variable named X (visible everywhere in this file), but the X = 88",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": null,
      "content": "assignment creates a local variable X (visible only within the def statement). If\nwe refer to X inside the function, we’ll get the function’s X, not the global:\nX = 99                     # Global (module) scope X\ndef func():\n    X = 88                 # Local (function) scope X: a different variable\nEven though both variables are named X, their scopes make them different. The\nnet effect is that function scopes help to avoid name clashes in your programs\nand help to make functions more self-contained program units—their code need\nnot be concerned with names used elsewhere.\nScopes Overview\nBefore we started writing functions, none of the code we wrote was nested in a\ndef, so the names we used either lived in a module or were built-ins predefined\nby Python like open and zip. This includes code typed at the REPL: the\ninteractive prompt is technically a module named __main__ that prints results\nand doesn’t correspond to a real file; in all other ways, though, it’s like the top\nlevel of a module file.\nFunctions, though, provide nested namespaces (scopes) that localize the names\nthey use, such that names inside a function won’t clash with those outside it (in a\nmodule or another function). Specifically, functions define a local scope and\nmodules define a global scope with the following properties:\nThe enclosing module is a global scope. Each module is a global\nscope—that is, a namespace in which variables created by assignment at\nthe top level of the module file live. Global variables become attributes\nof a module object to the outside world after imports but can also be\nused as simple variables within the module file itself.\nThe global scope spans a single file only. Don’t be fooled by the word\n“global” here—names at the top level of a file are global to code within\nthat single file only. There is no notion of an all-encompassing global\nscope that corresponds to user code in Python (built-ins are truly global,\nbut not user defined). Instead, names are partitioned into modules, and",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": null,
      "content": "you must always import a module explicitly if you want to be able to\nuse the names its file defines. When you hear “global” in Python, think\n“module.”\nAssigned names are local unless declared global or nonlocal. By\ndefault, all the names assigned inside a function definition are put in its\nlocal scope—the namespace associated with the function itself. To\nassign a name that lives at the top level of the module enclosing a\nfunction, declare it in a global statement inside the function. To assign\na name that lives in an enclosing def instead, declare it in a nonlocal\nstatement. Because lambda bodies are limited to expressions, these two\nstatements are just for def.\nAll other names are enclosing function locals, globals, or built-ins.\nNames not assigned a value in the function definition are assumed to be\neither locals defined in a physically enclosing function, globals that live\nin the enclosing module’s namespace, or built-ins in the predefined\nbuilt-ins module that Python provides. Enclosing scopes can arise for\nany def and lambda combo, though def can’t be coded in lambda.\nEach call to a function creates a new local scope. Every time you call\na function, you create a new local scope—that is, a namespace in which\nthe names created inside that function will live, barring global or\nnonlocal statements. You can think of each def statement and lambda\nexpression as defining a new local scope, but the local scope actually\ncorresponds to a function call, for reasons the next bullet explains.\nPer-call scopes matter in recursion and closures. It’s crucial that each\nactive call receive its own copy of the function’s local variables when\nusing recursion, an advanced technique that allows functions to loop by\ncalling themselves, and noted briefly in Chapter 9 for comparisons.\nRecursion is useful in functions we write as well, to process structures\nwhose shapes can’t be predicted ahead of time; we’ll explore it more\nfully in Chapter 19. Per-call scopes are also required for the closure\nfunctions covered ahead here: each call gets a fresh packet of the\nfunction’s locals, which can remember state-information objects to be\nused later.",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 615,
      "chapter": null,
      "content": "Beyond those basics, there are three subtleties worth underscoring. First, as\nnoted, code typed at the Python interactive prompt lives in a module, too, and\nfollows the normal scope rules: names assigned there are global variables,\naccessible to the entire interactive session. You’ll learn more about modules in\nthe next part of this book.\nSecond, also bear in mind that any type of assignment classifies a name as local\nor global, based on where the assignment appears. This includes = statements\nand := expressions; module names in import; function and argument names in\ndef; names in class covered later; and so on. If you assign a name in any way\nwithin a function, it will be local to that function by default. This includes its\narguments listed in its header, but also names in its body’s code.\nThird, in-place changes to objects do not classify names as locals; only actual\nname assignments do. For instance, if the name L is assigned to a list at the top\nlevel of a module, a statement L = X within a function will classify L as a local,\nbut L.append(X) will not. In the latter case, we are changing the list object that\nL references, not L itself—hence, L is found in the global scope as usual, and\nPython happily modifies it without requiring a global declaration. As usual, it\nhelps to keep the distinction between names and objects clear: changing an\nobject is not an assignment to a name.\nName Resolution: The LEGB Rule\nIf the prior section sounds complicated, it really boils down to three simple rules.\nWithin a def statement:\nName assignments create or change local names by default.\nName references search at most four scopes: local, then enclosing\nfunctions (if any), then global, then built-in.\nNames declared in global and nonlocal statements map assigned\nnames to enclosing module and function scopes, respectively.\nThe same goes for lambda, except for the last bullet: it doesn’t support\nstatements.",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": null,
      "content": "In other words, all names assigned inside a function def statement or lambda\nexpression are locals by default. Functions can freely use names assigned in\nsyntactically enclosing functions and the global scope, but they must declare\nsuch nonlocals and globals in order to change them.\nPython’s name-resolution scheme is usually called the LEGB rule, after the\nnames of the scopes it searches:\nWhen you use an unqualified name (not after a “.”) inside a function,\nPython searches up to four scopes—the local (L) scope of the function\nitself, then the local scopes of any enclosing (E) defs and lambdas, then\nthe global (G) scope of the surrounding module, and finally the built-in\n(B) scope—and stops at the first place the name is found. If the name is\nnot found during this search, Python reports an error.\nWhen you assign a name inside a function (instead of just referring to it\nin an expression), Python always creates or changes the name in the\nassigner’s local scope, unless it’s declared to be global or nonlocal\nthere.\nWhen you assign a name outside any function (i.e., at the top level of a\nmodule file, or at the interactive prompt), the local and global scope are\none and the same—the module’s namespace.\nBecause names must be assigned before they can be used (as we saw in\nChapter 6), there are no automatic components in this model: assignments\nalways determine name scopes unambiguously. Figure 17-1 illustrates Python’s\nfour scopes and LEGB rule. Note that the second scope lookup layer, E—the\nscopes of enclosing defs or lambdas—can technically correspond to more than\none lookup level. This layer only comes into play when you nest functions\nwithin functions for reasons you’ll meet ahead, and is enhanced by the nonlocal\nstatement.1",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 617,
      "chapter": null,
      "content": "Figure 17-1. The LEGB scope lookup rule for names\nAlso keep in mind that these rules apply only to simple variable names (e.g.,\nname). In Parts V and VI, you’ll see that qualified attribute names (e.g.,\nobject.name) live in particular objects and follow a completely different set of\nlookup rules than those covered here. References to attribute names following\nperiods (.) search one or more objects, not scopes, and in fact may invoke\nsomething called inheritance in Python’s OOP model; more on this in Part VI.\nPreview: Other Python scopes\nThough obscure at this point in the book, there are technically three more scopes\nin Python—temporary loop variables in comprehensions, exception reference\nvariables in try handlers, and local scopes in class statements. The first two of\nthese are special cases that rarely impact real code, and the third falls under the\nLEGB umbrella rule.\nImportantly, most statement blocks and other constructs do not localize the\nnames used within them. This includes loop variables in for loop statements,\nand the targets of := named assignment, which works the same as all others with\nregard to scopes. There are, however, some boundary cases whose variables are\nnot available to (and do not clash with) surrounding code, and which involve\ntopics covered in full elsewhere in this book:\nComprehension loop variables: The variable (or variables) X in [X for\nX in I] used to refer to the current iteration item in comprehension\nexpressions. Because they might clash with other names and reflect",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": null,
      "content": "internal state in generators, such names are local to the expression itself\nin all comprehension forms: list, dictionary, set, and generator. By\ncontrast, for loop statements never localize their loop variable (e.g., X\nin for X in I:) to the statement block as noted, despite the similarity.\nMoreover, names assigned by := within a comprehension do leak out to\nthe enclosing scope, per the info and demos in Chapter 20.\nException variables: The variable X in except E as X used to\nreference the raised exception in a try statement handler. Because this\nmight defer garbage collection’s memory recovery, such variables are\nlocal to that except block, and in fact are removed from the containing\nscope when the block is exited (even if you’ve used them earlier in your\ncode!). You’ll learn all about try statements in Chapter 34.\nNamed assignments in lambda: The variable X in X := Y used in\nassignment expressions. In terms of scopes, the := works the same as an\nunnested = assignment statement in general. Keep in mind, though, that\na lambda function expression creates a new local function scope just\nlike a def statement. Hence, any names assigned by a := nested in a\nlambda are local variables that can be used within the body of the\nlambda itself, but are not available to code outside the lambda\nexpression. That said, this is likely a rare coding pattern in most code\n(and pushes the envelope on complexity). For a refresher on :=, see\nChapter 11.\nMost of these contexts augment the LEGB rule, rather than modifying it. Loop\nvariables assigned in a comprehension, for example, are simply bound to a\nfurther nested and special-case local scope; all other names referenced within\nthese expressions follow the usual LEGB lookup rules.\nIt’s also worth noting that the class statement you’ll meet in Part VI creates a\nnew local scope, too, for the names assigned inside the top level of its block. As\nfor def, names assigned inside a class don’t clash with names elsewhere, and\nfollow the LEGB lookup rule, where the class block is the L level. As for\nimported modules, these class-local names also morph into class object attributes\nafter the class statements ends. For all practical purposes, classes are a local",
      "content_length": 2215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": null,
      "content": "scope (despite some rare border cases that likely qualify as glitches, and can be\nsafely omitted here).\nUnlike functions, though, class names are not created per call: class object calls\ngenerate instances, which inherit names assigned in the class and record per-\nobject state as per-instance attributes of their own. As you’ll also learn in\nChapter 29, although the LEGB rule is used to resolve names used in both the\ntop level of a class itself as well as the top level of method functions nested\nwithin it, classes themselves are skipped by scope lookups—their names must be\nfetched as object attributes. Hence, because Python searches enclosing functions\nfor referenced names, but not enclosing classes, the LEGB rule sketched in\nFigure 17-1 still applies to OOP code.\nScopes Examples\nBut enough theory! Let’s step through a live example that demos scope ideas.\nSuppose we wrote the code in Example 17-1 and saved it in a module file named\nscopes101.py.\nExample 17-1. scopes101.py\n# Global scope\nX = 99                # X and func assigned in module: global\ndef func(Y):          # Y and Z assigned in function: locals\n   # Local scope\n   Z = X + Y         # X is a global when referenced here\n   return Z\nfunc(1)               # func in module: result=100 (not printed)\nTo see this example’s result, import and call its function; the file’s global func is\na module attribute to importers (be sure to run this in the same folder as the file\nif it matters for imports in your REPL, as introduced in Chapter 3):\n>>> import scopes101\n>>> scopes101.func(1)\n100\nThis module and the function it contains use a number of names to do their\nbusiness. Applying Python’s scope rules, we can classify these names as follows:",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": null,
      "content": "Global names: X, func\nX is global because it’s assigned at the top level of the module file; it can be\nreferenced inside the function as a simple unqualified variable without being\ndeclared global. func is global for the same reason; the def statement makes\na function object and assigns it to the name func at the top level of the\nmodule.\nLocal names: Y, Z\nY and Z are local to the function (and exist only while the function runs)\nbecause they are both assigned values in the function definition: Z by virtue\nof the = statement, and Y because arguments are always passed by\nassignment. Hence both are assigned during a function call, and both are\nlocals.\nThe underlying rationale for this name-segregation scheme is that local variables\nserve as temporary names that you need only while a function is running. For\ninstance, in the preceding example, the argument Y and the addition result Z exist\nonly inside the function; these names don’t interfere with the enclosing module’s\nnamespace—or any other function, for that matter. In fact, local variables are\nremoved from memory when the function call exits, and objects they reference\nmay be garbage-collected if not referenced elsewhere. This is an automatic\ninternal step, but it helps minimize memory requirements.\nThe local/global distinction also makes functions easier to understand, as most of\nthe names a function uses appear in the function itself, not at an arbitrarily\ndistant place in a module file. Also, because you can be sure that local names\nwill not be changed by some remote function in your program, they tend to make\nprograms easier to debug and modify. In the following sort of code, for instance,\neach function’s variable cannot be changed—or even seen—anywhere else:\ndef func1():\n    X = 'hack'       # This X...",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 621,
      "chapter": null,
      "content": "def func2():\n    X = 'code'       # ...is not the same as this X\nThe net effect helps make functions self-contained units of software, by design.\nThe Built-in Scope\nWe’ve been talking about the built-in scope in the abstract, but it’s a bit simpler\nthan you may think. Really, the built-in scope is just a built-in module called\nbuiltins, but you have to import builtins to query built-ins because the name\nbuiltins is not itself a built-in...\nYes, seriously! The built-in scope is implemented as a standard-library module\nnamed builtins, but that name itself is not placed in the built-in scope, so you\nhave to import it in order to inspect it. Once you do, you can run a dir call to see\nwhich names are predefined (run this on your own for full fidelity: it can vary\nacross Python versions):\n>>> import builtins\n>>> dir(builtins)\n['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', \n'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError',\n…many more names omitted: 158 total in 3.12…\n'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', \n'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', \n'super', 'tuple', 'type', 'vars', 'zip']\nThe names in this list constitute the built-in scope in Python. Roughly the first\nhalf are built-in exceptions, and the second half are built-in functions. Also in\nthis list are the special names None, True, False, and Ellipsis we met earlier.\nBecause Python automatically searches this module last in its LEGB lookup (it’s\nB), you get all the names in this list “for free.” That is, you can use them without\nimporting any modules. Thus, there are really two ways to refer to a built-in\nfunction—by taking advantage of the LEGB rule, or by manually importing the\nbuiltins module:\n>>> zip                        # The normal way, per LEGB's B\n<class 'zip'>",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 622,
      "chapter": null,
      "content": ">>> import builtins            # The hard way: for customizations\n>>> builtins.zip\n<class 'zip'>\n>>> zip is builtins.zip        # Same object, different lookup routes\nTrue\nThe second of these approaches, though more to type, can be useful in advanced\nroles you’ll meet in the sidebar “Why You Will Care: Customizing open”.\nRedefining built-in names: For better or worse\nCareful readers might notice that because the LEGB lookup procedure takes the\nfirst occurrence of a name that it finds, names in the local scope may override\nvariables of the same name in both the global and built-in scopes, and global\nnames may override built-ins. A function can, for instance, create a local\nvariable called open by assigning to it:\ndef hider():\n    open = 'text'              # Local variable, hides built-in here (L before B)\n    …\n    open('data.txt')           # Error: this no longer opens files in this scope!\nHowever, this will hide the built-in function called open that lives in the built-in\n(outer) scope, such that the name open will no longer work within the function\nto open files—it’s now a string, not the opener function. This is sometimes\ncalled “shadowing” a built-in. This isn’t a problem if you don’t need to open\nfiles in this function, but triggers an error if you attempt to open through this\nname in this scope; your open is no longer the built-in open.\nThis can even occur more simply at the interactive prompt, which works as a\nglobal, implicit-module scope:\n>>> open = 99                  # Assign in global scope, hides built-in here too\nThat being said, there is nothing inherently wrong with using a built-in name for\nvariables of your own, as long as you don’t need the original built-in version.\nAfter all, if these were truly off limits, we would need to memorize the entire\nbuilt-in names list and treat all its names as reserved. With some 150 usable\nnames in this module in Python 3.12, that would be far too restrictive and",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 623,
      "chapter": null,
      "content": "daunting:\n>>> len(dir(builtins)), len([x for x in dir(builtins) if not x.startswith('__')])\n(158, 150)\nIn fact, there are times in advanced programming where you may really want to\nreplace a built-in name by redefining it in your code—to define a custom open\nthat augments access attempts, for instance (again, more in the sidebar at the end\nof the chapter). Exception: the special names None, True, and False in the built-\nin scope are also treated as reserved words today, so they can no longer be\nreassigned (fun though it once was!). All other built-in names, though, are fair\ngame.\nNevertheless, redefining a built-in name is often a bug, and a nasty one at that,\nbecause Python will not issue a warning message about it. You may find tools on\nthe web that can warn you of such mistakes, but knowledge may be your best\ndefense on this point: don’t redefine a built-in name you need. If you\naccidentally reassign a built-in name at the interactive prompt this way, though,\nyou can either restart your session or run a del name statement to remove the\nredefinition from your scope, thereby restoring the original in the built-in scope\nper LEGB.\nNote that functions can similarly hide global variables of the same name with\nlocals, but this is more broadly useful, and in fact is much of the point of local\nscopes—because they minimize the potential for name clashes, your functions\nare self-contained namespaces:\nX = 88                         # Global X\ndef func():\n    X = 99                     # Local X: hides global, but we want this here\nfunc()\nprint(X)                       # Prints 88: unchanged\nHere, the assignment within the function creates a local X that is a completely\ndifferent variable from the global X in the module outside the function. As a\nconsequence, though, there is no way to change a name outside a function\nwithout adding a global or nonlocal declaration to the def; the next section",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": null,
      "content": "takes up the former.\nNOTE\nMore built-in tongue twisters: Technically, the name __builtins__ is preset in most global\nscopes, including the interactive session, to reference the module known as builtins, so you\ncan often use __builtins__ without an import, but cannot run an import on the name\n__builtins__ itself—it’s a preset variable, not a module’s name. Thus, builtins is\n__builtins__ is True after you import builtins. The upshot is that we can often inspect the\nbuilt-in scope by simply running dir(__builtins__) sans imports, but are advised to use\nbuiltins for real work and customization. Who said documenting this stuff was easy?\nThe global Statement\nThe global statement and its nonlocal cousin are the only declaration\nstatements in Python that are actually used by Python (for contrast, see\nChapter 6’s discussion of unused type hinting). They are not type or size\ndeclarations, though; they are namespace declarations. The global statement,\nfor instance, tells Python that a function plans to change one or more global\nnames—that is, names that live in the enclosing module’s scope (namespace).\nWe’ve talked about global in passing already. Here’s a summary:\nGlobal names are variables assigned at the top level of the enclosing\nmodule file.\nGlobal names must be declared only if they are assigned within a\nfunction.\nGlobal names may be referenced within a function without being\ndeclared, per the LEGB rule.\nIn other words, global allows us to change names that live outside a def at the\ntop level of a module file. As you’ll see later, the nonlocal statement is almost\nidentical but applies to names in an enclosing def’s local scope, rather than\nnames in the enclosing module.\nThe global statement, usable in def but not lambda, simply consists of the",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 625,
      "chapter": null,
      "content": "reserved word global, followed by one or more names separated by commas.\nAll the listed names will be mapped to the enclosing module’s scope when\nassigned or referenced within the function body. For instance, the following is a\ntakeoff on the preceding example:\nX = 88                         # Global X\ndef func():\n    global X\n    X = 99                     # Global X: outside def\nfunc()\nprint(X)                       # Prints 99\nWe’ve added a global declaration to the example here, such that the X inside the\ndef now refers to the X outside the def; they are the same variable this time, so\nchanging X inside the function changes the X outside it. Here is a slightly more\ninvolved example of global at work:\ny, z = 1, 2                    # Global variables in module\ndef all_global():\n    global x                   # Declare globals assigned\n    x = y + z                  # No need to declare y or z: LEGB rule\nHere, x, y, and z are all globals inside the function all_global. Names y and z\nare global because they aren’t assigned in the function, and x is global because it\nwas listed in a global statement to map it to the module’s scope explicitly.\nWithout the global here, x would be considered local by virtue of the\nassignment.\nNotice that y and z are not declared global; Python’s LEGB lookup rule finds\nthem in the module (G) automatically. Also notice that x does not even exist in\nthe enclosing module before the function runs; in this case, the first assignment\nin the function creates x in the module. All of which works when needed, but\nyou really should try to avoid using globals like this whenever possible—as the\nnext section will explain.\nProgram Design: Minimize Global Variables",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": null,
      "content": "Functions in general, and global variables in particular, raise some larger design\nquestions. How, for example, should functions communicate? Although some\nanswers will become more apparent when you begin writing larger functions of\nyour own, a few guidelines up front might spare you from problems later. In\ngeneral, functions should rely on arguments and return values instead of globals,\nbut this may not at all be obvious to newcomers to programming.\nBy default, names assigned in functions are locals, so if you want to change\nnames outside functions you have to write extra code (e.g., global statements).\nThis is deliberate—you have to say more to do the potentially “wrong” thing.\nAlthough there are times when globals are useful (and even required), variables\nassigned in a def are local by default because that is normally the best policy.\nChanging globals can lead to well-known software-engineering problems:\nbecause the variables’ values are dependent on the order of calls to arbitrarily\ndistant functions, programs can become difficult to debug, or understand at all.\nConsider this module file, for example, which is presumably imported and used\nelsewhere:\nX = 99\ndef func1():\n    global X              # Change global X when called\n    X = 88\ndef func2():\n    global X              # But so does this, and when?\n    X = 77\nNow, imagine that it is your job to modify or reuse this code. What will the value\nof X be here? Really, that question has no meaning unless it’s qualified with a\npoint of reference in time—the value of X is timing-dependent, as it depends on\nwhich function was called last (something we can’t tell from this file alone).\nThe net effect is that to understand this code, you have to trace the flow of\ncontrol through the entire program. Hence, if you need to reuse or modify the\ncode, you have to keep the entire program in your head all at once. In fact, you\ncan’t really use one of these functions without bringing along the other. They are\ndependent on—that is, coupled with—the global variable. And that is the\nproblem with globals: they generally make code more difficult to understand and",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": null,
      "content": "reuse than code consisting of self-contained functions that rely on locals.\nOn the other hand, short of using tools like the nested scope closures covered\nahead or OOP with classes covered later, global variables are the most basic way\nto retain shared state information—information that a function needs to\nremember for use the next time it is called. Local variables disappear when the\nfunction returns, but globals do not. As you’ll see later, other techniques can\nachieve this, too, and allow for multiple copies of the retained information; but\nthey are more complex than pushing values out to the global scope for retention\nin simple cases where this applies.\nMoreover, some programs designate a single module to collect shared globals; as\nlong as this is expected, it is not as harmful. Programs that use multithreading for\nparallel processing also commonly depend on global variables—they become\nshared memory between functions running in parallel threads, and so act as a\ncommunication device.2\nFor now, though, and especially if you are relatively new to programming, avoid\nthe temptation to use globals whenever you can—they tend to make programs\ndifficult to understand and reuse, and won’t work for cases where one copy of\nsaved data is not enough. That’s why they are not the default in Python. Try to\ncommunicate with passed-in arguments and return values instead. Six months\nfrom now, both you and your coworkers may be glad you did.\nProgram Design: Minimize Cross-File Changes\nWhile we’re on the subject of globals, here’s another related design note:\nalthough we can change global variables in another file directly, we usually\nshouldn’t. Module files were introduced in Chapter 3 and are covered in depth in\nthe next part of this book, but their basics are simple. To demo their relationship\nto scopes, consider these two module files:\n# first.py\nX = 99                    # This code doesn't know about second.py\n# second.py\nimport first\nprint(first.X)            # OK: references a name in another file\nfirst.X = 88              # But changing it can be too subtle and implicit",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": null,
      "content": "The first defines a variable X, which the second prints and then changes by\nassignment. Notice that we must import the first module into the second to get to\nits variable at all—as we’ve learned, each module is a self-contained namespace\n(package of variables), and we must import one module to see inside it from\nanother. That’s the main purpose of modules: by segregating variables on a per-\nfile basis, they avoid name collisions across files, in much the same way that\nlocal variables avoid name clashes across functions.\nReally, though, in terms of this chapter’s topic, the global scope of a module file\nbecomes the attribute namespace of the module object once it is imported—\nimporters automatically have access to all of the file’s global variables, because\na file’s global scope morphs into an object’s attribute namespace when it is\nimported.\nAfter importing the first module, the second module prints its variable and then\nassigns it a new value. Referencing the module’s variable to print it is fine—this\nis how modules are linked together into a larger system normally. The problem\nwith the assignment to first.X, however, is that it is far too implicit: whoever’s\ncharged with maintaining or reusing the first module probably has no clue that\nsome arbitrarily far-removed module on the import chain can change X at\nruntime. In fact, the second module may be in a different folder, and so difficult\nto notice at all.\nAlthough such cross-file variable changes are always possible in Python, they\nare usually much more subtle than you will want. Again, this sets up too strong a\ncoupling between two components—because the files are both dependent on the\nvalue of the variable X, it’s difficult to understand or reuse one file without the\nother. Such implicit cross-file dependencies can lead to inflexible code at best,\nand surprising bugs at worst.\nHere again, and generally speaking, don’t do that—the better way to\ncommunicate across file boundaries is to call functions, passing in arguments\nand getting back return values. In this specific case, we would probably be better\noff coding an accessor function to manage the change:\n# first.py\nX = 99\ndef setX(new):            # Accessors make external changes explicit",
      "content_length": 2232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": null,
      "content": "global X              # And can manage access in a single place\n    X = new\n# second.py\nimport first\nfirst.setX(88)            # Call the function instead of changing directly\nThis requires more code and may seem like a trivial change, but it makes a huge\ndifference in terms of readability and maintainability—when a person reading\nthe first module by itself sees a function, that person will know that it is a point\nof interface and will expect the change to the X. In other words, it removes the\nelement of surprise that is rarely a good thing in software projects. Although we\ncannot prevent cross-file changes from happening (sans obscure hacks that we’ll\nomit here), common sense dictates that they should be minimized unless widely\nknown across the program.\nOther Ways to Access Globals\nInterestingly, because global-scope variables morph into the attributes of a\nloaded module object, we can emulate the global statement by importing the\nenclosing module and assigning to its attributes, as in the module file in\nExample 17-2. Code in this file accesses its enclosing module, first by importing\nit, and then by indexing the sys.modules dictionary that records all loaded\nmodules and is used in advanced roles (there’s more on this dictionary in\nPart V).\nExample 17-2. thismod.py\n\"Change a global three ways\"\nvar = 99                          # Global variable == module attribute\ndef local():\n   var = 0                       # Change local var\ndef glob1():\n   global var                    # Declare global (normal)\n   var += 1                      # Change global var\ndef glob2():\n   var = 0                       # Change local var\n   import thismod                # Import myself\n   thismod.var += 1              # Change global var",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": null,
      "content": "def glob3():\n   var = 0                              # Change local var\n   import sys                           # Import system table\n   thismod = sys.modules['thismod']     # Get module object (or use __name__)\n   thismod.var += 1                     # Change global var\ndef test():\n   print(var)\n   local(); glob1(); glob2(); glob3()\n   print(var)\nWhen we import and call this module’s test to invoke its other functions, this\nadds 3 to the global variable var—only its first function, local, does not impact\nthe global:\n>>> import thismod \n>>> thismod.test()\n99\n102\n>>> thismod.var\n102\nAll these global-access techniques work, and illustrate the equivalence of the\nglobal scope to module attributes, but they’re noticeably more work than using\nthe global statement to make your intentions explicit.\nAs we’ve seen, global allows us to easily change names in a module outside a\nfunction. It has a close relative named nonlocal that can be used to change\nnames in enclosing functions, too—but to understand how that can be useful, we\nfirst need to explore function nesting in general, the topic we turn to next.\nNested Functions and Scopes\nSo far, this chapter has largely omitted one part of Python’s scope rules on\npurpose, because it’s relatively uncommon to encounter it in practice. However,\nit’s time to take a deeper look at layer E in the LEGB lookup rule. This layer was\nadded during Python 2.X’s reign. It takes the form of the local scopes of any and\nall enclosing function’s local scopes. Enclosing scopes are sometimes also called\nstatically nested scopes. Really, the nesting is a lexical one—nested scopes\ncorrespond to physically and syntactically nested code structures in your",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": null,
      "content": "program’s source code text.\nNested Scopes Overview\nWith the addition of nested function scopes, variable lookup rules become\nslightly more complex. Within a function:\nA name reference (X) looks for the name X first in the current local\nscope (function); then in the local scopes of any lexically enclosing\nfunctions in your source code, from inner to outer; then in the current\nglobal scope (the module file); and finally in the built-in scope (which\nwe’ve seen means the built-in module builtins). global declarations\nin a def make the search begin in the global (module file) scope instead.\nA name assignment (e.g., X = value) creates or changes the name X in\nthe current local scope, by default. If X is declared global within the\nfunction, the assignment creates or changes the name X in the enclosing\nmodule’s scope instead. If, on the other hand, X is declared nonlocal\nwithin a def function, the assignment changes the name X in the local\nscope of the closest enclosing function that assigns the same name.\nNotice that the global declaration still maps variables to the enclosing module.\nWhen nested functions are present, variables in enclosing functions may be\nreferenced, but require nonlocal declarations to be changed. Also note that,\nunlike global, nonlocal does not create a name in an enclosing def if it’s not\nassigned there; nonlocal makes sense only if the variables it names are also\nassigned by an enclosing def—and is an error to use otherwise.\nThis section is primarily concerned with the first bullet in the preceding list:\nnested scope references. We’ll explore nested assignments and nonlocal in a\nmoment, but first we need to grok function nesting in general.\nNested Scopes Examples\nLet’s turn to code to illustrate some of the preceding points. Here is what an\nenclosing function scope looks like (type this into a script file or at the\ninteractive prompt to run it live):",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": null,
      "content": "X = 99                   # Global scope name: not used\ndef f1():\n    X = 88               # Enclosing def local\n    def f2():\n        print(X + 1)     # Reference made in nested def\n    f2()\nf1()                     # Prints 89: enclosing def local + 1\nFirst off, this is legal Python code: the def is simply an executable statement,\nwhich can appear anywhere any other statement can—including nested in\nanother def. Here, the nested def runs while a call to the function f1 is running;\nit makes a function and assigns it to the name f2, a local variable within f1’s\nlocal scope. In a sense, f2 is a temporary function that lives only during the\nexecution of (and is visible only to code in) the enclosing f1. As such, its name\nwon’t clash with any other part of the program—which is one reason to nest it\nthis way.\nBut notice what happens inside f2: when it uses the variable X, it refers to the X\nthat lives in the enclosing f1 function’s local scope. Because functions can\naccess names in all physically enclosing functions, the X in f2 is automatically\nmapped to the X in f1, by the LEGB lookup rule’s level E.\nIn fact, this enclosing scope lookup works even if the enclosing function’s call\nhas already ended. For example, the following code defines a function that\nmakes and returns another function, and introduces a common nesting role:\ndef f1():\n    X = 88\n    def f2():\n        print(X + 1)     # Remembers X in enclosing def scope\n    return f2            # Return f2 but don't call it\naction = f1()            # Make, return function\naction()                 # Call it now: prints 89\nIn this code, the call to action is really running the function we named f2 when\nf1 ran. This works naturally because functions are objects in Python like\neverything else and can be passed back as return values from other functions.",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 633,
      "chapter": null,
      "content": "More subtly, f2 “remembers” the enclosing scope’s X in f1—even though f1 is\nno longer active when f2 is run. Though abstract here, this memory behavior\nturns out to be one of the main reasons to nest functions, and warrants a closer\nlook in the next section.\nClosures and Factory Functions\nDepending on whom you ask, the preceding example’s behavior is sometimes\ncalled a closure or a factory function—the former describing a functional\nprogramming technique, and the latter denoting a design pattern. In code, a\nfactory function creates and returns a function that has a stateful closure, but the\ntwo terms are intertwined.\nWhatever the label, the function object in question remembers values in\nenclosing scopes regardless of whether those scopes are still active for a call. In\neffect, it has attached packets of memory (a.k.a. state retention), which are made\nanew for each copy of the nested function created, and often provide a simple\nalternative to classes in this role.\nClosures (a.k.a. factory functions) are sometimes used by programs that need to\ngenerate event handlers on the fly in response to conditions at runtime. For\ninstance, imagine a GUI that must define actions according to user inputs that\ncannot be anticipated when the GUI is built and vary per action. In such cases,\nwe need a function that creates and returns another function, with information\n(i.e., state) that differs per function made.\nTo demo this live in simplified terms, consider the following function, typed at\nthe interactive prompt:\n>>> def maker(N):\n        def action(X):                # Make and return action\n            return X ** N             # action retains N from enclosing scope\n        return action\nThis defines an outer function that simply makes and returns a nested function,\nwithout calling it—maker makes action, but simply returns action without\nrunning it. If we then call the outer function:\n>>> f = maker(2)                      # Pass 2 to argument N",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 634,
      "chapter": null,
      "content": ">>> f\n<function maker.<locals>.action at 0x101ae7ba0>\nWhat we get back is a reference to the nested function built—the one created and\nassigned to action when the nested def runs. If we now call what we got back\nfrom the outer function:\n>>> f(3)                              # Pass 3 to action's X\n9                                     # And maker's N remembers 2: 3 ** 2\n>>> f(4)                              # Pass 4 to X, N is still 2: 4 ** 2\n16\nWe invoke the nested function—the one called action within maker. In other\nwords, we’re calling the nested function that maker created and passed back.\nPerhaps the most unusual part of this, though, is that the nested function\nremembers integer 2, the value of the variable N in maker, even though maker\nhas returned and exited by the time we call action. In effect, N from the\nenclosing local scope is retained as state information attached to the generated\naction, which is why we get back its argument squared whenever it is later\ncalled.\nJust as important, if we now call the outer function again, we get back a new\nnested function with different state information attached. In the following, we\ncompute the argument cubed instead of squared when calling the new function,\nbut the original still squares as before:\n>>> g = maker(3)                      # g remembers 3, f remembers 2\n>>> g(4)                              # 4 ** 3\n64\n>>> f(4)                              # 4 ** 2\n16\nThis works because each call to a closure/factory function like this gets its own\nset of state information. In our demo, the function we assign to name g\nremembers 3, and f remembers 2, because each has its own state information\nretained by the variable N in maker. In a GUI, N might be a username, email\naddress, or other per-function state.\nThis is all literal inside Python. Nested functions have an attached closure",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 635,
      "chapter": null,
      "content": "storage area for the enclosing scope names they use. It’s available as the\n__closure__ attribute of such functions (and fetching cell_contents after\nindexing this tuple yields a state item), but we don’t need to understand Python\ninternals to use closures in our code.\nThis is also a somewhat advanced technique that you may not see commonly in\nmost code, and may be more popular among programmers with backgrounds in\nfunctional programming languages. On the other hand, enclosing scopes are\noften employed by the lambda function-creation expressions introduced in the\nprior chapter—because lambda is an expression, it is almost always nested in a\ndef. For example, a lambda can serve in place of a def in our demo, and also\nrelies on enclosing scope references to retain state—like N in the following\nvariation:\n>>> def maker(N):\n        return lambda X: X * N        # lambda functions retain state too\n>>> h, i = maker(2), maker(3)         # Make two closure functions\n>>> h('Py'), i('Py')                  # Run lambdas: ('Py' * 2) and ('Py' * 3)\n('PyPy', 'PyPyPy')\nFor a more tangible example of closures at work, see the sidebar “Why You Will\nCare: Customizing open”. It uses similar techniques to store information for later\nuse in an enclosing scope. As you’ll also see after the next section’s closer,\nclosures become more useful when their state becomes changeable with\nnonlocal.\nArbitrary Scope Nesting\nFinally, this tour of nested function scopes would be remiss if it didn’t point out\nthat scopes may nest arbitrarily, though only enclosing functions (not classes,\ndescribed in Part VI) are searched when names are referenced:\n>>> def f1():\n        x = 99\n        def f2():\n            def f3():\n                print(x)        # Found in f1's local scope!\n            f3()",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 636,
      "chapter": null,
      "content": "f2()\n>>> f1()\n99\nThis does work: Python will search the local scopes of all enclosing defs, from\ninner to outer, after the referencing function’s own local scope and before the\nmodule’s global scope or built-ins. However, this sort of code is even less likely\nto crop up in practice. Per coding aphorism, flat is better than nested, and this\nstill holds generally true even with nested scope closures in the toolbox. Except\nin limited contexts, your life (and the lives of your fellow travelers in the\nsoftware realm) will generally be better if you minimize nesting in function\ndefinitions.\nThe nonlocal Statement\nIn the prior section, we saw how nested functions can reference variables in an\nenclosing function’s scope, even if that function has already returned. As\nsuggested earlier, we can also change such enclosing scope variables, as long as\nwe declare them in nonlocal statements. With this statement, nested functions\ngain both read and write access to names in enclosing functions. This makes\nnested scope closures more useful, by making state changeable.\nThe nonlocal statement, usable in def but not lambda, is similar in both form\nand role to global, covered earlier. Like global, nonlocal declares that a name\n(or names) will be changed in an enclosing scope. Unlike global, though,\nnonlocal applies to a name in an enclosing function’s scope, not to the global\nmodule scope outside all functions. Also unlike global, nonlocal names must\nexist in an enclosing function’s scope—they are mapped only to enclosing\nfunctions and cannot be created by a first assignment in a nested def that uses\nnonlocal.\nIn other words, nonlocal both allows assignment to names in enclosing function\nscopes, and limits scope lookups for such names to enclosing functions. The net\neffect is a direct and reliable implementation of changeable state information, for\ncontexts that do not desire or need to use classes or other stateful tools.",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 637,
      "chapter": null,
      "content": "nonlocal Basics\nThe nonlocal statement has meaning only inside a function, and is an error to\nuse elsewhere. More specifically, it applies and is usable only when def is nested\nin another def: because the body of a lambda allows just an expression, it\nsupports neither nonlocal nor nested def statements.\nWhen used within a def, the nonlocal statement seals the fate of references—\nmuch like the global statement, nonlocal causes searches for the names listed\nin the statement to begin in the enclosing defs’ scopes, not in the local scope of\nthe declaring function. That is, nonlocal also means “skip my local scope\nentirely.” In fact, the names listed in a nonlocal must be assigned in an\nenclosing def, and never refer to names in the global or built-in scopes.\nImportantly, the addition of nonlocal does not alter name reference scope rules\nin general; they still work as before, per the LEGB rule described earlier. The\nnonlocal statement simply serves to allow names in enclosing scopes to be\nchanged rather than just referenced. The global and nonlocal statements,\nthough, tighten up and even restrict the lookup rules within a function for the\nnames that they list:\nglobal makes scope lookup begin in the enclosing module’s scope and\nallows names there to be assigned. Scope lookup continues on to the\nbuilt-in scope if the name does not exist in the module, but assignments\nto global names always create or change them in the module’s scope.\nnonlocal restricts scope lookup to just enclosing defs, requires that the\nnames exist there, and allows them to be reassigned. Scope lookup does\nnot continue on to the global or built-in scopes.\nnonlocal in Action\nLet’s move on to examples to make this more concrete. Simple references to\nenclosing def scopes work as we’ve already seen—in the following, outer\nbuilds and returns the function inner to be called later, and the state reference\nin inner maps the local scope of outer using the normal scope LEGB lookup\nrules:",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 638,
      "chapter": null,
      "content": ">>> def outer(start):\n        state = start             # Referencing nonlocals works normally\n        def inner(label):\n            print(label, state)   # Remembers state in enclosing scope\n        return inner\n>>> F = outer(0)\n>>> F('code')                     # State is the same on every inner run\ncode 0\n>>> F('hack')\nhack 0\nChanging a name in an enclosing def’s scope, however, is not allowed by\ndefault:\n>>> def outer(start):\n        state = start\n        def inner(label):\n            state += 1            # Cannot change by default\n            print(label, state)\n        return inner\n>>> F = outer(0)\n>>> F('code')\nUnboundLocalError: cannot access local variable 'state' …\nNow, if we declare state in the outer scope as nonlocal within inner, we get\nto both reference and change it inside the nested function. Again, this works\neven though outer has returned and exited by the time we call the returned\ninner function through the name F:\n>>> def outer(start):\n        state = start             # Each call gets its own state\n        def inner(label):\n            nonlocal state        # <= Remembers state in enclosing scope\n            state += 1            # Allowed to change it if nonlocal\n            print(label, state)\n        return inner\n>>> F = outer(0)\n>>> F('code')                     # Increments state on each call\ncode 1\n>>> F('hack')\nhack 2",
      "content_length": 1369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 639,
      "chapter": null,
      "content": "As usual with enclosing scope references, we can call the outer factory\n(closure) function multiple times to get multiple copies of its state in memory.\nThe state object in the enclosing scope is essentially attached to the new inner\nfunction object returned; each call makes a new, distinct state object, such that\nupdating one function’s state won’t impact the other. The following continues\nthe prior listing’s interaction:\n>>> G = outer(24)                 # Make a new outer that starts at 24\n>>> G('code')\ncode 25\n>>> G('hack')                     # G's state information updated to 26\nhack 26\n>>> F('more')                     # But F's state is where it left off: at 2\nmore 3                            # Each call has different state information\nAs you can see, Python’s nonlocals are much more functional than “static”\nfunction locals typical in some other languages: in a closure function, nonlocals\nare per-call, multiple copy data.\nnonlocal Boundary Cases\nThough useful, nonlocals come with some subtleties to be aware of. First, unlike\nthe global statement, nonlocal names really must be assigned in an enclosing\ndef’s scope—you cannot create them dynamically by assigning them anew in a\nnested function. The enclosing function’s assignment can appear either before or\nafter the nested function, but you’ll get an error if it’s missing. In fact, nonlocals\nare checked for an enclosing function assignment at function definition time,\nbefore either an enclosing or nested function is ever called:\n>>> def outer(start):\n        def inner(label):\n            nonlocal state        # Nonlocals must exist in an enclosing def\n            state = 0\n            print(label, state)\n        return inner\nSyntaxError: no binding for nonlocal 'state' found\n>>> def outer(start):\n        def inner(label):",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 640,
      "chapter": null,
      "content": "global state          # But globals don't have to exist when declared\n            state = 0\n            print(label, state)\n        return inner\n>>> F = outer(0)\n>>> F('glob')\nglob 0\n>>> state                         # Created by the assignment in nested inner\n0\nSecond, nonlocal restricts the scope lookup to just enclosing defs; nonlocals\nare not looked up in the enclosing module’s global scope or the built-in scope\noutside all defs, even if they are already there:\n>>> state = 0\n>>> def outer():\n        def inner():\n            nonlocal state            # Must be located in a def, not the module\n            state += 1                # Use global for globals, not nonlocal\n        return inner\nSyntaxError: no binding for nonlocal 'chapter' found\n>>> def outer():\n        def inner():\n            nonlocal ord              # Ditto for built-ins (and immediate calls)\n            print(ord)\n        inner()\nSyntaxError: no binding for nonlocal 'ord' found\nThese restrictions make sense once you realize that Python would not otherwise\ngenerally know in which enclosing scope to create a brand-new name. In the\nprior listing, should state be assigned in outer, or the module outside? Because\nthis is ambiguous, Python must resolve nonlocals at function creation time, not\nfunction call time.\nFinally, this chapter has been careful to say that nonlocal names must be\nassigned in an enclosing function, not the enclosing function. This can matter\nwhen function nesting runs deep: a name listed in nonlocal can technically\nappear anywhere in the hierarchy of enclosing functions—not just one level up\n—and the closest appearance is used. Even so, because such code seems unlikely",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 641,
      "chapter": null,
      "content": "to crop up in practice (and probably qualifies as cruel and unusual punishment),\nwe’ll forgo a formal demo here, but see “Arbitrary Scope Nesting” for most of\nthe morass.\nState-Retention Options\nGiven the extra complexity of nested functions, you might wonder why they’re\nworth the fuss. Although it’s difficult to see in our small examples, state\ninformation becomes essential in many programs. While functions can return\nresults, their local variables won’t normally retain other values that must live on\nbetween calls. Moreover, many applications require such values to differ per\ncontext of use.\nBroadly speaking, there are multiple ways for Python functions to retain state\nbetween calls. These include the global variables and enclosing scope references\nwe’ve already met, but also class-instance attributes and function attributes. To\nclose out this chapter, let’s review these options to see how they stack up.\nNonlocals: Changeable, Per-Call, LEGB\nFirst off, the following code is a recap from the prior section, repeated here for\ncompare and contrast. As we’ve seen, its nonlocal allows state in an enclosing\nscope to be saved and modified. Each call to outer makes a self-contained\npackage of changeable information, whose names and objects do not clash with\nany other part of the program:\n>>> def outer(start):\n        state = start                  # Each call gets its own state\n        def inner(label):\n            nonlocal state             # Remembers state in enclosing scope\n            state += 1                 # Allowed to change it if nonlocal\n            print(label, state)\n        return inner\n>>> F = outer(0)                       # State to be saved in enclosing scope\n>>> F('nonlocal1')                     # State visible within closure only\nnonlocal1 1\n>>> F('nonlocal2')\nnonlocal2 2",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 642,
      "chapter": null,
      "content": ">>> F.state\nAttributeError: 'function' object has no attribute 'state'\nWe need to declare variables nonlocal only if they must be changed (other\nenclosing scope name references are automatically retained as usual per LEGB),\nand nonlocal names are not visible outside the enclosing function.\nWhile this scheme works well, the next three sections present some alternatives.\nSome of the code in these sections uses tools we haven’t covered yet and is\nintended partially as preview, but we’ll keep the examples simple here so that\nyou can compare and contrast along the way.\nGlobals: Changeable but Shared\nOne common suggestion for achieving state retention without nonlocal is to\nsimply move saved info out to the global scope (the enclosing module):\n>>> def outer(start):\n        global state                 # Move it out to the module to change it\n        state = start                # global allows changes in module scope\n        def inner(label):\n            global state\n            state += 1\n            print(label, state)\n        return inner\n>>> F = outer(0)\n>>> F('global1')                     # Each call increments shared global state\nglobal1 1 \n>>> F('global2')\nglobal2 2\n>>> state                            # State accessible as global outside defs\n2\nThis works in this case, but it requires global declarations in both functions and\nis prone to name collisions in the global scope (what if “state” is already being\nused for something else?). A more subtle problem is that it only allows for a\nsingle shared copy of the state information in the module scope—if we call\nouter again, we’ll wind up resetting the module’s state variable, such that\nprior calls will see their state overwritten:",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 643,
      "chapter": null,
      "content": ">>> G = outer(24)                    # Resets state's single copy in global scope\n>>> G('global3')\nglobal3 25\n>>> F('global4')                     # But F's counter has been overwritten!\nglobal4 26\nAs shown earlier, when you use nonlocal and nested function closures instead\nof global, each call to outer remembers its own unique copy of the state\nobject. For per-call roles, globals don’t fit the bill.\nFunction Attributes: Changeable, Per-Call, Explicit\nAs another state-retention option, we can often achieve the same effect as\nnonlocals with function attributes—user-defined names attached to functions\nexplicitly. When you attach user-defined attributes to nested functions generated\nby factory functions, they can also serve as per-call, multiple copy, and writeable\nstate, just like nonlocal scope closures. Such user-defined attribute names won’t\nclash with names Python creates itself, and as for nonlocal, need be used only\nfor state variables that must be changed; other scope references are retained and\nwork normally.\nBecause factory functions make a new function on each call anyhow, this does\nnot require extra objects—the new function’s attributes become per-call state in\nmuch the same way as nonlocals, and are similarly associated with the new\nfunction in memory. In contrast, function attributes are perhaps less magical than\nscopes, and allow state variables to be accessed outside the nested function. With\nnonlocal, state variables can be seen directly only within the nested def; with\nattributes, state access is a simple function-attribute fetch.\nHere’s a mutation of our example based on this technique—it replaces a\nnonlocal with an attribute attached to the nested function. This scheme may not\nseem as intuitive to some at first glance; you must access state explicitly through\nthe function’s name instead of as simple variables, and must initialize after the\nnested def. Still, it allows state to be accessed externally, saves a line by\neliminating a nonlocal declaration, and makes state usage more explicit:\n>>> def outer(start):\n        def inner(label):",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 644,
      "chapter": null,
      "content": "inner.state += 1               # Change object attr, not inner itself\n            print(label, inner.state)      # inner is in the enclosing scope\n        inner.state = start                # Initialize state after inner defined\n        return inner\n>>> F = outer(0)\n>>> F('attr1')                       # F is an inner with state attached\nattr1 1\n>>> F('attr2')\nattr2 2\n>>> F.state                          # Can access state outside defs too\n2\nBecause each call to the outer function produces a new nested function object,\nthis scheme supports multiple-copy, per-call changeable data just like nonlocal\nclosures—a usage mode that global variables cannot provide:\n>>> G = outer(24)                    # G has own state, doesn't overwrite F's\n>>> G('attr3')\nattr3 25\n>>> F('attr4')                       # F's state varies\nattr4 3\n>>> F.state                          # State is accessible and per-call\n3\n>>> G.state\n25\n>>> F is G                           # Different function objects\nFalse\nFine points: this code relies on the fact that the function name inner is a local\nvariable in the outer scope enclosing inner; as such, it can be referenced freely\ninside inner per LEGB’s E. This code also relies on the fact that changing an\nobject in place is not an assignment to a name; when it increments inner.state,\nit is changing part of the object inner references, not the name inner itself\n(much like the L.append call we saw earlier). Because we’re not really assigning\na name in the enclosing scope, no nonlocal declaration is required.\nWe’ll explore function attributes further in Chapter 19. Importantly, you’ll see\nthere that Python uses naming conventions that ensure that the arbitrary names\nyou assign as function attributes won’t clash with names related to internal\nimplementation, making the namespace equivalent to a user scope. At the end of",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 645,
      "chapter": null,
      "content": "the day, function attributes both predate nonlocal and provide similar utility,\nmaking the latter technically redundant in some roles.\nNOTE\nState with enclosing scope mutables: On a related note, it’s also possible to change a mutable\nobject like a list in the enclosing scope without declaring its name nonlocal. The following,\nfor example, implements changeable per-call state information, and largely works the same as\nthe preceding version (though it does not support access to state info from outside the\nfunctions):\ndef outer(start):\n    def inner(label):\n        state[0] += 1              # Clever hack or dark magic?\n        print(label, state[0])     # Leverage in-place mutable change\n    state = [start]\n    return inner\nThis exploits the mutability of lists, and like function attributes, relies on the fact that in-place\nobject changes do not classify a name as local. This is perhaps more obscure than either\nfunction attributes or nonlocal, though—it’s a technique that predates others, and seems to lie\ntoday somewhere on the spectrum from arcane hack to dated workaround. You’re probably\nbetter off using named function attributes than lists and numeric offsets this way (but you can’t\ncontrol what others code).\nClasses: Changeable, Per-Call, OOP\nAnother standard prescription for changeable state information in Python is to\nuse classes with attributes. Like function attributes, this scheme makes state\ninformation access more explicit than the implicit magic of scope lookup rules.\nIn addition, each instance of a class gets a fresh copy of the state information, as\na natural byproduct of Python’s object model. Classes also support inheritance,\nmultiple behaviors, and other OOP tools above and beyond functions.\nAs an abstract and partial illustration, the following defines two stateful\ninstances of a class:\nclass Book:\n    …code to define method functions that manage and use state…\nlp6e = Book()               # Make an instance of the class",
      "content_length": 1970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 646,
      "chapter": null,
      "content": "lp6e.year = 2024            # Access instance state via attributes\nlp6e.python = 3.12\nlp5e = Book()               # Make another instance of the class\nlp5e.year = 2013            # Each instance has its own attributes/state\nlp5e.python = 3.3\nBecause classes support a broader array of tools, they tend to require more code\nthan closure functions, but may be better in more demanding roles. We haven’t\nexplored classes in any sort of detail yet, though, so we’ll have to cut this section\nshort here. Watch for classes, and their explicit flavor of multiple-copy state\ninformation, in Part VI.\nAnd the Winner Is…\nAs a wrap-up, globals, nonlocals, function attributes, and classes all offer\nchangeable state-retention options. Globals support only single-copy shared data,\nbut nonlocals, function attributes, and classes all support multiple-copy\nchangeable state. Of the latter, nonlocals rely on an implicit LEGB lookup,\nfunction attributes are manual but explicit and allow state access outside of\nfunctions, and classes are a larger solution that comes with OOP’s complexities.\nAs usual, the best tool for your program depends upon your program’s goals.\nWe’ve seen that nonlocal provides changeable state for nested functions with a\ndedicated statement, and is especially useful for simpler state-retention needs\nwhere global variables do not apply and classes may not be warranted. That\nbeing said, function attributes can often serve the same roles as nonlocal, with\narguably less implicit behavior.\nWe’ll revisit state-retention options introduced here in Chapter 39 for a more\nrealistic use case—decorators, a tool that by nature involves multilevel state\nretention. State options have additional selection factors (e.g., performance),\nwhich we’ll have to leave unexplored here for space (you’ll learn how to time\ncode speed in Chapter 21). For now, it’s time to explore one more technique,\nwhich both confounds the LEGB rule and segues to the next chapter.\nScopes and Argument Defaults",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 647,
      "chapter": null,
      "content": "In early versions of Python, the enclosing scope references we’ve used in this\nchapter failed because nested functions did not do anything about scopes—a\nreference to an enclosing function’s variable would search only the local, global,\nand built-in scopes. Because it skipped the scopes of enclosing functions, an\nerror would result. To work around this, programmers typically used default\nargument values to pass in and remember the objects in an enclosing scope.\nThough a preview of the next chapter’s arguments coverage, this also bears on\nscopes. In the following, a takeoff of an earlier example, a value from the\nenclosing function’s scope is passed into a nested function via an unused\nargument’s default:\ndef f1():\n    X = 88\n    def f2(X=X):            # Remember enclosing scope X with defaults\n        print(X + 1)\n    f2()\nf1()                        # Prints 89\nThis syntax also works in lambda, which, as we’ve learned, naturally and\nnormally creates nested-function scopes:\ndef f1():\n    X = 88\n    f2 = lambda X=X: print(X + 1)\n    f2()\nBoth of these examples still work in all Python releases, and rely on argument\ndefaults. In short, the syntax arg=val in a function header means that the\nargument arg will default to the value val if no real value is passed to arg in a\ncall. In the preceding code, this is used to explicitly assign enclosing scope state\nto be retained.\nSpecifically, in the modified f2s here, the X=X means that the argument X on the\nleft will default to the value of X in the enclosing scope—because the X on the\nright is evaluated before Python steps into the nested function, it still refers to the\nX in f1. In effect, each f2’s default argument X remembers what X was in the\nenclosing f1: the object 88.",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 648,
      "chapter": null,
      "content": "That’s fairly subtle, and it depends entirely on the timing of default-value\nevaluations. It’s also an incidental—if not accidental—feature: this works only if\na real value is never passed to argument X to overwrite the default. In fact, the\nnested scope lookup rule was added to Python in part to make defaults\nunnecessary for this role: today, Python automatically remembers any values\nrequired from the enclosing scope for use in nested defs and lambdas. As we’ve\nseen, this example today works the same sans defaults:\ndef f1():\n    X = 88\n    def f2():               # Remember enclosing scope X per LEGB rule\n        print(X + 1)        # And likewise for lambda\n    f2()\nThat said, flat is generally better than nested again, and function nesting in some\nsuch code makes programs more complex than they need be. The following, for\ninstance, is an equivalent of the prior examples that avoids nesting altogether.\nNotice the forward reference to f2 inside f1 in this code—it’s OK to call a\nfunction defined after the function that calls it, as long as the second def runs\nbefore the first function is actually called. Code inside a function’s body is never\nevaluated until the function is later called:\ndef f1():\n    X = 88                  # Pass x along instead of nesting\n    f2(X)                   # Forward reference OK\ndef f2(X):\n    print(X + 1)            # Flat is still often better than nested\nIf you avoid nesting this way, you can almost forget about the nested scopes\nconcept in Python. On the other hand, nested functions can avoid name clashes\nby localizing the names of functions used nowhere else, and nesting is the basis\nof closure functions, which support stateful callable objects useful in a variety of\nroles. When functions are nested for such reasons, the LEGB rule almost makes\ndefaults unnecessary for saving state from an enclosing function’s scope—except\nin the following case.\nLoops Require Defaults, Not Scopes",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 649,
      "chapter": null,
      "content": "So why bother learning an outdated scope-reference scheme? Because it’s still\nrequired in one common case: if a lambda or def is nested in a loop, and the\nnested function references an enclosing scope variable that is changed by that\nloop, then all functions generated within the loop will have the same value—the\nvalue the referenced variable had in the last loop iteration. In such cases, you\nmust still use defaults to save the variable’s current value instead.\nThis may seem obscure, but it can come up in practice more often than you may\nthink, especially in code that generates event-handler functions for a number of\nwidgets in a GUI—for instance, handlers for button-clicks for all the buttons in a\npanel. If these are created in a loop (and they often will be), you need to be\ncareful to save state with defaults, or all your buttons’ callbacks may wind up\ndoing the same thing.\nHere’s an illustration of this phenomenon reduced to simple code: the following\nattempts to build up a list of functions that each remember the current variable i\nfrom the enclosing scope:\n>>> def makeActions():\n        acts = []\n        for i in range(5):                       # Try to remember each i in 0..4\n            acts.append(lambda x: i ** x)        # But all remember same last i!\n        return acts\n>>> acts = makeActions()\n>>> acts[0]\n<function makeActions.<locals>.<lambda> at 0x101ae7ec0>\nInterestingly, this can also be coded as a list comprehension, where a lambda can\nbe used as the collection result, and the list comprehension serves as a local\nscope for its loop variable:\n>>> acts = [(lambda x: i ** x) for i in range(5)]\n>>> acts[0]\n<function <lambda> at 0x10de6bce0>\nEither way, though, this doesn’t quite work—because the enclosing scope\nvariable i is looked up when the nested functions are later called, they all\neffectively remember the same value: the value the loop variable had on the last\nloop iteration. That is, when we pass a power argument of 2 to x in each of the",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 650,
      "chapter": null,
      "content": "following calls, we get back 4 to the power of 2 for each function in the list,\nbecause i is the same in all of them—4:\n>>> acts[0](2)                                  # All are 4 ** 2, 4=last i\n16\n>>> acts[1](2)                                  # This should be 1 ** 2 (1)\n16\n>>> acts[2](2)                                  # This should be 2 ** 2 (4)\n16\n>>> acts[4](2)                                  # Only this should be 4 ** 2 (16)\n16\nIn this case, we still have to explicitly retain enclosing scope values with default\narguments, rather than enclosing scope references. That is, to make this sort of\ncode work, we must pass in the current value of the enclosing scope’s variable\nwith a default. Here’s the required mod for both the def and comprehension\nversions:\n>>> def makeActions():\n        acts = []\n        for i in range(5):                       # Use defaults instead\n            acts.append(lambda x, i=i: i ** x)   # Remember _current_ i\n        return acts\n>>> acts = [(lambda x, i=i: i ** x) for i in range(5)]\nIn either coding, because defaults are evaluated when the nested function is\ncreated (not when it’s later called), each remembers its own value for i:\n>>> acts = makeActions()\n>>> acts[0](2)                                   # 0 ** 2\n0\n>>> acts[1](2)                                   # 1 ** 2\n1\n>>> acts[2](2)                                   # 2 ** 2\n4\n>>> acts[4](2)                                   # 4 ** 2\n16\nNor are function attributes a fix here: because a function’s own name is a\nvariable from the enclosing scope that changes in the loop too, in every function\nit may reference the last function made by the last loop iteration. We’ll omit the",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 651,
      "chapter": null,
      "content": "gory details here, but keep in mind that any enclosing scope reference changed\nby a loop may require defaults.\nThis may seem an odd special case, but it reflects Python’s implementation of\nvariable scopes, and will become more likely to crop up as you start writing\nlarger programs. This case is also rooted in both scopes and arguments defaults,\nand to understand the latter in full, we have to move on to the next chapter.\nNOTE\nState with argument-default mutables: Also on a related note, it’s possible to retain state too\nwith mutable argument defaults like lists and dictionaries (e.g., def f(a=[])). Because\ndefaults are implemented as objects attached to functions at function creation time, mutable\ndefaults retain state from call to call, rather than being initialized anew on each call. Defaults\ncan also retain mutables from an enclosing function’s scope, thereby enabling changeable per-\ncall state information.\nDepending on whom you ask—and when you ask them—this is either a feature that supports\nstate retention, or a perilous and dark corner of the language to be avoided. Usually,\nprogrammers expect defaults initialized with literals like [] to be re-created on every call, and\nare surprised when they retain prior calls’ values. More on this in “Function Gotchas”.",
      "content_length": 1283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 652,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we studied one of two key concepts related to functions: scopes,\nwhich determine how variables are looked up when used. As we learned,\nvariables are considered local to the function definitions in which they are\nassigned, unless they are specifically declared to be global or nonlocal. We also\nexplored some more advanced scope concepts here, including nested function\nscopes and function attributes. Finally, we looked at some general design ideas,\nsuch as the need to minimize globals and cross-file changes.\nIn the next chapter, we’re going to continue our function tour with the second\nkey function-related concept: argument passing. As you’ll find, arguments are\npassed into a function by assignment, but Python also provides tools that allow\nfunctions to be flexible in how items are passed, including the defaults we\npreviewed here. Before we move on, let’s take this chapter’s quiz to review the\nscope concepts we’ve covered here.\nTest Your Knowledge: Quiz\n1. What is the output of the following code, and why?\n>>> X = 'Hack'\n>>> def func():\n        print(X)\n>>> func()\n2. What is the output of this code, and why?\n>>> X = 'Hack'\n>>> def func():\n        X = 'Py!'\n>>> func()",
      "content_length": 1215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 653,
      "chapter": null,
      "content": ">>> print(X)\n3. What does this code print, and why?\n>>> X = 'Hack'\n>>> def func():\n        X = 'Py!'\n        print(X)\n>>> func()\n>>> print(X)\n4. What output does this code produce? Why?\n>>> X = 'Hack'\n>>> def func():\n        global X\n        X = 'Py!'\n>>> func()\n>>> print(X)\n5. What about this code—what’s the output, and why?\n>>> X = 'Hack'\n>>> def func():\n        X = 'Py!'\n        def nested():\n            print(X)\n        nested()\n>>> func()\n>>> print(X)\n6. How about this example: what is its output, and why?",
      "content_length": 516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 654,
      "chapter": null,
      "content": ">>> def func():\n        X = 'Py!'\n        def nested():\n            nonlocal X\n            X = 'Hack'\n        nested()\n        print(X)\n>>> func()\n7. Name three or more ways to retain state information across calls in a\nPython function.\nTest Your Knowledge: Answers\n1. The output here is Hack because the function references a global\nvariable in the enclosing module (because it is not assigned in the\nfunction, it is considered global).\n2. The output here is Hack again because assigning the variable inside the\nfunction makes it a local and effectively hides the global of the same\nname. The print statement finds the variable unchanged in the global\n(module) scope.\n3. It prints Py! on one line and Hack on another, because the reference to\nthe variable within the function finds the assigned local and the\nreference in the print statement finds the global.\n4. This time it just prints Py! because the global declaration forces the\nvariable assigned inside the function to refer to the variable in the\nenclosing global scope, even though the variable is assigned inside the\nfunction.\n5. The output in this case is again Py! on one line and Hack on another,\nbecause the print statement in the nested function finds the name in the\nenclosing function’s local scope, and the display at the end finds the",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 655,
      "chapter": null,
      "content": "variable in the global scope.\n6. This example prints Hack because the nonlocal statement means that\nthe assignment to X inside the nested function changes X in the\nenclosing function’s local scope. Without this statement, this assignment\nwould classify X as local to the nested function, making it a different\nvariable; the code would then print Py! instead.\n7. Although the values of local variables go away when a function returns,\nyou can make a Python function retain state information by using\nshared global variables, nonlocal enclosing scope references within\nnested functions, or using default argument values. Function attributes\nalso allow state to be attached to the function itself, instead of looked up\nin scopes. Another alternative, using classes and OOP, sometimes\nsupports state retention better than any of the scope-based techniques\nbecause it makes it explicit with attribute assignments; we’ll explore\nthis option in Part VI. Changing mutable objects in scopes and defaults\nworks too, but may not be legal in some locales.\nWHY YOU WILL CARE: CUSTOMIZING OPEN\nFor another example of closures at work, consider changing the built-in open\ncall to a custom version as suggested earlier in this chapter. If the custom\nversion needs to call the original, it must save it before changing it, and\nretain it for later use—a classic state retention scenario. Moreover, if we\nwish to support multiple customizations to the same function, globals won’t\ndo: we need per-customizer state.\nThe following, coded in file makeopen.py, is one way to achieve this. It uses\na nested scope closure to remember a value for later use, without relying on\nglobal variables—which can clash and allow just one value, and without\nusing a class—that may require more code than is warranted here:\nimport builtins\ndef makeopen(id):\n    original = builtins.open\n    def custom(*pargs, **kargs):\n        print(f'Custom open call {id}' , pargs, kargs)",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 656,
      "chapter": null,
      "content": "return original(*pargs, **kargs)\n    builtins.open = custom\nTo change open for every module in a process, this code reassigns it in the\nbuilt-in scope to a custom version coded with a nested def, after saving the\noriginal in the enclosing scope so the customization can call it later. This\ncode is partially a preview, as it relies on starred-argument forms to collect\nand later unpack arbitrary positional and keyword arguments meant for open\n—a topic coming up in the next chapter. Much of the magic here, though, is\nscope closures: the custom open found by the LEGB rule retains the\noriginal:\n>>> file = '../Chapter14/data.txt'\n>>> F = open(file)                   # Call built-in open in builtins\n>>> F.read()\n'Testing file IO\\nLearning Python, 6E\\nPython 3.12\\n'\n>>> from makeopen import makeopen    # Import open resetter function\n>>> makeopen('MOD1')                 # Custom open calls built-in open\n>>> F = open(file)                   # Call custom open in builtins\nCustom open call MOD1 ('../Chapter14/data.txt',) {}\n>>> F.read()\n'Testing file IO\\nLearning Python, 6E\\nPython 3.12\\n'\nBecause each customization remembers the former built-in scope version in\nits own enclosing scope, they can even be nested naturally in ways that\nglobal variables cannot support—each call to the makeopen closure function\nremembers its own versions of id and original, so multiple customizations\nmay be run:\n>>> makeopen('MOD2')                 # Nested customizers work too\n>>> F = open(file)                   # Because each retains its own state\nCustom open call MOD2 ('../Chapter14/data.txt',) {}\nCustom open call MOD1 ('../Chapter14/data.txt',) {}\n>>> F.read()\n'Testing file IO\\nLearning Python, 6E\\nPython 3.12\\n'\nAs is, our function simply adds possibly nested call tracing to a built-in\nfunction, but the general technique may have other applications. A class-\nbased equivalent to this may require more code because it would need to",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 657,
      "chapter": null,
      "content": "save the id and original values explicitly in object attributes—but requires\nmore background knowledge than we yet have; stay tuned for state retention\nin classes in this book’s Part VI.\n1  The scope lookup rule was branded the “LGB rule” in the first edition of this book. The enclosing\n“E” layer was added later in Python to obviate the task of passing in enclosing scope names explicitly\nwith default arguments—an advanced topic that we’ll sample later in this chapter. Since this scope is\nnow addressed by the nonlocal statement, the lookup rule might have been better named “LNGB,”\nbut backward compatibility matters in books, too. The present form of this acronym also does not\naccount for the newer obscure scopes of comprehensions and exception handlers, but acronyms\nlonger than four letters tend to defeat their purpose!\n2  Multithreading runs function calls in parallel with the rest of a program and is supported by Python’s\nstandard-library modules _thread, threading, and queue. Because all threaded functions run in the\nsame process, global scopes often serve as one form of shared memory between them (threads may\nshare both names in global scopes, as well as objects in a process’s memory space). Threading is\ncommonly used for long-running tasks in GUIs, to implement nonblocking IO, and to utilize CPU\ncapacity. Threading is also well beyond this language book’s scope (a property it shares with async\nfunctions, which are nevertheless part of Python syntax today, as you’ll learn in Chapter 20). See\nPython’s library manual for more details.",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 658,
      "chapter": null,
      "content": "Chapter 18. Arguments\nThe preceding chapter explored Python’s scopes—the places where variables are\ndefined and looked up. As we saw, the place where a name is defined in our code\ndetermines much of its meaning. This chapter continues the function story by\nstudying the concepts in Python argument passing—the way that objects are sent\nto functions as inputs. As you’ll see, arguments (a.k.a. parameters) are assigned\nto names in a function, but have more to do with object references than with\nvariable scopes. You’ll also find that Python provides extra tools, such as\nkeywords, defaults, and argument collectors and extractors, that allow arguments\nto be sent to functions flexibly.\nArgument-Passing Basics\nEarlier in this part of the book, we learned that def and lambda are function\ndefinitions, and both include argument-list headers that name variables which\nreceive values passed by calls. These arguments are used in function bodies, and\nmay be matched between call and header by position, name, and other means\nwe’ll explore later in this chapter.\nMore fundamentally, though, it was also noted that all Python arguments are\npassed by assignment—which means object reference. This has some subtle\nramifications that aren’t always obvious to newcomers. Let’s start our arguments\nadventure by exploring how this works. Here is a rundown of the key points in\nthis model:\nArguments are passed by automatically assigning objects to local\nvariable names. Function arguments are just another instance of\nPython assignment at work: they are references to objects sent by, and\npossibly shared with, the caller. Because references are implemented as\npointers, all arguments are passed by opaque pointer. As for all\nassignments, objects passed as arguments are never automatically\ncopied.",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 659,
      "chapter": null,
      "content": "Assigning to argument names inside a function does not affect the\ncaller. Per assignment norms, when the function is run by a call,\nargument names in the function header simply become new names in\nthe local scope of the function. There is no aliasing between function\nargument names and variable names in the caller’s scope.\nChanging a mutable object argument in a function may impact the\ncaller. On the other hand, as arguments are simply references to passed-\nin objects, functions can change passed-in mutable objects in place, and\nthe results may affect the caller. Hence, mutable arguments can be both\ninput and output for functions.\nFor more details on references, see Chapter 6; everything we studied there also\napplies to function arguments, though the assignment to argument names is\nautomatic and implicit.\nPython’s pass-by-assignment scheme isn’t quite the same as C++’s reference\nparameters option, but it turns out to be similar to the argument-passing model of\nthe C language (and others) in practice. You don’t need to know these languages\nto use Python, of course, but the comparison might help those with backgrounds\nin other tools:\nImmutable arguments have the same effect as passing “by value.”\nObjects such as integers and strings are passed by object reference\ninstead of by copying, but because you can never change immutable\nobjects in place anyhow, the net result is much like making a copy in\nother languages: the caller’s values never morph.\nMutable arguments have the same effect as passing “by pointer.”\nObjects such as lists and dictionaries are also passed by object\nreference, which has similar consequences to passing arrays as pointers\nin C: mutable objects can be changed in place within the function, with\nside effects like those for C’s arrays.\nIf you’ve never used C, Python’s argument-passing mode will seem simpler still\n—it involves just the assignment of object references to names, and it works the\nsame whether the objects are mutable or not.",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 660,
      "chapter": null,
      "content": "Arguments and Shared References\nTo demo argument-passing properties at work, consider the following code:\n>>> def f(a):                 # a is assigned a reference to the passed object\n        a = 99                # Changes local variable a only\n>>> b = 88\n>>> f(b)                      # a and b both reference same 88 initially\n>>> b                         # But b is not changed by assignment to a in f\n88\nIn this example, the variable a is assigned the object 88 at the moment the\nfunction is called with f(b), but a lives only within the called function’s local\nscope. Changing a inside the function has no effect on the place where the\nfunction is called; it simply resets the local variable a to a completely different\nobject, 99.\nThat’s what is meant by a lack of name aliasing—assignment to an argument\nname inside a function (e.g., a=99) does not magically change a variable like b\nin the scope of the function call. Argument names may share passed objects\ninitially (they are essentially pointers to those objects), but only temporarily,\nwhen the function is first called. As soon as an argument name is reassigned, this\nrelationship ends.\nAt least, that’s the case for assignment to argument names themselves. When\narguments are passed mutable objects like lists and dictionaries, we also need to\nbe aware that in-place changes to such objects may live on after a function exits,\nand hence impact callers. Here’s an example that demonstrates this behavior:\n>>> def changer(a, b):        # Arguments assigned references to objects\n        a = 2                 # Changes local name's value only\n        b[0] = 'mod'          # Changes shared object in place: outlives call\n>>> X = 1\n>>> L = [1, 2]                # Caller:\n>>> changer(X, L)             # Pass immutable and mutable objects\n>>> X, L                      # X is unchanged, but L is different!\n(1, ['mod', 2])\nIn this code, the changer function assigns values to argument a itself, and to a",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 661,
      "chapter": null,
      "content": "component of the object referenced by argument b. These two assignments\nwithin the function are only slightly different in syntax but have radically\ndifferent results:\nBecause a is a local variable name in the function’s scope, the first\nassignment has no effect on the caller—it simply changes the local\nvariable a to reference a completely different object, and does not\nchange the value of the name X in the caller’s scope. This is the same as\nin the prior example.\nArgument b is a local variable name, too, but it is passed a mutable\nobject—the list that L also references in the caller’s scope. Because the\nassignment to b[0] in the function is an in-place change to a shared\nobject, its result impacts the value of L after the function returns.\nReally, the second assignment statement in changer doesn’t change b—it\nchanges part of the object that b currently references. This in-place change\nimpacts the caller only because the changed object outlives the function call. The\nname L hasn’t changed either—it still references the same, changed object—but\nit seems as though L differs after the call because the value it references has been\nmodified within the function. In effect, the list name L serves as both input to\nand output from the function.\nFigure 18-1 illustrates the name/object bindings that exist immediately after the\nfunction has been called, and before its code has run. When the call begins, two\nobjects are shared among four names.\nIf this example is still confusing, it may help to notice that the effect of the\nautomatic assignments of the passed-in arguments is the same as running a series\nof simple assignment statements. In terms of the first argument, the assignment\nhas no effect on the caller:\n>>> X = 1\n>>> a = X               # They share the same object\n>>> a = 2               # Name change resets 'a' only, 'X' is still 1\n>>> X\n1",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 662,
      "chapter": null,
      "content": "Figure 18-1. Function arguments and shared object references\nThe assignment through the second argument does affect a variable at the call,\nthough, because it is an in-place object change:\n>>> L = [1, 2]\n>>> b = L               # They share the same object\n>>> b[0] = 'mod'        # In-place change from 'b': 'L' sees the change too\n>>> L\n['mod', 2]\nIf you recall our discussions about shared mutable objects in Chapters 6 and 9,\nyou’ll recognize the phenomenon at work: changing a mutable object in place\ncan impact other references to that object. Here, the effect is to make one of the\narguments work like both an input and an output of the function.\nAvoiding Mutable Argument Changes\nThis behavior of in-place changes to mutable arguments isn’t a bug—it’s simply\nthe way argument passing works in Python, and turns out to be widely useful in\npractice. Arguments are normally passed to functions by reference because that\nis what we normally want. It means we can pass large objects around our",
      "content_length": 996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 663,
      "chapter": null,
      "content": "programs without making multiple copies along the way, and we can easily\nupdate these objects as we go. In fact, as you’ll see in Part VI, Python’s class\nand OOP model depends upon changing a passed-in “self” argument in place, to\nupdate mutable object state.\nIf we don’t want in-place changes within functions to impact objects we pass to\nthem, though, we can simply make explicit copies of mutable objects, as we saw\nin Chapter 6. For function arguments, we can always copy the list at the point of\ncall with tools like list, list.copy, or an empty slice (dictionaries have similar\ncopy tools):\nL = [1, 2]\nchanger(X, L[:])        # Pass a copy, so our 'L' does not change\nWe can also copy within the function itself, if we never want to change passed-in\nobjects, regardless of how the function is called:\ndef changer(a, b):\n    b = b.copy()        # Copy input list so we don't impact caller\n    a = 2\n    b[0] = 'mod'        # Changes our list copy only\nBoth of these copying schemes don’t stop the function from changing the object\n—they just prevent those changes from impacting the caller. To really prevent\nchanges, we can always convert to immutable objects to force the issue. Tuples,\nfor example, raise an exception when changes are attempted:\nL = [1, 2]\nchanger(X, tuple(L))    # Pass a tuple, so changes are errors\nTypeError: 'tuple' object does not support item assignment\nThis scheme uses the built-in tuple function, which builds a new tuple out of all\nthe items in a sequence (really, any iterable). It’s also something of an extreme—\nbecause it forces the function to be written to never change passed-in arguments,\nthis solution might impose more limitations on the function than it should, and\nso should generally be avoided (you never know when changing arguments\nmight come in handy for other calls in the future). Using this technique will also\nmake the function lose the ability to call any list-specific methods on the",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 664,
      "chapter": null,
      "content": "argument, including methods that do not change the object in place (e.g. copy,\nthough tuples have adopted list count and index).\nThe main point to remember here is that functions might update mutable objects\nlike lists and dictionaries passed into them. This isn’t necessarily a problem if\nit’s expected, and often serves useful purposes. Moreover, functions that change\npassed-in mutable objects in place are probably designed and intended to do so\n—the change is likely part of a well-defined API that you shouldn’t violate by\nmaking copies.\nHowever, you do have to be aware of this property—if objects change out from\nunder you unexpectedly, check whether a called function might be responsible,\nand make copies when objects are passed if needed.\nSimulating Output Parameters and Multiple Results\nHere’s another function topic from the assignments department. We’ve already\ndiscussed the return statement and used it in examples. What we haven’t yet\nseen is a common though unusual coding technique it enables: because return\ncan send back any sort of object, it can return multiple values by packaging them\nin a tuple or other collection type. In fact, although Python doesn’t support what\nsome languages label “call by reference” argument passing, we can simulate it\nby returning tuples and assigning the results back to the original argument names\nin the caller:\n>>> def multiple(x, y):\n        x = 2                # Changes local names only\n        y = [3, 4]\n        return x, y          # Return multiple new values in a tuple\n>>> X = 1\n>>> L = [1, 2]\n>>> X, L = multiple(X, L)    # Assign results to caller's names\n>>> X, L\n(2, [3, 4])\nIt looks like the code is returning two values here, but it’s really just one—a two-\nitem tuple with the optional surrounding parentheses omitted. After the call\nreturns, we can use tuple (a.k.a. sequence) assignment to unpack the parts of the\nreturned tuple. (If you’ve forgotten why this works, flip back to “Tuples” in",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 665,
      "chapter": null,
      "content": "Chapters 4 and 9, and “Assignments” in Chapter 11.) The net effect of this\ncoding pattern is to both send back multiple results and simulate the output\nparameters of other languages by explicit assignments. Here, X and L change\nafter the call—but only because the code said so. Lists would work, too, but\ntuples sans () are less to type, and hence common.\nSpecial Argument-Matching Modes\nAs we’ve just seen, arguments are always passed by assignment in Python;\nnames in a def or lambda definition’s header are assigned references to passed-\nin objects. On top of this model, though, Python provides additional tools that\nalter the way the argument objects in a call are matched with argument names in\nthe definition prior to assignment. These tools are all optional, but allow us to\ncode functions that support more flexible calling patterns and are commonly\nused by libraries you’re likely to encounter.\nBy default, arguments are matched between call and definition by position, from\nleft to right, and you must pass exactly as many arguments as there are argument\nnames in the function definition. However, you can also specify matching by\nname, provide default values, unpack and collect arbitrarily many arguments,\nand even specify passing-mode requirements. This section presents all these\nextra tools with a quick overview followed by examples, and a formal look at\nhow they interact at the end after we’ve had a chance to see the basics.\nArgument Matching Overview\nBefore we get into syntax details, it’s important to stress that these special modes\nare optional and deal only with matching objects to names; the underlying\npassing mechanism after the matching takes place is still assignment. In fact,\nsome of these tools are intended more for people writing libraries than for\napplication developers. That said, you may stumble across these modes even if\nyou don’t code them yourself, so the following summarizes all the options:\nPositionals: matched from left to right\nThe normal case, which we’ve mostly been using so far, is to match",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 666,
      "chapter": null,
      "content": "argument values (passed in a call) to argument names (listed in a function\ndefinition) by position, from left to right.\nKeywords: matched by argument name\nAlternatively, callers can explicitly specify which argument in the function is\nto receive a value, by giving the definition’s name for the argument with\nname=value syntax.\nDefaults: specify values for optional arguments that aren’t passed\nFunctions themselves can specify default values for arguments to receive if\nthe call passes too few values, again using the name=value syntax.\nStarred collectors: collect arbitrarily many positional or keyword arguments\nFunction definitions can use special arguments preceded with one or two *\ncharacters to collect an arbitrary number of arguments after other matching.\nThis feature is sometimes referred to as varargs, after a variable-length\nargument list tool in the C language; in Python, the arguments are collected\nin a normal object.\nStarred unpackers: pass arbitrarily many positional or keyword arguments\nFunction calls can also use the one or two * syntax to unpack argument\ncollections into separate arguments. This is the inverse of a * in a function\ndefinition—in the definition it means collect arbitrarily many arguments,\nwhile in the call it means unpack arbitrarily many arguments, and pass them\nindividually as discrete values.\nKeyword-only arguments: arguments that must be passed by name\nFunction definitions can also use a * in their headers to specify arguments\nthat must be passed by name as keyword arguments, not position. This is",
      "content_length": 1551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 667,
      "chapter": null,
      "content": "often used for configuration options that augment primary arguments.\nPositional-only arguments: arguments that must be passed by position\nAs of Python 3.8, function definitions may additionally use a / in their\nheader’s arguments list to specify that all arguments preceding it must be\npassed by position, not keyword-argument name.\nArgument Matching Syntax\nAs a reference and preview, Table 18-1 summarizes the syntax that invokes the\nargument-matching modes.\nTable 18-1. Function argument-matching forms\nSyntax\nLocation\nInterpretation\nfunc(value)\nCaller\nNormal argument: matched by position\nfunc(name=value)\nCaller\nKeyword argument: matched by name\nfunc(*iterable)\nCaller\nPass all objects in iterable as individual\npositional arguments\nfunc(**dict)\nCaller\nPass all key/value pairs in dict as\nindividual keyword arguments\ndef func(name)\nFunction\nNormal argument: matches any passed\nvalue by position or name\ndef func(name=va\nlue)\nFunction\nDefault argument value, if not passed in the\ncall\ndef func(*name)\nFunction\nMatches and collects remaining positional\narguments in a tuple\ndef func(**name)\nFunction\nMatches and collects remaining keyword\narguments in a dictionary",
      "content_length": 1168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 668,
      "chapter": null,
      "content": "def func(*name, \nname)\nFunction\nArguments that must be passed by keyword\nonly in calls\ndef func(*, name\n)\nFunction\nArguments that must be passed by keyword\nonly in calls\ndef func(name, /\n)\nFunction\nArguments that must be passed by position\nonly in calls\nThe argument-matching modes listed in this table break down between function\ncalls and definitions as follows:\nIn a function call (the first four rows of the table)\nSimple values are matched to definition arguments by position, but using the\nname=value form tells Python to match arguments by name instead; these\nare called keyword arguments. Any number of *iterable or **dict forms\ncan be used in a call, allowing us to bundle many positional or keyword\nobjects in iterables and mappings, respectively, and unpack them as separate,\nindividual arguments when they are passed to the function.\nIn a function definition (the rest of the table)\nA simple name is matched by position or name depending on how the caller\npasses it, but the name=value form specifies a default value. The *name form\ncollects any extra unmatched positional arguments in a tuple, and **name\ncollects unmatched keyword arguments in a dictionary. In addition, any\nnormal or defaulted argument names following a *name or a bare * are\nkeyword-only arguments and must be passed by keyword in calls, and\narguments preceding a / are positional-only arguments that must not be\npassed by keyword name.",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 669,
      "chapter": null,
      "content": "Of these, keyword arguments and defaults are probably the most commonly used\nin Python code. We’ve informally used both of these earlier in this book:\nWe’ve already used keywords to specify options to the print function,\nbut they are more general—keywords allow us to label any argument\nwith its name, to make calls more explicit and informational.\nWe met defaults earlier, too, as a way to pass in values from the\nenclosing function’s scope, but they are also more general—they allow\nus to make any argument optional, providing its default value in a\nfunction definition.\nAs you’ll see ahead, the combination of defaults in a function definition and\nkeywords in a call further allows us to pick and choose which defaults to\noverride per call.\nIn short, special argument-matching modes let you be fairly liberal about how\nmany arguments must be passed to a function. If a function specifies defaults,\nthey are used if you pass too few arguments. If a function uses the * argument-\ncollector forms, you can seemingly pass too many arguments; the * names\ncollect the extra arguments in data structures for processing within the function.\nArgument Passing Details\nIf you choose to use and combine the special argument-matching modes, Python\nwill ask you to follow some ordering rules among the modes’ optional\ncomponents. We’re going to defer full and formal coverage of these until we’ve\nhad a chance to observe these modes in action, but as some initial tips:\nIn a function call, any number of positionals, keywords, and starred\nunpackings can be used, but positional arguments must precede\nkeyword arguments and **dict unpackings, and *iterable\nunpackings must precede **dict unpackings.\nIn a function definition, arguments must appear in this order: any\npositional-only arguments; followed by any positional-or-keyword\narguments; followed by the optional *name positional collector or * and\nany keyword-only arguments; followed by the optional **name",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 670,
      "chapter": null,
      "content": "keyword collector. Arguments can have optional defaults (name=value),\nbut once a default is used, all arguments must use defaults up to a *,\nafter which keyword-only arguments allow defaults and nondefaults to\nbe freely mixed.\nIf you mix arguments in any other order, you will get a syntax error because the\ncombinations can be ambiguous. The steps that Python internally carries out to\nmatch arguments before assignment can roughly be described as follows:\n1. Unpack all *args at the call into nonkeyword arguments.\n2. Unpack all **args at the call into keyword arguments.\n3. Assign nonkeyword arguments by position.\n4. Assign keyword arguments by matching names.\n5. Collect extra nonkeyword arguments in the *name tuple.\n6. Collect extra keyword arguments in the **name dictionary.\n7. Assign default values to unassigned arguments.\nAfter arguments in call and definition are matched, Python checks to make sure\neach argument is passed just one value (or else an error is raised) and then\nassigns argument names to the objects passed to them and runs the function\nbody.\nThe actual matching algorithm Python uses is a bit more complex, so we’ll defer\nto Python’s standard language manual for a more exact description. It’s not\nrequired reading, but tracing Python’s matching algorithm may help you to\nunderstand some convoluted cases, especially when modes are mixed.\nWe’ll return to the ordering rules in function calls in definitions with higher\nfidelity after we’ve had a chance to meet all the players. Let’s get started in the\nnext section with the most common of the bunch.\nNOTE\nBut annotations are moot: Argument names in a def statement (only) can also have annotation\nvalues, specified as name:annot, or name:annot=default when defaults are present. This is",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 671,
      "chapter": null,
      "content": "simply additional syntax for arguments and does not augment or change argument-ordering\nrules. The function itself can also have an annotation value, given as def f(…)->annot, and\nPython attaches all annotation values to the function object. See the discussion of function\nannotation in Chapter 19 for more details, and the section on their role in unused type hinting\nin Chapter 6.\nKeyword and Default Examples\nArgument passing is simpler in code than the preceding descriptions may imply.\nFirst off, if you don’t use any special matching syntax, Python matches names by\nposition from left to right. For instance, if you define a function that requires\nthree arguments, you must call it with three arguments:\n>>> def f(a, b, c): print(a, b, c)\n>>> f(1, 2, 3)\n1 2 3\nHere, we pass by position—a is matched to 1, b is matched to 2, and so on. This\nworks like it does in most other programming languages.\nKeywords\nIn Python, though, you can be more specific about what goes where when you\ncall a function. Keyword arguments allow us to match by name, instead of by\nposition. Using the same function definition but a different call:\n>>> f(c=3, b=2, a=1)\n1 2 3\nThe c=3 in this call, for example, means send 3 to the argument named c. More\nformally, Python matches the name c in the call to the argument named c in the\nfunction definition, and then assigns the value 3 to that argument. The net effect\nof this call is the same as that of the prior call, but notice that the left-to-right\norder of the arguments no longer matters when keywords are used because\narguments are matched by name, not by position.\nIt’s even possible to combine positional and keyword arguments in a single call.\nIn this case, all positionals are matched first from left to right in the definition,",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 672,
      "chapter": null,
      "content": "before keywords are matched by name:\n>>> f(1, c=3, b=2)            # a gets 1 by position, b and c passed by name\n1 2 3\nWhen most people see this the first time, they wonder why one would use such a\ntool. Keywords typically have two roles in Python. First, they make your calls a\nbit more self-documenting (assuming that you use better argument names than a,\nb, and c!). For example, a call of this form:\nfunc(title='Learning Python', edition=6, year=2024, python=3.12)\nis much more meaningful than a call with three naked values separated by\ncommas, especially in larger programs—the keywords serve as labels for the\ndata in the call:\nfunc(title='Learning Python', 6, 2024, 3.12)\nThe second major use of keywords occurs in conjunction with defaults, which\nwe turn to next.\nDefaults\nWe talked about defaults in brief earlier, when discussing nested function scopes.\nIn short, defaults allow us to make selected function arguments optional; if not\npassed a value, the argument is assigned its default before the function runs. For\nexample, here is a function that requires one argument and defaults two others:\n>>> def f(a, b=2, c=3): print(a, b, c)          # a required, b and c optional\nAs noted earlier, defaults must appear after nondefaults at this point in a header\n(they can be mixed after a * as you’ll see ahead). When we call this function, we\nmust provide a value for a, either by position or by keyword; however, providing\nvalues for b and c is optional. If we don’t pass values to b and c, they default to\n2 and 3, respectively:\n>>> f(1)                   # Use defaults\n1 2 3",
      "content_length": 1589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 673,
      "chapter": null,
      "content": ">>> f(a=1)\n1 2 3\nIf we pass two values, only c gets its default, and when passing three values, no\ndefaults are used:\n>>> f(1, 4)                # Override defaults\n1 4 3\n>>> f(1, 4, 5)\n1 4 5\nFinally, here is how the keyword and default features interact. Because they\nsubvert the normal left-to-right positional mapping, keywords allow us to\nessentially skip over arguments with defaults:\n>>> f(1, c=6)              # Choose defaults: b in the middle\n1 2 6\nHere, a gets 1 by position, c gets 6 by keyword, and b, in between, defaults to 2.\nBe careful not to confuse the special name=value syntax in a function definition\nand a function call; in the call it means a match-by-name keyword argument,\nwhile in the definition it specifies a default for an optional argument. In both\ncases, this is not an assignment statement (despite its appearance); it is special\nsyntax for these two contexts, which modifies the default argument-matching\nmechanics.\nCombining keywords and defaults\nHere is a slightly larger example that demonstrates keywords and defaults in\naction. In the following, the caller must always pass at least two arguments (to\nmatch code and hack), but the other two are optional. If they are omitted,\nPython assigns script and app to the defaults specified in the definition:\ndef func(code, hack, script=0, app=0):   # First 2 required\n    print((code, hack, script, app))\nfunc(1, 2)                               # Output: (1, 2, 0, 0)\nfunc(1, app=1, hack=0)                   # Output: (1, 0, 0, 1)\nfunc(code=1, hack=0)                     # Output: (1, 0, 0, 0)\nfunc(script=1, hack=2, code=3)           # Output: (3, 2, 1, 0)",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 674,
      "chapter": null,
      "content": "func(1, 2, 3, 4)                         # Output: (1, 2, 3, 4)\nNotice again that when keyword arguments are used in the call, the order in\nwhich the arguments are listed doesn’t matter; Python matches by name, not by\nposition. The caller must supply values for code and hack, but they can be given\nby position or by name.\nAlso keep in mind that all the special definition-side argument-matching syntax\nwe’re exploring in this chapter works the same in def and lambda, though most\nexamples use the former, and the latter returns results implicitly:\n>>> func = lambda code, hack, script=0, app=0: (code, hack, script, app)\n>>> func(1, 2)\n(1, 2, 0, 0)\n>>> func(script=1, hack=2, code=3)\n(3, 2, 1, 0)\nNOTE\nBeware mutable defaults: As noted in the prior chapter, if you code a default to be a mutable\nobject (e.g., def f(a=[])), the same, single mutable object is reused every time the function is\nlater called—even if it is changed in place within the function. The net effect is that the\nargument’s default retains its value from the prior call and is not reset to its original value\ncoded in the function header. To reset anew on each call, move the assignment into the\nfunction body instead. Mutable defaults allow state retention, but this is often an unpleasant\nsurprise. Since this is such a common trap, we’ll postpone further exploration until this part’s\n“gotchas” list at the end of Chapter 21.\nArbitrary Arguments Examples\nThe last two argument-matching extensions, * and **, are designed to support\nany number of arguments. Both can appear in either the function definition or a\nfunction call, and they have related purposes in the two locations.\nDefinitions: Collecting arguments\nThe first use, in the function definition, collects unmatched positional arguments\ninto a tuple:\n>>> def f(*args): print(args)",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 675,
      "chapter": null,
      "content": "When this function is called, Python collects all the positional arguments into a\nnew tuple and assigns the variable args to that tuple. Because it is a normal\ntuple object, it can be indexed, stepped through with a for loop, and so on:\n>>> f()\n()\n>>> f(1)\n(1,)\n>>> f(1, 2, 3, 4)\n(1, 2, 3, 4)\nThe ** feature is similar, but it only works for keyword arguments—it collects\nthem into a new dictionary, which can then be processed with normal dictionary\ntools. In a sense, the ** form allows you to convert from keywords to\ndictionaries, which you can then step through with keys calls, dictionary\niterators, and the like (this is roughly what the dict call does when passed\nkeywords, but it returns the new dictionary):\n>>> def f(**args): print(args)\n>>> f()\n{}\n>>> f(a=1, b=2)\n{'a': 1, 'b': 2}\nFinally, function definitions can combine normal arguments, the *, and the ** to\nimplement wildly flexible call signatures. For instance, in the following, 1 is\npassed to a by position, 2 and 3 are collected into the pargs positional tuple, and\nx and y wind up in the kargs keyword dictionary:\n>>> def f(a, *pargs, **kargs): print(a, pargs, kargs)\n>>> f(1, 2, 3, x=1, y=2)\n1 (2, 3) {'x': 1, 'y': 2}\nNotice that the dictionary’s keys are ordered here: as of Python 3.6, the **\ncollector’s keys preserve the order in which keyword arguments were passed to\nthe function. This relies on the insertion order of keys in dictionaries at large (see\nChapter 8 for a refresher if you’ve forgotten what that means).",
      "content_length": 1497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 676,
      "chapter": null,
      "content": "Functions with both * and ** may be rare, but they show up in functions that\nneed to support multiple call patterns (for backward compatibility, for instance).\nIn fact, these features can be combined in even more complex ways that may\nseem ambiguous at first glance—an idea we will revisit later in this chapter\nwhen we lock down ordering rules. First, though, let’s see what happens when *\nand ** are coded in function calls instead of definitions.\nCalls: Unpacking arguments\nIt turns out that we can use the * syntax when we call a function, too. In this\ncontext, its meaning is the inverse of its meaning in the function definition—it\nunpacks a collection of arguments, rather than building a collection of\narguments. For example, we can pass four arguments to a function in a tuple or\nother iterable, and let Python unpack them into individual positional arguments:\n>>> def func(a, b, c, d): print(a, b, c, d)\n>>> args = (1, 2)\n>>> args += (3, 4)\n>>> func(*args)                            # Same as func(1, 2, 3, 4)\n1 2 3 4\nSimilarly, the ** syntax in a function call unpacks a dictionary or other mapping\nof key/value pairs into separate keyword arguments:\n>>> args = {'a': 1, 'b': 2, 'c': 3}\n>>> args['d'] = 4\n>>> func(**args)                           # Same as func(a=1, b=2, c=3, d=4)\n1 2 3 4\nMoreover, we can use other iterables like lists and other dictionary syntax like\ndict, and we may combine star, positional, and keyword arguments in the call in\nvery flexible ways:\n>>> func(*(1, 2), **{'d': 4, 'c': 3})      # Same as func(1, 2, d=4, c=3)\n1 2 3 4\n>>> func(1, *[2, 3], **dict(d=4))          # Same as func(1, 2, 3, d=4)\n1 2 3 4\n>>> func(1, c=3, *[2], **{'d': 4})         # Same as func(1, 2, c=3, d=4)\n1 2 3 4\n>>> func(1, *(2, 3), d=4)                  # Same as func(1, 2, 3, d=4)",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 677,
      "chapter": null,
      "content": "1 2 3 4\n>>> func(1, *[2], c=3, **dict(d=4))        # Same as func(1, 2, c=3, d=4)\n1 2 3 4\nWhile the preceding serves to demo the possibilities, its use of stars and literals\nis overkill when arguments are known; more typical code would build up\nargument collections ahead of the call and unpack by names:\n>>> pargs, kargs = (1, 2), dict(d=4, c=3)\n>>> func(*pargs, **kargs)\n1 2 3 4\nStar unpacking is convenient when you cannot predict the number of arguments\nthat will be passed to a function when you write your script; you can build up a\ncollection of arguments at runtime instead and call the function generically this\nway. Here again, though, don’t confuse the */** starred-argument syntax in the\nfunction definition and the function call—in the definition it collects any number\nof arguments, while in the call it unpacks any number of arguments. In both, one\nstar means positionals, and two applies to keywords.\nFinally, as of Python 3.5, and as noted in Chapter 11’s sidebar “The Many Stars\nof Python”, we can even use multiple * and ** items in calls to unpack multiple\niterables and mappings, respectively. The * unpacks into positional arguments,\nand ** into keyword arguments, though single stars must precede double stars,\nper formal rules coming up ahead. Again, the following uses literals to demo, but\nthese would usually be names assigned to prebuilt values:\n>>> def func(a, b, c, d): print(a, b, c, d)\n>>> func(*[1], *(2,), **dict(c=3), **{'d': 4})         # *Positionals + **keywords\n1 2 3 4\n>>> func(*[1], *(2,), *[3, 4])                         # All *positionals\n1 2 3 4\n>>> func(**dict(a=1, b=2), **dict(c=3), **{'d': 4})    # All **keywords\n1 2 3 4\n>>> func(*[1], 2, **dict(c=3), d=4)                    # All call modes at once!\n1 2 3 4\n>>> func(*[1], **dict(b=2, c=3), *[4])\nSyntaxError: iterable argument unpacking follows keyword argument unpacking",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 678,
      "chapter": null,
      "content": "NOTE\nUnpacking generality—and inconsistency: As previewed in Chapter 14, the * form in a call is\nan iteration tool, so it accepts any iterable object, not just tuples or other sequences used in\nexamples here. For instance, iterables like zip and range work after a * too and unpack into\nindividual arguments, and file objects automatically read and unpack their lines:\nfunc(*range(4))            # Same as func(0, 1, 2, 3)\nfunc(*open('filename'))    # Read+pass lines as arguments\nWatch for more examples of this utility in Chapter 20, after we study generators.\nOn the downside, stars also come with inconsistencies. For one, this generality holds true only\nfor calls—a * unpacking in a call allows any iterable, but a * in a function definition always\ncollects extra arguments into a tuple. Moreover, this collection behavior in definitions is\nsimilar in spirit and syntax to the * in the extended-unpacking assignment forms we met in\nChapter 11 (e.g., x, *y = z), but that star usage always creates lists, not tuples. Again: these\nare different rules for different tools—despite the same syntax.\nWhy arbitrary arguments?\nThe prior section’s examples may seem academic (if not downright esoteric), but\nthey are used more often than you might expect. Some programs need to call\narbitrary functions in a generic fashion, without hardcoding their names or\narguments ahead of time. In fact, the real power of the special starred-unpacking\ncall syntax is that you don’t need to know how many arguments a function call\nrequires before you write a script. For example, you can use if logic to select\nfrom a set of functions and argument lists, and call any of them generically\n(functions here are hypothetical):\nif sometest:\n    action, args = func1, (1,)             # Call func1 with one arg in this case\nelse:\n    action, args = func2, (1, 2, 3)        # Call func2 with three args instead\n…\naction(*args)                              # Dispatch generically\nThis leverages both the * form and the fact that functions are objects that may be\nboth referenced by, and called through, any variable. More generally, this\nunpacking call syntax is useful anytime you cannot predict the arguments list. If\nyour user selects an arbitrary function via a user interface, for instance, you may",
      "content_length": 2279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 679,
      "chapter": null,
      "content": "be unable to hardcode a function call when writing your script. To work around\nthis, simply build up the arguments list with sequence operations, and call it with\nstarred-argument syntax to unpack the arguments:\nargs = (2,3)\nargs += (4,)\n…\nfunc3(*args)\nBecause the arguments list is passed in as a tuple here, the program can build it\nat runtime. This technique also comes in handy for functions that test or time\nother functions. For instance, the code in Example 18-1 supports any function\nwith any arguments by passing along whatever arguments were sent in (see\ntracer0.py in the example package).\nExample 18-1. tracer0.py\ndef tracer(func, *pargs, **kargs):         # Accept arbitrary arguments\n   print('calling:', func.__name__)\n   return func(*pargs, **kargs)           # Pass along arbitrary arguments\ndef func(a, b, c, d):\n   return a + b + c + d\nprint(tracer(func, 1, 2, c=3, d=4))\nThis code uses the built-in __name__ attribute attached to every function (as you\nmight expect, it’s the function’s name string), and uses stars to collect and then\nunpack the arguments intended for the traced function. In other words, when this\ncode is run, arguments are intercepted by the tracer and then propagated with\nunpacking call syntax:\n$ python3 tracer0.py\ncalling: func\n10\nFor another example of this technique, see the preview near the end of the\npreceding chapter, where it was used to reset the built-in open function (though\nit probably makes more sense to you now). We’ll code additional examples of\nsuch roles later in this book; see especially the sequence timing examples in\nChapter 21 and the various decorator utilities we will code in Chapter 39 (after a",
      "content_length": 1668,
      "extraction_method": "Direct"
    },
    {
      "page_number": 680,
      "chapter": null,
      "content": "preview in the next chapter). It’s a common technique in general tools.\nKeyword-Only Arguments\nBesides accepting keyword (i.e., pass-by-name) arguments in general, function\ndefinitions can also specify that some arguments must always be passed by\nkeyword only and will never be filled in by a positional argument. This\nextension, known as keyword-only arguments, can be useful if we want a\nfunction to both process any number of arguments and accept possibly optional\nconfiguration options.\nSyntactically, keyword-only arguments are coded as named arguments that may\nappear after *name in the arguments list. All such arguments must be passed\nusing keyword syntax in the call. For example, in the following, a may be passed\nby name or position, b collects any extra positional arguments, and c must be\npassed by keyword only:\n>>> def kwonly(a, *b, c): print(a, b, c)\n>>> kwonly(1, 2, c=3)\n1 (2,) 3\n>>> kwonly(a=1, c=3)\n1 () 3\n>>> kwonly(1, 2, 3)\nTypeError: kwonly() missing 1 required keyword-only argument: 'c'\nIf we don’t need to collect arbitrary positionals, we can also use a bare *\ncharacter by itself in the arguments list to introduce keyword-only arguments.\nThis indicates that a function does not accept a variable-length argument list but\nstill expects all arguments following the * to be passed as keywords. In the next\nfunction, a may be passed by position or name again, but b and c must be\nkeywords, and no extra positionals are allowed:\n>>> def kwonly(a, *, b, c): print(a, b, c)\n>>> kwonly(1, c=3, b=2)\n1 2 3\n>>> kwonly(c=3, b=2, a=1)\n1 2 3\n>>> kwonly(1, 2, 3)",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 681,
      "chapter": null,
      "content": "TypeError: kwonly() takes 1 positional argument but 3 were given\n>>> kwonly(1)\nTypeError: kwonly() missing 2 required keyword-only arguments: 'b' and 'c'\nYou can still use defaults for keyword-only arguments, even though they appear\nafter the * in the function. In the following, a may be passed by name or\nposition, and b and c are optional but must be passed by keyword if used:\n>>> def kwonly(a, *, b='code', c='app'): print(a, b, c)\n>>> kwonly(1)\n1 code app\n>>> kwonly(1, c='hack')\n1 code hack\n>>> kwonly(a='py')\npy code app\n>>> kwonly(c=3, b=2, a=1)\n1 2 3\n>>> kwonly(1, 2)\nTypeError: kwonly() takes 1 positional argument but 2 were given\nIn fact, keyword-only arguments with defaults are optional, but those without\ndefaults effectively become required keywords for the function—like b in the\nfollowing:\n>>> def kwonly(a, *, b, c='hack'): print(a, b, c)\n>>> kwonly(1, b='code')\n1 code hack\n>>> kwonly(1, c='code')\nTypeError: kwonly() missing 1 required keyword-only argument: 'b'\n>>> kwonly(1, 2)\nTypeError: kwonly() takes 1 positional argument but 2 were given\nAs noted earlier, keyword-only arguments also allow defaults and nondefaults to\nbe mixed, unlike their otherwise more flexible cohorts coded before the optional\n*—an ostensible inconsistency we’ll return to later:\n>>> def kwonly(a, *, b=2, c, d=4): print(a, b, c, d)\n>>> kwonly(1, c=3)\n1 2 3 4\n>>> kwonly(1, c=5, b=6)",
      "content_length": 1384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 682,
      "chapter": null,
      "content": "1 6 5 4\n>>> kwonly(1)\nTypeError: kwonly() missing 1 required keyword-only argument: 'c'\n>>> kwonly(1, 2, 3)\nTypeError: kwonly() takes 1 positional argument but 3 were given\n>>> def badmix(b=2, c, d=5): …\nSyntaxError: parameter without a default follows parameter with a default\nFinally, note that keyword-only arguments must be specified after a single star,\nnot two—nothing can appear after the **args arbitrary-keywords form, and,\nunlike *, a ** can’t appear by itself in the arguments list. More generally,\nkeyword-only arguments must be coded between the * and the optional **, and\nan argument that appears before * is a possibly default argument that can be\npassed by position or keyword, not keyword-only:\n>>> def kwonly(a, **kargs, b, c):\nSyntaxError: arguments cannot follow var-keyword argument\n>>> def kwonly(a, **, b, c):\nSyntaxError: invalid syntax\n>>> def mixed(a, *b, **d, c=6):\nSyntaxError: arguments cannot follow var-keyword argument\nThese failures will make more sense after we get formal about argument\nordering rules later in this chapter. They may also appear to be worst cases in the\nartificial examples here, but they can come up in real practice, especially for\npeople who write libraries for other Python programmers to use—which leads to\nthe next point.\nWhy keyword-only arguments?\nSo why care about keyword-only arguments? In short, they make it easier to\nallow a function to accept both any number of positional arguments to be\nprocessed, and configuration options passed as keywords. While their use is\noptional, without keyword-only arguments extra work may be required to\nprovide defaults for such options and to verify that no superfluous keywords\nwere passed.\nImagine a function that processes a set of passed-in objects and allows a tracing\nflag to be passed:",
      "content_length": 1793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 683,
      "chapter": null,
      "content": "process(X, Y, Z)                    # Use flag's default\nprocess(X, Y, notify=True)          # Override flag default\nWithout keyword-only arguments we have to use both *args and **args and\nmanually inspect the keywords, but with keyword-only arguments less code is\nrequired. The following guarantees that no positional argument will be\nincorrectly matched against notify and requires that it be a keyword if passed:\ndef process(*args, notify=False): …\nSince we’re going to explore a more realistic example of this later in this\nchapter, in “Example: Rolling Your Own Print”, we’ll postpone the rest of this\nstory until then. For an additional example of keyword-only arguments in action,\nsee the upcoming iteration-options timing case study in “Timer Module: Take 2”.\nPositional-Only Arguments\nBeginning with Python 3.8, function definitions may also include a / in the\narguments list to designate that all arguments preceding it (i.e., to its left) must\nbe passed by position, not by keyword-argument name. Though arguably ad hoc\non first sighting in an arguments list, this notation was being used in\ndocumentation for built-in functions that did not accept keywords; making it\navailable to function coders as part of Python’s syntax was deemed a logical\nextension for library developers who may not want to expose argument names\nfor use by clients as keywords.\nTo demo, the following function specifies that a and b must be passed by\nposition, though c is more flexible:\n>>> def mostlypos(a, b, /, c): print(a, b, c)\n \n>>> mostlypos(1, 2, 3)\n1 2 3\n>>> mostlypos(1, 2, c=3)\n1 2 3\nPassing either of the first two arguments by keyword, however, fails:\n>>> mostlypos(1, b=2, c=3)",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 684,
      "chapter": null,
      "content": "TypeError: mostlypos() got some positional-only arguments passed as keyword \narguments: 'b'\n>>> mostlypos(c=3, b=2, a=1)\nTypeError: mostlypos() got some positional-only arguments passed as keyword \narguments: 'a, b'\nTo define a function that allows only positional arguments, simply code the slash\nat the end:\n>>> def allpos(a, b, c, /): print(a, b, c)\n>>> allpos(1, 2, 3)\n1 2 3\n>>> allpos(1, 2, c=3)\nTypeError: mostlypos() got some positional-only arguments passed as keyword …\nAs you should expect, the slash works the same in a lambda argument list:\n>>> f = lambda a, b, /, c: print(a, b, c)\n>>> f(1, 2, c=3)\n1 2 3\n>>> f(1, b=2, c=3)\nTypeError: <lambda>() got some positional-only arguments passed as keyword …\nAnd functions can combine positional- and keyword-only arguments to be as\nrigid as they wish:\n>>> def combo(a, b, /, *, c, d): print(a, b, c, d)\n>>> combo(1, 2, c=3, d=4)\n1 2 3 4\n>>> combo(1, 2, 3, 4)\nTypeError: combo() takes 2 positional arguments but 4 were given\n>>> combo(a=1, b=2, c=3, d=4)\nTypeError: combo() got some positional-only arguments passed as keyword …\nIt’s up to you to ponder whether or not use cases for this syntax justify its\nconvolution of function definitions that follows in the next section. Given that\nPython users somehow got by without it for over three decades, though, this\nseems a tough sell. For more on the rationale and usage of the positional-only\nslash, see Python’s standard manuals. Also watch for an example in Chapter 21",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 685,
      "chapter": null,
      "content": "that uses it to avoid name clashes—and may or may not be compelling.\nArgument Ordering: The Gritty Details\nSo far, we’ve been fairly loose about the rules surrounding argument-matching\ntools, because they don’t crop up very often in the simpler usage patterns of\ntypical code. In more sophisticated roles, though, you need to verify that your\ncode follows Python’s expectations. Now that we’ve seen all of its subjects, the\nordering rules for function arguments can finally be summarized in full. While\nfunction definitions and calls share some similar syntax, their rules are\ncompletely different, owing to their different roles. Let’s take a more formal look\nat both.\nDefinition Ordering\nIn function definitions, argument lists are enclosed in parentheses in a def\nstatement and coded before a colon in a lambda expression, but follow the same\nformat in both. In short, they consist of four optional parts that must appear in\nthe following order, where position means a simple value in calls, and keyword\nmeans a name=value pair:\n1. One or more arguments that must be passed by position only, followed\nby a single /\n2. Any number of arguments that can be passed by either position or\nkeyword\n3. A single * by itself or a single *name positional-argument collector,\noptionally followed by any number of arguments that must be passed by\nkeyword only\n4. A single **name keyword-argument collector\nIn all cases, individual arguments, including a bare / or *, are separated by\ncommas. In addition, any nonstarred argument name can have a\nname=expression default, but all names must have defaults after the first that\ndoes, up to the * (keyword-only argument runs following a star may freely mix",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 686,
      "chapter": null,
      "content": "default and nondefault names).\nInherent in this ordering, positional-only arguments must appear first, *name\nends positional-argument runs and collects unmatched positionals, and **name\nends the entire arguments list and collects unmatched keywords. As noted\nearlier, the **name collector’s keys retain the order in which keyword arguments\nwere passed to the function.\nFormal definition\nMore concisely, the ordering of arguments in function definitions can be defined\nas follows, where -or- is notation for a choice and [] encloses an optional part\n(neither is part of the actual code you type):\ndef name(arguments-list): statements\nlambda arguments-list: expression\narguments-list = \n         [positional-only-arguments, /]\n         [positional-or-keyword-arguments]\n         [* -or- *positional-collector, [keyword-only-arguments]] \n         [**keyword-collector]\nAs a real example, the following defines a function with all these parts in\nactions:\n>>> def f(a, /, b, c=3, *ps, e=4, **ks): \n        print(f'{a=}, {b=}, {c=}, {ps=}, {e=}, {ks=}')\n>>> f(0, 1, 2, 3, 4, e=5, f=6, g=7)\na=0, b=1, c=2, ps=(3, 4), e=5, ks={'f': 6, 'g': 7}\nBoundary cases\nAs consequences of the ordering in function definitions, / must precede a star,\nand the double-star keyword collector must be coded last:\n>>> def f(a, *ps, /, b): pass\nSyntaxError: / must be ahead of *\n>>> def f(a, **ks, b): pass\nSyntaxError: arguments cannot follow var-keyword argument",
      "content_length": 1437,
      "extraction_method": "Direct"
    },
    {
      "page_number": 687,
      "chapter": null,
      "content": "Though enforced separately from basic argument-ordering rules, once a default\nis used in a definition, all subsequent arguments must also have defaults—\nincluding those following the positional-only / delimiter:\n>>> def f(a, b, c=3, d, e): pass\nSyntaxError: parameter without a default follows parameter with a default\n>>> def f(a, b=2, /, c): pass\nSyntaxError: parameter without a default follows parameter with a default\nHowever, this constraint applies only to arguments preceding a * or *name—\ndefaults and nondefaults can be mixed freely in keyword-only arguments, which\nseems inconsistent, though a case for sanity could be made here on the grounds\nthat keyword-only arguments never match by position, which reduces ambiguity\nof defaults:\n>>> def f(a, *, x=1, y): ...      # OK: x and y must be keywords\n>>> def f(a=1, b, *, x=1): ...    # Not OK: a and b may be positional\nSyntaxError: parameter without a default follows parameter with a default\nNot shown here, def can also be preceded by a decorator (e.g., @value), and, as\nnoted earlier, any of its arguments may be followed by annotations (e.g.,\n:value)—extensions we’ll explore in the next chapter and later in this book.\nNeither of these impact argument ordering or matching, and neither work in\nlambda due to its limited syntax.\nCalls Ordering\nOn the other side of the fence, the syntax is similar but the rules differ. In\nfunction calls, all positional arguments must precede all keyword arguments, and\nany number of starred unpackings can be mixed in with individual values: one\nstar unpacks into multiple positional arguments, two unpacks into keywords, and\nall positional arguments and one-star unpackings must precede all two-star\nunpackings.\nIn more detail, function (and by extension, method) calls consist of three\noptional parts in the following order:",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 688,
      "chapter": null,
      "content": "1. Any combination of one or more expression positional arguments, and\n*expression iterable unpackings transformed into positional\narguments\n2. Any combination of one or more name=expression keyword\narguments, and *expression iterable unpackings transformed into\npositional arguments\n3. Any combination of one or more name=expression keyword\narguments, and **expression mapping unpackings transformed into\nkeyword arguments\nIn all cases, all arguments, starred or otherwise, are separated by commas.\nFormal definition\nMore concisely again, the ordering of arguments in function calls can be defined\nas follows, where function is an expression that evaluates to a callable object,\nand -or- and [] here again mean a choice and optional part, respectively\n(they’re not part of the code you type):\nfunction([positional-values -or- *iterable-positional-unpackings]\n         [keyword-arguments -or- *iterable-positional-unpackings]\n         [keyword-arguments -or- **mapping-keyword-unpackings])\nAs a concrete example, the following passes a variety of positional, keyword,\nand unpacking arguments (this code serves as a demo here, but is not exactly the\nsort of thing you should strive to craft in practice!):\n>>> def f(a, b, c, d, e, f, g, h, i): \n        print(a, b, c, d, e, f, g, h, i)\n>>> f(*[1], 2, *[3], 4, f=6, *[5], **dict(g=7), h=8, **{'i': 9})\n1 2 3 4 5 6 7 8 9\nBoundary cases\nAs one consequence of the argument ordering in calls, once you use a keyword\nargument, you can no longer use any unstarred positionals—all subsequent\narguments must also be keywords, or single or double stars:",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 689,
      "chapter": null,
      "content": ">>> f(1, 2, c=3, 4)\nSyntaxError: positional argument follows keyword argument unpacking\nThis is similar to defaults in definitions—but not exactly. Again, try not to\nconflate function definitions and calls. Despite their reuse of similar syntax, it\nhas very different roles and rules in these tools. The * and =, for example, are\nused for unpacking and keywords in calls, but mean collection and defaults in\ndefinitions.\nOnce you code a double star, both positionals and single stars are also out of the\ngame, though this seems inconsistent too—single stars can be freely mixed with\nkeyword arguments (which is what a double star unpacks into), and keyword\narguments cannot be mixed with positionals (which is what a single star unpacks\ninto):\n>>> f(1, 2, **{'d': 3}, 4)\nSyntaxError: positional argument follows keyword argument unpacking\n>>> f(1, 2, **{'d': 3}, *[4])\nSyntaxError: iterable argument unpacking follows keyword argument unpacking\n>>> f(1, 2, d=3, *[4])    # OK, but why? – like first error above\n>>> f(1, 2, d=3, 4)       # Not OK, but why? – like preceding line\nSyntaxError: positional argument follows keyword argument\nPerspective\nAnd if this is starting to make your head spin, it probably should. Python’s\nargument-matching rules have been accumulated over time to incorporate new\nconvolutions, and they are complex and perhaps even kludgy. The first rule of\nprogramming applies to function definitions and calls as everywhere else: keep it\nsimple, unless it has to be complex. If you have to agonize over argument-\nordering rules to understand code, it’s probably time to reevaluate priorities.\nExample: The min Wakeup Call\nOK—it’s time for something more realistic. To make the concepts here more\nconcrete, the rest of this chapter works through a set of examples that\ndemonstrate practical applications of argument-matching tools. First up is an\nexercise borrowed from live classes and used to rouse learners like you starting",
      "content_length": 1948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 690,
      "chapter": null,
      "content": "to succumb to the knottiness of argument rules.\nHere’s the problem statement: suppose you’re asked to code a function that is\nable to compute the minimum value from an arbitrary set of arguments, which\nmay be arbitrary sorts of objects. That is, the function should accept zero or\nmore arguments—as many as you wish to pass. Moreover, the function should\nwork for all kinds of Python object types: numbers, strings, lists, lists of lists,\nfiles, and even None. To keep this fair, you don’t need to support dictionaries or\nmixed nonnumeric types, because neither supports direct comparisons, per\nChapters 8 and 9.\nThe first requirement provides a natural example of how the * feature can be put\nto good use—we can handle arbitrary arguments by collecting them in a tuple,\nand stepping over each with a simple for loop. The second part of the problem\ndefinition is easy in Python: because nearly every object type supports\ncomparisons, we don’t have to specialize the function per type (an application of\npolymorphism); we can simply compare objects blindly and let Python worry\nabout what sort of comparison to perform according to the objects being\ncompared.\nFull Credit\nThe following script file, mins.py in Example 18-2, shows four ways to code this\noperation (some of which were suggested by students in a group exercise\ndesigned to prevent post-lunch napping):\nThe first function fetches the first argument from its args tuple, and\ntraverses the rest by slicing off the first (there’s no point in comparing\nthe first object to itself, especially if it might be a large structure).\nThe second version lets Python pick off the first and rest of the\narguments automatically, and so avoids an index and slice; the code is\nsimpler, and may be faster (though it would take many calls to matter).\nThe third converts from a tuple to a list with the built-in list call and\nemploys the list sort method: the first item has lowest value after an\nascending-value sort.",
      "content_length": 1960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 691,
      "chapter": null,
      "content": "The fourth sorts too, but skips the list conversion (and two lines) by\nusing the sorted built-in function.\nPython sorting tools are coded in C, so they can be quicker than the other\napproaches at times, but the linear scans of the first two techniques may often\nmake them faster.1\nExample 18-2. mins.py\n\"Find minimum value among all passed arguments of comparable types\"\ndef min1(*args):\n   res = args[0]\n   for arg in args[1:]:\n       if arg < res:\n           res = arg\n   return res\ndef min2(first, *rest):\n   for arg in rest:\n       if arg < first:\n           first = arg\n   return first\ndef min3(*args):\n   tmp = list(args)\n   tmp.sort()\n   return tmp[0]\ndef min4(*args):\n   return sorted(args)[0]\nfor func in (min1, min2, min3, min4):           # Test all 4 functions\n   print(func.__name__ + '...')\n   print(func(3, 4.0, 1, 2))                   # Mixed numerics\n   print(func('bb', 'aa'))                     # Strings: code points\n   print(func([2, 2], [1, 1], [3, 3]))         # Lists: recursive\n   print(func(*'hack'))                        # Unpacked characters\nThis script’s testing code uses the __name__ attribute we met earlier, along with\na for loop to run each function one at a time (remember, functions are objects\nthat work in a tuple too). All four solutions produce the same result when the file\nis run, so we’ll list just the first’s output here. Run this file live, or import it as a\nmodule and type a few calls to its function interactively to experiment with them\non your own (see Chapter 3 for tips on both modes):",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 692,
      "chapter": null,
      "content": "$ python3 mins.py\nmin1...\n1\naa\n[1, 1]\na\n…and the same for others…\nNotice that none of these four variants tests for the case where no arguments are\npassed in. They could, but there’s probably no point in doing so here—in all four\nsolutions, Python will automatically raise an exception to signal the error if no\narguments are sent. The first variant raises an exception when we try to fetch\nargument 0, the second when Python detects an argument list mismatch, and the\nthird and fourth when we try to return item 0 post sort.\nThis is exactly what we want to happen—because these functions support any\nobject (including None), there is no value that we could pass back to designate an\nerror, so we may as well let the exception be raised. There are exceptions to this\nexceptions rule (e.g., you might test for errors yourself if you’d rather avoid\nactions that run before reaching the code that triggers an error automatically).\nBut in general—and especially when errors aren’t common—it’s better to\nassume that arguments will work in your functions’ code, and let Python raise\nerrors for you when they do not.\nBonus Points\nYou can get bonus points here for changing these functions to compute the\narguments’ maximum, rather than minimum, value. This one’s trivial: the first\ntwo versions only require changing < to >, and the last two simply require that\nwe return item [−1] instead of [0]. For an extra point, be sure to mod the\nfunction name to “max” as well (though this part is strictly optional).\nTrue curve busters might also note that it’s possible to generalize a single\nfunction to compute either a minimum or a maximum value, by evaluating\ncomparison expression strings with a tool like the eval built-in function\n(described in Python’s library manual, and at various appearances here,\nespecially its note in Chapter 10), or by passing in an arbitrary comparison\nfunction. Example 18-3 shows how to implement the latter scheme for one of the",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 693,
      "chapter": null,
      "content": "coding options.\nExample 18-3. minmax.py\n\"Find minimum -or- maximum value of arguments\"\ndef minmax(test, *args):\n   res = args[0]\n   for arg in args[1:]:\n       if test(arg, res):\n           res = arg\n   return res\ndef lessthan(x, y): return x < y                # See also: lambda, eval\ndef grtrthan(x, y): return x > y\nprint(minmax(lessthan, 4, 2, 1, 5, 6, 3))       # Self-test code\nprint(minmax(grtrthan, 4, 2, 1, 5, 6, 3))\nRunning this script prints both minimum and maximum, per its self-test code at\nthe end:\n$ python3 minmax.py\n1\n6\nAgain, functions are just another kind of object, which allows them to be passed\ninto other functions as done here. To make this a max (or other) function, for\nexample, we simply pass in the right sort of test function to minmax. This may\nseem like extra work, but the main point of generalizing functions this way—\ninstead of cutting and pasting to change just a single character—is that we’ll\nonly have one version to change in the future, not two.\nThe Punch Line\nOf course, all this was just a coding exercise. There’s really no reason to write\nmin or max functions, because both are built-ins in Python! We met them briefly\nin Chapter 5 in conjunction with numeric tools, and again in Chapter 14 when\nexploring iteration contexts. The built-in versions work almost exactly like ours,\nbut they’re coded in C for optimal speed and accept either a single iterable or\nmultiple arguments. Still, though it’s superfluous in this context, the general\ncoding pattern we used here might be useful in other scenarios.",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 694,
      "chapter": null,
      "content": "Example: Generalized Set Functions\nOur next example also demos special argument-matching modes at work. At the\nend of Chapter 16, we wrote a function that returned the intersection of two\nsequences (really, it picked out items that appeared in both). Example 18-4 codes\nan augmented version that intersects an arbitrary number of sequences (one or\nmore) by using the argument-matching form *args to collect all the passed-in\narguments. Because the arguments come in as a tuple, we can process them in a\nsimple for loop. Just for fun, we’ll code a union function that also accepts an\narbitrary number of arguments to collect items that appear in any of the\noperands.\nExample 18-4. inter2.py\n\"\"\"\nImplement intersection and union for one or more arguments.\nInputs may be any sort of iterable that supports multiple in\ntests, and results are always lists.  This intersect avoids \nduplicates in results by in test, but may be slow: improve me.\n\"\"\"\ndef intersect(*args):\n   res = []\n   for x in args[0]:                    # Scan first sequence\n       if x in res: continue            # Skip duplicates in [0]\n       for other in args[1:]:           # For all other args\n           if x not in other: break     # Item in each one?\n       else:                            # No: break out of loop\n           res.append(x)                # Yes: add items to end\n   return res\ndef union(*args):\n   res = []\n   for seq in args:                     # For all args\n       for x in seq:                    # For all in this arg\n           if not x in res:\n               res.append(x)            # Add new items to result\n   return res\nBecause these tools are potentially worth reusing (and are too big to retype\ninteractively), we’ll store the functions in a module file called inter2.py. Again,\nif you’re unsure about how modules and imports work, see the introduction in\nChapter 3, or stay tuned for in-depth coverage in Part V. This chapter’s module\nusage is simple, but per Chapter 3, be sure to launch your REPL in the same",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 695,
      "chapter": null,
      "content": "folder as the file, so imports can find it.\nLike Chapter 16’s original intersect, both of the functions in this module work\non any kind of sequence. Here they are live at the REPL, processing strings,\nmixed types, and more than two sequences:\n$ python3\n>>> from inter2 import intersect, union\n>>> s1, s2, s3 = 'HACKK', 'CODE', 'CASH'\n>>> intersect(s1, s2), union(s1, s3)           # Two operands\n(['C'], ['H', 'A', 'C', 'K', 'S'])\n \n>>> intersect([1, 2, 3, 4], (1, 4))            # Mixed types\n[1, 4]\n \n>>> intersect(s1, s2, s3)                      # Three operands\n['C']\n \n>>> union(s1, s2, s3)\n['H', 'A', 'C', 'K', 'O', 'D', 'E', 'S']\nTesting the Code\nTo test more thoroughly, the following continues our REPL session to code a\nfunction that applies the two tools to arguments in different orders using a simple\nshuffling technique that we saw in Chapter 13—it slices to move the first to the\nend on each loop, uses a * to unpack arguments, and sorts so results are\ncomparable. Notice that arguments for the function are sent as a sequence (not\ndiscrete items), and the trace configuration option is keyword-only here:\n>>> def tester(func, items, *, trace=True):\n       for i in range(len(items)):\n           items = items[1:] + items[:1]         # Move front item to back\n           if trace: print(items)\n           print(sorted(func(*items)))           # Test with reordered items\n>>> tester(intersect, (s1, s2, s3))              # Use strings from prior listing\n('CODE', 'CASH', 'HACKK')\n['C']\n('CASH', 'HACKK', 'CODE')\n['C']\n('HACKK', 'CODE', 'CASH')\n['C']",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 696,
      "chapter": null,
      "content": ">>> tester(union, (s1, s2, s3), trace=False)\n['A', 'C', 'D', 'E', 'H', 'K', 'O', 'S']\n['A', 'C', 'D', 'E', 'H', 'K', 'O', 'S']\n['A', 'C', 'D', 'E', 'H', 'K', 'O', 'S']\n>>> tester(intersect, (s1, s2, s3), trace=False)\n['C']\n['C']\n['C']\nTwo context notes here: first, because duplicates won’t appear in these\nintersection and union functions, they qualify as set operations mathematically,\nbut may not be optimal in term of speed. Still, there’s not much point in\nimproving this demo’s code—intersection and union, like min and max, are built-\nin operations today: the set object we explored in Chapter 5 does intersection\nand union with & and |, and has methods that take multiple operands too. Hence,\noptimizing this code is left as suggested exercise, but see inter3.py in the\nexamples package for pointers.\nSecond, the argument scrambling in the tester here doesn’t generate all possible\nargument orders (that would require a full permutation, and 6 orderings for 3\narguments), but suffices to check if argument order impacts results. As a\npreview, though, the tester would be simpler and more flexible if it delegated\nscrambling to a reusable function. Watch for this revision in Chapter 20, after\nwe’ve explored how to code user-defined generators. We’ll also recode set tools\none last time in Chapter 32 and a solution to a Part VI exercise, as classes that\nadd them to the list object as methods.\nExample: Rolling Your Own Print\nTo close out the chapter, let’s look at one last example of argument matching at\nwork: this section uses both * and ** arbitrary-arguments collectors up front, and\nkeyword-only arguments later, to emulate most of what Python’s print function\ndoes. Like the preceding examples, there’s no urgent reason to code tools that\nPython provides. Also like the others, though, this is both instructive and may be\na basis for custom variants of built-in tools.\nFor both purposes, the module file in Example 18-5 does roughly the same job",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 697,
      "chapter": null,
      "content": "as print in a small amount of reusable and modifiable code, by building and\nrouting the print string per configuration arguments.\nExample 18-5. print3.py\nr\"\"\"\nEmulate most of the Python 3.X print function as customizable code.\nCall signature: print3(*args, sep=' ', end='\\n', file=sys.stdout).\n\"\"\"\nimport sys\ndef print3(*args, **kargs):\n   sep  = kargs.get('sep', ' ')            # Keyword arg defaults\n   end  = kargs.get('end', '\\n')\n   file = kargs.get('file', sys.stdout)\n   output = ''                             # Build+print a string\n   first  = True\n   for arg in args:\n       output += ('' if first else sep) + str(arg)\n       first = False\n   file.write(output + end)\nNotice that this module’s docstring uses a raw string to retain its backslash for\nhelp (per Chapter 15). Also note that this module’s function need not be called\n“print3” because “print” is a built-in but not a reserved word, but using a\ndifferent name avoids inadvertently hiding the built-in. To test it, import this into\nanother file or the interactive prompt, and use it like the print built-in.\nExample 18-6 codes a test script that imports our printer as a demo.\nExample 18-6. test-print3.py\nfrom print3 import print3\nprint3(1, 2, 3)                         # Defaults\nprint3(1, 2, 3, sep='')                 # Suppress separator\nprint3(1, 2, 3, sep='...')              # Custom separator\nprint3(1, [2], (3,), sep='...')         # Various object types\nprint3(4, 5, 6, sep='', end='')         # Suppress newline\nprint3(7, 8, 9)                         # Finish line\nprint3()                                # Blank line\nimport sys\nprint3(1, 2, 3, sep='?', end='.\\n', file=sys.stderr)    # Redirect to stream\nprint3('LP6E was here', file=open('log.txt', 'w'))      # Redirect to a file\nprint3(open('log.txt').read())",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 698,
      "chapter": null,
      "content": "When this is run, our print3 produces the same results as the print built-in.\nFine points here: stderr goes to your console by default, and it’s OK to use a\ndash in this script’s name because it’s run, not imported (again, we’ll be focusing\non such module details in this book’s next part):\n$ python3 test-print3.py\n1 2 3\n123\n1...2...3\n1...[2]...(3,)\n4567 8 9\n1?2?3.\nLP6E was here\nAs usual, the generality of its toolset allows us to prototype or develop concepts\nin the Python language itself. In this case, argument-matching tools are as\nflexible in Python code as they are in Python’s internal implementation.\nUsing Keyword-Only Arguments\nOur print emulator works, but has a minor flaw baked in: it assumes that all\npositional arguments are to be printed, and all keywords are for options only.\nAny extra keyword arguments are silently ignored, and neither printed nor\nreported. A call like the following, for instance, will ignore the extra—and likely\nerroneous—sap argument:\n$ python3\n>>> from print3 import print3\n>>> print3(3.12, 'py', sap='@')\n3.12 py\nIt may not make sense to print superfluous keywords, but we can detect them\nmanually by using dict.pop() to delete fetched keywords, and checking if the\ndictionary is not empty when we’re done. The version in Example 18-7 does—\nit’s equivalent to the original, but triggers a built-in exception with a raise\nstatement when unexpected keyword arguments are sent by a call (this is partly\npreview: we’ll study exceptions and raise in depth in Part VII).\nExample 18-7. print3_pops.py",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 699,
      "chapter": null,
      "content": "\"Use keyword-collector arguments with deletion and defaults\"\nimport sys\ndef print3(*args, **kargs):\n   sep  = kargs.pop('sep', ' ')\n   end  = kargs.pop('end', '\\n')\n   file = kargs.pop('file', sys.stdout)\n   if kargs: raise TypeError(f'extra keywords: {kargs}')\n   output = ''\n   first  = True\n   for arg in args:\n       output += ('' if first else sep) + str(arg)\n       first = False\n   file.write(output + end)\nNotice that this file’s name uses an underscore instead of a dash: because it’s to\nbe imported, its name must follow the rules for variables of Chapter 11. This\nversion works as before, but it now catches extraneous keyword arguments:\n>>> from print3_pops import print3\n>>> print3(3.12, 'py', sep='@')\n3.12@py\n>>> print3(3.12, 'py', sap='@')\nTypeError: extra keywords: {'sap': '@'}\nIt’s OK to reimport the same print3 name from a different file here: this simply\nreplaces the prior version just like reassigning any other variable (more on this\nlater in this book). That being coded, this example could also use keyword-only\narguments to automatically validate configuration arguments, as the final variant\nin Example 18-8 illustrates.\nExample 18-8. print3_kwonly.py\n\"Use keyword-only arguments to emulate print\"\nimport sys\ndef print3(*args, sep=' ', end='\\n', file=sys.stdout):\n   output = ''\n   first  = True\n   for arg in args:\n       output += ('' if first else sep) + str(arg)\n       first = False\n   file.write(output + end)\nThis version works the same way for valid calls, but catches invalid keywords\nwith keyword-only arguments instead of manual code, and is a prime example of",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 700,
      "chapter": null,
      "content": "how keyword-only arguments can address coding needs:\n>>> from print3_kwonly import print3\n>>> print3(3.12, 'py', sep='@')\n3.12@py\n>>> print3(3.12, 'py', sap='@')\nTypeError: print3() got an unexpected keyword argument 'sap'\nGiven that this version of the function also requires four fewer lines of code than\nits predecessor, keyword-only arguments can simplify a specific category of\nfunctions that accept both arguments and options. A similar case can be made for\npositional-inly arguments versus manual code, but it’s more obscure, and this\nchapter has run out of space. For another example of keyword-only arguments at\nwork, stay tuned for the iteration-timing case study in Chapter 21.\nAnd for more inspiration, also see the sidebar “Why You Will Care: Customizing\nopen”. Much as we did there, our print emulator could be assigned to\nbuiltins.print to replace the built-in with our custom version everywhere in a\nprogram. There’s no reason to do that for a version that’s the same, of course, but\nthis technique can be used to install a replacement printer that mods or extends\nthe built-in (e.g., with logging).",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 701,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we studied the second of two key concepts related to functions:\nthe arguments used to send objects to a function. As we saw, arguments are\npassed to a function by assignment, which means by object reference (which\nreally means by pointer), and are open to the usual side effects for shared\nmutable objects, desired or not. We also studied some advanced extensions that\ngeneralize argument matching, including default and keyword arguments, tools\nfor collecting and unpacking arbitrarily many arguments, and keyword- and\npositional-only arguments. Finally, we explored a few larger examples that\nemployed argument tools, and previewed module topics of this book’s next part.\nThe next chapter continues our look at functions with a grab bag of more\nadvanced function-related ideas: function annotations, recursion, and more on\nlambda and functional tools such as map and filter. Many of these concepts\nstem from the fact that functions are normal objects in Python, and so support\nflexible processing tools and modes. Before diving into those topics, however,\ntake this chapter’s quiz to review the argument ideas we’ve studied here.\nTest Your Knowledge: Quiz\nThis quiz asks you to trace through examples of function definitions and calls to\npredict their outputs. Try to work out the answers on your own before resorting\nto cut and paste in a REPL:\n1. What is the output of the following code, and why?\n>>> def func(a, b=4, c=5):\n        print(a, b, c)\n>>> func(1, 2)\n2. What is the output of this code, and why?\n>>> def func(a, b, c=5):",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 702,
      "chapter": null,
      "content": "print(a, b, c)\n>>> func(1, c=3, b=2)\n3. How about this code: what is its output, and why?\n>>> def func(a, *pargs):\n        print(a, pargs)\n>>> func(1, 2, 3)\n4. What does this code print, and why?\n>>> def func(a, **kargs):\n        print(a, kargs)\n>>> func(a=1, c=3, b=2)\n5. What gets printed by this, and why?\n>>> def func(a, b, c=3, d=4): print(a, b, c, d)\n>>> func(1, *(5, 6))\n6. One last time: what is the output of this code, and why?\n>>> def func(a, b, c): \n        a = 2; b[0] = 'x'; c['a'] = 'y'\n>>> L=1; M=[1]; N={'a': 0}\n>>> func(L, M, N)\n>>> L, M, N\nTest Your Knowledge: Answers",
      "content_length": 587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 703,
      "chapter": null,
      "content": "1. The output here is 1 2 5, because 1 and 2 are passed to a and b by\nposition, and c is omitted in the call and hence defaults to 5.\n2. The output this time is 1 2 3: 1 is passed to a by position, and b and c\nare passed 2 and 3 by name (the left-to-right order doesn’t matter when\nkeyword arguments are used like this).\n3. This code prints 1 (2, 3), because 1 is passed to a and the *pargs\ncollects the remaining positional arguments into a new tuple object. We\ncan step through the extra positional arguments tuple with any iteration\ntool (e.g., for arg in pargs: …).\n4. This time the code prints 1 {'c': 3, 'b': 2}, because 1 is passed to\na by name and the **kargs collects the remaining keyword arguments\ninto a dictionary. We could step through the extra keyword arguments\ndictionary by key with any iteration tool (e.g., for key in kargs: …).\nNote that the order of the dictionary’s keys reflects the order in which\nkeyword arguments are passed, in recent Pythons.\n5. The output here is 1 5 6 4: the 1 matches a by position, 5 and 6 match\nb and c by * positional unpacking (6 overrides c’s default), and d\ndefaults to 4 because it was not passed a value.\n6. This displays (1, ['x'], {'a': 'y'})—the first assignment in the\nfunction doesn’t impact the caller, but the second two do because they\nchange passed-in mutable objects in place.\nWHY YOU WILL CARE: KEYWORD ARGUMENTS\nAs you can probably tell, some argument-matching mode combos can be\ncomplex. They are also largely optional in your code; in fact, you can get by\nwith just simple positional matching, and it’s probably a good idea to do so\nwhen you’re starting out. However, because many Python tools make good\nuse of them, some general knowledge of these modes is important.\nFor example, keyword arguments play a key role in tkinter, the de facto\nstandard GUI API for Python. We touch on tkinter only briefly at various\npoints in this book, but in terms of its call patterns, keyword arguments set",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 704,
      "chapter": null,
      "content": "configuration options when GUI components are built. For instance, a call of\nthe form:\nfrom tkinter import Button\nwidget = Button(text='Press me', command=someFunction)\ncreates a new button and specifies its text displayed in the GUI and callback\nfunction run on a press, using the text and command keyword arguments.\nSince the number of configuration options for a widget can be large,\nkeyword arguments let you pick and choose which to apply. Without them,\nyou might have to either list all the possible options by position or hope for a\njudicious positional-argument defaults protocol that would handle every\npossible option arrangement.\nMany built-in functions in Python expect us to use keywords for usage-mode\noptions as well, which may or may not have defaults. As we learned in\nChapter 8, for instance, the sorted built-in:\nsorted(iterable, key=None, reverse=False)\nexpects us to pass an iterable object to be sorted, but also allows us to pass in\noptional keyword arguments to specify both a value-transform function and\na reversal flag, which default to None and False, respectively. Since we\nnormally don’t use these options, they may be omitted to apply defaults.\nAs we’ve also seen, the dict, str.format, and print calls accept keywords\nas well—other usages we had to introduce in earlier chapters because of\ntheir forward dependence on argument-passing modes we’ve finally studied\nhere (alas, those who change Python already know Python!).\n1  Actually, it’s complicated. CPython’s sort (used by both list.sort and sorted) is coded in C and\nuses a heavily optimized algorithm that attempts to take advantage of partial ordering in the items to\nbe sorted. Still, sorting is an inherently busy operation (it must chop up the sequence and put it back\ntogether many times), and the other versions simply perform one linear left-to-right scan. This\nsuggests that sorting may be quicker if the arguments are partially ordered, but is likely to be slower\notherwise. Even so, Python performance changes regularly; the fact that sorting is implemented in the\nC language can help greatly; and the speed difference may not matter in many programs. For an exact\nanalysis, you should time the alternatives with the time or timeit modules—you’ll see how soon in",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 705,
      "chapter": null,
      "content": "Chapter 21, but file mins-timings.txt in the examples package demos the idea if you can’t wait. The\ngist: in CPython, the nonsort mins are faster for random arguments, but slower for ordered—today!",
      "content_length": 197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 706,
      "chapter": null,
      "content": "Chapter 19. Function Odds and\nEnds\nThis chapter presents a medley of function-related topics: recursive functions;\nfunction attributes, annotations, and decorations; and more on both the lambda\nexpression and functional-programming tools such as map and filter. These are\nall somewhat advanced tools that, depending on your job description, you may\nnot encounter on a regular basis. Because of their roles in some domains, though,\na basic understanding can be useful. lambda, for instance, makes regular\nappearances in GUIs, and functional programming techniques have grown\ncommon in Python code.\nSome of the art of using functions lies in the interfaces between them, so we will\nalso explore some general function design principles here. The next chapter\ncontinues the advanced themes here with an exploration of generator functions\nand expressions and a revival of list comprehensions in the context of the\nfunctional tools we will study here.\nFunction Design Concepts\nNow that we’ve studied function essentials in Python, let’s open this chapter\nwith some perspective. When you start using functions in earnest, you’re faced\nwith choices about how to glue components together—for instance, how to\ndecompose a task into purposeful functions (known as cohesion), and how your\nfunctions should communicate (called coupling). You also need to take note of\nthe size of your functions because it directly impacts code usability. Some of this\nfalls into the category of structured analysis and design, but it applies to Python\ncode as to any other.\nWe explored some ideas related to function and module coupling in Chapter 17\nwhen studying scopes, but here is a review of a few general guidelines for\nreaders new to function design principles:",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 707,
      "chapter": null,
      "content": "Coupling: use arguments for inputs and return for outputs.\nGenerally, you should strive to make a function independent of the\nworld outside of it. Arguments and return statements are often the best\nways to isolate external dependencies to a small number of well-known\nplaces in your code.\nCoupling: use global variables only when truly necessary. As we’ve\nseen, global variables (i.e., names in the enclosing module) are usually a\npoor way for functions to communicate. They can create dependencies\nand timing issues that make programs difficult to debug, change, and\nreuse.\nCoupling: don’t change mutable arguments unless the caller\nexpects it. As we’ve also seen, functions can change parts of passed-in\nmutable objects, but as with global variables, this creates a tight\ncoupling between the caller and callee, which can make a function too\nspecific and brittle.\nCohesion: each function should have a single, unified purpose.\nWhen designed well, each of your functions should do one thing—\nsomething you can summarize in a simple declarative sentence. If that\nsentence is very broad (e.g., “this function implements my whole\nprogram”) or contains lots of conjunctions (e.g., “this function gives\nemployee raises and submits a pizza order”), you might want to think\nabout splitting it into separate and simpler functions. Otherwise, there is\nno way to reuse the code of the individual steps embedded in the\nfunction.\nSize: each function should be relatively small. This naturally follows\nfrom the preceding goal, but if your functions start spanning multiple\npages on your display, it’s probably time to split them. Especially given\nthat Python code is so concise to begin with, a long or deeply nested\nfunction is often a symptom of design problems. Keep it simple, and\nkeep it short.\nCoupling: avoid changing variables in another module file directly.\nWe also introduced this concept in Chapter 17, and we’ll revisit it in the\nnext part of the book when we focus on modules. For reference, though,",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 708,
      "chapter": null,
      "content": "remember that changing variables across file boundaries sets up a\ncoupling between modules similar to how global variables couple\nfunctions—the modules become difficult to understand and reuse\nseparately. Use accessor functions whenever possible, instead of direct\nassignment statements.\nFigure 19-1 summarizes the ways functions can talk to the outside world; inputs\nmay come from items on the left side, and results may be sent out in any of the\nforms on the right. Nonlocals might belong in this sketch too, but they’re mostly\na state-retention tool in the same category as other local variables. Despite the\narray of options, good function designs prefer to use only arguments for inputs\nand return statements for outputs, whenever possible.\nFigure 19-1. Function execution environment\nOf course, there are plenty of exceptions to the preceding design rules, including\nsome related to Python’s OOP support. As you’ll see in Part VI, Python classes\ndepend on changing a passed-in mutable object—class functions set attributes of\nan automatically passed-in argument called self to change per-object state\ninformation (e.g., self.edition=6). Moreover, if classes are not used, global\nvariables are a straightforward way for functions in modules to retain single-\ncopy state between calls. Side effects are usually dangerous only if they’re\nunexpected.\nIn general though, you should strive to minimize external dependencies in\nfunctions and other program components. The more self-contained a function is,",
      "content_length": 1505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 709,
      "chapter": null,
      "content": "the easier it will be to understand, reuse, and modify. Making code as\nfreestanding as possible is especially important when functions go multilevel\nwith recursion, per the next section.\nRecursive Functions\nWe mentioned recursion in relation to comparisons of core types in Chapter 9.\nWhile discussing scope rules near the start of Chapter 17, we also briefly noted\nthat Python supports recursive functions—functions that call themselves either\ndirectly or indirectly in order to loop. In this section, we’ll explore what this\nlooks like in our functions’ code.\nRecursion is a somewhat advanced topic, and it’s relatively uncommon to see in\nPython, partly because Python’s procedural shed includes simpler looping tools.\nStill, it’s a useful technique to know about, as it allows programs to traverse\nstructures that have arbitrary and unpredictable shapes and depths—planning\ntravel routes, analyzing language, and crawling links on the web, for example.\nRecursion is even an alternative to simple loops and iterations, though not\nnecessarily the simplest or most efficient one.\nSummation with Recursion\nLet’s turn to some examples. To sum a list (or other sequence) of numbers, we\ncan either use the built-in sum function or write a more custom version of our\nown. Example 19-1 shows what a custom summing function might look like\nwhen coded with recursion.\nExample 19-1. mysum.py\ndef mysum(L):\n   if not L:\n       return 0\n   else:\n       return L[0] + mysum(L[1:])       # Call myself recursively\nTo use, either add self-test code to the bottom of this file and run it as a script, or\nimport it as a module and test at the REPL (again, the file may need to be in the\nfolder where you’re working either way, per Chapter 3). With the latter:\n>>> from mysum import mysum              # Import file as a module in a REPL",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 710,
      "chapter": null,
      "content": ">>> mysum([1, 2, 3, 4, 5])               # Sum all the numbers in any sequence\n15\nAt each level, this function calls itself recursively to compute the sum of the rest\nof the list, which is later added to the item at the front. This recursive loop ends\nand zero is returned when the list becomes empty. When using recursion like\nthis, each open level of call to the function has its own copy of the function’s\nlocal scope on the runtime call stack. Here, that means L is different in each\nlevel, so each remembers its own segment of the list.\nIf this is difficult to understand (and it often is for new programmers), try adding\na print of L to the function and run it again, to trace the current list at each call\nlevel; here’s the required mod pasted at the REPL for variety:\n>>> def mysum(L):\n        print(L)                         # Trace recursive levels\n        if not L:                        # L shorter at each level\n            return 0\n        else:\n            return L[0] + mysum(L[1:])\n>>> mysum([1, 2, 3, 4, 5])\n[1, 2, 3, 4, 5]\n[2, 3, 4, 5]\n[3, 4, 5]\n[4, 5]\n[5]\n[]\n15\nAs you can see, the list to be summed grows smaller at each recursive level, until\nit becomes empty—the termination of the recursive loop. The sum is then\ncomputed as the recursive calls unwind on returns.\nCoding Alternatives\nInterestingly, we can use Python’s if/else ternary expression (described in\nChapter 12) to save some code real estate here. We can also generalize for any\nsummable type (which is easier if we assume at least one item in the input) and\nuse extended-unpacking assignment to make the first/rest unpacking simpler (as\ncovered in Chapter 11). Example 19-2 collects all three of these mods, ready to",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 711,
      "chapter": null,
      "content": "be run in a file or pasted into a REPL.\nExample 19-2. mysum_alts.py\ndef mysum(L):\n   return 0 if not L else L[0] + mysum(L[1:])           # Use ternary expression\ndef mysum(L):\n   return L[0] if len(L) == 1 else L[0] + mysum(L[1:])  # Any type, assume one+\ndef mysum(L):\n   first, *rest = L\n   return first if not rest else first + mysum(rest)    # Use extended unpacking\nWhen tested individually, all three of these alternatives handle numeric\nsummation the same as the original:\n>>> mysum([1, 2, 3, 4, 5])\n15\nUniquely, the latter two fail for empties (e.g., mysum([])), but handle sequences\nof any object type that supports +, not just numbers (for strings, the effect is\nsimilar to ''.join(L)):\n>>> mysum(('h', 'a', 'c', 'k'))          # The last two fail on mysum([])\n'hack'\n>>> mysum(['hack', 'app', 'code'])       # But they support nonnumeric types\n'hackappcode'\nRun some tests on your own for more insight. If you study these three variants,\nyou’ll also find that:\nThe latter two work on a single string argument (e.g., mysum('hack')),\nbecause strings are sequences of one-character strings (though this use\ncase isn’t very useful: you get back the same string).\nThe third variant also works on arbitrary iterables, including open input\nfiles (mysum(open(name))), but the others’ indexing generally fails on\nnonsequences (see Chapter 14 for extended-unpacking demos).\nYou may also notice that the third variant’s unpacking assignment is similar to a\n* collector in a function header, and it’s tempting to recode it as such. This won’t",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 712,
      "chapter": null,
      "content": "quite work, though, because it would expect individual arguments, not a single\niterable—unless we also star both the top-level input and recursive call. Here’s\nthe end result, though by summing discrete arguments, it solves a different\nproblem than both the prior versions and built-in sum:\n>>> def mysum(first, *rest):\n        return first if not rest else first + mysum(*rest)\n>>> mysum(*[1, 2, 3, 4, 5])\n15\n>>> mysum(*'hack')\n'hack'\nFinally, bear in mind that recursion can be either direct, as in the examples so\nfar, or indirect, as in the following—a function that calls another function, which\ncalls back to its caller. The net effect is the same, though there are two function\ncalls at each level instead of one:\n>>> def mysum(L):\n        if not L: return 0\n        return nonempty(L)                  # Call a function that calls me\n>>> def nonempty(L):\n        return L[0] + mysum(L[1:])          # Indirectly recursive\n>>> mysum([1.1, 2.2, 3.3, 4.4])\n11.0\nLoop Statements Versus Recursion\nThough recursion works for summing in the prior sections’ examples, it’s\nprobably overkill in this context. In fact, recursion is not used nearly as often in\nPython as in more esoteric languages like Prolog or Lisp, because Python\nemphasizes simpler procedural statements like loops, which are usually more\nnatural. The while, for example, often makes things more concrete, and it\ndoesn’t require that a function be defined to allow recursive calls:\n>>> L = [1, 2, 3, 4, 5]\n>>> tot = 0\n>>> while L:\n        tot += L[0]\n        L = L[1:]",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 713,
      "chapter": null,
      "content": ">>> tot\n15\nBetter yet, for loops iterate for us automatically, making recursion largely\nextraneous in many cases (and, in all likelihood, less efficient in terms of\nmemory space and execution time):\n>>> L = [1, 2, 3, 4, 5]\n>>> tot = 0\n>>> for x in L: tot += x\n>>> tot\n15\nWith looping statements, we don’t require a fresh copy of a local scope on the\ncall stack for each iteration, and we avoid the speed costs associated with\nfunction calls in general. (Stay tuned for Chapter 21’s timer case study for ways\nto compare the execution times of alternatives like these.)\nHandling Arbitrary Structures\nOn the other hand, recursion—or equivalent and explicit stack-based algorithms\nwe’ll explore shortly—can be required to traverse arbitrarily shaped structures.\nAs a simple example of recursion’s role in this context, consider the task of\ncomputing the sum of all the numbers in a nested sublists structure like this:\n[1, [2, [3, 4], 5], 6, [7, 8]]                  # Arbitrarily nested sublists\nNeither our prior summers nor simple looping statements will work here because\nthis is not a linear iteration. Nested looping statements do not suffice either—\nbecause the sublists may be nested to arbitrary depth and in an arbitrary shape,\nthere’s no way to know how many nested loops to code to handle all cases.\nInstead, the function in Example 19-3 accommodates such general nesting by\nusing recursion to visit sublists along the way.\nExample 19-3. sumtree.py\ndef sumtree(L, trace=False):\n   tot = 0\n   for x in L:                                 # For each item at this level",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 714,
      "chapter": null,
      "content": "if not isinstance(x, list):\n           tot += x                            # Add numbers directly\n           if trace: print(x, end=', ')\n       else:\n           tot += sumtree(x, trace)            # Recur for sublists\n   return tot\nIn this file’s function, each recursive level runs a for loop to add numbers, or\nrecur into sublists to open new levels. Recall from Chapter 9 that isinstance\ncompares object types; it’s used here to detect nested sublists.\nThis code is also instrumented to trace items as they are added to the total: if\ntrace is passed a true value, you can see how the object is scanned left to right.\nBecause it also steps down into sublists with recursion along the way, though,\nthe traversal is really both horizontal and vertical:\n>>> from sumtree import sumtree\n>>> sumtree([1, [2, [3, 4], 5], 6, [7, 8]])\n36\n>>> sumtree([1, [2, [3, 4], 5], 6, [7, 8]], trace=True)\n1, 2, 3, 4, 5, 6, 7, 8, 36\nTesting with a separate script\nAt this point, we could test other cases by typing them interactively or by adding\ncode to the bottom of the file, but you’re probably starting to see that this can be\na bit limiting. Instead, the script in Example 19-4 makes the process automatic\nand easily repeatable. As a bonus, it can be used for other summers we’ll code in\na moment.\nExample 19-4. sumtree_tester.py\ntests = (\n[1, [2, [3, 4], 5], 6, [7, 8]],      # Mixed nesting => 36\n[1, [2, [3, [4, [5]]]]],             # Right-heavy nesting => 15\n[[[[[1], 2], 3], 4], 5])             # Left-heavy nesting => 15\ndef tester(sumtree, trace=True):\n   for test in tests:\n       print(sumtree(test, trace))\nTo use this tester, simply import both summer and tester, and pass the former to\nthe latter. When run, our summer prints numbers as added with the final sum at\nthe end, for each of three canned tests:",
      "content_length": 1807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 715,
      "chapter": null,
      "content": ">>> from sumtree import sumtree              # Get the summer\n>>> from sumtree_tester import tester        # Get the tester\n>>> tester(sumtree)                          # Run the tester on the summer\n1, 2, 3, 4, 5, 6, 7, 8, 36\n1, 2, 3, 4, 5, 15\n1, 2, 3, 4, 5, 15\nWithin tester, sumtree refers to the summer function passed into it. Again,\nbecause functions are objects, passing them around this way is natural, and\nmakes code flexible.\nRecursion versus queues and stacks\nIt sometimes helps recursion newcomers to understand that internally, Python\nimplements recursion by pushing information on a call stack at each recursive\ncall, so it remembers where it must return and continue later. In fact, it’s\ngenerally possible to implement recursive-style procedures without recursive\ncalls, by using an explicit stack or queue of your own to keep track of remaining\nsteps.\nFor instance, Example 19-5 computes the same sums as the prior example, but\nuses an explicit list to schedule when it will visit items in the subject, instead of\nissuing recursive calls. The item at the front of the list is always the next to be\nprocessed and summed.\nExample 19-5. sumtree_queue.py\ndef sumtree(L, trace=False):                     # Breadth-first, explicit queue\n   tot = 0\n   items = list(L)                              # Start with copy of top level\n   while items:\n       front = items.pop(0)                     # Fetch/delete front item\n       if not isinstance(front, list):\n           tot += front                         # Add numbers directly\n           if trace: print(front, end=', ')\n       else:\n           items.extend(front)                  # <== Append all in nested list\n   return tot\nTechnically, this code traverses the list in breadth-first fashion (across before\ndown), because it adds nested lists’ contents to the end of the list—forming a\nFIFO (first-in-first-out) queue. The net effect sums by horizontal levels. To test,\nwe can either import and use the new summer directly, or route it to the",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 716,
      "chapter": null,
      "content": "Example 19-4 tester to be exercised automatically with tracing:\n>>> from sumtree_queue import sumtree            # Get the new summer\n>>> sumtree([1, [2, [3, 4], 5], 6, [7, 8]])\n36\n>>> from sumtree_tester import tester            # Unless already imported \n>>> tester(sumtree)                              # Run tester on _this_ summer\n1, 6, 2, 5, 7, 8, 3, 4, 36\n1, 2, 3, 4, 5, 15\n5, 4, 3, 2, 1, 15\nFine points: we don’t have to reimport the tester again if it’s already been\nimported in this session, and importing the same-named summer just works—the\nnew summer’s filename makes it unique, and sumtree is always the latest\nversion imported if you import more than one, because imports assign names\n(see Chapter 18’s print emulators for another example of this pattern at work,\nand watch for more on imports in this book’s next part).\nMore importantly, notice how the order in which numbers are visited here is\ndifferent than in the original recursive-call version, due to the breadth-first\nqueue. Trace through the tester’s tests to see how this pans out.\nIf we instead want to emulate the traversal of the recursive-call version more\nclosely, we can change this code to perform depth-first traversal (down before\nacross) simply by adding the contents of nested lists to the front of the list—\nforming a last-in-first-out (LIFO) stack. Example 19-6 makes the required mods,\nbut the only way it differs from the breadth-first version is the line that adds to\nthe front instead of the end, marked with <== in a comment.\nExample 19-6. sumtree_stack.py\ndef sumtree(L, trace=False):                     # Depth-first, explicit stack\n   tot = 0\n   items = list(L)                              # Start with copy of top level\n   while items:\n       front = items.pop(0)                     # Fetch/delete front item\n       if not isinstance(front, list):\n           tot += front                         # Add numbers directly\n           if trace: print(front, end=', ')\n       else:\n           items[:0] = front                    # <== Prepend all in nested list\n   return tot\nAs before, we can use this function directly, or pass it to the same tester; its file",
      "content_length": 2157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 717,
      "chapter": null,
      "content": "makes it distinct, and its name refers to the latest import. When run, this summer\nvisits numbers in the same order as the recursive-calls original, but manages the\ntraversal with an explicit stack instead of recursion:\n>>> from sumtree_stack import sumtree            # Same name, different file\n>>> sumtree([1, [2, [3, 4], 5], 6, [7, 8]])\n36\n>>> from sumtree_tester import tester            # Optional if already imported\n>>> tester(sumtree)\n1, 2, 3, 4, 5, 6, 7, 8, 36\n1, 2, 3, 4, 5, 15\n1, 2, 3, 4, 5, 15\nFor more on the last two examples (plus another breadth-first coding variant\nomitted here), see file sumtree_etc.py in the book’s examples package. It adds\nadditional tracing so you can watch it walk structures in more detail.\nIn general, though, once you get the hang of recursive calls, they may be more\nnatural than the explicit scheduling lists they automate, and are generally\npreferred unless you need to traverse structures in specialized ways. Some\nprograms, for example, perform a best-first search that requires an explicit\nsearch queue ordered by relevance or other criteria. If you think of a web crawler\nthat scores sites visited by content, the applications may start to become clearer.\nCycles, paths, and stack limits\nAs is, these programs suffice as demos, but larger recursive applications can\nsometimes require a bit more infrastructure than shown here: they may need to\navoid cycles or repeats, record paths taken for later use, and expand stack space\nwhen using recursive calls instead of explicit queues or stacks.\nFor instance, neither the recursive-call nor the explicit queue/stack examples in\nthis section do anything about avoiding cycles—visiting a location already\nvisited. That’s not required here, because we’re traversing strictly hierarchical\ntrees of list objects. If data can be a cyclic graph, though, both these schemes\nwill fail: the recursive-call scheme will fall into an infinite recursive loop (and\nmay run out of call-stack space), and the others will fall into simple infinite\nloops, re-adding the same items to their lists (and may or may not run out of\ngeneral memory). In fact, it’s easy to demo the perils by creating a cyclic object\nwith the strange code we met in an exercise at the end of Chapter 3:",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 718,
      "chapter": null,
      "content": ">>> L = [1, 2]\n>>> L.append(L)      # Make a cyclic object: L references itself\n>>> L\n[1, 2, [...]]\n>>> from sumtree import sumtree\n>>> sumtree(L)\nRecursionError: maximum recursion depth exceeded\n>>> from sumtree_queue import sumtree\n>>> sumtree(L)\n…hang or crash, and ditto for stack…\nSome programs also need to avoid repeated processing for a state reached more\nthan once, even if that wouldn’t lead to a loop. To do better, a recursive-call\ntraversal might make and pass along a mutable set, dictionary, or list of states\nvisited so far and check for repeats as it goes. We will use this scheme in later\nrecursive examples in this book:\n  if state not in visited:\n      visited.add(state)          # x.add(state), x[state]=True, or x.append(state)\n      …proceed…\nNonrecursive alternatives might similarly avoid adding states already visited\nwith code like the following. Subtly, object cycles may require is (not in), and\nsimply checking for duplicates already on the items list would avoid scheduling\na state twice but would not prevent revisiting a state visited earlier and hence\nremoved from that list:\n  visited.add(front)\n  …proceed…\n  items.extend([x for x in front if x not in visited])\nThis model doesn’t quite apply to this section’s use case that simply adds\nnumbers in lists, but other applications will generally be able to identify repeated\nstates—a URL of a previously visited web page, for instance. In fact, we’ll use\nsuch techniques to avoid cycles and repeats in the later examples listed in the\nnext section.\nSome programs may also need to record complete paths for each state followed\nso they can report solutions when finished. In such cases, each item in the",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 719,
      "chapter": null,
      "content": "nonrecursive scheme’s stack or queue may be a full path list that suffices for a\nrecord of states visited, and contains the next item to explore at either end.\nAlso note that standard Python limits the depth of its runtime call stack—crucial\nto recursive-call programs—to trap infinite recursion errors. To expand it for\ndeeper journeys, use the sys module:\n>>> sys.getrecursionlimit()         # 1000 calls deep default\n1000\n>>> sys.setrecursionlimit(10000)    # Allow deeper nesting\n>>> help(sys.setrecursionlimit)     # Read more about it\nThe maximum allowed setting can vary per platform. This isn’t required for\nprograms that use stacks or queues to avoid recursive calls and gain more control\nover the traversal process (though they also won’t catch infinite loops).\nMore recursion examples\nAlthough this section’s example is artificial, it is representative of a larger class\nof programs; inheritance trees and module import chains, for example, can\nexhibit similarly general structures, and computing tools such as permutations\ncan require arbitrarily many nested loops. In fact, we’ll use recursion again in\nsuch roles later in this book:\nIn Chapter 20’s permute.py, to shuffle arbitrary sequences\nIn Chapter 25’s reloadall.py, to traverse import chains\nIn Chapter 29’s classtree.py, to traverse class inheritance trees\nIn Chapter 31’s lister.py, to traverse class inheritance trees again\nIn Appendix B, “Solutions to End-of-Part Exercises” at the end of this\npart of the book: countdowns and factorials\nThe second and third of these will also detect states already visited to avoid\ncycles and repeats. Although simple loops should generally be preferred to\nrecursion for linear iterations on the grounds of simplicity and efficiency, you’ll\nfind that recursion is essential in scenarios like those in these later examples.\nMoreover, you sometimes need to be aware of the potential of unintended",
      "content_length": 1903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 720,
      "chapter": null,
      "content": "recursion in your programs. As you’ll also see later in the book, some operator-\noverloading methods in classes such as __setattr__ and __getattribute__\nand even __repr__ have the potential to recursively loop if used incorrectly.\nRecursion is a powerful tool, but it tends to be best when both understood and\nexpected!\nFunction Tools: Attributes, Annotations, Etc.\nLet’s move on to a category of tools that may seem less ethereal than recursion\nto some earthlings. As we’ve seen in this part of the book, functions in Python\nare much more than code-generation specifications for a compiler—they are full-\nblown objects, stored in pieces of memory all their own. As such, they can be\nfreely passed around a program and called indirectly. They also support\noperations that have little to do with calls at all, including attributes and\nannotation. We’ve sampled some of these topics earlier, but this section provides\nexpanded coverage.\nThe First-Class Object Model\nBecause Python functions are objects, you can write programs that process them\ngenerically. Function objects may be reassigned to other names, passed to other\nfunctions, embedded in data structures, returned from one function to another,\nand more, as if they were simple numbers or strings. Function objects happen to\nsupport a special operation—they can be called by listing arguments in\nparentheses—but they belong to the same general category as other objects.\nAs we’ve seen, this is usually called a first-class object model; it’s ubiquitous in\nPython, and a necessary part of functional programming. We’ll explore this\nprogramming mode more fully in this and the next chapter; because its motif is\nfounded on the notion of applying functions, it treats functions as a kind of data.\nWe’ve explored some generic use cases for functions in earlier examples, but a\nquick review helps to underscore the model. For example, there’s really nothing\nspecial about the name used in a def statement: it’s just a variable assigned in\nthe current scope, as if it had appeared on the left of an = sign. Because the\nfunction name is simply a reference to an object after a def runs, you can",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 721,
      "chapter": null,
      "content": "reassign that object to other names freely and call it through any reference:\n>>> def exclaim(message):              # Name exclaim assigned to function object\n        print(message + '!')\n>>> exclaim('Direct call')             # Call object through original name\nDirect call!\n>>> x = exclaim                        # Now x references the function too\n>>> x('Indirect call')                 # Call object through name x by adding ()\nIndirect call!\nAnd because arguments are passed by assigning objects, it’s just as easy to pass\nfunctions to other functions as arguments. The callee may then call the passed-in\nfunction just by adding arguments in parentheses (see the earlier summer tester\nin Example 19-4 for another example of this pattern):\n>>> def generic(func, arg):\n        func(arg)                      # Call the passed-in object by adding ()\n>>> generic(exclaim, 'Argument call')  # Pass the function to another function\nArgument call!\nYou can even stuff function objects into data structures, as though they were\nintegers or strings. The following, for example, embeds the function twice in a\nlist of tuples, as a sort of actions table. Because Python compound types like\nthese can contain any sort of object, there’s no special case here, either\n(Example 18-2 used similar code):\n>>> schedule = [ (exclaim, 'Hack'), (exclaim, 'Code') ]\n>>> for (func, arg) in schedule:\n        func(arg)                      # Call functions embedded in containers\nHack!\nCode!\nThis code simply steps through the schedule list, calling the exclaim function\nwith one argument each time through. As we saw in Chapter 17’s examples,\nfunctions can also be created and returned for use elsewhere—the closure\ncreated in this mode also retains state from the enclosing scope:",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 722,
      "chapter": null,
      "content": ">>> def implore(verb):                 # Make a function but don't call it\n        def exclaim(noun):\n            print(f'{verb} {noun}!')\n        return exclaim\n>>> I = implore('Hack')                # Label in enclosing scope is retained\n>>> I('Code')                          # Call the function that make returned\nHack Code!\n>>> I('App')\nHack App!\nPython’s first-class object model and lack of type constraints make for a very\nflexible programming language.\nFunction Introspection\nIn fact, functions are more flexible than you might expect. Because they are\nobjects, we can also process functions with normal object tools. For instance, by\nnow we know that once we make a function we can call it as usual:\n>>> def func(a):\n        b = 'Hack'\n        return b * a\n>>> func(8)\n'HackHackHackHackHackHackHackHack'\nBut the call expression is just one of a set of operations defined to work on\nfunction objects. For instance, we can also inspect their attributes generically:\n>>> func.__name__\n'func'\n>>> dir(func)\n'__annotations__', '__builtins__', '__call__', '__class__', '__closure__',\n…more omitted: 38 total…\n'__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__type_params__']\nIntrospection—internals access—tools like this allow us to explore\nimplementation details. For example, functions have attached code objects,\nwhich provide details on aspects such as the functions’ local variables and\narguments:",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 723,
      "chapter": null,
      "content": ">>> func.__code__\n<code object func at 0x103ef3910, file \"<stdin>\", line 1>\n>>> dir(func.__code__)\n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__',\n…more omitted: 48 total…\n'co_posonlyargcount', 'co_qualname', 'co_stacksize', 'co_varnames', 'replace']\n>>> func.__code__.co_varnames\n('a', 'b')\n>>> func.__code__.co_argcount\n1\nTool writers can make use of such information to manage functions—in fact, we\nwill too in Chapter 39, to implement validation of function arguments with the\ndecorators introduced ahead. Whether you code or use such tools, object\nintrospection boosts function utility.\nFunction Attributes\nNor are function objects limited to the system-defined attributes of the prior\nsection: it’s also possible to attach arbitrary user-defined attributes to them. This\ntopic was introduced in Chapter 17, but this section expands on it with additional\ncontext and examples. As usual in Python, function attributes are created by\nsimple assignments:\n>>> def func(): pass\n>>> func\n<function func at 0x1043771a0> \n>>> func.count = 0\n>>> func.count += 1\n>>> func.count\n1\n>>> func.handles = 'Button-Press'\n>>> func.handles\n'Button-Press'\n>>> dir(func)\n…most double-underscore names omitted…\n'__str__', '__subclasshook__', '__type_params__', 'count', 'handles']\nPython’s own implementation-related data stored on functions follows naming\nconventions that prevent them from clashing with the more arbitrary attribute\nnames you might assign yourself. Specifically, all function internals’ names have",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 724,
      "chapter": null,
      "content": "leading and trailing double underscores (“__X__”):\n>>> len(dir(func))\n40\n>>> [x for x in dir(func) if not x.startswith('__')]\n['count', 'handles']\nIf you’re careful not to name attributes the same way as Python, you can safely\nuse the function’s namespace as though it were your own namespace or scope.\nNaturally, all of this works the same for functions made with lambda:\n>>> F = lambda: None\n>>> len(dir(F))\n38\n>>> F.book = 'LP6E'\n>>> F.book\n'LP6E'\nAs covered in Chapter 17, such attributes can be used to attach state information\nto function objects directly, instead of using other techniques such as globals,\nnonlocals, and classes. Unlike nonlocals, such attributes are accessible anywhere\nthe function itself is, even from outside its code.\nIn a sense, this is also a way to emulate “static locals” in other languages—\nvariables whose names are local to a function, but whose values are retained\nafter a function exits. Attributes are related to objects instead of scopes (and\nmust be referenced through the function name within its code), but the net effect\nis similar.\nMoreover, as also explored in Chapter 17, when attributes are attached to\nfunctions generated by other factory functions, they also support multiple copy,\nwriteable, and per-call state retention, as an alternative to closures and class-\ninstance attributes. This makes function attributes a broadly useful tool—like the\ntopics of the next section.\nFunction Annotations and Decorations\nFor tools roles, functions also support attached annotations—arbitrary user-\ndefined info about a function’s arguments and result that augment the function.\nPython provides syntax for coding annotations, but it doesn’t do anything with",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 725,
      "chapter": null,
      "content": "them itself—annotations are completely optional, don’t impact function behavior\nin any way, and when present are simply attached to the function object’s\n__annotations__ attribute for use by other tools.\nWhile not of general interest to most application programmers, third-party tools\nand libraries might use annotations in the context of enhanced error checking, or\nAPI directives. Type hinting, discussed in Chapter 6, is also based on\nannotations, but is optional and unused, and doesn’t preclude other roles for\nannotations today (more on this ahead).\nWe studied the formal rules for arguments in function definitions in the\npreceding chapter. Annotations don’t modify these rules but simply extend their\nsyntax to allow extra expressions to be associated with named arguments and\nfunction results. Consider the following nonannotated function, coded with three\narguments and a returned result:\n>>> def func(a, b, c):\n        return a + b + c\n>>> func(1, 2, 3)\n6\nSyntactically, function annotations are coded in def header lines (only), as\narbitrary expressions associated with arguments and return values. For\narguments, they appear after a colon immediately following the argument’s\nname; for return values, they are written after a -> following the arguments list’s\nclosing parenthesis. The following code, for example, annotates all three of the\nprior function’s arguments, as well as its return value:\n>>> def func(a: 'hack', b: (1, 10), c: float) -> int:\n        return a + b + c\n>>> func(1, 2, 3)\n6\nCalls to an annotated function work as usual, but when annotations are present\nPython collects them in a dictionary and attaches it to the function object itself\nas its __annotations__. In this, argument names become keys; the return value\nannotation is stored under key return if coded (which suffices because this",
      "content_length": 1825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 726,
      "chapter": null,
      "content": "reserved word can’t be used as an argument name); and the values of annotation\nkeys are assigned to the results of the annotation expressions:\n>>> func.__annotations__\n{'a': 'hack', 'b': (1, 10), 'c': <class 'float'>, 'return': <class 'int'>}\nBecause they are just Python objects attached to a Python object, annotations are\nstraightforward to process. The following annotates just two of three arguments\nand steps through the attached annotations generically:\n>>> def func(a: 'python', b, c: 3.12):\n        return a + b + c\n>>> func(1, 2, 3)\n6\n>>> func.__annotations__\n{'a': 'python', 'c': 3.12}\n>>> for arg in func.__annotations__:\n       print(arg, '=>', func.__annotations__[arg])\na => python\nc => 3.12\nThere are two fine points to note here. First, you can still use defaults for\narguments if you code annotations—the annotation (and its : character) appears\nbefore the default (and its = character). In the following, for example, a: 'hack'\n= 4 means that argument a defaults to 4 and is annotated with the string 'hack':\n>>> def func(a: 'hack' = 4, b: (1, 10) = 5, c: float = 6) -> int:\n        return a + b + c\n>>> func(1, 2, 3)                # No defaults used\n6\n>>> func()                       # a=4 + b=5 + c=6 (all defaults)\n15\n>>> func(1, c=10)                # 1 + b=5 + 10 (keywords work normally)\n16\n>>> func.__annotations__\n{'a': 'hack', 'b': (1, 10), 'c': <class 'float'>, 'return': <class 'int'>}\nSecond, note that the blank spaces in the prior example are all optional—you can\nuse spaces between components in function headers or not, but omitting them",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 727,
      "chapter": null,
      "content": "might degrade your code’s readability to some observers (and probably improve\nit to others):\n>>> def func(a:'hack'=4, b:(1,10)=5, c:float=6)->int:\n        return a + b + c\n>>> func(1, 2)                   # 1 + 2 + c=6\n9\n>>> func.__annotations__\n{'a': 'hack', 'b': (1, 10), 'c': <class 'float'>, 'return': <class 'int'>}\nWhile annotations are optional and uncommon, they may be used to specify\nconstraints for types or values, and larger APIs might use this feature as a way to\nregister function interface information. In fact, you’ll see a potential application\nin Chapter 39, when we code annotations as an alternative to function decorator\narguments—a more general concept in which augmentation info is coded outside\nthe function header and so is not limited to a single role.\nFunction decorators alternative: Preview\nBecause we’re going to devote an entire chapter to decorators, we’ll omit most\nof their story here. But as a very brief preview, decorators are simply functions\nthat augment other functions. They are applied to another function’s def with an\n@ prefix that rebinds the other function’s name to the result of passing it to the\ndecorator. This syntax:\n@decorator\ndef func(args): …           # Decorated function def\nis automatically morphed into the following equivalent, where decorator is a\none-argument callable object (or an expression that returns one), which returns a\ncallable object having arguments compatible with func (or func itself):\ndef func(args): …\nfunc = decorator(func)      # Rebind func to decorator's result\nDecorators can use this hook to wrap another function in an extra layer of code\nfor nearly arbitrary purposes, as in the following code that adds a message when\na decorated function is called:",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 728,
      "chapter": null,
      "content": "def echo(F):\n    def proxy(*args): \n        print('calling', F.__name__)       # Add actions here\n        return F(*args)                    # Run decorated function\n    return proxy\n@echo\ndef func(x, y):                            # Rebinds func = echo(func)\n    print('I am running...', x, y)         # func is run by the proxy closure\n>>> func(1, 2)                             # Runs a proxy, which runs func\ncalling func\nI am running... 1 2\nAs you’ll learn later, decorators can also take arguments (e.g., @echo(args))\nwhose utility can overlap with annotations—argument info can be sent to the\ndecorator instead of being embedded in headers as annotations. Because this is\nan advanced tool that can also be applied to classes, we’ll pause this thread until\nChapters 32 and 39.\nCompared to annotations, though, decorators don’t complicate function headers\nthemselves with extra syntax, and more naturally support multiple roles.\nAnnotations may make functions difficult to read, and generally can have just\none role—one that’s hijacked by the optional type hinting of Chapter 6 in\nprograms that choose to employ it.\nIn fact, type hinting advocates have tried to deprecate all other roles for\nannotations. While these divisive (and perhaps even rude) efforts have thankfully\nfailed to date, decorators face no such challenge, and may be a safer bet going\nforward. Today, though, both annotations and decorations are tools whose roles\nare limited only by your imagination.\nFinally, note that annotations and decorations work in def statements—but not\nin lambda expressions, whose syntax by design limits the functions they can\ndefine. Coincidentally, this brings us to this potpourri’s next topic.\nAnonymous Functions: lambda\nWe first met the lambda expression back in Chapter 16 and have used it in a\nhandful of examples since then. This section reviews the basics and takes a",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 729,
      "chapter": null,
      "content": "deeper second look, to both reinforce and expand on this topic.\nAs we’ve seen, besides the def statement, Python provides an expression that\ncreates function objects. Because of its similarity to a tool in other languages, it’s\ncalled lambda. Like def, this expression creates a function to be called later, but\nit returns the function instead of assigning it to a name. This is why lambdas are\nsometimes known as anonymous functions. In practice, they are used to inline\nfunction definitions, or defer execution of code.\nNOTE\nWhat’s in a name?: The lambda tends to intimidate people, largely due to the name “lambda”\nitself—a name that comes from the Lisp language, which got it from lambda calculus, which is\na form of symbolic logic. Obscure mathematical heritage aside, though, lambda in Python is\nreally just a word that introduces an expression syntactically, and its expression is simply an\nalternative way to code a function—albeit without statements, decorators, or annotations.\nlambda Basics\nAs a refresher, the lambda’s general form is the keyword lambda, followed by\nzero or more arguments (just like the arguments you enclose in parentheses in a\ndef header, sans annotations), followed by an expression after a colon:\nlambda argument1, argument2,… argumentN : expression-using-arguments\nParentheses are not allowed around lambda arguments and are generally optional\naround the entire lambda itself, though they’re required in some contexts and\nmay boost clarity in others. Functions returned by lambda work the same as\nthose assigned by def, but lambda differs in ways that make it useful in\nspecialized roles:\nlambda is an expression, not a statement. Because of this, a lambda\ncan appear in places a def is not allowed by Python’s syntax—inside a\nlist literal or a function call’s arguments, for example. With def,\nfunctions can be referenced by name in such locations, but must be\ncreated elsewhere. As an expression, lambda returns a value (a new",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 730,
      "chapter": null,
      "content": "function) that can be assigned a name, or used where the lambda\nappears.\nlambda’s body is a single expression, not a block of statements. The\nlambda’s body is similar to what you’d put in a def body’s return\nstatement; you simply type the result as a naked expression, instead of\nexplicitly returning it. Because it is limited to an expression, a lambda is\nless general than a def—you can only squeeze so much logic into a\nlambda body without full-blown statements. This is by design, to limit\nprogram nesting: lambda is designed for coding simple functions, and\ndef handles larger tasks.\nApart from those distinctions, defs and lambdas do the same sort of work. For\ninstance, by this point we should be pros at making a function with a def\nstatement:\n>>> def func(x, y, z): return x + y + z\n>>> func(2, 3, 4)\n9\nBut we can achieve the same effect with a lambda expression by explicitly\nassigning its result to a name through which you can later call the otherwise-\nanonymous function:\n>>> func = lambda x, y, z: x + y + z\n>>> func(2, 3, 4)\n9\nHere, func is manually assigned the function object the lambda expression\ncreates; this is how def works, too, but its assignment is automatic. Defaults and\nother argument-matching syntax work in lambda too, just like in a def:\n>>> x = (lambda a='hack', b='python', c='code': a + b + c)\n>>> x('write')\n'writepythoncode'\nThe code in a lambda body also follows the same scope lookup rules as code\ninside a def. lambda expressions introduce a new local scope much like a nested",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 731,
      "chapter": null,
      "content": "def, which automatically sees names in enclosing functions, the module, and the\nbuilt-in scope—via the LEGB rule we studied in Chapter 17:\n>>> def editions(title):                          # title in enclosing scope\n        return (lambda e: title + ', ' + e)       # Return a function object\n>>> labeler = editions('Learning Python')         # Make+save nested function\n>>> labeler('6E')                                 # '6E' passed to e in lambda\n'Learning Python, 6E'\nWith those basic lambda “hows” in hand, the natural next question is “why,”\ntaken up in the next section.\nWhy Use lambda?\nGenerally speaking, lambda is a sort of function shorthand that allows you to\nembed a function’s definition within the code that uses it. It is entirely optional\n—you can always use def instead, and should if your function requires the\npower of full statements that the lambda’s expression cannot provide. But\nlambda may be easier to use in scenarios where you just need to embed a small\nbit of executable code inline at the place where it is to be later used.\nFor instance, you’ll see later that callback handlers are frequently coded as inline\nlambda expressions embedded directly in a registration call’s arguments list,\ninstead of being defined with a def elsewhere in a file and referenced by name\n(see the sidebar “Why You Will Care: lambda Callbacks” for an example).\nlambda is also commonly used to code jump tables, which are lists or\ndictionaries of actions to be performed on demand. For example:\nL = [lambda x: x * 2,                # Inline function definition\n     lambda x: x ** 2,\n     lambda x: x // 2]               # A list of three callable functions\nfor f in L:\n    print(f(5))                      # Prints 10, 25, and 2\nprint(L[1](5))                       # Prints 25\nThe lambda expression works well when you need to stuff small pieces of",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 732,
      "chapter": null,
      "content": "executable code into places where statements like def are not allowed. The\npreceding code snippet, for example, builds up a list of three functions by\nembedding lambda expressions inside a list literal; a def won’t work inside a\nliteral like this because it is a statement, not an expression. The equivalent def\ncoding would require temporary function names (which might clash with others)\nand function definitions outside the context of intended use (which might be\nhundreds of lines away):\ndef f1(x): return x * 2\ndef f2(x): return x ** 2             # Define named functions\ndef f3(x): return x // 4             # Separate from their place of use\nL = [f1, f2, f3]                     # Reference by name\nfor f in L:\n    print(f(2))                      # Also prints 10, 25, and 2\nMultiway branches: The finale\nIn fact, you can do the same sort of thing with dictionaries and other data\nstructures in Python to build up more general sorts of action tables. Here’s\nanother example to illustrate at the interactive prompt:\n>>> key = 'loop'\n>>> {'hack': lambda s: s.upper(),\n     'code': lambda s: s.lower(),\n     'loop': lambda s: f'{s * 4}!'}[key]('Py')\n'PyPyPyPy!'\nHere, when Python makes the temporary dictionary, each of the nested lambdas\ngenerates and leaves behind a function to be called later. Indexing by key fetches\none of those functions, and parentheses force the fetched function to be called.\nWhen coded this way, a dictionary becomes a more general multiway branching\ntool than this book could fully reveal in Chapter 12’s coverage of if statements.\nTo make this work without lambda, you’d need to instead code three def\nstatements somewhere else in your file, outside the dictionary in which the\nfunctions are to be used, and reference the functions by name:\n>>> def f1(s): return s.upper()",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 733,
      "chapter": null,
      "content": ">>> def f2(s): return s.lower()\n>>> def f3(s): return f'{s * 4}!'\n>>> key = 'loop'\n>>> {'hack': f1, 'code': f2, 'loop': f3}[key]('Py')\n'PyPyPyPy!'\nThis works, too, but your defs may be arbitrarily far away in your file, even if\nthey are just little bits of code. The code proximity that lambda provides is\nespecially useful for functions that will only be used in a single context—if the\nthree functions here are not useful anywhere else, it makes sense to embed their\ndefinitions within the dictionary as lambdas. Moreover, the def form requires\nyou to make up names for these little functions that, again, may clash with other\nnames in this file (perhaps unlikely, but always possible).\nYou can use the match statement today with similar results, but it may require\neven more code than the def scheme, especially since you’d have to nest it\nwithin a def to support an argument like s; see Chapter 12 for more info.\nlambda is also handy in function-call argument lists as a way to inline temporary\nfunction definitions not used anywhere else in your program; you’ll see\nexamples of such other uses later in this chapter, when we study map.\nNOTE\nThe siren song of eval: In principle, you could skip the dispatch table in the preceding code if\nthe function name is the same as its string lookup key—an eval(key)(arg) would kick off\nthe call. While true in this case and sometimes useful, as we saw earlier (e.g., Chapter 10),\neval is relatively slow (it must compile and run code) and insecure (you must trust the string’s\nsource). More fundamentally, jump tables are generally subsumed by polymorphic method\ndispatch in Python: calling a method does the “right thing” based on the type of object that’s\nthe subject of the call, no switching logic required. To see why, stay tuned for Part VI.\nHow (Not) to Obfuscate Your Python Code\nThe fact that the body of a lambda has to be a single expression (not a series of\nstatements) would seem to place severe limits on how much logic you can pack\ninto a lambda. If you know what you’re doing, though, you can code most\nstatements in Python as expression-based equivalents.",
      "content_length": 2117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 734,
      "chapter": null,
      "content": "For example, if you want to print from the body of a lambda function, simply\nuse print or sys.stdout.write (recall from Chapter 11 that this is what print\nreally does). And to execute multiple actions, code a sequence like a tuple, to\nevaluate nested multiple expressions from left to right (tuples require\nparentheses in this context):\n>>> series = lambda a, b: (print(a.upper()), print(b.lower()))     # > 1 actions\n>>> ignore = series('Hack', 'Code')\nHACK\ncode\nSimilarly, to nest logic in a lambda, you can use the if/else ternary expression\nintroduced in Chapter 12, or the equivalent but trickier and/or combination also\ndescribed there. As you learned earlier, the following statement:\nif a:\n    b\nelse:\n    c\ncan be emulated by either of these equivalent expressions (the second is only\nroughly the same, but close enough):\nb if a else c\n((a and b) or c)\nBecause expressions like these can be placed inside a lambda, they may be used\nto implement selection logic within a lambda function (the lambda is\nparenthesized here only for variety and subjective clarity):\n>>> lower = (lambda x, y: x if x < y else y)         # Ifs in lambda: ternary\n>>> lower('bb', 'aa')\n'aa'\n>>> lower('aa', 'bb')\n'aa'\nFurthermore, if you need to perform loops within a lambda, you can also embed\nthings like list comprehension expressions and map calls—tools we met in earlier\nchapters and will revisit in this and the next chapter:",
      "content_length": 1417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 735,
      "chapter": null,
      "content": ">>> showall = lambda x: [print(y) for y in x]        # Loops in lambda: list comp\n>>> t = showall(['lp5e', 'lp6e'])\nlp5e\nlp6e\n>>> showall = lambda x: list(map(print, x))          # Loops in lambda: maps\n>>> t = showall(('py3.3', 'py3.12')) \npy3.3\npy3.12\n>>> showall = lambda x: print(*x, sep='', end='')    # Another option: *unpacking\nAnd while it’s limited by the local scope of the function that lambda makes,\nassignment is in scope (pun intended) for lambda expressions that use the :=\nnamed-assignment expression:\n>>> namer = lambda x: (res := x + 1) + res           # Assignment in lambda: :=\n>>> namer(2)\n6\n>>> res                                              # But it doesn't persist\nNameError: name 'res' is not defined\nThere is a limit to emulating statements with expressions: you can’t assign\nnonlocals, for instance, though tools like the setattr built-in, the __dict__ of\nnamespaces, and methods that change mutable objects in place can sometimes\nstand in—and can quickly lead you deep into the heart of expression-\nconvolution darkness.\nBut now that this book has shown you these tricks, it must also humbly implore\nyou to use them only as a last resort. Without due care, they can yield unreadable\n(a.k.a. obfuscated) Python code, despite the language’s best intents. In general,\nsimple is better than complex, explicit is better than implicit, and full statements\nare better than arcane expressions. That’s why lambda is limited to expressions.\nIf you have larger logic to code, use def; lambda is for small pieces of inline\ncode. On the other hand, you may find these techniques useful—when taken in\nmoderation.\nScopes: lambdas Can Be Nested Too\nOne last lambda note: as we saw in Chapter 17, lambda is also the main\nbeneficiary of enclosing-function scope lookup—the E in the LEGB scope rule.\nAs a review, in the following the lambda appears inside a def, its typical coding,",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 736,
      "chapter": null,
      "content": "and so can access the value that name x had in the enclosing function’s scope\nduring its call:\n>>> def action(x):\n        return (lambda y: x + y)               # Make function, remember x\n>>> act = action(99)\n>>> act(2)                                     # Call what action returned\n101\nWhat wasn’t illustrated in the prior discussion of nested function scopes is that a\nlambda also has access to the names in any enclosing lambda. This case is\nsomewhat obscure, but imagine if we recoded the prior def with a lambda:\n>>> action = (lambda x: (lambda y: x + y))     # lambda scopes nest too\n>>> act = action(99)\n>>> act(3)\n102\n>>> ((lambda x: (lambda y: x + y))(99))(4)     # Even if you don't save them\n103\nHere, the nested lambda structure makes a function that makes a function when\ncalled. In both cases, the nested lambda’s code has access to the variable x in the\nenclosing lambda. This works, but it seems fairly convoluted code; in the\ninterest of readability, nested lambdas may be best avoided. The good news,\nperhaps, is that def cannot be nested in lambda: because lambda’s body is an\nexpression, statements like def won’t work—thankfully!\nFunctional Programming Tools\nLast up in this chapter’s gumbo is a reprisal of a category of tools we met in\nearlier in this book, with a few new tips to round out the topic. By most\ndefinitions, today’s Python blends support for multiple programming paradigms:\nprocedural (with its basic statements), object-oriented (with its classes), and\nfunctional.\nThe criteria for the latter of these categories are somewhat loose, but by most\nmeasures Python’s functional programming toolbox includes its first-class object",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 737,
      "chapter": null,
      "content": "model, nested scope closures, and anonymous function lambdas that we met\nearlier; its generators and comprehensions, which we’ll be expanding on in the\nnext chapter; and perhaps its function and class decorators previewed here but\nfleshed out in this book’s final part.\nThis toolbox also includes built-in functions that apply other functions to\niterables automatically—including functions that call other functions on an\niterable’s items (map); select items based on a test function (filter); and apply\nfunctions to pairs of items and running results (reduce). Let’s wrap up this\nchapter with a quick survey of this trio.\nMapping Functions over Iterables: map\nOne of the more common things programs do with lists and other sequences is\napply an operation to each item and collect the results—selecting columns in\ndatabase tables, incrementing pay fields of employees in a company, parsing\nemail attachments, and so on. Python has multiple tools that make such\ncollection-wide operations easy to code. For instance, we’ve seen that updating\nall the counters in a list can be done easily with a for loop:\n>>> counters = [1, 2, 3, 4]\n>>> updated = []\n>>> for x in counters:\n        updated.append(x + 10)                 # Add 10 to each item\n>>> updated, counters\n([11, 12, 13, 14], [1, 2, 3, 4])\nBut because this is such a common operation, Python also provides built-ins that\ndo most of the work for you. Among them, the map function applies a passed-in\nfunction to each item in an iterable object and returns a list containing all the\nfunction-call results. For example, assuming counters is intact:\n>>> def inc(x): return x + 10                  # Function to be run\n>>> list(map(inc, counters))                   # Collect call results\n[11, 12, 13, 14]",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 738,
      "chapter": null,
      "content": "We met map briefly in Chapters 13 and 14, as a way to apply a built-in function\nto items in an iterable. Here, we make more general use of it by passing in a\nuser-defined function to be applied to each item in the list—map calls our inc on\neach list item and collects all the return values into a new list. Remember that\nmap is a nonsequence iterable, so a list call is used to force it to produce all its\nresults for display here (per Chapter 14 coverage).\nBecause map expects a function to be passed in and applied, it also happens to be\none of the places where lambda commonly appears:\n>>> list(map((lambda x: x + 3), counters))     # Function expression\n[4, 5, 6, 7]\nHere, the function adds 3 to each item in the counters list; as this little function\nisn’t needed elsewhere, it was written inline as a lambda. Because such uses of\nmap are equivalent to for loops, with a little extra code you can always code a\ngeneral mapping utility yourself:\n>>> def mymap(func, iter):\n        res = []\n        for x in iter: res.append(func(x))\n        return res\nAssuming the function inc is still as it was when it was shown previously, we\ncan map it across a sequence (or other iterable) with either the built-in or our\nequivalent:\n>>> list(map(inc, [1, 2, 3]))               # Built-in is an iterable\n[11, 12, 13]\n>>> mymap(inc, [1, 2, 3])                   # Ours builds a list (see generators)\n[11, 12, 13]\nHowever, as map is a built-in, it’s always available, always works the same way,\nand may have performance benefits (as we’ll prove in Chapter 21, it’s faster than\na manually coded for loop in some usage modes). Moreover, map can be used in\nmore advanced ways than shown here. For instance, given multiple sequence\narguments, it sends items taken from sequences in parallel as distinct arguments\nto the function:",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 739,
      "chapter": null,
      "content": ">>> pow(3, 4)                               # 3**4\n81\n>>> list(map(pow, [1, 2, 3], [2, 3, 4]))    # 1**2, 2**3, 3**4\n[1, 8, 81]\nWith multiple sequences, map expects an N-argument function for N sequences.\nHere, the pow function takes two arguments on each call—one from each\nsequence passed to map. It’s not much extra work to simulate this multiple-\nsequence generality in code, too, but we’ll postpone doing so until later in the\nnext chapter, after we’ve explored some additional iteration tools (hint: it would\nbe better to generate items on demand, like the built-in).\nThe map call is also similar to the list comprehension expressions we studied in\nChapter 14 and will revisit in the next chapter from a functional perspective:\n>>> list(map(inc, [1, 2, 3, 4]))\n[11, 12, 13, 14]\n>>> [inc(x) for x in [1, 2, 3, 4]] \n[11, 12, 13, 14]\nIn some cases, map may be faster to run than a list comprehension, and it may\nalso require less code. On the other hand, because map applies a function call to\neach item instead of an arbitrary expression, it is a somewhat less general tool,\nand often requires extra helper functions or lambdas. Moreover, wrapping a\ncomprehension in parentheses instead of square brackets creates an object that\ngenerates values on request to save memory and increase responsiveness, much\nlike map—a topic we’ll take up in the next chapter.\nSelecting Items in Iterables: filter\nThe map function is a primary and relatively straightforward representative of\nPython’s functional programming toolset. Its close relatives, filter and\nreduce, select an iterable’s items based on a test function and apply functions to\nitem pairs, respectively.\nBecause it also returns an iterable, filter (like range) requires a list call to\ndisplay all its results in a REPL. For example, the following filter call picks\nout items in a sequence that are greater than zero:",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 740,
      "chapter": null,
      "content": ">>> list(range(−5, 5)) \n[−5, −4, −3, −2, −1, 0, 1, 2, 3, 4]\n>>> list(filter((lambda x: x > 0), range(−5, 5)))\n[1, 2, 3, 4]\nWe met filter briefly earlier in the sidebar “Why You Will Care: Booleans”\nand while exploring iterables in Chapter 14. Items in the sequence or iterable for\nwhich the function returns a true result are added to the result list. Like map, this\nfunction is roughly equivalent to a for loop, but it is built-in, concise, and often\nfast:\n>>> res = []\n>>> for x in range(−5, 5):                   # The statement equivalent of filter\n        if x > 0:                            # Simple but slower today, probably\n            res.append(x)\n>>> res\n[1, 2, 3, 4]\nAlso like map, filter can be emulated by list comprehension syntax with often\nsimpler results (especially when it can avoid creating a new function), and with a\nsimilar generator expression when coded with enclosing parentheses to delay\nproduction of results—though again, the generator story will have to remain a\nteaser for the next chapter:\n>>> [x for x in range(−5, 5) if x > 0]\n[1, 2, 3, 4]\nCombining Items in Iterables: reduce\nThe functional reduce call—once a built-in but now a resident of the functools\nstandard-library module—is more complex. It accepts an iterable to process, but\nit’s not an iterable itself: it returns a single result that aggregates an iterable’s\nitems. To demo, here are two reduce calls that compute the sum and product of\nthe items in a list:\n>>> from functools import reduce \n>>> reduce((lambda x, y: x + y), [1, 2, 3, 4])\n10",
      "content_length": 1541,
      "extraction_method": "Direct"
    },
    {
      "page_number": 741,
      "chapter": null,
      "content": ">>> reduce((lambda x, y: x * y), [1, 2, 3, 4])\n24\nAt each step, reduce passes the current sum or product, along with the next item\nfrom the list, to the passed-in lambda function. By default, the first item in the\nsequence initializes the starting value. To make that more concrete again, here’s\nthe for loop equivalent to the first of these calls, with the addition hardcoded\ninside the loop:\n>>> L = [1, 2, 3, 4]\n>>> res = L[0]\n>>> for x in L[1:]:\n        res += x\n>>> res\n10\nIn fact, coding your own reusable and customizable version of reduce is fairly\nstraightforward too. The following function emulates most of the built-in’s\nbehavior and helps demystify its operation in general:\n>>> def myreduce(function, sequence):\n        tally = sequence[0]\n        for next in sequence[1:]:\n            tally = function(tally, next)\n        return tally\n>>> myreduce((lambda x, y: x + y), [1, 2, 3, 4])\n10\n>>> myreduce((lambda x, y: x * y), [1, 2, 3, 4])\n24\nThe built-in reduce also allows an optional third argument, effectively placed\nbefore the items in the sequence to serve as an initial value and a default result\nwhen the sequence is empty, but we’ll leave this extension as a suggested\nexercise (again, emulating built-in tools is instructive, but superfluous).\nIf this coding technique has sparked your interest, you might also be interested in\nthe standard-library operator module, which provides functions that correspond\nto built-in expressions and so comes in handy for some uses of functional tools\n(consult help in a REPL or Python’s library manual for more details on this",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 742,
      "chapter": null,
      "content": "module):\n>>> import operator, functools\n>>> functools.reduce(operator.add, [2, 4, 6])        # Function-based +\n12\n>>> functools.reduce((lambda x, y: x + y), [2, 4, 6])\n12\nTogether, map, filter, and reduce support powerful functional programming\ntechniques. As mentioned, many observers would also extend the functional\nprogramming toolset in Python to include the nested function closures and\nanonymous function lambdas we’ve explored, as well as the generators and\ncomprehensions we’ve met in piecemeal fashion along the way. To fully grok the\nlatter two, though, we must move on to the next chapter.",
      "content_length": 602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 743,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter’s collage took us on a tour of function-related topics: function\ndesign guidelines; recursive functions; function attributes, annotations, and\ndecorators; lambda expressions; and the map, filter, and reduce built-ins.\nSome of these are advanced, but most are common in Python programming. The\nnext chapter continues the advanced-topics motif with a reprisal of\ncomprehensions and an unmasking of generators—tools that are just as related to\nfunctional programming as to looping statements. Before you move on, though,\nmake sure you’ve mastered the concepts covered here by working through this\nchapter’s quiz.\nTest Your Knowledge: Quiz\n1. How are lambda expressions and def statements related?\n2. What’s the point of using lambda?\n3. Compare and contrast map, filter, and reduce.\n4. What are function annotations, and how are they used?\n5. What are recursive functions, and how are they used?\n6. What are some general design guidelines for coding functions?\n7. Name three or more ways that functions can communicate results to a\ncaller.\nTest Your Knowledge: Answers\n1. Both lambda and def create function objects to be called later. Because\nlambda is an expression, though, it returns a function object instead of\nassigning it to a name, and it can be used to nest a function definition in\nplaces where a def will not work syntactically. A lambda allows for",
      "content_length": 1386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 744,
      "chapter": null,
      "content": "only a single implicit return value expression, though; because it does\nnot support a block of statements, it is not ideal for larger functions.\n2. lambda allows us to “inline” small units of executable code, defer its\nexecution, and provide it with state in the form of default arguments and\nenclosing scope variables. Using a lambda is never required; you can\nalways code a def instead and reference the function by name. lambda\ncomes in handy, though, to embed small pieces of deferred code that are\nunlikely to be used elsewhere in a program. It commonly appears in\ncallback-based programs such as GUIs, and has a natural affinity with\nfunctional tools like map and filter that expect a processing function.\n3. These three built-in functions all apply another function to items in a\nsequence (or other iterable) object and collect results. map passes each\nitem to the function and collects call results, filter collects items for\nwhich the function returns a true value, and reduce computes a single\nvalue by applying the function to an accumulator and successive items.\nUnlike the other two, reduce is available in the functools module, not\nthe built-in scope (in modern Python history, at least).\n4. Function annotations are syntactic embellishments of a function’s\narguments and result, which are collected into a dictionary assigned to\nthe function’s __annotations__ attribute. Python places no semantic\nmeaning on these annotations, but simply packages them for potential\nuse by other tools.\n5. Recursive functions call themselves either directly or indirectly in order\nto loop (i.e., repeat). They may be used to traverse arbitrarily shaped\nstructures, but they can also be used for iteration in general (though the\nlatter role is often more simply and efficiently coded with looping\nstatements). Recursion can often be simulated or replaced by code that\nuses explicit stacks or queues to have more control over traversals.\n6. Functions should generally be small and as self-contained as possible,\nhave a single unified purpose, and communicate with other components\nthrough input arguments and return values. They may use mutable\narguments to communicate results too if changes are expected, and",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 745,
      "chapter": null,
      "content": "some types of programs imply other communication mechanisms.\n7. Functions can send back results with return statements, by changing\npassed-in mutable arguments, and by setting global variables. Globals\nare generally frowned upon (except for very special cases, like\nmultithreaded programs) because they can make code more difficult to\nunderstand and use. return statements are usually best, but changing\nmutables is fine (and even useful), if expected. Functions may also\ncommunicate results with system devices such as files and sockets, but\nthese are beyond our scope here.\nWHY YOU WILL CARE: LAMBDA CALLBACKS\nAnother common role for lambda is to define inline callback functions for\nPython’s tkinter GUI API. For example, the following creates a button that\nprints a message on the console when pressed, assuming tkinter is present\nin your Python (it is by default on most PCs and at least one Android app):\nfrom tkinter import Button, mainloop\nx = Button(\n        text='Press me',\n        command=lambda: print('Tapped!'))\nx.pack()\nmainloop() # This may be optional in some REPLs\nHere, we register the callback handler by passing a function generated with a\nlambda to the command keyword argument. The advantage of lambda over\ndef in this is that the code that handles a button press is right here,\nembedded in the button-creation call.\nIn effect, the lambda defers execution of the handler until the event occurs:\nthe print call happens on button presses, not when the button is created, and\neffectively “knows” the string it should write when the event occurs. Real\nGUIs rarely print to consoles, of course, but this demos the coding pattern.\nBecause the nested function scope rules apply to lambda, they automatically\nsee names in the functions in which they are coded and hence don’t require\npassed-in defaults in most cases (except for loop variables, per Chapter 17).",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 746,
      "chapter": null,
      "content": "This is especially useful for accessing the special self instance argument\nthat is a local variable in enclosing class method functions (which we’ll\nstudy in Part VI, so take this as preview only):\nclass MyGui:\n    def makewidgets(self):\n        Button(command=lambda: self.onPress('Tapped!'))\n    def onPress(self, message):\n        …use message…\nAs you’ll see later, both class objects with __call__ and bound methods\noften serve in callback roles too—watch for coverage of these in Chapters 30\nand 31.\nAnd all of this applies to coding event callbacks in other commonly used and\nportable GUI toolkits for Python—including Kivy, Toga (in BeeWare),\nPyQT, and wxPython. tkinter gets more press here only because it’s\nshipped with Python’s standard library. As for all tools, you should vet GUIs\nfor yourself.",
      "content_length": 808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 747,
      "chapter": null,
      "content": "Chapter 20. Comprehensions and\nGenerations\nThis chapter explores a set of advanced function-related tools and topics. Its\nmain subjects are generator functions and their generator expression relatives—\nuser-defined ways to produce results on demand the same way that many built-\nins do. Because generators apply the iteration protocol and generator\nexpressions reuse comprehension syntax, this chapter is also partly a follow-up\nto Chapter 14 (hence its title). We’ll extend these topics to their completion here\nand demo with larger examples that tie ideas together.\nFinally, this chapter provides just enough of an intro to get you started with\nasync coroutines—tools that build on generators, but assume knowledge of\nparallel programming, which is outside the scope of this book and the needs of\nmost Python learners. You won’t become an async master here, but you’ll get a\nhead start for further explorations.\nIteration and generation in Python also encompasses user-defined classes, but\nwe’ll defer that final part of this story until Part VI, when we study operator\noverloading. The next chapter continues threads spun here by timing the relative\nperformance of some of this chapter’s tools as a larger case study. Before that,\nthough, let’s pick up the comprehensions and iterations story where it was last\nleft, and extend it to include value generators.\nComprehensions: The Final Act\nAs mentioned early in this book, Python supports the procedural, object-\noriented, and function programming paradigms. Among these, Python has a host\nof tools that most observers would consider functional in nature, which we\nenumerated in the preceding chapter—closures, generators, lambdas,\ncomprehensions, maps, decorators, function objects, and more. These tools allow\nus to apply and combine functions in powerful ways, and often offer state\nretention and coding solutions that are alternatives to classes and OOP.",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 748,
      "chapter": null,
      "content": "For instance, the prior chapter explored tools such as map and filter—key\nmembers of Python’s early functional-programming toolset inspired by the Lisp\nlanguage—that map operations over iterables and collect results. Because this is\nsuch a common task in coding, Python eventually sprouted a new expression—\nthe list comprehension—a device inspired by languages like Haskell, which is\neven more flexible than the original functional toolset.\nAs we’ve seen, list comprehensions apply an arbitrary expression to items in an\niterable, rather than applying a function. Accordingly, they can be more general\ntools. In later Pythons, the list comprehension was extended to other roles—sets,\ndictionaries, and even the value generator expressions we’ll explore in this\nchapter. Hence, it’s not just for lists anymore, and we’ll simply call it\ncomprehension when referring to all its forms collectively.\nWe first met comprehensions in Chapter 4’s preview, introduced them in Part II’s\ncoverage of sets, lists, and dictionaries, and studied them further in Chapter 14,\nin conjunction with looping statements. Because they’re also related to\nfunctional programming tools like the map and filter calls and repurposed by\ngenerator expressions, though, let’s resurrect the topic here for one last look.\nList Comprehensions Review\nHere’s a brief example to refresh a few neurons. As we learned in Chapter 7,\nPython’s built-in ord function returns the integer code point of a single\ncharacter. If we wish to collect code points of all characters in an entire string, a\nstraightforward approach is to use a simple for loop and append the results to a\nlist. In a REPL:\n>>> res = []\n>>> for x in 'text':\n        res.append(ord(x))                 # Manual results collection\n>>> res\n[116, 101, 120, 116]\nNow that we know about map, though, we can achieve similar results with a\nsingle function call without having to manage list construction in the code:\n>>> res = list(map(ord, 'text'))           # Apply function to iterable",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 749,
      "chapter": null,
      "content": ">>> res\n[116, 101, 120, 116]\nHowever, we can get the same results from a list comprehension expression—\nwhile map maps a function over an iterable, list comprehensions map an\nexpression:\n>>> res = [ord(x) for x in 'text']         # Apply expression to iterable\n>>> res\n[116, 101, 120, 116]\nList comprehensions collect the results of applying an expression to an iterable\nof values, and return them in a new list. Syntactically, list comprehensions are\nenclosed in square brackets—to remind you that they construct lists. In their\nbasic form, within the brackets you code an expression that names a variable,\nfollowed by what looks like a for loop header that names the same variable. You\nget back the expression’s results for each iteration of the implied loop.\nThe effect of the preceding example is similar to that of the manual for loop and\nthe map call. List comprehensions become more convenient, though, when we\nwish to apply an arbitrary expression instead of a function:\n>>> [x ** 2 for x in range(10)]            # Squares of numbers 0 to 9\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\nTo do similar work with a map call, we may need to invent a little function to\nimplement the square operation. Because we won’t need this function elsewhere,\nwe’d typically (but not necessarily) code it inline, with a lambda:\n>>> list(map((lambda x: x ** 2), range(10)))\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\nThis does the same job, and it’s only a few keystrokes longer than the equivalent\nlist comprehension. It’s also only marginally more complex (at least, once you\nunderstand the lambda). For more advanced roles, though, list comprehensions\nwill often require considerably less typing.\nFor instance, as we learned in Chapter 14, you can code an if clause after the",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 750,
      "chapter": null,
      "content": "comprehension’s for to add selection logic. List comprehensions with if\nclauses can be thought of as analogous to the filter built-in in the preceding\nchapter—they skip an iterable’s items for which the if clause is not true. As a\ndemo, following are both schemes, along with the equivalent for, picking up\neven numbers from 0 to 9 (x % 2 is zero for evens only):\n>>> [x for x in range(10) if x % 2 == 0]\n[0, 2, 4, 6, 8] \n>>> list(filter((lambda x: x % 2 == 0), range(10)))\n[0, 2, 4, 6, 8] \n>>> res = []\n>>> for x in range(10):\n        if x % 2 == 0:\n            res.append(x)\n>>> res\n[0, 2, 4, 6, 8]\nThough it’s penalized by having to code a function to be applied, the filter call\nhere is still not much longer than the list comprehension either. However, the list\ncomprehension can combine an if clause and an arbitrary expression, to give it\nthe effect of a filter and a map—in a single expression:\n>>> [x ** 2 for x in range(10) if x % 2 == 0]\n[0, 4, 16, 36, 64]\nThis time, we collect the squares of the even numbers from 0 through 9: the for\nloop skips numbers for which the attached if clause on the right is false, and the\nexpression on the left computes the squares. The equivalent map call would\nrequire a lot more work on our part—we would have to combine filter\nselections with map iteration, making for a noticeably more complex expression:\n>>> list( map((lambda x: x**2), filter((lambda x: x % 2 == 0), range(10))) )\n[0, 4, 16, 36, 64]\nFormal Comprehension Syntax\nIn fact, list comprehensions are more general still. In their simplest form, you",
      "content_length": 1558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 751,
      "chapter": null,
      "content": "must always code an accumulation expression and a single for clause:\n[ expression for target in iterable ]\nThough all other parts are optional, they allow richer iterations to be expressed\n—you can code any number of nested for loops in a list comprehension, and\neach may have an optional associated if test to act as a filter. The general\nstructure of list comprehensions looks like this:\n[ expression for target1 in iterable1 if condition1\n             for target2 in iterable2 if condition2 ...\n             for targetN in iterableN if conditionN ]\nThis exact same syntax is inherited by set and dictionary comprehensions as well\nas the generator expressions coming up, though these use different enclosing\ncharacters—curly braces for the first two and often-optional parentheses for the\nlatter—and the dictionary comprehension begins with two expressions separated\nby a colon (for key and value).\nWe experimented with the if filter clause in the previous section. When for\nclauses are nested within a list comprehension, they work like equivalent nested\nfor loop statements. For example:\n>>> res = [x + y for x in [0, 1, 2] for y in [100, 200, 300]]\n>>> res\n[100, 200, 300, 101, 201, 301, 102, 202, 302]\nThis has the same effect as this substantially more verbose equivalent:\n>>> res = []\n>>> for x in [0, 1, 2]:\n        for y in [100, 200, 300]:\n            res.append(x + y)\n>>> res\n[100, 200, 300, 101, 201, 301, 102, 202, 302]\nAlthough list comprehensions construct list results, remember that they can\niterate over any iterable object. Here’s a similar bit of code that traverses strings\ninstead of lists of numbers, and so collects concatenation results:",
      "content_length": 1664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 752,
      "chapter": null,
      "content": ">>> [x + y for x in 'orm' for y in 'ORM']                     # 3 * 3 results\n['oO', 'oR', 'oM', 'rO', 'rR', 'rM', 'mO', 'mR', 'mM']\nEach for clause can have an associated if filter, no matter how deeply the loops\nare nested—though use cases for the following sort of code, apart perhaps from\nthe multidimensional arrays ahead, start to become more and more difficult to\nimagine at this level:\n>>> [x + y for x in 'orm' if x in 'ro' for y in 'ORM' if y > 'M']\n['oO', 'oR', 'rO', 'rR']\n>>> [x + y + z for x in 'hack' if x > 'c'\n               for y in 'CODE' if y in 'OD'\n               for z in '123'  if z > '1']\n['hO2', 'hO3', 'hD2', 'hD3', 'kO2', 'kO3', 'kD2', 'kD3']\nFinally, here is a similar list comprehension that illustrates the effect of attached\nif selections on nested for clauses applied to numeric objects rather than\nstrings:\n>>> [(x, y) for x in range(5) if x % 2 == 0 \n                for y in range(5) if y % 2 == 1]\n[(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\nThis expression combines even numbers from 0 through 4 with odd numbers\nfrom 0 through 4. The if clauses filter out items in each iteration. Here is the\nequivalent statement-based code:\n>>> res = []\n>>> for x in range(5):\n        if x % 2 == 0:\n            for y in range(5):\n                if y % 2 == 1:\n                    res.append((x, y))\n>>> res\n[(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\nRecall that if you’re confused about what a complex list comprehension does,\nyou can always nest the list comprehension’s for and if clauses inside each\nother like this—indenting each clause successively further to the right—to derive",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 753,
      "chapter": null,
      "content": "the equivalent statements. The result is longer, but perhaps clearer in intent to\nsome human readers on first glance, especially those more familiar with basic\nstatements.\nThe map and filter equivalent of this last example would be wildly complex\nand deeply nested, so this book will leave its coding as an exercise for Zen\nmasters, ex–Lisp programmers, and those with far too much free time.\nExample: List Comprehensions and Matrixes\nNot all list comprehensions are so artificial, of course. Let’s look at one more\napplication to stretch our comprehension muscles. As we saw in Chapters 4 and\n8, one basic way to code matrixes (a.k.a. multidimensional arrays) in Python is\nwith nested list structures. The following, for example, defines two 3 × 3\nmatrixes as lists of nested lists:\n>>> M = [[1, 2, 3],\n         [4, 5, 6],\n         [7, 8, 9]]\n>>> N = [[2, 2, 2],\n         [3, 3, 3],\n         [4, 4, 4]]\nGiven this structure, we can always index rows, and columns within rows, using\nnormal index operations:\n>>> M[1]              # Row 2\n[4, 5, 6]\n>>> M[1][2]           # Row 2, item 3\n6\nList comprehensions are powerful tools for processing such structures because\nthey automatically scan rows and columns. For instance, although this code\nstores the matrix by rows, to collect the second column we can simply iterate\nacross the rows and pull out the desired column, or iterate through positions in\nthe rows and index as we go:\n>>> [row[1] for row in M]                          # Column 2",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 754,
      "chapter": null,
      "content": "[2, 5, 8]\n>>> [M[row][1] for row in (0, 1, 2)]               # Using offsets\n[2, 5, 8]\nGiven positions, we can also easily perform tasks such as pulling out a diagonal.\nThe first of the following expressions uses range to generate the list of offsets\nand then indexes with the row and column the same, picking out M[0][0], then\nM[1][1], and so on. The second scales the column index to fetch M[0][2], M[1]\n[1], etc. (we assume the matrix has the same number of rows and columns):\n>>> [M[i][i] for i in range(len(M))]               # Diagonals\n[1, 5, 9]\n>>> [M[i][len(M)-1-i] for i in range(len(M))]\n[3, 5, 7]\nChanging such a matrix in place requires assignment to offsets (use range twice\nif shapes differ):\n>>> L = [[1, 2, 3], [4, 5, 6]]\n>>> for i in range(len(L)):\n        for j in range(len(L[i])):                 # Update in place\n            L[i][j] += 10\n>>> L\n[[11, 12, 13], [14, 15, 16]]\nWe can’t really do the same with list comprehensions, as they make new lists,\nbut we could always assign their results to the original name for a similar effect.\nFor example, we can apply an operation to every item in a matrix, producing\nresults in either a simple vector or a matrix of the same shape:\n>>> [col + 10 for row in M for col in row]         # Assign to M to retain new value\n[11, 12, 13, 14, 15, 16, 17, 18, 19]\n>>> [[col + 10 for col in row] for row in M]\n[[11, 12, 13], [14, 15, 16], [17, 18, 19]]\nTo understand these, translate to their simple statement form equivalents that\nfollow—indent parts that are further to the right in the expression (as in the first",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 755,
      "chapter": null,
      "content": "loop in the following), and make a new list when comprehensions are nested on\nthe left (like the second loop in the following). As its statement equivalent makes\nclearer, the second expression in the preceding works because the row iteration\nis an outer loop: for each row, it runs the nested column iteration to build up one\nrow of the result matrix:\n>>> res = []\n>>> for row in M:                                  # Statement equivalents\n        for col in row:                            # Indent parts further right\n            res.append(col + 10)\n>>> res\n[11, 12, 13, 14, 15, 16, 17, 18, 19]\n>>> res = []\n>>> for row in M:\n        tmp = []                                   # Left-nesting starts new list\n        for col in row:\n            tmp.append(col + 10)\n        res.append(tmp)\n>>> res\n[[11, 12, 13], [14, 15, 16], [17, 18, 19]]\nFinally, with a bit of creativity, we can also use list comprehensions to combine\nvalues of multiple matrixes. The following first builds a flat list that contains the\nresult of multiplying the matrixes pairwise, and then builds a nested list structure\nhaving the same values by nesting list comprehensions again:\n>>> M\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n>>> N\n[[2, 2, 2], [3, 3, 3], [4, 4, 4]]\n>>> [M[row][col] * N[row][col] for row in range(3) for col in range(3)]\n[2, 4, 6, 12, 15, 18, 28, 32, 36]\n>>> [[M[row][col] * N[row][col] for col in range(3)] for row in range(3)]\n[[2, 4, 6], [12, 15, 18], [28, 32, 36]]\nThis last expression works because the row iteration is an outer loop again; it’s\nequivalent to this statement-based code:",
      "content_length": 1579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 756,
      "chapter": null,
      "content": "res = []\nfor row in range(3):\n    tmp = []\n    for col in range(3):\n        tmp.append(M[row][col] * N[row][col])\n    res.append(tmp)\nAnd for more fun, we can use zip to pair items to be multiplied—the following\ncomprehension and loop statement both produce the same list-of-lists pairwise\nmultiplication result as the last preceding example (and because zip is a\ngenerator of values, this isn’t as inefficient as it may seem):\n[[col1 * col2 for (col1, col2) in zip(row1, row2)] for (row1, row2) in zip(M, N)]\nres = []\nfor (row1, row2) in zip(M, N):\n    tmp = []\n    for (col1, col2) in zip(row1, row2):\n        tmp.append(col1 * col2)\n    res.append(tmp)\nCompared to their statement equivalents, the list comprehension versions here\nrequire only one line of code, might run substantially faster for large matrixes,\nand just might make your head explode. Which brings us to the next section.\nWhen not to use list comprehensions: Code obfuscation\nWith such generality, list comprehensions can quickly become, well,\nincomprehensible, especially when nested. Some programming tasks are\ninherently complex, and we can’t sugarcoat them to make them any simpler than\nthey are (see the upcoming permutations for a prime example). Tools like\ncomprehensions are powerful solutions when used wisely, and there’s nothing\ninherently wrong with using them in your scripts.\nAt the same time, code like that at the end of the prior section may introduce\nmore complexity than it should—and, frankly, tends to disproportionately spark\nthe interest of those holding the darker and misinformed assumption that code\nobfuscation somehow implies talent. In the interest of software citizenship, some\nperspective seems in order here.\nThis book demonstrates advanced comprehensions to teach, but in the real",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 757,
      "chapter": null,
      "content": "world, programming is not about being clever and obscure—it’s about how\nclearly your program communicates its purpose. Writing tricky comprehensions\nmay be a fun academic recreation, but it doesn’t work in programs that others\nwill someday need to understand.\nIn other words, the age-old acronym KISS applies here as always: Keep It\nSimple—traditionally followed either by a word that is now too sexist, or\nanother that is too colorful for a G-rated book like this. If you have to translate\ncode to simpler statements to understand it, it should probably be simpler\nstatements in the first place.\nWhen to use list comprehensions: Speed, conciseness, etc.\nNevertheless, in this case, there is currently a performance advantage to the extra\ncomplexity: based on tests run under Python today, map calls and list\ncomprehensions can be significantly faster than equivalent for loops. This speed\ndifference can vary per usage and Python, but is due to the fact that map and list\ncomprehensions run at compiled-language speed inside the interpreter, which is\ngenerally faster than running for loop bytecode within the PVM.\nAlso in the plus column, list comprehensions offer a code conciseness that’s\ncompelling and even warranted when that reduction in size doesn’t also imply a\nreduction in meaning for the next programmer; many find the expressiveness of\ncomprehensions to be a powerful ally; and because map and list comprehensions\nare both expressions, they also can show up syntactically in places that for loop\nstatements cannot, such as in lambda and object literals.\nBecause of all this, list comprehensions and map calls are worth knowing and\nusing for simple sorts of iterations, especially if your application’s speed is an\nimportant consideration. For example, it’s hard to argue with the ease and power\nof code like either of the following:\n[line.rstrip() for line in open('myfile')]\nmap((lambda line: line.rstrip()), open('myfile'))\nTo achieve the same memory economy and execution time division as map,\nthough, list comprehensions must be coded as generator expressions—which is\nwhy we’ve run through this review in the first place, and one of the major topics",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 758,
      "chapter": null,
      "content": "this chapter turns to next.\nGenerator Functions and Expressions\nPython today supports procrastination much more than it did in the past—it\nprovides tools that produce results only when needed, instead of all at once.\nWe’ve seen this at work in built-in tools: files that read lines on request, and\nfunctions like map and zip that produce items on demand. Such laziness isn’t\nconfined to Python itself, though. In particular, two language constructs delay\nresult creation whenever possible in user-defined operations:\nGenerator functions are coded as normal def statements, but use yield\nstatements to return results one at a time, suspending and resuming their\nstate between each.\nGenerator expressions are similar to the list comprehensions of the\nprior section, but they return an object that produces results on demand\ninstead of building a result list.\nBecause neither of these constructs a result list all at once, they both save\nmemory space and allow computation to be split across requests to avoid pauses\nand enable large (and even “infinite”) results. As you’ll see, both of these\nultimately perform their delayed-results magic by implementing the iteration\nprotocol we studied in Chapter 14.\nThese features date back to at least Python 2.2 (and were explored even earlier in\nlanguages like Icon), and are common in Python code today. Though they may\ninitially seem unusual if you’re accustomed to simpler models, you’ll probably\nfind generators to be a powerful tool. Moreover, because they are a natural\nextension to the function, comprehension, and iteration topics we’ve already\nexplored, you already know more about coding generators than you might\nexpect.\nGenerator Functions: yield Versus return\nIn this part of the book, we’ve learned about coding normal functions that\nreceive input parameters and send back a single result immediately. It is also",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 759,
      "chapter": null,
      "content": "possible, however, to write functions that may send back a value and later be\nresumed, picking up where they left off. Such functions are known as generator\nfunctions because they generate a series of values over time instead of stopping\nafter just one.\nGenerator functions are like normal functions in most respects, and in fact are\ncoded with normal def statements. However, when created, they are compiled\nspecially into an object that supports the iteration protocol. When called, they\ndon’t return a result: they return a result generator that can appear in any\niteration context. We studied iterables in Chapter 14, and Figure 14-1 gave a\nformal and graphic summary of their operation. Here, we’ll revisit the subject to\nsee how it applies to generators.\nState suspension\nUnlike normal functions that return a value and exit, generator functions\nautomatically suspend and resume their execution and state around the point of\nvalue generation. Because of that, they are often a useful alternative to both\ncomputing an entire series of values up front and manually saving and restoring\nstate in classes. The state that generator functions retain when they are\nsuspended includes both their code location and their entire local scope. Hence,\ntheir local variables retain information between results, and make it available\nwhen the functions are resumed.\nThe chief code difference between generator and normal functions is that a\ngenerator yields a value, rather than returning one—the yield statement\nsuspends the function and sends a value back to the caller, but retains enough\nstate to enable the function to resume from where it left off. When resumed, the\nfunction continues execution immediately after the last yield run. From the\nfunction’s perspective, this allows its code to produce a series of values over\ntime, rather than computing them all at once and sending them back in\nsomething like a list.\nIteration protocol integration\nTo truly understand generator functions, you need to know that they are closely\nbound up with the notion of the iteration protocol in Python. As we’ve seen,\niterator objects define a __next__ method, which either returns the next item in",
      "content_length": 2181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 760,
      "chapter": null,
      "content": "the iteration, or raises the StopIteration exception to end the iteration. An\niterable object’s iterator is fetched initially with the __iter__ method, though\nthis step is a no-op for objects that are their own iterator.\nPython for loops, and all other iteration tools, use this iteration protocol to step\nthrough a sequence or value generator, if the protocol is supported (if not,\niteration falls back on repeatedly indexing sequences instead). Any object that\nsupports this interface works in all iteration tools, and the iter and next built-\nins simplify manual iteration by calling the corresponding double-underscore\nmethod (or its internal equivalent).\nTo support this protocol, functions containing a yield statement are compiled\nspecially as generators—they are not normal functions, but rather are built to\nreturn an object with the expected iteration-protocol methods. When later called,\nsuch functions return an iterable object that supports the iteration protocol with\nan automatically created method named __next__ to start or resume execution.\nBesides yield, generator functions may also use a return statement that, along\nwith falling off the end of the def block, terminates the generation of values by\nautomatically raising a StopIteration exception. A generator’s return can\nalso give an object that becomes the value attribute of the StopIteration\nexception, but it’s ignored by iteration tools and uncommon. From the caller’s\nperspective, the generator’s __next__ method simply starts or resumes the\nfunction and runs until either the next yield result is returned, or a\nStopIteration is raised.\nThe net effect is that generator functions, coded as def statements containing\nyield statements, are automatically made to support the iteration methods\nprotocol and thus may be used in any iteration tool to produce results over time\nand on demand. The presence of a yield in a def suffices to make all this\nhappen, and none of this applies to lambda because it doesn’t allow statements\nlike yield (which is yet another reason to prefer def).\nGenerator functions in action\nAs usual, this is probably simpler in code than narrative. The following defines a\ngenerator function that can be used to produce the squares of a series of numbers",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 761,
      "chapter": null,
      "content": "—over time:\n>>> def gensquares(n):\n        for i in range(n):\n            yield i ** 2        # Resume here later\nThis function yields a value, and so returns to its caller, each time through the\nloop. When it is resumed, its prior state is restored, including the last values of\nits variables i and n, and control picks up again immediately after the yield\nstatement. For example, when it’s used in the body of a for loop, the first\niteration starts the function and gets its first result; thereafter, control returns to\nthe function after its yield statement each time through the loop:\n>>> for i in gensquares(5):     # Resume the function\n        print(i, end=' ')       # Print last yielded value\n0 1 4 9 16\nTo end the generation of values, functions either use a return statement or\nsimply allow control to fall off the end of the function body. As noted, return\ncan give a value attached to the exit exception, but usually does not.\nTo most people, this process seems a bit implicit (if not enchanted) on first\nencounter. It’s actually quite tangible, though. If you really want to see what is\ngoing on inside the for, call the generator function directly:\n>>> x = gensquares(3)           # Generator of 0..2 squares\n>>> x\n<generator object gensquares at 0x10dc6d700>\nYou get back a generator object that supports the iteration protocol—the\ngenerator function was compiled to return this automatically. The returned\ngenerator object in turn has a __next__ method that starts the function or\nresumes it from where it last yielded a value, and raises a StopIteration\nexception when the end of the series of values is reached and the function\nreturns. As noted, next(X) here calls X.__next__() for convenience:\n>>> next(x)                     # Run to first yield\n0",
      "content_length": 1769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 762,
      "chapter": null,
      "content": ">>> next(x)                     # Resume after yield, run to next yield\n1\n>>> next(x)                     # Resume after yield, run to next yield\n4\n>>> next(x)                     # Resume after yield, exception on return\nStopIteration\nPer Chapter 14, for loops and other iteration tools work with generators in the\nsame way—by calling the __next__ method repeatedly, until the exit exception\nis caught. For generators, the result produces yielded values over time.\nNotice that the top-level iter call of the iteration protocol isn’t required here\nbecause generators are their own iterator, supporting just one active iteration\nscan. To put that another way, generators return themselves for iter and support\nnext directly. This also holds true in the generator expressions you’ll meet later\nin this chapter:\n>>> y = gensquares(5)           # Returns a generator which is its own iterator\n>>> iter(y) is y                # iter() is not required: a no-op here\nTrue\n>>> next(y)                     # Can run next() immediately\n0\nWhy generator functions?\nGiven the simple example we’re using to illustrate fundamentals, you might be\nwondering just why you’d ever care to code a generator at all. In code so far, for\ninstance, we could simply build the list of result values all at once:\n>>> def buildsquares(n):\n        res = []\n        for i in range(n): res.append(i ** 2)\n        return res\n>>> for x in buildsquares(5): print(x, end=' ')\n0 1 4 9 16\nFor that matter, we could use any of the for loop, map, or list comprehension\ntechniques we’ve already mastered:\n>>> for x in [n ** 2 for n in range(5)]:",
      "content_length": 1604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 763,
      "chapter": null,
      "content": "print(x, end=' ')\n0 1 4 9 16\n>>> for x in map((lambda n: n ** 2), range(5)):\n        print(x, end=' ')\n0 1 4 9 16\nHowever, generators can be better in terms of both memory use and performance\nin larger programs. They allow functions to avoid doing all the work up front,\nwhich is especially useful when the result lists are large or when it takes a lot of\ncomputation to produce each value. Generators distribute the time required to\nproduce the series of values among loop iterations, and can even produce a series\nof values that has no end at all (an “infinite” result).\nMoreover, for more advanced uses, generators can provide a simpler alternative\nto manually saving the state between iterations in class objects—with generators,\nvariables accessible in the function’s scopes are saved and restored\nautomatically. You’ll be able to judge this contrast after we discuss class-based\niterables in more detail in Part VI.\nGenerator functions are also more broadly focused than implied so far. They can\noperate on and return any type of object, and as iterables may appear in any of\nChapter 14’s iteration tools, including tuple calls, enumerations, and dictionary\ncomprehensions:\n>>> def ups(line):\n        for sub in line.split(','):          # Substring generator\n            yield sub.upper()\n>>> tuple(ups('aaa,bbb,ccc'))                # All iteration contexts\n('AAA', 'BBB', 'CCC')\n>>> {i: s for (i, s) in enumerate(ups('aaa,bbb,ccc'))}\n{0: 'AAA', 1: 'BBB', 2: 'CCC'}\nIn a moment you’ll observe the same assets for generator expressions—a tool\nthat trades function flexibility for comprehension conciseness. Later in this\nchapter you’ll also see that generators can sometimes enable otherwise\nimpractical tasks, by producing components of result sets that would be far too",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 764,
      "chapter": null,
      "content": "large to create all at once. First, though, let’s explore some advanced generator\nfunction features.\nExtended generator function protocol: send versus next\nSomewhere along generators’ evolutionary path (in Python 2.5), a send method\nwas added to the generator function protocol. The send method advances to the\nnext item in the series of results, just like __next__, but also provides a way for\nthe caller to communicate with the generator, and hence to affect its operation.\nTechnically, yield is an expression form that returns the item passed to send,\nnot a statement. It can be coded either way (and usually is a statement), but when\nused as an expression must be enclosed in parentheses unless it’s the only item\non the right side of an assignment statement. For example, X = yield Y is OK,\nas is X = (yield Y) + Z.\nWhen this extra protocol is used, values are sent into a generator G by calling\nG.send(value). The generator’s code is then resumed, and the yield\nexpression inside the generator returns the value passed to send by the caller. If\nthe regular G.__next__() method (or its next(G) equivalent) is called to\nadvance, the yield simply returns None. For example:\n>>> def gen():\n       for i in range(10):\n           X = yield i\n           print('=>', X)\n>>> G = gen()\n>>> next(G)              # Must call next() first, to start generator\n0\n>>> G.send(77)           # Advance, and send value to yield expression\n=> 77\n1\n>>> G.send(88)\n=> 88\n2\n>>> next(G)              # next() and X.__next__() send None\n=> None\n3\nThe send method can be used, for example, to code a generator that its caller can",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 765,
      "chapter": null,
      "content": "terminate by sending a termination code, or redirect by passing a new position in\ndata being processed inside the generator.\nIn addition, generators also support a throw(type) method to raise an exception\ninside the generator at the latest yield, and a close method that raises a special\nGeneratorExit exception inside the generator to terminate the iteration entirely.\nTogether with send, these are advanced features added to make generators more\nlike a tool called coroutines, a role eventually subsumed in part by the upcoming\nasync. Hence, we won’t delve further here; see Python’s standard manuals for\nmore information, and watch for more on exceptions in Part VII.\nNote, though, that while Python provides a next(X) convenience built-in that\ncalls the X.__next__() method of an object, other generator methods, like send,\nmust be called as methods of generator objects directly (e.g., G.send(X)). This\nmakes sense if you realize that these extra methods are implemented on built-in\ngenerator objects only, whereas the __next__ method applies to all iterable\nobjects—both built-in types and user-defined classes.\nThe yield from extension\nEven later, Python 3.3 introduced extended syntax for the yield statement with\na from generator clause that allows generators to delegate to nested generators\n(known as subgenerators). In simple cases, it’s the equivalent to a yielding for\nloop. As a demo, the list call in the following forces a generator to produce\nvalues from two sources:\n>>> def both(N):\n        for i in range(N): yield i\n        for i in map(lambda x: x ** 2, range(N)): yield i\n>>> list(both(5))\n[0, 1, 2, 3, 4, 0, 1, 4, 9, 16]\nThe yield from syntax makes this more concise and explicit, and supports all\nthe usual generator usage contexts. In the following, both is called a delegating\ngenerator, and the range and map built-ins serve as subgenerators (join also\nuses a generator expression here: see the next section):",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 766,
      "chapter": null,
      "content": ">>> def both(N):\n        yield from range(N)\n        yield from map(lambda x: x ** 2, range(N))\n>>> list(both(5))\n[0, 1, 2, 3, 4, 0, 1, 4, 9, 16]\n>>> ' : '.join(str(i) for i in both(5))\n'0 : 1 : 2 : 3 : 4 : 0 : 1 : 4 : 9 : 16'\nThe yield from also supports arbitrary chains of generators. In the following,\nfor example, two generators yield from others, the last of which ultimately\nuses yield to send results up though the chain:\n>>> def c(n):\n        yield n * 8                   # The bottom of the \"chain\"\n        yield n ** 2\n>>> def b(n):\n        yield from c(n)               # Delegate to subgenerator\n \n>>> def a(n):\n        yield from b(n)               # Delegate to subgenerator\n>>> g = a(4)                          # Make top generator, fetch results\n>>> g\n<generator object a at 0x10bd17940>\n>>> next(g)\n32\n>>> for i in a(4): print(i)           # Force results production\n32\n16\nIn more advanced roles, though, this extension allows subgenerators to receive\nsent and thrown values directly from the delegating generator’s caller, and return\na final value to the delegating generator as the result of the yield from\nexpression. The net effect allows generators to be split into multiple\nsubgenerators much as a single function can be split into multiple subfunctions,\nbut still operate as though their code appeared inline where the yield from\nappears.\nSince this is both uncommon and beyond this chapter’s scope, we’ll again defer",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 767,
      "chapter": null,
      "content": "to Python’s standard manuals for more details. For an additional yield from\nexample, see the solution to Exercise 11 from “Test Your Knowledge: Part IV\nExercises”, and stay tuned for a glimpse of async def coroutines ahead—whose\nawait is something like a generator yield from, with a delegation to an event\nloop to allow other tasks to be run during pauses. Here, let’s move on to a tool\nclose enough to yield to be called a fraternal twin.\nGenerator Expressions: Iterables Meet Comprehensions\nBecause the delayed evaluation of generator functions was so useful, later\nPythons eventually combined the notions of iterables and comprehensions in a\nnew tool: generator expressions. Syntactically, generator expressions are just\nlike normal list comprehensions, and support all their syntax—including if\nfilters and for loop nesting—but they are enclosed in parentheses instead of\nsquare brackets (like tuples, their enclosing parentheses are often optional):\n>>> [x ** 2 for x in range(5)]          # List comprehension: build a list\n[0, 1, 4, 9, 16]\n>>> (x ** 2 for x in range(5))          # Generator expression: make an iterable\n<generator object <genexpr> at 0x109607ac0>\nIn fact, at least on a functionality basis, coding a list comprehension is\nessentially the same as wrapping a generator expression in a list built-in call to\nforce it to produce all its results in a list at once:\n>>> list(x ** 2 for x in range(5))      # List comprehension equivalence\n[0, 1, 4, 9, 16]\nOperationally, however, generator expressions are very different: instead of\nbuilding the result list in memory, they return a generator object—an\nautomatically created iterable. This iterable object in turn supports the iteration\nprotocol to yield one piece of the result list at a time in any iteration tool. The\niterable object also retains generator state while active—the variable x in the\npreceding expressions, along with the generator’s code location.\nThe net effect is much like that of generator functions, but in the context of a\ncomprehension expression: we get back an object that remembers where it left",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 768,
      "chapter": null,
      "content": "off after each part of its result is returned. Also like generator functions, looking\nunder the hood at the protocol that these objects automatically support can help\ndemystify them; the iter call is again not required at the top here, for reasons\nwe’ll expand on ahead:\n>>> G = (x ** 2 for x in range(3))\n>>> iter(G) is G                        # iter(G) optional: __iter__ returns self\nTrue\n>>> next(G)                             # Generator objects: automatic methods\n0\n>>> next(G)\n1\n>>> next(G)\n4\n>>> next(G)\nStopIteration\n>>> G\n<generator object <genexpr> at 0x109607920>\nAgain, we don’t typically see the next iterator machinery under the hood of a\ngenerator expression like this because for loops and similar tools trigger it for\nus automatically:\n>>> for num in (x ** 2 for x in range(3)):        # Calls next() automatically\n        print(num, num / 2.0, sep=', ')\n0, 0.0\n1, 0.5\n4, 2.0\nAs we’ve already seen, every iteration tool does this—including for loops; the\nlist call we used earlier; comprehensions of all kinds; the map and filter\nfunctional tools; and other iteration tools we explored in Chapter 14, including\nthe sum, sorted, any, and all built-in functions. As iterables, generator\nexpressions can be used in any of these iteration tools, just like both physical\nsequences and the results of a generator-function call.\nFor example, the following deploys generator expressions in the string join\nmethod call and tuple assignment, iteration tools both. Recall that join with an\nempty-string delimiter concatenates values produced by its iterable:",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 769,
      "chapter": null,
      "content": ">>> ''.join(x.upper() for x in 'aaa,bbb,ccc'.split(','))\n'AAABBBCCC'\n>>> a, b, c = (x + '\\n' for x in 'aaa,bbb,ccc'.split(','))\n>>> a, c\n('aaa\\n', 'ccc\\n')\nNotice how the join call in the preceding doesn’t require extra parentheses\naround the generator. Syntactically, parentheses are not required around a\ngenerator expression that is the sole item already enclosed in parentheses used\nfor other purposes—like those of a function call. Parentheses are required in all\nother cases, however, even if they seem extra, as in the second call to sorted\nthat follows:\n>>> sum(x ** 2 for x in range(3))                           # Parens optional\n5\n>>> sorted(x ** 2 for x in range(3))                        # Parens optional\n[0, 1, 4]\n>>> sorted((x ** 2 for x in range(3)), reverse=True)        # Parens required!\n[4, 1, 0]\nLike the often-optional parentheses in tuples, there is no widely accepted rule on\nthis, though a generator expression does not have as clear a role as a fixed\ncollection of other objects as a tuple, making extra parentheses seem perhaps\nmore spurious here.\nWhy generator expressions?\nJust like generator functions, generator expressions are a memory-space\noptimization—they do not require the entire result list to be constructed all at\nonce, as the square-bracketed list comprehension does. Also like generator\nfunctions, they divide the work of results production into smaller time slices—\nthey yield results in piecemeal fashion, instead of making the caller wait for the\nfull set to be created in a single call.\nOn the other hand, because generator expressions often run slower today than\nlist comprehensions, they may be best reserved for result sets that are very large,\nor programs that cannot wait for full results generation. A more authoritative\nstatement about performance, though, will have to await the timing scripts we’ll\ncode in the next chapter.",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 770,
      "chapter": null,
      "content": "On the subjective upside, generator expressions also offer significant coding\nadvantages—as the next sections show.\nGenerator expressions versus map\nOne way to see the coding benefits of generator expressions is to compare them\nto other functional tools, as we did for list comprehensions. For example,\ngenerator expressions often are equivalent to map calls, because both generate\nresult items on request. Like list comprehensions, though, generator expressions\nmay be simpler to code when the operation applied is not a function call:\n>>> list(map(abs, (-1, -2, 3, 4)))                        # Map function on tuple\n[1, 2, 3, 4]\n>>> list(abs(x) for x in (-1, -2, 3, 4))                  # Generator expression\n[1, 2, 3, 4]\n>>> list(map(lambda x: x * 2, (1, 2, 3, 4)))              # Nonfunction case\n[2, 4, 6, 8]\n>>> list(x * 2 for x in (1, 2, 3, 4))                     # Simpler as generator?\n[2, 4, 6, 8]\nThe same holds true for text-processing use cases like the join call we saw\nearlier—a list comprehension makes an extra temporary list of results, which is\ncompletely pointless in this context because the list is not retained, and map loses\nsimplicity points compared to generator expression syntax when the operation\nbeing applied is not a call:\n>>> line = 'aaa,bbb,ccc'\n>>> ''.join([x.upper() for x in line.split(',')])         # Makes temporary list\n'AAABBBCCC'\n>>> ''.join(x.upper() for x in line.split(','))           # Generates results\n'AAABBBCCC'\n>>> ''.join(map(str.upper, line.split(',')))              # Generates results\n'AAABBBCCC'\n>>> ''.join(x * 2 for x in line.split(','))               # Simpler as generator?\n'aaaaaabbbbbbcccccc'\n>>> ''.join(map(lambda x: x * 2, line.split(',')))\n'aaaaaabbbbbbcccccc'\nBoth map and generator expressions can also be arbitrarily nested; when coded",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 771,
      "chapter": null,
      "content": "this way, a list call or other iteration tool starts the entire process of producing\nresults. For example, the list comprehension in the following produces the same\nresult as the map and generator equivalents that follow it, but makes two physical\nlists. The others generate just one integer at a time with nested generators, and\nthe generator expression form may more clearly reflect its intent:\n>>> [x * 2 for x in [abs(x) for x in (-1, -2, 3, 4)]]        # Nested list comps\n[2, 4, 6, 8]\n>>> list(map(lambda x: x * 2, map(abs, (-1, -2, 3, 4))))     # Nested maps\n[2, 4, 6, 8]\n>>> list(x * 2 for x in (abs(x) for x in (-1, -2, 3, 4)))    # Nested generators\n[2, 4, 6, 8]\nAlthough the effect of all three of these is to combine operations, the generators\ndo so without making multiple temporary lists. In fact, the next example both\nnests and combines generators—the nested generator expression is activated by\nmap, which in turn is only activated by list:\n>>> import math\n>>> list(map(math.sqrt, (x ** 2 for x in range(4))))         # Nested combos\n[0.0, 1.0, 2.0, 3.0]\nTechnically speaking, the range on the right in the preceding is a value\ngenerator too, activated by the generator expression itself—forming three levels\nof value generation, which produce individual values from inner to outer only on\nrequest, and which “just works” because of Python’s iteration tools and protocol.\nIn fact, generator nestings can be arbitrarily mixed and deep, though some may\nbe more valid than others:\n>>> list(map(abs, map(abs, map(abs, (-1, 0, 1)))))           # Nesting gone bad?\n[1, 0, 1]\n>>> list(abs(x) for x in (abs(x) for x in (abs(x) for x in (-1, 0, 1))))\n[1, 0, 1]\nThese last examples illustrate how general generators can be, but are also coded\nin an intentionally complex form to underscore that generator expressions have\nthe same potential for abuse as the list comprehensions discussed earlier—as",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 772,
      "chapter": null,
      "content": "usual, you should keep them simple unless they must be complex.\nWhen used well, though, generator expressions combine the expressiveness of\nlist comprehensions with the space and time benefits of other iterables. The\nfollowing nonnested alternatives, for example, provide simpler solutions to the\npreceding three listings’ nested codings, but still leverage generators’ strengths:\n>>> list(abs(x) * 2 for x in (-1, -2, 3, 4))                 # Same, unnested\n[2, 4, 6, 8]\n>>> list(math.sqrt(x ** 2) for x in range(4))                # Flat may be better\n[0.0, 1.0, 2.0, 3.0]\n>>> list(abs(x) for x in (-1, 0, 1))\n[1, 0, 1]\nGenerator expressions versus filter\nGenerator expressions also support all the usual list comprehension syntax—\nincluding if clauses, which work like the filter call we met earlier. Because\nfilter is an iterable that generates its results on request, a generator expression\nwith an if clause is operationally equivalent. Again, the join in the following is\nan iteration tool that suffices to force all forms to produce their results:\n>>> line = 'aa bbb c'\n>>> ''.join(x for x in line.split() if len(x) > 1)           # Generator with if\n'aabbb'\n>>> ''.join(filter(lambda x: len(x) > 1, line.split()))      # Equivalent filter\n'aabbb'\nThe generator seems just marginally simpler than the filter here. As for list\ncomprehensions, though, adding processing steps to filter results requires a\nmap too, which makes filter noticeably more complex:\n>>> ''.join(x.upper() for x in line.split() if len(x) > 1)\n'AABBB'\n>>> ''.join(map(str.upper, filter(lambda x: len(x) > 1, line.split())))\n'AABBB'\nIn effect, generator expressions provide more general coding structures that do\nnot rely on functions, but still delay results production. Also like list\ncomprehensions, there is always a statement-based equivalent to a generator",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 773,
      "chapter": null,
      "content": "expression, though it sometimes renders substantially more code:\n>>> res = ''\n>>> for x in line.split():                              # Statement equivalent?\n        if len(x) > 1:                                  # This is also a join\n            res += x.upper()\n>>> res\n'AABBB'\nIn this case, though, the statement form isn’t quite the same—it cannot produce\nitems one at a time, and it’s also emulating the effect of the join that forces\nresults to be produced all at once. The true equivalent to a generator expression\nwould be a generator function with a yield, as the next section will show.\nGenerator Functions Versus Generator Expressions\nLet’s recap what we’ve covered so far in this section:\nGenerator functions\nA function def statement that contains a yield statement is turned into a\ngenerator function. When called, it returns a new generator object with\nautomatic retention of local scope and code position; an automatically\ncreated __iter__ method that simply returns itself; and an automatically\ncreated __next__ method that starts the function or resumes it where it last\nleft off, and raises StopIteration when finished producing results.\nGenerator expressions\nA comprehension expression enclosed in parentheses is known as a generator\nexpression. When run, it returns a new generator object with the same\nautomatically created method interface and state retention as a generator\nfunction call’s results—with an __iter__ method that simply returns itself;\nand a __next__ method that starts the implied loop or resumes it where it",
      "content_length": 1547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 774,
      "chapter": null,
      "content": "last left off, and raises StopIteration when finished producing results.\nThe net effect is to automatically produce results on demand in all iteration tools\nthat employ either of these.\nAs implied by preceding coverage, the same iteration can often be coded with\neither a generator function or a generator expression. The following generator\nexpression, for example, repeats each character in a string four times:\n>>> G = (c * 4 for c in 'hack')           # Generator expression\n>>> list(G)                               # Force generator to produce results\n['hhhh', 'aaaa', 'cccc', 'kkkk']\nThe equivalent generator function requires slightly more code, but its multiple-\nstatement block will be able to code more logic and use more state information if\nneeded. In fact, this is essentially the same as the prior chapter’s trade-off\nbetween lambda and def—expression conciseness versus statement power:\n>>> def timesfour(S):                     # Generator function\n        for c in S:\n            yield c * 4\n>>> G = timesfour('hack')\n>>> list(G)                               # Iterate automatically\n['hhhh', 'aaaa', 'cccc', 'kkkk']\nTo their clients, the two are more similar than different. For instance, both\nexpressions and functions support both automatic and manual iteration—the\nprior list call iterates automatically, and the following iterate manually:\n>>> G = (c * 4 for c in 'hack')\n>>> I = iter(G)                           # Iterate manually (expression)\n>>> next(I)\n'hhhh'\n>>> next(I)\n'aaaa'\n>>> G = timesfour('hack')\n>>> I = iter(G)                           # Iterate manually (function)\n>>> next(I)\n'hhhh'\n>>> next(I)",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 775,
      "chapter": null,
      "content": "'aaaa'\nIn either case, Python automatically creates a generator object, which has both\nthe methods required by the iteration protocol, and state retention for variables in\nthe generator’s code and its current code location. Notice how we make new\ngenerators here to iterate again—as explained in the next section, generators are\nsingle-pass iterators.\nFirst, though, here’s the true statement-based equivalent of the expression at the\nend of the prior section: a function that yields values—though the difference is\nirrelevant if the code using it produces all results with a tool like join:\n>>> line = 'aa bbb c'\n>>> ''.join(x.upper() for x in line.split() if len(x) > 1)    # Expression\n'AABBB'\n>>> def gensub(line):                                         # Function\n        for x in line.split():\n            if len(x) > 1:\n                yield x.upper()\n>>> ''.join(gensub(line))                                     # But why generate?\n'AABBB'\nWhile generators have valid roles, in cases like this the use of generators over\nthe simple statement equivalent shown earlier may be difficult to justify, except\non stylistic grounds: if you’re just going to immediately join generated results\nanyhow, you might as well skip generators and use simple loops. On the other\nhand, trading four lines for the generator expression’s one may to many seem\nfairly compelling stylistic grounds!\nGenerator Odds and Ends\nThis section wraps up generators with a quick rundown of associated but lesser\ntopics. After this, we’ll move on to larger examples, but make sure you have a\nhandle on the smaller bits before jumping ahead.\nGenerators are single-pass iterables\nFirst up, a subtle but important point: both generator functions and generator",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 776,
      "chapter": null,
      "content": "expressions are their own iterators and thus support just one active iteration—\nunlike some built-in types, you can’t have multiple iterators of either generator\npositioned at different locations in the stream of results. Because of this, a\ngenerator’s iterator is the generator itself; in fact, as suggested earlier, calling\niter on a generator expression or function is an optional no-op:\n>>> G = (c * 4 for c in 'hack')\n>>> iter(G) is G                          # My iterator is myself: G has __next__\nTrue\nIf you do iterate over the results stream manually with multiple iterators, they\nwill all point to the same position:\n>>> G = (c * 4 for c in 'hack')           # Make a new generator\n>>> I1 = iter(G)                          # Iterate manually\n>>> next(I1)\n'hhhh'\n>>> next(I1)\n'aaaa'\n>>> I2 = iter(G)                          # Second iterator at same position!\n>>> next(I2)\n'cccc'\nMoreover, once any iteration runs to completion, all are exhausted—we have to\nmake a new generator to start again:\n>>> list(I1)                              # Collect the rest of I1's items\n['kkkk'] \n>>> next(I2)                              # Other iterators exhausted too\nStopIteration\n>>> I3 = iter(G)                          # Ditto for new iterators\n>>> next(I3)\nStopIteration\n>>> I3 = iter(c * 4 for c in 'hack')      # New generator to start over\n>>> next(I3)\n'hhhh'\nThe same holds true for generator functions—the following def statement-based\nequivalent supports just one active iterator and is exhausted after one pass:",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 777,
      "chapter": null,
      "content": ">>> def timesfour(S):\n        for c in S:\n            yield c * 4\n>>> G = timesfour('hack')                 # Generator functions work the same way\n>>> iter(G) is G                          # One scan per generator (call)\nTrue\n>>> I1, I2 = iter(G), iter(G)\n>>> next(I1)\n'hhhh'\n>>> next(I1)\n'aaaa'\n>>> next(I2)                              # I2 at same position as I1\n'cccc'\nThis is different from the behavior of some built-in objects like lists, which\nsupport multiple iterators and passes and even reflect their in-place changes in\nactive iterators:\n>>> L = [1, 2, 3, 4]\n>>> I1, I2 = iter(L), iter(L)\n>>> next(I1)\n1\n>>> next(I1)\n2\n>>> next(I2)                              # Lists support multiple iterators\n1\n>>> del L[2:]                             # Changes reflected in iterators\n>>> next(I1)\nStopIteration\nThough not readily apparent in these simple examples, this can matter in your\ncode: if you wish to scan a generator’s values multiple times, you must either\ncreate a new generator for each scan or build a rescannable list out of its values\n—a single generator’s values will be consumed and exhausted after a single pass.\nSee this chapter’s sidebar “Why You Will Care: Iteration Versus Python Morph”\nfor a prime example of the sort of code impacted by this.\nWhen we begin coding class-based iterables in Part VI, you’ll also see that it’s\nup to us to decide how many iterations we wish to support for our objects, if any.\nIn general, objects that wish to support multiple scans will return supplemental\nclass objects instead of themselves for iter. The next section previews more of\nthis model.",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 778,
      "chapter": null,
      "content": "Generation in built-ins and classes\nAlthough we’ve focused on coding value generators ourselves in this section,\ndon’t forget that many built-in types behave the same way. As we saw in\nChapter 14, for example, dictionaries and files generate results too:\nfor key in dictionary: …                          # See Chapter 14 et al.\nfor line in file: …\nThough beyond this book’s scope, many Python standard-library tools generate\nvalues too, including its directory walker—which at each level of a folder tree\nyields a tuple of the current directory, its subdirectories, and its files:\n>>> import os\n>>> for (root, subs, files) in os.walk('..'):     # Directory-walk generator\n        for name in files:                        # A Python 'find' operation\n            if name.endswith('.py'):              # Also os.path.join(root, name)\n                print(root, name)\n../Chapter02 script0.py\n../Chapter03 myfile.py\n../Chapter03 script1.py\n…etc…\nBecause the current Python-coded implementation of os.walk uses yield to\nreturn results, it’s a normal generator function, and hence iterable object, that\ngenerates a three-item tuple on each iteration:\n>>> G = os.walk('..')       # A single-scan iterator: iter(G) optional\n>>> next(G)\n('..', ['Chapter02', 'Chapter03', 'Chapter04', …etc… 'Chapter37'], [])\n>>> next(G)\n('../Chapter02', ['__pycache__'], ['script0.py'])\nBy yielding results as it goes, the walker does not require its clients to wait for\nan entire tree to be scanned. See Python’s manuals for more on this tool. For\nsystem-tools fans, also see the next chapter for an example that uses os.popen—\na related iterable used to run a shell command and read its output. And for\nadditional examples of built-in value generators and the iteration tools that use\nthem, review Chapter 14.",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 779,
      "chapter": null,
      "content": "Finally, although beyond the scope of this chapter, it is also possible to\nimplement arbitrary user-defined generator objects with classes that conform to\nthe iteration protocol. Such classes define __iter__ and __next__ methods\nexplicitly to support the protocol:\nclass SomeIterable:\n    def __iter__(...): ...     # On iter(): return self or supplemental object\n    def __next__(...): ...     # On next(): coded here, or in another class\nAs the prior section suggested, these classes usually return their objects directly\nfor single-iteration behavior, or a supplemental object with scan-specific state for\nmultiple-scan support.\nAlternatively, a user-defined iterable class’s __iter__ may use yield to\ntransform itself into a generator, and a __getitem__ indexing method is also\navailable as a fallback option for iteration with trade-offs we’ll explore later.\nHowever this is coded, the iterator and generator story won’t really be complete\nuntil we’ve seen how it also maps to classes. For now, we’ll have to settle for\npostponing its conclusion—and its final sequel—until Chapter 30.\nComprehensions versus type calls and generators\nWe’ve also been focusing on list comprehensions and generators in this chapter,\nbut keep in mind that there are two other comprehension expression forms: the\nset and dictionary comprehensions we met earlier in Chapter 14 and Part II. For\nreference and closure, here’s a summary of all the comprehension forms in\nPython:\n>>> [x * x for x in range(10)]               # List comprehension: builds list\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]         # Fixed order, made all at once\n>>> (x * x for x in range(10))               # Generator expression: yields items\n<generator object <genexpr> at 0x100a1f920>\n>>> {x * x for x in range(10)}               # Set comprehension: builds set\n{0, 1, 64, 4, 36, 9, 16, 49, 81, 25}         # Random order, no duplicates\n>>> {x: x * x for x in range(10)}            # Dict comprehension: insert order\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\nAll of these forms use the same formal comprehension syntax we met earlier, but",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 780,
      "chapter": null,
      "content": "their enclosing delimiters (and key, for dictionaries) denote their differing roles.\nIn a sense, list, set, and dictionary comprehensions are syntactic sugar for\npassing generator expressions to type names. Because all accept any iterable, a\ngenerator works here too:\n>>> list(x ** 2 for x in range(10))          # Generator plus type name\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n>>> set(x * x for x in range(10))            # Similar to a set comprehension\n{0, 1, 64, 4, 36, 9, 16, 49, 81, 25}\n>>> dict((x, x * x) for x in range(10))\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\nBut don’t read too much into such equivalences. Because implementations may\nvary arbitrarily, you should generally collect performance data before adopting\nan alternative. In this case, generators are actually slower than the equivalent list\ncomprehension today, as we’ll prove in the next chapter. Programs that run lots\nof them may need to care.\nScopes and comprehension variables\nNow that we’ve seen all comprehension forms, Chapter 17’s overview of the\nlocalization of loop variables in these expressions may make more sense. In all\nforms, the loop variable (or variables) after the for is local to the expression—it\nwon’t clash with names outside, but is also not available there, and this differs\nfrom for statements:\n>>> X = 99\n>>> [X for X in range(5)]         # All comprehensions localize loop variables\n[0, 1, 2, 3, 4]\n>>> X                             # Enclosing-scope X was not changed\n99\n>>> for X in range(5): pass       # But loop statements do not localize!\n>>> X\n4\nAs noted in Chapter 17, loop variables assigned in a comprehension really live in\na further nested special-case scope, but other names referenced within these\nexpressions follow the usual LEGB rules. In the following generator, for\nexample, Z is localized in the comprehension, but Y and X are found in the",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 781,
      "chapter": null,
      "content": "enclosing local and global scopes as usual:\n>>> X = 'aaa'\n>>> def func():\n        Y = 'bbb'\n        print('-'.join(Z for Z in X + Y))    # Z=comprehension, Y=local, X=global\n>>> func()\na-a-a-b-b-b\nOne exception here: names assigned by the := expression inside a\ncomprehension do leak out of comprehension, and generally behave as though\nthey were assigned in the scope containing the comprehension itself:\n>>> S = 'hack'\n>>> [(temp := S[i]) + temp.upper() for i in range(len(S))]\n['hH', 'aA', 'cC', 'kK']\n>>> temp\n'k'\n>>> i\nNameError: name 'i' is not defined. Did you mean: 'id'?\nWhile this allows comprehensions to both reuse and export values, keep in mind\nthat a lambda container’s local scope effectively plugs the leak—temp in the\nfollowing, for example, lives only in the lambda’s scope:\n>>> del temp\n>>> f = lambda: [(temp := S[i]) + temp.upper() for i in range(len(S))]\n>>> f()\n['hH', 'aA', 'cC', 'kK']\n>>> temp\nNameError: name 'temp' is not defined\nWe’ll code such a := nesting in a sequence-scrambler example ahead, though\nsome observers may deem it much less transparent than simple assignment\nstatements, and other real-world roles must await your discovery. See Chapter 11\nfor more background on := named assignment.\nGenerating “infinite” (well, indefinite) results\nFinally, it was noted earlier in passing that generators can even produce\n“infinite” results that tools like list comprehensions cannot. This may sound",
      "content_length": 1430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 782,
      "chapter": null,
      "content": "more impressive that it is; really, this just means that a generator can keep\nyielding results from its retained state’s local variables indefinitely—until its\nclient grows tired of them:\n>>> def squares(n):\n        while True:                      # Generate results \"forever\"\n            yield n ** 2                 # Or until no more next calls...\n            n += 1\n>>> G = squares(2)\n>>> next(G)\n4\n>>> next(G)\n9\n>>> for i in range(10): print(next(G), end=' ')\n16 25 36 49 64 81 100 121 144 169\nThis pattern may be useful in limited roles like test-parameter generation, and\nother tools can’t compete in this event; list comprehensions, for example, must\ncollect all results at once. In the end, though, this is a narrow role, generators are\nmortal, and so are we—so let’s move on to some code that’s a bit more tangible\nin the next section.\nExample: Shuffling Sequences\nTo demonstrate the power of iteration and generation tools in action, let’s turn to\nsome more complete examples. In Chapter 18, we wrote a testing function,\nbased on earlier code in Chapter 13, that scrambled the order of arguments used\nto verify generalized intersection and union functions. There, it was noted that\nthis might be better coded as a generator of values. Now that we’ve learned how\nto write generators, this serves to illustrate a practical application.\nOne note up front: because they slice and concatenate objects, all the examples\nin the section (including the permutations at the end) work only on sequences\nlike strings and lists, not on arbitrary iterables like files, maps, and other\ngenerators. That is, some of these examples will be generators themselves,\nproducing values on request, but they cannot process generators as their inputs.\nGeneralization for broader categories is left as an open issue, though the code",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 783,
      "chapter": null,
      "content": "here will suffice unchanged if you wrap nonsequence generators in list calls\nbefore passing them in.\nScrambling Sequences\nFirst, let’s review. As coded in Chapter 18, we can reorder a sequence with\nslicing and concatenation, moving the front item to the end on each loop; slicing\ninstead of indexing the item allows + to work for arbitrary sequence types:\n>>> L, S = [1, 2, 3], 'code'\n>>> for i in range(len(S)):            # Coding 1: for repeat counts 0..3\n        S = S[1:] + S[:1]              # Move front item to the end\n        print(S, end=' ')\nodec deco ecod code\n>>> for i in range(len(L)):\n        L = L[1:] + L[:1]              # Slice so any sequence type works\n        print(L, end=' ')\n[2, 3, 1] [3, 1, 2] [1, 2, 3]\nAlternatively, as we also saw in Chapter 13, we get the same results by moving\nan entire front section to the end, though the order of the results varies slightly:\n>>> for i in range(len(S)):            # Coding 2: for positions 0..3\n        X = S[i:] + S[:i]              # Rear part + front part (same effect)\n        print(X, end=' ')\ncode odec deco ecod\nTrace this code to see how it works; each version produces N results for an N-\nitem sequence passed in.\nSimple functions\nAs is, this code works on specific named variables only, and simply prints its\nresult. As we’ve seen, it’s easy to generalize this by turning it into a normal\nfunction that can both work on any object passed to its argument and return its\nresult for arbitrary use. The following does so for the second coding alternative;",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 784,
      "chapter": null,
      "content": "since its first cut exhibits the classic list comprehension pattern, we can also save\nsome work by coding it as such in the second:\n>>> def scramble(seq):\n        res = []\n        for i in range(len(seq)):\n            res.append(seq[i:] + seq[:i])\n        return res\n>>> scramble('code')\n['code', 'odec', 'deco', 'ecod']\n>>> def scramble(seq):\n        return [seq[i:] + seq[:i] for i in range(len(seq))]\n>>> scramble('code')\n['code', 'odec', 'deco', 'ecod']\n>>> for x in scramble((1, 2, 3)):\n        print(x, end=' ')\n(1, 2, 3) (2, 3, 1) (3, 1, 2)\nWe could use recursion here as well, but it’s probably overkill in this fixed and\nlinear context.\nGenerator functions\nThe preceding section’s simple approach works, but must build an entire result\nlist in memory all at once (not great on memory usage if it’s massive), and\nrequires the caller to wait until the entire list is complete (less than ideal if this\ntakes a substantial amount of time). You should know by now that we can do\nbetter on both fronts by translating this to a generator function that yields one\nresult at a time, using either coding scheme:\n>>> def scramble(seq):\n        for i in range(len(seq)):\n            seq = seq[1:] + seq[:1]               # Generator function, coding 1\n            yield seq                             # Assignments work here\n>>> def scramble(seq):\n        for i in range(len(seq)):                 # Generator function, coding 2\n            yield seq[i:] + seq[:i]               # Yield one item per iteration",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 785,
      "chapter": null,
      "content": "Both of these alternatives produce values when used by an iteration tool like\nlist or for, though the second version’s results order (shown here) again varies\nslightly in ways that probably won’t matter to future clients:\n>>> list(scramble('code'))                        # list() generates all results\n['code', 'odec', 'deco', 'ecod'] \n>>> list(scramble((1, 2, 3)))                     # Any sequence type works\n[(1, 2, 3), (2, 3, 1), (3, 1, 2)]\n>>> for x in scramble((1, 2, 3)):                 # for loops generate results\n        print(x, end=' ')\n(1, 2, 3) (2, 3, 1) (3, 1, 2)\nBoth generator functions retain their local scope state (seq and i) while active,\nminimize memory space requirements, and divide the work into shorter time\nslices. As full functions, they are also very general. Moreover, because iteration\ntools work the same whether stepping through a real list or a generator of values,\nthe function can select between the two codings freely, and even change\nstrategies in the future without impacting callers.\nGenerator expressions\nAs we’ve also seen, generator expressions—comprehensions in parentheses\ninstead of square brackets—also generate values on request and retain their local\nstate. As expressions, they’re not as flexible as full functions, but because they\nyield their values automatically, generator expressions can often be more concise\nin specific use cases like this:\n>>> S = 'code'\n>>> G = (S[i:] + S[:i] for i in range(len(S)))         # Generator expr, coding 2\n>>> list(G)\n['code', 'odec', 'deco', 'ecod']\nA generator expression can’t use the seq assignment statement of the first\ngenerator-function coding, but it can achieve the same effect by nesting the :=\nnamed-assignment expression covered in Chapter 11, because := both changes\nassigned names in the containing scope and returns the assigned value so it\nappears in the results stream:",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 786,
      "chapter": null,
      "content": ">>> S = 'code'\n>>> G = (S:=(S[1:] + S[:1]) for i in range(len(S)))    # Generator expr, coding 1\n>>> list(G)\n['odec', 'deco', 'ecod', 'code']\nEither way, we’re still operating on a specific variable here, S. To generalize a\ngenerator expression for an arbitrary subject, wrap it in a simple lambda function\nthat takes an argument and returns a generator that uses it (subtle bit: note that\nseq in the second of these is not local to the generator, but is local to the lambda\nthat encloses it):\n>>> F = lambda seq: (seq[i:] + seq[:i] for i in range(len(seq)))\n>>> list(F(S))\n['code', 'odec', 'deco', 'ecod']\n>>> F = lambda seq: (seq:=(seq[1:] + seq[:1]) for i in range(len(seq)))\n>>> list(F(S))\n['odec', 'deco', 'ecod', 'code'] \n>>> list(F([1, 2, 3]))\n[[2, 3, 1], [3, 1, 2], [1, 2, 3]] \n>>> F(S)\n<generator object <lambda>.<locals>.<genexpr> at 0x100b23300>\n>>> for x in F((1, 2, 3)): print(x, end=' ')\n(2, 3, 1) (3, 1, 2) (1, 2, 3)\nTester client\nFinally, we can use either the generator function or its expression equivalent in\nChapter 18’s tester to produce scrambled arguments. As packaged in the\nmodule of Example 20-1, the sequence scramblers become reusable tools.\nExample 20-1. scramble.py\n\"Generate shuffles of a sequence, by function or expression\"\ndef scramble1(seq):\n   for i in range(len(seq)):                \n       yield seq[i:] + seq[:i]        # Yield one shuffle per iteration\nscramble2 = lambda seq: (seq[i:] + seq[:i] for i in range(len(seq)))\nThough it requires a bit of page flipping to see how, moving the values",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 787,
      "chapter": null,
      "content": "generation out to an external tool like this also makes the testing function\nnoticeably simpler:\n>>> from scramble import scramble1           # Choose your scrambler\n>>> from inter2 import intersect, union      # Functions to be tested\n>>> def tester(func, items, trace=True):\n        for args in scramble1(items):        # Use either generator\n            if trace: print(args)\n            print(sorted(func(*args)))       # Test for this scramble\n>>> tester(intersect, ('aab', 'abcde', 'ababab'), False)\n['a', 'b']\n['a', 'b']\n['a', 'b']\nTo make this work for yourself, make sure all imported files are in the current\ndirectory: either copy inter2.py from Chapter 18’s folder to Chapter 20’s, or copy\nExample 20-1’s scramble.py to Chapter 18’s folder and work there (the examples\npackage has already done the former). Alternatively, you can modify the module\nimport search path’s PYTHONPATH setting to include any folder—as you’ll learn\nwhen we cover modules in full after the next chapter.\nPermutating Sequences\nGenerators have many other real-world applications—consider parsing\nattachments in an email message or computing points to be plotted in a GUI.\nMoreover, other types of sequence scrambles serve central roles in other\napplications, from searches to mathematics. As is, our sequence scrambler of the\nprior section is a simple reordering, but some programs warrant the more\nexhaustive set of all possible orderings we get from permutations—produced\nusing recursive functions in both list-builder and generator forms by the module\nfile in Example 20-2.\nExample 20-2. permute.py\n\"Permute sequences: as a list or generator of values\"\ndef permute1(seq):\n   if not seq:                               # Shuffle any sequence: list\n       return [seq]                          # Empty sequence\n   else:",
      "content_length": 1805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 788,
      "chapter": null,
      "content": "res = []\n       for i in range(len(seq)):\n           rest = seq[:i] + seq[i+1:]        # Delete current node\n           for x in permute1(rest):          # Permute the others\n               res.append(seq[i:i+1] + x)    # Add node at front\n       return res\ndef permute2(seq):\n   if not seq:                               # Shuffle any sequence: generator\n       yield seq                             # Empty sequence\n   else:\n       for i in range(len(seq)):\n           rest = seq[:i] + seq[i+1:]        # Delete current node\n           for x in permute2(rest):          # Permute the others\n               yield seq[i:i+1] + x          # Add node at front\nBoth of these functions produce the same results, though the second defers much\nof its work until it is asked for a result. This code is a bit advanced, especially\nthe second of these functions (and to some Python newcomers might even be\ncategorized as cruel and unusual punishment!). Still, as you’ll learn in a moment,\nthere are cases where the generator approach can be very useful—and even\nessential.\nStudy and test this code for more insight, and add prints to trace if it helps. If it’s\nstill a mystery, try to make sense of the first version first; remember that\ngenerator functions simply return objects with methods that handle next\noperations run by for loops at each level, and don’t produce any results until\niterated; and trace through some of the following examples to see how they’re\nhandled by this code.\nPermutations produce more orderings than the original scrambler—for N items,\nwe get N! (factorial, not surprise) results instead of just N (24 for 4: 4 * 3 * 2 *\n1). In fact, that’s why we need recursion here: the number of nested loops is\narbitrary, and depends on the length of the sequence permuted. Here’s a demo of\nboth shufflers at work:\n>>> from scramble import scramble1\n>>> from permute import permute1, permute2\n>>> list(scramble1('abc'))                         # Simple scrambles: N = 3\n['abc', 'bca', 'cab']\n>>> permute1('abc')                                # Permutations larger: N! = 6",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 789,
      "chapter": null,
      "content": "['abc', 'acb', 'bac', 'bca', 'cab', 'cba'] \n>>> list(permute2('abc'))                          # Generate all ordered combos\n['abc', 'acb', 'bac', 'bca', 'cab', 'cba'] \n>>> G = permute2('abc')                            # Iterate: iter() not needed\n>>> next(G)\n'abc'\n>>> next(G)\n'acb'\n>>> for x in permute2('abc'): print(x)             # Automatic iteration\n…prints six lines…\nThe list and generator versions’ results are the same, though the generator\nminimizes both space usage and delays for results. For larger items, the set of all\npermutations from both is much larger than the simpler scrambler’s:\n>>> permute1('hack') == list(permute2('hack'))\nTrue\n>>> len(list(permute2('hack'))), len(list(scramble1('hack')))\n(24, 4)\n>>> list(scramble1('hack'))\n['hack', 'ackh', 'ckha', 'khac'] \n>>> list(permute2('hack'))\n['hack', 'hakc', 'hcak', 'hcka', 'hkac', 'hkca', 'ahck', 'ahkc', 'achk', 'ackh',\n 'akhc', 'akch', 'chak', 'chka', 'cahk', 'cakh', 'ckha', 'ckah', 'khac', 'khca', \n 'kahc', 'kach', 'kcha', 'kcah']\nPer Chapter 19, there are nonrecursive alternatives here too, using explicit stacks\nor queues, and other sequence orderings are common (e.g., fixed-size subsets\nand combinations that filter out duplicates of differing order), but these require\ncoding extensions we’ll forgo here. Experiment further on your own for more\ninsights.\nWhy generators here: Space, time, and more\nGenerators are a somewhat advanced tool, and might be better treated as an\noptional topic, but for the fact that they permeate the Python language today. As\nwe’ve seen, fundamental built-in tools such as range, map, dictionary keys, and\neven files are now generators, so you must be familiar with the concept even if\nyou don’t write new generators of your own. Moreover, user-defined generators\nare increasingly common in Python code that you might come across today—in",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 790,
      "chapter": null,
      "content": "the Python standard library, for instance.\nThough your code is yours to code, the same guidelines given for list\ncomprehensions apply here as well: don’t complicate your code with user-\ndefined generators if they are not warranted. Especially for smaller programs and\ndata sets, such tools may not make sense. Simple lists of results may suffice,\nmay be easier to understand, will be garbage-collected automatically, and might\nbe produced quicker (and are today: see the next chapter). Advanced tools like\ngenerators that rely on implicit “magic” can be fun to experiment with, but may\nbe subpar when optional.\nThat being said, there are specific use cases that generators can address well.\nThey can reduce memory footprint in some programs, reduce delays in others,\nand can occasionally make the impossible possible. Consider, for example, a\nprogram that must produce all possible permutations of a nontrivial sequence.\nSince the number of combinations is a factorial that explodes exponentially, the\npreceding permute1 recursive list-builder function will either introduce a\nnoticeable and perhaps interminable pause, or fail completely due to memory\nrequirements.\nBy contrast, the permute2 recursive generator returns each individual result\nquickly, and can handle very large result sets. Even at just 10 items, for instance,\nthe difference starts becoming stark (per Python’s math module):\n>>> import math\n>>> f'{math.factorial(10):,}'        # 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1\n'3,628,800'\n>>> from permute import permute1, permute2\n>>> seq = list(range(10))\n>>> p1 = permute1(seq)               # ~17 seconds on a 2.3GHz i9 macOS host\n>>> len(p1)                          # Creates a list of 3.6M numbers\n3628800\n>>> p1[0], p1[-1]\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\nIn this case, the permute1 list builder pauses for 17 seconds to build a 3.6-\nmillion-item list, but the generator can begin returning individual results\nimmediately—neither permute2 nor any next pause in the following:",
      "content_length": 2026,
      "extraction_method": "Direct"
    },
    {
      "page_number": 791,
      "chapter": null,
      "content": ">>> p2 = permute2(seq)               # Returns generator immediately\n>>> next(p2)                         # Returns each result quickly on demand\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> next(p2)\n[0, 1, 2, 3, 4, 5, 6, 7, 9, 8]\nWhile collecting all results from the generator is still slow, it’s faster than the list\nbuilder, and not intended usage:\n>>> p2 = list(permute2(seq))         # About 10 seconds, though still impractical\n>>> p1 == p2                         # Same set of results generated\nTrue\nNaturally, we might be able to optimize the list builder’s code to run quicker\n(e.g., an explicit stack instead of recursion might change its performance), but\nfor larger sequences, it’s not an option at all—at just 50 items, the number of\npermutations wholly precludes building a results list, and would take far too long\nfor mere mortals like us in any event (and larger values will overflow the preset\nrecursion stack depth limit: see the preceding chapter). The generator, however,\nis still viable—despite the factorial size of the problem, it is able to produce\nindividual results immediately:\n>>> math.factorial(50)\n30414093201713378043612608166064768844377641568960512000000000000\n>>> p3 = permute2(list(range(50)))       # permute1 is not an option here!\n>>> next(p3)\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n44, 45, 46, 47, 48, 49]\nFor more fun—and to yield results that are more variable and less obviously\ndeterministic—we could use Python’s random module of Chapter 5 to randomly\nshuffle the sequence to be permuted before the permuter begins its work. In the\nfollowing, for example, each permute2 and next call returns immediately as\nbefore and permutes a random sequence, but permute1 hangs (and presumably\nperishes from memory starvation if allowed to run long enough):\n>>> seq = list(range(20))\n>>> print(f'{math.factorial(20):,}')     # permute1 is not an option here\n2,432,902,008,176,640,000",
      "content_length": 2035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 792,
      "chapter": null,
      "content": ">>> import random\n>>> random.shuffle(seq)                  # Shuffle sequence randomly first\n>>> p = permute2(seq)\n>>> next(p)\n[10, 19, 5, 6, 2, 13, 1, 8, 11, 7, 14, 16, 4, 3, 0, 18, 9, 12, 17, 15]\n>>> next(p)\n[10, 19, 5, 6, 2, 13, 1, 8, 11, 7, 14, 16, 4, 3, 0, 18, 9, 12, 15, 17]\n>>> random.shuffle(seq)\n>>> p = permute2(seq)\n>>> next(p)\n[17, 15, 14, 7, 10, 8, 2, 6, 18, 19, 13, 4, 1, 12, 5, 0, 3, 9, 16, 11]\n>>> next(p)\n[17, 15, 14, 7, 10, 8, 2, 6, 18, 19, 13, 4, 1, 12, 5, 0, 3, 9, 11, 16]\nIn fact, we might be able to use the random shuffler instead of manual shuffles in\nsome roles, as long as we either can assume that it won’t repeat shuffles during\nthe time we consume them, or test its results against prior shuffles to avoid\nrepeats—and hope that we do not live in the strange universe where a random\nsequence repeats the same result an infinite number of times! When full\ncoverage is important, manual shuffles offer better control.\nThe main point here is that generators can sometimes produce results from large\nsolution sets when list builders cannot. Then again, it’s not clear how common\nsuch use cases may be in the real world, and this doesn’t necessarily justify the\nimplicit flavor of value generation that we get with generator functions and\nexpressions. As you’ll see in Part VI, value generation can also be coded as\niterable objects with classes. Class-based iterables can produce items on request\ntoo, and are more explicit than the magic objects and methods produced for\ngenerator functions and expressions.\nPart of programming is finding a balance among trade-offs like these, and there\nare no absolute rules here. While the benefits of generators may sometimes\njustify their use, maintainability counts too. Like comprehensions, generators\nalso offer an expressiveness and code economy that’s hard to resist if you\nunderstand how they work—but you’ll want to weigh this against the frustration\nof coworkers who might not.\nExample: Emulating zip and map",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 793,
      "chapter": null,
      "content": "Let’s take a quick look at one more example of generators in action that\nillustrates just how expressive they can be. Once you know about\ncomprehensions, generators, and other iteration tools, you’ll find that emulating\nmany of Python’s functional built-ins is both straightforward and instructive, and\ncan be useful for custom roles.\nFor example, we’ve already seen how the built-in zip and map functions\ncombine iterables and project functions across them, respectively. With multiple\niterable arguments, map projects the function across items taken from each\niterable in much the same way that zip combines them, and both functions\ntruncate at the shortest iterable’s length:\n>>> list(zip('abc', 'xyz123'))                 # zip combines items from iterables\n[('a', 'x'), ('b', 'y'), ('c', 'z')]\n>>> list(zip([1, 2, 3], [2, 3, 4, 5]))         # N M-ary iterables: M N-ary tuples\n[(1, 2), (2, 3), (3, 4)]\n>>> list(zip([−2, −1, 0, 1, 2]))               # 1 5-ary iterable: 5 1-ary tuples\n[(−2,), (−1,), (0,), (1,), (2,)]\n>>> list(map(abs, [−2, −1, 0, 1, 2]))          # Single iterable: 1-ary function\n[2, 1, 0, 1, 2]\n>>> list(map(pow, [1, 2, 3], [2, 3, 4, 5]))    # N iterables: N-ary function\n[1, 8, 81]\nAs covered earlier, both also work on any type of iterable, including files that\nread their lines automatically. Though they’re ultimately used for different\npurposes, if you study these examples long enough, you might notice a\nrelationship between zip results and map function arguments that our next\nexample can exploit.\nCoding Your Own map\nAlthough the map and zip built-ins are fast and convenient, they’re easy to\nimplement in customizable code of our own. In the preceding chapter, for\nexample, we wrote a function that emulated the map built-in for a single iterable\nargument. Per Example 20-3, it doesn’t take much more work to allow for\nmultiple iterables, as the built-in does.\nExample 20-3. mymap-lists.py\n\"Emulate map: support multiple arguments, build a list result\"",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 794,
      "chapter": null,
      "content": "def mymap(func, *seqs):\n   res = []\n   for args in zip(*seqs):\n       res.append(func(*args))\n   return res\nprint(mymap(abs, [-2, -1, 0, 1, 2]))\nprint(mymap(pow, [1, 2, 3], [2, 3, 4, 5]))\nThis version relies upon * argument syntax—it collects multiple sequence\n(really, iterable) arguments; unpacks them as zip arguments to combine; and\nthen unpacks the combined zip results as arguments to the passed-in function.\nThat is, we’re using the fact that the zipping is essentially a nested operation in\nmapping. The test code at the bottom applies this to both one and two sequences\nto test—with the same results generated by the built-in map:\n$ python3 mymap-list.py\n[2, 1, 0, 1, 2]\n[1, 8, 81]\nReally, though, the preceding code exhibits the classic list comprehension\npattern, building a list of operation results within a for loop. We can rewrite our\nmapper more concisely as an equivalent one-line list comprehension:\ndef mymap(func, *seqs):\n    return [func(*args) for args in zip(*seqs)]\nWhen this is run the result is the same as before, but the code is more concise\nand might run faster (more on performance in Chapter 21). Both of the preceding\nmymap versions build result lists all at once, though, and this can waste memory\nfor larger lists. Now that we know about generator functions and expressions,\nit’s simple to recode both these alternatives to produce results on demand\ninstead, per Example 20-4.\nExample 20-4. mymap-generate.py\n\"Emulate map: support multiple arguments, generate results on request\"\ndef mymap_func(func, *seqs):\n   for args in zip(*seqs):\n       yield func(*args)",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 795,
      "chapter": null,
      "content": "def mymap_expr(func, *seqs):\n   return (func(*args) for args in zip(*seqs))\nfor mymap in (mymap_func, mymap_expr): \n   print(list(mymap(abs, [-2, -1, 0, 1, 2])))\n   print(list(mymap(pow, [1, 2, 3], [2, 3, 4, 5])))\nBoth these new versions produce the same results but return generators designed\nto support the iteration protocol—the first yields one result at a time explicitly,\nand the second returns a generator expression’s result to do the same implicitly.\nAs generators, we must wrap them in list calls to force them to produce their\nvalues all at once:\n$ python3 mymap-generate.py\n[2, 1, 0, 1, 2]\n[1, 8, 81]\n[2, 1, 0, 1, 2]\n[1, 8, 81]\nNo real work is done here until the list calls force the generators to run by\nactivating the iteration protocol. The list calls allow for display in testing, but\nthis is not generally desirable in other contexts. The generators returned by these\nfunctions themselves, as well as the generators returned by the zip built-in they\nuse, produce results only on demand, and that’s the whole point of generators—\ndelaying results saves memory space, and avoids pausing callers for full results.\nCoding Your Own zip and 2.X map\nOf course, much of the secret behind the success of the examples shown so far\nlies in their use of the zip built-in to combine arguments from multiple iterables.\nUsing iteration tools, we can also code workalikes that emulate both today’s\ntruncating zip, as well as the former padding behavior of Python 2.X’s map\nwhen passed a None for its function—a still potentially useful tool despite its\ndemise in 3.X, and the sort of thing that custom code can provide. Per\nExample 20-5, zip and this padding map are related in utility and nearly the\nsame in code.\nExample 20-5. myfptools-list.py\n# Emulate zip and padding map: build lists",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 796,
      "chapter": null,
      "content": "def myzip(*seqs):\n   seqs = [list(S) for S in seqs]\n   res  = []\n   while all(seqs):\n       res.append(tuple(S.pop(0) for S in seqs))\n   return res\ndef mymapPad(*seqs, pad=None):\n   seqs = [list(S) for S in seqs]\n   res  = []\n   while any(seqs):\n       res.append(tuple((S.pop(0) if S else pad) for S in seqs))\n   return res\nS1, S2 = 'abc', 'xyz123'\nprint(myzip(S1, S2))\nprint(mymapPad(S1, S2))\nprint(mymapPad(S1, S2, pad=99))\nBoth of these functions work on any type of iterable object because they run\ntheir arguments through the list built-in to force result generation (e.g., files\nwould work as arguments, in addition to sequences like strings). Notice the use\nof the all and any built-ins here—as we saw briefly in Chapter 14, these return\nTrue if all or any items in an iterable are True (or equivalently, nonempty),\nrespectively. These built-ins are used to stop looping when any or all of the\nlistified arguments become empty after deletions.\nAlso note the use of the keyword-only argument, pad; unlike the 2.X map, our\nversion will allow any pad object to be specified. When these functions are run,\nthe following results are printed—a zip and two padding maps:\n$ python3 myfptools-list.py\n[('a', 'x'), ('b', 'y'), ('c', 'z')]\n[('a', 'x'), ('b', 'y'), ('c', 'z'), (None, '1'), (None, '2'), (None, '3')]\n[('a', 'x'), ('b', 'y'), ('c', 'z'), (99, '1'), (99, '2'), (99, '3')]\nThese functions aren’t amenable to list comprehension translation because their\nloops are too specific. As before, though, while our zip and map workalikes\ncurrently build and return result lists, it’s just as easy to turn them into\ngenerators with yield so that they each return one piece of their result set at a\ntime. Example 20-6 shows how.",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 797,
      "chapter": null,
      "content": "Example 20-6. myfptools_generate.py\n# Emulate zip and padding map: generate results\ndef myzip(*seqs):\n   seqs = [list(S) for S in seqs]\n   while all(seqs):\n       yield tuple(S.pop(0) for S in seqs)\ndef mymapPad(*seqs, pad=None):\n   seqs = [list(S) for S in seqs]\n   while any(seqs):\n       yield tuple((S.pop(0) if S else pad) for S in seqs)\nThe results are the same as before, but this file assumes it will be used or tested\nelsewhere (it has no self-test code), and we need to use list again to force the\ngenerators to yield their values for display in the REPL:\n$ python3\n>>> from myfptools_generate import myzip, mymapPad\n>>> list(myzip('abc', 'xyz123'))\n[('a', 'x'), ('b', 'y'), ('c', 'z')]\n>>> list(mymapPad('abc', 'xyz123', pad=99))\n[('a', 'x'), ('b', 'y'), ('c', 'z'), (99, '1'), (99, '2'), (99, '3')]\nFinally, here’s an alternative implementation of our zip and map emulators—\nrather than deleting arguments from lists with the pop method, the following\nversions do their job by calculating the minimum and maximum argument\nlengths. Armed with these lengths, it’s easy to code nested list comprehensions\nto step through argument index ranges:\ndef myzip(*seqs):\n    minlen = min(len(S) for S in seqs)\n    return [tuple(S[i] for S in seqs) for i in range(minlen)]\ndef mymapPad(*seqs, pad=None):\n    maxlen = max(len(S) for S in seqs)\n    index  = range(maxlen)\n    return [tuple((S[i] if len(S) > i else pad) for S in seqs) for i in index]\nBecause these use len and indexing, they assume that arguments are sequences\nor similar, not arbitrary iterables, much like our earlier sequence scramblers and\npermuters. The outer comprehensions here step through argument index ranges,",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 798,
      "chapter": null,
      "content": "and the inner comprehensions (passed to tuple) step through the passed-in\nsequences to pull out arguments in parallel (though not at the same time: see the\nnext section!). When they’re run, the results are as before.\nMost strikingly, generators and iterators seem to run rampant in this example.\nThe arguments passed to min and max are generator expressions, which run to\ncompletion before the nested comprehensions begin iterating. Moreover, the\nnested list comprehensions employ two levels of delayed evaluation—the range\nbuilt-in is an iterable, as is the generator expression argument to tuple.\nIn fact, no results are produced here until the square brackets of the list\ncomprehensions request values to place in the result list—which forces all the\ncomprehensions and generators to run. To turn these functions themselves into\ngenerators instead of list builders, simply use parentheses instead of square\nbrackets again. Here’s the case for our zip:\ndef myzip(*seqs):\n    minlen = min(len(S) for S in seqs)\n    return (tuple(S[i] for S in seqs) for i in range(minlen))\nS1, S2 = 'abc', 'xyz123'\nprint(list(myzip(S1, S2)))         # Go!... [('a', 'x'), ('b', 'y'), ('c', 'z')]\nIn this case, it takes a list call to activate the generators and other iterables to\nproduce their results. Experiment with these on your own for more details.\nDeveloping further coding alternatives is left as a suggested exercise (but see the\nsidebar “Why You Will Care: Iteration Versus Python Morph” for an exploration\nof one such option). Here, we have time for just one more related tale from the\ngenerations saga.\nAsynchronous Functions: The Short Story\nNow that you’ve survived all the foregoing twists and turns of the functions\nstory in Python, there’s one last topic to go. Python 3.5 debuted an extension\ncalled asynchronous (usually shortened to async) functions, which are able to\nmanually pause their execution while waiting for a result, in order to allow other\nsuch functions to run.",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 799,
      "chapter": null,
      "content": "Such tools are available in other languages (e.g., JavaScript), which provided\nsome of the inspiration and blueprint for their appearance in Python. Indeed,\nmatch, :=, f-strings, type hinting, and a host of other Python tools owe their\npresence to this same programming-languages arms race, whose logical outcome\nwould make all languages the same.\nOrigins aside, the pathological use case for the async extension is input/output\n(IO) operations: a function can pause itself until an IO transfer completes so that\nother parts of the program can run during the wait. While other tools like\nmultiple threads can and still do address this need too, some may judge async\nfunctions to be a lighter-weight option.\nThat said, there are downsides to this topic. For one thing, it has been morphing\nalmost constantly since its first murmurs in Python 3.4, which makes it difficult\nto document in books with lifespans much longer than web pages and blogs. For\nanother, it’s something of an all-or-nothing proposition, because async code\nrequires other async code.\nMore fundamentally, async functions are part of the applications-level domain of\nparallel programming—a category that also includes multiprocessing and\nmultithreading. As such, they are a heavyweight subject of interest to only a\nsubset of Python’s user base, and a lot to ask of newcomers to Python or\nprogramming in general. Especially given Python’s already skyrocketing\ncomplexity, this may be best taken as an optional add-on topic, and out of scope\nhere.\nNevertheless, async functions are not truly optional in Python: they were added\ndeeply to the language as new syntax, via the async and await reserved words,\nand are now fair game in code you may have to reuse (and job-interview tests\nyou may have to suffer). For all these reasons, this section provides just enough\nof an overview to whet your appetite, but defers to Python’s online resources for\nmore details when you’re ready to tackle this advanced topic.\nAsync Basics\nIn a nutshell, async functions enable a cooperative, nonpreemptive multitasking\nmodel, with coding trade-offs similar to those imposed by asynchronous network\nservers. For instance, tasks must generally be short-lived to avoid monopolizing",
      "content_length": 2227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 800,
      "chapter": null,
      "content": "the host device’s processor. Though begun earlier, Python 3.5 made this part of\nthe language with the async def statement and await expression, as well as the\nasync for iterator and async with context manager.\nAsync tools and syntax can be used only in functions defined with async def.\nLike the yield and yield from that this model extend, async def causes a\nfunction to be compiled specially: when called, instead of running its body, such\na function returns an awaitable object that supports an expected method-based\nprotocol.\nTo invoke this object, functions await results explicitly, or use an async for or\nasync with that does so implicitly. Moreover, an event loop must be launched\nto run the show at large. Although the event loop runs just one task at a time\n(they’re not truly parallel), it can switch to other tasks when a task runs an await\nfor a pending result.\nRunning serial tasks with normal blocking calls\nAs usual, this may be easier to grok in code than narrative, so let’s turn to some\nexamples to demo the salient bits. All of this section’s example snippets live in\nthe file all_async_demos.py in the examples package, previewed in Example 20-\n7. To work along, ether cut and paste from this file or emedia, or run the\nexamples file in its entirety or with parts stubbed out with triple quotes. All\nexamples here assume that the preamble at the top of Example 20-7 has been\nrun.\nExample 20-7. all_async_demos.py (preamble)\nimport time, asyncio\ndef now(): \n   return time.strftime('[%H:%M:%S]')      # Local time, as hour:minute:second\n…all examples here…\nWe need a handful of standard-library tools here: some in the asyncio standard-\nlibrary module are required, and we’ll display times with time.strftime and\npause with sleep calls in both time and asyncio to simulate a long-running task.\nSee Python’s library manual for these tools’ fine print that we’ll skip here for\nspace and scope.",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 801,
      "chapter": null,
      "content": "To get started, consider the following non-async code, in which a main function\ncalls another, producer, that simply pauses for two seconds using the\ntime.sleep call before returning a result. The sleep stands in for a real blocking\noperation—like an IO call, network transfer, or user interaction. As usual with\nnormal functions, main simply waits for each producer call to run to completion\nand return a result, before it proceeds with a next step:\ndef producer(label):\n    time.sleep(2)                              # Pause for two seconds: blocking\n    return f'All done, {label}, {now()}'       # And return a result\ndef main():\n    print('Start =>', now())\n    print(producer(f'serial task 1'))          # Run three steps in sequence\n    print(producer(f'serial task 2'))          # Waiting for each one to finish\n    print(producer(f'serial task 3'))          # Before doing anything else\n    print('Stop  =>', now())\nmain()\nWhen run, this code produces the following mundane results. As you can see,\neach producer call runs a two-second sleep before another starts, so the total\ntime required is six seconds—two for each call run in series by main:\nStart => [19:00:28]\nAll done, serial task 1, [19:00:30]\nAll done, serial task 2, [19:00:32]\nAll done, serial task 3, [19:00:34]\nStop  => [19:00:34]\nRunning concurrent tasks with “await” and “async def”\nNow, let’s do the same work with async functions. In the next listing, the\nproducer is coded as an async def function, which makes Python compile it\nspecially: it’s now a coroutine—an object which, when called, doesn’t produce\nits result, but instead returns an automatically created object that supports the\ncoroutine method protocol. That protocol generally includes a method named\n__await__, which returns an iterator that produces coroutine results on\n__next__, but may instead use __anext__ for generators. We can safely ignore\nall of these here.",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 802,
      "chapter": null,
      "content": "The async def syntax is always required for functions that use await, as well\nas aync for and async with demoed ahead. Because async def is required to\nboth use and define awaitable coroutines, it tends to spread viruslike throughout\ncode (in fact, some tools now come with libraries in two separate forms,\nasynchronous and not).\nAlso in the following, producer uses await to suspend itself until the sleep\nexpires. await is similar to a yield from, but the event loop is free to pause the\ncoroutine running the await and resume another if the awaited result is not\nready. Notice that await is used as a statement in producer; it’s also an\nexpression that returns the awaited result, as in main, where it returns the result\nof the awaited objects:\nasync def producer(label):                     # await requires async\n    await asyncio.sleep(2)                     # Call nonblocking/awaitable sleep\n    return f'All done, {label}, {now()}'       # Result of await expression\nasync def main():\n    print('Start =>', now())\n    task1 = asyncio.create_task(producer(f'async task 1'))\n    task2 = asyncio.create_task(producer(f'async task 2'))\n    task3 = asyncio.create_task(producer(f'async task 3'))\n    print(await task1) \n    print(await task2) \n    print(await task3)                         # Wait for tasks to finish\n    print('Stop  =>', now())\nasyncio.run(main())                            # Start event-loop schedule\nCrucially, the awaited asyncio.sleep call is like time.sleep, but is both\nnonblocking—it uses the event-loop’s API to ensure that the caller doesn’t pause\nfor its completion—and awaitable—it uses async def to allow itself to be\npaused until a timeout elapses. Operations at the end of await chains must\ngenerally follow this model.\nAlso crucially, main uses asyncio.create_task to wrap each producer object\nin a task—an object run by the event loop that suspends execution of the\ncoroutine it wraps while the coroutine waits for completion of a future, and\nreturns the result of its wrapped coroutine when awaited itself. Future objects\nsignal progress in ways that are too low level for this overview; here, it’s enough",
      "content_length": 2146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 803,
      "chapter": null,
      "content": "to know that each task suspends itself while its coroutine is sleeping.\nAs a result, tasks can be overlapped in time, to run concurrently. In our code, all\nthree tasks—and the producer coroutines they manage—finish at the same time,\nand the total program takes just two seconds instead of six:\nStart => [19:00:34]\nAll done, async task 1, [19:00:36]\nAll done, async task 2, [19:00:36]\nAll done, async task 3, [19:00:36]\nStop  => [19:00:36]\nNaturally, we can use loops here to make the steps more automated. The\nfollowing runs the same as the prior version—taking two total seconds, and two\nfor each overlapped task:\nasync def producer(label):\n    await asyncio.sleep(2)\n    return f'All done, {label}, {now()}'\nasync def main():\n    print('Start =>', now())\n    tasks = []\n    for i in range(3):\n        tasks.append(asyncio.create_task(producer(f'async task {i+1}')))\n    for task in tasks:\n        print(await task)\n    print('Stop  at', now())\nasyncio.run(main())\nIn both producer and main, an await, much like a yield from, suspends\nexecution of the coroutine that runs it until the awaited object returns its result.\nTogether with the event loop and tasks, the net effect interleaves code.\nHow not to use async functions\nTo understand the preceding example, it may help to see other codings that do\nnot work. In this example, the awaits, tasks, and event loop are all essential: the\ncode won’t work without them. For one thing, calling main directly without\nrouting it to the event loop fails right out of the gate with a warning. The call to\nmain simply creates an awaitable object but does nothing with it (for brevity,",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 804,
      "chapter": null,
      "content": "snippets in this section show just differing code, followed by its results):\nasync def main():\n    print(producer('xxx'))\nmain()\n# Result…\n…/all_async_blunders.py:12: RuntimeWarning: coroutine 'main' was never awaited\nFor another, calling producer directly instead of awaiting it fails too, and for the\nsame reason. Much like generators, calling a coroutine returns the awaitable\nobject, but doesn’t run it; we must await the awaitable to make it go, just like\nwe have to run next to get results from a generator:\nasync def main():\n    print(producer('xxx'))\nasyncio.run(main())\n# Result…\n<coroutine object producer at 0x10142e740>\n…/all_async_blunders.py:22: RuntimeWarning: coroutine 'producer' was never awaited\nMore vitally, tasks are key to the concurrency here. If the example’s main simply\nruns an await for a producer directly, it will pause for the sleep too. In the\nfollowing miscoding, both coroutines await the sleep’s end, and the two\nproducers run serially—yielding a total of four seconds for the program at large\ninstead of two:\nasync def main():\n    print('Start =>', now())\n    print(await producer('xxx'))\n    print(await producer('yyy'))\n    print('Stop  =>', now())\nasyncio.run(main())\n# Result…\nStart => [18:41:56]\nAll done, xxx, [18:41:58]\nAll done, yyy, [18:42:00]\nStop  => [18:42:00]",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 805,
      "chapter": null,
      "content": "This is also true if we defer the awaits by assignments—main continues to run\nafter creating the producer coroutine awaitables, but still hangs for each later\nawait that runs the coroutine’s code, with no concurrency to be found:\nasync def main():\n    print('Start =>', now())\n    p1 = producer('xxx')\n    p2 = producer('yyy')\n    print('Await =>', now())\n    print(await p1)\n    print(await p2)\n    print('Stop  =>', now())\nasyncio.run(main())                            \n# Result…\nStart => [18:42:00]\nAwait => [18:42:00]\nAll done, xxx, [18:42:02]\nAll done, yyy, [18:42:04]\nStop  => [18:42:04]\nThat’s why tasks are needed in the prior example: they enable coroutines to be\npaused and run by the event loop to overlap their execution. That is, they enable\nconcurrency—along with alternatives like those of the next section.\nRunning concurrent tasks with “as_completed” and “gather”\nBut enough common mistakes: let’s get back to coding concurrent coroutines the\nright way. Besides creating tasks, we can use the gather method to wait for all\nof our coroutines automatically, or as_completed to wait for results as they\nfinish. The latter can be used in a normal for to watch for completions:\nasync def producer(label):\n    await asyncio.sleep(2)\n    return f'All done, {label}, {now()}'\nasync def main():\n    print('Start =>', now())\n    coros = [producer(f'async task {i+1}') for i in range(3)]\n    for nextdone in asyncio.as_completed(coros):\n        print(await nextdone)\n    print('Stop  at', now())",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 806,
      "chapter": null,
      "content": "asyncio.run(main())\nThis also finishes in two total seconds, though coroutine results may filter in in\nany order (as_completed can also be used with async for of the next section\nin ways covered by Python’s manuals):\nStart => [19:00:38]\nAll done, async task 2, [19:00:40]\nAll done, async task 3, [19:00:40]\nAll done, async task 1, [19:00:40]\nStop  at [19:00:40]\nAlternatively, the gather call schedules tasks for coroutines, awaits all its\narguments, and returns a list of task return values in the same order as its\narguments when all have finished:\nasync def producer(label):\n    await asyncio.sleep(2)\n    return f'All done, {label}, {now()}'\nasync def main():\n    print('Start =>', now())\n    coro1 = producer(f'async task 1')\n    coro2 = producer(f'async task 2')\n    coro3 = producer(f'async task 3')\n    results = await asyncio.gather(coro1, coro2, coro3)\n    print(results) \n    print('Stop  at', now())\nasyncio.run(main())\nResults vary because they are in a list (formatted for display here), but it’s still\njust two seconds for the entire gig:\nStart => [19:00:40]\n['All done, async task 1, [19:00:42]', \n 'All done, async task 2, [19:00:42]', \n 'All done, async task 3, [19:00:42]']\nStop  at [19:00:42]\nFor any line counters in the audience, a starred list comprehension can be used to\ncreate awaitables for gather more succinctly, and produces the same two-second",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 807,
      "chapter": null,
      "content": "result as the preceding version:\nasync def producer(label):\n    await asyncio.sleep(2)\n    return f'All done, {label}, {now()}'\nasync def main():\n    print('Start =>', now())\n    print(await asyncio.gather(*[producer(f'async task {i+1}') for i in range(3)]))\n    print('Stop  at', now())\nasyncio.run(main())\nWhile this code works, Python’s manuals today include a note that recommends\nTaskGroup over gather on the grounds of better exception handling. Should\nyou care to follow that advice, you’ll need to move on to the next section.\nRunning concurrent tasks with “async with” and “async for”\nAs yet another concurrency option, the combination of the async with\nstatement and a manager object can automatically handle both protocols and\nexceptions, and wait for tasks to finish. As half of this pair, async with\ninternally awaits the awaitables returned by its subject’s __aenter__ and\n__aexit__ methods, the same way that the normal with statement calls the\ncontext-manager protocol’s __enter__ and __exit__.\nHaving said that, we unfortunately won’t study either the normal with or its\nmethod protocols until Part VII, because they require knowledge of classes and\nOOP (yet again, Python’s toolset assumes you must already know Python to use\nPython!), so you’ll have to take this on faith for now. In the following code,\nthough, it’s enough to know that using a TaskGroup asynchronous context\nmanager in concert with async with will automatically await results from all of\nits tasks before the statement exits, as well as handle some thorny issues\ninvolving exceptions:\nasync def producer(label):\n    await asyncio.sleep(2)\n    return f'All done, {label}, {now()}'\nasync def main():\n    print('Start =>', now())",
      "content_length": 1713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 808,
      "chapter": null,
      "content": "async with asyncio.TaskGroup() as tg:\n        tasks = [tg.create_task(producer(f'async task {i+1}')) for i in range(3)]\n    for task in tasks:\n        print(task.result())\n    print('Stop  at', now())\nasyncio.run(main())\nThe net effect runs all three producer calls concurrently in two seconds as\nbefore:\nStart => [19:00:44]\nAll done, async task 1, [19:00:46]\nAll done, async task 2, [19:00:46]\nAll done, async task 3, [19:00:46]\nStop  at [19:00:46]\nFinally, when a function is coded with async def and also uses yield, it’s\nconsidered an asynchronous generator. When called, it returns an asynchronous\niterable object which uses the __anext__ method noted earlier, and can be used\nin an async for statement to execute the body of the function. (And no, there\nare no async variants of other statements like while or if; with and for were\nthe only targets deemed worthy of the async twist—so far.)\nInternally, async for uses methods __aiter__ and __anext__ much like the\nnormal iteration protocol’s __iter__ and __next__ that we studied in\nChapter 14. The __anext__ method returns an awaitable that produces a next\nvalue, and raises StopAsyncIteration to signal the end of values, instead of\nStopIteration. async for simply gets a next value by awaiting a call to\n__anext__, and running the loop’s block of code for every value obtained this\nway. This encroaches on classes and OOP again, but statement users may ignore\nthe details.\nTo avoid confusion, keep in mind that async for awaits the next item in the\niterable it scans—which is not the same as awaiting for something else in the\nloop body of a normal for, as in prior examples here. Moreover, async for\ndoes not automatically parallelize anything—it’s really just like a normal for,\nbut adds an await for each next item obtained from its iterable, instead of\nissuing a potentially blocking call. async for’s implicit awaits between loops",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 809,
      "chapter": null,
      "content": "allow it to be interleaved with other ready-to-run tasks, but don’t help much\notherwise:\nasync def producer(label):\n    for i in range(3):\n        await asyncio.sleep(2)\n        yield f'All done, {label} {i+1}, {now()}'\nasync def main():\n    print('Start =>', now())\n    async for reply in producer('async task'):\n        print(reply)\n    print('Stop  at', now())\nasyncio.run(main())\nAs coded here, the result is the same six-second showing of the original serial\nversion, because there’s nothing else to run in this simple demo during the\nsleeps:\nStart => [19:00:46]\nAll done, async task 1, [19:00:48]\nAll done, async task 2, [19:00:50]\nAll done, async task 3, [19:00:52]\nStop  at [19:00:52]\nAnd lest this section hasn’t yet managed to send you screaming into the night,\nit’s also possible to use await and async for within a comprehension that’s\ncoded in an async def—but this must remain a story for another day:\nasync def hmm():\n    result = [i async for i in asynciter() if i > 0]      # This code is not real\n    result = [await func() for func in coroutines]        # It's also unreal…\nThe Async Wrap-Up\nThat’s all the time and space we have for this topic (humanely, perhaps).\nPython’s async functions may be more useful than type hinting (which, as we\nsaw in Chapter 6, is both completely unused and at odds with Python’s core\nideas), but a full survey of either topic could fill chapters or books, and would be\nof interest to only a subset of Python users.",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 810,
      "chapter": null,
      "content": "Moreover, this text isn’t covering Python’s multithreading or multiprocessing\ntools at all, even though they are at least as valid as async functions for\nparallelizing tasks. The fact that async alone was afforded dedicated syntax in\nthe language both subjectively favors just one option in this domain and raises\nthe bar for Python newcomers.\nThat said, your goals and views may differ, so please see Python’s standard\nmanuals for the full story if and when you wish to wade deeper into the async\nsea. Here, we must step back from the ledge of optional and stunningly\nconvoluted extensions, and get back to the fundamentals that nearly all Python\nprograms and programmers use.",
      "content_length": 677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 811,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter wrapped up our coverage of built-in comprehension and iteration\ntools. It explored list comprehensions in the context of functional tools, and\npresented generator functions and expressions as additional iteration protocol\ntools. As a finale, we also explored some larger examples of iterations,\ncomprehensions, and generations in action, and briefly glimpsed the\nasynchronous-functions extension to pique further study. Though we’ve now\nseen all the built-in iteration tools, the subject will resurface when we explore\nuser-defined iterable classes in Chapter 30.\nThe next chapter is something of a continuation of the theme of this one—it\nrounds out this part of the book with a case study that times the performance of\nthe tools we’ve studied here, and serves as a more realistic example at the\nmidpoint of this book (yes, you’re half done!). Before we move ahead to\nbenchmarking comprehensions and generators, though, this chapter’s quizzes\ngive you a chance to review what you’ve learned about them here.\nTest Your Knowledge: Quiz\n1. What is the difference between enclosing a list comprehension in square\nbrackets and parentheses?\n2. How are generators and iterators related?\n3. How can you tell if a function is a generator function?\n4. What does a yield statement do?\n5. How are map calls and list comprehensions related? Compare and\ncontrast the two.\n6. What do async def and await mean in a Python script?\nTest Your Knowledge: Answers",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 812,
      "chapter": null,
      "content": "1. List comprehensions in square brackets produce the result list all at\nonce in memory. When they are enclosed in parentheses instead, they\nare actually generator expressions—they have the same internal syntax\nand a similar meaning but do not produce the result list all at once.\nInstead, generator expressions return a generator object, which yields\none item in the result at a time when used in an iteration tool or iterated\nmanually with next.\n2. Generators are iterable objects that support the iteration protocol\nautomatically—they have an iterator with a __next__ method (run by\nnext) that repeatedly advances to the next item in a series of results and\nraises an exception at the end of the series. In Python, we can code\ngenerator functions with def and yield, generator expressions with\nparenthesized comprehensions, and generator objects with classes that\ndefine a special method named __iter__ (discussed later in the book).\n3. A generator function has a yield statement (with or without its from\nextension) somewhere in its code. Generator functions are otherwise\nidentical to normal functions syntactically, but they are compiled\nspecially to return an iterable generator object when called. That object\nretains state and code location between values. (This means that\ndeleting yield makes a function normal, but code deletions can cause\nall sorts of issues.)\n4. When present, this statement makes Python compile the function\nspecially as a generator; when called, the function returns a generator\nobject that supports the iteration protocol. When the yield statement is\nrun, it sends a result back to the caller and suspends the function’s state;\nthe function can then be resumed after the last yield statement, in\nresponse to a next built-in or __next__ method call issued by the\ncaller. In more advanced roles, the generator send method similarly\nresumes the generator, but can also pass a value that shows up as the\nyield expression’s value. Generator functions may also have a return\nstatement, which terminates the generator (and attaches an optional\nvalue to the automatic StopIteration exception).",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 813,
      "chapter": null,
      "content": "5. The map call is similar to a list comprehension—both produce a series\nof values, by collecting the results of applying an operation to each item\nin a sequence or other iterable, one item at a time. The primary\ndifference is that map applies a function call to each item, and list\ncomprehensions apply arbitrary expressions. Because of this, list\ncomprehensions are more general; they can apply a function call\nexpression like map, but map requires a function to apply other kinds of\nexpressions. List comprehensions also support extended syntax—nested\nfor loops, and if clauses that subsume the filter built-in. map also\ndiffers by producing a generator of values; the list comprehension\nmaterializes the result list in memory all at once, and this may matter\nfor large lists.\n6. The async def is used to define an asynchronous function (usually\ncalled a coroutine), and await waits for another asynchronous object to\nproduce its value, possibly yielding control back to an event loop to\nallow other code to run during the wait. async def is required to use\nawait, async for, and async with, and both an event loop and\nscheduling calls in module asyncio are generally required to achieve\nconcurrency in this model. There’s more to the async story than told\nhere; see Python’s manuals for next steps on this topic, as well as its\nmultithreading and multiprocessing alternatives in Python’s standard\nlibrary.\nWHY YOU WILL CARE: ITERATION VERSUS PYTHON\nMORPH\nIn Chapter 14, we noted that some built-ins (like map) support only a single\ntraversal and are empty after it occurs, and this book promised to show you\nan example of why that can be important in practice. Now that we’ve studied\niteration topics in full, it can make good on this promise—and demo the\nnegative impacts of Python changes on your code at the same time.\nIn earlier editions of this book, the following clever coding for zip\nemulation, adapted from a version in Python’s docs at the time, worked\nbecause map returned a physical list instead of a generator of values:",
      "content_length": 2037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 814,
      "chapter": null,
      "content": "def myzip(*args):                           # Before 3.0...\n    iters = map(iter, args)                 \n    while iters:                            # Guarantee >=1, force looping\n        res = [next(i) for i in iters]      # Any empty? StopIteration=return\n        yield tuple(res)                    # Else return res, suspend state      \n                                            # Exit=return=StopIteration\n>>> list(myzip('ab', 'lmn'))\nKeyboardInterrupt\nExcept this broke in Python 3.0. This code relied on that fact that map’s result\nsupported multiple scans. When it mutated into a single-scan generator in\n3.0, as soon as the list comprehension traversed iters once, the generator\nwas exhausted but still True; later list comprehensions returned []; and this\ncode would loop until killed by a Ctrl+C. To fix, the prior edition added a\nlist around map to make it reiterable:\ndef myzip(*args):                           # Before 3.7...\n    iters = list(map(iter, args))           # <== Make iters reiterable, 3.0+\n    while iters:                            # map() is a one-scan generator\n        res = [next(i) for i in iters]\n        yield tuple(res)                                      \n>>> list(myzip('ab', 'lmn'))\nRuntimeError: generator raised StopIteration\nExcept this broke in Python 3.7. This code relied on the fact that an uncaught\nStopIteration from any iterable in iters was propagated through this\nfunction and sufficed to terminate its generation of values. Python 3.7\nchanged to suppress this “bubbling” propagation of StopIteration through\na generator and issue a RunTimeError in response. The workaround is to\nexplicitly catch the StopIteration and return—even though return\nsimply raises StopIteration:\ndef myzip(*args):                           # After 3.7, and before ?...\n    iters = list(map(iter, args))\n    while iters:\n        try:                                # <== Catch StopIteration, 3.7+\n            res = [next(i) for i in iters]  # StopIteration won't propagate\n        except StopIteration: \n            return                          # How generators should exit\n        yield tuple(res)                    # But exit=return=StopIteration!",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 815,
      "chapter": null,
      "content": ">>> list(myzip('ab', 'lmn'))\n[('a', 'l'), ('b', 'm')]\nYou’ll have to wait until Part VII for more on the try statement, and may\nhave to similarly translate exceptions to returns this way in rare generators\nthat rely on the former bubbling behavior. And while this final version works\ntoday, we’re not placing any bets on the future.\nThe takeaways here: wrapping map calls in list calls is not just for display;\ngenerators no longer propagate exit exceptions from code they run; and\nPython mods can and do break code—both subtly and repeatedly!",
      "content_length": 543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 816,
      "chapter": null,
      "content": "Chapter 21. The Benchmarking\nInterlude\nNow that we’ve fully explored function coding and iteration tools, we’re going\nto take a short side trip to put both of them to work. This chapter closes out the\nfunction part of this book with a larger case study that times the relative\nperformance of the iteration tools we’ve met so far, in both standard Python and\none of its alternatives.\nAlong the way, this case study surveys Python’s code-timing tools, discusses\nbenchmarking techniques in general, and develops code that’s more realistic and\nuseful than most of what we’ve seen up to this point. We’ll also measure the\nspeed of code we’ve used—data points that may or may not be significant,\ndepending on your programs’ goals.\nFinally, because this is the last chapter in this part of the book, we’ll close with\nthe usual sets of “gotchas” and exercises to help you start coding the ideas\nyou’ve read about. First, though, let’s have some fun with tangible Python code.\nBenchmarking with Homegrown Tools\nWe’ve met quite a few iteration alternatives in this book. Like much in\nprogramming, they represent trade-offs—in terms of both subjective factors like\nexpressiveness, and more objective criteria such as performance. Part of your job\nas a programmer and engineer is selecting tools based on factors like these.\nIn terms of performance, this book has mentioned a few times that list\ncomprehensions and map calls sometimes have a speed advantage over for loop\nstatements. It has also noted that sorting speed varies with ordering, and the\ngenerator functions and expressions of the preceding chapter tend to be slower\nthan all the others, though they minimize memory space requirements and don’t\ndelay the caller for result generation when there are many results to generate.\nAll that is generally true today in common usage. That being said, benchmarking",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 817,
      "chapter": null,
      "content": "comes with some big caveats: both code structure and host architecture can\ninfluence speed arbitrarily; Python’s performance can vary over time because its\ninternals are constantly being changed and optimized; and the speed of\nalternative Pythons may differ widely. As noted in Chapter 2, for example, the\nstandard CPython may adopt a standard JIT in the future which could change its\nspeed in some contexts, and optimized Pythons like PyPy have very difference\nperformance profiles.\nIn short, if you want to verify speed for yourself, you need to time your code, on\nyour own device, with the Python or Pythons you plan to use, and in the here and\nnow. That’s a lot of qualifiers, but benchmarking is an empirical task.\nTimer Module: Take 1\nLuckily, Python makes it easy to time code with custom tools—though perhaps\ndeceptively so. For example, to get the total time taken to run multiple calls to a\nfunction with arbitrary positional arguments, Example 21-1’s function coded in a\nmodule file might suffice as a first cut.\nExample 21-1. timer0.py\n\"Simplistic timing function\"\nimport time\ndef timer(func, *args):                    # Any positional arguments (only)\n   start = time.perf_counter()\n   for i in range(100_000):               # Hardcoded reps, range() timed\n       func(*args)\n   return time.perf_counter() - start     # Total elapsed time in seconds\nThis function fetches time values from Python’s standard-library time module,\nand subtracts the system start time from the stop time after running 100,000 calls\nto the passed-in function with the passed-in arguments.\nTime comes from time.perf_counter, which is defined to be a portable clock\nwith the best resolution to measure a short duration. The difference between two\ncalls is generally used for performance measurement, but includes time for\nsleeps and is system-wide time. The alternative time.process_time is per-\nprocess CPU time, and may also be useful in some roles. See Python’s manuals\nfor more details; these calls replace the former and less portable time.clock,\nwhich was deprecated and dropped since this book’s prior edition (breaking most",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 818,
      "chapter": null,
      "content": "examples here!).\nApart from its time calls, the code in Example 21-1 is straightforward and uses\ntools we’ve already met. When used on this book’s main development computer\nusing macOS and CPython 3.12, its results are these in a REPL:\n>>> from timer0 import timer\n>>> timer(pow, 2, 1000)               # Time to call pow(2, 1000) 100k times\n0.08591239107772708\n>>> timer(str.upper, 'hack' * 100)    # Time to call 'hack...'.upper() 100k times\n0.04990812297910452\nThough functional and simple, the preceding timer is also fairly limited, and\ndeliberately exhibits some classic mistakes in both function design and\nbenchmarking. Among these, it:\nHardcodes the repetitions count at 100k\nCharges the cost of range to the tested function’s time\nDoesn’t support keyword arguments in the tested function\nDoesn’t give callers a way to verify that the tested function actually\nworked\nOnly gives total time, which might fluctuate on some busy host\nmachines\nIn other words, timing code is more complex than you might expect!\nTimer Module: Take 2\nTo be more general and accurate, let’s rewrite the preceding section’s code to\ndefine still simple but more useful timer utility functions we can both use to see\nhow iteration alternative options stack up now, and apply to other timing needs\nin the future. These functions, in Example 21-2, are coded in a module file again\nso they can be used in a variety of programs, and have docstrings giving some\nbasic usage details that help and PyDoc can display; see Chapter 15 for tips on\nviewing the in-code documentation of this chapter’s timing modules.\nExample 21-2. timer.py",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 819,
      "chapter": null,
      "content": "\"\"\"\nHomegrown timing tools for arbitrary function calls.\nTimes one call, total of N, best of N, and best of totals of N.\nPass any number of positional and keyword arguments for each func. \n\"\"\"\nimport time\ntimer = time.perf_counter                         # See also time.process_time()\ndef once(func, *pargs, **kargs):                  # Collect arguments for func\n   \"\"\"\n   Time to run func(...) one time.\n   Returns (time, result).\n   \"\"\"\n   start   = timer()\n   result  = func(*pargs, **kargs)               # Unpack arguments for func\n   elapsed = timer() - start\n   return (elapsed, result)                      # Return result to verify\ndef total(reps, func, *pargs, **kargs):           # Collect arguments for func\n   \"\"\"\n   Total time to run func(...) reps times.\n   Returns (total-time, last-result).\n   \"\"\"\n   total = 0                                     # Don't charge range() time\n   for i in range(reps):\n       time, result = once(func, *pargs, **kargs)\n       total += time\n   return (total, result)                        # Return last result to verify\ndef bestof(reps, func, *pargs, **kargs):\n   \"\"\"\n   Best time among reps runs of func(...).\n   Returns (best-time, best-time-result).\n   \"\"\"\n   return min(once(func, *pargs, **kargs) for i in range(reps))\ndef bestoftotal(reps1, reps2, func, *pargs, **kargs):\n   \"\"\"\n   Best total time among reps1 runs of [reps2 runs of func(...)].\n   Returns (best-total-time, best-total-time-last-result).\n   \"\"\"\n   return min(total(reps2, func, *pargs, **kargs) for i in range(reps1))\nOperationally, this module implements both total and best times, and a best of\ntotals that combines the other two. In each mode, it times calls to any subject\nfunction that takes any positional and keyword arguments, by fetching the start\ntime, calling the function with arbitrary arguments, and subtracting the start time",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 820,
      "chapter": null,
      "content": "from the stop time. Here are the salient points to notice about how this version\naddresses the shortcomings of its predecessor:\nThe repetitions count is no longer hardcoded, but passed in as a required\nargument (or arguments) before the test function and its own arguments,\nto allow repetitions to vary per call.\nThe range call’s construction and iteration costs are no longer charged\nto timed functions, because timing has been factored out to the separate\nonce function that times just the subject function.\nAny number of both positional and keyword arguments for the timed\nfunction are now collected and unpacked with starred-argument syntax.\nThey must be sent individually, not in a sequence or dictionary, though\ncallers can unpack argument collections into individual arguments with\nstars in the top-level call.\nAll functions in this module return one of the timed function’s return\nvalues so callers can verify that the function worked. The return value is\nprovided in a two-item result tuple, along with the requested time result.\nNew best-of modes return minimum times to address fluctuations on the\nhost device, per the next paragraph.\nThis version’s functions also support multiple use cases: once times a single call\nfor simple cases; total runs many calls, to allow time to accumulate for short-\nlived functions; bestof returns the minimum time among all single calls, to\nfilter out the impacts of other activity on the host; and bestoftotal selects the\nminimum of nested total-time tests, to both apply a best-of filter and run many\ncalls for functions too fast to produce meaningful times.\nImportantly, the min calls in the best-of variants work to select the best and\nlowest time, because Python compares collections recursively (from left to right)\nas we’ve learned, and result tuples begin with time: because it’s first in these\n(time, result) tuples, time dominates and determines the min calls’ results:\n>>> min(tup for tup in [(2.0, 3), (3.0, 3), (1.0, 3), (0.0, 3), (4.0, 3)])\n(0.0, 3)",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 821,
      "chapter": null,
      "content": "From a larger perspective, because these functions are coded in a module file,\nthey become generally useful tools anywhere we wish to import them. Modules\nand imports were introduced in Chapter 3, and you’ll learn more about them in\nthe next part of this book; for now, simply import the module and call its\nfunction to use one of this file’s timers. Its results on the same host and in a\nREPL are similar to its timer0.py predecessor, but are more robust:\n>>> import timer                                  # Import file in this directory\n>>> help(timer)                                   # Display module's docs nicely\n>>> reps, text = 100_000, 'hack' * 100\n>>> timer.once(pow, 2, 1000)[0]                   # Not useful for fast calls\n6.182119250297546e-06\n>>> timer.once(str.upper, text)                   # (time, result)\n(3.363005816936493e-06, 'HACKHACKHACK…etc…')\n>>> timer.total(reps, pow, 2, 1000)[0]            # Compare to timer0 results\n0.08979405369609594\n>>> timer.total(reps, str.upper, text)[0]         # (time, last call's result)\n0.050884191412478685 \n>>> timer.bestof(50, pow, 2, 1000)[0]             # Not useful for fast calls\n1.6265548765659332e-06\n>>> timer.bestof(50, str.upper, text)[0]          # (best time, best time result)\n8.619390428066254e-07 \n>>> timer.bestoftotal(50, reps, pow, 2, 1000)[0]\n0.07521858718246222\n>>> timer.bestoftotal(50, reps, str.upper, text)[0]\n0.03947464330121875\nThe last two calls here calculate the best-of-totals times—the lowest time among\n50 runs, each of which computes the total time to call a function 100k times—\nroughly corresponding to the total times earlier in this listing, but repeated for\na minimum that filters out host fluctuations (and sans an extra charge for range).\nThe function used in these last two calls is really just a convenience that wraps\ntotal for better accuracy, but this is a common timing mode.\nNote that bestoftotal might replace its min of total with code like the\nfollowing to nest total in bestof:\n>>> timer.bestof(50, timer.total, reps, str.upper, text)\n(0.07037258706986904, (0.039281503297388554, 'HACKHACKHACK…etc…'))",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 822,
      "chapter": null,
      "content": "But this isn’t quite the same. For one thing, the result is nested tuples, reflecting\nthe nested calls. For another, this really times the entire total function, not just\nthe subject function it runs. The net effect charges an extra admin-code overhead\nthat’s enough to skew the best-of time up, and make it larger than the best total\ntime shown in the nested tuple. By using min of total instead, bestoftotal\navoids timing this overhead skew. Subtle but true!\nTiming Runner and Script\nNow, to time iteration tool speed (our original goal), we’ll write a script that\ndefines and submits test functions to the timer module. To make it easy to code\na variety of tests, let’s first define a utility module that does the heavy lifting as a\nreusable intermediary. Example 21-3 defines a function named runner that takes\nany number of test functions as arguments and passes them off for timing to the\nbestoftotal function imported from Example 21-2.\nExample 21-3. timer_runner.py\n\"Run passed-in test functions with the timer.py module\"\nimport timer, sys\ndef runner(*tests):\n   results = []\n   print('Python', sys.version.split()[0], 'on', sys.platform)\n   # Time\n   for test in tests:\n       besttime, result = timer.bestoftotal(10, 1000, test)\n       results.append(result)\n       print(f'{test.__name__:<9}: '\n             f'{besttime:.5f} => [{result[0]}…{result[-1]}]')\n   # Verify\n   print('Results differ!'\n          if any(result != results[0] for result in results[1:])\n          else 'All results same.')\nFine points here: this module’s runner function displays context with sys tools\ndocumented in Python’s manuals, and steps through all the passed-in functions,\nprinting the __name__ of each (as we’ve seen, this is a built-in attribute that\ngives a function’s name). The test-runner code also saves results to verify that",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 823,
      "chapter": null,
      "content": "they are all the same in a ternary expression at the very end of the process, to be\nsure we’re comparing apples to apples.\nLast but not least, the script in Example 21-4 defines the actual tests to be timed\nand passes them to the imported runner of Example 21-3, which hands them off\nto the imported timer of Example 21-2. We’ll run the file in Example 21-4 as a\ntop-level script to time the relative speeds of the various iteration codings we’ve\nstudied in this book so far.\nExample 21-4. timer_tests.py\n\"Test the relative speed of iteration coding alternatives.\"\nfrom timer_runner import runner\nrepslist = list(range(10_000))\ndef forLoop():\n   res = []\n   for x in repslist:\n       res.append(abs(x))\n   return res\ndef listComp():\n   return [abs(x) for x in repslist]\ndef mapCall():\n   return list(map(abs, repslist))              # Use list() to force results\ndef genExpr():\n   return list(abs(x) for x in repslist)        # Use list() to force results\ndef genFunc():\n   def gen():\n       for x in repslist:\n           yield abs(x)\n   return list(gen())                           # Use list() to force results \nrunner(forLoop, listComp, mapCall, genExpr, genFunc)\nThis script tests five alternative ways to build lists of results. In combination\nwith the test runner, its reported times reflect some 100 million steps for each of\nthe 5 test functions—each builds a list of 10,000 items 1,000 times, and this\nprocess is repeated 10 times to get the best-of times. Applying this for each of\nthe 5 test functions yields a whopping 500 million total steps for the script at\nlarge (impressive but reasonable on most machines these days).",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 824,
      "chapter": null,
      "content": "Notice how we have to run the results of the generator expression and function\nthrough the built-in list call to force them to yield all of their values; if we did\nnot, we would just produce generators that never do any real work. We must do\nthe same for the map result, since it is an iterable, on-demand object as well.\nFor similar reasons, the inner loops’ range result is hoisted out to the top of the\nmodule to remove its construction cost from total time, and wrapped in a list\ncall so that its traversal cost isn’t skewed by being a generator. This may be\novershadowed by the cost of the inner iterations’ loops, but it’s best to remove as\nmany variables as we can. For example, though range supports multiple scans,\ntests’ times inflate by some 25% if its list wrapper is removed.\nIteration Results\nWhen the top-level script of the prior section is run under the standard CPython\n3.12 on the same macOS host, it prints the following with total times in seconds\n—after cueing the drum roll, that is:\n$ python3 timer_tests.py\nPython 3.12.2 on darwin\nforLoop  : 0.26035 => [0...9999]\nlistComp : 0.20781 => [0...9999]\nmapCall  : 0.14399 => [0...9999]\ngenExpr  : 0.41133 => [0...9999]\ngenFunc  : 0.41203 => [0...9999]\nAll results same.\nIn short, map calls are faster than list comprehensions, which are quicker than\nfor loops, and both generator expressions and functions come in last and\nroughly tied for slowest. Perhaps surprisingly, generator expressions run much\nslower than equivalent list comprehensions today. As warned in the prior\nchapter, although wrapping a generator expression in a list call makes it\nfunctionally equivalent to a list comprehension, the internal implementations of\nthe two expressions appear to differ:\nreturn [abs(x) for x in repslist]            # 0.20 seconds\nreturn list(abs(x) for x in repslist)        # 0.41 seconds: differs internally",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 825,
      "chapter": null,
      "content": "We’re also effectively timing the list call for the generator test, but given that\nthis does not seem to hamper map, it’s likely moot. Though the exact cause for\nthe difference would require deeper analysis (and probably source code\nspelunking), this seems to make sense given that the generator expression must\ndo extra work to save and restore its state during value production; the list\ncomprehension does not, and runs quicker here and in other tests ahead.\nOther Pythons’ results\nFor comparison, following are the same tests’ speed results on the same host\nusing the current PyPy—the optimized Python implementation discussed in\nChapter 2, whose current 7.3 release implements the Python 3.10 language.\nPyPy is roughly 5X quicker than CPython here (and up to 10X), though its\ntiming results can vary widely if its JIT has not yet compiled code in full (and a\nfuture and currently hypothetical CPython JIT may or may not even the race):\n$ pypy3 timer_tests.py \nPython 3.10.14 on darwin\nforLoop  : 0.04329 => [0...9999]\nlistComp : 0.01876 => [0...9999]\nmapCall  : 0.04132 => [0...9999]\ngenExpr  : 0.08744 => [0...9999]\ngenFunc  : 0.08726 => [0...9999]\nAll results same.\nOn PyPy alone, list comprehensions win the title in this test today, but generators\nstill lose soundly, and the fact that all of PyPy’s results are so much quicker\ntoday seems the larger point here. On CPython, map is still quickest so far.\nInterestingly, these results are very different than they were in this book’s prior\nedition on Windows under CPython 3.3—when generators placed in the middle\nof the pack between list comprehensions and for loops, for loops fared\nsubstantially worse than they do today, and the script had a different name for\nhistorical reasons:\nC:\\code> c:\\python33\\python timeseqs.py\n3.3.0 (v3.3.0:bd8afb90ebf2, Sep 29 2012, 10:57:17) [MSC v.1600 64 bit (AMD64)]\nforLoop  : 1.33290 => [0...9999]\nlistComp : 0.69658 => [0...9999]\nmapCall  : 0.56483 => [0...9999]\ngenExpr  : 1.08457 => [0...9999]",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 826,
      "chapter": null,
      "content": "genFunc  : 1.07623 => [0...9999]\nWhile these absolute times naturally reflect older and slower test hosts, these\ntools’ relative performance has clearly changed in the last 12 years—and\nprobably should be expected to do so again in another dozen!\nFor more good times: Function calls and map\nAll of the foregoing is true as advertised, but watch what happens when\nExample 21-5 performs an inline operation on each iteration, such as a +\nexpression, instead of calling a built-in function like abs. Only the test functions’\noperations (in bold) need to be modified here, and the imported and reused\nrunner handles tests generically.\nExample 21-5. timer_tests2.py\nfrom timer_runner import runner\nrepslist = list(range(10_000))\ndef forLoop():\n   res = []\n   for x in repslist:\n       res.append(x + 10)\n   return res\ndef listComp():\n   return [x + 10 for x in repslist]\ndef mapCall():\n   return list(map((lambda x: x + 10), repslist))\ndef genExpr():\n   return list(x + 10 for x in repslist)\ndef genFunc():\n   def gen():\n       for x in repslist:\n           yield x + 10\n   return list(gen())\nrunner(forLoop, listComp, mapCall, genExpr, genFunc)\nNow, map is slower than the for loop statements, despite the fact that the\nlooping-statements version is larger in terms of code. This could either mean that\nthe need to call a user-defined function makes map slower—or equivalently, that",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 827,
      "chapter": null,
      "content": "the lack of function calls makes the others quicker. On CPython 3.12 and the\nsame host as before:\n$ python3 timer_tests2.py\nPython 3.12.2 on darwin\nforLoop  : 0.28682 => [10...10009]\nlistComp : 0.24389 => [10...10009]\nmapCall  : 0.49622 => [10...10009]\ngenExpr  : 0.44047 => [10...10009]\ngenFunc  : 0.44476 => [10...10009]\nAll results same.\nThese results have also been consistent in CPython: the prior edition’s Python\n3.3 results on a slower machine were again relatively similar, discounting test\nmachine differences. Because the interpreter optimizes so much internally,\nperformance analysis of Python code like this is a very tricky affair. Without\nnumbers, it’s virtually impossible to guess which method will perform the best;\nagain, the best you can do is time your code with your test parameters.\nIn this case, what we can say is that on this Python, using a user-defined lambda\nfunction in map calls seems to slow its performance disproportionately (though +\nis also slower than a trivial abs across the board), and that list comprehensions\nrun quickest in this case (though slower than map in some others). List\ncomprehensions seem consistently faster than for loops, but even this must be\nqualified—the list comprehension’s relative speed might be affected by its extra\nsyntax (e.g., if filters), Python changes, and usage modes we did not time here.\nFor deeper truth, Example 21-6 codes one last takeoff on our tests to apply a\nsimple user-defined function in all five iterations timed. Again, we must only\nmodify the relevant (and bold) bits of the test functions themselves.\nExample 21-6. timer_tests3.py\nfrom timer_runner import runner\nrepslist = list(range(10_000))\ndef F(x): return x\ndef forLoop():\n   res = []\n   for x in repslist:\n       res.append(F(x))\n   return res",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 828,
      "chapter": null,
      "content": "def listComp():\n   return [F(x) for x in repslist]\ndef mapCall():\n   return list(map(F, repslist))\ndef genExpr():\n   return list(F(x) for x in repslist)  \ndef genFunc():\n   def gen():\n       for x in repslist:\n           yield F(x)\n   return list(gen())\nrunner(forLoop, listComp, mapCall, genExpr, genFunc)\nWhen coded this way and run in CPython 3.12 on the same host again, map\nimproves its relative times, and comes in second between list comprehensions\nand for loops—instead of being slower than all others as it was for +:\n$ python3 timer_tests3.py \nPython 3.12.2 on darwin\nforLoop  : 0.36206 => [0...9999]\nlistComp : 0.31181 => [0...9999]\nmapCall  : 0.35479 => [0...9999]\ngenExpr  : 0.50531 => [0...9999]\ngenFunc  : 0.50290 => [0...9999]\nAll results same.\nThat is, map may be slower simply because it requires function calls, and\nfunction calls are relatively slow in general. Since map can’t avoid calling\nfunctions, it may lose by association, and the other iteration tools may win when\nthey can use expressions instead. On the other hand, this hypothesis alone can’t\nexplain the better showing for map in the abs results of the first timer_tests.py:\ncalling built-in functions may be a special and fast case for map in CPython (a\ntheory supported by “Conclusion: Comparing tools” and ord at the end of this\nchapter’s benchmarking safari).\nAll this being said, performance should not be your primary concern when\nwriting Python code—the first thing you should do to optimize Python code is to\nnot optimize Python code! Write for readability and simplicity first, then",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 829,
      "chapter": null,
      "content": "optimize later, if and only if needed. It could very well be that any of the five\niteration alternatives we’ve timed is quick enough for the data sets your program\nneeds to process; if so, program clarity should be the chief goal.\nMore Module Mods\nOur timer.py module works as designed, but it could be a bit more user-friendly.\nMost obviously, its functions require passing in repetition counts as first\narguments, and provide no defaults for them—a minor point, perhaps, but less\nthan ideal in a general-purpose tool. To do better, it could allow repetition counts\nto be passed in as keyword arguments with defaults, much the same way we did\nfor the print emulators of Chapter 18.\nHere, though, these arguments’ names would have to be distinct to avoid\nclashing with those of the timed function (e.g., _reps instead of reps). While\nwe’re at it, the function object could also be a positional-only argument to\nprevent name func from clashing with a keyword argument of the same name in\nthe timed subject. Example 21-7 codes the required mods (sans docs for space).\nReview Chapter 18’s argument-ordering coverage for more insight.\nExample 21-7. timer2.py\n\"Use keyword-only arguments with defaults for reps, and positional-only for func.\"\nimport time\ntimer = time.perf_counter\ndef once(func, /, *pargs, **kargs):\n   start   = timer()\n   result  = func(*pargs, **kargs)\n   elapsed = timer() - start\n   return (elapsed, result)\ndef total(func, /, *pargs, _reps=100_000, **kargs):\n   total = 0\n   for i in range(_reps):\n       time, result = once(func, *pargs, **kargs)\n       total += time\n   return (total, result)\ndef bestof(func, /, *pargs, _reps=5, **kargs):\n   return min(once(func, *pargs, **kargs) for i in range(_reps))\ndef bestoftotal(func, /, *pargs, _reps1=50, **kargs):",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 830,
      "chapter": null,
      "content": "return min(total(func, *pargs, **kargs) for i in range(_reps1))    # _reps => **\nThis version is not backward compatible: it uses different names and modes for\nrepetition arguments, which means the timer_runner.py we wrote earlier would\nrequire a minor edit to use it (see the examples package’s timer2_*.py).\nOtherwise, it works the same—compare its output on the same host with\ntimer.py’s results listed at Example 21-2:\n$ python3\n>>> import timer2\n>>> timer2.total(pow, 2, 1000, _reps=100_000)[0]\n0.0865794476121664\n>>> timer2.total(str.upper, 'hack' * 100, _reps=100_000)[0]\n0.04620114527642727\n>>> timer2.bestoftotal(pow, 2, 1000, _reps1=50, _reps=100_000)[0]\n0.07179990829899907\n>>> timer2.bestoftotal(str.upper, 'hack' * 100, _reps1=50, _reps=100_000)[0]\n0.0393348871730268\nThis time, though, we can allow the functions’ repetition defaults to apply or not:\n>>> timer2.total(str.upper, 'hack' * 100)[0]\n0.047992002684623\n>>> timer2.bestoftotal(str.upper, 'hack' * 100)[0]\n0.03935634717345238\n>>> timer2.bestoftotal(str.upper, 'hack' * 100, _reps=10_000)[0]\n0.00393257150426507\nNotice how the _reps argument for total, if passed, in bestoftotal calls is\npropagated along in **kargs because it’s not matched otherwise. For more\nvetting of this, time user-defined functions with richer argument headers as in\nthe following. Per the first four timings, passing keyword arguments to a timed\nfunction adds a small time cost for unpacking—an unavoidable overhead shared\nby the original timer.py, but irrelevant when collecting relative times:\n>>> def f(a, b, c=88, d=99): return(a, b, c, d)\n>>> f(1, 2)\n(1, 2, 88, 99)\n>>> timer2.bestoftotal(f, 1, 2, _reps1=50, _reps=100_000)\n(0.014080120716243982, (1, 2, 88, 99))\n>>> timer2.bestoftotal(f, 1, 2, c=66, d=77, _reps1=50, _reps=100_000)",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 831,
      "chapter": null,
      "content": "(0.021575820166617632, (1, 2, 66, 77))\n>>> timer2.bestoftotal(f, 1, 2, c=66, d=77, _reps1=50)\n(0.021694606635719538, (1, 2, 66, 77))\n>>> timer2.bestoftotal(f, 1, 2, c=66, d=77, _reps=100_000)\n(0.021556788589805365, (1, 2, 66, 77))\n>>> def f(a, *b, c=88, **d): return(a, b, c, d)\n>>> f(1, 2, 3, c=66, d=77, e=88)\n(1, (2, 3), 66, {'d': 77, 'e': 88})\n>>> timer2.bestoftotal(f, 1, 2, 3, c=66, d=77, e=88)\n(0.030859854072332382, (1, (2, 3), 66, {'d': 77, 'e': 88}))\n>>> timer2.bestoftotal(f, 1, 2, 3, c=66, d=77, e=88, _reps1=10)\n(0.030802161898463964, (1, (2, 3), 66, {'d': 77, 'e': 88}))\n>>> timer2.bestoftotal(f, 1, 2, 3, c=66, d=77, e=88, _reps=10_000)\n(0.0030745575204491615, (1, (2, 3), 66, {'d': 77, 'e': 88}))\nBeyond this, custom benchmarking code is fun but open ended, and this section\nmust stop here for space. As next steps, you might modify the timing script to\nmeasure the speed of set and dictionary comprehensions and their for\nequivalents (open questions we’ll return to ahead); explore Python’s profile\nand cProfile modules (tools that create full-program profiles instead of focused\nbenchmarks); or explore Python’s timeit module, which works much like some\nof this chapter’s homegrown code, and offers extra options—as you’ll learn in\nthe next section.\nNOTE\nDecorator timers preview: Notice how we must pass functions into the timers manually here.\nIn Chapter 39, we’ll code decorator-based timer alternatives with which timed functions are\ncalled normally, but require extra @ preamble syntax where defined. Decorators may be more\nuseful to instrument functions with timing logic when they are already being used within a\nlarger system, and don’t as easily support the specific test-call patterns assumed here—when\ndecorated, every call to the function runs the timing logic, which is either a plus or minus\ndepending on your goals.\nBenchmarking with Python’s timeit\nThe preceding section used custom timing functions to compare code speed. As",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 832,
      "chapter": null,
      "content": "teased there, the Python standard library also ships with a module named timeit\nthat can be used in similar ways, but offers added flexibility, with support for\ntiming code strings in addition to functions, as well as a command-line mode.\nAs usual in Python, it’s important to understand fundamental principles like\nthose illustrated in the prior section. Python’s “batteries included” approach\nmeans you’ll usually find precoded options for most goals, though you still need\nto know the ideas underlying them to choose and use them properly. Indeed, the\ntimeit module is a prime example of this—it’s had a history of being misused\nby newcomers who didn’t yet understand the concepts it embodies. Now that\nwe’ve learned the basics, though, let’s move ahead to a tool that can automate\nmuch of our work.\nBasic timeit Usage\nLet’s start with this module’s fundamentals before leveraging them in larger\nscripts. With timeit, tests are specified by either callable objects or statement\nstrings; the latter can hold multiple statements if they use ; separators or \\n\ncharacters for line breaks, and spaces or tabs to indent statements in nested\nblocks (e.g., \\n\\t). Tests may also give setup actions, and can be launched from\nboth command lines and API calls, and from both scripts and the REPL.\nAPI-calls mode\nFor example, the timeit module’s repeat call returns a list giving the total time\ntaken to run a test a number of times, for each of repeat runs—the min of this\nlist yields the best time among the runs, and helps filter out system load\nfluctuations that can otherwise skew timing results artificially high (like our\nearlier bestoftotal). Code to be timed is passed to the stmt keyword (or first\npositional) argument, and may be a string or no-argument function.\nThe following shows this call in action in REPLs on macOS, timing a list\ncomprehension on both CPython 3.12 and the optimized PyPy implementation\nof Python described in Chapter 2 (as earlier, its tested 7.3 release implements the\nPython 3.10 language). The results here give the best total time in seconds\namong 5 runs that each execute the code string 1,000 times; the code string itself\nconstructs a 1,000-item list of integers each time through (timeit also has",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 833,
      "chapter": null,
      "content": "reasonable defaults for repeat counts, but being explicit ensures comparability):\n$ python3\n>>> import timeit\n>>> min(timeit.repeat(stmt='[x ** 2 for x in range(1000)]', number=1000, repeat=5))\n0.05187674192711711\n$ pypy3\n>>>> import timeit\n>>>> min(timeit.repeat(stmt='[x ** 2 for x in range(1000)]', number=1000, repeat=5))\n0.00252467580139637\nYou’ll notice that PyPy checks in at 20X faster than CPython 3.12. This is a\nsmall artificial benchmark, of course, but stunning nonetheless, and reflects a\nrelative speed ranking that is generally supported by other tests run in this book\n(though CPython will still beat PyPy on some types of code ahead, and again,\nmay improve with a future JIT). To be fair, this particular test measures the\nspeed of both a list comprehension and integer math, but integer math is\nubiquitous in Python code, and PyPy’s win is larger on noninteger tests (try\nsquaring a float to see for yourself).\nThese results also differ from the preceding section’s relative version speeds,\nwhere PyPy was some 10X quicker for list comprehensions. Apart from the\ndifferent type of code being timed here, the different coding structure inside\ntimeit may have an effect too—for code strings like those tested here, timeit\nbuilds, compiles, and executes a function def statement string that embeds the\ntest string, thereby avoiding a function call per inner loop. This is irrelevant from\na relative-speed perspective, though: times from the same given tool are\ncomparable.\nYou’ll also notice that code to be timed is a string here. As you’ll see ahead, this\nrequires manually coded line breaks and indentation in some usage modes, and\nall code to be timed this way must conform to Python’s rules for string literals,\nquotes, and escapes. For instance, a code string cannot embed the same quotes\nused to enclose it without also escaping them. You can avoid this potential\ndownside by timing callables, though they’re limited to zero arguments.\nCommand-line mode\nThe timeit module also can be run as a script from a command line—either by",
      "content_length": 2052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 834,
      "chapter": null,
      "content": "explicit pathname, or, more portably, automatically located on the module search\npath with Python’s –m switch (we used this earlier to launch PyDoc in\nChapter 15 and IDLE in Chapter 3). In this mode, you’ll need to quote or escape\nPython code in the command line per your console shell’s rules; double quotes\nare generally portable, but this also qualifies as a potential downside.\nAlso in this mode, timeit reports the average time for a single –n loop, in either\n“usec” microseconds (millionths), “msec” milliseconds (thousandths), “sec”\nseconds, or “nsec” nanoseconds; to compare results here to the total time values\nreported by API calls, multiply by the number of loops run—51 usec here *\n1,000 loops is 51k usec, 51 msec, and 0.051 seconds in total time. For CPython\n3.12 on macOS:\n$ python3 -m timeit -n 1000 \"[x ** 2 for x in range(1000)]\"\n1000 loops, best of 5: 51.9 usec per loop\n$ python3 -m timeit -n 1000 -r 50 \"[x ** 2 for x in range(1000)]\"\n1000 loops, best of 50: 51.8 usec per loop\nAs another example, we can use command lines to verify that choice of timer\ncall doesn’t impact speed comparisons run in this chapter so far. timeit uses\ntime.perf_counter by default but its -p flag instructs it to instead use\ntime.process_time, both discussed earlier (spoiler: as tested, there’s no\ndiscernible difference):\n$ python3 -m timeit -p -n 1000 -r 50 \"[x ** 2 for x in range(1000)]\"\n1000 loops, best of 50: 51.8 usec per loop\nWe can also use timeit to see how PyPy stacks up again—but we’re in for a bit\nof a surprise:\n$ pypy3 -m timeit -n 1000 -r 50 \"[x ** 2 for x in range(1000)]\"\nWARNING: timeit is a very unreliable tool. use pyperf or something \nelse for real measurements\npypy3 -m pip install pyperf\npypy3 -m pyperf timeit -n '1000' -r '50' '[x ** 2 for x in range(1000)]'\n------------------------------------------------------------\n1000 loops, average of 50: 1.54 +- 0.568 usec per loop (using standard deviation)\nAs you can see, PyPy has customized command-line mode in its version of",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 835,
      "chapter": null,
      "content": "timeit to both emit a subjective redirect to an alternative timer, as well as\nmodify the result display to show averages instead of minimums. These are both\nopinionated mods made since this book’s prior edition, though perhaps\nunderstandable for a tool so focused on (and sensitive to) benchmark results. See\nthe web for more on the suggested pyperf; it’s not part of Python’s standard\nlibrary and requires a separate install, but may offer more advanced timing\noptions that may or may not apply to your goals.\nHandling multiline statements\nHappily, timeit’s API mode is still opinion-free today. To time larger blocks of\ncode in API-call mode, either load code from a file, use a triple-quoted block\nstring, or use newlines and tabs or spaces to satisfy Python’s syntax. Because\nyou pass Python string objects to a Python function in this mode, there are no\nshell considerations, though be careful to handle nested quotes with escapes or\nmixed quotes if needed. The following, for instance, times Chapter 13 loop\nalternatives in CPython 3.12; you can use the same pattern to time the file-line-\nreader alternatives in Chapter 14:\n$ python3\n>>> import timeit\n>>> min(timeit.repeat(number=100_000, repeat=5,\n        stmt='L = [1, 2, 3, 4, 5]\\nfor i in range(len(L)): L[i] += 1'))\n0.028153221122920513\n>>> min(timeit.repeat(number=100_000, repeat=5,\n        stmt='L = [1, 2, 3, 4, 5]\\ni=0\\nwhile i < len(L):\\n\\tL[i] += 1\\n\\ti += 1'))\n0.03337613819167018\n>>> min(timeit.repeat(number=100_000, repeat=5,\n        stmt='L = [1, 2, 3, 4, 5]\\nM = [x + 1 for x in L]'))\n0.021507765166461468\nTo run multiline statements like these in command-line mode, appease your shell\nby passing each statement line as a separate argument, with whitespace for\nindentation—timeit concatenates all the lines together with a newline character\nbetween them, and later re-indents for its own statement-nesting purposes.\nLeading spaces may work better for indentation than tabs in this mode due to\nshell variability, and be sure to quote the code arguments if required by your\nshell (the first of the following was line-split here to fit, but must be input on a",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 836,
      "chapter": null,
      "content": "single line in some shells):\n$ python3 -m timeit -n 100000 -r 5 \"L = [1,2,3,4,5]\" \"i=0\" \"while i < len(L):\"\n \"    L[i] += 1\" \"    i += 1\"\n100000 loops, best of 5: 332 nsec per loop\n$ python3 -m timeit -n 100000 -r 5 \"L = [1,2,3,4,5]\" \"M = [x + 1 for x in L]\"\n100000 loops, best of 5: 218 nsec per loop\nOther timeit usage modes\nThe timeit module also allows you to provide setup code that is run in the main\nstatement’s scope, but whose time is not charged to the main statement’s total—\nuseful for initialization code you wish to exclude from total time, such as\nimports of required modules and builds of test data. Because they’re run in the\nsame scope, any names created by setup code are available to the main test\nstatement; names defined in the interactive shell generally are not.\nTo specify setup code, use a –s in command-line mode (or many of these for\nmultiline setups) and a setup argument string in API-call mode. This can focus\ntests more sharply, as in the following, whose second command splits list\ninitialization off to a setup statement to time just iteration in command-line\nmode. As a general rule of thumb, though, the more code you include in a test\nstatement, the more applicable its results will generally be to realistic code:\n$ python3 -m timeit -n 100000 -r 5 \"L = [1,2,3,4,5]\" \"M = [x + 1 for x in L]\"\n100000 loops, best of 5: 211 nsec per loop\n$ python3 -m timeit -n 100000 -r 5 -s \"L = [1,2,3,4,5]\" \"M = [x + 1 for x in L]\"\n100000 loops, best of 5: 175 nsec per loop\nTiming sort speed\nTo demo setup code in API-call mode, the interaction that follows times a sort-\nbased option in Chapter 18’s minimum-value example. To avoid page flipping,\nhere’s the function it times, in the module mins.py (Example 18-2):\ndef min4(*args):\n    return sorted(args)[0]\nAnd here are the results—proving indirectly that sequences sort much faster",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 837,
      "chapter": null,
      "content": "when they are ordered than they do with randomly ordered contents (to see for\nyourself, run this in Chapter 18’s code folder):\n>>> from timeit import repeat                   # Standard-library module\n>>> min(repeat(number=1000, repeat=5,           # Sort an ordered list in min4\n        setup='from mins import min4\\n'         # Adjacent strings concatenated\n              'vals=list(range(1000))',         # Code within () spans lines\n        stmt= 'min4(*vals)'))                   # First import prints output\n0.011066518723964691\n>>> min(repeat(number=1000, repeat=5,           # Sort a randomly ordered list\n        setup='from mins import min4\\n'\n              'import random\\n'\n              'vals=[random.random() for i in range(1000)]',\n        stmt= 'min4(*vals)'))\n0.068130933213979\nSee Python’s manuals for more on the random module used here and in earlier\nchapters, as well as more on timeit. Not shown here, timeit also lets you ask\nfor just total time, time callable objects instead of strings, use a class-based API,\nand leverage additional command-line switches and API-call arguments we\ndon’t have space to cover. Instead, the next section codes a timeit utility that\ngoes beyond manual command lines and REPL calls.\nAutomating timeit Benchmarking\nRather than going into more timeit details, let’s study a program that deploys it\nto time both coding alternatives and Python versions. This will let us easily\ndefine a set of tests to time in a separate file, and will allow us collect data from\nmultiple Pythons, both individually and grouped (though grouped mode comes\nwith limits today, as you’ll see).\nBenchmark module\nTo get started, the module file in Example 21-8, pybench.py, is set up to time a\nsequence of statements coded in scripts that import and use it, with either the\nPython running its code or all Python versions named in a list. It uses some\napplication-level tools described ahead. Because it mostly applies ideas we’ve\nalready explored and is amply documented, though, it’s listed as mostly self-",
      "content_length": 2035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 838,
      "chapter": null,
      "content": "study material, and an exercise in reading Python code.\nExample 21-8. pybench.py\nr\"\"\"\nTime the speed of one or more Pythons on multiple code-string \nbenchmarks with timeit.  This is a function, to allow timed tests\nto vary.  It times all code strings in a passed list, in either:\n1) The Python running this script, by timeit API calls\n2) Multiple Pythons whose paths are passed in a list, by reading\n  the output of timeit command lines run by os.popen that use \n  Python's -m switch to find timeit on the module search path\nIn command-line mode (2) only, this replaces all \" in timed code\nwith ', to avoid clashes with argument quoting; splits multiline \nstatements into one quoted argument per line so all will be run;\nand replaces all \\t in indentation with 4 spaces for uniformity.\nCaveats: \n- Command-line mode (only) uses naive quoting and MAY FAIL if code\n embeds and requires double quotes; quoted code is incompatible \n with the host shell; or command length exceeds shell limits.\n- PyPy is largely unusable in command-line mode (2) today, as its\n modified timeit output in this mode is jarring in the report.\n- This does not (yet?) support a setup statement in any mode: the\n time of all code in the test stmt is charged to its total time.\n \nAs fallbacks on fails, use either this module's API-call mode to \ntest one Python at a time, or the homegrown timer.py module.\n\"\"\"\nimport sys, os, time, timeit\ndefnum, defrep= 1000, 5    # May vary per stmt\ndef show_context(): \n   \"\"\"\n   Show run's context using an arguably gratuitous f-string\n   that fails on 3.10 PyPy without \"...\" for nested ' quotes.\n   \"\"\"\n   print(f\"Python {'.'.join(str(x) for x in sys.version_info[:3])}\"\n         f' on {sys.platform}'\n         f\" at {time.strftime('%b-%d-%Y, %H:%M:%S')}\")\ndef runner(stmts, pythons=None, tracecmd=False):\n   \"\"\"\n   Main logic: run tests per input lists which determine usage modes.\n   stmts:   [(number?, repeat?, stmt-string)]\n   pythons: None=host python only, or [python-executable-paths]\n   \"\"\"",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 839,
      "chapter": null,
      "content": "show_context()\n   for (number, repeat, stmt) in stmts:\n       number = number or defnum\n       repeat = repeat or defrep    # 0=default\n       if not pythons:\n           # Run stmt on this python: API call\n           # No need to split lines or quote here\n           best = min(timeit.repeat(stmt=stmt, number=number, repeat=repeat))\n           print(f'{best:.4f}  {stmt[:70]!r}')\n       else:\n           # Run stmt on all pythons: command line\n           # Split lines into quoted arguments\n           print('-' * 80)\n           print(repr(stmt))                                         # show quotes\n           for python in pythons:\n               stmt  = stmt.replace('\"', \"'\")                        # all \" => '\n               stmt  = stmt.replace('\\t', ' ' * 4)                   # tab => ____\n               lines = stmt.split('\\n')                              # line => arg\n               args  = ' '.join(f'\"{line}\"' for line in lines)       # arg => \"arg\"\n               oscmd = f'{python} -m timeit -n {number} -r {repeat} {args}'\n               print(oscmd if tracecmd else python)\n               print('\\t' + os.popen(oscmd).read().rstrip())\nBenchmark script\nThis preceding file is really only half the picture. Testing scripts use this\nmodule’s function, passing in concrete though variable lists of statements and\nPythons to be tested, as appropriate for the usage mode desired. For example, the\nscript in Example 21-9, pybench_tests.py, tests a handful of statements and\nPythons by importing and using Example 21-8, and allows command-line\narguments to determine part of its operation: –a tests all listed Pythons instead of\njust one, and an added –t traces constructed command lines in full so you can\nsee how quotes, multiline statements, and tabs are handled per timeit’s\ncommand-line formats shown earlier (see both files’ docstrings for more details).\nExample 21-9. pybench_tests.py\n\"\"\"\nRun pybench.py to time one or more Pythons on multiple code strings.\nUse command-line arguments (which appear in sys.argv) to select modes:\n<python> pybench_tests.py\n   times just the hosting Python on all code listed in stmts below",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 840,
      "chapter": null,
      "content": "python3 pybench_tests.py -a\n   times all stmts in all pythons whose paths are listed below \npython3 pybench_tests.py -a -t\n   same as -a, but also traces command lines in full\nEdit stms below to change tested code, and edit pythons below to give \npaths of Python executables to be tested in -a mode.  To find a Python's \npath, start its REPL, run \"import sys\", and inspect \"sys.executable\".\n\"\"\"\nimport pybench, sys\npythons = [\n   '/Library/Frameworks/Python.framework/Versions/3.12/bin/python3',\n   '/Users/me/Downloads/pypy3.10-v7.3.16-macos_x86_64/bin/pypy3',\n]\nstmts = [\n# Iterations\n   (0, 0, '[x ** 2 for x in range(1000)]'),                        # (num,rpt,stmt)\n   (0, 0, 'res=[]\\nfor x in range(1000): res.append(x ** 2)'),     # \\n=multistmt\n   (0, 0, 'list(map(lambda x: x ** 2, range(1000)))'),             # \\n\\t=indent\n   (0, 0, 'list(x ** 2 for x in range(1000))'),\n# String ops\n   (0, 0, \"s = 'hack' * 2500\\nx = [s[i] for i in range(10_000)]\"),\n   (0, 0, \"s = '?'\\nfor i in range(10_000): s += '?'\"),            # A PyPy loss!\n]\ntracecmd = '-t' in sys.argv                           # -t: trace command lines?\npythons  = pythons if '-a' in sys.argv else None      # -a: all in list, else one?\npybench.runner(stmts, pythons, tracecmd)              # Time pythons on all stmts\nTiming individual Pythons\nThe following is the preceding script’s output when run to test a specific Python\n—the one running the script. This mode uses direct timeit API calls, not\ncommand lines, with total time listed in the left column, the statement tested\nquoted on the right, and a context line at the top courtesy of Python’s sys and\ntime modules (see its manuals for more info). These two runs again use\nCPython 3.12 and PyPy 7.3 (i.e., 3.10), respectively, on the same host:\n$ python3 pybench_tests.py\nPython 3.12.2 on darwin at Jun-27-2024, 15:21:02\n0.0533  '[x ** 2 for x in range(1000)]'\n0.0605  'res=[]\\nfor x in range(1000): res.append(x ** 2)'\n0.0804  'list(map(lambda x: x ** 2, range(1000)))'",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 841,
      "chapter": null,
      "content": "0.0759  'list(x ** 2 for x in range(1000))'\n0.4042  \"s = 'hack' * 2500\\nx = [s[i] for i in range(10_000)]\"\n0.8061  \"s = '?'\\nfor i in range(10_000): s += '?'\"\n$ pypy3 pybench_tests.py\nPython 3.10.14 on darwin at Jun-27-2024, 15:22:14\n0.0020  '[x ** 2 for x in range(1000)]'\n0.0040  'res=[]\\nfor x in range(1000): res.append(x ** 2)'\n0.0030  'list(map(lambda x: x ** 2, range(1000)))'\n0.0077  'list(x ** 2 for x in range(1000))'\n0.0529  \"s = 'hack' * 2500\\nx = [s[i] for i in range(10_000)]\"\n1.2942  \"s = '?'\\nfor i in range(10_000): s += '?'\"\nDrawing conclusions from this is left as a suggested exercise, but notice that\nPyPy actually loses to CPython on the very last test run. Again, performance can\nvary per code, and absolutes in benchmarking are perilous at best.\nTiming multiple Pythons\nAs noted, this script can also test multiple Pythons for each statement string, by\nadding a -a to the command line. In this mode the script itself is run by CPython\n3.12, which launches shell command lines that start other Pythons to run the\ntimeit module on the statement strings. To make this work, this mode must\nsplit, format, and quote multiline statements for use in command lines according\nto both timeit expectations and shell requirements.\nThis mode also relies on the -m Python command-line flag to locate timeit on\nthe module search path and run it as a script, as well as the os.popen and\nsys.argv standard-library tools to run a shell command and inspect command-\nline arguments, respectively. Add a -t to trace commands run, and see Python\nmanuals and other resources for more on these tools; os.popen is also\nmentioned briefly in Chapters 3, 9, and 13. Here is this mode’s result:\n$ python3 pybench_tests.py -a\nPython 3.12.2 on darwin at Jun-27-2024, 16:12:39\n--------------------------------------------------------------------------------\n'[x ** 2 for x in range(1000)]'\n/Library/Frameworks/Python.framework/Versions/3.12/bin/python3\n    1000 loops, best of 5: 52.7 usec per loop\n/Users/me/Downloads/pypy3.10-v7.3.16-macos_x86_64/bin/pypy3\n    WARNING: timeit is a very unreliable tool. use pyperf or something \n    else for real measurements",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 842,
      "chapter": null,
      "content": "pypy3 -m pip install pyperf\npypy3 -m pyperf timeit -n '1000' -r '5' '[x ** 2 for x in range(1000)]'\n------------------------------------------------------------\n1000 loops, average of 5: 2.66 +- 1.55 usec per loop (using standard deviation)\n…more output clipped…\nRegrettably, this all-Pythons mode is nearly unusable today: the opinionated\ninserts and mods made to timeit by PyPy render its command-line output very\ndifferent from the norm, and very difficult to read or list here. Moreover, there’s\nno way to disable these (short of dropping CPython’s timeit.py into PyPy’s\nlibrary folder, which seems too much for this demo to ask). Hence, while this\nscript’s -a mode still works in this edition, it may be best used to time a set of\nPythons that do not subjectively mod the behavior of a widely used standard-\nlibrary module like timeit.\nTiming set and dictionary iterations\nThe good news is that single-Python mode still allows us to easily script a set of\nbenchmarks—even on PyPy. The script in Example 21-10, for instance, uses the\ndriver module in Example 21-8 to see how sets and dictionaries fare.\nExample 21-10. pybench_tests2.py\nimport pybench\nstmts = [\n# Sets\n   (0, 0, '{x ** 2 for x in range(1000)}'),\n   (0, 0, 'set(x ** 2 for x in range(1000))'),\n   (0, 0, 's=set()\\nfor x in range(1000): s.add(x ** 2)'),\n# Dicts\n   (0, 0, '{x: x ** 2 for x in range(1000)}'),\n   (0, 0, 'dict((x, x ** 2) for x in range(1000))'),\n   (0, 0, 'd={}\\nfor x in range(1000): d[x] = x ** 2'),\n]\npybench.runner(stmts, None, False)    # No -a mode in this script\nAs suggested in the preceding chapter, passing a generator to a type name is\nindeed substantially slower than other construction schemes—as this script’s\noutput on both CPython 3.12 and PyPy 7.3 (3.10) finally proves:\n$ python3 pybench_tests2.py\nPython 3.12.2 on darwin at Jun-27-2024, 16:20:19",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 843,
      "chapter": null,
      "content": "0.0746  '{x ** 2 for x in range(1000)}'\n0.0947  'set(x ** 2 for x in range(1000))'\n0.0834  's=set()\\nfor x in range(1000): s.add(x ** 2)'\n0.0745  '{x: x ** 2 for x in range(1000)}'\n0.1174  'dict((x, x ** 2) for x in range(1000))'\n0.0754  'd={}\\nfor x in range(1000): d[x] = x ** 2'\n$ pypy3 pybench_tests2.py \nPython 3.10.14 on darwin at Jun-27-2024, 16:22:18\n0.0191  '{x ** 2 for x in range(1000)}'\n0.0222  'set(x ** 2 for x in range(1000))'\n0.0186  's=set()\\nfor x in range(1000): s.add(x ** 2)'\n0.0188  '{x: x ** 2 for x in range(1000)}'\n0.0398  'dict((x, x ** 2) for x in range(1000))'\n0.0185  'd={}\\nfor x in range(1000): d[x] = x ** 2'\nConclusion: Comparing tools\nIf you have time, check out pybench_tests3.py in the book example’s package\n(omitted here for space) for another test that verifies that this section’s timeit\ntools turn in results similar to the earlier timer.py homegrown tools. Its findings\ngenerally jive with those in “Iteration Results”, though they arrive at them by\nvery different means:\n$ python3 pybench_tests3.py \nPython 3.12.2 on darwin at Jun-27-2024, 16:26:44\n0.2766  \"res=[]\\nfor x in 'hack' * 2500: res.append(ord(x))\"\n0.2173  \"[ord(x) for x in 'hack' * 2500]\"\n0.1430  \"list(map(ord, 'hack' * 2500))\"\n0.4172  \"list(ord(x) for x in 'hack' * 2500)\"\nFor more fidelity, study this chapter’s code and run more tests on your own.\nBenchmarks can be great sport, but we’ll have to leave further excursions as\nsuggested exercises. Here, it’s time to wrap up this chapter and part, so we can\nmove on to learn more about the modules we’ve been using informally since\nChapter 16.\nNOTE\nCross-platform results: As a bonus, folder _benchmark-platform-results in the book’s\nexamples package collects CPython’s results for this chapter’s timer and pybench tests on\nmultiple platforms—macOS, Windows, Android, and Linux. Due to Python and host\ndifferences, not all its results are directly comparable, but they are relatively similar.",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 844,
      "chapter": null,
      "content": "Surprisingly, an Android foldable phone beats all the PCs, though these tests are CPU bound,\nand operations like file access may fare differently. Caveat: these results will change; retest\nwith your Pythons and devices for current data.\nFunction Gotchas\nNow that we’ve reached the end of the function story, let’s review some common\npitfalls. Functions have some jagged edges that you might not expect. They’re\nall relatively obscure, and a few have started to fall away from the language\ncompletely in recent releases, but most have been known to trip up new users.\nLocal Names Are Detected Statically\nAs you’ve learned, Python classifies names assigned in a function as locals by\ndefault; they live in the function’s scope and exist only while the function is\nrunning. What you may not realize is that Python detects locals statically, when\nit compiles the def’s code, rather than by noticing assignments as they happen at\nruntime. This leads to one of the most common oddities posted on the Python\nnewsgroup by beginners.\nNormally, a name that isn’t assigned in a function is looked up in the enclosing\nmodule:\n>>> X = 99\n>>> def selector():       # X used but not assigned\n        print(X)          # X found in global scope\n>>> selector()\n99\nHere, the X in the function resolves to the X in the module. But watch what\nhappens if you add an assignment to X after the reference:\n>>> def selector():\n        print(X)          # Does not yet exist!\n        X = 88            # X classified as a local name (everywhere)\n                          # Can also happen for \"import X\", \"def X\"...\n>>> selector()\nUnboundLocalError: cannot access local variable 'X' where it is not associated with",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 845,
      "chapter": null,
      "content": "a value\nYou get the name-usage error shown here, but the reason is subtle. Python reads\nand compiles this code when it’s typed interactively or imported from a module.\nWhile compiling, Python sees the assignment to X and decides that X will be a\nlocal name everywhere in the function. But when the function is actually run,\nbecause the assignment hasn’t yet happened when the print executes, Python\nsays you’re using an undefined name. According to its name rules, it should say\nthis; the local X is used before being assigned. In fact, any assignment in a\nfunction body makes a name local. Imports, =, nested defs, nested classes, and\nso on are all susceptible to this behavior.\nThe problem occurs because assigned names are treated as locals everywhere in\na function, not just after the statements where they’re assigned. Really, the\nprevious example is ambiguous: was the intention to print the global X and create\na local X, or is this a real programming error? Because Python treats X as a local\neverywhere, it’s seen as an error; if you mean to print the global X, you need to\ndeclare it in a global statement:\n>>> def selector():\n        global X                # Force X to be global (everywhere in function)\n        print(X)\n        X = 88\n>>> selector()\n99\nRemember, though, that this means the assignment also changes the global X, not\na local X. Within a function, you can’t use both local and global versions of the\nsame simple name. If you really meant to print the global and then set a local of\nthe same name, you’d need to import the enclosing module and use module\nattribute notation to get to the global version:\n>>> X = 99\n>>> def selector():\n        import __main__         # Import enclosing module\n        print(__main__.X)       # Qualify to get to global version of name\n        X = 88                  # Unqualified X classified as local\n        print(X)                # Prints local version of name",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 846,
      "chapter": null,
      "content": ">>> selector()\n99\n88\nQualification (the .X part) fetches a value from a namespace object. The\ninteractive namespace is a module called __main__, so __main__.X reaches the\nglobal version of X. If that isn’t clear, review Chapter 17.\nIn recent versions, Python has improved on this story somewhat by issuing for\nthis case the more specific “unbound local” error message shown in the example\nlisting (it used to simply raise a generic name error); this gotcha is still present in\ngeneral, though.\nDefaults and Mutable Objects\nAs noted briefly in Chapters 17 and 18, mutable values for default arguments\ncan retain state between calls, though this is often unexpected. In general, default\nargument values are evaluated and saved once when a def statement is run, not\neach time the resulting function is later called. Internally, Python saves one\nobject per default argument, attached to the function itself.\nThat’s usually what you want—because defaults are evaluated at def time, they\nlet you save values from the enclosing scope, if needed (functions defined within\nloops by factories may even depend on this behavior—see ahead). But because a\ndefault retains an object between calls, you have to be careful about changing\nmutable defaults. For instance, the following function uses an empty list as a\ndefault value, and then changes it in place each time the function is called:\n>>> def saver(x=[]):               # Saves away a list object\n        x.append(1)                # Changes same object each time!\n        print(x)\n>>> saver([2])                     # Default not used\n[2, 1]\n>>> saver()                        # Default used\n[1]\n>>> saver()                        # Grows on each call!\n[1, 1]\n>>> saver()\n[1, 1, 1]",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 847,
      "chapter": null,
      "content": "Some see this behavior as a feature—because mutable default arguments retain\ntheir state between function calls, they can serve some of the same roles as\n“static” local function variables in the C language. In a sense, they work much\nlike global variables, but their names are local to the functions and so will not\nclash with names elsewhere in a program.\nTo other observers, though, this seems like a gotcha—especially the first time\nthey run into it. There are better ways to retain state between calls in Python\n(e.g., using the nested scope closures and function attributes we met in this part,\nand the classes we will study in Part VI).\nMoreover, mutable defaults can be overridden by real values, and are tricky to\nremember (and to understand at all). They depend upon the timing of default\nobject construction. In the prior example, there is just one list object for the\ndefault value—the one created when the def is executed. You don’t get a new\nlist every time the function is called, so the list grows with each new append; it\nis not reset to empty each time.\nIf that’s not the behavior you want, simply make a copy of the default at the start\nof the function body, or move the default value expression into the function\nbody. As long as the value resides in code that’s actually executed each time the\nfunction is called, you’ll get a new object each time through:\n>>> def saver(x=None):\n        if x is None:             # No argument passed?\n            x = []                # Run code to make a new list each time\n        x.append(1)               # Changes new list object\n        print(x)\n>>> saver([2])\n[2, 1]\n>>> saver()                       # Doesn't grow here\n[1]\n>>> saver()\n[1]\nBy the way, the if statement in this example could almost be replaced by the\nassignment x = x or [], which takes advantage of the fact that Python’s or\nreturns one of its operand objects: if no argument was passed, x would default to\nNone, so the or would return the new empty list on the right.",
      "content_length": 1998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 848,
      "chapter": null,
      "content": "However, this isn’t exactly the same. If an empty list were passed in, the or\nexpression would cause the function to extend and return a newly created list,\nrather than extending and returning the passed-in list like the if version. (The\nexpression becomes [] or [], which evaluates to the new empty list on the\nright; see Chapter 12’s “Truth Values Revisited” if you don’t recall why.) Real\nprogram requirements may call for either behavior.\nToday, another way to achieve the value retention effect of mutable defaults in a\npossibly less confusing way is to use the function attributes we discussed in\nChapter 19:\n>>> def saver():\n        saver.x.append(1)\n        print(saver.x)\n>>> saver.x = []\n>>> saver()\n[1]\n>>> saver()\n[1, 1]\n>>> saver()\n[1, 1, 1]\nThe function name is global to the function itself, but it need not be declared\nbecause it isn’t changed directly within the function. This isn’t used in exactly\nthe same way, but when coded like this, the attachment of an object to the\nfunction is much more explicit (and arguably less magical).\nFunctions Without returns\nIn Python functions, return (and yield) statements are optional. When a\nfunction doesn’t return a value explicitly, the function exits when control falls\noff the end of the function body. Technically, all functions return a value; if you\ndon’t provide a return statement, your function returns the None object\nautomatically:\n>>> def proc(x):\n        print(x)                 # No return is a None return\n>>> x = proc('testing 123...')\ntesting 123...",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 849,
      "chapter": null,
      "content": ">>> print(x)\nNone\nFunctions such as this without a return are Python’s equivalent of what are\ncalled “procedures” in some languages. They’re usually invoked as statements,\nand the None results are ignored, as they do their business without computing a\nuseful result.\nThis is worth remembering, because Python won’t tell you if you try to use the\nresult of a function that doesn’t return one. As we noted in Chapter 11, for\ninstance, assigning the result of a list append method won’t raise an error, but\nyou’ll get back None, not the modified list:\n>>> list = [1, 2, 3]\n>>> list = list.append(4)        # append is a \"procedure\"\n>>> print(list)                  # append changes list in place\nNone\n“Common Coding Gotchas” discusses this more broadly. In general, any\nfunctions that do their business as a side effect are usually designed to be run as\nstatements, not expressions.\nMiscellaneous Function Gotchas\nHere are two additional function-related gotchas—mostly reviews, but common\nenough to reiterate.\nEnclosing scopes and loop variables\nWe met this gotcha in Chapter 17’s discussion of enclosing function scopes, but\nas a reminder: when coding factory functions (a.k.a. closures), be careful about\nrelying on enclosing function scope lookup for variables that are changed by\nenclosing loops. When a generated function is later called, all such references\nwill remember the value of the last loop iteration in the enclosing function’s\nscope. In this case, you must use defaults to save loop variable values instead of\nrelying on automatic lookup in enclosing scopes. See “Loops Require Defaults,\nNot Scopes” in Chapter 17 for more details on this topic.\nHiding built-ins by assignment",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 850,
      "chapter": null,
      "content": "Also in Chapter 17, we saw how it’s possible to reassign built-in names in a\ncloser local or global scope; the reassignment effectively hides (“shadows”) and\nreplaces that built-in’s name for the remainder of the scope where the assignment\noccurs. This means you won’t be able to use the original built-in value for the\nname. As long as you don’t need the built-in value of the name you’re assigning,\nthis isn’t an issue—many names are built in, and they may be freely reused.\nHowever, if you reassign a built-in name your code relies on, you may have\nproblems. So don’t do that, unless you really mean to. The good news is that the\nbuilt-ins you commonly use will soon become second nature, and Python’s error\ntrapping will alert you early in testing if your built-in name is not what you think\nit is.",
      "content_length": 802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 851,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter rounded out our look at functions and built-in iteration tools with a\nlarger case study that measured the performance of iteration alternatives and\nother tools we’ve met along the way, as well as one alternative Python\nimplementation. We used both custom timer code as well as Python’s timeit\nwith a helper to time code in a variety of modes. We also reviewed common\nfunction-related mistakes to help you avoid pitfalls.\nThis concludes the functions part of this book. The next part expands on what\nwe already know about modules—files of tools that form the topmost\norganizational unit in Python programs, and the structure in which functions\nreside. After that, we will explore classes, which are largely packages of\nfunctions with special first arguments. As you’ll see, classes can implement\nobjects that tap into the iteration protocol, just like the generators and iterables\nwe used here. In fact, everything we have learned in this part of the book will\napply when functions take the guise of class methods.\nBefore moving on to modules, though, be sure to work through this chapter’s\nquiz, as well as the exercises for this part of the book, to practice what you’ve\nlearned about functions here.\nTest Your Knowledge: Quiz\n1. What conclusions can you draw from this chapter about the relative\nspeed of Python iteration tools?\n2. What conclusions can you draw from this chapter about the relative\nspeed of the Pythons timed?\nTest Your Knowledge: Answers\n1. In CPython today, list comprehensions are often the quickest of the\nbunch; map beats list comprehensions in Python when all tools must call\nbuilt-in functions; for loops tend to be slower than comprehensions;",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 852,
      "chapter": null,
      "content": "and generator functions and expressions are slower than all other\noptions. Under PyPy, some of these findings differ; map often turns in a\ndifferent relative performance, for example, and list comprehensions\nseem always quickest, perhaps due to function-level optimizations.\nAt least that’s the case today on the Python versions tested, on the test\nmachine used, and for the type of code timed—these results may vary if\nany of these three variables differ. Use the homegrown timer or\nstandard library timeit to test your use cases for more relevant results.\nAlso keep in mind that iteration is just one component of a program’s\ntime: more code gives a more complete picture.\n2. In general, PyPy 7.3 (implementing Python 3.10) is substantially faster\nthan CPython 3.12. In some cases timed, PyPy was 5X–20X faster than\nCPython, though in isolated cases (e.g., some string operations), PyPy\nwas slower than CPython, and PyPy’s use of a JIT can impact\nbenchmark results.\nAt least that’s the case today on the Python versions tested, on the test\nmachine used, and for the type of code timed—these results may vary if\nany of these three variables differ. Use the homegrown timer or\nstandard library timeit to test your use cases for more relevant results.\nThis is especially true when timing Python implementations, which may\nbe arbitrarily optimized in each new release—in fact, CPython may\nsoon adopt a JIT like PyPy, which could invalidate results here. We also\ndidn’t test any of the many other Python implementations; see Chapter 2\nfor other options to time on your own.",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 853,
      "chapter": null,
      "content": "Test Your Knowledge: Part IV Exercises\nIn these exercises, you’re going to start coding more sophisticated programs. Be\nsure to check the solutions in “Part IV, Functions and Generators” in\nAppendix B, and be sure to start writing your code in module files. You won’t\nwant to retype these exercises in a REPL if you make a mistake.\n1. The basics: At the Python interactive prompt, write a function named\necho that prints its single argument to the screen and call it\ninteractively, passing a variety of object types: string, integer, list,\ndictionary. Then, try calling it without passing any argument. What\nhappens? What happens when you pass two arguments?\n2. Arguments: Write a function called adder in a Python module file\nnamed adder1.py. The function should accept two arguments and return\nthe sum (or concatenation) of the two. Then, add code at the bottom of\nthe file to call the adder function with a variety of object types (two\nstrings, two lists, two floating points), and run this file as a script from\nthe system command line. Do you have to print the call statement\nresults to see results on your screen?\n3. Arbitrary arguments: Copy the file you wrote in the last exercise to\nadder2.py, generalize its adder function to compute the sum of an\narbitrary number of arguments, and change the calls to pass more or\nfewer than two arguments. What type is the returned sum? (Hints: a\nslice such as S[:0] returns an empty sequence of the same type as S,\nand the type built-in function can test types; but see the manually coded\nmin examples in Chapter 18 for a simpler approach.) What happens if\nyou pass in arguments of different types? What about passing in\ndictionaries?\n4. Keywords: In an adder3.py, change the adder function from exercise 2\nto accept and sum/concatenate three arguments: def adder(red,\ngreen, blue). Now, provide default values for each argument, and\nexperiment with calling the function interactively or code tests in the",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 854,
      "chapter": null,
      "content": "file. Try passing one, two, three, and four arguments. Then, try passing\nkeyword arguments. Does the call adder(blue=1, red=2) work?\nWhy? Finally, copy and generalize the new adder to accept and\nsum/concatenate an arbitrary number of keyword arguments in an\nadder4.py. This is similar to what you did in exercise 3, but you’ll need\nto iterate over a dictionary, not a tuple. (Hint: the dict.keys method\nreturns an iterable you can step through with a for or while, but be\nsure to wrap it in a list call to index it; dict.values may help here\ntoo.)\n5. Dictionary tools: Write a function called copyDict(dict) that copies\nits dictionary argument. It should return a new dictionary containing all\nthe items in its argument. Use the dictionary keys method to iterate (or\nstep over a dictionary’s keys without calling keys). Copying sequences\nis easy (X[:] makes a top-level copy); does this work for dictionaries,\ntoo? As explained in this exercise’s solution, because dictionaries come\nwith similar tools, this and the next exercise are just coding exercises\nbut still serve as representative functions.\n6. Dictionary tools: Write a function called addDict(dict1, dict2) that\ncomputes the union (i.e., merge) of two dictionaries. It should return a\nnew dictionary containing all the items in both its arguments (which are\nassumed to be dictionaries). If the same key appears in both arguments,\nfeel free to pick a value from either. Test your function by writing it in a\nfile and running the file as a script. What happens if you pass lists\ninstead of dictionaries? How could you generalize your function to\nhandle this case, too? (Hint: see the type built-in function used earlier.)\nDoes the order of the arguments passed in matter? Dictionary merge is\nalso a built-in today (actually, several), but you’re trying to stretch\nyourself a bit by coding it manually.\n7. More argument-matching examples: First, define the following six\nfunctions (either interactively or in a module file that can be imported):\ndef f1(a, b): print(a, b)            # Normal args",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 855,
      "chapter": null,
      "content": "def f2(a, *b): print(a, b)           # Positional collectors\ndef f3(a, **b): print(a, b)          # Keyword collectors\ndef f4(a, *b, **c): print(a, b, c)   # Mixed modes\ndef f5(a, b=2, c=3): print(a, b, c)  # Defaults\ndef f6(a, b=2, *c): print(a, b, c)   # Defaults and positional collectors\nNow, test the following calls interactively, and try to explain each result;\nin some cases, you’ll probably need to fall back on the matching rules\ncovered in Chapter 18. Do you think mixing matching modes is a good\nidea in general? Can you think of cases where it would be useful?\n>>> f1(1, 2)\n>>> f1(b=2, a=1)\n>>> f2(1, 2, 3)\n>>> f3(1, x=2, y=3)\n>>> f4(1, 2, 3, **dict(x=2, y=3))\n>>> f5(1)\n>>> f5(1, 4)\n>>> f5(1, c=4)\n>>> f6(1)\n>>> f6(1, *[3, 4])\n8. Primes revisited: Recall the following code snippet from Chapter 13,\nwhich simplistically determines whether a positive integer is prime:\nx = num // 2                              # For some num > 1, start at \nhalf\nwhile x > 1:\n    if num % x == 0:                      # Remainder 0? Factor found",
      "content_length": 1041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 856,
      "chapter": null,
      "content": "print(num, 'has factor', x)\n        break                             # Exit now and skip else\n    x -= 1\nelse:                                     # Normal exit, when x reaches 1\n    print(num, 'is prime')\nPackage this code as a reusable function in a module file (num should be\na passed-in argument), and add some calls to the function at the bottom\nof your file to test. While you’re at it, experiment with replacing the\nfirst line’s // operator with / to see how true division breaks this code\n(refer back to Chapter 5 if you need a reminder). What can you do about\nnegatives, and the values 0 and 1? How about speeding this up? Your\noutputs should look something like this:\n13 is prime\n13.0 is prime\n15 has factor 5\n15.0 has factor 5.0\n9. Iterations and comprehensions: Write code to build a new list\ncontaining the square roots of all the numbers in this list: [2, 4, 9,\n16, 25]. Code this as a for loop first, then as a map call, then as a list\ncomprehension, and finally as a generator expression. Use the sqrt\nfunction in the built-in math module to do the calculation (i.e., import\nmath and say math.sqrt(X)). Of the four, which approach do you like\nbest?\n10. Timing tools: In Chapter 5, we saw three ways to compute square roots:\nmath.sqrt(X), X ** .5, and pow(X, .5). If your programs run a lot\nof these, their relative performance might become important. To see\nwhich is quickest, use the timer2.py module (Example 21-7) we wrote\nin this chapter to time each of these three tools. Use its bestoftotal\nfunction to test. Which of the three square root tools seems to run fastest\non your device and Python in general? Finally, how might you use\ntimer2.py to interactively time the speed of dictionary comprehensions",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 857,
      "chapter": null,
      "content": "versus for loops? What about comprehensions with if clauses and\nnested for loops?\n11. Recursive functions: Write a simple recursion function named\ncountdown that prints numbers as it counts down to zero. For example,\na call countdown(5) will print: 5 4 3 2 1 stop. There’s no obvious\nreason to code this with an explicit stack or queue, but what about a\nnonfunction approach? Would a generator make sense here?\n12. Computing factorials: Finally, a computer-science classic (but\ndemonstrative nonetheless). We employed the notion of factorials in\nChapter 20’s coverage of permutations: N!, computed as N*(N-1)*(N-\n2)*...1. For instance, 6! is 6*5*4*3*2*1, or 720. Code and time four\nfunctions that, for a call fact(N), each return N!. Code these four\nfunctions (1) as a recursive countdown per Chapter 19; (2) using the\nfunctional reduce call per Chapter 19; (3) with a simple iterative\ncounter loop per Chapter 13; and (4) using the math.factorial library\ntool per Chapter 20. Use this chapter’s timeit to time each of your\nfunctions. What conclusions can you draw from your results?",
      "content_length": 1083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 858,
      "chapter": null,
      "content": "Part V. Modules and Packages",
      "content_length": 28,
      "extraction_method": "OCR"
    },
    {
      "page_number": 859,
      "chapter": null,
      "content": "Chapter 22. Modules: The Big\nPicture\nThis chapter begins our in-depth look at the Python module—the highest-level\nprogram organization unit, which packages program code and data for reuse, and\nprovides self-contained namespaces that minimize variable name clashes across\nyour programs. Modules were introduced in Chapter 3, and we’ve been using\nthem more and more since Chapter 16, but this part of the book provides a\nfocused, detailed look at this Python tool.\nThis first chapter in Part V reviews module basics, and offers a general look at\nthe role of modules in the overall structure of programs. In the chapters that\nfollow, we’ll dig into the coding details behind that theory. Along the way, we’ll\nalso flesh out module fine points omitted so far—you’ll learn about reloads, the\n__name__ and __all__ attributes, package imports, relative import syntax,\nnamespace packages, the __getattr__ hook, the __main__.py file, and so on.\nBecause modules and classes are really just glorified namespaces, this part\nformalizes namespace concepts as well.\nModule Essentials\nIn simple and concrete terms, modules typically correspond to Python source\ncode files. Each file of code is a module automatically, and modules import other\nmodules to use the names they define. Modules might also correspond to\nextensions coded in external languages such as C, Java, or C#, and even to entire\ndirectories in package imports, which extend the model for nested files. In all\ntheir forms, modules are processed with two statements and one tool:\nimport\nLets a client (importer) fetch a module as a whole\nfrom",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 860,
      "chapter": null,
      "content": "Allows clients to fetch particular names from a module\nimportlib.reload\nProvides a way to reload a module’s code without stopping Python\nWe’ve used imports in prior examples to load both Python standard-library\nmodules, as well as code files that reside in the current directory—the one we\nare in when launching the REPL or a script file. While straightforward on the\nsurface, these tools imply an underlying model that’s richer than you might\nthink. Before we delve into its details, though, let’s begin by getting a handle on\nthe purpose of modules in our Python programs.\nWhy Use Modules?\nIn short, modules provide an easy way to organize components into a system, by\nserving as self-contained packages of variables known as namespaces. All the\nnames defined at the top level of a module file become attributes of the imported\nmodule object, but a file’s names can’t be seen without imports, and don’t clash\nwith names in other files.\nThis model is related to the scopes we studied in the last part of this book. As we\nlearned in Chapter 17, imports give access to names in a module’s global scope.\nThe file surrounding a function is always that function’s own global scope, but\nimports allow one file to see the global names of another file as attributes. That\nis, the module file’s global scope morphs into the module object’s attribute\nnamespace when it is imported.\nUltimately, this allows us to link individual files into a larger program system,\nwith three primary benefits:\nCode reuse\nModules make code permanent. Because a module’s code is saved in a file,\nyou can both run it multiple times and use it in multiple programs. As you’ve\nlearned, code you type at a Python REPL goes away when you exit Python,",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 861,
      "chapter": null,
      "content": "but code in module files can be rerun as many times as you wish. Moreover,\nthe tools you define in modules may be reused by any number of external\nclients, and in programs coded both now and in the future.\nMinimizing redundancy\nModule reuse naturally enables shared copies of common code. If more than\none file uses the same or similar code, you can write it once in a module that\ncan then be imported by many clients. For example, in the prior chapter’s\nbenchmarking examples, test scripts imported common timer and test-runner\nfunctions, instead of repeating them. In other words, modules—like\nfunctions—help us factor code to avoid the redundancy that results from\ncopy-and-paste programming. As for functions, this minimizes work when\ncommon code must be changed.\nNamespace partitioning\nModules are also the highest-level namespace structure in a Python program.\nAlthough they are fundamentally just packages of names, these packages are\nalso self-contained—you can never see a name in another file unless you\nexplicitly import that file. Much like the local scopes of functions, this helps\navoid name clashes across your programs. In fact, you can’t avoid this\nfeature—everything “lives” in a module. Because both the code you run and\nthe objects you create are always implicitly enclosed in modules, modules\ngroup components by nature.\nAt least that’s the abstract story. To truly understand the role of modules in a\nPython system, we need to digress for a moment and explore the general\nstructure of a Python program.",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 862,
      "chapter": null,
      "content": "Python Program Architecture\nSo far in this book, many examples have sugarcoated the complexity of Python\nprograms. In practice, programs usually involve more than just one file. For all\nbut the simplest scripts, your programs will take the form of multifile systems—\nas the code-benchmarking programs of the preceding chapter illustrated. Even if\nyou can get by with coding a single file yourself, you will almost certainly wind\nup using external files that someone else has already written.\nThis section reviews the general architecture of Python programs—the way you\ndivide a program into a collection of source code files (a.k.a. modules) and link\nthe parts into a whole. As you’ll see, Python fosters a modular program structure\nthat groups functionality into coherent and reusable units, in ways that are\nalmost automatic. Along the way, this section also explores the central concepts\nof Python modules, imports, and object attributes.\nHow to Structure a Program\nAt a base level, a Python program consists of text files containing Python\nstatements, with one main top-level file, and zero or more supplemental files\nknown as modules.\nHere’s how this works. The top-level (a.k.a. script) file contains the main flow of\ncontrol of your program—this is the file you run to launch your application. The\nmodule files are libraries of tools, used to group components used by the top-\nlevel file, and possibly elsewhere. Top-level files use tools defined in module\nfiles, and modules use tools defined in other modules.\nAlthough they are files of code too, module files generally don’t do anything\nwhen run directly; rather, they define tools intended for use in other files. A file\nimports a module to gain access to the tools it defines, which are known as its\nattributes—variable names attached to objects such as functions. Ultimately, we\nimport modules and access their attributes to use their tools.\nImports and Attributes\nTo make this a bit more concrete, Figure 22-1 sketches the structure of a Python\nprogram composed of three files: a.py, b.py, and c.py. The file a.py is chosen to",
      "content_length": 2090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 863,
      "chapter": null,
      "content": "be the top-level script file; it will be a simple text file of statements, which is\nexecuted from top to bottom when launched. The files b.py and c.py are\nmodules; they are simple text files of statements as well, but they are not usually\nlaunched directly. Instead, as explained previously, modules are normally\nimported by other files that wish to use the tools the modules define.\nFigure 22-1. Program architecture in Python\nFor instance, suppose the file b.py in Figure 22-1 defines a function called job\nfor external use, per Example 22-1 (and ignoring c.py for the moment). As you\nlearned when studying functions in Part IV, b.py will contain a Python def\nstatement to generate the function, which you can later run by passing values in\nparentheses after the function’s name.\nExample 22-1. b.py (module)\ndef job(tool):\n   print(tool, 'coder')\nThis function probably seems trivial at this point in this book, but we’re keeping\nit simple to focus on module basics. Now, suppose a.py wants to use job. To this\nend, it might contain Python statements like those in Example 22-2.\nExample 22-2. a.py (script)",
      "content_length": 1108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 864,
      "chapter": null,
      "content": "import b\nb.job('Python')\nThe first of these, a Python import statement, gives the file a.py access to\neverything defined by top-level code—that is, code not nested inside a function\nor class—within the file b.py. The code import b roughly means:\nLoad the file b.py (unless it’s already loaded), and give me access to all its\nattributes through the name b.\nTo satisfy such goals, import (and, as you’ll see later, from) statements execute\nand load other files on request. More formally, in Python, cross-file module\nlinking is not resolved until such import statements are executed at runtime;\ntheir net effect is to assign module names—simple variables like b—to loaded\nmodule objects. In fact, the module name used in an import statement serves\ntwo purposes: it identifies the external file to be loaded (by base name b here),\nbut it also becomes a variable assigned to the loaded module.\nSimilarly, objects defined by a module’s code are also created at runtime, while\nthe import is executing: import literally runs statements in the target file one at a\ntime to create its contents. Along the way, every name assigned at the top level\nof the file becomes an attribute of the module, accessible to importers. For\nexample, the second of the statements in a.py calls the function job defined in\nthe module b—and created by running its def statement during the import—\nusing object attribute notation. The code b.job means:\nFetch the value of the name job that lives within the object b.\nThis happens to be a callable function in our example, so we pass a string in\nparentheses ('Python'). If you run a.py, the words “Python coder” will be\nprinted—hardly a shocker if you’ve read prior chapters, but illustrative.\nAs we’ve seen, the object.attribute notation is general and pervasive in\nPython code because most objects have useful attributes that are fetched with the\n“.” operator. Some attributes reference callable objects like functions that take\naction (e.g., a salary computer), while others are simple values that denote data\n(e.g., a person’s name).\nImports are similarly general because any file can import tools from any other",
      "content_length": 2135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 865,
      "chapter": null,
      "content": "file. For instance, the file a.py in Figure 22-1 may import b.py to call its\nfunction, but b.py might also import c.py to leverage different tools defined\nthere. In fact, import chains can go as deep as you like: in this example, module\na can import b, which can import c, which can import b again, and so on. More\nrealistically, in the benchmarking code of the prior chapter, test scripts imported\nrunner modules, which imported timer modules.\nThe main point behind all this is that modules (and module packages, described\nin Chapter 24) are the topmost level of code reuse in Python. Components coded\nin module files can be used both in your original program and in any other\nprograms you may write later. For instance, if we later discover that the function\nb.job in Example 22-1 is widely useful, we can deploy it in completely different\nprograms; all we have to do is import b again from the other programs. While\nunlikely in this simple demo, modules by nature create packages of reusable\ntools.\nStandard-Library Modules\nNotice the rightmost portion of Figure 22-1. As we’ve seen along the way, some\nof the modules that your programs will import are provided by Python itself and\nare not files you will code.\nPython automatically comes with a large collection of utility modules known as\nthe standard library. This collection, hundreds of modules large at last count,\ncontains platform-independent support for common programming tasks:\noperating system interfaces, object persistence, text pattern matching, network\nand internet scripting, GUI construction, multithreading, and much more.\nNone of these tools are part of the Python language itself, but you can use them\nby simply importing the appropriate modules on any standard Python\ninstallation. Because they are standard-library modules, you can also be\nreasonably sure that they will be available and will work portably on most\nplatforms on which you will run Python code.\nThis book’s examples employ a few of the standard library’s modules—time,\ntimeit, sys, and os in the last chapter’s code, for instance—but we’ll really\nonly scratch the surface of the library’s story here. For a complete look, browse\nthe Python standard-library reference manual, available online at python.org and",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 866,
      "chapter": null,
      "content": "elsewhere. See Chapter 15 for more on these manuals; the PyDoc tool discussed\nthere also provides library-module info and lists every importable module,\nincluding the standard library.\nBecause there are so many standard-library modules, browsing is the best way to\nget a feel for what tools are available. You can also find tutorials on Python\nlibrary tools in books that cover application-level programming, but the standard\nmanuals are free, viewable in any web browser, and updated each time Python is\nrereleased. As of Python 3.10, sys.stdlib_module_names also provides a\nsimple list of all standard-library modules—importable or not:\n$ python3\n>>> len(sys.stdlib_module_names)           # That's a lot of modules\n300\nHow Imports Work\nThe prior section talked about importing modules without fully explaining what\nhappens when you do so. Because imports are at the heart of program structure\nin Python, this section goes into more formal detail on the import operation to\nmake this process less abstract.\nSome C programmers like to compare the Python module import operation to a C\n#include, but they really shouldn’t—in Python, imports are not textual\ninsertions of one file into another. They are really runtime operations that\nperform three distinct steps the first time a program imports a given file:\n1. Find the module’s file.\n2. Compile it to bytecode (if needed).\n3. Run the module’s code to build the objects it defines.\nTo help you better understand module imports, the following sections explore\neach of these steps in turn.\nFirst, though, bear in mind that all three of these steps are carried out only the\nfirst time a module is imported during a program’s execution. Later imports of\nthe same module in a program’s run bypass all three of these steps and simply",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 867,
      "chapter": null,
      "content": "fetch the already loaded module in memory.\nPython does this by storing loaded modules in a normal dictionary named\nsys.modules, and checking for a module’s name there at the start of an import\noperation. In fact, sys.modules['name'] means the same as name after an\nimport name, and sys.modules’ keys iterator or keys method lists all imported\nmodules:\n>>> import sys, os\n>>> sys.modules['os'] is os                # Name string => module\nTrue\n>>> sorted(sys.modules)                    # Or sys.modules.keys()\n…names of all loaded modules…\nWe’ll explore other roles for sys.modules in upcoming chapters. On imports,\nthough, if the requested module is not already present in sys.modules, a three-\nstep process begins.\nStep 1: Find It\nFirst, Python must locate the module file referenced by an import statement.\nNotice that the import statement in the prior section’s example names the file\nwithout a .py extension and without its directory path: it just says import b,\ninstead of something like import c:\\dir\\b.py or similar on Unix. Path and\nextension details are omitted in imports on purpose; instead, Python uses a\nstandard module search path along with known file types to locate the module\nfile corresponding to an import statement.\nBecause this is the main part of the import operation that programmers must\nknow about, we’ll return to this topic by itself in a moment.\nStep 2: Compile It (Maybe)\nAfter finding a source code file that matches an import statement by traversing\nthe module search path, Python next compiles it to a lower-level form known as\nbytecode, if necessary. We discussed bytecode in Chapter 2, but it’s a bit richer\nthan explained there.",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 868,
      "chapter": null,
      "content": "When you first import a module, Python compiles the module’s .py source code\nfile to bytecode, and saves the bytecode in a file with a .pyc extension if\npossible. On later program runs, Python will load the bytecode from its .pyc file\nand skip the compile step, as long as the bytecode file uses a compatible format\nand was made by the importing Python, and you have not edited and saved the\nsource code file since the bytecode file was made.\nImportantly, if you change a module’s source code, its bytecode file will be re-\ncreated the next time you run a program that imports the module. This ensures\nthat a module’s bytecode is always in sync with its source, and your Python.\nAll of this is automatic and can generally be taken on faith by most Python users,\nbut a brief look at the complete story can help make the process less mysterious\nthan it should be. In more detail, bytecode files can be created in two flavors, the\nfirst of which is used by default, and the second of which came online in Python\n3.7:\nTimestamp-based bytecode files are created by simply importing\nmodules and are the default, original, and most common option. This is\nthe flavor to use if you want imports to be fast, and don’t have atypical\nneeds.\nHash-based bytecode files are created by using the compileall or\npy_compile library modules. Once created, the Python command-line\nswitch --check-hash-based-pycs may be used to configure their\noperation (see Python’s manual for this flag’s three options).\nIn either model, Python automatically saves enough info to know when a\nbytecode file must be re-created. Specifically, bytecode files embed a “magic”\nnumber identifying the bytecode’s format, along with either the source code\nfile’s last-modified timestamp and size, or a hash value derived from the source\ncode file’s content. Bytecode filenames also include the implementation name\nand version of the Python that created them.\nWhen a module is imported, Python first checks to see if it was previously\nimported by the program, and uses the already loaded module if so. Otherwise, it\nlooks for a usable bytecode file corresponding to the module’s source code file,\nby comparing the info saved with the bytecode file against the current specs of",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 869,
      "chapter": null,
      "content": "the running Python and the source code file. A .pyc bytecode file is usable if it\nhas the same base name as the source code file, and:\nUses a compatible format—by checking the “magic” number\nIs up to date with the source code file—by comparing either saved\ntimestamp and size, or hash value\nWas created by the running Python—by inspecting implementation and\nversion tags in the filename\nIf there is a bytecode file that passes all these checks, it is loaded, and the\ncompilation step is skipped. If not, the source code is compiled to bytecode, and\neither saved or resaved in a bytecode file with all the noted info.\nPrograms still run if bytecode files cannot be saved (compiled code is then\nsimply created in memory and discarded on exit), and the -B Python command-\nline switch can turn off bytecode saves, though it’s rarely needed. When they are\nsaved, bytecode files are written to and loaded from a subdirectory named\n__pycache__ that’s located alongside their corresponding source code files.\nThe __pycache__ subdirectory avoids both clutter in your source code folders,\nand contention and recompiles when multiple Pythons are installed. For\nexample, here’s what happens when the same module file in this chapter’s\nexamples folder is imported by three different Pythons—two CPythons, and the\nPyPy we used in the preceding chapter (on Windows, use dir and py instead of\nls and Python commands here):\n$ ls\ncodefile.py\n$ python3.12\n>>> import codefile\n$ python3.8\n>>> import codefile\n$ pypy3\n>>>> import codefile\n$ ls\n__pycache__   codefile.py",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 870,
      "chapter": null,
      "content": "$ ls __pycache__\ncodefile.cpython-312.pyc     codefile.cpython-38.pyc     codefile.pypy310.pyc\nTechnically, the __pycache__ subdirectory was introduced in Python 3.2 and\nisn’t available earlier, but this book is focused on Python 3.X only, and you’re\nvery unlikely to come across a 3.1 or 3.0 in the wild today.\nAlso, keep in mind that bytecode compilation happens when a file is being\nimported. Because of this, you will not usually see a .pyc bytecode file for the\ntop-level file of your program, unless it is also imported elsewhere—only\nimported files leave behind .pyc files on your machine. The bytecode of top-\nlevel files is used internally and discarded; bytecode of imported files is saved in\nfiles to speed up future imports.\nTop-level files are often designed to be executed directly and not imported at all.\nLater, though, you’ll see that it is possible to design a file that serves both as the\ntop-level code of a program and as a module of tools to be imported. Such a file\nmay be either executed and imported and does generate a .pyc in the latter role.\nTo learn how this works, watch for the discussion of the special __name__\nattribute and __main__ in Chapter 25.\nNOTE\nRunning from bytecode only: As a now-special case, programs will also run if Python finds\nonly bytecode files but no source code files, though this involves more than just deleting your\n.py files. As of Python 3.2, it generally requires that m.pyc files be located where m.py files\nnormally would be—via either moving and renaming from __pycache__ or generating with\nthe legacy option of Python’s compileall (-b in its command-line mode). For instance, the\nfollowing makes .pyc files from all .py files in the current directory without requiring moves\nand renames:\n$ python3 -m compileall -b -l .\nRunning from just bytecode requires a compatible Python and doesn’t fully conceal your code,\nbut is used by some tools to ship programs in standalone form when the hosting Python\nversion can be included or ensured. See Python docs for more info, and the related solution in\n“Part I, Getting Started” in Appendix B.\nStep 3: Run It",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 871,
      "chapter": null,
      "content": "After compiling or loading the module’s bytecode, the final step of an import\noperation executes the bytecode. This effectively runs all the statements in the\nmodule’s file in turn, from top to bottom, and any assignments made to names\nduring this step generate attributes of the resulting module object. This is how\nthe tools defined by the module’s code are created. For instance, def statements\nin a file are run at import time to create functions and assign attributes within the\nmodule to those functions. The functions can then be called later in the program\nby the file’s importers.\nBecause this last import step actually runs the file’s code, if any top-level code in\na module file does real work, you’ll see its results at import time. For example,\ntop-level print statements in a module show output when the file is imported.\nFunction def statements (and class statements up later in this book) simply\ndefine objects for later use.\nAs you can see, import operations involve quite a bit of work—they search for\nfiles, possibly run a compiler, and run Python code. Because of this, any given\nmodule is imported only once per process by default. Future imports skip all\nthree import steps and reuse the already loaded module in memory, per the\nsys.modules check noted earlier. If you need to import a file again after it has\nalready been loaded (for example, to support dynamic customizations), you can\nforce the issue with an importlib.reload call—a tool we’ll study in the next\nchapter.\nBear in mind that this process is completely automatic—it’s a side effect of\nrunning programs—and most programmers probably won’t care about or even\nnotice the mechanics, apart from faster startups due to skipped compile steps.\nOne part of this process is likely to show up on your radar, though, per the next\nsection’s elaboration.\nThe Module Search Path\nAs mentioned earlier, the part of the import procedure that most programmers\nwill need to care about is usually the first “find it” part—locating the file to be\nimported. Because you may need to tell Python where to look to find files to\nimport, you need to know how to tap into its module search path (the set of\ndirectories searched) in order to extend it. This section covers the components",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 872,
      "chapter": null,
      "content": "that make up the search path and describes how to mod it for folders of your\nown.\nSpecial case: built-in modules like sys, coded in C and statically linked into\nPython, as well as frozen modules like os, optimized for faster startup as of\nPython 3.11, are always checked first before scanning the module search path\nand hence take precedence. Given that there are only a few standard-library\nmodules in these categories, we can safely ignore them here and focus on the\nsearch used for the vast majority of modules you’ll import—including your own.\nIf you’re curious, sys.builtin_module_names lists the items in the first of\nthese extrasearch categories:\n>>> sum(1 for m in sys.builtin_module_names if m[0] != '_')        # Count non _X\n11\nSearch-Path Components\nIn many cases, you can rely on the automatic nature of the module search path\nand won’t need to configure this path at all. If you want to be able to import\nuser-defined files across directory boundaries, though, you will need to\ncustomize this path. Roughly, Python’s module search path is composed of the\nconcatenation of the following components, some of which are preset for you\nand some of which you can tailor to tell Python where to look:\n1. The home directory of the program\n2. Directories listed in PYTHONPATH (if set)\n3. Standard-library directories and files\n4. The site-packages directory of third-party extensions\n5. The directories listed in any .pth files (if present)\nUltimately, the concatenation of these five components initializes the built-in\nsys.path—a changeable list of directory-name strings that are searched from\nfirst to last for imported files (we’ll revisit this later in this section). The first,\nthird, and fourth components of the search path are defined automatically. The\nsecond and fifth components, though, can be used to extend the path to include",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 873,
      "chapter": null,
      "content": "your own source code directories. Here’s a rundown on all five:\nHome directory (automatic)\nPython first looks for the imported file in the code’s “home” directory. The\nmeaning of this entry depends on how you are running the code. When\nyou’re running a program, this entry is the directory containing your\nprogram’s top-level script file. When you’re working interactively in a\nREPL, this entry is instead the directory in which you are currently working\n(which is why we’ve used this directory for imported files so far). Code run\nwith Python’s -c and -m switches used in earlier chapters also uses the\ncurrent working directory for the home component.\nBecause this directory is always searched first, if a program is located\nentirely in a single directory, all of its imports will work automatically with\nno path configuration required. On the other hand, because this directory is\nsearched first, its files will also override modules of the same name in\ndirectories elsewhere on the path; be careful not to accidentally hide\nstandard-library modules this way if you need them in your program, or use\npackage tools you’ll meet later that can partially sidestep this issue with\nnested folders.\nAlso remember that this home directory pertains only to the search path used\nto resolve imports, and does not impact the current working directory used\nfor relative filenames in your script. Pathless files created by a script will still\nshow up where you are when you launch it (specifically, in the folder\nreturned by os.getcwd), not in the script’s home folder at the front of the\nmodule search path. Module imports and file access are disjoint ideas.\nAny PYTHONPATH directories (configurable)\nNext, Python searches all directories listed in your PYTHONPATH environment\nvariable setting, from left to right (assuming you have set this at all: it’s not\ngenerally preset for you). In brief, PYTHONPATH is simply a list of user-\ndefined and platform-specific names of directories that contain Python\nsource code or bytecode files. You can add all the directories from which you\nwish to be able to import, and Python will extend the module search path to\ninclude all the directories your PYTHONPATH lists. See Appendix A for tips on",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 874,
      "chapter": null,
      "content": "setting this variable.\nBecause Python searches the home directory first, this setting is only\nimportant when importing files across directory boundaries—that is, if you\nneed to import a file that is stored in a different directory from the file that\nimports it. You may need to set your PYTHONPATH variable once you start\nwriting substantial programs and tools, but when you’re first starting out, as\nlong as you save all your module files in the directory in which you’re\nworking (i.e., the home directory) your imports will work without needing to\nmake this setting.\nStandard-library directories (automatic)\nNext, Python automatically searches the directories where the standard-\nlibrary modules are installed on your machine. These include modules coded\nin Python, and others coded in C. Because these directories are always\nsearched, they normally do not need to be added to your PYTHONPATH (or\nincluded in path files, ahead).\nThe standard-library site-packages directory of third-party extensions\n(automatic)\nNext, Python automatically adds the site-packages subdirectory of its\nstandard library to the module search path. By convention, this is the place\nwhere most third-party extensions are installed, often automatically by\nPython’s pip install tool. Because the site-packages install directory of such\nextensions is always part of the module search path, clients can import the\nmodules of these extensions without any path settings.\nAny .pth path-file directories (configurable)\nFinally, a lesser-used feature of Python allows users to add directories to the\nmodule search path by simply listing them, one per line, in a text file whose\nname ends with a .pth suffix (for “path”). These path-configuration files are a\nsomewhat advanced installation-related feature; we won’t cover them fully",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 875,
      "chapter": null,
      "content": "here, but they provide an alternative to PYTHONPATH settings.\nIn short, text files of directory names dropped in an appropriate directory can\nserve roughly the same role as the PYTHONPATH environment variable setting.\nFor instance, a file with .pth extension and any name may be placed in the\nsite-packages subdirectory of the installed Python’s standard library to\nextend the module search path. To locate this subdirectory inspect sys.path\nfor a path ending in site-packages, per its coverage ahead.\nWhen such a file is present, Python will add the directories listed on each\nline of the file, from first to last, near the end of the module search path list\n—currently after the site-packages directory as described here. In fact,\nPython will collect the directory names in all the .pth path files it finds and\nfilter out any duplicates and nonexistent entries. Because they are files rather\nthan shell settings, path files can apply to all users of a given Python, instead\nof just one user or shell, and may be simpler to set up than environment\nvariables in some contexts.\nFor more details on this feature, consult Python’s library manual, and\nespecially its documentation for the standard-library module site—this\nmodule configures the locations of Python libraries and path files, and its\ndocumentation describes the expected locations of path files in general.\nWhen getting started, though, you may be better served by setting\nPYTHONPATH, and only if you must import across directories. Path files are\nused more often by third-party libraries installed in Python’s site-packages.\nAll that being said, Python’s import mechanism is wildly extensible, and even\nmore convoluted than described here. For example, PYTHONPATH entries may\nalso name ZIP files treated like read-only folders; PYTHONHOME can set standard-\nlibrary locations; a file named with suffix ._pth can fully override sys.path\nnorms; “virtual environments” made with the module venv avoid package\nversion clashes by localizing search paths to install folders; the relative-import\n“.” syntax of Chapter 24 constrains module search in packages; and all this has\nmorphed regularly, and may again.\nWhile this book covers common usage, its description is intentionally narrow for\nspace, durability, and audience. You should consult Python’s manuals for more",
      "content_length": 2322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 876,
      "chapter": null,
      "content": "on the imports story if and when unique requirements arise.\nConfiguring the Search Path\nThe net effect of all of the foregoing is that both the PYTHONPATH and path file\ncomponents of the search path allow you to tailor the places where imports look\nfor files. The way you set environment variables and where you store path files\nvaries per platform. For instance, on macOS and Windows, you might set\nPYTHONPATH to a list of directories separated by colons and semicolons,\nrespectively, like this:\n/Users/me/pycode/utilities:/Volumes/ssd/pycode/package1      # Unix paths list\nC:\\Users\\me\\pycode\\utilities;d:\\pycode\\package1              # Windows paths list\nOn Unix systems (e.g., macOS, Linux, and Android), an export shell command\nsets environment variables for the shell and anything it runs, and can be coded in\nstartup files like ~/.bash_profile for permanence. The following on macOS sets\nthe path to enable imports of modules from the code folders of Chapters 21 and\n20 (“…” is snipped text):\n$ pwd\n/Users/me/…/LP6E/Chapter22\n$ export PYTHONPATH=/Users/me/…/LP6E/Chapter21:/Users/me/…/LP6E/Chapter20\n$ python3\n>>> import pybench, permute\n>>> pybench\n<module 'pybench' from '/Users/me/…/LP6E/Chapter21/pybench.py'>\n>>> permute\n<module 'permute' from '/Users/me/…/LP6E/Chapter20/permute.py'>\n>>> permute.permute1([1, 2, 3, 4])\n>>> pybench.runner([(100, 5, '[x ** 2 for x in range(2 ** 16)]')])\nPath syntax like “..” for parent folders also works on PYTHONPATH and is\ninterpreted relative to the current working directory—which is not necessarily\nthe home directory of launched programs, if their top-level scripts live\nelsewhere. Windows consoles use similar commands (e.g., replace export with\nset, or use setx for permanence), but Windows users generally set environment\nvariables in Settings; search for the environment-variables GUI there.",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 877,
      "chapter": null,
      "content": "Instead of—or in addition to—PYTHONPATH, you might create a text file in your\nPython install’s site-packages folder named with a .pth extension (e.g.,\nmypath.pth), which looks like this on macOS:\n/Users/me/pycode/utilities               # Unix .pth paths file\n/Volumes/ssd/pycode/package1             # One path per line\nThese settings are analogous on all platforms, but the details vary too widely for\nfully inclusive coverage here. See Appendix A for pointers on extending your\nmodule search path with PYTHONPATH on various platforms.\nTo see how your Python configures the module search path on your platform,\nand to find the location of its site-packages folder which can host path files, you\ncan always inspect sys.path—the topic of the next section.\nThe sys.path List\nIf you want to see how the module search path is truly configured on your\nmachine, print the built-in sys.path list. This list of directory-name strings is\nthe actual module search path within Python; on imports, Python searches each\ndirectory in this list from left to right and uses the first file match it finds.\nInspecting the module search path\nPython configures sys.path at program startup, merging the path components\nwe met earlier. The result is a list of directories searched on each import of a\nnew file. Python exposes this list for multiple reasons. For one thing, it provides\na way to verify the search-path settings you’ve made—if you don’t see your\nsettings somewhere in this list after restarting Python, you need to recheck your\nwork. On macOS, for example, with these custom settings:\nPYTHONPATH set to /Users/me/pycode1:/Users/me/pycode2\nA mypath.pth path file in the Python install’s site-packages that lists\n/Users/me/pycode3\nThe search path looks like this when inspected in a REPL or printed from a\nscript:",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 878,
      "chapter": null,
      "content": ">>> import sys\n>>> sys.path\n['', '/Users/me/pycode1', '/Users/me/pycode2', \n'/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip',\n'/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12',\n'/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload',\n'/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages', \n'/Users/me/pycode3']\nThe empty string at the front means the current directory, and the two custom\nsettings are merged in per the order given earlier. The rest are standard-library\nfolders and files, and the site-packages home for third-party extensions.\nChanging the module search path\nThe sys.path list also provides a way for scripts to tailor their search paths\nmanually at runtime. By modifying the program-wide sys.path list, you modify\nthe search path for all future imports made anywhere in a program’s run. Such\nchanges last only for the duration of the single run, however; PYTHONPATH and\n.pth files offer more permanent ways to modify the path—the first per user, and\nthe second per installed Python.\nOn the other hand, some programs really do need to change sys.path. Scripts\nthat run on web servers, for example, often run as the user “nobody” to limit\nmachine access. Because such scripts cannot usually depend on “nobody” to\nhave set PYTHONPATH in any particular way (grammatically incorrect but true),\nthey sometimes set sys.path manually to include required source directories,\nprior to running import statements. A sys.path.append, sys.path.insert, or\nother list operation will often suffice, though will endure for a single program\nrun only:\n>>> sys.path.append('/Users/me/pycode4')    # Extend search path for this run\n>>> import module                           # All imports search the new dir last\nYou can mod sys.path arbitrarily in code to influence future imports, and this\nalso works in a REPL. Bear in mind, though, that deleting folders may remove\naccess to important tools, and your changes will be discarded when the program\nor REPL ends. To make path changes span scripts and sessions, use other\ntechniques instead.",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 879,
      "chapter": null,
      "content": "Module File Selection\nBesides folders, file types also factor into imports. As noted earlier, filename\nextensions (e.g., .py) are omitted from import statements by design: Python\nchooses the first file it can find on the search path that matches the imported\nname. This need not, however, be a .py source code file or a .pyc bytecode file,\nas the next section explains.\nModule sources\nReally, imports are the point of interface to a host of external components—\nsource code, bytecode, compiled extensions, ZIP files, Java classes, and more.\nPython automatically selects any type that matches a module’s name. For\nexample, an import statement of the form import b might today load or resolve\nto any of the following:\nA source code file named b.py\nA bytecode file in __pycache__ named b.cpython-312.pyc or similar\nA bytecode file named b.pyc if no b.py source code file was located\nA directory named b, for package imports described in Chapter 24\nA compiled built-in module, coded in C and statically linked into\nPython when it is built\nA compiled extension module (e.g., b.so or b.pyd) coded in C and\ndynamically linked on import\nA source or bytecode file embedded in a ZIP file, automatically\nextracted when imported\nAn in-memory image’s module, for frozen (standalone) executables\nA Java class, in the Jython version of Python\nA .NET component, in the IronPython version of Python\nMost of the items on this list extend imports beyond simple files. To importers,\nthough, differences in the loaded file type are completely irrelevant, both when\nimporting and when fetching module attributes. Saying import b gets whatever",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 880,
      "chapter": null,
      "content": "module b is, according to your module search path, and b.attr fetches an item\nin the module, be it a Python variable or a linked-in C function. Some standard-\nlibrary modules used in this book, for example, are actually coded in C, not\nPython; because they look just like Python-coded module files, their clients\ndon’t have to care.\nSelection priorities\nGiven all the options listed in the prior section, there is a potential for conflicts.\nPython will always load the item with a matching name found in the first\n(leftmost) directory of your module search path, during the left-to-right scan of\nsys.path. But what happens if it finds multiple matching items in the same\ndirectory? In this case, Python follows a standard picking order, though this\norder is not guaranteed to stay the same over time or across implementations.\nIn general, you should not depend on which type of file Python will choose\nwithin a given directory—make your module names distinct, or configure your\nmodule search path to make your module-selection preferences explicit.\nPath Outliers: Standalones and Packages\nFinally, while all of the foregoing reflects normal module usage, some tools bend\nthe rules enough to merit a word in closing. For instance, module search paths\nare not relevant when you run standalone executables discussed in Chapter 2,\nwhich typically embed bytecode within their runnable files, and run without path\nsettings. This is a delivery option outside the scope of this book, but see\nAppendix A for another overview of options in this domain.\nIn addition, while it’s syntactically illegal to include path and extension details in\na standard import, package imports, covered in Chapter 24, allow import\nstatements to include part of the directory path leading to a file as a set of\nperiod-separated names. Even so, package imports still rely on the normal\nmodule search path to locate the leftmost directory in a package path (they are\nrelative to a directory in the search path), and cannot make use of any platform-\nspecific path syntax in the import statements (such syntax works only on the\nsearch path). As hinted earlier, packages can also use “.” syntax to restrict import\nsearches—but we’ll save this story for Chapter 24.",
      "content_length": 2229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 881,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we covered the basics of modules and explored the operation of\nimport statements. We learned that imports find the designated file on the\nmodule search path, compile it to bytecode, and execute all of its statements to\ngenerate its contents. We also learned how to configure the search path to be able\nto import from directories other than the home and standard-library directories,\nprimarily with PYTHONPATH settings.\nAs this chapter also discussed, the import operation and modules are at the heart\nof program architecture in Python. Larger programs are divided into multiple\nfiles, which are linked together at runtime by imports. Imports in turn use the\nmodule search path to locate files, and modules define attributes for external use.\nThe net effect divides a program’s logic into reusable and self-contained\nsoftware components.\nYou’ll see what this all means in terms of actual statements and code in the next\nchapter. Before we move on, though, let’s run through the usual chapter quiz.\nTest Your Knowledge: Quiz\n1. How does a module source code file become a module object?\n2. Why might you have to set your PYTHONPATH environment variable?\n3. Name the five major components of the module search path.\n4. Name four file types that Python might load in response to an import\noperation.\n5. What is a module namespace, and what does a module’s namespace\ncontain?\nTest Your Knowledge: Answers\n1. A module’s source code file automatically becomes a module object",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 882,
      "chapter": null,
      "content": "when that module is imported. Technically, the module’s source code is\nrun during the import, one statement at a time, and all the names\nassigned in the process become attributes of the generated module\nobject.\n2. You need to set PYTHONPATH only to import from directories other than\nPython’s standard library and the “home” directory—the current\ndirectory when working interactively, or the directory containing your\nprogram’s top-level file. In practice, this might be required for\nnontrivial programs that use libraries of tools.\n3. The five major components of the module search path are the top-level\nscript’s home directory (the directory containing it), all directories listed\nin the PYTHONPATH environment variable, the standard-library\ndirectories, the site-packages root directory for third-party extension\ninstalls, and all directories listed in .pth path files located in standard\nplaces. Of these, programmers can customize PYTHONPATH and .pth\nfiles.\n4. Python might load a source code (.py) file, a bytecode (.pyc) file, a C\nextension module, or a directory of the same name for package imports.\nImports may also load more exotic things such as ZIP file components,\nJava classes under the Jython version of Python, .NET components\nunder IronPython, and statically linked C modules that have no files\npresent at all. In fact, with import extensions, imports can load nearly\narbitrary items.\n5. A module namespace is a self-contained package of variables, which are\nknown as the attributes of the module object. A module’s namespace\ncontains all the names assigned by code at the top level of the module\nfile (i.e., not nested in def or class statements). A module’s global\nscope morphs into the module object’s attributes namespace. A\nmodule’s namespace may also be altered by assignments from other\nfiles that import it, though this is generally frowned upon (see\nChapter 17 for more on the downsides of cross-file changes).",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 883,
      "chapter": null,
      "content": "Chapter 23. Module Coding\nBasics\nNow that we’ve studied the larger ideas behind modules, let’s turn to some\nexamples of modules in action. Although some of the early topics in this chapter\nwill be review for linear readers who have already applied them in previous\nchapters’ demos, even simple modules can quickly lead us to further details that\nwe haven’t yet encountered in full, such as nesting, reloads, scopes, and more,\nwhich we’ll pick up here.\nIn general, Python modules are easy to create; they’re just files of Python\nprogram code created with a text editor, and require no special syntax. Because\nPython does all the work of finding and loading modules, they are also easy to\nuse; simply import a module or its names, and use the objects they reference.\nLet’s explore both sides of this fence.\nCreating Modules\nTo define a module, simply use your text editor to type Python code into a text\nfile, and save it with a .py extension; any such file is automatically considered a\nPython module. As we’ve seen, all the names assigned at the top level of the\nmodule become its attributes (names associated with the module object) and are\nexported for clients to use—they morph from variable to module object attribute\nautomatically.\nFor instance, if you type the code in Example 23-1 into a file called module1.py\nand import it, you create a module object with one attribute—the name printer,\nwhich happens to be a reference to a function object.\nExample 23-1. module1.py\ndef printer(x):           # Module attribute\n   print(x)\nAgain, this code may seem simplistic if you read the content of this book that",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 884,
      "chapter": null,
      "content": "precedes this chapter, but our goal here is to strip out the extraneous bits so we\ncan study modules in isolation.\nModule Filenames\nBefore we go on, let’s get more formal about module filenames. You can call\nmodules just about anything you like, but module filenames should end in a .py\nsuffix if you plan to import them as modules. The .py is technically optional for\ntop-level files that will be run but not imported (i.e., for scripts), but adding it in\nall cases makes your files’ types more obvious, may enable Python-specific\nfeatures in some text editors and file explorers, and allows you to import any of\nyour files in the future (recall that this is one way to run a file).\nBecause module names become variable names inside a Python program\n(without the .py), they should also follow the normal variable name rules\noutlined in Chapter 11. For instance, you can create a module file named if.py,\nbut you cannot import it because if is a reserved word—when you try to run\nimport if, you’ll get a syntax error. In fact, both the names of module files and\nthe names of directories used in package imports (discussed in the next chapter)\nmust conform to the rules for variable names presented in Chapter 11; they may,\nfor example, contain only letters, digits, and underscores. Package directories\nalso cannot contain platform-specific syntax such as spaces in their names.\nWhen a module is imported, Python maps the module name to an external\nfilename by adding a directory path from the module search path of the last\nchapter to the front, and a .py or other extension at the end. For instance, a\nmodule named M ultimately maps to an external file directory/M.extension\nthat contains the module’s code.\nOther Kinds of Modules\nAs mentioned in the preceding chapter, it is also possible to create a Python\nmodule by writing code in an external language such as C, C++, and others (e.g.,\nJava, in the Jython implementation of the language). Such modules are called\nextension modules, and they are generally used to wrap up external libraries for\nuse in Python scripts or optimize parts of a program. When imported by Python\ncode, extension modules look and feel the same as modules coded as Python",
      "content_length": 2201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 885,
      "chapter": null,
      "content": "source code files—they are accessed with import statements, and they provide\nfunctions and objects as module attributes. They’re also beyond the scope of this\nbook; see Python’s standard manuals for more details.\nUsing Modules\nOn the other side of the fence, clients can use the simple module file we just\nwrote by running an import or from statement. Both statements were introduced\nin Chapter 3, and have been used in earlier examples. They both find, compile,\nand run a module file’s code if it hasn’t yet been loaded, per the process covered\nin the prior chapter. The chief difference is that import fetches the module as a\nwhole, so you must qualify to fetch its names, whereas from fetches (really,\ncopies) specific names out of the module.\nLet’s see what this means in terms of code. All of the following examples wind\nup calling the printer function defined in module file module1.py of\nExample 23-1 but in different ways.\nThe import Statement\nIn the first example that follows, the name module1 serves two different\npurposes—it identifies an external file to be loaded, and it becomes a variable in\nthe script, which references the module object after the import. In a REPL:\n>>> import module1                         # Get module as a whole (one or more)\n>>> module1.printer('Hello world!')        # Qualify to get names\nHello world!\nThe import statement simply lists one or more names of modules to load,\nseparated by commas. Because it gives a name that refers to the whole module\nobject, we must go through the module name to fetch its attributes (e.g.,\nmodule1.printer).\nThe from Statement\nBy contrast, because from copies specific names from one file over to another\nscope, it allows us to use the copied names directly in the script without going",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 886,
      "chapter": null,
      "content": "through the module (e.g., printer):\n>>> from module1 import printer            # Copy out a variable (one or more)\n>>> printer('Hello world!')                # No need to qualify name (and can't!)\nHello world!\nThis form of from allows us to list one or more names to be copied out,\nseparated by commas. Here, it has the same effect as the prior example, but\nbecause the imported name is copied into the scope where the from statement\nappears, using that name in the script requires less typing—we can use it directly\ninstead of naming the enclosing module. In fact, we must; from doesn’t assign\nthe name of the module itself, so module1 is undefined.\nAs you’ll see in more detail later, the from statement is really just a minor\nextension to the import statement—it imports the module file as usual (running\nthe full three-step procedure of the preceding chapter), but adds an extra step that\ncopies one or more names (not objects) out of the file. The entire file is loaded,\nbut you’re given names for more direct access to its parts.\nThe from * Statement\nFinally, the next example uses a special form of from: when we use a * instead\nof specific names, we get copies of all names assigned at the top level of the\nreferenced module. Here again, we can then use the copied name printer in our\nscript without going through the module name:\n>>> from module1 import *                   # Copy out _all_ variables\n>>> printer('Hello world!')                 # Use names unqualified\nHello world!\nTechnically, both import and from statements invoke the same import operation;\nthe from * form simply adds an extra step that copies all the names in the\nmodule into the importing scope. It essentially merges one module’s namespace\ninto another, which again means less typing for us, albeit at the expense of name\nsegregation.\nNote that only a single * works in this context; you can’t use arbitrary pattern\nmatching to select a subset of names (you could in principle by looping through",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 887,
      "chapter": null,
      "content": "a module’s __dict__ discussed ahead, but it’s substantially more work). Also,\nnote the from * may not really get “all” names in the module if that module uses\nspecial tricks like _X naming or an __all__ list to hide some of its names from\nthis statement, but we’ll defer to Chapter 25 for more on such tools.\nAnd that’s it—apart from the search-path configurations of the prior chapter,\nmodules really are simple to use. To give you a better understanding of what\nreally happens when you define and use modules, though, let’s move on to look\nat some of their properties in more detail.\nNOTE\nOK, there is a special case: The * form of the from statement form described here can be used\nonly at the top level of a module file, not within a function (and generates a syntax error there),\nbecause this would make it impossible for Python to detect local variables before the function\nruns. It’s rare to see either import or from inside a function anyhow, and best practice\nrecommends listing all your imports at the top of a module file; it’s not required, but makes\nthem easier to spot, and avoids a speed penalty for re-importing modules on every call to a\nfunction. The from * has other issues enumerated ahead, and may be best limited to one per\nfile, or interactive REPLs.\nImports Happen Only Once\nOne of the most common questions people seem to ask when they start using\nmodules is, “Why won’t my imports keep working?” They often report that the\nfirst import works fine, but later imports during an interactive session (or\nprogram run) seem to have no effect. In fact, this is by design—modules are\nloaded and run on the first import or from, and only the first. Because importing\nis an expensive operation, by default Python does it just once per file, per\nprocess. Later import operations simply fetch the already loaded module object.\nInitialization code\nAs one consequence, because top-level code in a module file is usually executed\nonly once, you can use it to initialize names once, but allow their state to change.\nAs a demo, consider the file init.py in Example 23-2.\nExample 23-2. init.py\nprint('hello')",
      "content_length": 2116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 888,
      "chapter": null,
      "content": "flag = 1                   # Initialize variable – just once!\nIn this example, the print and = statements run the first time the module is\nimported, and the variable flag is initialized at import time (recall from\nChapter 17 that the REPL works like another module file here):\n$ python3\n>>> import init            # First import: loads and runs file's code\nhello\n>>> init.flag              # Assignment makes an attribute\n1\nSecond and later imports don’t rerun the module’s code; they just fetch the\nalready created module object from Python’s internal modules table. Thus, the\nvariable flag is not reinitialized:\n>>> init.flag = 2          # Change attribute in module\n>>> import init            # Just fetches already loaded module\n>>> init.flag              # Code wasn't rerun: attribute unchanged\n2\nOf course, sometimes you really want a module’s code to be rerun on a\nsubsequent import. You’ll see how to do this with Python’s reload function later\nin this chapter.\nImports Are Runtime Assignments\nJust like def, import and from are executable statements, not compile-time\ndeclarations. They may be nested in if tests, to select among module options;\nappear in function defs, to be loaded only on calls (subject to the preceding\nnote); be used in try statements, to provide defaults on errors; and so on. As an\nabstract example:\nif sometest:\n    from moduleA import name\nelse: \n    from moduleB import name\nWherever they appear, imports are not resolved or run until Python reaches them",
      "content_length": 1492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 889,
      "chapter": null,
      "content": "while executing your program. As one upshot, imported modules and names are\nnot available until their associated import or from statements run.\nChanging mutables in modules\nAlso like def, import and from are implicit assignments:\nimport assigns an entire module object to a single name.\nfrom assigns one or more names to objects of the same names in\nanother module.\nHence, everything you’ve already learned about assignment applies to module\naccess, too. For instance, names copied with a from become references to shared\nobjects; much like function arguments, reassigning a copied name has no effect\non the module from which it was copied, but changing a shared mutable object\nthrough a copied name can also change it in the module from which it was\nimported. Consider the following file, share.py, in Example 23-3.\nExample 23-3. share.py\nx = 1\ny = [1, 2]\nWhen importing with from, we copy names to the importer’s scope that initially\nshare objects referenced by the module’s names (again, the REPL serves as the\nimporting module here):\n$ python3\n>>> from share import x, y         # Copy two names out\n>>> x = 'hack'                     # Changes local x only\n>>> y[0] = 'hack'                  # Changes shared mutable in place\n>>> x, y                           # This module's x and y\n('hack', ['hack', 2])\nHere, x is not a shared mutable object, but y is. The names y in the importer and\nthe imported modules both reference the same list object, so changing it from\none place changes it in the other; continuing the REPL session:\n>>> import share                   # Get module name (from doesn't)\n>>> share.x                        # share's x is not my x\n1",
      "content_length": 1664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 890,
      "chapter": null,
      "content": ">>> share.y                        # But we share a changed mutable\n['hack', 2]\nFor more background on this, see Chapter 6. And for a graphical picture of what\nfrom assignments do with references, flip back to Figure 18-1 (function\nargument passing), and mentally replace “caller” and “function” with “imported”\nand “importer.” The effect is the same, except that here we’re dealing with\nnames in modules, not functions. Assignment works the same everywhere in\nPython.\nCross-file name changes\nIn the preceding example, the assignment to x in the interactive session changed\nthe name x in that scope only, not the x in the file—there is no link from a name\ncopied with from back to the file it came from. To really change a global name\nin another file, you must use import:\n$ python3\n>>> from share import x, y         # Copy two names out\n>>> x = 23                         # Changes my x only\n>>> import share                   # Get module name\n>>> share.x = 23                   # Changes x in other module\nThis phenomenon was introduced in Chapter 17. Because changing variables in\nother modules like this is a common source of confusion (and often a subpar\ndesign choice), we’ll revisit this technique again later in this part of the book.\nSubtly, the change to y[0] in the prior session is different; it changes an object,\nnot a name, and the name in both modules references the same, changed object.\nThis can be similarly subpar unless all module clients expect it.\nimport and from Equivalence\nNotice in the prior example that we have to execute an import statement after\nthe from to access the share module name at all. from only copies names from\none module to another; it does not assign the module name itself. It may help to\nremember that, at least conceptually, a from statement like this one:",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 891,
      "chapter": null,
      "content": "from module import name1, name2     # Copy these two names out (only)\nis equivalent to this statement sequence:\nimport module                       # Fetch the module object\nname1 = module.name1                # Copy names out by assignment\nname2 = module.name2\ndel module                          # Get rid of the module name – here only\nLike all assignments, the from statement creates new variables in the importer,\nwhich initially refer to objects of the same names in the imported file. Only the\nnames are copied out, though, not the objects they reference, and not the name of\nthe module itself. When we use the from * form of this statement (from module\nimport *), the equivalence is the same, but all the top-level names in the\nmodule are copied over to the importing scope this way.\nImportantly, the first step of the from runs a normal import operation, with all\nthe semantics outlined in the preceding chapter. As a result, the from always\nimports the entire module into memory if it has not yet been imported, regardless\nof how many names it copies out of the file. There is no way to load just part of\na module file (e.g., just one function), but because modules are bytecode in\nstandard Python instead of machine code, the performance implications are\ngenerally negligible.\nPotential Pitfalls of the from Statement\nOne downside of the from statement is that it makes the meaning of a variable\nmore obscure: name is less useful to the reader than module.name, and may\nrequire a search for the from that loaded it. Because of this, some Python users\nrecommend coding import instead of from most of the time. Like most advice,\nthough, this doesn’t always make sense. from is commonly and widely used,\nwithout dire consequences. Moreover, it’s convenient not to have to type a\nmodule’s name every time you wish to use one of its tools, especially when\nworking in the REPL.\nIt is true that the from statement has the potential to corrupt namespaces, at least\nin principle—if you use it to import variables that happen to have the same\nnames as existing variables in your scope, your variables will be silently",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 892,
      "chapter": null,
      "content": "overwritten. This problem doesn’t occur with the import statement because you\nmust always go through a module’s name to get to its contents: module.attr\nwill not clash with a variable named attr in your scope. As long as you\nunderstand that this can happen when using from, though, this isn’t a concern in\npractice: assigning a variable with from has the same effect as any other\nassignment in your code.\nOn the other hand, the from statement has legitimate issues when used in\nconjunction with the reload call, as imported names might reference prior\nversions of objects. Moreover, the from * form really can trash namespaces and\nmake code difficult to understand, especially when applied to more than one file\n—in this case, there is no way to tell which module a name came from, short of\nsearching external files. In effect, the from * form collapses one namespace into\nanother, and so defeats the namespace partitioning purpose of modules. We’ll\nsave demos of these issues for “Module Gotchas” (at the end of this part of the\nbook), and meet tools that can minimize the from * damage with data hiding in\nChapter 25.\nProbably the best real-world advice here is to generally prefer import to from\nfor simple modules; to explicitly list the variables you want in most from\nstatements; and to limit the from * form to just one per file, or the REPL’s\nthrow-away code. That way, any names not called out by attribute qualification\nor from lists can be assumed to live in the sole module imported by the from *.\nSome care is required when using the from statement, but armed with a little\nknowledge, most programmers find it to be a convenient way to access modules.\nWhen import is required\nThe only time you really must use import instead of from is when you must use\nthe same name defined in two different modules. For example, imagine that two\nfiles define the same name differently, as in this abstract snippet:\n# M.py\ndef func():\n    …do something…\n# N.py\ndef func():",
      "content_length": 1970,
      "extraction_method": "Direct"
    },
    {
      "page_number": 893,
      "chapter": null,
      "content": "…do something else…\nIf you must use both versions of this name in your program, the from statement\nwill fail—you can have only one assignment to the name in your scope:\n# O.py\nfrom M import func\nfrom N import func             # This overwrites the one we fetched from M\nfunc()                         # Calls N.func only!\nAn import will work here, though, because including the name of the enclosing\nmodule makes the two names unique:\n# O.py\nimport M, N                    # Get the whole modules, not their names\nM.func()                       # We can call both names now\nN.func()                       # The module names make them unique\nThis case is unusual enough that you’re unlikely to encounter it very often in\npractice. If you do, though, import allows you to avoid the name collision.\nAnother way out of this dilemma is using the as extension, a renaming tool that\nwe’ll cover fully in Chapter 25 but is simple enough to introduce here:\n# O.py\nfrom M import func as mfunc    # Rename uniquely with \"as\"\nfrom N import func as nfunc\nmfunc(); nfunc()               # Call one or the other\nThe as extension works in both import and from as a simple renaming tool (it\ncan also be used to give a shorter synonym for a long module name in import);\nmore on this form in Chapters 24 and 25.\nModule Namespaces\nModules are probably best understood as packages of names—i.e., places to\ndefine names you want to make visible to the rest of a system. Technically,\nmodules usually correspond to files, and Python creates a module object to\ncontain all the names assigned in a module file. But in simple terms, modules are",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 894,
      "chapter": null,
      "content": "just namespaces (places where names are created), and the names that live in a\nmodule are its attributes. This section expands on the details behind this model.\nHow Files Generate Namespaces\nWe’ve seen that files morph into namespaces, but how does this actually\nhappen? The short answer is that every name that is assigned a value at the top\nlevel of a module file (i.e., not nested in a function or class body) becomes an\nattribute of that module.\nFor instance, given an assignment statement such as X = 1 at the top level of a\nmodule file M.py, the name X becomes an attribute of M, which we can refer to\nfrom outside the module as M.X. The name X also becomes a global variable to\nother code inside M.py, but we need to firm up the relationship of module\nloading and scopes to understand why:\nModule statements run on the first import. The first time a module is\nimported anywhere in a system, Python creates an empty module object\nand executes the statements in the module file one after another, from\nthe top of the file to the bottom.\nTop-level assignments create module attributes. During an import,\nstatements at the top level of the file not nested in a def or class that\nassign names (e.g., =, def) create attributes of the module object.\nNames assigned by these statements are stored in the module’s\nnamespace.\nModule namespaces are dictionaries. Module namespaces created by\nimports may be accessed through the built-in __dict__ dictionary\nattribute of module objects, and may be inspected with the dir function.\ndir is the same as the sorted keys of __dict__ for modules, but\nincludes inherited names for classes.\nModules are a single scope (local is global). As we saw in Chapter 17,\nnames at the top level of a module follow the same scope rules as names\nin a function, but the local and global scopes are the same—or, more\nformally, they follow the LEGB scope rule presented in Chapter 17, but\nwithout the L and E lookup layers.",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 895,
      "chapter": null,
      "content": "Module scopes outlive imports. A module’s global scope becomes an\nattribute dictionary of a module object after the module has been\nloaded. Unlike function scopes, where the local namespace exists only\nwhile a function call runs, a module file’s scope lives on after the\nimport, providing a source of tools to importers.\nHere’s a demonstration of these ideas. Suppose we create the module file in\nExample 23-4 with a text editor, and call it spaces.py.\nExample 23-4. spaces.py\nprint('starting to load...')\nimport sys\nvar = 23\ndef func(): pass\nclass klass: pass\nprint('done loading.')\nThe first time this module is imported (or run as a program), Python executes its\nstatements from top to bottom. Some statements create names in the module’s\nnamespace as a side effect, but others do actual work while the import is going\non. For instance, the two print statements in this file execute at import time:\n>>> import spaces\nstarting to load...\ndone loading.\nOnce the module is loaded, its scope becomes an attribute namespace in the\nmodule object we get back from import. We can then access attributes in this\nnamespace by qualifying them with the name of the enclosing module:\n>>> spaces.sys\n<module 'sys' (built-in)>\n>>> spaces.var\n23\n>>> spaces.func\n<function func at 0x101ce7b00>\n>>> spaces.klass\n<class 'spaces.klass'>",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 896,
      "chapter": null,
      "content": "Here, sys, var, func, and klass were all assigned while the module’s\nstatements were being run, so they are attributes after the import. We’ll study\nclasses in Part VI, but notice the sys attribute—import statements assign\nmodule objects to names, and any type of assignment to a name at the top level\nof a file generates a module attribute.\nNamespace Dictionaries: __dict__\nIn fact, internally, module namespaces are stored as dictionary objects. These are\njust normal dictionaries with all the usual methods. When needed, we can access\na module’s namespace dictionary through the module’s __dict__ attribute—for\ninstance, to write tools that list module content generically, as we will in\nChapter 25. Continuing the prior section’s example:\n>>> spaces.__dict__.keys()\ndict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', \n'__file__', '__cached__', '__builtins__', 'sys', 'var', 'func', 'klass'])\nThe names we assigned in the module file become dictionary keys internally, so\nsome of the keys here reflect top-level assignments in our file. The value of var,\nfor example, can be had two ways (though the first is normal):\n>>> spaces.var, spaces.__dict__['var']\n(23, 23)\nPython also adds some useful names in the module’s namespace. For instance,\n__file__ gives the name of the file from which the module was loaded (useful\nto find resources bundled with code), and __name__ gives its name as known to\nimporters (without the .py extension and directory path):\n>>> spaces.__name__, spaces.__file__\n('spaces', '/Users/me/…/LP6E/Chapter23/spaces.py')\nTo see just the names your code assigns, filter out double-underscore names as\nwe did in Chapter 15’s dir coverage and Chapter 17’s built-in scope coverage,\nbut this time with a generator expression. We can also omit keys because\ndictionaries generate keys automatically, and the dir built-in for modules is just",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 897,
      "chapter": null,
      "content": "the sorted keys list of __dict__:\n>>> list(name for name in spaces.__dict__ if not name.startswith('__'))\n['sys', 'var', 'func', 'klass']\n>>> dir(spaces) == sorted(spaces.__dict__)\nTrue\nAs another lesser-used alternative, Python’s vars built-in function simply\nfetches the __dict__ of a module passed to it (an option, perhaps, after you’ve\nwritten enough Python code to wear out your keyboard’s underscore key?):\n>>> vars(spaces) == spaces.__dict__\nTrue\n>>> dir(spaces) == sorted(vars(spaces))\nTrue\nSee Chapter 7 for other vars roles. You’ll see similar __dict__ dictionaries on\nclass-related objects in Part VI too. In all cases, attribute fetch is similar to\ndictionary indexing, though only the former kicks off inheritance in classes.\nAttribute Name Qualification\nSpeaking of attribute fetch, now that you’re becoming more familiar with\nmodules, we should firm up the notion of name qualification more formally too.\nIn Python, you can access the attributes of any object that has attributes using the\nqualification (a.k.a. attribute fetch) syntax object.attribute.\nQualification is really an expression that returns the value assigned to an\nattribute name associated with an object. For example, the expression\nspaces.sys in the previous example fetches the value assigned to sys in\nspaces. Similarly, if we have a built-in list object L, L.append returns the\nappend method object associated with that list.\nIt’s important to keep in mind that attribute qualification has nothing to do with\nthe scope rules we studied in Chapter 17; it’s an independent concept. When you\nuse qualification to access names, you give Python an explicit object from which\nto fetch the specified names. The LEGB scope rule applies only to bare,\nunqualified names; it may be used for the leftmost name in a qualification path,\nbut later names after dots search specific objects instead.",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 898,
      "chapter": null,
      "content": "This distinction may seem blurred by the fact that module scopes morph into\nattributes at the end of an import, but thereafter, names are part of a module\nobject. For reference, here are the full rules for name resolution in Python:\nSimple variables\nX means search for the name X in the current scopes (following the LEGB\nrule of Chapter 17).\nQualification\nX.Y means find X in the current scopes, then search for the attribute Y in the\nobject X (not in scopes).\nQualification paths\nX.Y.Z means look up the name Y in the object X, then look up Z in the object\nX.Y.\nGenerality\nQualification works on all objects with attributes: modules, classes, C\nextension types, etc.\nAs noted earlier, in Part VI you’ll see that attribute qualification means a bit\nmore for classes—it’s also the place where inheritance happens. In general,\nthough, the rules outlined here apply to all names in Python.\nImports Versus Scopes\nAs we’ve seen, it is never possible to access names defined in another module\nfile without first importing that file. That is, you never automatically get to see\nnames in another file, regardless of the structure of imports or function calls in\nyour program. A variable’s meaning is always determined by the locations of\nassignments in your source code, and attributes are always requested of an object\nexplicitly.",
      "content_length": 1324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 899,
      "chapter": null,
      "content": "For example, consider the following two simple modules. The first, lex1.py in\nExample 23-5, defines a variable X global to code in its file only, along with a\nfunction that changes the global X in this file.\nExample 23-5. lex1.py\nX = 88                        # My X: global to this file only\ndef f():\n   global X                  # Change this file's X\n   X = 99                    # Cannot see names in other modules\nThe second module, lex2.py in Example 23-6, defines its own global variable X\nand imports and calls the function in the first module.\nExample 23-6. lex2.py\nX = 11                        # My X: global to this file only\nimport lex1                   # Gain access to names in lex1\nlex1.f()                      # Sets lex1.X, not this file's X\nprint(X, lex1.X)\nWhen run, lex1.f changes the X in lex1, not the X in lex2. The global scope for\nlex1.f is always the file enclosing it, regardless of which module it is ultimately\ncalled from:\n$ python3 lex2.py\n11 99\nIn other words, import operations never give upward visibility to code in\nimported files—an imported file cannot see names in the importing file. More\nformally:\nFunctions can never see names in other functions unless they are\nphysically enclosing.\nModule code can never see names in other modules unless they are\nexplicitly imported.\nSuch behavior is part of the lexical scoping notion—in Python, the scopes\nsurrounding a piece of code are completely determined by the code’s physical\nposition in your file. Scopes are never influenced by function calls or module",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 900,
      "chapter": null,
      "content": "imports. Some languages act differently and provide for dynamic scoping, where\nscopes really may depend on runtime calls. This tends to make code trickier,\nthough, because the meaning of a variable can differ over time. In Python,\nscopes more simply correspond to the text of your program.\nNamespace Nesting\nFinally, although imports do not nest namespaces upward, they do in some sense\nnest downward. That is, although an imported module never has direct access to\nnames in a file that imports it, using attribute qualification paths it is possible to\ndescend into arbitrarily nested modules and access their attributes. For example,\nconsider the next three files. First, nest3.py of Example 23-7 defines a single\nglobal name and attribute by assignment.\nExample 23-7. nest3.py\nX = 3\nNext, nest2.py of Example 23-8 defines its own X, then imports nest3 and uses\nname qualification to access the imported module’s attribute.\nExample 23-8. nest2.py\nX = 2\nimport nest3\nprint(X, end=' ')             # My global X\nprint(nest3.X)                # nest3's X\nAnd at the top, nest1.py of Example 23-9 also defines its own X, then imports\nnest2, and fetches attributes in both the first and second files.\nExample 23-9. nest1.py\nX = 1\nimport nest2\nprint(X, end=' ')             # My global X\nprint(nest2.X, end=' ')       # nest2's X\nprint(nest2.nest3.X)          # Nested nest3's X\nReally, when nest1 imports nest2 here, it sets up a two-level namespace\nnesting. By using the path of names nest2.nest3.X, it can descend into nest3,\nwhich is nested in the imported nest2. The net effect is that nest1 can see the Xs",
      "content_length": 1606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 901,
      "chapter": null,
      "content": "in all three files, and hence has access to all three global scopes:\n$ python3 nest1.py\n2 3\n1 2 3\nThe reverse, however, is not true: nest3 cannot see names in nest2, and nest2\ncannot see names in nest1. This example may be easier to grasp if you don’t\nthink in terms of namespaces and scopes, but instead focus on the objects\ninvolved. Within nest1, nest2 is just a name that refers to an object with\nattributes, some of which may refer to other objects with attributes (import is an\nassignment). For paths like nest2.nest3.X, Python simply evaluates from left\nto right, fetching attributes from objects along the way.\nNote that nest1 can say import nest2, and then nest2.nest3.X, but it cannot\nsay import nest2.nest3—this syntax invokes something called package\n(directory) imports, the topic we’ll take up in the next chapter after we study\nreloads here. Package imports also create module namespace nesting, but their\nimport statements are taken to reflect directory trees, not simple file import\nchains.\nReloading Modules\nAs we’ve seen, a module’s code is run only once per process by default. To force\na module’s code to be reloaded and rerun, you need to ask Python to do so\nexplicitly by calling the reload built-in function, introduced briefly in\nChapter 3. In this section, we’ll explore how to use reloads to make your systems\nmore dynamic. In a nutshell:\nImports—via both import and from statements—load and run a\nmodule’s code only the first time the module is imported in a process.\nLater imports use the already loaded module object without reloading or\nrerunning the file’s code. This is true even if you resave the module’s\nsource code file during the program run.\nThe reload function forces an already loaded module’s code to be",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 902,
      "chapter": null,
      "content": "reloaded and rerun. Assignments in the file’s new code change the\nexisting module object in place.\nSo why care about reloading modules? In short, REPL testing and customization.\nWhen testing code interactively, it may be easier to reload a module you’ve\nchanged in another window than it is to restart the REPL.\nThe more grandiose rationale is dynamic customization: the reload function\nallows parts of a program to be changed without stopping the whole program.\nWith reload, the effects of changes in components can be observed\nimmediately. Reloading doesn’t help in every situation, but where it does, it\nmakes for a much shorter development cycle. For instance, imagine a database\nprogram that must connect to a server on startup; because program changes or\ncustomizations can be tested immediately after reloads, you need to connect only\nonce while debugging. Long-running servers can update themselves this way,\ntoo.\nBecause Python is interpreted (more or less), it already gets rid of the\ncompile/link steps you need to go through to get a C program to run: modules\nare loaded dynamically when imported by a running program. Reloading offers a\nfurther performance advantage by allowing you to also change parts of running\nprograms without stopping.\nOne note here: our use of reload in this book is limited to modules written in\nPython. Compiled extension modules coded in a language such as C can be\ndynamically loaded by imports at runtime, too, but they are out of scope here\n(though most users probably prefer to code customizations in Python anyhow!).\nreload Basics\nUnlike import and from:\nreload is a function in Python, not a statement.\nreload is passed an existing module object, not a string name.\nreload lives in a standard-library module and must be imported itself.\nBecause reload expects an object, a module must have been previously",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 903,
      "chapter": null,
      "content": "imported successfully before you can reload it (and if the import was\nunsuccessful due to a syntax or other error, you may need to repeat it before you\ncan reload the module). Furthermore, the syntax of import statements and\nreload calls differs: as a function, reloads require parentheses, but import\nstatements do not. Abstractly, reloading looks like this:\nimport module                     # Initial module import\n…use module.attributes… \n…change the module file…\nfrom importlib import reload      # Get reload itself\nreload(module)                    # Get updated module\n…use module.attributes…\nThe typical usage pattern is that you import a module, then change its source\ncode in a text editor, and then reload it. This can occur when working\ninteractively, but also in larger programs that reload periodically.\nWhen you call reload, Python rereads the module’s code and reruns its top-\nlevel statements. Perhaps the most important thing to know about reload is that\nit changes a module object in place; it does not delete and re-create the module\nobject. Because of that, every reference to an entire module object anywhere in\nyour program is automatically affected by a reload. Here are the details:\nreload runs a module file’s new code in the module’s current\nnamespace. Rerunning a module file’s code overwrites its existing\nnamespace, rather than deleting and re-creating it.\nTop-level assignments in the file replace names with new values. For\ninstance, rerunning a def statement replaces the prior version of the\nfunction in the module’s namespace by reassigning the function name.\nReloads impact all clients that use import to fetch modules. Because\nclients that use import qualify to fetch attributes, they’ll find new\nvalues in the module object after a reload.\nReloads impact future from clients only. Clients that used from to\nfetch attributes in the past won’t be affected by a reload; they’ll still\nhave references to the old objects fetched before the reload.",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 904,
      "chapter": null,
      "content": "Reloads apply to a single module only. You must run them on each\nmodule you wish to update unless you use code or tools that apply\nreloads transitively.\nreload Example\nTo demonstrate, here’s a more concrete example of reload in action. In the\nfollowing, we’ll change and reload a module file without stopping the interactive\nPython session. Reloads are useful in other scenarios, too, but we’ll keep things\nsimple for illustration here. First, in the text editor of your choice, write a\nmodule file named changer.py with the contents given in Example 23-10.\nExample 23-10. changer.py (start)\nmessage = 'First version'\ndef printer():\n   print(message)\nThis module creates and exports two names—one bound to a string, and another\nto a function. Now, start the Python interpreter (i.e., your local REPL), import\nthe module, and call the function it exports. The function will print the value of\nthe global message variable as you probably expect:\n$ python3\n>>> import changer\n>>> changer.printer()\nFirst version\nKeeping the interpreter active, now edit and save the module file in another\nwindow. Change the global message variable, as well as the printer function\nbody:\nmessage = 'After editing'\ndef printer():\n    print('reloaded:', message)\nThen, return to the Python window and reload the module to fetch the new code.\nNotice in the following interaction that importing the module again has no\neffect; we get the original message, even though the file’s been changed. We\nhave to call reload in order to get the new version:",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 905,
      "chapter": null,
      "content": ">>> import changer\n>>> changer.printer()                 # No effect: uses loaded module\nFirst version\n>>> from importlib import reload\n>>> reload(changer)                   # Forces new code to load/run\n<module 'changer' from '/…/LP6E/Chapter23/changer.py'>\n>>> changer.printer()                 # Runs the new version now\nreloaded: After editing\nNotice that reload actually returns the module object for us—its result is\nusually ignored, but because expression results are printed at the interactive\nprompt, Python shows a default <module …> representation.\nYou can also reload a module by string name using the sys.modules dictionary\ndemoed in the prior chapter if you don’t have a variable assigned to it by\nimport:\n>>> import sys\n>>> reload(sys.modules['changer'])\nIn this case, it’s probably easier to run import changer to get a handle on the\nmodule, but the sys.modules scheme may be useful in programs that need to\nreload modules more generically. It’s also possible to delete modules from\nsys.modules to force a reload, but this is generally discouraged for reasons\nwe’ll skip here; use reload.\nreload Odds and Ends\nFinally, four brief module-reload notes in closing:\nIf you use reload, you’ll probably want to pair it with import instead\nof from, as names fetched with the latter are not updated by reload\noperations—leaving your names in a state that’s strange enough to\nwarrant postponing elaboration until this part’s “gotchas” at the end of\nChapter 25.\nBy itself, reload updates only a single module, but it’s straightforward\nto code a function that applies it transitively to related modules—an",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 906,
      "chapter": null,
      "content": "extension we’ll save for a case study near the end of Chapter 25.\nSome development tools (e.g. Jupyter notebooks) have REPLs that offer\nan auto-reload mode that may obviate manual reload calls. This works\nonly in specific tools, though, and doesn’t address customization roles.\nAnd last, reload has had a long and checkered past—morphing from\nbuilt-in function, to imp module attribute in this book’s prior edition, to\nits current importlib host. While this may be a symptom of Python’s\nconstant morph, which is apt to relocate reload again, it must be asked:\ndoes this function have trouble working with others?!",
      "content_length": 613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 907,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter drilled down into the essentials of module coding tools—the\nimport and from statements, and the reload call. It showed how the from\nstatement simply adds an extra step that copies names out of a file after it has\nbeen imported, and how reload forces a file to be imported again without\nstopping and restarting Python. This chapter also detailed what happens when\nimports are nested, explored the way files become module namespaces, and\ncovered some potential pitfalls of the from statement.\nAlthough you’ve already learned enough to handle module files in most\nprograms, the next chapter extends the coverage of the import model by\npresenting package imports—a way for import statements to specify part of the\ndirectory path leading to the desired module. As you’ll find, package imports\ngive us a hierarchy that is useful in larger systems and allow us to break conflicts\nbetween same-named modules. Before we move on, though, here’s a quick quiz\non the concepts presented here.\nTest Your Knowledge: Quiz\n1. How do you make a module?\n2. How is the from statement related to the import statement?\n3. How is the reload function related to imports?\n4. When must you use import instead of from?\n5. Name three potential pitfalls of the from statement.\nTest Your Knowledge: Answers\n1. To create a module, you simply write a text file containing Python\nstatements; every source code file is automatically a module, and there\nis no syntax for declaring one. Import operations load module files into",
      "content_length": 1520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 908,
      "chapter": null,
      "content": "module objects in memory. You can also make a module by writing\ncode in an external language like C or Java, but such extension modules\nare beyond the scope of this book (and most of its readers).\n2. The from statement imports an entire module, like the import\nstatement, but as an extra step, it also copies one or more variables from\nthe imported module into the scope where the from appears. This\nenables you to use the imported names directly (name) instead of having\nto go through the module (module.name).\n3. By default, a module is imported only once per process. The reload\nfunction forces a module to be imported again. It is mostly used to pick\nup new versions of a module’s source code during development, and in\ndynamic customization scenarios where users change part of a system\nwithout restarting it.\n4. You must use import instead of from only when you need to access the\nsame name in two different modules. To make the two names unique,\nqualify with the names of their enclosing modules obtained with\nimport. The as extension can render from usable in this context as\nwell, by renaming imports uniquely.\n5. The from statement can obscure the meaning of a variable (which\nmodule it is defined in), can have problems with the reload call (names\nmay reference prior versions of objects), and can corrupt namespaces (it\nmight silently overwrite names you are using in your scope). The from\n* form is worse in most regards—it can corrupt namespaces arbitrarily\nand obscure the meaning of variables, so it is probably best used\nsparingly.",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 909,
      "chapter": null,
      "content": "Chapter 24. Module Packages\nSo far, when we’ve imported modules, we’ve been loading files. This represents\ntypical module usage, and it’s probably the technique you’ll use for most imports\nyou’ll code, especially early in your Python career. The module import story,\nthough, is richer than implied up to this point. This chapter extends it to present\nmodule packages—collections of module files that normally correspond to\nfolders (a.k.a. directories) on your device. It covers four topics:\nPackage imports, which give part of a folder path leading to a file\nPackages themselves, which organize modules into folder bundles\nPackage-relative imports, which use dots within a package to limit\nsearch\nNamespace packages, which build a package that may span multiple\nfolders\nAs you’ll find, a package import turns a folder on your computer into another\nPython namespace, with attributes corresponding to the module files and\nsubfolders that the folder contains. As you’ll also learn, package imports are\nsometimes required to resolve ambiguity when multiple program files of the\nsame name are installed on a device.\nPackages are a somewhat advanced topic, which many readers can defer until\nthey gain experience with file-based modules. That said, packages provide an\neasy way to organize code files that avoids same-name conflicts and is employed\nby many standard-library and third-party tools that you’ll be using. While\npackages, like much in Python, have grown convoluted over time, a basic\nknowledge of their usage can be beneficial to most Python learners.\nUsing Packages\nTo load items in a package folder, you’ll use the normal import statements and\ntools we’ve already met, but provide a path of names that reflects a path of",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 910,
      "chapter": null,
      "content": "nested folders. Let’s get started with this user-guide side of the packages story.\nPackage Imports\nCoding package imports is straightforward. In all the places where you have\nbeen naming a simple file in your import operations, you can instead list a path\nof names separated by periods. For instance, this works in an import statement:\nimport dir1.dir2.mod\nThe same goes for from statements:\nfrom dir1.dir2.mod import var\nAnd this also applies to already-imported items in reload calls:\nfrom importlib import reload\nreload(dir1.dir2.mod)\nThe “dotted path” in these contexts is assumed to correspond to a path through\nthe folder hierarchy on your device, leading to the module file mod.py, or other\ncomponent with basename mod (as we’ve seen, it might also be a bytecode file,\nC extension module, or other). Most importantly here, the preceding paths\nindicate that on your device there is a directory dir1, which has a subdirectory\ndir2, which contains a module file mod.py (or similar).\nFurthermore, these paths imply that dir1 resides within some container directory\ndir0, which is a part of the normal module search path. In other words, these\nimports and reloads imply a directory structure that looks something like this on\nUnix and Windows, respectively:\ndir0/dir1/dir2/mod.py            # Or mod.pyc, mod.so, etc.\ndir0\\dir1\\dir2\\mod.py            # Ditto, on Windows\nThe container directory dir0 needs to be added to your module search path\nunless it’s an automatic part of that path, exactly as if dir1 were a simple module\nfile.",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 911,
      "chapter": null,
      "content": "More formally, the leftmost component in a package import path is relative to\n(located in) a directory included in the sys.path module search-path list we\nexplored in Chapter 22. From there down, though, the import statements in your\nscript explicitly give the directory paths leading to modules in packages.\nThe dotted-path syntax of packages is platform neutral, but also reflects the fact\nthat folder paths in import statements become nested objects: dir1.dir2.mod\ntraverses three module objects after an import. This syntax also explains why\nPython complains about a file not being a package if you forget to omit the .py\nextension in import statements: it’s taken to mean a package import!\nPackages and the Module Search Path\nIf you use this feature, keep in mind that the directory paths in your import\nstatements can be only variables separated by periods. You cannot use any\nplatform-specific path syntax in your import statements, such as C:\\dir1,\n/Users/me/dir1, or ../dir1—these do not work syntactically. Instead, use any\nsuch platform-specific syntax in your module search path settings to name the\ndirectories containing your packages.\nFor instance, in the prior example, dir0—the directory name you add to your\nmodule search path—can be an arbitrarily long and platform-specific directory\npath leading up to dir1. You cannot use an invalid statement like this:\nimport C:\\Users\\me\\mycode\\dir1\\dir2\\mod      # Error: illegal syntax (Windows)\nimport /Users/me/mycode/dir1/dir2/mod        # Ditto, on Unix\nBut you can add a path like C:\\Users\\me\\mycode or /Users/me/mycode to either\nyour PYTHONPATH environment variable, a strategically placed .pth file, or\nsys.path itself in manual code, and then say this in your script:\nimport dir1.dir2.mod                         # OK: variables and periods\nIn effect, entries on the module search path provide platform-specific directory\nprefixes, which lead to the leftmost names of package paths in import and from\nstatements. These import statements themselves provide the remainder of the\ndirectory path in a platform-neutral fashion.",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 912,
      "chapter": null,
      "content": "As for simple file imports, you do not need to add the container directory to your\nmodule search path if it’s already there. Per Chapter 22, the search path\nautomatically includes the “home” directory (where you’re working in a REPL,\nor the container of a launched program’s top-level file), along with the standard\nlibrary’s containers, and the site-packages third-party install root. While none of\nthese components require search-path mods, your module search path must\ninclude all the directories containing leftmost components in your code’s\npackage-import statements.\nCreating Packages\nTo make packages of your own, you’ll bundle module files and nested folders in\na package folder. A package folder can also optionally contain an __init__.py file\nrun on first import, and a __main__.py file run when the entire package folder is\nrun as a bundle. The following sections demo what this looks like in code, one\nstep at a time.\nBasic Package Structure\nIn their simplest form, packages are just folders containing normal module files,\nand perhaps nested subfolders of the same. Let’s set one up as a demo. The\nfollowing uses indentation to represent the folder nesting we’ll be using in this\nand the following sections:\ndir0/                        # Container listed on module search path\n    dir1/                    # Package root folder dir1\n        mod.py               # Module file dir1.mod in package\n        dir2/                # Nested package folder dir1.dir2\n            mod.py           # Module dir1.dir2.mod in package\nThese nested folders can be created in file explorers, or with the following\ncommands on most platforms; use backslashes on Windows, or simply use this\nChapter’s folder in the examples package, which has prebuilt the structure:\n$ mkdir dir1\n$ mkdir dir1/dir2            # Use backward slashes on Windows",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 913,
      "chapter": null,
      "content": "Admin note: the names of package folders, like those of simple module files,\nmust follow the rules for variable names because they become variables where\nimported. See Chapter 11 for a refresher on the constraints, but in short, use\nletters, digits, and underscores, and avoid reserved words.\nNow, add the nested module files listed in Examples 24-1 and 24-2. Their top-\nlevel code runs on first import and creates attributes as usual, and their titles give\ntheir paths (using just Unix forward-slash separators for brevity).\nExample 24-1. dir1/mod.py\nvar = 'hack'\nprint('Loading dir1.mod')\nExample 24-2. dir1/dir2/mod.py\nvar = 'code'\nprint('Loading dir1.dir2.mod')\nUsing the basic package\nTo use our module package, import its modules from a REPL or another file just\nas you would for a simple .py file, but use dots to specify the path below the dir0\nfolder in the search path—which is the current directory in a REPL:\n$ python3\n>>> import dir1.mod                      # Package paths in import\nLoading dir1.mod\n>>> dir1.mod.var                         # Module code run on import\n'hack'\n>>> import dir1.dir2.mod                 # A further-nested path\nLoading dir1.dir2.mod\n>>> dir1.dir2.mod.var                    # Repeat path to get item at end\n'code'\nAs for simple top-level modules, the code of modules nested in a package is run\nonly when first imported:\n>>> import dir1.mod                      # No-op if already imported\n>>> import dir1.dir2.mod\nAnd you generally must import a nested module in order to use its attributes—\nbecause modules corresponding to folders don’t go back to the filesystem on",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 914,
      "chapter": null,
      "content": "attribute fetches, it’s not enough to import just a root folder:\n$ python3\n>>> import dir1                          # Gets dir1, nothing below it\n>>> dir1.dir2.mod.var\nAttributeError: module 'dir1' has no attribute 'dir2'\n$ python3\n>>> import dir1.dir2\n>>> dir1.dir2.mod.var\nAttributeError: module 'dir1.dir2' has no attribute 'mod'\n$ python3\n>>> import dir1.dir2.mod                 # List full path to target\nLoading dir1.dir2.mod\n>>> dir1.dir2.mod.var\n'code'\nAll this works the same in the from statement—which, as we’ve seen, is really\njust import with an extra copy of names. Again, though, we must list a folder by\nits path in an import statement to be able to access its contents, and the paths\nlisted in imports are taken literally, not fetched from variables:\n$ python3\n>>> from dir1.mod import var             # Package paths in from\nLoading dir1.mod\n>>> var                                  # Don't repeat path to item\n'hack'\n>>> from dir1.dir2.mod import var\nLoading dir1.dir2.mod\n>>> var\n'code'\n>>> from dir1 import dir2                # Paths taken literally\n>>> from dir2 import mod                 # Not from variable values\nModuleNotFoundError: No module named 'dir2'\nOne potential advantage of from here is that it avoids repeating a package path\nevery time an item in a package is used. With import, you generally must repeat\nthe full path each time, but from lets you code the path just once, and use the\nsimple name fetched from it everywhere; this is especially useful if the\npackage’s directory structure later changes (as software is wont to do):",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 915,
      "chapter": null,
      "content": ">>> import dir1.dir2.mod                 # import requires paths\n>>> dir1.dir2.mod.var\n'code'\n>>> from dir1.dir2.mod import var        # from can shorten paths\n>>> var\n'code'\n>>> from dir1.dir2 import mod            # And avoids repeating them\n>>> mod.var\n'code'\n>>> import dir1.dir2.mod as mod          # Though \"as\" can too\n>>> mod.var\n'code'\nAs the last example shows, though, the as extension introduced in the preceding\nchapter can shorten up paths the same way as from—as can a simple manual\nreassignment, which the as sugarcoats (more on as in the next chapter).\nPackage __init__.py Files\nIf you need to run setup code when a package folder is imported, put it in a file\nnamed __init__.py and store it in the package’s folder. Python will automatically\nrun this file’s code the first time the package is imported during a program run\n(or REPL session). Here’s how this augments our package’s structure:\ndir0/                       \n    dir1/\n        __init__.py          # Run on first import of dir1\n        mod__.py\n        dir2/\n            __init__.py      # Run on first import of dir1.dir2\n            mod.py\nThe new __init__.py files are listed in Examples 24-3 and 24-4.\nExample 24-3. dir1/__init__.py\nvar = 'Python'\nprint('Running dir1.__init__.py')\nExample 24-4. dir1/dir2/__init__.py\nvar = 3.12\nprint('Running dir1.dir2.__init__.py')",
      "content_length": 1351,
      "extraction_method": "Direct"
    },
    {
      "page_number": 916,
      "chapter": null,
      "content": "Using the updated package\nThese special __init__.py files are optional in each folder of a package. Because\nthey are run on the first import of or through a folder level, though, they provide\na natural hook for kicking off package-specific initializations (hence their\nabbreviated names). In fact, their assignments serve to initialize the namespace\nthat corresponds to a folder on your device—as for files, they create attributes of\nthe package’s module object:\n$ python3\n>>> import dir1.dir2.mod                 # Runs all __init__.py files\nRunning dir1.__init__.py\nRunning dir1.dir2.__init__.py\nLoading dir1.dir2.mod\n>>> dir1.var                             # Name in __init__.py's namespace\n'Python'\n>>> dir1.dir2.var\n3.12\n>>> dir1.dir2.mod.var                    # Name in nested mod.py namespace\n'code'\nTechnically speaking, packages with __init__.py files are called “regular”\npackages, and those without them are called “namespace” packages, and the\ndemo was a single-folder “namespace” package until adding __init__.py made it\n“regular.” We’ll get into the details behind this distinction later, but apart from\nthe fact that “regular” packages have precedence during module search, the\ndifference is mostly a historical artifact today, and won’t matter in most roles. Of\ncourse, __init__.py files can also do more than print beacons; we’ll also explore\ntheir roles later in this chapter.\nAs noted earlier, reload works with package paths too, if you’ve already\nimported the paths in question. Continuing the prior REPL session:\n>>> from importlib import reload\n>>> reload(dir1)\nRunning dir1.__init__.py\n<module 'dir1' from '/…/LP6E/Chapter24/dir1/__init__.py'>\n>>> reload(dir1.dir2)\nRunning dir1.dir2.__init__.py\n<module 'dir1.dir2' from '/…/LP6E/Chapter24/dir1/dir2/__init__.py'>\n \n>>> reload(dir1.dir2.mod)",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 917,
      "chapter": null,
      "content": "Loading dir1.dir2.mod\n<module 'dir1.dir2.mod' from '/…/LP6E/Chapter24/dir1/dir2/mod.py'>\nNotice from the initialization prints that this call reloads just the single module\non the right end of the path. To do better, we’ll code a reloader tool in the next\nchapter that, given a package root, can load all the items on a package path like\nthis, one at a time; See “Example: Transitive Module Reloads”.\nPackage __main__.py Files\nFinally, if you want a package’s users to be able to run the package as though it\nwere a program or script, add __main__.py files to each folder where you wish\nto support this mode; Python will automatically run these files each time their\ncontainer folder is launched like a program. Here’s how this last bit augments\nour package’s structure:\ndir0/                       \n    dir1/\n        __main__.py          # Run whenever dir1 bundle is run\n        __init__.py\n        mod__.py\n        dir2/\n            __main__.py      # Run whenever dir1.dir2 bundle is run\n            __init__.py \n            mod.py\nThe added __main__.py files are listed in Examples 24-5 and 24-6; they’re\nsimple so we can focus on packages.\nExample 24-5. dir1/__main__.py\nprint('Executing dir1.__main__.py')\nExample 24-6. dir1/dir2/__main__.py\nprint('Executing dir1.dir2.__main__.py')\nUsing the updated package\nWhen present, __main__.py files are automatically run for a variety of launch\noptions, including direct console command lines that name the containing\npackage folder—which need not be on the module search path in this mode:",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 918,
      "chapter": null,
      "content": "$ python3 dir1                           # Runs __main__.py (not __init__.py)\nExecuting dir1.__main__.py\n$ python3 dir1/dir2\nExecuting dir1.dir2.__main__.py\n$ python3 dir1/dir2/mod.py               # Runs nested mod.py\nLoading dir1.dir2.mod\nPackage __main__.py files are also run for the Python -m mode, which, as we’ve\nseen, locates an item on the module search path and runs it as a top-level script.\nUnlike direct console command lines, this mode kicks off the full package-\nimport machinery and runs any __init__.py files along the path, but requires the\npackage to be on the search path:\n$ python3 -m dir1                        # Runs both __init__.py and __main__.py\nRunning dir1.__init__.py\nExecuting dir1.__main__.py\n$ python3 -m dir1.dir2                   # And package must be on search path\nRunning dir1.__init__.py\nRunning dir1.dir2.__init__.py\nExecuting dir1.dir2.__main__.py\n$ python3 -m dir1.dir2.mod               # Runs mod.py, and parents' __init__.py\nRunning dir1.__init__.py\nRunning dir1.dir2.__init__.py\nLoading dir1.dir2.mod\nThis is similar in spirit to app “bundles” on macOS and smartphones, which are\nreally folders but can be run as an executable item. The roles for __main__.py\nfiles are many, but they’re often used to provide a command-line interface to a\npackage of tools. This way, you can import the package to use its tools in another\nprogram, but also launch it as a whole to use the tools in a standalone program.\nThis file might also be used to host test code for the package.\nA __main__.py file is also run if located in a ZIP file on the search path. As\nnoted earlier, ZIP files are treated like a normal folder if encountered during a\nmodule search, but see Python’s docs for more on this option.\nOne subtlety here: intrapackage (same-package) imports in __main__.py may\nneed to use the package-relative from . syntax we’ll study ahead when run by\nthe -m argument only. This syntax fails for launches from direct command lines,\nso you may need to choose a launch mode to support when a __main__.py\nrequires same-package imports. As you’ll find, code that wants to support both",
      "content_length": 2117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 919,
      "chapter": null,
      "content": "package and program usage may solve this dilemma with full-path imports.\nWhy Packages?\nNow that you’ve seen how to use and create packages, you might be wondering\nwhy in the world anyone would go to all the bother. It’s a valid question. In\ntruth, and despite what you may have heard, packages are completely optional:\nthey are probably overkill early in your coding journey, and even later, you can\nwrite amazing programs without them.\nIn general, though, packages are useful to know about because they help make\nnames unique in both larger programs and libraries published for others to use.\nBy organizing your code as a package folder, you can be fairly sure that the\nnames of its files won’t clash with those of other software on hosting devices.\nThis is the same namespace-segregation role that file-based modules and local\nscopes in functions play, but extended to the filesystem: because references to\nyour package are qualified by the name of the package’s folder, there’s less risk\nof same-name collisions.\nIn addition, packages can make imports more descriptive, ease the task of\ntracking down the locations of variables in code files, and simplify your module\nsearch-path settings. The only time package imports are actually required,\nhowever, is to resolve ambiguities that may arise when multiple programs with\nsame-named files are installed on the same machine. This is something of an\ninstall issue, but it can also become a concern in general practice—especially\ngiven the tendency of developers to use simple and similar names for module\nfiles.\nTo help you better understand why all these benefits matter, let’s take a brief\ndetour into the land of the abstract.\nA Tale of Two Systems\nTo illustrate the roles of packages, suppose that a programmer develops a Python\nprogram that contains a file called utilities.py for common utility code, and a\ntop-level file named main.py that users launch to start the program. All over this\nprogram, its files say import utilities to load and use the common code.",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 920,
      "chapter": null,
      "content": "When the program is installed, it unpacks all its files into a single directory\nnamed system1 on the target machine that looks like this:\nsystem1/\n    utilities.py        # Common utility functions, classes\n    main.py             # Launch this to start the program\n    other.py            # Import utilities to load my tools\nNow, suppose that a second programmer develops a different program with files\nalso called utilities.py and main.py, and again uses import utilities\nthroughout the program to load its own common code file. When this second\nsystem is fetched and installed on the same computer as the first system, its files\nwill unpack into a new directory called system2 on the host—ensuring that they\ndo not overwrite same-named files from the first system:\nsystem2/\n    utilities.py        # Common utilities\n    main.py             # Launch this to run\n    other.py            # Imports utilities\nSo far, there’s no problem: both systems can coexist and run on the same\ncomputer. In fact, you won’t even need to configure the module search path to\nuse these programs on your computer—because Python always searches the\nhome directory first (that is, the directory containing the top-level file), imports\nin either system’s files will automatically see all the files in that system’s own\ndirectory. For instance, if you run system1/main.py, all imports will search\nsystem1 first. Similarly, if you launch system2/main.py, system2 will be searched\nfirst instead. Remember, module search path settings are only needed to import\nacross directory boundaries.\nHowever, suppose that after you’ve installed these two programs on your\nmachine, you decide that you’d like to use some of the code in each of the\nutilities.py files in a system of your own. It’s common utility code, after all, and\nPython code by nature “wants” to be reused. In this case, you’d like to be able to\nsay the following from code that you’re writing in a third directory to load one\nof the two files:\nimport utilities\nutilities.func('hack')",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 921,
      "chapter": null,
      "content": "Now the problem starts to materialize. To make this work at all, you’ll have to\nset the module search path to include the directories containing the utilities.py\nfiles. But which directory do you put first in the path—system1 or system2?\nThe issue here is the linear nature of the sys.path search path. It is always\nscanned from left to right, so no matter how long you ponder this dilemma, you\nwill always get just one utilities.py—from the directory listed first (leftmost) on\nthe search path. As is, you’ll never be able to import this file from the other\ndirectory at all.\nYou could try changing sys.path within your script before each import\noperation, but that’s both extra work and error-prone. And changing PYTHONPATH\nbefore each Python program run is too tedious, and won’t allow you to use both\nversions in a single file in an event. By default, you’re stuck.\nThis is the issue that packages actually fix. Rather than installing programs in\nindependent directories listed on the module search path directly and\nindividually, you can package and install them as subdirectories under a\ncommon root. For instance, you might organize all the code in this example as\nan install hierarchy that looks like this:\nroot/\n    system1/\n        utilities.py\n        main.py\n        other.py\n    system2/\n        utilities.py\n        main.py\n        other.py\n    mycode/                     # Here or elsewhere\n        myfile.py               # Your new code here\nNow, add just the common root directory to your search path. If your code’s\nimports are all relative to this common root, you can import either system’s\nutility file with a package import—the enclosing directory name makes the path\n(and hence, the module reference) unique. In fact, you can import both utility\nfiles in the same module, if you use an import statement and repeat the full path\neach time you reference the utility modules:",
      "content_length": 1897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 922,
      "chapter": null,
      "content": "import system1.utilities                # Import from one package\nimport system2.utilities                # Import from another\nsystem1.utilities.function('hack')      # And use names from either\nsystem2.utilities.function('code')      # Or both!\nIn short, the names of the enclosing directories here make the module references\nunique.\nNote that you have to use import instead of from with packages only if you\nneed to access the same attribute name in two or more paths. If the name of the\ncalled function here were different in each path, you could use from statements\nto avoid repeating the full package path whenever you call one of the functions,\nas described earlier; as also mentioned, the as extension in import and from can\nbe used to provide unique synonyms too.\nTechnically, in this case, the mycode folder doesn’t have to be under root—just\nthe packages of code from which you will import. Because you never know\nwhen your own modules might be useful in other programs, though, placing\nthem under the common root directory, too, may avoid similar name-collision\nproblems in the future.\nImportantly, path configuration also becomes simple if you’re careful to unpack\nall your Python systems under a common root like this; you’ll only need to add\nthe common root directory once. Moreover, both of the two original systems’\nimports will keep working unchanged. Because their home directories are\nsearched first, the addition of the common root on the search path is irrelevant to\ncode in system1 and system2; they can keep saying just import utilities and\nexpect to find their own files when run as programs. As you’ll see ahead,\nthough, if they are also used as packages, they may need to use from . package-\nrelative imports (and main.py could be __main__.py).\nFinally, keep in mind that even if you never create packages of your own, you’ll\nprobably use standard-library and third-party tools that do. As a random sample\nof standard-library packages:\nfrom email.message import Message                    # Email parsing/construction\nfrom tkinter.filedialog import askopenfilename       # Portable GUI toolkit\nfrom http.server import CGIHTTPRequestHandler        # Web-server utilities (till \nPython 3.15?)",
      "content_length": 2217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 923,
      "chapter": null,
      "content": "By bundling their code in a package, such tools become more self-contained, and\navoid name conflicts. Whether you opt to do the same for your own code is up to\nyou, but the next few sections dig deeper for if and when you opt-in.\nThe Roles of __init__.py Files\nNow that we’ve seen the basics and addressed the why of packages, let’s explore\nsome of the details behind their usage. The __init__.py files in our opening demo\nwere simple, but these files can contain arbitrary Python code, just like normal\nmodule files. Their names are special because their code is run automatically the\nfirst time a Python program imports a directory, and thus serves primarily as a\nhook for performing initialization steps required by the package. These files can\nalso be completely empty, though, and sometimes have additional roles.\nSpecifically, the __init__.py file serves as a hook for package initialization-time\nactions, generates a module namespace for a directory, declares a directory as a\n“normal” Python package, and implements the behavior of from * statements\nwhen used for folders in package imports:\nPackage initialization\nThe first time a Python program imports through a directory, it automatically\nruns all the code in the directory’s __init__.py file. Because of that, these\nfiles are a natural place to put code to initialize the state required by files in a\npackage. For instance, a package might use its initialization file to create\nrequired data files, open connections to databases, and so on. Typically,\n__init__.py files are not meant to be useful if executed directly (that’s what\n__main__.py is for); instead, they are run automatically when a package is\nfirst imported for use elsewhere.\nModule namespace initialization\nAs noted earlier, in the package import model, the directory paths in your\nscript become real nested object paths after an import. For instance, after an",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 924,
      "chapter": null,
      "content": "import in the preceding demo, the expression dir1.dir2 works and returns a\nmodule object whose namespace contains all the names assigned by dir2’s\n__init__.py initialization file. The variable assignments in such files provide\nattribute namespaces for module objects created for folders, which have no\nreal associated module file, and would otherwise have no place to define\nnames.\nPackage indicator for search\nPackage __init__.py files are also partly present to declare that a directory is\na Python package. In this role, these files serve to prevent directories with\ncommon names from unintentionally hiding true modules that appear later on\nthe module search path. Without this safeguard, Python might pick a\ndirectory that has nothing to do with your code, just because it appears\nnested in an earlier directory on the search path. As you’ll see later, the\ngeneralizations added for namespace packages subsume some of this role\nalgorithmically, by scanning ahead on the search path to find later items\nbefore settling on simple folders. Package __init__.py files, though, still give\na package higher priority than a same-name folder elsewhere on the search\npath.\nfrom * statement behavior\nAs an advanced feature, a list of name strings named __all__ at the top\nlevel of an __init__.py file defines what is exported for the from * statement\nform. Specifically, the __all__ list in an __init__.py file is taken to be the\nnames of nested submodules that should be automatically imported when\nfrom * is used on the package directory itself. If __all__ is not set, the\nfrom * does not automatically load submodules nested in the package",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 925,
      "chapter": null,
      "content": "directory; instead, it loads just names defined by assignments in the\ndirectory’s __init__.py file, including any submodules explicitly imported by\ncode in this file. For instance, from submodule import X in a directory’s\n__init__.py makes the name X available in that directory’s namespace\nwithout an __all__.\nYou’ll see an example of the __all__ list later, and more coverage of it in\nChapter 25 where you’ll find that it also serves to declare from * exports of\nsimple file-based modules, not just packages, and is part of the larger topic of\ndata hiding. You can also simply leave __init__.py files empty to give your\npackage precedence over namespace packages during an import search, but to\nunderstand why that matters, we need to move ahead.\nNOTE\nClasses conflation caution: As a preview, package __init__.py files are not the same as the\nclass __init__ constructor methods you’ll meet in the next part of this book. The former are\nfiles of code run when imports first step through a package folder in a program run, while the\nlatter are functions called whenever an instance is created. Both have initialization roles and\nare optional, but they are otherwise very different—despite their names.\nPackage-Relative Imports\nThe coverage of package imports so far has focused on importing package files\nfrom outside the package. Within the package itself, imports of same-package\nfiles can use the same full path syntax as imports from outside the package—and\nas you’ll see, sometimes should. However, files within a package can also make\nuse of intrapackage syntax that makes imports relative to the package itself, and\ncan ensure that imports in a package load the package’s own files.\nRelative and Absolute Imports\nThe code behind this is straightforward. Imports run by files used as part of a\npackage can use special syntax like the following, which works only in files used",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 926,
      "chapter": null,
      "content": "as package components, and only in from statements, not import:\nfrom . import module           # Import a module in this package (only)\nfrom .module import name       # Import a name from a module in this package\nfrom .. import module          # Import a module sibling of the parent folder\nfrom ..module import name      # Import a name from a parent-sibling module\nThis syntax wouldn’t make sense in import, because that statement assigns\nmodules to simple names, not paths. In a from, though, when the source of the\nimport begins with (or is only) dots like this, the import is known as relative—it\nidentifies an item relative to the folder of the enclosing package itself.\nThis name comes from the similarity to relative filename paths, which reference\na file per the current working directory (CWD), as covered in Chapter 9. Much\nas in filenames, “.” means the immediately enclosing package folder, and “..”\nmeans its parent folder another level up (and each additional dot means one level\nhigher, though this is rarely used). Here, though, the effect names an item\nrelative to an importer’s package, not a content file relative to the CWD.\nImportantly, relative imports within a package search only the referenced\npackage: they never check the folders listed in sys.path as normal imports do\n(and skip the built-in and frozen modules precheck). Moreover, relative-import\nsyntax can be used only when a file is being utilized as part of a package, and\nfails otherwise. This has consequences both good and bad that we’ll explore in a\nmoment.\nImports in files being used as part of a package can still be coded without dots as\nusual, in both import and from:\nimport module                  # Import a module from a folder in sys.path\nfrom module import name        # Import a name from the same\nSans leading periods like this, an import is known as absolute—it identifies an\nitem that’s located in a folder listed in sys.path. This is the normal behavior of\nimports in Python, and there’s really nothing “absolute” about it per the filename\nanalogy (absolute filename paths give a complete filesystem path). In fact, these\n“absolute” imports are really relative to an entry in the module search path, but\nwe’re stuck with the terminology today.",
      "content_length": 2247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 927,
      "chapter": null,
      "content": "Also importantly, absolute imports within a package do not automatically check\nthe package itself: they skip the package and move right to a search of folders on\nsys.path. As we’ve seen, sys.path may include the package’s folder anyhow,\nby virtue of the REPL’s CWD or the home folder of the top-level script, and\nPYTHONPATH or other settings. By themselves, though, absolute imports don’t\ncheck the package for imports run in package files.\nRelative-Import Rationales and Trade-Offs\nSo why the dots? In short, relative imports allow a package to ensure that its\nimports will load its own modules. Because relative imports search only the\npackage itself, they won’t inadvertently load a same-named but unrelated\nmodule elsewhere on the host that happens to be accessible through sys.path.\nThis in turn makes the package more self-contained: without relative imports,\nsuch an unrelated module might break the package’s code; with them, packages\nare less reliant on client search-path settings that they cannot predict or control.\nThe downside is that this model is an all-or-nothing proposition. You essentially\nmust choose a mode for your files—package or program. Relative imports give\nvisibility to the package itself, but cannot be used in nonpackage mode, and\nabsolute imports can be used in nonpackage mode, but do not give visibility to\nthe package itself. This combination seems a catch-22 that limits your code’s\nutility.\nThat being said, if you’re coding a library of tools, this may be a nonissue: you\ncan adopt relative imports throughout your package, and provide a __main__.py\nfile to run the package as a program with the -m switch. If you’re writing a more\ntraditional folder of code that you want to use arbitrarily, though, your options\nappear to be limited.\nOne easy solution to this dilemma is to simply avoid relative imports altogether,\nand use full, explicit, and “absolute” package-import paths everywhere in your\ncode. That is, use imports that list the path to the desired item from and\nincluding the package’s root folder itself:\nimport package.module                # Import a module in this package\nfrom package import module           # Same as: from . import module\nform package.module import name      # Same as: from .module import name",
      "content_length": 2267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 928,
      "chapter": null,
      "content": "For this to work, the package’s root folder must reside in a folder listed on\nsys.path, but this will be the normal case—there’s no way for clients to import\nthe package’s code otherwise. This also doesn’t directly support exotic imports\nlike “..” parent siblings, but these seem likely to be rare (e.g., only one standard-\nlibrary tool uses them today).\nWith this nonrelative equivalence, code folders can be used more flexibly, and\nboth direct command lines and the -m module-mode switch can be used to\nlaunch files in the package as programs. The next section shows how.\nPackage-Relative Imports in Action\nTo demo all of the foregoing points, let’s return to the simple package we coded\nat the start of this chapter. As you’ll recall, its __main__.py file is run\nautomatically when the folder is run as a bundle, but it will also play the role of\nany top-level file in the folder launched as a script. When we last met this file, it\nsimply contained a print; here’s a refresher of the last-known state of the files\nwe’ll be using here so you don’t have to flip back:\n# prior dir1/mod.py\nvar = 'hack'\nprint('Loading dir1.mod')\n# prior dir1/__main__.py\nprint('Executing dir1.__main__.py')\nThe normal-import warm-up\nThis __main__.py worked as advertised, but things get more complicated if a file\nrun as a top-level script tries to import another module in its own package folder\n—as in the rewrite of Example 24-7.\nExample 24-7. dir1/__main__.py (modified)\nimport mod\nprint('Executing dir1.__main__.py:', mod.var.upper())\nCoded this way, the file can be run with a direct command line (and both as a\nfolder and a script), but not with Python’s -m module-mode switch, which brings\nthe package machinery online (notice the __init__.py output in this mode):",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 929,
      "chapter": null,
      "content": "$ python3 dir1                              # Launch with a direct command line\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 dir1/__main__.py                  # As folder bundle or explicit file\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 -m dir1                           # Launch with Python -m module switch\nRunning dir1.__init__.py\nModuleNotFoundError: No module named 'mod'\n$ python3 -m dir1.__main__                  # As package root or explicit module\nRunning dir1.__init__.py\nModuleNotFoundError: No module named 'mod'\nThe first two commands in this work because __main__.py is run as a normal,\nnonpackage program, and finds its imported sibling per the “home” entry on\nsys.path, the top-level file’s own folder. The problem with the last two\ncommands is that they use the file as part of a package: the import mod in\n__main__.py is then interpreted as an absolute import—which skips the\nenclosing package. Hence the fails.\nThe relative-import adventure\nOur first reaction might be to add relative imports to appease the package system\n—as in Example 24-8.\nExample 24-8. dir1/__main__.py (modified)\nfrom . import mod\nprint('Executing dir1.__main__.py:', mod.var.upper())\nCoded this way, we’ve fixed -m usage, but broken direct command lines:\n$ python3 -m dir1\nRunning dir1.__init__.py\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 -m dir1.__main__\nRunning dir1.__init__.py\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 dir1\nImportError: attempted relative import with no known parent package\n$ python3 dir1/__main__.py\nImportError: attempted relative import with no known parent package",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 930,
      "chapter": null,
      "content": "The first two commands in this work because -m invokes package behavior,\nwhich enables the relative “.” import syntax that searches the package and finds\nthe target module. The problem with the last two commands is that Python\ndoesn’t allow relative imports to be used in nonpackage mode—which dooms\nthese runs from the start, regardless of search-path settings. Hence the fails.\nUnder this regime, then, it would seem we must choose package or nonpackage\nroles for our code.\nThe absolute-import solution\nAs noted, though, an easy way out of this constraint is to use normal package\nimports that explicitly spell out absolute import paths (which, again, are actually\nrelative to sys.path) from the package root—as in Example 24-9.\nExample 24-9. dir1/__main__.py (modified)\nimport dir1.mod\nprint('Executing dir1.__main__.py:', dir1.mod.var.upper())\nAs also noted, the package root, dir1, will normally be on sys.path, because\nthat’s the only way to use its code from outside the package (clients must import\nwith paths that start at the dir1 package root too).\nTo demo here, we’ll add the package root explicitly using a relative—in\nfilesystem terms—path of “.” for the current directory (in typical use, this might\ninstead be an absolute path, or Python’s automatic site-packages root of installed\npackages). We must set this here because the top-level file’s “home” on the\nsearch path in direct-command mode is one level below the package root, and\nboth direct-command and -m modes need access to the package root in general:\n$ export PYTHONPATH=.                   # Or similar outside Unix: see Chapter 22\n$ python3 dir1\nRunning dir1.__init__.py\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 dir1/__main__.py\nRunning dir1.__init__.py\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 -m dir1                       # Both usage modes work, sans \".\" imports\nRunning dir1.__init__.py",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 931,
      "chapter": null,
      "content": "Loading dir1.mod\nExecuting dir1.__main__.py: HACK\n$ python3 -m dir1.__main__\nRunning dir1.__init__.py\nLoading dir1.mod\nExecuting dir1.__main__.py: HACK\nNow launches by both direct command lines and the -m switch work, because\nwe’re not using a tool that limits the utility of our code to just one mode. The\nend result is dual-mode code—it can be used as both program and package.\nIf you’d rather not repeat import paths at each item reference, both launch modes\nalso work if we code __main__.py in any of these ways (test on your own to\nverify; this is just import norms at work):\nimport dir1.mod as mod\nprint('Executing dir1.__main__.py:', mod.var.upper())\nfrom dir1 import mod\nprint('Executing dir1.__main__.py:', mod.var.upper())\nfrom dir1.mod import var\nprint('Executing dir1.__main__.py:', var.upper())\nFor more fun, you can also use a from * in this scheme, if you add an __all__\nto the package root’s initialization file (subject to all the standard disclaimers\nabout the evils of from * outlined in Chapter 23):\n# dir1/__init__.py\n__all__ = ['mod']\n# dir1/__main__.py\nfrom dir1 import *\nprint('Executing dir1.__main__.py:', mod.var.upper())\nAgain, if you’re sure that your code will only ever be used as part of a package,\nyou could use relative imports in all of its files to access same-package modules.\nMoreover, the absolute-path solution arrived at here could be applied only for\nfiles meant to be lunched as top-level scripts; other files only imported can use\npackage-relative imports.\nIn both cases, relative imports come with the advantage that a same-named",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 932,
      "chapter": null,
      "content": "module earlier on sys.path won’t accidentally override a crucial module in the\npackage. But they also limit a file to package-only roles; if you want package\nfiles to support both launches and imports, relative imports are not your best\noption.\nIt should also be noted that we’re skipping some details here for space. For one,\n-m mode unusually also checks the CWD for absolute imports, making the\ndemo’s PYTHONPATH setting optional for this mode and demo only. For another,\nsome other resources suggest that relative-import failures in scripts stem from\ntheir name '__main__', but files run with -m have the same name and employ\nother protocols that are too obscure to cover here.\nBecause package-relative imports are both prone to change and mostly meant for\nprogrammers building large-scale libraries of code for others to use, this Python-\nlearners text will defer to Python’s docs for further details. Modules are feature-\nrich tools, to say the least, but the good news is that namespace packages—the\ntopic of the next and final section of this chapter—do not add any new syntax,\nbut simply allow a module to span folders. To see how, let’s move on.\nNamespace Packages\nNow that you’ve learned all about package and package-relative imports, there’s\none last package-related subject to cover. The evolutionary path of modules in\nPython eventually led to what are known as namespace packages—packages\nthat may be composed of one or more folders located on different parts of the\nmodule search path.\nWhile packages split across folders are probably atypical in the wild, the\ngeneralizations added to support namespace packages in the import-search\nalgorithm (procedure) also allow for packages without __init__.py files in\ngeneral. Since this revised algorithm is now just the import search, namespace\npackages are something of an incidental topic.\nTo understand namespace packages’ place in the broader modules picture,\nthough, as well as the changes they ushered in, we need a quick history lesson.\nPython Import Models",
      "content_length": 2024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 933,
      "chapter": null,
      "content": "All told, Python has four distinct import schemes. The following enumerates\nthem from original to newest, with representative but incomplete examples (as\nwe’ve seen, from also allows a * wildcard and reload reloads any module\npassed to it, but these are just add-on topics):\nBasic modules\nThe original model, used to import files and their contents, relative to the\nsys.path module search path:\nimport module\nfrom module import name\nBasic packages\nThe original package model used to import from folder paths relative to the\nsys.path search path, where each package is contained in a single directory\nthat has an __init__.py file:\nimport folder.folder.module\nfrom folder.module import name\nPackage-relative imports\nThe model used for intrapackage (same-package) imports of the prior\nsection, with its relative and absolute lookup rules for imports with and\nwithout leading dots, respectively:\nfrom . import module\nfrom .module import name\nNamespace packages\nThe newest package model, which is still relative to sys.path, but allows\npackages to span multiple directories, and removes the requirement that",
      "content_length": 1102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 934,
      "chapter": null,
      "content": "packages must define __init__.py files:\nimport anyfolder.anyfolder.module\nfrom anyfolder import name\nThe first two of these models are self-contained, but the third tightens up the\nsearch order and extends syntax for same-package imports, and the fourth\nupends some of the notions and requirements of the prior package model. In fact,\nPython now formally defines two flavors of packages:\nThe original model, now known as regular packages\nThe alternative model, known as namespace packages\nThe original and alternative package models are not mutually exclusive and can\nbe used simultaneously in the same program. In fact, the namespace package\nmodel works as something of a fallback option, recognized only if basic modules\nand regular packages of the same name are not present on the module search\npath. Despite their showy title, namespace packages are really just a mutation of\nimport search, as the following sections explain.\nNamespace-Package Rationales\nFirst off, it’s important to know that a namespace package is not fundamentally\ndifferent from a regular package: it is just a different way of creating packages.\nMoreover, they are still relative to sys.path at the top level: the leftmost\ncomponent of a dotted namespace-package path must still be located in an entry\non the normal module search path.\nIn terms of physical structure, though, regular and namespace packages have\nnotable differences. Regular packages have an __init__.py file that is run\nautomatically, and they reside in a single directory. Namespace packages, by\ncontrast, cannot contain an __init__.py, and may or may not span multiple\ndirectories collected at import time. Because the presence of __init__.py\ndifferentiates package type, none of the directories that make up a namespace\npackage can have this file, but the content nested within each of them is treated\nas a single composite package.",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 935,
      "chapter": null,
      "content": "The rationale for namespace packages is rooted in package installation goals\nthat may seem obscure unless you are responsible for such tasks. In short,\nthough, they resolve a potential for collision of multiple __init__.py files when\npackage parts are merged, by removing this file completely. Moreover, by\nproviding standard support for packages that can be split across multiple\ndirectories and located in multiple sys.path entries, namespace packages both\nenhance install flexibility and replace multiple incompatible solutions that had\narisen to address this goal.\nThough split-folder packages have some narrow roles, average Python users will\nprobably find namespace packages’ biggest benefit to be that they remove the\nrequirement for package __init__.py initialization files, and thus allow any\ndirectory of code to be used as an importable package. To see how, let’s move on\nto the nitty-gritty of import search.\nThe Module Search Algorithm\nTo understand the way that namespace packages change and extend import\nsearch, we have to look under the hood to see how the import operation works.\nAs we’ve seen, Python fulfills imports by searching for a name among a set of\ncandidate folders. For the leftmost components of imports, the set of candidates\nis all the directories listed on the sys.path module search path. For components\nof packages either nested in package imports or named in relative imports, the\nset of candidates is just the package itself. While package folders have just one\ninstance of a given name, search paths may have many.\nThe way the import search selects items from these candidates is subtler than\nimplied so far. For each directory in an import search’s set of candidates,\nPython tests for a variety of matches to an imported name, in the following order\n(using Unix / for folder separators and {…} to mean a choice):\n1. If directory/name/__init__.py is found, a regular package is\nimported and returned.\n2. If directory/name.{py, pyc, or other module extension} is found, a\nsimple module is imported and returned.",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 936,
      "chapter": null,
      "content": "3. If directory/name is found and is a directory, it is recorded and the\nscan continues with the next directory in the search’s set of candidates.\n4. If none of the above was found, the scan continues with the next\ndirectory in the search’s set of candidates.\nIf this search’s candidate scan completes without returning a regular package or\nmodule by steps 1 or 2, and at least one directory was recorded by step 3, then a\nnamespace package is created and returned. Else an error is reported.\nThe creation of the namespace package happens immediately and is not deferred\nuntil a lower-level import occurs. The new namespace package is a module\nobject that does not have a __file__ attribute, but has a __path__ set to an\niterable of the directory path strings that were found and recorded during the\ncandidates scan by step 3.\nThis __path__ attribute is then used as the set of candidates for later and deeper\naccesses, to search the one or more component folders of the namespace\npackage. That is, each recorded entry on a namespace package’s __path__ is\nsearched whenever further-nested items are requested, instead of the sole\ndirectory of a regular package.\nViewed another way, the __path__ attribute of a namespace package serves the\nsame role for lower-level components that sys.path does at the top for the\nleftmost component of package import paths; it becomes the “parent” search\npath for accessing lower items using the same four-step procedure just sketched.\nThe net result is that a namespace package is a sort of virtual concatenation of\ndirectories located on one or more search candidates. Once a namespace package\nis created, though, there is no functional difference between it and a regular\npackage; it supports everything we’ve learned for regular packages, including\npackage-relative import syntax.\nImportantly, because a single directory lacking an __init__.py but nested in a\nsearch-candidate folder is classified as a namespace package by this algorithm’s\nstep 3, any such directory qualifies as a package. The only operational difference\nbetween such a single folder and a regular package folder is that the former has\nlower search precedence: both same-named folders with an __init__.py and\nsimple modules anywhere in a search path are chosen first by steps 1 and 2.",
      "content_length": 2291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 937,
      "chapter": null,
      "content": "In other words, although the mods made to support namespace packages make\n__init__.py files optional in package folders, adding one, even if empty, ensures\nthat a package will be selected instead of a same-named folder later on a search\npath (it may also boost import speed by ending search-path scans at step 1, but\nthis is a one-time event). Whether this, or the other roles of __init__.py we met\nearlier, warrants including this file will naturally depend on your goals.\nNamespace Packages in Action\nTechnically, we’ve already seen namespace packages at work: the opening step\nof the demo at the start of this chapter made single-folder namespace packages\nbecause its folders didn’t yet have __init__.py files. Again, any folder nested in a\nsys.path search-path folder qualifies as a package, as long as it’s not hidden by\na same-named “regular” package with __init__.py or a simple module elsewhere\non the path.\nTo see the grander folder-concatenation effect of “virtual” namespace packages\nwork, let’s work through a quick demo. To begin, make two modules in a nested\ndirectory structure that has subdirectories named sub located in different parent\ndirectories, part1 and part2—like this in indentation notation meant to\nrepresent nesting:\nns/\n    part1/\n        sub/\n            mod1.py\n    part2/\n        sub/\n            mod2.py\nNote that there are no __init__.py files here—as noted earlier, these files cannot\nbe used in namespace packages, as this is their chief differentiation from regular,\nsingle-folder packages. We can create this demo’s folders with console\ncommands like the following; translate / to \\ and omit the -p on Windows, or\nuse a file explorer or the prebuilt folders in the examples package if you prefer:\n$ mkdir -p ns/part1/sub       # Two subdirs of same name in different dirs\n$ mkdir -p ns/part2/sub       # And similar on Windows",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 938,
      "chapter": null,
      "content": "The modules at the end of these paths are coded in Examples 24-10 and 24-11 to\nsimply print on imports as a trace.\nExample 24-10. ns/part1/sub/mod1.py\nprint('Loading ns/part1/sub/mod1')\nExample 24-11. ns/part2/sub/mod2.py\nprint('Loading ns/part2/sub/mod2')\nNow, if we add both part1 and part2 to the module search path, sub becomes a\nnamespace package spanning both folders, with the two module files available\nunder that name even though they live in separate physical directories. Here’s\nthe path-setting command on Unix (for Windows, use set and :, and see\nChapter 22 for more tips); this uses paths relative to the CWD, but in practice\nwould more likely list two full, absolute paths instead:\n$ export PYTHONPATH=ns/part1:ns/part2\nWhen imported directly, the namespace package is the virtual concatenation of\nits individual directory components, and allows further nested parts to be\naccessed through its single, composite name with normal imports (as usual,\npaths have been shortened here for space with “…” and line breaks were added\nfor fit):\n$ python3\n>>> import sub\n>>> sub                                 # Namespace packages: nested search paths\n<module 'sub' (namespace) from \n['/…/LP6E/Chapter24/ns/part1/sub', '/…/LP6E/Chapter24/ns/part2/sub']>\n>>> from sub import mod1\nLoading ns/part1/sub/mod1\n>>> import sub.mod2                     # Content from two different directories\nLoading ns/part2/sub/mod2\n>>> mod1\n<module 'sub.mod1' from '/…/LP6E/Chapter24/ns/part1/sub/mod1.py'>\n>>> sub.mod2\n<module 'sub.mod2' from '/…/LP6E/Chapter24/ns/part2/sub/mod2.py'>\nThis split-folder package also works if we import through the namespace",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 939,
      "chapter": null,
      "content": "package name immediately—because the namespace package is made when first\nreached, the timing of path extensions is irrelevant:\n$ python3\n>>> import sub.mod1\nLoading ns/part1/sub/mod1\n>>> import sub.mod2                     # One package spanning two directories\nLoading ns/part2/sub/mod2\n>>> sub.mod1\n<module 'sub.mod1' from '/…/LP6E/Chapter24/ns/part1/sub/mod1.py'>\n>>> sub.mod2\n<module 'sub.mod2' from '/…/LP6E/Chapter24/ns/part2/sub/mod2.py'>\n>>> sub\n<module 'sub' (namespace) from \n['/…/LP6E/Chapter24/ns/part1/sub', '/…/LP6E/Chapter24/ns/part2/sub']>\n>>> sub.__path__\n_NamespacePath(\n['/…/LP6E/Chapter24/ns/part1/sub', '/…/LP6E/Chapter24/ns/part2/sub'])\nInterestingly, relative imports work in namespace packages too, if we add one as\nin Example 24-12.\nExample 24-12. ns/part1/sub/mod1.py (modified)\nfrom . import mod2\nprint('Loading ns/part1/sub/mod1')\nThe added package-relative import statement references a file in the package—\neven though the referenced file resides in a different directory:\n$ python3\n>>> import sub.mod1                     # Relative import of mod2 in another dir\nLoading ns/part2/sub/mod2\nLoading ns/part1/sub/mod1 \n>>> import sub.mod2                     # Already imported by mod1.py: not rerun\n>>> sub.mod2\n<module 'sub.mod2' from '/…/LP6E/Chapter24/ns/part2/sub/mod2.py'>\nAs you can see, namespace packages are like ordinary single-directory packages\nin every way, except for having a split physical storage. By extension, this is\nwhy code folders without __init__.py files are exactly like regular packages, but\nwith no initialization logic to be run; they’re just an instance of namespace\npackages with a single directory.",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 940,
      "chapter": null,
      "content": "Like package-relative imports, this book is also going to defer to Python’s\nmanuals for more details on this subject. Namespace packages are a potentially\nuseful tool, but most Python learners are probably better served by first\nmastering packages that map to real folders, before tackling those that are spread\nvirtually across a host’s directories.",
      "content_length": 350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 941,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter introduced Python’s package import model—an optional but useful\nway to explicitly list part of the directory path leading up to modules. Package\nimports are still relative to a directory on your module search path, but your\nscript gives the rest of the path to the module explicitly. Packages may be built\nwith a simple folder and can make imports more meaningful, simplify import\nsearch-path settings, and resolve ambiguities when there is more than one\nmodule of the same name—the name of the enclosing directory makes them\nunique.\nBecause it’s relevant only to code in packages, we also explored relative\nimports: a way for imports in package files to select modules in the same\npackage explicitly using leading dots in from, instead of relying on a search in\nthe host. Finally, we surveyed namespace packages: a tool that allows a package\nto span multiple directories as a fallback option of import searches, and make\ninitialization files optional in single-folder packages.\nThe next chapter surveys a handful of both common and advanced module-\nrelated topics, such as the __name__ usage mode variable, the __getattr__\nattribute hook, and name-string imports, and codes useful module tools along the\nway. As usual, though, let’s close out this chapter first with a short quiz to\nreview what you’ve learned here.\nTest Your Knowledge: Quiz\n1. What is the purpose of an __init__.py file in a module package\ndirectory?\n2. How can you avoid repeating the full package path every time you\nreference a package’s content?\n3. Which directories require __init__.py files?\n4. When must you use import instead of from with packages?",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 942,
      "chapter": null,
      "content": "5. What is the difference between from pkg import name and from .\nimport name?\n6. What is a namespace package?\nTest Your Knowledge: Answers\n1. The __init__.py file serves to declare and initialize a regular module\npackage; Python automatically runs its code the first time you import\nthrough a directory in a process. Its assigned variables become the\nattributes of the module object created in memory to correspond to that\ndirectory. It’s optional for package folders but gives a folder search\nprecedence over other folders of the same name that don’t have this file.\n2. Use the from statement with a package to copy names out of the\npackage directly, or use the as extension with the import statement to\nrename the path to a shorter synonym. In both cases, the path is listed in\nonly one place, in the from or import statement.\n3. Trick question! These files used to be required for packages in earlier\nPythons but became optional with accommodations for namespace\npackages in the module search algorithm. As noted in answer 1, though,\nthese folders still have valid, if optional, roles, including boosting a\nfolder’s import-search precedence.\n4. You must use import instead of from with packages only if you need to\naccess the same name defined in more than one path. With import, the\npath makes the references unique, but from allows only one version of\nany given name (unless you also use the as extension to rename\nuniquely).\n5. The from pkg import name is an absolute import—the search for pkg\nskips an enclosing package and then tries the “absolute” directories in\nsys.path. A statement from . import name, on the other hand, is a\nrelative import—name is looked up relative to the package in which this\nstatement is contained, only.",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 943,
      "chapter": null,
      "content": "6. A namespace package is an extension to the import model that\ncorresponds to one or more directories that do not have __init__.py\nfiles. When Python finds these during an import search and does not\nfind a simple module or regular package first, it creates a namespace\npackage that is the virtual concatenation of all found directories having\nthe requested module name. Further nested components are looked up\nin all the namespace package’s directories. The effect is similar to a\nregular package, but content may be split across multiple directories.",
      "content_length": 552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 944,
      "chapter": null,
      "content": "Chapter 25. Module Odds and\nEnds\nThis chapter concludes this part of the book with an assortment of module-\nrelated topics—data hiding, the __future__ module, the __name__ variable,\nname-string imports, the __gettattr__ hook, transitive reloads, and more—\nalong with the usual set of gotchas and exercises related to what we’ve covered\nin this part of the book. Along the way, we’ll build some useful tools that\ncombine functions and modules. Like functions, modules are more effective\nwhen their interfaces are well-defined, so this chapter also briefly reviews\nmodule design concepts.\nThough some coverage here might qualify as advanced and optional, this is\nmostly a miscellany of additional module subjects. Because some of the topics\ndiscussed here are very widely used—especially the __name__ dual-mode trick\n—be sure to browse here before moving on to classes in the next part of the\nbook.\nModule Design Concepts\nFirst up, some perspective. Like functions, modules present design trade-offs:\nyou have to think about which functions go in which modules, module\ncommunication mechanisms, and so on. All of this will become clearer when\nyou start writing bigger Python systems, but here are a few general ideas to keep\nin mind:\nYou’re always in a module in Python. There’s no way to write code\nthat doesn’t live in some module. As mentioned briefly in Chapters 17\nand 21, even code typed at the interactive prompt (a.k.a. REPL) really\ngoes in a built-in module called __main__; the only unique things about\nthe interactive prompt are that code runs and is discarded immediately,\nand expression results are printed automatically.",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 945,
      "chapter": null,
      "content": "Minimize module coupling: global variables. Like functions, modules\nwork best if they’re written to be mostly closed boxes. As a rule of\nthumb, they should be as independent of global variables used within\nother modules as possible, except for functions and classes imported\nfrom them. The only things a module should share with the outside\nworld are the tools it uses, and the tools it defines.\nMaximize module cohesion: unified purpose. Also like functions, you\ncan minimize a module’s couplings by maximizing its cohesion. If all\nthe components of a module share a general purpose, they’re less likely\nto depend on external names.\nModules should rarely change other modules’ variables. We\nillustrated this with code in Chapter 17, but it’s worth repeating here:\nit’s perfectly OK to use globals defined in another module (that’s how\nclients import services, after all), but changing globals in another\nmodule is often a symptom of a design problem. There are exceptions,\nof course, but you should try to communicate results through devices\nsuch as function arguments and return values, not cross-module\nchanges. Otherwise, your globals’ values become dependent on the\norder of arbitrarily remote assignments in other files, and your modules\nbecome harder to understand and reuse.\nAs a summary, Figure 25-1 sketches the environment in which modules operate.\nModules contain variables, functions, and classes, and import other modules for\nthe tools they define. Functions have local variables of their own, as do classes\n—objects that live within modules and which we’ll begin studying in the next\nchapter. As we saw in Part IV, functions can nest, too, but all are ultimately\ncontained by modules at the top.",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 946,
      "chapter": null,
      "content": "Figure 25-1. Module execution environment\nData Hiding in Modules\nNext, we turn to private matters. As we’ve seen, a Python module exports all the\nnames assigned at the top level of its file. There is no syntax for declaring which\nnames should and shouldn’t be visible outside the module. In fact, there’s no\nway to prevent a client from changing names inside a module if it wants to.\nIn Python, data hiding in modules is a convention, not a syntactical constraint. If\nyou want to break a module by trashing its names, you can, but most\nprogrammers don’t count this as a life goal. Some purists object to this liberal\nattitude toward data hiding, claiming that it means Python can’t implement\nencapsulation. However, encapsulation in Python is more about packaging than\nabout restricting. We’ll expand on this idea in the next part in relation to classes,\nwhich also have no privacy syntax but can often emulate its effect in code.\nMinimizing from * Damage: _X and __all__\nThat being said, as a limited special case, you can prefix names with a single",
      "content_length": 1050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 947,
      "chapter": null,
      "content": "underscore (e.g., _X) to prevent them from being copied out when a client\nimports a module’s names with a from * statement. This really is intended only\nto minimize namespace pollution; because from * copies out all names, the\nimporter may get more than it’s bargained for (including names that overwrite\nnames in the importer). But underscores aren’t “private” declarations: you can\nstill see and change such names with other import forms. Example 25-1 demos\nthe idea.\nExample 25-1. unders.py\na, b, _c, _d = 1, 2, 3, 4              # Control from * exports, take 1\nWhen names both with and without underscores are assigned this way, from *\ncan’t see the former, but import and normal from can:\n$ python3\n>>> from unders import *               # Load non _X names only on from *\n>>> a, b\n(1, 2)\n>>> _c\nNameError: name '_c' is not defined. Did you mean: '_'?\n>>> from unders import _c              # But other importers get every name\n>>> _c\n3\n>>> import unders\n>>> unders._d\n4\nAlternatively, you can achieve a hiding effect similar to the _X naming\nconvention by assigning a list of variable name strings to the variable __all__ at\nthe top level of the module. When this feature is used, the from * statement will\ncopy out only those names listed in the __all__ list, though other imports work\nas before.\nIn effect, this is the converse of the _X convention: __all__ identifies names to\nbe copied, while _X identifies names not to be copied. Python looks for an\n__all__ list in the module first, and copies its names irrespective of any\nunderscores; if __all__ is not found, from * copies all names without a single\nleading underscore. To demo, Example 25-2 uses both name-hiding tools.",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 948,
      "chapter": null,
      "content": "Example 25-2. alls.py\n__all__ = ['a', '_c']                  # Control from * exports, take 2\na, b, _c, _d = 1, 2, 3, 4              # __all__ has precedence over _X\nOn imports, from * gets everything in __all__, but no others; other importers\nagain get everything:\n$ python3\n>>> from alls import *                 # Load __all__ names - only\n>>> a, _c                              # Even if they have underscores\n(1, 3)\n>>> b\nNameError: name 'b' is not defined\n>>> from alls import a, b, _c, _d      # But other importers get every name\n>>> a, b, _c, _d\n(1, 2, 3, 4)\n>>> import alls\n>>> alls.a, alls.b, alls._c, alls._d\n(1, 2, 3, 4)\nLike the _X convention, the __all__ list has meaning only to the from *\nstatement form and does not amount to a privacy declaration: other import\nstatements can still access all names, as the last two tests show. Still, module\nwriters can use either technique to implement modules that are well-behaved\nwhen used with from *.\nSee also the discussion of __all__ lists in package __init__.py files in\nChapter 24. In this context, these lists declare nested submodules to be\nautomatically loaded for a from * run on their container. The effect is similar to\nname hiding in module files, though packages extend it to apply to the content of\na package folder in the filesystem.\nManaging Attribute Access: __getattr__ and __dir__\nOn the subject of data hiding in modules, Python 3.7 added support for special\nfunctions at a module’s top level that can be used to manage access to a\nmodule’s attributes. If defined, a module’s __getattr__ function is\nautomatically run when a module attribute is not found, and its __dir__",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 949,
      "chapter": null,
      "content": "overrides the normal attribute-list fetch run for the dir built-in. These can be\nused to implement both basic access constraints and arbitrarily dynamic\ninterfaces.\nThese functions also shadow same-named tools in classes and are meant in part\nto obviate a long-standing and obscure trick that reset a module’s object in the\nsys.modules table to an instance of a class with these same methods. This, of\ncourse, means that these functions may make more sense after we study classes\nin Part VI, but the artificial module in Example 25-3 demos the basics.\nExample 25-3. gamod.py\nvar = 2                                   # Real attribute returned directly\ndef __getattr__(name):                    # Undefined attr fetches routed here\n   print(f'(virtual {name})', end=' ')\n   match name:\n       case 'test':\n           return name * var\n       case 'hack' | 'code':\n           return name.upper()\n       case _:\n           raise AttributeError(f'{name} is undefined')\ndef __dir__():\n   return ['var', 'test', 'hack', 'code']\nWhen imported, fetches of real attributes defined in the module work normally\n(subject to the _X and __all__ of the prior section for from *), but missing\nnames are routed to __getattr__, which can manage the request. It may also\nuse a raise statement to flag an invalid request with an exception—a topic we’ll\nstudy in full later in the book because it’s also dependent on classes today:\n>>> import gamod\n>>> gamod.var                  # Real: __getattr__ not called\n2\n>>> gamod.test                 # Virtual: computed when fetched\n(virtual test) 'testtest'\n>>> gamod.hack             \n(virtual hack) 'HACK'\n>>> gamod.nonesuch         \nAttributeError: nonesuch is undefined\n>>> dir(gamod)\n['code', 'hack', 'test', 'var']",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 950,
      "chapter": null,
      "content": "The from statement invokes __getattr__ too, though from * requires names to\nbe listed on __all__ (you can largely ignore the spurious __path__ fetch here,\nthough a __getattr__ must accommodate it):\n>>> from gamod import code\n(virtual __path__) (virtual code) \n>>> code\n'CODE'\n>>> from gamod import *\n(virtual __path__) (virtual __all__)\n>>> var\n2\n>>> hack\nNameError: name 'hack' is not defined\nImportantly, __getattr__ is not run for global-scope lookup within the module\nitself, so in-file undefined names remain undefined. It’s really just for attribute\nfetches from other modules and does not catch assignments anywhere. For\nexample, the first line of the following added at the bottom of Example 25-3\nwould fail, and the second line run in the REPL would make a new attribute in\nthe module which bypasses __getattr__ thereafter:\nprint(test)                # File: does NOT call __getattr__ (raises NameError)\ngamod.hack = 'real'        # REPL: does NOT call __getattr__ (makes attribute)\nAlthough this all works as advertised, it is a tool-builder’s hook, and you’ll have\nto unearth legitimate use cases. It may be useful in narrow roles, but it also\nconflates modules with classes and discounts the fact that module learners do not\nalready understand these functions’ origins in classes. This is a regrettably\ncommon theme in Python: additions often come with forward dependencies that\nseem to expect users to already know Python in order to use Python. Python is\nnot just for Python experts, but that’s a message baked into many a mod.\nThe good news here may be that a later proposal to add classes’ __setattr__\nfor module-attribute assignment was rejected by Python’s steering committee—\nthough only after allowing __getattr__ and __dir__ to sneak in. As usual, you\nshould weigh the convolutions of this extension against its real-world\napplications.",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 951,
      "chapter": null,
      "content": "Enabling Language Changes: __future__\nSpeaking of changes, Python mods that may break existing code are often\nintroduced gradually. This is not always as “gradual” as it might be, and version\n3.0 was a glaring exception (though 2.X was supported for 12 more years after\n3.X’s release). Sometimes, though, changes initially appear as optional\nextensions, which are disabled by default. To enable such an extension in\nPythons that predate its official arrival, use a special import statement of this\nform:\nfrom __future__ import featurename\nWhen coded in a script, this statement must appear as the first executable\nstatement in the file (possibly following a docstring or comment), because it\nenables special compilation of code on a per-module basis. It’s also possible to\nsubmit this statement at the interactive prompt to experiment with upcoming\nlanguage changes; the feature will then be available for the remainder of the\ninteractive session.\nFor example, the prior edition of this book used this statement in Python 2.X to\nactivate 3.X true division of Chapter 5, 3.X print calls of Chapter 11, and 3.X\nabsolute imports for packages of Chapter 24. Earlier editions used this statement\nform to demonstrate generator functions, which require a yield that was not yet\nenabled by default.\nThis edition is boldly going forward with the present, but __future__ can be\nused in older Pythons to enable Python 3.7’s StopIteration “bubbling”\nbehavior described at the end of Chapter 20:\nfrom __future__ import generator_stop\nFor a list of futurisms you may import and turn on this way, see the Python\nlibrary manual’s entry for __future__. Per its documentation, none of its feature\nnames will ever be removed, so it’s safe to leave in a __future__ import even in\ncode run by a version of Python where the feature is enabled normally. The\nfuture does not erase the past.",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 952,
      "chapter": null,
      "content": "Dual-Usage Modes: __name__ and __main__\nOur next module-related trick lets you both import a file as a module and run it\nas a standalone script, a hook that is widely used in Python files. It’s actually so\nsimple that some learners miss the point at first. Each module has a built-in\nattribute called __name__, which Python creates and assigns automatically as\nfollows:\nIf the file is being run as a top-level script file, __name__ is set to the\nstring '__main__' when it starts.\nIf the file is being imported instead, __name__ is set to the module’s\nname as known by its clients.\nThe upshot is that a module can test its own __name__ to determine whether it’s\nbeing run or imported. For example, suppose we create the code file named\ndualmode.py in Example 25-4, with a single function called title.\nExample 25-4. dualmode.py\ndef title():\n   print('Learning Python, 6E')\nif __name__ == '__main__':           # Only when run\n   title()                          # Not when imported\nThis module defines a function for clients to import and use as usual:\n$ python3\n>>> import dualmode\n>>> dualmode.title()\nLearning Python, 6E\nBut the module also includes code at the bottom that is set up to call the function\nautomatically when this file is run as a program:\n$ python3 dualmode.py\nLearning Python, 6E\nIn effect, a module’s __name__ variable serves as a usage mode flag, allowing its\ncode to be leveraged as both an importable library and a top-level script. Though",
      "content_length": 1462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 953,
      "chapter": null,
      "content": "simple, you’ll see this hook used in many of the Python program files you are\nlikely to encounter in the wild—both for testing and dual usage.\nFor instance, one of the most common ways you’ll see the __name__ test applied\nis for self-test code. In short, you can package code that tests a module’s exports\nin the module itself by wrapping it in a __name__ test at the bottom of the file.\nThis way, you can use the file in clients by importing it, but also test its logic by\nrunning it from the system shell or other launching scheme.\nCoding self-test code at the bottom of a file under the __name__ test is probably\nthe most common and simplest unit-testing protocol in Python. It’s much more\nconvenient than retyping all your tests at the interactive prompt. (Preview:\nChapter 36 will discuss other commonly used options for testing Python code—\nas you’ll see, the unittest and doctest standard-library modules provide more\nadvanced testing tools.)\nIn addition, the __name__ trick is also commonly used when you’re writing files\nthat can be useful both as command-line utilities and as tool libraries. For\ninstance, suppose you write a file-finder script in Python. You can get more\nmileage out of your code if you package it in functions, and add a __name__ test\nin the file to automatically call those functions when the file is run standalone.\nThat way, the script’s code becomes reusable in other programs.\nNOTE\nWhat’s in a __name__?: Don’t confuse the __main__ hook here with the __main__.py file\ndiscussed in the prior chapter. That file serves as a script when running an entire package\nfolder as a program, but testing whether __name__ is '__main__' is used to give two roles to a\nsingle file. Python often conflates the same names for similar but different purposes—see\n__getattr__!\nExample: Unit Tests with __name__\nIn fact, we’ve already seen numerous cases in this book where the __name__\ncheck could be useful. As one example, we coded a script in Chapter 18 that\ncomputed the minimum value from the set of arguments sent—Example 18-3,\nwhose code is repeated here for ease of reference:",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 954,
      "chapter": null,
      "content": "def minmax(test, *args):\n    res = args[0]\n    for arg in args[1:]:\n        if test(arg, res):\n            res = arg\n    return res\ndef lessthan(x, y): return x < y\ndef grtrthan(x, y): return x > y\nprint(minmax(lessthan, 4, 2, 1, 5, 6, 3))      # Self-test code\nprint(minmax(grtrthan, 4, 2, 1, 5, 6, 3))\nThis script includes self-test code at the bottom, so we can test it without having\nto retype test code in the REPL each time we run it. The problem with the way it\nis currently coded, however, is that the output of the self-test call will appear\nwhen this file is imported from another file to be used as a tool—not exactly a\nclient-friendly feature! To do better, we can wrap up the self-test call in a\n__name__ check so that it will be launched only when the file is run as a top-\nlevel script, not when it is imported. Example 25-5 lists this new-and-improved\nversion of the module.\nExample 25-5. minmax.py\nprint('I am:', __name__)\ndef minmax(test, *args):\n   res = args[0]\n   for arg in args[1:]:\n       if test(arg, res):\n           res = arg\n   return res\ndef lessthan(x, y): return x < y\ndef grtrthan(x, y): return x > y\nif __name__ == '__main__':\n   print(minmax(lessthan, 4, 2, 1, 5, 6, 3))      # Self-test code\n   print(minmax(grtrthan, 4, 2, 1, 5, 6, 3))\nWe’re also printing the value of __name__ at the top here to trace its value\n(something you wouldn’t do in a real library module). Python creates and assigns\nthis usage-mode variable as soon as it starts loading a file. When we run this file\nas a top-level script, its name is set to __main__, so its self-test code kicks in\nautomatically:",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 955,
      "chapter": null,
      "content": "$ python3 minmax.py\nI am: __main__\n1\n6\nIf we import the file, though, its name is not __main__, so we must explicitly\ncall the function to make it run:\n$ python3\n>>> import minmax\nI am: minmax\n>>> minmax.minmax(minmax.lessthan, *'hack')\n'a'\nAgain, regardless of whether this is used for testing, the net effect is that we get\nto use our code in two different roles—as a library module of tools, or as an\nexecutable program. It’s buy-one-get-one code. You’ll also see programs that\nroute program-mode runs into a module called main:\ndef main():\n    …\nif __name__ == '__main__':\n    main()\nThis works, but it’s extra code, and there’s nothing special about a function\nnamed main in Python—unlike some other languages, which may be part of the\ninspiration for this indirection’s appearance in Python code.\nNOTE\nFishing tutorial past: For another example of the __name__ == '__main__' test at work, see\nthe dual-mode script/module formats.py in this book’s examples package. It formats numbers\nwith commas and currency conventions and demos how you can code your own flexible tools\ninstead of relying on built-ins. It didn’t add much here and was cut in this edition for space, but\nprovides optional self-study code—and underscores that learning to fish generally beats being\ngiven one.\nThe as Extension for import and from",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 956,
      "chapter": null,
      "content": "Next on the tour is a follow-up on a topic introduced in Chapter 23’s name-\ncollision coverage. As a minor but useful convenience, both the import and\nfrom statements support an optional as clause, which simply renames a name\nimported by your script. For example, the following import statement using the\nas extension:\nimport modulename as name                     # And use name, not modulename\nis equivalent to the following set of three statements, which renames the module\nin the importer’s scope only (it’s still known by its original name to other files),\nand drops the original name in the importer’s scope altogether:\nimport modulename                             # Run a normal import\nname = modulename                             # Rename the module - here\ndel modulename                                # Discard the original name - here\nAfter an import with as, you can—and in fact, must—use the name listed after\nthe as to refer to the module. The longer equivalent works because modules are\nfirst-class objects just like functions, and can be passed around freely.\nThe as extension works in a from statement, too, to assign a name imported\nfrom a file to a different name in the importer’s scope. As before, you get only\nthe new name you provide, not its original:\nfrom modulename import attrname as name       # And use name, not attrname\nThis in turn works the same as the following statements:\nfrom modulename import attrname\nname = attrname\ndel attrname\nAs noted in Chapter 23, this extension is commonly used both to provide shorter\nsynonyms for longer names and to avoid name clashes when you are already\nusing a name in your script that would otherwise be overwritten by a normal\nimport:\nimport reallylongmodulename as name           # Use shorter nickname",
      "content_length": 1776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 957,
      "chapter": null,
      "content": "name.func()                                   # Rename to make shorter\nfrom module1 import utility as util1          # Can have only one \"utility\"\nfrom module2 import utility as util2          # Rename to make unique\nutil1(); util2()\nBy way of review, the as clause also comes in handy for providing a short,\nsimple name for an entire directory path and avoiding name collisions when\nusing the package import feature described in Chapter 24:\nimport dir1.dir2.mod as mod                   # Only list full path once\nmod.func()                                    # Only one change if path changes\nfrom dir1.dir2.mod import func as modfunc     # Rename to make unique if needed\nmodfunc()                                     # Allow func to be something else\nFinally, the as clause is also something of a hedge against name changes: if a\nnew release of a library renames a module or tool your code uses extensively, or\nprovides a new alternative you’d rather use instead, you can simply rename it to\nits prior name on import to avoid breaking your code:\nimport newname as oldname\nfrom library import newname as oldname\n…and keep happily using oldname until you have time to update all your code…\nThat said, if all software changes were just name changes, we’d have a lot less to\nfill our time!\nModule Introspection\nNext up is module plumbing. Because modules expose most of their interesting\nproperties as built-in attributes, it’s easy to write programs that manage other\nprograms—tools we usually call metaprograms, because their subjects are other\nprograms. This domain is also referred to as introspection, because programs can\nsee and process object internals. Introspection is a somewhat advanced feature,\nbut it can be useful for building programming tools.\nFor instance, to fetch the value of a module’s attribute, we can use attribute\nqualification or index the module’s attribute dictionary, exposed in the built-in",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 958,
      "chapter": null,
      "content": "__dict__ attribute we explored in Chapter 23. As we’ve also seen, Python’s\nvars built-in is an alternative way to access __dict__, and its sys.modules\ndictionary records all loaded modules by import-name string. In addition, its\ngetattr built-in lets us fetch attributes from their string names—it’s like saying\nobject.attr, but attr is an expression that produces a string at runtime.\nHence, all the following expressions reach the same attribute and object named\nname after importing M and sys:\nM.name                              # Qualify object by attribute\nM.__dict__['name']                  # Index namespace dictionary manually\nvars(M)['name']                     # Namespace dictionary alternative\nsys.modules['M'].name               # Index loaded-modules table manually\ngetattr(M, 'name')                  # Call built-in fetch function\nsys.modules['M'].__dict__['name']   # Module and attribute name strings\nDemoing with Example 25-5 (and chained comparisons that imply an and and a\nright-side repeat):\n>>> import minmax, sys\n>>> (minmax.lessthan \n        is minmax.__dict__['lessthan']    is vars(minmax)['lessthan']\n        is sys.modules['minmax'].lessthan is getattr(minmax, 'lessthan')\n        is sys.modules['minmax'].__dict__['lessthan'])\nTrue\nOf course, the first of these is much easier on the eyes (and keyboard), but the\nothers support more generic access.\nExample: Listing Modules with __dict__\nBy exposing module internals like this, Python helps you build programs about\nprograms. As a demo, the module in Example 25-6, named mydir.py, puts these\nideas to work to implement a customized and expanded version of the built-in\ndir function. It defines and exports a function called listing, which takes a\nmodule object as an argument and prints a formatted display of the module’s\nnamespace sorted by attribute name.\nExample 25-6. mydir.py\n\"\"\"",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 959,
      "chapter": null,
      "content": "mydir.py: a module that lists the namespaces of other modules.\nImport this module's listing and pass an imported module, or \nrun this file as a script to perform its self-test code.\n\"\"\"\nsepchr = '-'\nseplen = 60\ndef listing(module, verbose=True, unders=True):\n   \"\"\"\n   List module: just attributes if verbose=False, \n   hide built-in __X__ attributes if unders=False. \n   \"\"\"\n   sepline = sepchr * seplen\n   if verbose:\n       print(sepline)\n       print(f'name: {module.__name__}\\nfile: {module.__file__}')\n       print(sepline)\n   # Scan namespace keys\n   for (count, attr) in enumerate(sorted(module.__dict__)):\n       prefix = f'{count + 1:02d}) {attr}'\n       if attr.startswith('__'):\n           if unders:\n              print(prefix, '<built-in name>')    # Skip __file__, etc.\n       else:\n          print(prefix, getattr(module, attr))    # Or module.__dict__[attr]\n   if verbose:\n       print(sepline)\n       print(f'{module.__name__} has {count + 1} names')\n       print(sepline)\nif __name__ == '__main__':\n   import mydir\n   listing(mydir)                                  # Self-test code: list myself\nNotice the docstrings in this module; because we may want to use this as a\ngeneral tool, the docstrings provide functional information accessible via help\nand the browser mode of PyDoc—tools that use similar introspection tools to do\ntheir jobs (see Chapter 15 for usage info). A self-test is also provided at the\nbottom of this module, which narcissistically imports and lists itself; here’s the\nsort of output produced (with path edits for space):\n$ python3 mydir.py\n------------------------------------------------------------\nname: mydir",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 960,
      "chapter": null,
      "content": "file: /Users/me/…/LP6E/Chapter25/mydir.py\n------------------------------------------------------------\n01) __builtins__ <built-in name>\n02) __cached__ <built-in name>\n03) __doc__ <built-in name>\n04) __file__ <built-in name>\n05) __loader__ <built-in name>\n06) __name__ <built-in name>\n07) __package__ <built-in name>\n08) __spec__ <built-in name>\n09) listing <function listing at 0x1077758a0>\n10) sepchr -\n11) seplen 60\n------------------------------------------------------------\nmydir has 11 names\n------------------------------------------------------------\nTo use this as a tool for listing other modules, simply pass the modules in as\nobjects to this file’s function. Here it is listing itself manually, as well as\nattributes in the tkinter GUI module in the Python standard library; it will\ntechnically work on any object with __name__, __file__, and __dict__\nattributes:\n>>> from mydir import listing\n>>> import mydir, tkinter\n>>> listing(mydir, unders=False, verbose=False)\n09) listing <function listing at 0x10800fba0>\n10) sepchr -\n11) seplen 60\n>>> listing(tkinter, unders=False)\n------------------------------------------------------------\nname: tkinter\nfile: /…/lib/python3.12/tkinter/__init__.py\n------------------------------------------------------------\n01) ACTIVE active\n02) ALL all\n03) ANCHOR anchor\n04) ARC arc\n...more names omitted...\n166) re <module 're' from '/…/lib/python3.12/re/__init__.py'>\n167) sys <module 'sys' (built-in)>\n168) types <module 'types' from '/…/lib/python3.12/types.py'>\n169) wantobjects 1\n------------------------------------------------------------\ntkinter has 169 names",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 961,
      "chapter": null,
      "content": "------------------------------------------------------------\nYou’ll meet getattr and its relatives again later. The point to notice here is that\nmydir is a program that lets you browse other programs. Because Python\nexposes its internals, you can process objects generically.\nNOTE\nREPL startup tip: You can preload tools such as mydir.listing and the reloader we’ll code in\na moment into the interactive REPL by importing them in a file named by the PYTHONSTARTUP\nenvironment variable. Because code in the startup file runs in the interactive namespace,\nimporting common tools in this file can save you some typing. See Appendix A for more info.\nImporting Modules by Name String\nFinally, it’s time for something more dynamic. By now, you’ve probably noticed\nthat the module name in an import or from statement is a hardcoded variable\nname. Sometimes, though, your program will get the name of a module to be\nimported as a string at runtime—from a user selection in a GUI, or a parse of an\nXML document, for instance. Unfortunately, you can’t use import statements\ndirectly to load a module given its name as a string—Python expects a variable\nname that’s taken literally and not evaluated, not a string or expression. For\ninstance:\n>>> import 'string'\nSyntaxError: invalid syntax\nIt also won’t work to simply assign the string to a variable name:\n>>> x = 'string'\n>>> import x\nModuleNotFoundError: No module named 'x'\nHere, Python will try to import a file x.py, not the string module—the name in\nan import statement both becomes a variable assigned to the loaded module and\nidentifies the external file literally.",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 962,
      "chapter": null,
      "content": "Running Code Strings\nTo get around this, you need to use special tools to load a module dynamically\nfrom a string that is generated at runtime. The most general approach is to\nconstruct an import statement as a string of Python code and pass it to the exec\nbuilt-in function to run:\n>>> modname = 'string'\n>>> exec('import ' + modname)      # Run a string of code\n>>> string                         # Imported in this namespace\n<module 'string' from '/…/lib/python3.12/string.py'>\nWe met the exec function—and its cousin for expressions, eval—earlier, in\nChapters 3, 5, 9, and 10. exec compiles a string of code and passes it to the\nPython interpreter to be executed. In Python, the bytecode compiler is available\nat runtime, so you can write programs that construct and run other programs like\nthis. By default, exec runs the code in the current scope (as if pasted there), but\nyou can get more specific by passing in optional namespace dictionaries. It also\nhas security issues noted earlier in the book, which may be moot in a code string\nyou are building yourself.\nDirect Calls: Two Options\nThe only real drawback to exec here is that it must compile the import\nstatement each time it runs, and compiling can be slow. Precompiling to\nbytecode with the compile built-in may help for code strings run many times,\nbut in most cases, it’s probably simpler and may run quicker to use the built-in\n__import__ function to import from a name string. The effect is similar, but\n__import__ returns the module object—assign it to a name to keep it:\n>>> modname = 'string'\n>>> string = __import__(modname)\n>>> string\n<module 'string' from '/…/lib/python3.12/string.py'>\nBecause imports work by invoking __import__, it loads the named module\nnormally. The newer standard-library call importlib.import_module does the\nsame job; Python’s docs describe it as a simplified wrapper around __import__",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 963,
      "chapter": null,
      "content": "for “everyday” use (though our code is growing longer as our tools are growing\nnewer):\n>>> import importlib\n>>> modname = 'string'\n>>> string = importlib.import_module(modname)\n>>> string\n<module 'string' from '/…/lib/python3.12/string.py'>\nThis call works the same as __import__ in its basic roles, but see Python’s\nmanuals for more details on both calls’ advanced usage and arguments. Python’s\ndocs also seem to prefer the newer importlib call for importing by name string,\nthough this seems subjective, either call works, and the imports system has been\na frequent morpher.\nOn callout here: both calls also work for the package imports of the prior\nchapter, but the first returns the leftmost component in a package path, and the\nsecond returns the last—in fact, this is their most prominent difference:\n>>> import importlib\n>>> __import__('email.message')\n<module 'email' from '/…/lib/python3.12/email/__init__.py'>\n>>> importlib.import_module('email.message')\n<module 'email.message' from '/…/lib/python3.12/email/message.py'>\nThe importlib call also works for a package-relative import string (with\nleading dots), if also passed the string name of a package path from which to\nresolve the import. See Chapter 24 for the story of package imports.\nExample: Transitive Module Reloads\nTo tie together and apply some of the topics we’ve studied, this section develops\na module tool that serves as a larger case study to close out this chapter and part.\nWe explored module reloads in Chapter 23, as a way to pick up changes in code\nwithout stopping and restarting a program or REPL. When reloading a module,\nthough, Python reloads only that particular module’s file; it doesn’t\nautomatically reload modules that the file being reloaded happens to import.\nFor example, if you reload some module A, and A imports modules B and C, the",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 964,
      "chapter": null,
      "content": "reload applies only to A—not to B and C. The statements inside A that import B\nand C are rerun during the reload, but they just fetch the already loaded B and C\nmodule objects (assuming they’ve been imported before). In abstract code,\nhere’s the file A.py:\n# A.py\nimport B                   # Not reloaded when A is!\nimport C                   # Just imports of already loaded modules: no-ops\n$ python3\n>>> …import and use A…\n>>> from importlib import reload\n>>> reload(A)\nBy default, this means that you cannot depend on reloads to pick up changes in\nall the modules in your program transitively. Instead, you must use multiple\nreload calls to update the subcomponents independently. This can require\nsubstantial work for large systems you’re testing interactively. You can design\nyour systems to reload their subcomponents automatically by adding reload\ncalls in parent modules like A, but this complicates the code.\nA recursive reloader\nA better approach is to write a general tool to do transitive reloads automatically,\nby scanning a module’s __dict__ attributes dictionary and checking the type of\neach attribute’s value to find nested modules to reload. Such a utility function\ncould call itself recursively to navigate arbitrarily shaped and deep import-\ndependency chains. The module __dict__ was introduced in Chapter 23 and\nemployed by mydir.py earlier, recursion was explored in Chapter 19, and the\ntype call was presented in Chapter 9; we just need to combine these tools for\nthis new role.\nTo this end, the module reloadall.py listed in Example 25-7 defines a\nreload_all function that automatically reloads a module, every module that the\nmodule imports, and so on, all the way to the bottom of each import chain. It\nuses a dictionary to keep track of already reloaded modules, recursion to walk\nthe import chains, and the standard library’s types module, which simply\npredefines type results for built-in types like modules. Its visited dictionary",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 965,
      "chapter": null,
      "content": "avoids repeats when imports are recursive or redundant (module objects are\nimmutable, and so can be dictionary keys); as we saw in Chapters 5 and 8, a set\ncould work similarly (and will in rewrites ahead).\nExample 25-7. reloadall.py\n\"\"\"\nreloadall.py: transitively reload nested modules.\nCall reload_all with one or more imported modules as arguments.\nThese modules, and all the modules they import, are reloaded.\n\"\"\"\nimport types\nfrom importlib import reload\ndef status(module):\n   print('reloading', module.__name__)\ndef tryreload(module):\n   try:\n       reload(module)                                   # Imports might fail\n   except:\n       print('FAILED:', module)\ndef transitive_reload(module, visited):\n   if not module in visited:                            # Trap cycles, duplicates\n       status(module)                                   # Reload this module\n       tryreload(module)                                # And visit children\n       visited[module] = True\n       for attrobj in module.__dict__.values():         # For all attrs in mod\n           if type(attrobj) == types.ModuleType:        # Recur if nested module\n               transitive_reload(attrobj, visited)\ndef reload_all(*args):\n   visited = {}                                         # Main entry point\n   for arg in args:                                     # For all passed in\n       if type(arg) == types.ModuleType:\n           transitive_reload(arg, visited)\ndef tester(reloader, modname):                           # Self-test: cmd or passed\n   import importlib, sys                                # Imports for tests only\n   if len(sys.argv) > 1:                                # Command-line argument?\n       modname = sys.argv[1]\n   module = importlib.import_module(modname)            # Import by name string\n   reloader(module)                                     # Test passed-in reloader\nif __name__ == '__main__':\n   tester(reload_all, 'reloadall')                      # Test: reload self or arg",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 966,
      "chapter": null,
      "content": "Besides namespace dictionaries, this script makes use of other tools we’ve\nstudied before: it includes a __name__ test to launch self-test code when run as a\ntop-level script only, and its tester function uses sys.argv to inspect\ncommand-line arguments and importlib to import a module by name string\npassed in as a function or command-line argument. Review earlier coverage for\nmore info if needed.\nOne curious bit: notice how this code’s tryreload wraps the basic reload call\nin a try statement to catch exceptions. Reloads may fail for many reasons, and\nit’s best to be defensive when using system interfaces. As you’ll see in a\nmoment, for example, an unreloadable monitoring module added to sys in\nPython 3.12 would otherwise crash the reloader. The try was previewed in\nChapter 10 and will be covered in full in Part VII.\nTesting recursive reloads\nTo use this module normally, import its reload_all function and pass it an\nalready loaded module object—just as you would for the built-in reload\nfunction. Like reload, its module argument is usually obtained by a top-level\nimport; as we’ve seen, sys.modules fetches work too, but modules accessed\nonly by from don’t apply.\nTo test first, run the module standalone. Its tester function runs a passed-in\nreloader on a module imported by name string—which is taken from a\ncommand-line argument if present, else a passed-in name. In this mode, the\nmodule’s self-test code calls tester to run reload_all on its own imported\nmodule by default if no command-line arguments are used (its own name is not\ndefined in the file without an import):\n$ python3 reloadall.py\nreloading reloadall\nreloading types\nWith a command-line argument, the tester instead reloads the listed module by\nits name string—in the following, the Chapter 21 folder’s benchmark module we\ncoded in Example 21-8. To run this live, you need both the reloader module here\nand the module it reloads. One way to handle this is to add Chapter 21’s code",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 967,
      "chapter": null,
      "content": "folder to your PYTHONPATH setting per Chapter 22. Copying files in either\ndirection is subpar, and simply running in the Chapter21 folder won’t work\nbecause the reloader script’s Chapter25 folder is “home.” Note that we give a\nmodule name in this mode, not a filename; because the script imports the module\nusing the search path just like import, the .py extension is omitted:\n$ pwd                                      # In Chapter 25's code folder\n/Users/me/…/LP6E/Chapter25\n$ export PYTHONPATH=../Chapter21           # Extend path: your shell may vary\n$ python3 reloadall.py pybench             # Import+reload Chapter 21 module\nreloading pybench\nreloading sys\nreloading sys.monitoring\nFAILED: <module 'sys.monitoring'>\nreloading os\nreloading abc\nreloading stat\nreloading posixpath\nreloading genericpath\nreloading time\nreloading timeit\nreloading gc\nreloading itertools\nMore usefully, we can also deploy this module at the interactive prompt. This\nworks like the built-in reload, but adds recursive reloads for the module or\nmodules passed—here, for standard-library modules:\n$ python3\n>>> from reloadall import reload_all       # Reload stdlib modules in REPL mode\n>>> import os, tkinter\n>>> reload_all(os)\nreloading os\nreloading abc\nreloading sys\nreloading sys.monitoring\nFAILED: <module 'sys.monitoring'>\nreloading stat\nreloading posixpath\nreloading genericpath\n>>> reload_all(tkinter)\nreloading tkinter\nreloading collections\nreloading collections.abc",
      "content_length": 1456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 968,
      "chapter": null,
      "content": "…etc…\nreloading _sre\nreloading functools\nreloading copyreg\nIn either mode, the reloader also works on module packages—here, for the\nstandard library’s email package:\n$ python3 reloadall.py email.message       # Import+reload a stdlib package\nreloading email.message\nreloading binascii\nreloading re\n…etc…\n$ python3\n>>> from reloadall import reload_all       # Same, but in REPL mode\n>>> import email.message\n>>> reload_all(email.message)\nreloading email.message\nreloading binascii\nreloading re\n…etc…\nThe following runs the reloader on the dir1.dir2.mod path we coded in “Basic\nPackage Structure”. All items in the path are reloaded from a package root, and a\nsys.path mod in the REPL gives import access to another chapter’s code folder\n—much like the PYTHONPATH setting used earlier (again, per Chapter 22):\n>>> import sys\n>>> sys.path.append('../Chapter24')        # Extend the search path manually\n>>> import dir1.dir2.mod                   # A package in Chapter 24's folder\nRunning dir1.__init__.py\nRunning dir1.dir2.__init__.py\nLoading dir1.dir2.mod\n>>> reload_all(dir1)                       # Reloads all on path: mods in mods\nreloading dir1\nRunning dir1.__init__.py\nreloading dir1.dir2\nRunning dir1.dir2.__init__.py\nreloading dir1.dir2.mod\nLoading dir1.dir2.mod\nFinally, here is a simple session that demos the effect of normal versus transitive",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 969,
      "chapter": null,
      "content": "reloads—changes made to the two nested files are not picked up by reloads\nunless our transitive utility is used (files are listed inline here for brevity):\n# File ra.py \nimport rb\nX = 1\n# File rb.py\nimport rc\nY = 2\n# File rc.py\nZ = 3\n$ python3\n>>> import ra\n>>> ra.X, ra.rb.Y, ra.rb.rc.Z                 # Three-level import chain\n(1, 2, 3)\nNow, without stopping Python, change all three files’ assignment values, save\nthe files, and reload back in the REPL:\n>>> from importlib import reload\n>>> reload(ra)                                # Built-in reload is top-level only\n<module 'ra' from '/…/LP6E/Chapter25/ra.py>\n>>> ra.X, ra.rb.Y, ra.rb.rc.Z\n(111, 2, 3)\n>>> from reloadall import reload_all\n>>> reload_all(ra)                            # Normal usage mode\nreloading ra\nreloading rb\nreloading rc\n>>> ra.X, ra.rb.Y, ra.rb.rc.Z                 # Reloads all nested modules too\n(111, 222, 333)\nThis is similar to the preceding package-path reload, but the imports here are\nexplicit; module nesting in packages is implied. Study the reloader’s code and\nresults for more on its operation. The next section exercises its tools further.\nAlternative codings\nFor all the recursion fans in the audience (and we know who we are),\nExample 25-8 lists an alternative recursive coding for the original function in",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 970,
      "chapter": null,
      "content": "Example 25-7. This new version uses a set instead of a dictionary to detect\nrepeats and cycles, is marginally more direct because it eliminates a top-level\nloop, and serves to illustrate recursive coding in general. Compare with the\noriginal to see how this differs.\nThis version also gets some of its work for free from the original; in fact, this\nmodule essentially extends the original to replace just the parts that vary. Notice\nhow it calls the original version’s tester, passing in the reload_all defined\nhere—which ensures that this module’s reloader is run when this script is\nlaunched in standalone mode.\nExample 25-8. reloadall2.py\n\"\"\"\nreloadall2.py: transitively reload nested modules.\nAlternative coding: recursive, refactored.\n\"\"\"\nimport types\nfrom reloadall import status, tryreload, tester\ndef transitive_reload(objects, visited):\n   for obj in objects:\n       if type(obj) == types.ModuleType and obj not in visited:\n           status(obj)\n           tryreload(obj)                          # Reload this, recur to attrs\n           visited.add(obj)\n           transitive_reload(obj.__dict__.values(), visited)\ndef reload_all(*args):\n   transitive_reload(args, set())\nif __name__ == '__main__':\n   tester(reload_all, 'reloadall2')                # Test: reload myself or arg\nAs we saw in Chapter 19, there is usually an explicit stack or queue equivalent to\nrecursive functions, which may be preferable in some contexts. Example 25-9\nlists one such transitive reloader—it uses a stack instead of recursion, and a set\nto skip repeats and cycles. On each loop, all of a new module’s attribute values\nare added to the end of the objects stack and filtered later, and this is repeated\nuntil the list of candidate objects becomes empty. A generator expression could\nfilter out nonmodules in the extend call to avoid some pops, but this would be\nmore complex.",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 971,
      "chapter": null,
      "content": "Because it both pops and adds items at the end of its list, this version is stack-\nbased, though the order of both pushes and dictionary values influences the order\nin which it reaches and reloads modules—it visits submodules in namespace\ndictionaries from right to left, unlike the left-to-right order of the recursive\nversions (trace through the code to see how). We could change this to match by\nreversing values, but reload order is unimportant.\nExample 25-9. reloadall3.py\n\"\"\"\nreloadall3.py: transitively reload nested modules.\nAlternative coding: nonrecursive, explicit stack.\n\"\"\"\nimport types\nfrom reloadall import status, tryreload, tester\ndef transitive_reload(objects, visited):\n   while objects:\n       next = objects.pop()                        # Delete next item at end\n       if (type(next) == types.ModuleType          # Is it a module object?\n           and next not in visited):               # Not already reloaded?\n           status(next)                            # Reload this, push attrs\n           tryreload(next)\n           visited.add(next)\n           objects.extend(next.__dict__.values())\ndef reload_all(*args):\n   transitive_reload(list(args), set())\nif __name__ == '__main__':\n   tester(reload_all, 'reloadall3')                # Test: reload myself or arg\nIf the recursion and nonrecursion used in these examples is confusing, see the\ndiscussion of recursive functions in Chapter 19 for more background on the\nsubject.\nTesting reload variants\nTo prove that these two alternative reloaders work the same as the original, let’s\ntest all three of our reloader variants. Thanks to their common testing function,\nwe can run all three from a command line both with no arguments to test the\nmodule reloading itself, and with the name of a module to be reloaded listed on\nthe command line (in sys.argv):",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 972,
      "chapter": null,
      "content": "$ python3 reloadall.py\nreloading reloadall\nreloading types\n$ python3 reloadall2.py\nreloading reloadall2\nreloading types\n$ python3 reloadall3.py\nreloading reloadall3\nreloading types\nThough it’s hard to see here, we really are testing the individual reloader\nalternatives—each of these tests shares a common tester function but passes it\nthe reload_all from its own file. Here are the variants reloading the tkinter\nGUI module and all the modules its imports reach; again, the third’s reloads\norder varies:\n$ python3 reloadall.py tkinter\nreloading tkinter\nreloading collections\nreloading collections.abc \n…etc…\n$ python3 reloadall2.py tkinter\nreloading tkinter\nreloading collections\nreloading collections.abc \n…etc…\n$ python3 reloadall3.py tkinter\nreloading tkinter\nreloading re\nreloading copyreg \n…etc…\nAs usual, we can test interactively, too, by importing and calling either a\nmodule’s main reload entry point with a module object, or the testing function\nwith a reloader function and module name string:\n$ python3\n>>> import reloadall, reloadall2, reloadall3\n>>> import tkinter\n>>> reloadall.reload_all(tkinter)                           # Normal use case\nreloading tkinter\nreloading collections\nreloading collections.abc",
      "content_length": 1223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 973,
      "chapter": null,
      "content": "…etc…\n>>> reloadall.tester(reloadall2.reload_all, 'tkinter')      # Testing utility\nreloading tkinter\nreloading collections\nreloading collections.abc\n…etc…\n>>> reloadall.tester(reloadall3.reload_all, 'reloadall3')   # Mimic self-test code\nreloading reloadall3\nreloading types\nFinally, as noted, the third reloader’s results will generally vary by order; reload\norder in all reloaders depends on namespace dictionary ordering (which, as\nwe’ve learned is deterministically ordered by key insertion time today), but the\nlast also relies on the order in which items are added to its stack. To ensure that\nall three are reloading the same modules irrespective of the order in which they\ndo so, we can use sets or sorts to test for order-neutral equality of their printed\nmessages—obtained here by running shell commands with the os.popen utility\nwe used in Chapter 21:\n>>> import os\n>>> res1 = os.popen('python3 reloadall.py tkinter').readlines()\n>>> res2 = os.popen('python3 reloadall2.py tkinter').readlines()\n>>> res3 = os.popen('python3 reloadall3.py tkinter').readlines()\n>>> res1[:3]\n['reloading tkinter\\n', 'reloading collections\\n', 'reloading collections.abc\\n']\n>>> res3[:3]\n['reloading tkinter\\n', 'reloading re\\n', 'reloading copyreg\\n'] \n>>> res1 == res2, res2 == res3\n(True, False) \n>>> set(res2) == set(res3)                  # Order-neutral equality\nTrue\n>>> sorted(res2) == sorted(res3)            # Ditto\nTrue\nRun these scripts, study their code, and experiment on your own for more\ninsight; these are the sort of importable tools you might want to add to your own\nsource code library. Watch for a similar testing technique in the coverage of class\ntree listers in Chapter 31, where we’ll apply it to passed class objects and extend\nit further.\nCaveats: keep in mind that all the transitive reloaders, like the reload built-in",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 974,
      "chapter": null,
      "content": "that they use, rely on the fact that module reloads update module objects in\nplace, such that all references to those modules in any namespace will see the\nupdated version automatically. Because from importers copy names out, they are\nnot updated by reloads, transitive or not. Perhaps worse, modules imported only\nby from won’t be reloaded, because they do not exist in any importer’s\nnamespace scanned. Doing better may require either source code analysis or\nimport customization.\nTool impacts like this are perhaps another reason to prefer import to from—\nwhich brings us to the end of this chapter and part, and the standard set of\nwarnings for this part’s topic.\nModule Gotchas\nIn this section, we’ll explore the usual collection of boundary cases that can\nmake life interesting for Python beginners. Some are review here, and a few are\nso obscure that coming up with representative examples can be a challenge, but\nmost illustrate something important about the language.\nModule Name Clashes: Package and Package-Relative\nImports\nIf you have two modules of the same name, you may only be able to import one\nof them—by default, the one whose directory is leftmost in the sys.path\nmodule search path will always be chosen. This isn’t an issue if the module you\nprefer is in your top-level script’s directory; since that is always first in the\nmodule path, its contents will be located first automatically. For cross-directory\nimports, however, the linear nature of the module search path means that same-\nnamed files can clash.\nTo fix this, either avoid same-named files or use the package imports feature of\nChapter 24. If you really need to get to two files of the same name, the latter is\nthe solution: structure your source files in subdirectories, such that package-\nimport directory names make the module references unique. As long as the\nenclosing package directory names are unique, you’ll be able to access either or\nboth of the same-named modules.",
      "content_length": 1960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 975,
      "chapter": null,
      "content": "This issue can also crop up if you accidentally use a name for a module of your\nown that happens to be the same as a standard-library module you need—your\nlocal module in the program’s home directory (or another directory early in the\nmodule path) can hide and replace the library module.\nTo fix that, either avoid using the same name as another module you need or\nstore your modules in a package directory and use the package-relative import\nmodel of Chapter 24. In this model, normal imports skip the package directory to\naccess the library’s version, but special dotted import statements can still select\nthe local version of the module.\nStatement Order Matters in Top-Level Code\nAs we’ve seen, when a module is first imported (or later reloaded), Python\nexecutes its statements one by one, from the top of the file to the bottom. This\nhas a few subtle implications regarding forward references that are worth\nunderscoring here:\nCode at the top level of a module file (not nested in a function) runs as\nsoon as Python reaches it during an import; because of that, it cannot\nreference names assigned lower in the file.\nCode inside a function body doesn’t run until the function is called;\nbecause names in a function aren’t resolved until the function actually\nruns, they can usually reference names anywhere in the file.\nIn other words, forward references are usually only a concern in top-level\nmodule code that executes immediately; functions can reference names\narbitrarily. Here’s a file that illustrates forward reference dos and don’ts:\nfunc1()                           # Error: func1 not yet assigned\ndef func1():\n    print(func2())                # OK: func2 looked up later\nfunc1()                           # Error: func2 not yet assigned\ndef func2():\n    return \"Hello\"\nfunc1()                           # OK: func1 and func2 assigned",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 976,
      "chapter": null,
      "content": "When this file is imported (or run as a standalone program), Python executes its\nstatements from top to bottom. The first call to func1 fails because the func1\ndef hasn’t run yet. The call to func2 inside func1 works as long as func2’s def\nhas been reached by the time func1 is called—and it hasn’t when the second\ntop-level func1 call is run. The last call to func1 at the bottom of the file works\nbecause func1 and func2 have both been assigned.\nMixing defs with top-level code is not only difficult to read, but it’s also\ndependent on statement ordering. As a rule of thumb, if you need to mix\nimmediate code with defs, put your defs at the top of the file and your top-level\ncode at the bottom. That way, your functions are guaranteed to be defined and\nassigned by the time Python runs the code that uses them.\nfrom Copies Names but Doesn’t Link\nAlthough it’s commonly used, the from statement is the source of a variety of\npotential gotchas in Python. As we’ve seen, the from statement is really an\nassignment to names in the importer’s scope—a name-copy operation, not a\nname aliasing. The implications of this are the same as for all assignments in\nPython, but they’re especially subtle for names that live in different files. As a\nrefresher, suppose we define the simple module nested.py in Example 25-10.\nExample 25-10. nested.py\nX = 99\ndef printer(): print(X)\nIf we import its two names using from in another module (or the REPL, which\nstands in for one), we get copies of those names, not links to them. Changing a\nname in the importer resets only the binding of the local version of that name,\nnot the name in nested1.py:\n>>> from nested import X, printer     # Copy names out\n>>> X = 88                            # Changes my X only!\n>>> printer()                         # nested1's X is still 99\n99\nIf we instead use import to get the whole module and assign to a qualified\nname, we change the name in nested1 (the module’s loaded image, not its",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 977,
      "chapter": null,
      "content": "source code). Attribute qualification directs Python to a name in the module\nobject, rather than a name in the importer:\n>>> import nested                     # Get module as a whole\n>>> nested.X = 88                     # Change nested1's X\n>>> nested.printer()\n88\nTakeaway: changes to names obtained with from don’t impact any other module.\nAs covered in Chapter 23, changes to mutable objects shared by names copied\nwith from can impact other modules, but name changes cannot.\nfrom * Can Obscure the Meaning of Variables\nThis was mentioned earlier but its demo was saved for here. Because you don’t\nlist the variables you want when using the from * statement form, it can\naccidentally overwrite names you’re already using in your scope. Worse, it can\nmake it difficult to determine where a variable comes from. This is especially\ntrue if the from * form is used on more than one imported file.\nFor example, if you use from * on three modules in the following, you’ll have\nno way of knowing what a raw function call really means, short of searching all\nthree external module files—all of which may be in other directories:\n>>> from module1 import *          # May overwrite names silently\n>>> from module2 import *          # No way to tell what we get\n>>> from module3 import *          # No way to see name origins\n>>> func()                         # Huh?\nThe solution is simply not to do this: list the attributes you want in most from\nstatements, and use at most one from * per file. That way, any undefined names\nmust by deduction be in the module named in the single from *. You can avoid\nthe issue altogether if you always use import instead of from, but that advice is\ntoo harsh; like much else in programming, from is a convenient tool if used\nwisely. Even this example isn’t an absolute evil—it’s OK for a program to use\nthis technique to collect names in a single module for convenience, as long as\nit’s well known.",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 978,
      "chapter": null,
      "content": "reload May Not Impact from Imports\nHere’s another from-related gotcha: as discussed previously, because from\ncopies (assigns) names when run, there’s no link back to the modules where the\nnames came from. Names imported with from simply become references to\nobjects, which happen to have been referenced by the same names in the\nimportee when the from ran.\nBecause of this behavior, reloading the module of origin has no effect on clients\nthat import its names using from. That is, the client’s names will still reference\nthe original objects fetched with from, even if the names in the original module\nare later reset. Here’s the story in abstract code:\nfrom module import X               # X may not reflect any module reloads\n…\nfrom importlib import reload\nreload(module)                     # Changes module, but not my names\nX                                  # Still references old object!\nTo make reloads more effective, use import and name qualification instead of\nfrom. Because qualifications always go back to the module, they will find the\nnew bindings of module names after reloading has updated the module’s content\nin place:\nimport module                      # Get module, not names\n…\nfrom importlib import reload\nreload(module)                     # Changes module in place\nmodule.X                           # Get current X: reflects module reloads\nThis is why our transitive reloader earlier in this chapter doesn’t apply to names\nfetched with from, only import; again, if you’re going to use reloads, you’re\nprobably better off with import.\nreload, from, and Interactive Testing\nIn fact, the prior gotcha is even more nuanced than it appears. Chapter 3 warned\nthat it’s usually better not to launch programs with imports and reloads because\nof the complexities involved. Things get even worse when from is brought into",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 979,
      "chapter": null,
      "content": "the mix. Python beginners most often stumble onto its issues in scenarios like\nthis—imagine that after opening a module file in a text edit window, you launch\nan interactive session to load and test your module with from:\nfrom module import function\nfunction(1, 2, 3)\nFinding a bug, you jump back to the edit window, make a change, and try to\nreload the module this way:\nfrom importlib import reload\nreload(module)\nThis doesn’t work, because the from statement assigned only the name\nfunction, not module. To refer to the module in a reload, you have to first bind\nits name with an import statement at least once:\nfrom importlib import reload\nimport module\nreload(module)\nfunction(1, 2, 3)\nBut this doesn’t quite work either—reload updates the module object in place,\nbut as discussed in the preceding section, names like function that were copied\nout of the module in the past still refer to the old objects; in this instance,\nfunction is still the original version of the function. To really get the new\nfunction, you must refer to it as module.function after the reload, or rerun the\nfrom:\nfrom importlib import reload\nimport module\nreload(module)\nfrom module import function        # Or give up and use module.function()!\nfunction(1, 2, 3)\nNow, the new version of the function will finally run, but it seems an awful lot of\nwork to get there.\nAs you can see, there are problems inherent in using reload with from: not only",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 980,
      "chapter": null,
      "content": "do you have to remember to reload after imports, but you also have to remember\nto rerun your from statements after reloads. This is complex enough to trip up\neven an expert once in a while. In fact, the situation grew even worse with\nPython 3.X, because you must also remember to import reload itself!\nThe short story is that you should not expect reload and from to play together\nnicely. Again, the best policy is not to combine them at all—use reload with\nimport, or launch your programs other ways, as suggested in Chapter 3: using\nmenu options in IDLE, file icon clicks, system command lines, the exec built-in\nfunction, or other.\nRecursive from Imports May Not Work\nThe most bizarre (and, thankfully, obscure) gotcha has been saved for last.\nBecause imports execute a file’s statements from top to bottom, you need to be\ncareful when using modules that import each other. This is often called recursive\nimports, but the recursion doesn’t really occur (in fact, circular may be a better\nterm here)—such imports won’t get stuck in infinite importing loops. Still,\nbecause the statements in a module may not all have been run when it imports\nanother module, some of its names may not yet exist.\nIf you use import to fetch the module as a whole, this probably doesn’t matter;\nthe module’s names won’t be accessed until you later use qualification to fetch\ntheir values, and by that time the module is likely complete. But if you use from\nto fetch specific names, you must bear in mind that you will only have access to\nnames in that module that have already been assigned when a recursive import is\nkicked off.\nAs a demo of this phenomenon, consider the modules recur1 and recur2, in\nExamples 25-11 and 25-12.\nExample 25-11. recur1.py\nX = 1\nimport recur2            # Run recur2 now if it doesn't exist\nY = 2\nExample 25-12. recur2.py\nfrom recur1 import X     # OK: X already assigned\nfrom recur1 import Y     # Error: Y not yet assigned",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 981,
      "chapter": null,
      "content": "Module recur1 assigns a name X and then imports recur2 before assigning the\nname Y. At this point, recur2 can fetch recur1 as a whole with an import—it\nalready exists in Python’s internal modules table, which makes it importable, and\nalso prevents the imports from looping. But if recur2 uses from, it will be able\nto see only the name X; the name Y, which is assigned below the import in\nrecur1, doesn’t yet exist, so you get an error:\n$ python3\n>>> import recur1\nImportError: cannot import name 'Y' from partially initialized module 'recur1' \n(most likely due to a circular import) (/…/LP6E/Chapter25/recur1.py)\nPython avoids rerunning recur1’s statements when they are imported\nrecursively from recur2 (otherwise the imports would send the script into an\ninfinite loop that might require a Ctrl+C solution or worse), but recur1’s\nnamespace is incomplete when it’s imported by recur2.\nThe solution? Don’t use from in recursive imports (no, really!). Python won’t get\nstuck in a cycle if you do, but your programs will once again be dependent on\nthe order of the statements in the modules. In fact, there are two ways out of this\ngotcha:\nYou can usually eliminate import cycles like this by careful design—\nmaximizing cohesion and minimizing coupling per the start of this\nchapter are good first steps.\nIf you can’t break the cycles completely, postpone module name\naccesses by using import and attribute qualification (instead of from\nand direct names), or by running your froms either inside functions\n(instead of at the top level of the module) or near the bottom of your file\nto defer their execution.\nThere is additional perspective on this issue in the exercises at the end of this\nchapter—which we’ve officially reached.",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 982,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter surveyed module topics, some of which qualify as advanced. We\nstudied data-hiding techniques, enabling new language features with the\n__future__ module, the __name__ usage-mode variable, transitive reloads,\nimporting by name strings, and more. We also explored module design issues,\nwrote some substantial programs, and looked at common mistakes related to\nmodules to help you avoid them in your code.\nThe next chapter begins our exploration of Python’s class—its object-oriented\nprogramming tool. Much of what we’ve covered in the last few chapters will\napply there, too: classes live in modules and are namespaces as well, but they\nadd an extra component to attribute lookup called inheritance search. As this is\nthe last chapter in this part of the book, however, before we dive into classes, be\nsure to work through this part’s set of lab exercises. And before that, here is this\nchapter’s quiz to review the topics covered here.\nTest Your Knowledge: Quiz\n1. What is significant about variables at the top level of a module whose\nnames begin with a single underscore?\n2. What does it mean when a module’s __name__ variable is the string\n'__main__'?\n3. How might you step through all the attributes in a module with a loop?\n4. If the user interactively types the name of a module to test, how can\nyour code import it?\n5. If the module __future__ allows us to import from the future, can we\nalso import from the past?\nTest Your Knowledge: Answers",
      "content_length": 1477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 983,
      "chapter": null,
      "content": "1. Variables at the top level of a module whose names begin with a single\nunderscore are not copied out to the importing scope when the from *\nstatement form is used. They can still be accessed by an import or the\nnormal from statement form, though. The __all__ list is similar, but\nthe logical converse; its contents are the only names that are copied out\nfor a from *.\n2. If a module’s __name__ variable is the string '__main__', it means that\nthe file is being executed as a top-level script instead of being imported\nfrom another file in the program. That is, the file is being used as a\nprogram, not a library. This usage mode variable supports dual-mode\ncode and tests.\n3. By using the module’s built-in __dict__ attribute. This is a normal\ndictionary that holds all of the module’s attributes, so code can iterate\nover its keys, values, or key/value pairs. When needed, attribute values\ncan also be fetched for string names by indexing __dict__, or calling\nthe getattr built-in function.\n4. User input usually comes into a script as a string; to import the\nreferenced module given its string name, you can build and run an\nimport statement with exec, or pass the string name in a call to the\n__import__ or importlib.import_module functions.\n5. No, we can’t import from the past in Python. We can install (or\nstubbornly use) an older version of the language, but the latest Python is\ngenerally the best Python (with apologies to 2.X fans in the audience).",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 984,
      "chapter": null,
      "content": "Test Your Knowledge: Part V Exercises\nSee “Part V, Modules and Packages” in Appendix B for the solutions.\n1. Import basics: Write a program that counts the lines and characters in a\nfile (similar in spirit to part of what wc does on Unix). With your text\neditor, code a Python module called mymod.py that exports three top-\nlevel names:\nA countLines(name) function that reads an input file\nspecified by string name, and counts the number of lines in it\n(hint: file.readlines does most of the work for you, and len\ndoes the rest, though you could count with for and file\niterators to support massive files too).\nA countChars(name) function that reads an input file and\ncounts the number of characters in it (hint: file.read returns a\nsingle string, which may be used in similar ways).\nA test(name) function that calls both counting functions with\na given input filename. Such a filename generally might be\npassed in, hardcoded, input from a user like you with the input\nbuilt-in function, or pulled from a command line via the\nsys.argv list demoed in this chapter’s reloadall.py example;\nfor now, you can assume it’s a passed-in function argument.\nAll three mymod functions should expect a filename string to be passed\nin. If you type more than two or three lines per function, you’re working\nmuch too hard—use the hints given!\nNext, test your module interactively, using import and attribute\nreferences to fetch your exports. Does your PYTHONPATH need to include\nthe directory where you created mymod.py? Try running your module\non itself: for example, test('mymod.py'). Note that test opens the\nfile twice; if you’re feeling ambitious, you may be able to improve this\nby passing an open file object into the two count functions (hint:",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 985,
      "chapter": null,
      "content": "file.seek(0) is a file rewind).\n2. from/from *: Test your mymod module from exercise 1 interactively by\nusing from to load the exports directly, first by name, then using the\nfrom * variant to fetch everything.\n3. __main__: Add a line in your mymod module that calls the test function\nautomatically only when the module is run as a script, not when it is\nimported. The line you add will probably test the value of __name__ for\nthe string '__main__', as shown in this chapter. Try running your\nmodule from the system command line or other program-launch\nscheme; then, import the module and test its functions interactively.\nDoes it still work in both modes?\n4. Nested imports: Write a second module, myclient.py, that imports mymod\nand tests its functions; then run myclient from the system command\nline or other scheme. If myclient uses from to fetch from mymod, will\nmymod’s functions be accessible from the top level of myclient? What\nif it imports with import instead? Try coding both variations in\nmyclient and test interactively by importing myclient and inspecting\nits __dict__ attribute.\n5. Package imports: Import your mymod.py file from a package. Create a\nsubdirectory called mypkg nested in a directory on your module import\nsearch path, copy or move the mymod.py module file you created in\nexercise 1 or 3 into the new directory, and try to import it with a\npackage import of the form import mypkg.mymod and call its functions.\nTry to fetch your counter functions with a from too.\nThis works on all Python platforms (that’s part of the reason Python\nuses “.” as a path separator). The package directory you create can be\nsimply a subdirectory of the one you’re working in; if it is, it will be\nfound via the home directory component of the search path, and you\nwon’t have to configure your path.\nYou also don’t need an __init__.py file in the package directory your\nmodule was moved into to make this go, but make one with some basic",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 986,
      "chapter": null,
      "content": "prints in it and see if they run on each import or reload of the package\nfolder. Finally, also copy mymod.py to the package folder’s\n__main__.py and invoke it by running the folder itself; does it make\nsense to do that here? Can you still run the nested mymod.py module\nitself?\n6. Reloads: Experiment with module reloads: if you haven’t already,\nperform the tests in Chapter 23’s changer.py (Example 23-10), changing\nthe called function’s message or behavior repeatedly, without stopping\nthe Python REPL. Depending on your device, you might edit changer\nin another window, or suspend the Python interpreter and edit in the\nsame window (on Unix, a Ctrl+Z key combination usually suspends the\ncurrent process, and an fg command later resumes it, though a separate\ntext-editor window can work just as well).\n7. Circular imports: In the section on recursive (a.k.a. circular) import\ngotchas, importing recur1 raised an error. But if you restart Python and\nimport recur2 interactively, the error doesn’t occur—test this and see\nfor yourself. Why do you think it works to import recur2, but not\nrecur1? (Hint: Python records new modules before running their code,\nand later imports fetch the module first, whether the module is\n“complete” yet or not.)\nNow, run recur1 as a top-level script file: python3 recur1.py. Do\nyou get the same error that occurs when recur1 is imported\ninteractively? Why? (Hint: when modules are run as programs, they\naren’t imported, so this case has the same effect as importing recur2\ninteractively; recur2 is the first module imported.) What happens when\nyou run recur2 as a script? Circular imports are uncommon in practice.\nOn the other hand, if you can understand why they are a potential\nproblem, you know a lot about Python’s import semantics.",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 987,
      "chapter": null,
      "content": "Part VI. Classes and OOP",
      "content_length": 24,
      "extraction_method": "OCR"
    },
    {
      "page_number": 988,
      "chapter": null,
      "content": "Chapter 26. OOP: The Big Picture\nSo far in this book, we’ve been using the term “object” generically. Really, the\ncode written up to this point has been object-based—we’ve passed objects\naround our scripts, used them in expressions, called their methods, and so on.\nFor our code to qualify as being truly object-oriented (OO), though, our objects\nwill generally need to also participate in something called an inheritance\nhierarchy.\nThis chapter begins our exploration of the Python class—a coding structure and\ndevice used to implement new kinds of objects in Python that support\ninheritance. Classes are Python’s main object-oriented programming (OOP) tool,\nso we’ll also study OOP basics along the way in this part of the book. OOP\noffers a different and often more effective way of programming. Like functions,\nwe can use classes to factor code to minimize redundancy. Unlike functions,\nclasses make it easy to write new programs by customizing existing code instead\nof changing it in place.\nIn Python, classes are created with a new statement: the class. As you’ll see,\nthe objects defined with classes can look a lot like the built-in object types we\nemployed earlier in the book. In fact, classes really just apply and extend the\nideas we’ve already covered; roughly, they are packages of functions that use\nand process built-in objects. Classes, though, are designed to create and manage\nnew objects, and support inheritance—a mechanism of code customization and\nreuse above and beyond anything we’ve seen so far.\nOne note up front: in Python, OOP is entirely optional, and you don’t need to use\nclasses just to get started. You can get plenty of work done with simpler\nconstructs such as functions, or even simple top-level script code. Because using\nclasses well requires some up-front planning, they tend to be of more interest to\npeople who work in strategic mode (doing long-term product development) than\nto people who work in tactical mode (where time is in very short supply).\nStill, as you’ll see in this part of the book, classes turn out to be one of the most\nuseful tools Python provides. When used well, classes can actually cut",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 989,
      "chapter": null,
      "content": "development time radically. They’re also employed in popular Python libraries,\nso most Python programmers will usually find at least a working knowledge of\nclass basics helpful.\nWhy Use Classes?\nRemember when this book told you that programs “do things with stuff” in\nChapters 4 and 10? In simple terms, classes are just a way to define new sorts of\nstuff, reflecting real objects in a program’s domain. For instance, suppose we\ndecide to implement that hypothetical pizza-making robot we used as an\nexample in Chapter 16. If we implement it using classes, we can model more of\nits real-world structure and relationships. Two aspects of OOP could be useful\nhere:\nInheritance\nPizza-making robots are kinds of robots, so they possess the usual robot-y\nproperties. In OOP terms, we say they “inherit” properties from the general\ncategory of all robots. These common properties need to be implemented\nonly once for the general case and can be reused in part or in full by all types\nof robots we may build in the future.\nComposition\nPizza-making robots are really collections of components that work together\nas a team. For instance, for our robot to be successful, it might need arms to\nroll dough, motors to maneuver to the oven, and so on. In OOP parlance, our\nrobot is an example of composition; it contains other objects that it activates\nto do its bidding. Each component might be coded as a class, which defines\nits own behavior and relationships.\nWhile you may never build pizza-making robots, general OOP ideas like\ninheritance and composition apply to any application that can be decomposed\ninto a set of objects. For example, in typical GUI systems, interfaces are written",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 990,
      "chapter": null,
      "content": "as collections of widgets—buttons, labels, and so on—which are all drawn when\ntheir container is drawn (composition). Moreover, we may be able to write our\nown custom widgets—buttons with unique fonts, labels with new color schemes,\nand the like—which are specialized versions of more general interface devices\n(inheritance).\nFrom a more concrete programming perspective, classes are Python program\nunits, just like functions and modules: they are another compartment for\npackaging logic and data. In fact, classes also define new namespaces, much like\nmodules. But, compared to other program units we’ve already seen, classes have\nthree critical distinctions that make them more useful when it comes to building\nnew objects:\nMultiple instances\nClasses are essentially factories for generating one or more objects. Every\ntime we call a class, we generate a new object with a distinct namespace.\nEach object generated from a class has access to the class’s attributes and\ngets a namespace of its own for data that varies per object. This is similar to\nthe per-call state retention of Chapter 17’s closure functions, but is explicit\nand natural in classes, and is just one of the things that classes do. Classes\noffer a more complete programming solution.\nCustomization via inheritance\nClasses also support the OOP notion of inheritance: we can extend a class by\nredefining its attributes outside the class itself in new software components\ncoded as subclasses. More generally, classes can build up namespace\nhierarchies, which define names to be used by objects created from classes\nin the hierarchy. This supports multiple customizable behaviors more directly\nthan other tools.\nOperator overloading\nBy providing special protocol methods, classes can define objects that",
      "content_length": 1769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 991,
      "chapter": null,
      "content": "respond to the sorts of operations we saw at work on built-in types. For\ninstance, objects made with classes can be sliced, concatenated, indexed, and\nso on. Python provides hooks that classes can use to intercept and implement\nany built-in type operation.\nAt its base, the mechanism of OOP in Python largely boils down to just two bits\nof magic: a special first argument in functions (to receive the subject of a call)\nand inheritance attribute search (to support programming by customization).\nOther than this, the model is largely just functions that ultimately process built-in\ntypes. While not radically new, though, OOP adds an extra layer of structure that\nsupports programming better than flat procedural models. Along with the\nfunctional tools we met earlier, it represents a major abstraction step above\ncomputer hardware that helps us build more sophisticated programs.\nOOP from 30,000 Feet\nBefore we dig into what this all means in terms of code, let’s get a better handle\non the general ideas behind OOP. If you’ve never done anything object-oriented\nin your life before now, some of the terminology in this chapter may seem a bit\nperplexing on the first pass. Moreover, the motivation for these terms may be\nelusive until you’ve had a chance to study the ways that programmers apply\nthem in larger systems. OOP is as much an experience as a technology.\nAttribute Inheritance Search\nThe good news is that OOP is much simpler to understand and use in Python\nthan in some other languages like C++ or Java. As a dynamically typed scripting\nlanguage, Python removes much of the syntactic clutter and complexity that\nclouds OOP in other tools. In fact, much of the OOP story in Python boils down\nto this expression:\nobject.attribute\nWe’ve been using this expression throughout the book to access module",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 992,
      "chapter": null,
      "content": "attributes, call methods of objects, and so on. When we say this to an object that\nis derived from a class statement, however, the expression kicks off a search in\nPython—it searches a tree of linked objects, looking for the first appearance of\nattribute that it can find. When classes are involved, the preceding Python\nexpression effectively translates to the following in natural language:\nFind the first occurrence of attribute by looking in object, then in all\nclasses above it, from bottom to top and left to right.\nIn other words, attribute fetches are simply tree searches. The term inheritance is\napplied to it because objects lower in a tree inherit attributes attached to objects\nhigher in that tree. As the search proceeds from the bottom up, in a sense, the\nobjects linked into a tree are the union of all the attributes defined in all their tree\nparents, all the way up the tree.\nIn Python, this is all very literal: we really do build up trees of linked objects\nwith code, and Python really does climb this tree at runtime searching for\nattributes every time we use the object.attribute expression. To make this\nmore concrete, Figure 26-1 sketches an example of one of these trees.\nFigure 26-1. A class tree: instances (I1 and I2), a class (C1), and superclasses (C2 and C3)",
      "content_length": 1289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 993,
      "chapter": null,
      "content": "In this figure, there is a tree of five objects labeled with variables, all of which\nhave attached attributes, ready to be searched. More specifically, this tree links\ntogether three class objects (the ovals C1, C2, and C3) and two instance objects\n(the rectangles I1 and I2) into an inheritance-search tree. Notice that in the\nPython object model, classes and the instances you generate from them are two\ndistinct object types:\nClasses\nServe as instance factories. Their attributes provide behavior—data and\nfunctions—that is inherited by all the instances generated from them (e.g., a\nfunction to compute an employee’s salary from pay and hours).\nInstances\nRepresent the concrete items in a program’s domain. Their attributes record\ndata that varies per specific object (e.g., an employee’s pay rate and hours\nworked).\nIn terms of search trees, an instance inherits attributes from its class, and a class\ninherits attributes from all classes above it in the tree.\nIn Figure 26-1, we can further categorize the ovals by their relative positions in\nthe tree. We usually call classes higher in the tree (like C2 and C3) superclasses;\nclasses lower in the tree (like C1) are known as subclasses. These terms refer to\nboth relative tree positions and roles. Superclasses provide behavior shared by\nall their subclasses, but because the search proceeds from the bottom up,\nsubclasses may override behavior defined in their superclasses by redefining\nsuperclass names lower in the tree.1\nAs these last few words are really the crux of the matter of software\ncustomization in OOP, let’s expand on this concept. Suppose we build up the tree\nin Figure 26-1, and then say this:\nI2.w\nRight away, this code invokes inheritance. Because this is an",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 994,
      "chapter": null,
      "content": "object.attribute expression, it triggers a search of the tree in Figure 26-1—\nPython will search for the attribute w by looking in I2 and above. Specifically, it\nwill search the linked objects in this order:\nI2, C1, C2, C3\nand stop at the first attached w it finds (or raise an error if w isn’t found at all). In\nthis case, w won’t be found until C3 is searched because it appears only in that\nobject. In other words, I2.w resolves to C3.w by virtue of the automatic search.\nIn OOP terminology, I2 “inherits” the attribute w from C3.\nUltimately, the two instances inherit four attributes from their classes: w, x, y,\nand z. Other attribute references will wind up following different paths in the\ntree. For example:\nI1.x and I2.x both find x in C1 and stop because C1 is lower than C2.\nI1.y and I2.y both find y in C1 because that’s the only place y appears.\nI1.z and I2.z both find z in C2 because C2 is further to the left than C3.\nI2.name finds name in I2 without climbing the tree at all.\nTrace these searches through the tree in Figure 26-1 to get a feel for how\ninheritance searches work in Python.\nThe first item in the preceding list is perhaps the most important to notice—\nbecause C1 redefines the attribute x lower in the tree, it effectively replaces the\nversion above it in C2. As you’ll see in a moment, such redefinitions are at the\nheart of software customization in OOP—by redefining and replacing the\nattribute, C1 effectively customizes what it inherits from its superclasses.\nClasses and Instances\nAlthough they are technically two separate object types in the Python model, the\nclasses and instances we put in these trees are almost identical—each type’s\nmain purpose is to serve as another kind of namespace—a package of variables,\nand a place where we can attach attributes. If classes and instances therefore",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 995,
      "chapter": null,
      "content": "sound like modules, they should; however, the objects in class trees also have\nautomatically searched links to other namespace objects, and classes and\ninstances correspond to statements and calls, respectively, not entire files.\nThe primary difference between classes and instances is that classes are a kind of\nfactory for generating instances. For example, in a realistic application, we might\nhave an Employee class that defines what it means to be an employee; from that\nclass, we generate actual Employee instances. This is another difference between\nclasses and modules—we only ever have one instance of a given module in\nmemory (that’s why we have to reload a module to get its new code), but with\nclasses, we can make as many unique instances as we need.\nOperationally, classes will usually have functions attached to them (e.g.,\ncomputeSalary), and the instances will have more basic data items used by the\nclass’s functions (e.g., hoursWorked). In fact, the object-oriented model is not\nthat different from the classic data-processing model of programs plus records—\nin OOP, instances are like records with “data,” and classes are the “programs”\nfor processing those records. In OOP, though, we also have the notion of an\ninheritance hierarchy, which supports software customization better than earlier\nmodels.\nMethod Calls\nIn the prior section, we saw how the attribute reference I2.w in our example\nclass tree was translated to C3.w by the inheritance search procedure in Python.\nPerhaps just as important to understand as the inheritance of attributes, though, is\nwhat happens when we try to call methods—functions attached to classes as\nattributes.\nIf this I2.w reference is a function call, what it really means is “call the C3.w\nfunction to process I2.” That is, Python will automatically map the call I2.w()\ninto the call C3.w(I2), passing in the instance as the first argument to the\ninherited function as the implied subject of the call.\nIn fact, whenever we call a function attached to a class in this fashion, an\ninstance of the class is always implied. This implied subject is part of the reason\nwe refer to this as an object-oriented model—there is always a subject object\nwhen an operation is run. In a more realistic example, we might invoke a method",
      "content_length": 2276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 996,
      "chapter": null,
      "content": "called giveRaise attached as an attribute to an Employee class; such a call has\nno meaning unless qualified with the employee to whom the raise should be\ngiven.\nAs you’ll see in more detail later, Python passes in the implied instance to a\nspecial first argument in the method, called self by strong convention. Methods\ngo through this argument to process the subject of the call. As you’ll also learn\nlater, methods can be called either through an instance—pat.giveRaise()—or\nthrough a class—Employee.giveRaise(pat)—and both forms serve purposes\nin our scripts. In fact, these calls illustrate both of the key ideas in OOP; to run a\npat.giveRaise() method call, Python:\n1. First looks up giveRaise from pat, by inheritance.\n2. Then passes pat to the located giveRaise function, in the special self\nfunction argument.\nWhen you run the call Employee.giveRaise(pat), you’re just performing both\nsteps yourself.\nThis description is technically just the default case (Python has additional\nmethod types you’ll meet later, called static and class methods), but it applies to\nthe vast majority of the OOP code written in the language. To see how methods\nreceive their subjects, though, we need to move on to some code.\nCoding Class Trees\nAlthough we are speaking in the abstract here, there is tangible code behind all\nthese ideas, of course. We construct trees and their objects with class\nstatements and class calls, which we’ll explore in more detail later. In short,\nthough:\nEach class statement generates a new class object.\nEach time a class is called, it generates a new instance object.\nInstances are automatically linked to the classes from which they are\ncreated.",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 997,
      "chapter": null,
      "content": "Classes are automatically linked to their superclasses according to the\nway we list them in parentheses in a class header line; the left-to-right\norder there gives the order in the tree.\nTo build the tree in Figure 26-1, for example, we would run Python code of the\nsort in Example 26-1. Like function definition, classes are normally coded in\nmodule files and are run during an import (the guts of the following class\nstatements are omitted here for brevity, though ... qualifies as a no-op\nstatement per Chapter 13 if run live).\nExample 26-1. classtree1.py\nclass C2: ...                 # Make class objects (ovals)\nclass C3: ...\nclass C1(C2, C3): ...         # Linked to superclasses - in this order\nI1 = C1()                     # Make instance objects (rectangles)\nI2 = C1()                     # Linked to their class\nHere, we build the three class objects by running three class statements, and\nmake the two instance objects by calling the class C1 twice—as though it were a\nfunction (Python lumps classes and functions together as “callable” objects\ninvoked with parentheses, though def requires header parentheses and class\ndoes not). The instances remember the class they were made from, and the class\nC1 remembers its listed superclasses.\nTechnically, this example uses something called multiple inheritance, which\nsimply means that a class has more than one superclass above it in the class tree\n—a useful technique when you wish to combine multiple tools. In Python, if\nthere is more than one superclass listed in parentheses in a class statement (like\nC1’s here), their left-to-right order gives the order in which those superclasses\nwill be searched for attributes by inheritance. The leftmost version of a name is\nused by default, though you can always choose a name by asking for it from the\nclass it lives in (e.g., C3.z). The search also picks names to the right over higher\nduplicates, but we can safely ignore that for now.\nBecause of the way inheritance searches proceed, the object to which you attach\nan attribute turns out to be crucial—it determines the name’s scope. Attributes\nattached to instances pertain only to those single instances, but attributes\nattached to classes are shared by all their subclasses and instances. Later, we’ll",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 998,
      "chapter": null,
      "content": "study the code that hangs attributes on these objects in depth. As you’ll find, it’s\nall about where an assignment is run:\nAttributes are usually attached to classes by assignments made at the\ntop level in class statement blocks, and not nested inside function def\nstatements there.\nAttributes are usually attached to instances by assignments to the\nspecial argument passed to functions coded inside classes, called self.\nFor example, classes provide behavior for their instances with functions we\ncreate by coding def statements inside class statements. Because such nested\ndef statements assign function names within the class, they wind up attaching\nattributes to the class object that will be inherited by all instances and subclasses\n—as is Example 26-2, which lists changed parts in bold.\nExample 26-2. classtree2.py\nclass C2: ...                    # Make superclass objects\nclass C3: ...\nclass C1(C2, C3):                # Make and link class C1\n   def setname(self, who):      # Assign name: C1.setname\n       self.name = who          # Self is either I1 or I2\nI1 = C1()                        # Make two instances\nI2 = C1()\nI1.setname('sue')                # Sets I1.name to 'sue'\nI2.setname('bob')                # Sets I2.name to 'bob'\nprint(I1.name)                   # Prints 'sue'\nThere’s nothing syntactically unique about def in this context. Operationally,\nthough, when a def appears inside a class like this, it is usually known as a\nmethod, and it automatically receives a special first argument—called self by\nvery strong convention—that provides a handle back to the instance to be\nprocessed. Any values you pass to the method yourself go to arguments after\nself (here, to who).2\nBecause classes are factories for multiple instances, their methods usually go\nthrough this automatically passed-in self argument whenever they need to fetch\nor set attributes of the particular instance being processed by a method call. In\nthe preceding code, self is used to store a name in one of two instances.",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 999,
      "chapter": null,
      "content": "Like simple variables, attributes of classes and instances are not declared ahead\nof time, but spring into existence the first time they are assigned values. When a\nmethod assigns to a self attribute, it creates or changes an attribute in an\ninstance at the bottom of the class tree (i.e., one of the rectangles in Figure 26-1)\nbecause self automatically refers to the instance being processed—the subject\nof the call.\nIn fact, because all the objects in class trees are just namespace objects, we can\nfetch or set any of their attributes by going through the appropriate names.\nSaying C1.setname is as valid as saying I1.setname, as long as the names C1\nand I1 are in your code’s scopes.\nOperator Overloading\nAs currently coded, our C1 class doesn’t attach a name attribute to an instance\nuntil the setname method is called. Indeed, referencing I1.name before calling\nI1.setname would produce an undefined name error. If a class wants to\nguarantee that an attribute like name is always set in its instances, it more\ntypically will fill out the attribute at construction time, as demoed by\nExample 26-3.\nExample 26-3. classtree3.py\nclass C2: ...                    # Make superclass objects\nclass C3: ...\nclass C1(C2, C3):\n   def __init__(self, who):     # Set name when constructed\n       self.name = who          # Self is either I1 or I2\nI1 = C1('sue')                   # Sets I1.name to 'sue'\nI2 = C1('bob')                   # Sets I2.name to 'bob'\nprint(I1.name)                   # Prints 'sue'\nIf it’s either coded or inherited, Python automatically calls a method named\n__init__ each time an instance is generated from a class. The new instance is\npassed in to the self argument of __init__ as usual, and any values listed in\nparentheses in the class call go to arguments two and beyond. The effect here is\nto initialize instances when they are made, without requiring extra method calls.\nThe __init__ method is known as the constructor because of when it is run. It’s",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1000,
      "chapter": null,
      "content": "the most commonly used representative of a larger category called operator-\noverloading methods, which we’ll explore in later chapters. Such methods are\ninherited in class trees as usual and have double underscores at the start and end\nof their names to make them distinct. Python runs them automatically when\ninstances that support them appear in the corresponding operations, and they are\nmostly an alternative to using simple method calls. They’re also optional: if\nomitted, the operations are not supported. If no __init__ is present, class calls\nreturn an empty instance, without initializing it.\nFor example, a custom set intersection might be coded as a method named\nintersect called explicitly, or as a method named __and__ that is called\nautomatically by the & expression operator. Because the operator scheme makes\ninstances look and feel more like built-in types, it allows some classes to provide\na consistent and natural interface, and be compatible with code that expects a\nbuilt-in type. Still, apart from the __init__ constructor—which appears in most\nrealistic classes—many programs may be better off with simpler named methods\nunless their objects are similar to built-ins. A giveRaise may make sense for an\nEmployee, but an & might not.\nOOP Is About Code Reuse\nAnd that, along with a few syntax details, is most of the OOP story in Python. Of\ncourse, there’s a bit more to it than just inheritance. For example, operator\noverloading is much more general than described so far—classes may also\nprovide their own implementations of operations such as indexing, fetching\nattributes, printing, and more. By and large, though, OOP is about looking up\nattributes in trees with a special first argument in functions.\nSo why would we be interested in building and searching trees of objects?\nAlthough it takes some experience to see how, when used well, classes support\ncode reuse in ways that other Python program components cannot. In fact, this\nis, by most accounts, their highest purpose. With classes, we code by\ncustomizing existing software, instead of either changing existing code in place\nor starting from scratch for each new project. This turns out to be a powerful\nparadigm in realistic programming.\nAt a fundamental level, classes are really just packages of functions and other",
      "content_length": 2303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1001,
      "chapter": null,
      "content": "names, much like modules. However, the automatic attribute inheritance search\nthat we get with classes supports customization of software above and beyond\nwhat we can do with modules and functions. Moreover, classes provide a natural\nstructure for code that packages and localizes both logic and names, and so aids\nin debugging.\nTo be fair, because methods are simply functions with a special first argument,\nwe can mimic some of their behavior by manually passing subject objects to\nsimple functions. The participation of methods in class inheritance, though,\nallows us to naturally extend and customize software by coding subclasses with\nnew methods, rather than modifying code that already works. There is really no\nsuch concept with modules and functions.\nPolymorphism and classes\nAs an abstract example, suppose you’re assigned the task of implementing an\nemployee database application. As a Python OOP programmer, you might begin\nby coding a general superclass that defines default behaviors common to all the\nkinds of employees in your organization (code in this section is hypothetical and\npartial):\nclass Employee:                      # General superclass\n    def computeSalary(self): …       # Common or default behaviors\n    def giveRaise(self): …\n    def promote(self): …\n    def retire(self): …\nOnce you’ve coded this general behavior, you can specialize it for each specific\nkind of employee to reflect how the various types differ from the norm. That is,\nyou can code subclasses that customize just the bits of behavior that vary per\nemployee type; the rest of the employee types’ behavior will be inherited from\nthe more general class. For instance, if engineers have a unique salary\ncomputation rule (perhaps it’s not hours times rate), you can replace just that one\nmethod in a subclass:\nclass Engineer(Employee):            # Specialized subclass\n     def computeSalary(self): …      # Something custom here\nBecause the computeSalary version here appears lower in the class tree, it will",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1002,
      "chapter": null,
      "content": "replace (override) the general version in Employee. All other methods, though,\nare inherited from the superclass verbatim. You then create instances of the kinds\nof employee classes that the real employees belong to, to get the correct\nbehavior:\nsue = Employee()                     # Default behavior\nbob = Employee()                     # Default behavior\npat = Engineer()                     # Custom salary calculator\nNotice that you can make instances of any class in a tree, not just the ones at the\nbottom—the class you make an instance from determines the level at which the\nattribute search will begin, and thus which versions of the methods it will\nemploy (pun accidental).\nUltimately, these three instance objects might wind up embedded in a larger\ncontainer object—for instance, a list, dictionary, or an instance of another class\n—that represents a department or company using the composition idea\nmentioned at the start of this chapter. When you later ask for these employees’\nsalaries, they will be computed according to the classes from which the objects\nwere made, due to the principles of the inheritance search:\ncompany = [sue, bob, pat]            # A composite object\nfor emp in company:\n    print(emp.computeSalary())       # Run this emp's version: default or custom\nThis is yet another instance of the idea of polymorphism introduced in Chapter 4\nand expanded in Chapter 16. Recall that polymorphism means that the meaning\nof an operation depends on the object being operated on. That is, code shouldn’t\ncare about what an object is, only about what it does. Here, the method\ncomputeSalary is located by inheritance search in each object before it is called,\nper the object’s class. The net effect is that we automatically run the correct\nversion for the object being processed—default or custom. Trace the code to see\nwhy.\nIn other applications, polymorphism might also be used to encapsulate (i.e.,\nabstract away) interface differences. For example, a program that processes data\nstreams might be coded to expect objects with input and output methods, without\ncaring what those methods actually do:",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1003,
      "chapter": null,
      "content": "def processor(reader, converter, writer):\n    while True:\n        data = reader.read()\n        if not data: break\n        data = converter(data)\n        writer.write(data)\nBy passing in instances of subclasses that specialize the required read and\nwrite method interfaces for various data sources, we can reuse the processor\nfunction for any data source we need to use, both now and in the future:\nclass Reader:\n    def other(self): …               # Default behavior and tools\nclass FileReader(Reader):\n    def read(self): …                # Read from a local file\nclass SocketReader(Reader):\n    def read(self): …                # Read from a network socket\n    \n…and others…\nprocessor(FileReader(…),   converter,  FileWriter(…))\nprocessor(SocketReader(…), converter,  FileWriter(…))\nprocessor(FtpReader(…),    converter,  JsonWriter(…))\nMoreover, because the internal implementations of those read and write\nmethods have been factored into single locations, they can be changed without\nimpacting code that uses them. The processor function might even be a class\nitself to allow the conversion logic of converter to be filled in by inheritance,\nand to allow readers and writers to be embedded by composition (you’ll see how\nthis works later in this part of the book).\nProgramming by customization\nOnce you get used to programming this way (by software customization), you’ll\nfind that when it’s time to write a new program, much of your work may already\nbe done—your task largely becomes one of mixing together existing\nsuperclasses that already implement the behavior required by your program. For\nexample, someone else might have written the Employee and Reader classes in\nthis section’s examples for use in completely different programs. If so, you get",
      "content_length": 1757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1004,
      "chapter": null,
      "content": "all of that person’s code “for free.”\nIn fact, in many application domains, you can fetch or purchase collections of\nsuperclasses, known as frameworks, that implement common programming tasks\nas classes, ready to be mixed into your applications. These frameworks might\nprovide database interfaces, testing protocols, GUI toolkits, and so on. With\nframeworks, you often simply code a subclass that fills in a handful of expected\nmethods; the framework classes higher in the tree do most of the work for you.\nProgramming in such an OOP world is just a matter of combining and\nspecializing already debugged code by writing subclasses of your own.\nOf course, it takes a while to learn how to leverage classes to achieve such OOP\nutopia. In practice, object-oriented work also entails substantial design work to\nfully realize the code reuse benefits of classes. To this end, programmers catalog\ncommon OOP structures, known as design patterns, to help with design choices.\nThe actual code you write to do OOP in Python, though, is so simple that it will\nnot in itself pose an additional obstacle to your OOP quest. To see why, you’ll\nhave to move on to Chapter 27.",
      "content_length": 1159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1005,
      "chapter": null,
      "content": "Chapter Summary\nWe made an initial, abstract pass over classes and OOP in this chapter, taking in\nthe big picture before we dive into syntax details. As we’ve seen, OOP is mostly\nabout an argument named self, and a search for attributes in trees of linked\nobjects called inheritance. Objects at the bottom of the tree inherit attributes\nfrom objects higher up in the tree—a feature that enables us to program by\ncustomizing code, rather than changing it or starting from scratch. When used\nwell, this model of programming can cut development time radically.\nThe next chapter will begin to fill in the coding details behind the picture painted\nhere. As we get deeper into Python classes, though, keep in mind that the OOP\nmodel in Python is very simple; as we’ve seen here, it’s really just about looking\nup names in object trees and a special function argument. Before we move on,\nhere’s a quick quiz to review what we’ve covered here.\nTest Your Knowledge: Quiz\n1. What is the main point of OOP in Python?\n2. Where does an inheritance search look for an attribute?\n3. What is the difference between a class object and an instance object?\n4. Why is the first argument in a class’s method function special?\n5. What is the __init__ method used for?\n6. How do you create a class instance?\n7. How do you create a class?\n8. How do you specify a class’s superclasses?\nTest Your Knowledge: Answers\n1. OOP is about code reuse—you factor code to minimize redundancy, and",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1006,
      "chapter": null,
      "content": "program by customizing what already exists instead of changing code in\nplace or starting from scratch.\n2. An inheritance search looks for an attribute first in the instance object,\nthen in the class the instance was created from, then in all higher\nsuperclasses, progressing from the bottom to the top of the object tree,\nand from left to right (normally). The search stops at the first place the\nattribute is found. Because the lowest version of a name found along the\nway wins, class hierarchies naturally support customization by\nextension in new subclasses.\n3. Both class and instance objects are namespaces—packages of variables\nthat appear as attributes. The main difference between them is that\nclasses are a kind of factory for creating multiple instances. Classes also\nsupport operator-overloading methods, which instances inherit, and treat\nany functions nested in the class as methods for processing instances.\n4. The first argument in a class’s method function is special because it\nalways receives the instance object that is the implied subject of the\nmethod call. It’s usually called self by convention. Because method\nfunctions always have this implied subject—and object context—by\ndefault, we say they are “object-oriented” (i.e., designed to process or\nchange objects).\n5. If the __init__ method is coded or inherited in a class, Python calls it\nautomatically each time an instance of that class is created. It’s known\nas the constructor method; it is passed the new instance implicitly, as\nwell as any arguments passed explicitly to the class name. It’s also the\nmost commonly used operator-overloading method. If no __init__\nmethod is present, instances simply begin life as empty namespaces.\n6. You create a class instance by calling the class name as though it were a\nfunction; any arguments passed into the class name show up as\narguments two and beyond in the __init__ constructor method (if there\nis one). The new instance remembers the class it was created from for\ninheritance purposes.\n7. You create a class by running a class statement; like function",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1007,
      "chapter": null,
      "content": "definitions, these statements normally run when the enclosing module\nfile is imported (more on this in the next chapter).\n8. You specify a class’s superclasses by listing them in parentheses in the\nclass statement, after the new class’s name. The left-to-right order in\nwhich the classes are listed in the parentheses gives the left-to-right\ninheritance search order in the class tree.\n1  In other literature and circles, you may also occasionally see the terms base classes and derived\nclasses used to describe superclasses and subclasses, respectively. Most Python people and this book\ntend to use the latter terms.\n2  If you’ve ever used C++ or Java, you’ll recognize that Python’s self is much like these languages’\nthis pointer/reference, but self is always explicit in both headers and bodies of Python methods to\nmake attribute accesses more obvious: a name has fewer possible meanings to consider, if it cannot\nbe magically associated with a hidden object. Explicit is generally better.",
      "content_length": 994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1008,
      "chapter": null,
      "content": "Chapter 27. Class Coding Basics\nNow that we’ve talked about OOP in the abstract, it’s time to see how this\ntranslates to actual code. This chapter begins to fill in the syntax details behind\nthe class model in Python.\nIf you’ve never been exposed to OOP in the past, classes can seem somewhat\ncomplicated if taken in a single dose. To make class coding easier to absorb,\nwe’ll begin our detailed exploration of OOP by taking a first look at some basic\nclasses in action in this chapter. We’ll expand on the details introduced here in\nlater chapters of this part of the book, but in their basic form, Python classes are\neasy to understand.\nIn fact, classes have just three primary distinctions. At a base level, they are\nmostly just namespaces, much like the modules we studied in Part V. Unlike\nmodules, though, classes also have support for generating multiple objects, for\nnamespace inheritance, and for operator overloading. Let’s begin our class\nstatement tour by exploring each of these three distinctions in turn.\nClasses Generate Multiple Instance Objects\nTo understand how the multiple objects idea works, you have to first understand\nthat there are two kinds of objects in Python’s OOP model: class objects and\ninstance objects. Class objects provide default behavior and are used to create\ninstance objects. Instance objects are the tangible objects your programs process\n—each is a namespace in its own right but inherits (i.e., has automatic access to)\nnames in the class from which it was created. Class objects come from\nstatements, and instances come from calls; each time you call a class, you get a\nnew instance of that class.\nThis object-generation concept is very different from most of the other program\nconstructs we’ve seen so far in this book. In effect, classes are essentially\nfactories for generating multiple instances. By contrast, only one copy of each\nmodule is ever imported into a single program. In fact, this is why reload works",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1009,
      "chapter": null,
      "content": "as it does, updating a single instance and shared object in place. With classes,\neach instance can have its own independent data, supporting multiple versions of\nthe object that the class models.\nIn this role, class instances are similar to the per-call state of the closure (a.k.a.\nfactory) functions of Chapter 17, but this is a natural part of the class model, and\nstate in classes is explicit attributes instead of implicit scope references.\nMoreover, this is just part of what classes do—they also support customization\nby inheritance, operator overloading, and multiple behaviors via methods.\nHence, classes are a more complete programming tool, though OOP and\nfunctional programming are not mutually exclusive paradigms. We may combine\nthem by using functional tools in methods, by coding methods that are\nthemselves generators, by writing user-defined iterators, and so on.\nThe following is a quick summary of the bare essentials of Python OOP in terms\nof its two object types. As you’ll see, Python classes are in some ways similar to\nboth defs and modules, but they may be quite different from what you’re used to\nin other languages.\nClass Objects Provide Default Behavior\nWhen we run a class statement, we get a class object. Here’s a rundown of the\nmain properties of Python classes:\nThe class statement creates a class object and assigns it a name.\nJust like the function def statement, the Python class is an executable\nstatement. When reached and run, it generates a new class object and\nassigns it to the first name in the class header. Also, like defs, class\nstatements typically run when the files they are coded in are first\nimported or run as a top-level script.\nAssignments inside class statements make class attributes. Just like\nin module files, top-level assignments within a class statement (not\nnested in a def) generate attributes in a class object. Technically, the\nclass statement defines a local scope that morphs into the attribute\nnamespace of the class object, just like a module’s global scope. After\nrunning a class statement, class attributes may be accessed by name",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1010,
      "chapter": null,
      "content": "qualification: class.name.\nClass attributes provide object state and behavior. Attributes of a\nclass object record state information and behavior to be shared by all\ninstances created from the class. Most notably and commonly, function\ndef statements nested inside a class generate methods, which process\ninstances.\nInstance Objects Are Concrete Items\nWhen we call a class object, we get an instance object. Here’s an overview of the\nkey points behind class instances:\nCalling a class object like a function makes a new instance object.\nEach time a class is called, it creates and returns a new instance object.\nInstances represent material items in your program’s domain.\nEach instance object inherits class attributes and gets its own\nnamespace. Instance objects created from classes are new namespaces;\nthey start out empty but inherit attributes that live in the class objects\nfrom which they were generated.\nAssignments to attributes of self in methods make per-instance\nattributes. Inside a class’s method functions, the first argument (called\nself by convention) references the instance object being processed;\nassignments to attributes of self create or change data in the instance,\nnot the class.\nThe end result is that classes define common, shared data and behavior, and\ngenerate instances. Instances reflect palpable application entities and record per-\ninstance data that may vary per object.\nA First Example\nLet’s turn to a real example to show how these ideas work in practice. To begin,\nlet’s define a class named FirstClass by running a Python class statement\ninteractively in any REPL:",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1011,
      "chapter": null,
      "content": ">>> class FirstClass:               # Define a class object\n        def setdata(self, value):   # Define class's methods\n            self.data = value       # self is the instance\n        def display(self):\n            print(self.data)        # self.data: per instance\nWe’re working interactively here, but typically, such a statement would be run\nwhen the module file it is coded in is imported. Like functions created with defs,\nthis class won’t even exist until Python reaches and runs this statement.\nLike all compound statements, the class starts with a header line that lists the\nclass name, followed by a body of one or more nested and (usually) indented\nstatements. Here, the nested statements are defs; they define functions that\nimplement the behavior the class means to export.\nAs we learned in Part IV, def is really an assignment. Here, it assigns function\nobjects to the names setdata and display in the class statement’s scope, and\nso generates attributes attached to the class—FirstClass.setdata and\nFirstClass.display. In fact, any name assigned at the top level of the class’s\nnested block becomes an attribute of the class.\nFunctions inside a class are usually and traditionally called methods. They’re\ncoded with normal defs, and they support everything we’ve learned about\nfunctions already—they can have defaults, return values, yield items on request,\nand so on. But in a method function, the first argument automatically receives an\nimplied instance object when called—the subject of the call. Let’s create a\ncouple of instances of our class to see how this works:\n>>> x = FirstClass()                # Make two instances\n>>> y = FirstClass()                # Each is a new namespace\nBy calling the class this way (notice the parentheses), we generate instance\nobjects, which are just namespaces that have access to their classes’ attributes.\nProperly speaking, at this point, we have three objects: two instances and a class.\nAnd really, we have three linked namespaces, as sketched in Figure 27-1. In\nOOP terms, we say that x “is a” FirstClass, as is y—they both inherit names\nattached to the class.",
      "content_length": 2125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1012,
      "chapter": null,
      "content": "Figure 27-1. Classes and instances: namespaces in a class tree searched by inheritance\nThe two instances start out empty but have links back to the class from which\nthey were generated. If we qualify an instance with the name of an attribute that\nlives in the class object, Python fetches the name from the class by inheritance\nsearch (unless it also lives in the instance):\n>>> x.setdata('coding')             # Call methods: self is x\n>>> y.setdata(3.14159)              # Runs: FirstClass.setdata(y, 3.14159)\nNeither x nor y has a setdata attribute of its own, so to find it, Python follows\nthe link from instance to class. And that’s about all there is to inheritance in\nPython: it happens at attribute qualification time, and it just involves looking up\nnames in linked objects—here, by following the is-a links in Figure 27-1.\nIn the setdata function inside FirstClass, the value passed in is assigned to\nself.data. Within a method, self—the name given to the leftmost argument\nby convention—automatically refers to the instance being processed (x or y at\nthis point), so the assignments store values in the instances’ namespaces, not the\nclass’s. That’s how the data names in Figure 27-1 are created.\nBecause classes can generate multiple instances, methods must go through the\nself argument to get to the instance to be processed. When we call the class’s\ndisplay method to print self.data, we see that it’s different in each instance;\non the other hand, the name display itself is the same in x and y, as it comes (is\ninherited) from the class:",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1013,
      "chapter": null,
      "content": ">>> x.display()                     # Runs: FirstClass.display(x) \ncoding\n>>> y.display()                     # self.data differs in each instance\n3.14159\nNotice that we stored different object types in the data member in each instance\n—a string and a floating-point number. As with everything else in Python,\ninstance attributes (sometimes called members) are not predeclared and have no\ntype constraints; they spring into existence the first time they are assigned\nvalues, just like simple variables. In fact, if we were to call display on one of\nour instances before calling setdata, we would trigger an undefined name error\n—the attribute named data doesn’t even exist in memory until it is assigned\nwithin the setdata method.\nAs another way to appreciate how dynamic this model is, consider that we can\nchange instance attributes either inside the class itself, by assigning to self in\nmethods, or outside the class, by assigning to an explicit instance object:\n>>> x.data = 'hacking'              # Can get/set attributes\n>>> x.display()                     # Outside the class too\nhacking\nAlthough less common, we could even generate an entirely new attribute in the\ninstance’s namespace by assigning to its name outside the class’s method\nfunctions:\n>>> x.anothername = 'apps'          # Can set new attributes here too!\nThis would attach a new attribute called anothername, which may or may not be\nused by any of the class’s methods, to the instance object x. Classes usually\ncreate all of the instance’s attributes by assignment to the self argument, but\nthey don’t have to—programs can fetch, change, or create attributes on any\nobjects to which they have references.\nIt usually doesn’t make sense to add data that the class cannot use, and it’s\npossible to prevent this with extra “privacy” code based on attribute-access\noperator overloading, as we’ll discuss elsewhere in this book (in Chapters 30 and\n39). Still, free attribute access translates to less syntax, and there are cases where",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1014,
      "chapter": null,
      "content": "it’s even useful—for example, in coding data records of the sort we’ll build later\nin this chapter.\nClasses Are Customized by Inheritance\nLet’s move on to the second major distinction of classes. Besides serving as\nfactories for generating multiple instance objects, classes also allow us to make\nchanges by introducing new components (called subclasses), instead of changing\nexisting components in place.\nAs we’ve seen, instance objects generated from a class inherit the class’s\nattributes. Python also allows classes to inherit from other classes, opening the\ndoor to coding hierarchies of classes that specialize behavior—by redefining\nattributes in subclasses that appear lower in the hierarchy, we override the more\ngeneral definitions of those attributes higher in the tree. In effect, the further\ndown the hierarchy we go, the more specific the software becomes. Here, too,\nthere is no parallel with modules, whose attributes live in a single, flat\nnamespace that is not as amenable to customization.\nIn Python, instances inherit from classes, and classes inherit from superclasses.\nHere are the key ideas behind the machinery of attribute inheritance:\nSuperclasses are listed in parentheses in a class header. To make a\nclass inherit attributes from another class, just list the other class in\nparentheses in the new class statement’s header line. The class that\ninherits is usually called a subclass, and the class that is inherited from\nis its superclass.\nClasses inherit attributes from their superclasses. Just as instances\ninherit the attribute names defined in their classes, classes inherit all of\nthe attribute names defined in their superclasses. Python finds these\nnames automatically when they’re accessed if they don’t exist in the\nsubclasses.\nInstances inherit attributes from all accessible classes. Each instance\ngets names from the class it’s generated from, as well as all of that\nclass’s superclasses. When looking for a name, Python checks the\ninstance, then its class, then all superclasses above its class.",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1015,
      "chapter": null,
      "content": "Each object.attribute reference invokes a new, independent\nsearch. Python performs an independent search of the class tree for\neach attribute fetch expression. This includes references to instances\nand classes made outside class statements (e.g., X.attr), as well as\nreferences to attributes of the self instance argument in a class’s\nmethod functions. That is, each self.attr expression in a method\ninvokes a new search for attr in self and above.\nThe net effect—and the main purpose of all this searching—is that classes\nsupport factoring and customization of code better than any other language tool\nwe’ve seen so far. On the one hand, they allow us to minimize code redundancy\n(and so reduce maintenance costs) by factoring operations into a single, shared\nimplementation; on the other, they allow us to program by customizing what\nalready exists, rather than changing it in place or starting from scratch.\nNOTE\nFull inheritance disclosure: Strictly speaking, Python’s inheritance is richer than described\nhere, when we factor in “diamond” inheritance patterns and “metaclasses”—advanced topics\nwe’ll study later—but we can safely restrict our scope to instances and their classes, both at\nthis point in the book and in most Python application code. We’ll explore diamonds and the\n“MRO” inheritance search order that accommodates them in Chapter 31, but our definition of\ninheritance won’t be fully complete until Chapter 40, because it regrettably requires metaclass\ninfo that’s beyond almost all Python programmers’ interest levels and pay grades (thankfully!).\nA Second Example\nTo illustrate the role of inheritance, this next example builds on the previous one.\nFirst, we’ll define a new class, SecondClass, that inherits all of FirstClass’s\nnames and provides one of its own:\n>>> class SecondClass(FirstClass):                   # Inherits setdata\n        def display(self):                           # Changes display\n            print(f'Current value = \"{self.data}\"')\nSecondClass defines the display method to print with a different format. By\ndefining an attribute with the same name as an attribute in FirstClass,",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1016,
      "chapter": null,
      "content": "SecondClass effectively replaces the display attribute in its superclass.\nRecall that inheritance searches proceed upward from instances to subclasses to\nsuperclasses, stopping at the first appearance of the attribute name that it finds.\nIn this case, since the display name in SecondClass will be found before the\none in FirstClass, we say that SecondClass overrides FirstClass’s display.\nSometimes we call this act of replacing attributes by redefining them lower in\nthe tree overloading.\nThe net effect here is that SecondClass specializes FirstClass by changing the\nbehavior of the display method. On the other hand, SecondClass (and any\ninstances created from it) still inherits the setdata method in FirstClass\nverbatim. Let’s make a new instance to demonstrate:\n>>> z = SecondClass()\n>>> z.setdata('LP6e')       # Finds setdata in FirstClass\n>>> z.display()             # Finds overridden method in SecondClass\nCurrent value = \"LP6e\"\nAs before, we make a SecondClass instance object by calling it. The setdata\ncall still runs the version in FirstClass, but this time the display attribute\ncomes from SecondClass and prints a custom message. Figure 27-2 sketches the\nnamespaces involved.\nFigure 27-2. Specialization: overriding inherited names by redefining them in subclasses\nNow, here’s a crucial thing to notice about OOP: the specialization introduced in\nSecondClass is completely external to FirstClass. That is, it doesn’t affect\nexisting or future FirstClass objects, like the x from the prior example\n(assuming we’re continuing the same REPL session):",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1017,
      "chapter": null,
      "content": ">>> x.display()             # x is still a FirstClass instance (old message)\nhacking\nRather than changing FirstClass, we customized it. Naturally, this is an\nartificial example, but as a rule, because inheritance allows us to make changes\nlike this in external components (i.e., in subclasses), classes often support\nextension and reuse better than functions or modules can.\nClasses Are Attributes in Modules\nBefore we move on, remember that there’s nothing magic about a class name.\nIt’s just a variable assigned to an object when the class statement runs, and the\nobject can be referenced with any normal expression. For instance, if our\nFirstClass were coded in a module file instead of being typed interactively, we\ncould import it and use its name normally in a class header line:\nfrom modulename import FirstClass           # Copy name into my scope\nclass SecondClass(FirstClass):              # Use class name directly\n    def display(self): …\nOr equivalently:\nimport modulename                           # Access the whole module\nclass SecondClass(modulename.FirstClass):   # Qualify to reference\n    def display(self): …\nLike everything else, class names always live within a module, so they must\nfollow all the rules we studied in Part V. For example, more than one class can\nbe coded in a single module file—like other statements in a module, class\nstatements are run during imports to define names, and these names become\ndistinct module attributes. More generally, each module may arbitrarily mix any\nnumber of variables, functions, and classes, and all names in a module behave\nthe same way. The following hypothetical file demonstrates:\n# names.py\nvar1 = 6                 # names.var1\nvar2 = 3.12             \ndef func1(): …           # names.func1\ndef func2(): …",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1018,
      "chapter": null,
      "content": "class Cls1: …            # names.Cls1\nclass Cls2: …            # names.Cls2\nThis holds true even if the module and class happen to have the same name. For\nexample, given the following imaginary file, person.py:\nclass person: …\nwe need to go through the module to fetch the class as usual:\nimport person                                 # Import module\nx = person.person()                           # Class within module\nAlthough this path may look redundant, it’s required: person.person refers to\nthe person class inside the person module. Saying just person gets the module,\nnot the class, unless the from statement is used:\nfrom person import person                     # Get class from module\nx = person()                                  # Use class name\nAs with any other variable, we can never see a class in a file without first\nimporting and somehow fetching it from its enclosing file. If this seems\nconfusing, don’t use the same name for a module and a class within it. In fact,\ncommon convention in Python recommends that class names should begin with\nan uppercase letter, and module names with a lowercase letter, to help make\nthem more distinct (it’s not required, but nearly common enough to be a rule):\nimport person                                 # Lowercase for modules\nx = person.Person()                           # Uppercase for classes\nAlso, keep in mind that although classes and modules are both namespaces for\nattaching attributes, they correspond to very different source code structures: a\nmodule reflects an entire file, but a class is a statement within a file. We’ll say\nmore about such distinctions later in this part of the book.\nClasses Can Intercept Python Operators",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1019,
      "chapter": null,
      "content": "Let’s move on to the third and final major difference between classes and\nmodules: operator overloading. In simple terms, operator overloading lets\nobjects coded with classes intercept and respond to operations that work on\nbuilt-in types: addition, slicing, printing, qualification, and so on. It’s mostly just\nan automatic dispatch mechanism—expressions and other built-in operations\nroute control to implementations in classes. Here, too, there is nothing similar in\nmodules: modules can implement function calls, but not the behavior of\nexpressions (apart, that is, from the odd special case added in Python 3.7 for\nmodule __getattr__ and __dir__ functions covered in Chapter 25, and\nbemoaned there as a confusing conflation with classes—for reasons you’re about\nto see for yourself).\nAlthough we could implement all class behavior as normally named methods,\noperator overloading lets objects be more tightly integrated with Python’s object\nmodel. Moreover, because operator overloading makes our own objects act like\nbuilt-ins, it tends to foster object interfaces that are more consistent and easier to\nlearn, and it allows class-based objects to be processed by code written to expect\na built-in object’s interface. Here is a quick rundown of the main ideas behind\noverloading operators:\nMethods named with double underscores (__X__) are special hooks.\nIn Python classes, we implement operator overloading by providing\nspecially named methods to intercept operations. The Python language\ndefines a fixed and unchangeable mapping from each of these\noperations to a specially named method.\nSuch methods are called automatically when instances appear in\nbuilt-in operations. For instance, if an instance object inherits an\n__add__ method, that method is called whenever the object appears in a\n+ expression. The method’s return value becomes the result of the\ncorresponding expression.\nClasses may override most built-in type operations. There are dozens\nof special operator-overloading method names for intercepting and\nimplementing nearly every operation available for built-in object types.\nThis includes expressions, but also basic operations like printing and\nobject creation.",
      "content_length": 2185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1020,
      "chapter": null,
      "content": "Most operator-overloading methods have no default, and none are\nrequired. If a class does not define or inherit an operator-overloading\nmethod, it just means that the corresponding operation is not supported\nfor the class’s instances. If there is no __add__, for example, +\nexpressions raise exceptions. As you’ll learn later, a root class named\nobject that’s an implicit superclass to every class does provide defaults\nfor some __X__ methods, but not for many (e.g., object has a default\nfor print strings, but not +).\nImportantly, operator overloading is an optional feature; it’s used primarily by\npeople developing tools for other Python programmers, not by application\ndevelopers. And, candidly, you probably shouldn’t use it just because it seems\nclever. Unless a class needs to mimic built-in object interfaces, it should usually\nstick to nonoperator method names whose calls are more explicit. Expressions\nlike * and +, for example, may make sense for a numeric object like a matrix, but\nother code would generally serve its clients better with mnemonically named\nmethods.\nBecause of this, we won’t go into details on every operator-overloading method\navailable in Python in this learner’s book. Still, as previewed in the prior chapter,\nthere is one operator-overloading method you are likely to see in almost every\nPython class: the __init__ method, which is known as the constructor method\nand is used to initialize instance objects’ state. Pay special attention to this\nmethod, because __init__, along with the self argument and inheritance\nsearch, turns out to be a core requirement for reading and understanding most\nOOP code in Python.\nA Third Example\nOn to another example. This time, we’ll define a subclass of the prior section’s\nSecondClass that implements three specially named attributes that Python will\ncall automatically:\n__init__ is run when a new instance object is created: self is the new\nThirdClass object.1\n__add__ is run when a ThirdClass instance appears in a + expression.",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1021,
      "chapter": null,
      "content": "__str__ is run when an object is printed (technically, when it’s\nconverted to its print string by the str built-in function or its Python-\ninternals equivalent).\nOur new subclass also defines a normally named method called mul, which\nchanges the instance object in place. Here’s the new subclass (copy-and-pasters:\nin REPLs, you may need to omit blank lines added here for clarity):\n>>> class ThirdClass(SecondClass):                     # Inherit from SecondClass\n        def __init__(self, value):                     # On \"ThirdClass(value)\"\n            self.data = value\n        def __add__(self, other):                      # On \"self + other\"\n            return ThirdClass(self.data + other)\n        def __str__(self):                             # On \"print(self)\", \"str()\"\n            return f'[ThirdClass: {self.data}]'\n        def mul(self, other):                          # In-place change: named\n            self.data *= other\nThirdClass “is a” SecondClass, so its instances inherit the customized\ndisplay method from SecondClass of the preceding section. This time, though,\nThirdClass creation calls pass an object to the value argument in the __init__\nconstructor, where it is assigned to self.data. The net effect is that\nThirdClass arranges to set the data attribute automatically at construction time,\ninstead of requiring later setdata calls:\n>>> a = ThirdClass(3)               # __init__ called\n>>> a.display()                     # Inherited method called\nCurrent value = \"3\"\nThirdClass objects can also now show up in + expressions and print calls. For\n+, Python passes the instance object on the left to the self argument in __add__\nand the value on the right to other, as illustrated in Figure 27-3; whatever\n__add__ returns becomes the result of the + expression (more on its result in a\nmoment):\n>>> b = a + 3                       # __add__: makes a new instance\n>>> b.display()                     # b has all ThirdClass methods",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1022,
      "chapter": null,
      "content": "Current value = \"6\"\nFor print, Python passes the object being printed to self in __str__; whatever\nstring this method returns is taken to be the print string for the object. With\n__str__ (or its broader twin __repr__, which we’ll leverage in the next\nchapter), we can use a normal print to display objects of this class, instead of\ncalling the display method. As a contrast, the added mul method also changes\nthe instance in place for a named call:\n>>> print(b)                        # __str__: returns display string\n[ThirdClass: 6]\n>>> a.mul(3)                        # mul: changes instance in place\n>>> print(a)                        # Print this instance's data\n[ThirdClass: 9]\nFigure 27-3. Operator overloading: class methods are run for operators and operations\nSpecially named methods such as __init__, __add__, and __str__ are\ninherited by subclasses and instances, just like any other names assigned in a\nclass. If they’re not coded in a class, Python looks for such names in all its\nsuperclasses, as usual. As for all attributes, the lowest (most specific) version is\nused.\nOperator-overloading method names are also not built-in or reserved words; they\nare just attributes that Python looks for when objects appear in various contexts.",
      "content_length": 1249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1023,
      "chapter": null,
      "content": "Python usually calls them automatically, but they may occasionally be called by\nyour code as well. For example, the __init__ method is often called manually\nto trigger required initialization steps in a superclass constructor, as you’ll see in\nthe next chapter.\nReturning results—or not\nSome operator-overloading methods like __str__ require results, but others are\nmore flexible. For example, notice how the __add__ method makes and returns a\nnew instance object of its class, by calling ThirdClass with the result value—\nwhich in turn triggers __init__ to initialize the result. This is a common\nconvention, and explains why b in the listing has a display method; it’s a\nThirdClass object too, because that’s what + returns for this class’s objects.\nThis essentially propagates the object type.\nBy contrast, mul changes the current instance object in place, by reassigning the\nself attribute. We could overload the * expression to do the same with __mul__,\nbut this would be too different from the behavior of * for built-in types such as\nnumbers and strings, for which it always makes new objects. Per common\npractice, overloaded operators should work the same way that built-in operations\ndo. Because operator overloading is really just an expression-to-method dispatch\nmechanism, though, you can interpret operators any way you like in your own\nclasses. Also, stay tuned for Chapter 30’s related coverage of in-place operator\nmethods (preview: __imul__ handles *=).\nOther operator-overloading methods\nAlthough we won’t cover every operator-overloading method in this book, we’ll\nsurvey additional common operator-overloading techniques in Chapter 30.\nAgain, while this is an optional tool that doesn’t apply to most application\nprograms, __str__ is not uncommon, and the __init__ constructor method is a\nnorm that will be present in most Python classes you’ll come across. In fact,\neven though instance attributes need not be predeclared in Python, you can\nusually find out which attributes an instance will have by inspecting its class’s\n__init__ method.",
      "content_length": 2060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1024,
      "chapter": null,
      "content": "The World’s Simplest Python Class\nDespite the details of the class statement that we’ve begun to uncover in this\nchapter, you should keep in mind that the basic inheritance model behind classes\nis very simple—all it really involves is searching for attributes in trees of linked\nobjects. In fact, we can create a class with nothing in it at all. The following\nstatement makes a class with no attributes attached, an empty namespace object:\n>>> class rec: pass              # Empty namespace object\nWe need the no-operation pass placeholder statement (discussed in Chapter 13)\nhere because we don’t have any methods to code. After we make the class by\nrunning this statement interactively, we can start attaching attributes to the class\nby assigning names to it completely outside of the original class statement:\n>>> rec.name = 'Pat'             # Just objects with attributes\n>>> rec.age  = 40\nAnd, after we’ve created these attributes by assignment, we can fetch them with\nthe usual syntax. When used this way, a class is roughly similar to a “struct” in\nC, or a “record” in Pascal. It’s basically an object with field names attached to it,\nand may be easier to code than alternatives like dictionaries:\n>>> print(rec.name)              # Like a C struct or a record\nPat\nNotice that this works even though there are no instances of the class yet; classes\nare objects in their own right, even without instances. In fact, they are just self-\ncontained namespaces; as long as we have a reference to a class, we can set or\nchange its attributes anytime we wish. Watch what happens when we do create\ntwo instances, though:\n>>> x = rec()                    # Instances inherit class names\n>>> y = rec()\nThese instances begin their lives as completely empty namespace objects.\nBecause they remember the class from which they were made, though, they will\nobtain the attributes we attached to the class by inheritance:",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1025,
      "chapter": null,
      "content": ">>> x.name, y.name               # name is stored on the class only\n('Pat', 'Pat')\nReally, these instances have no attributes of their own; they simply fetch the\nname attribute from the class object where it is stored. If we do assign an\nattribute to an instance, though, it creates (or changes) the attribute in that object,\nand no other—crucially, attribute references kick off inheritance searches, but\nattribute assignments affect only the objects in which the assignments are made.\nHere, this means that x gets its own name, but y still inherits the name attached to\nthe class above it:\n>>> x.name = 'Sue'               # But assignment changes x only\n>>> rec.name, x.name, y.name\n('Pat', 'Sue', 'Pat')\nClasses: Under the Hood\nIn fact, as we’ll explore in more detail in Chapter 29, the attributes of a\nnamespace object are usually implemented as dictionaries, and class inheritance\ntrees are, generally speaking, just dictionaries with links to other dictionaries. If\nyou know where to look, you can see this explicitly.\nFor example, the __dict__ attribute is the namespace dictionary for most class-\nbased objects, much as in modules. Some classes may also—or instead—define\nattributes in __slots__, an advanced and seldom-used feature called slots with\nimpacts that we’ll note along the way, but whose full coverage we’ll largely\npostpone until Chapter 32. Normally, though, __dict__ literally is the attribute\nnamespace of an instance or class.\nTo illustrate, the following inspects the namespaces of objects in the prior\nsection’s REPL session, filtering out built-ins in the class with a generator\nexpression as we’ve done before leaving just the names we’ve assigned:\n>>> list(key for key in rec.__dict__ if not key.startswith('__'))\n['name', 'age'] \n>>> list(x.__dict__)\n['name']\n>>> list(y.__dict__)",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1026,
      "chapter": null,
      "content": "[]\nHere, the class’s namespace dictionary shows the name and age attributes we\nassigned to it, x has its own name, and y is still empty. Because of this model, an\nattribute can often be fetched by either dictionary indexing or attribute notation,\nbut only if it’s present on the object in question—attribute notation kicks off\ninheritance search, but indexing looks in the single object only. As you’ll see\nlater, both have valid roles, and the dir built-in collects inherited names:\n>>> x.name, x.__dict__['name']        # Attributes present here are dict keys\n('Sue', 'Sue')\n>>> x.age                             # But attribute fetch checks classes too\n40\n>>> x.__dict__['age']                 # Indexing dict does not do inheritance\nKeyError: 'age'\n>>> x.__dict__                        # Object namespace versus inheritance\n{'name': 'Sue'}\n>>> [attr for attr in dir(x) if attr[:2] != '__']\n['age', 'name']\nIn addition, to facilitate inheritance search on attribute fetches, each instance has\na link to its class that Python creates for us—it’s called __class__, if you want\nto inspect it:\n>>> x.__class__                       # Instance-to-class link\n<class '__main__.rec'>\nClasses also have a __bases__ attribute, which is a tuple of references to a\nclass’s superclass objects—in this example just the implicit and automatic\nobject root class we’ll explore later:\n>>> rec.__bases__                     # Class-to-superclasses link\n(<class 'object'>,)\nThese two attributes are how class trees are literally represented in memory by\nPython. Internal details like these are not required knowledge—class trees are\nimplied by the code you run, and their search is normally automatic—but they\ncan often help demystify the model.",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1027,
      "chapter": null,
      "content": "The main point here is that Python’s class model is extremely dynamic. Classes\nand instances are just linked namespace objects, with attributes created on the fly\nby assignment. Those assignments usually happen within the class statements\nyou code, but they can occur anywhere you have a reference to one of the\nobjects in the tree.\nIn fact, even methods, normally created by a def nested in a class, can be\ncreated completely independently of any class object. The following, for\nexample, defines a simple function outside of any class that takes one argument:\n>>> def uppername(obj):\n        return obj.name.upper()       # Still needs a self argument (obj)\nThere is nothing about a class here yet—it’s a simple function, and it can be\ncalled as such at this point, provided we pass in an object obj with a name\nattribute, whose value in turn has an upper method—our class and instances\nhappen to fit the expected interface and kick off string uppercase conversion:\n>>> uppername(rec), uppername(x), uppername(y)\n('PAT', 'SUE', 'PAT')\nIf we assign this simple function to an attribute of our class, though, it becomes a\nmethod, callable through any instance, as well as through the class name itself as\nlong as we pass in an instance manually—a technique we’ll leverage further in\nthe next chapter:2\n>>> rec.method = uppername            # Now it's a class's method!\n>>> x.method()                        # Run method to process x\n'SUE'\n>>> y.method()                        # Same, but pass y to self\n'PAT'\n>>> rec.method(x)                     # Can call through instance or class\n'SUE'\nNormally, classes are filled out by class statements, and instance attributes are\ncreated by assignments to self attributes in method functions. The point again,\nthough, is that they don’t have to be; OOP in Python really is mostly about\nlooking up attributes in linked namespace objects.",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1028,
      "chapter": null,
      "content": "Records Revisited: Classes Versus Dictionaries\nAlthough the simple classes of the prior section are meant to illustrate class\nmodel basics, the techniques they employ can also be used for real work. For\nexample, Chapters 8 and 9 showed how to use dictionaries, tuples, and lists to\nrecord properties of entities in our programs, generically called records. It turns\nout that classes can often serve better in this role—they package information like\ndictionaries, but can also bundle processing logic in the form of methods. For\nreference, here is an example for tuple- and dictionary-based records we used\nearlier in the book (using one of many dictionary coding techniques):\n>>> rec = ('Pat', 40.5, ['dev', 'mgr'])     # Tuple-based record\n>>> print(rec[0])\nPat\n>>> rec = {}\n>>> rec['name'] = 'Pat'                     # Dictionary-based record\n>>> rec['age']  = 40.5                      # Or {...}, dict(n=v), etc.\n>>> rec['jobs'] = ['dev', 'mgr']\n>>> \n>>> print(rec['name'])\nPat\nAs we just saw, though, there are also multiple ways to do the same with classes.\nPerhaps the simplest is this—trading keys for attributes:\n>>> class rec: pass\n>>> rec.name = 'Pat'                        # Class-based record\n>>> rec.age  = 40.5\n>>> rec.jobs = ['dev', 'mgr']\n>>> \n>>> print(rec.name)\nPat\nThis code has substantially less syntax than the dictionary equivalent. It uses an\nempty class statement to generate an empty namespace object, which we then\nfill out by assigning class attributes over time, as before. This works, but a new\nclass statement will be required for each distinct record we will need. Perhaps\nmore typically, we can instead generate instances of an empty class to represent\neach distinct entity:",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1029,
      "chapter": null,
      "content": ">>> class rec: pass\n>>> pers1 = rec()                           # Instance-based records\n>>> pers1.name = 'Bob'\n>>> pers1.jobs = ['dev', 'mgr']\n>>> pers1.age  = 40.5\n>>>\n>>> pers2 = rec()\n>>> pers2.name = 'Sue'\n>>> pers2.jobs = ['dev', 'cto']\n>>>\n>>> pers1.name, pers2.name\n('Bob', 'Sue')\nHere, we make two records from the same class. Instances start out life empty,\njust like classes. We then fill in the records by assigning to attributes. This time,\nthough, there are two separate objects, and hence two separate name attributes. In\nfact, instances of the same class don’t even have to have the same set of attribute\nnames; in this example, one has a unique age name. Instances really are distinct\nnamespaces, so each has a distinct attribute dictionary. Although they are\nnormally filled out consistently by a class’s methods, they are more flexible than\nyou might expect.\nFinally, we might instead code a more full-blown class to implement the record\nand its processing—something that data-oriented dictionaries do not directly\nsupport:\n>>> class Person:\n        def __init__(self, name, jobs, age=None):      # class = data + logic\n            self.name = name\n            self.jobs = jobs\n            self.age  = age\n        def info(self):\n            return (self.name, self.jobs)\n>>> rec1 = Person('Bob', ['dev', 'mgr'], 40.5)         # Construction calls\n>>> rec2 = Person('Sue', ['dev', 'cto'])\n>>>\n>>> rec1.jobs, rec2.info()                             # Attributes + methods\n(['dev', 'mgr'], ('Sue', ['dev', 'cto']))\nThis scheme also makes multiple instances, but the class is not empty this time:\nwe’ve added logic (methods) to initialize instances at construction time and",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1030,
      "chapter": null,
      "content": "collect attributes into a tuple on request. The constructor imposes some\nconsistency on instances here by always setting the name, job, and age\nattributes, even though the latter can be omitted when an object is made.\nTogether, the class’s methods and instance attributes create a package, which\ncombines both data and logic.\nWe could further extend this code by adding logic to compute salaries, parse\nnames, and so on. Ultimately, we might link the class into a larger hierarchy to\ninherit and customize an existing set of methods via the automatic attribute\nsearch of classes, or perhaps even store instances of the class in a file with\nPython object pickling to make them persistent. In fact, we will—in the next\nchapter, we’ll expand on this analogy between classes and records with a more\nrealistic running example that demonstrates class basics in action.\nTo be fair to other tools, in this form, the two preceding class construction calls\nmore closely resemble dictionaries made all at once, but still seem less cluttered\nand provide extra processing methods. In fact, the class’s construction calls more\nclosely resemble Chapter 9’s named tuples—which makes sense, given that\nnamed tuples really are classes with extra logic to map attributes to tuple offsets:\n>>> rec = dict(name='Pat', jobs=['dev', 'mgr'], age=40.5)        # Dictionaries\n>>> rec = {'name': 'Pat', 'jobs': ['dev', 'mgr'], 'age': 40.5}   # Ditto\n>>> …setup code…\n>>> rec = Rec('Pat', ['dev', 'mgr'], 40.5)                       # Named tuples\nIn the end, although types like dictionaries and tuples are flexible, classes allow\nus to add behavior to objects in ways that built-in types and simple functions do\nnot directly support. Although we can store functions in dictionaries too (e.g.,\nunder key info to mimic the class’s method), using them to process implied\ninstances is nowhere near as natural and structured as it is in classes, and key\nlookup has no notion of inheritance to enable customization.\nBut to vet the purported benefits of classes firsthand, you’ll have to move ahead\nto the next chapter.",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1031,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter introduced the basics of coding classes in Python. We studied the\nsyntax of the class statement and learned how to use it to build up a class\ninheritance tree. We also studied how Python automatically fills in the first\nargument in method functions, how attributes are attached to objects in a class\ntree by simple assignment, and how specially named operator-overloading\nmethods intercept and implement built-in operations for our instances (e.g.,\nexpressions and printing).\nNow that we’ve explored the mechanics of classes in Python, the next chapter\nturns to a larger and more realistic example that ties together much of what\nwe’ve learned about OOP so far and introduces some new topics. After that,\nwe’ll continue our look at class coding, taking a second pass over the model to\nfill in some of the details that were omitted here to keep things simple. First,\nthough, let’s work through a quiz to review the basics we’ve covered so far.\nTest Your Knowledge: Quiz\n1. How are classes related to modules?\n2. How are instances and classes created?\n3. Where and how are class attributes created?\n4. Where and how are instance attributes created?\n5. What does self mean in a Python class?\n6. How is operator overloading coded in a Python class?\n7. When might you want to support operator overloading in your classes?\n8. Which operator-overloading method is most commonly used?\n9. What are the three key concepts required to understand Python OOP\ncode?",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1032,
      "chapter": null,
      "content": "Test Your Knowledge: Answers\n1. Classes are always nested inside a module; they are attributes of a\nmodule object. Classes and modules are both namespaces, but classes\ncorrespond to statements (not entire files) and support the OOP notions\nof multiple instances, inheritance, and operator overloading (modules\nmostly do not). In a sense, a module is like a single-instance class,\nwithout inheritance, which corresponds to an entire file of code.\n2. Classes are made by running class statements; instances are created by\ncalling a class as though it were a function.\n3. Class attributes are created by assigning attributes to a class object.\nThey are normally generated by top-level assignments nested in a class\nstatement—each name assigned in the class statement block becomes\nan attribute of the class object (technically, the class statement’s local\nscope morphs into the class object’s attribute namespace, much like a\nmodule). Class attributes can also be created, though, by assigning\nattributes to the class anywhere a reference to the class object exists—\neven outside the class statement.\n4. Instance attributes are created by assigning attributes to an instance\nobject. They are normally created within a class’s method functions\ncoded inside the class statement, by assigning attributes to the self\nargument (which is always the implied instance). Again, though, they\nmay be created by assignment anywhere a reference to the instance\nappears, even outside the class statement. Usually, all instance\nattributes are initialized in the __init__ constructor method; that way,\nlater method calls can assume the attributes already exist.\n5. self is the name commonly given to the first (leftmost) argument in a\nclass’s method function; Python automatically fills it in with the\ninstance object that is the implied subject of the method call. This\nargument need not be called self (though this is a very strong\nconvention); its position is what is significant. (Ex-C++ or Java\nprogrammers might prefer to call it this because in those languages\nthat name reflects the same idea; in Python, though, this argument must",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1033,
      "chapter": null,
      "content": "always be explicit in method headers and code.)\n6. Operator overloading is coded in a Python class with specially named\nmethods; they all begin and end with double underscores to make them\nunique. These are not built-in or reserved names; Python just runs them\nautomatically when an instance appears in the corresponding operation.\nPython itself defines the mappings from operations to special method\nnames.\n7. Operator overloading is useful to implement objects that resemble built-\nin types (e.g., sequences or numeric objects such as matrixes), and to\nmimic the built-in type interface expected by a piece of code.\nMimicking built-in type interfaces enables you to pass in class instances\nthat also have state information (i.e., attributes that remember data\nbetween operation calls). You generally shouldn’t use operator\noverloading when a simple named method will suffice, though.\n8. The __init__ constructor method is the most commonly used; almost\nevery class uses this method to set initial values for instance attributes\nand perform other startup tasks. The __str__ method (and its __repr__\nsibling) is not uncommon, but not as much of a fixture as __init__.\n9. The special self argument in method functions, the inheritance search\nfor attributes, and the __init__ constructor method are the\ncornerstones of OOP code in Python. If you get these, you should be\nable to read the text of most OOP Python code—apart from these, it’s\nlargely just packages of functions. In classes, self represents the\nautomatic object argument, and __init__ is commonly used to\ninitialize instances.\n1  Not to be confused with the __init__.py files in module packages! The method here is a class\nconstructor function used to initialize the newly created instance, not a namespace for a module-\npackage folder. See Chapter 24 for more details.\n2  In fact, this is one of the reasons the self argument must always be explicit in Python methods—\nbecause methods can be created as simple functions independent of a class, they need to include the\nimplied instance argument explicitly. They can be called as either functions or methods, and Python\ncan neither guess nor assume that a simple function might eventually become a class’s method. The",
      "content_length": 2228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1034,
      "chapter": null,
      "content": "main reason for the explicit self argument, though, is to make the meanings of names more\napparent: names not referenced through self are simple variables mapped to scopes, while names\nreferenced through self with attribute notation are obviously instance attributes.",
      "content_length": 267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1035,
      "chapter": null,
      "content": "Chapter 28. A More Realistic\nExample\nWe’ll dig into more class syntax details in the next chapter. Before we do,\nthough, let’s take a short detour to explore a realistic example of classes in action\nthat’s more practical than what we’ve seen so far. In this chapter, we’re going to\nbuild a set of classes that do something concrete—recording and processing\ninformation about people. As you’ll see, what we call instances and classes in\nPython programming can often serve the same roles as records and programs in\nmore traditional terms. The main difference here is the customization that\ninheritance will enable.\nSpecifically, in this chapter we’re going to code two classes:\nPerson—a class that creates and processes information about people\nManager—a customization of Person that modifies inherited behavior\nAlong the way, we’ll make instances of both classes and test out their\nfunctionality. When we’re done, this chapter will also show you a nice example\nuse case for classes—we’ll store our instances in a simple object-oriented\ndatabase, to make them permanent. That way, you can use this code as a\ntemplate for fleshing out a full-blown personal database of your own written\nentirely in Python.\nBesides actual utility, though, our aim here is also educational: this chapter\nprovides a tutorial on object-oriented programming in Python. Often, people\ngrasp the last chapter’s class syntax in the abstract but have trouble seeing how\nto get started when confronted with coding a new class from scratch. Toward this\nend, we’ll take it one step at a time here, to help you learn the basics; we’ll build\nup the classes gradually, so you can see how their features come together in\ncomplete programs.\nIn the end, our classes will still be relatively small in terms of code, but they will\ndemonstrate all of the main ideas in Python’s OOP model. Despite its syntax",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1036,
      "chapter": null,
      "content": "details, Python’s class system really is largely just a matter of searching for an\nattribute in a tree of objects, along with a special first argument for functions.\nStep 1: Making Instances\nOK, so much for the design phase—let’s move on to implementation. Our first\ntask is to start coding the main class, Person. In your favorite text editor, open a\nnew file for the code we’ll be writing.\nAs noted in the prior chapter, it’s a fairly strong convention in Python to begin\nmodule names with a lowercase letter and class names with an uppercase letter.\nLike the name of self arguments in methods, this is not required by the\nlanguage, but it’s so common that deviating might be confusing to people who\nlater read your code. To conform, we’ll call our new module file person.py and\nour class within it Person, as in Example 28-1.\nExample 28-1. person_1.py (start)\nclass Person:                             # Start a class\nWe’re going to change this file as we go. To help you keep track of its variations,\nwe’ll append an example number to the filename in captions, and use that\nnumber both in the examples package and in launches and imports here.\nChanges will also be shown in bold font on each revision. The intent is to show\nmods to a single file, but books are linear.\nAs noted in the prior chapter, we can code any number of functions and classes\nin a single module file in Python, and this one’s person.py name might not make\nmuch sense if we add unrelated components to it later. For now, we’ll assume\neverything in it will be Person-related. It probably should be anyhow—as we’ve\nlearned, modules tend to work best when they have a single, cohesive purpose.\nCoding Constructors\nNow, the first thing we want to do with our Person class is record basic\ninformation about people—to fill out record fields, if you will. Of course, these\nare known as instance object attributes in Python-speak, and they generally are\ncreated by assignment to self attributes in a class’s method functions. The",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1037,
      "chapter": null,
      "content": "normal way to give instance attributes their first values is to assign them to self\nin the __init__ constructor method, which contains code run automatically by\nPython each time an instance is created. Let’s add one to our class, in\nExample 28-2.\nExample 28-2. person_2.py (add attribute initialization)\nclass Person:\n   def __init__(self, name, job, pay):      # Constructor takes three arguments\n       self.name = name                     # Fill out fields when created\n       self.job  = job                      # self is the new instance object\n       self.pay  = pay\nThis is a very common coding pattern: we pass in the data to be attached to an\ninstance as arguments to the constructor method and assign them to self to\nretain them permanently. In OO terms, self is the newly created instance object,\nand name, job, and pay become state information—descriptive data saved on an\nobject for later use. Although other techniques (such as enclosing scope\nreference closures) can save details, too, instance attributes make this very\nexplicit and easy to understand.\nNotice that the argument names appear twice here. This code might even seem a\nbit redundant at first, but it’s not. The job argument, for example, is a local\nvariable in the scope of the __init__ function, but self.job is an attribute of\nthe instance that’s the implied subject of the method call. They are two different\nvariables, which happen to have the same name. By assigning the job local to\nthe self.job attribute with self.job=job, we save the passed-in job on the\ninstance for later use. As usual in Python, where a name is assigned, or what\nobject it is assigned to, determines what it means.\nSpeaking of arguments, there’s really nothing magical about __init__, apart\nfrom the fact that it’s called automatically when an instance is made, and has a\nspecial first argument. Despite its weird name, it’s a normal function and\nsupports all the features of functions we’ve already covered. We can, for\nexample, provide defaults for some of its arguments, so they need not be\nprovided in cases where their values aren’t available or useful.\nTo demonstrate, let’s make the job argument optional—it will default to None,\nmeaning the person being created is not (currently?) employed. If job defaults to",
      "content_length": 2276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1038,
      "chapter": null,
      "content": "None, we’ll probably want to default pay to 0, too, for consistency (unless some\nof the people you know manage to get paid without having jobs). In fact, we\nhave to specify a default for pay because according to Python’s syntax rules and\nChapter 18, any arguments in a function’s header after the first default must all\nhave defaults, too. Example 28-3 codes the mod.\nExample 28-3. person_3.py (add constructor defaults)\nclass Person:\n   def __init__(self, name, job=None, pay=0):         # Normal function args\n       self.name = name\n       self.job  = job\n       self.pay  = pay\nWhat this code means is that we’ll need to pass in a name when making\nPersons, but job and pay are now optional; they’ll default to None and 0 if\nomitted. The self argument, as usual, is filled in by Python automatically to\nrefer to the instance object—assigning values to attributes of self attaches them\nto the new instance.\nTesting as You Go\nThis class doesn’t do much yet—it essentially just fills out the fields of a new\nrecord—but it’s a real working class. At this point, we could add more code to it\nfor more features, but we won’t do that yet. As you’ve probably begun to\nappreciate already, programming in Python is really a matter of incremental\nprototyping—you write some code, test it, write more code, test again, and so\non. Because Python provides both an interactive session and nearly immediate\nturnaround after code changes, it’s more natural to test as you go than to write a\nhuge amount of code to test all at once.\nBefore adding more features, then, let’s test what we’ve got so far by making a\nfew instances of our class and displaying their attributes as created by the\nconstructor. We could do this interactively, but as you’ve also probably surmised\nby now, interactive testing has its limits—it gets tedious to have to reimport\nmodules and retype test cases each time you start a new testing session. More\ncommonly, Python programmers use the interactive prompt for simple one-off\ntests but do more substantial testing by writing code at the bottom of the file that\ncontains the objects to be tested, as in Example 28-4.",
      "content_length": 2128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1039,
      "chapter": null,
      "content": "Example 28-4. person_4.py (add incremental self-test code)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\nbob = Person('Bob Smith')                         # Test the class\nsue = Person('Sue Jones', job='dev', pay=100000)  # Runs __init__ automatically\nprint(bob.name, bob.pay)                          # Fetch attached attributes\nprint(sue.name, sue.pay)                          # sue's and bob's attrs differ\nNotice here that the bob object accepts the defaults for job and pay, but sue\nprovides values explicitly. Also, note how we use keyword arguments when\nmaking sue; we could pass by position instead, but the keywords may help\nremind us later what the data is, and they allow us to pass the arguments in any\nleft-to-right order we like. Again, despite its unusual name, __init__ is a\nnormal function, supporting everything you already know about functions—\nincluding both defaults and pass-by-name keyword arguments.\nWhen this file runs as a script, the test code at the bottom makes two instances of\nour class and prints two attributes of each—name and pay:\n$ python3 person_4.py\nBob Smith 0\nSue Jones 100000\nYou can also type this file’s test code at Python’s interactive prompt (assuming\nyou import the Person class there first), but coding canned tests inside the\nmodule file like this makes it much easier to rerun them in the future.\nAlthough this is fairly simple code, it’s already demonstrating something\nimportant. Notice that bob’s name is not sue’s, and sue’s pay is not bob’s. Each\nis an independent record of information. Technically, bob and sue are both\nnamespace objects—like all class instances, they each have their own\nindependent copy of the state information created by the class. Because each\ninstance of a class has its own set of self attributes, classes are a natural for\nrecording information for multiple objects this way; just like built-in types such\nas lists and dictionaries, classes serve as a sort of object factory.\nOther Python program structures, such as functions and modules, have no such",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1040,
      "chapter": null,
      "content": "concept. Chapter 17’s closure functions come close in terms of per-call state but\ndon’t have the multiple methods, inheritance, and larger structure we get from\nclasses.\nUsing Code Two Ways\nAs is, the test code at the bottom of the file works, but there’s a big catch—its\ntop-level print statements run both when the file is run as a script and when it is\nimported as a module. This means if we ever decide to import the class in this\nfile in order to use it somewhere else (and we will soon in this chapter), we’ll see\nthe output of its test code every time the file is imported. That’s not very good\nsoftware citizenship, though: client programs probably don’t care about our\ninternal tests and won’t want to see our output mixed in with their own.\nAlthough we could split the test code off into a separate file, it’s often more\nconvenient to code tests in the same file as the items to be tested. It would be\nbetter to arrange to run the test statements at the bottom only when the file is run\nfor testing, not when the file is imported. As linear readers of this book have\nalready learned, that’s exactly what the module __name__ check is designed for.\nExample 28-5 shows what this addition looks like—simply add the required test\nand indent your self-test code.\nExample 28-5. person_5.py (support both imports and run/tests)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\nif __name__ == '__main__':                  # When run for testing only\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob.name, bob.pay)\n   print(sue.name, sue.pay)\nNow, we get exactly the behavior we’re after—running the file as a top-level\nscript tests it because its __name__ is __main__, but importing it as a library of\nclasses later does not:\n$ python3 person_5.py\nBob Smith 0",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1041,
      "chapter": null,
      "content": "Sue Jones 100000\n$ python3\n>>> import person_5\n>>>\nWhen imported, the file now defines the class but does not use it. When run\ndirectly, this file creates two instances of our class as before, and prints two\nattributes of each; again, because each instance is an independent namespace\nobject, the values of their attributes differ.\nStep 2: Adding Behavior Methods\nEverything looks good so far—at this point, our class is essentially a record\nfactory; it creates and fills out fields of records (attributes of instances, in\nPythonic terms). Even as limited as it is, though, we can still run some\noperations on its objects. Although classes add an extra layer of structure, they\nultimately do most of their work by embedding and processing core object types\nlike lists and strings. In other words, if you already know how to use Python’s\nsimple core objects, you already know much of the Python class story; classes\nare really just a minor structural extension.\nFor example, the name field of our objects is a simple string, so we can extract\nlast names from our objects by splitting on spaces and indexing. These are all\ncore-object operations, which work whether their subjects are embedded in class\ninstances or not:\n>>> name = 'Bob Smith'      # Simple string, outside class\n>>> name.split()            # Extract last name\n['Bob', 'Smith']\n>>> name.split()[-1]        # Or [1], if always just two parts\n'Smith'\nSimilarly, we can give an object a pay raise by updating its pay field—that is, by\nchanging its state information in place with an assignment. This task also\ninvolves basic operations that work on Python’s core objects, regardless of\nwhether they are standalone or embedded in a class structure (formatting here\nmasks any extraneous digits):",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1042,
      "chapter": null,
      "content": ">>> pay = 100000            # Simple variable, outside class\n>>> pay *= 1.10             # Give a 10% raise\n>>> print(f'{pay:,.2f}')    # Or: pay = pay * 1.10, if you like to type\n110,000.00                  # Or: pay = pay + (pay * .10), if you really do\nTo apply these operations to the Person objects created by our script, simply do\nto bob.name and sue.pay what we just did to name and pay, as listed in\nExample 28-6. The operations are the same, but the subjects are attached as\nattributes to objects created from our class.\nExample 28-6. person_6.py (process embedded built-in objects)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob.name, bob.pay)\n   print(sue.name, sue.pay)\n   print(bob.name.split()[-1])            # Extract object's last name\n   sue.pay *= 1.10                        # Give this object a raise\n   print(f'{sue.pay:,.2f}')\nWe’ve added the last three lines here; when they’re run, we extract bob’s last\nname by using basic string and list operations on his name field, and give sue a\npay raise by modifying her pay attribute in place with basic number operations.\nIn a sense, sue is also a mutable object—her state changes in place just like a list\nafter an append call. Here’s the new version’s output when run as a top-level\nscript:\n$ python3 person_6.py\nBob Smith 0\nSue Jones 100000\nSmith\n110,000.00\nThe preceding code works as planned, but if you show it to a veteran software\ndeveloper he or she will probably tell you that its general approach is not a great\nidea in practice. Hardcoding operations like these outside of the class can lead to",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1043,
      "chapter": null,
      "content": "maintenance problems in the future.\nFor example, what if you’ve hardcoded the last-name-extraction formula at\nmany different places in your program? If you ever need to change the way it\nworks (to support a new name structure, for instance), you’ll need to hunt down\nand update every occurrence. Similarly, if the pay-raise code ever changes (e.g.,\nto require approval or database updates), you may have multiple copies to\nmodify. Just finding all the appearances of such code may be problematic in\nlarger programs—they may be scattered across many files and folders, split into\nindividual steps, and so on. In a prototype like this, frequent change is almost\nguaranteed.\nCoding Methods\nWhat we really want to do here is employ a software design concept known as\nencapsulation—wrapping up operation logic behind interfaces, such that each\noperation is coded only once in our program. That way, if our needs change in\nthe future, there is just one copy to update. Moreover, we’re free to change the\nsingle copy’s internals almost arbitrarily, without breaking the code that uses it.\nIn Python terms, we want to code operations on objects in a class’s methods,\ninstead of littering them throughout our program. In fact, this is one of the things\nthat classes are very good at—factoring code to remove redundancy and thus\noptimize maintainability. As an added bonus, turning operations into methods\nenables them to be applied to any instance of the class, not just those that they’ve\nbeen hardcoded to process.\nThis is all simpler in code than it may sound in theory. Example 28-7 achieves\nencapsulation by moving the two operations from code outside the class to\nmethods inside the class. While we’re at it, let’s change our self-test code at the\nbottom to use the new methods we’re creating, instead of hardcoding operations.\nExample 28-7. person_7.py (add methods to encapsulate operations)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\n   def lastName(self):                               # Behavior methods\n       return self.name.split()[-1]                  # self is implied subject",
      "content_length": 2171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1044,
      "chapter": null,
      "content": "def giveRaise(self, percent):\n       self.pay = int(self.pay * (1 + percent))      # Must change here only\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob.name, bob.pay)\n   print(sue.name, sue.pay)\n   print(bob.lastName(), sue.lastName())             # Use the new methods\n   sue.giveRaise(.10)                                # instead of hardcoding\n   print(sue.pay)\nAs we’ve learned, methods are simply normal functions that are attached to\nclasses and designed to process instances of those classes. The instance is the\nsubject of the method call and is passed to the method’s self argument\nautomatically.\nThe transformation to the methods in this version is straightforward. The new\nlastName method, for example, simply does for self what the previous version\nhardcoded for bob, because self is the implied subject when the method is\ncalled. lastName also returns the result because this operation is a called\nfunction now; it computes a value for its caller to use arbitrarily, even if it is just\nto be printed. Similarly, the new giveRaise method just does for self what we\ndid for sue before.\nWhen run now, our file’s output is similar to before—we’ve mostly just\nrefactored the code to allow for easier changes in the future (command lines that\nrun the most recent mod are often omitted from here on for space):\nBob Smith 0\nSue Jones 100000\nSmith Jones\n110000\nA few coding details are worth pointing out here. First, notice that sue’s pay is\nnow still an integer after a pay raise—we convert the math result back to an\ninteger by calling the int built-in within the method. Changing the value to\neither int or float is probably not a significant concern for this demo: integer\nand floating-point objects have the same interfaces and can be mixed within\nexpressions. Still, we may need to address truncation and rounding issues in a",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1045,
      "chapter": null,
      "content": "real system—money probably is significant to Persons!\nAs covered in Chapter 5, we might handle this by using the round(N, 2) built-\nin to round and retain cents, using the decimal type to fix precision, or storing\nmonetary values as full floating-point numbers and displaying them with a\nformatting string to show cents as we did earlier. For now, we’ll simply truncate\nany cents with int.\nSecond, notice that we’re also printing sue’s last name this time—because the\nlast-name logic has been encapsulated in a method, we get to use it on any\ninstance of the class. As we’ve seen, Python tells a method which instance to\nprocess by automatically passing it in to the first argument, usually called self.\nSpecifically:\nIn the first call, bob.lastName(), bob is the implied subject passed to\nself.\nIn the second call, sue.lastName(), sue goes to self instead.\nTrace through these calls to see how the instance winds up in self—it’s a key\nconcept. The net effect is that the method fetches the name of the implied subject\neach time. The same happens for giveRaise. We could, for example, give bob a\nraise by calling giveRaise for both instances this way, too. Unfortunately for\nbob, though, his zero starting pay will prevent him from getting a raise as the\nprogram is currently coded—nothing times anything is nothing, something we\nmay want to address in a future 2.0 release of our software.\nFinally, notice that the giveRaise method assumes that percent is passed in as\na floating-point number between zero and one. That may be too radical an\nassumption in the real world (a 1,000% raise would probably be a bug for most\nof us!); we’ll let it pass for this prototype, but we might want to test or at least\ndocument this in a future iteration of this code. Stay tuned for a rehash of this\nidea in later chapters, where we’ll explore function decorators and Python’s\nassert statement as alternatives that can do the validity test for us automatically\nduring development. In Chapter 39, for example, we’ll write a tool that lets us\nvalidate with strange incantations like the following:\n    @rangetest(percent=(0.0, 1.0))               # Use decorator to validate",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1046,
      "chapter": null,
      "content": "def giveRaise(self, percent):\n        self.pay = int(self.pay * (1 + percent))\nStep 3: Operator Overloading\nAt this point, we have a fairly full-featured class that generates and initializes\ninstances, along with two new bits of behavior for processing instances in the\nform of methods. So far, so good.\nAs it stands, though, testing is still a bit less convenient than it needs to be—to\ntrace our objects, we have to manually fetch and print individual attributes (e.g.,\nbob.name, sue.pay). It would be nice if displaying an instance all at once\nactually gave us some useful information. Unfortunately, the default display\nformat for an instance object isn’t very good—it displays the object’s class name\nand its address in memory (which is essentially useless in Python, except as a\nunique identifier).\nTo see this, change the last line in the latest version of our script to print(sue)\nso it displays the object as a whole. Here’s what you’ll get—as coded, the output\nsays that sue is an object, but not much more:\nBob Smith 0\nSue Jones 100000\nSmith Jones\n<__main__.Person object at 0x104972630>\nProviding Print Displays\nFortunately, it’s easy to do better by employing operator overloading—coding\nmethods in a class that intercept and process built-in operations when run on the\nclass’s instances. Specifically, we can make use of what are probably the second\nmost commonly used operator-overloading methods in Python, after __init__:\nthe __repr__ method we’ll deploy here, and its __str__ twin introduced in the\npreceding chapter.\nThese methods are run automatically every time an instance is converted to its\nprint string. Because that’s what print normally does, the net transitive effect is\nthat printing an object displays whatever is returned by the object’s __str__ or",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1047,
      "chapter": null,
      "content": "__repr__ method, if the object either defines one itself or inherits one from a\nsuperclass. Double-underscored names are inherited just like any other.\nTechnically, __str__ is preferred by print and str, and __repr__ is used both\nas a fallback for these, as well as by interactive echoes and nested appearances.\nAlthough the two can be used to implement different displays in different\ncontexts (e.g., user- or developer-focused), coding __repr__ alone suffices to\ngive a single display in all cases. This allows clients to provide an alternative\ndisplay with __str__, though this is a moot point for this demo.\nThe __init__ constructor method we’ve already coded is, strictly speaking,\noperator overloading too—it is run automatically at construction time to\ninitialize a newly created instance. Constructors are so common, though, that\nthey almost seem like a special case. More focused methods like __repr__\nallow us to tap into specific operations and provide specialized behavior when\nour objects are used in those contexts.\nLet’s put this into code. Example 28-8 extends our class to give a custom display\nthat lists attributes when our class’s instances are displayed as a whole, instead\nof relying on the less useful default display.\nExample 28-8. person_8.py (add __repr__ overload method for displays)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\n   def lastName(self):\n       return self.name.split()[-1]\n   def giveRaise(self, percent):\n       self.pay = int(self.pay * (1 + percent))\n   def __repr__(self):                                      # Added method\n       return f'[Person: {self.name} ${self.pay:,}]'        # String to print\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob)\n   print(sue)\n   print(bob.lastName(), sue.lastName())\n   sue.giveRaise(.10)\n   print(sue)",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1048,
      "chapter": null,
      "content": "Notice that we’re doing f-string formatting to build the display string in\n__repr__ here, with comma separators for pay; at the bottom, classes use built-\nin operations like these to get their work done. Again, everything you’ve already\nlearned about both built-in objects and user-defined functions applies to class-\nbased code. Classes largely just add an additional layer of structure that\npackages functions and data together and supports extensions.\nWe’ve also changed our self-test code to print objects directly, instead of printing\nindividual attributes. When run, the output is more uniform now; the “[…]” lines\nare returned by __repr__, run automatically by print operations:\n[Person: Bob Smith $0]\n[Person: Sue Jones $100,000]\nSmith Jones\n[Person: Sue Jones $110,000]\nStep 4: Customizing Behavior by Subclassing\nAt this point, our class captures much of the OOP machinery in Python: it makes\ninstances, provides behavior in methods, and even does a bit of operator\noverloading now to intercept print operations in __repr__. It effectively\npackages our data and logic together into a single, self-contained software\ncomponent, making it easy to locate code and straightforward to change it in the\nfuture. By allowing us to encapsulate behavior, it also allows us to factor that\ncode to avoid redundancy and its associated maintenance headaches.\nThe only major OOP concept it does not yet capture is customization by\ninheritance. In some sense, we’re already doing inheritance, because instances\ninherit methods from their classes. To demonstrate the real power of OOP,\nthough, we need to define a superclass/subclass relationship that allows us to\nextend our software and replace bits of inherited behavior. That’s the main idea\nbehind OOP, after all; by fostering a coding model based upon customization of\nwork already done, it can dramatically cut development time when used well.\nCoding Subclasses\nAs a next step, then, let’s put OOP’s methodology to use and customize our\nPerson class by extending our software hierarchy. For the purpose of this",
      "content_length": 2060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1049,
      "chapter": null,
      "content": "tutorial, we’ll define a subclass of Person called Manager that replaces the\ninherited giveRaise method with a more specialized version. Our new class\nbegins as follows:\nclass Manager(Person):                          # Define a subclass of Person\nThis code means that we’re defining a new class named Manager, which inherits\nfrom and may add customizations to the superclass Person. In plain terms, a\nManager is almost like a Person… (admittedly, a very long journey for a very\nsmall joke), but Manager has a custom way to give raises.1\nFor the sake of argument, let’s assume that when a Manager gets a raise, it\nreceives the passed-in percentage as usual, but also gets an extra bonus that\ndefaults to 10%. For instance, if a Manager’s raise is specified as 10%, it will\nreally get 20%. (Any relation to Persons living or dead is, of course, strictly\ncoincidental.) Our new method begins as follows; because this redefinition of\ngiveRaise will be closer in the class tree to Manager instances than the original\nversion in Person, it effectively replaces, and thereby customizes, the operation.\nRecall that according to the inheritance search rules, the lowest version of the\nname wins (we’ll add this code to the file in a moment):\nclass Manager(Person):                          # Inherit Person attrs\n    def giveRaise(self, percent, bonus=.10):    # Redefine to customize\nAugmenting Methods: The Bad Way\nNow, there are two ways we might code this Manager customization: a good way\nand a bad way. Let’s start with the bad way since it might be a bit easier to\nunderstand. The bad way is to cut and paste the code of giveRaise in Person\nand modify it for Manager, like this:\nclass Manager(Person):\n    def giveRaise(self, percent, bonus=.10):\n        self.pay = int(self.pay * (1 + percent + bonus))   # Bad: cut and paste\nThis would work as advertised—when we later call the giveRaise method of a\nManager instance, it would run this custom version, which tacks on the extra",
      "content_length": 1977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1050,
      "chapter": null,
      "content": "bonus. So what’s wrong with something that runs correctly?\nThe problem here is a very general one: anytime you copy code with cut and\npaste, you essentially double your maintenance effort in the future. Think about\nit: because we copied the original version, if we ever have to change the way\nraises are given (and we probably will), we’ll have to change the code in two\nplaces, not one. Although this is a small and artificial example, it’s also\nrepresentative of a universal issue—anytime you’re tempted to program by\ncopying code this way, you probably want to look for a better approach.\nAugmenting Methods: The Good Way\nWhat we really want to do here is somehow augment the original giveRaise,\ninstead of replacing it altogether. The good way to do that in Python is by calling\nto the original version directly, with augmented arguments, like this:\nclass Manager(Person):\n    def giveRaise(self, percent, bonus=.10):\n        Person.giveRaise(self, percent + bonus)            # Good: augment original\nThis code leverages the fact that a class’s method can always be called either\nthrough an instance (the usual way, where Python sends the instance to the self\nargument automatically) or through the class (the less common scheme, where\nyou must pass the instance manually). In more symbolic terms, a normal method\ncall of this form:\ninstance.method(args…)\nis automatically translated by Python into this equivalent form:\nclass.method(instance, args…)\nwhere the class containing the method to be run is determined by the inheritance\nsearch rule applied to the method’s name. You can code either form in your\nscript, but there is a slight asymmetry between the two—you must remember to\npass along the instance manually if you call through the class directly.\nThe method always needs a subject instance one way or another, and Python",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1051,
      "chapter": null,
      "content": "provides it automatically only for calls made through an instance. For calls\nthrough the class name, you need to send an instance to self yourself; and for\ncode inside a method like giveRaise, self already is the subject of the call, and\nhence the instance to pass along.\nCalling through the class directly effectively subverts inheritance and kicks the\ncall higher up the class tree to run a specific version. In our case, we can use this\ntechnique to invoke the default giveRaise in Person, even though it’s been\nredefined at the Manager level. In fact, we must call through Person this way,\nbecause a self.giveRaise() inside Manager’s giveRaise code would loop—\nsince self already is a Manager, self.giveRaise() would resolve again to\nManager’s giveRaise, and so on, recursively, until available memory is\nexhausted.\nCall syntax aside, this “good” version may seem like a small difference in code,\nbut it can make a huge difference for future code maintenance—because the\ngiveRaise logic lives in just one place now (Person’s method), we have only\none version to change in the future as needs evolve. And really, this form\ncaptures our intent more directly anyhow—we want to perform the standard\ngiveRaise operation, but simply tack on an extra bonus. Example 28-9 lists our\nentire module file with this step applied.\nExample 28-9. person_9.py (add method customization in a subclass)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\n   def lastName(self):\n       return self.name.split()[-1]\n   def giveRaise(self, percent):\n       self.pay = int(self.pay * (1 + percent))\n   def __repr__(self):\n       return f'[Person: {self.name} ${self.pay:,}]'\nclass Manager(Person):\n   def giveRaise(self, percent, bonus=.10):           # Redefine at this level\n       Person.giveRaise(self, percent + bonus)        # Call Person's version\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1052,
      "chapter": null,
      "content": "print(bob)\n   print(sue)\n   print(bob.lastName(), sue.lastName())\n   sue.giveRaise(.10)\n   print(sue)\n   pat = Manager('Pat Jones', 'mgr', 50000)           # Make a Manager: __init__\n   pat.giveRaise(.10)                                 # Runs custom version\n   print(pat.lastName())                              # Runs inherited method\n   print(pat)                                         # Runs inherited __repr__\nTo test our Manager subclass customization, we’ve also added self-test code that\nmakes a Manager, calls its methods, and prints it. When we make a Manager, we\npass in a name, and an optional job and pay as before—because Manager had no\n__init__ constructor, it inherits that in Person. Here’s the new version’s output:\n[Person: Bob Smith $0]\n[Person: Sue Jones $100,000]\nSmith Jones\n[Person: Sue Jones $110,000]\nJones\n[Person: Pat Jones $60,000]\nEverything looks good here: bob and sue are as before, and when pat the\nManager is given a 10% raise, he or she really gets 20% (pay goes from $50K to\n$60K), because the customized giveRaise in Manager is run for this person\nonly. Also notice how printing pat as a whole at the end of the test code displays\nthe nice format defined in Person’s __repr__: Manager objects get this,\nlastName, and the __init__ constructor method’s code “for free” from Person,\nby inheritance.\nTHE SUPER ALTERNATIVE\nTo extend inherited methods, the examples in this chapter simply call the\noriginal through the superclass name: Person.giveRaise(…). This is the\nexplicit, traditional, and simplest scheme in Python, and the one used in most\nof this book. It also follows the same pattern we’ll code to access class\nattributes: names attached to a class and shared by all instances, introduced\nahead. Methods are just callable class attributes.\nJava programmers may especially be interested to know that Python also has",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1053,
      "chapter": null,
      "content": "a super built-in function that allows calling back to a superclass’s methods\nmore generically. In the custom giveRaise of Example 28-9’s Manager\nclass, both of the following would work the same way:\nPerson.giveRaise(self, percent + bonus)     # Explicit, general use\nsuper().giveRaise(percent + bonus)          # Implicit, special case\nThe super built-in, however, relies on unusual semantics; works unevenly\nwith Python’s operator overloading; and does not always mesh well with\nmultiple inheritance, where a single method call may not suffice. It’s a\nspecial-case tool with a single role, which is redundant with general class-\nname references.\nIn its defense, the super call has a valid role too—cooperative same-named\nmethod dispatch in multiple inheritance trees—but this relies on the “MRO”\nclass ordering of Chapter 31, which many find artificial; requires universal\ndeployment to be used reliably; does not fully support method replacement\nand varying argument lists; and to many observers seems an esoteric solution\nto a role that is rare in real Python code.\nBecause of these downsides, this book prefers to call superclasses by explicit\nname instead of super, recommends the same policy for newcomers, and\ndefers presenting super until Chapter 32. You’re of course free to draw your\nown conclusions there, but super is usually best had after you learn the\nsimpler and more explicit ways of achieving the same goals in Python,\nespecially if you’re new to OOP. Topics like cooperative multiple-\ninheritance dispatch are a lot to digest—for beginners and experts alike.\nAnd to any Java programmers in the audience: try resisting the temptation to\nuse Python’s super until you’ve had a chance to study its subtle\nimplications. This call is benign in single-inheritance of the sort you’re used\nto, but once you step up to Python’s multiple inheritance, super is not what\nyou think it is, and more than you probably expect. The class it picks can\nvary per context, and may not even be a superclass at all!\nPolymorphism in Action",
      "content_length": 2033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1054,
      "chapter": null,
      "content": "To make this acquisition of inherited behavior even more striking, add the\nfollowing code at the end of our file temporarily (this is file person_9_plus.py in\nthe examples package, but doesn’t warrant a full listing or caption here):\nif __name__ == '__main__':\n    …\n    print('--All three--')\n    for obj in (bob, sue, pat):               # Process objects generically\n        obj.giveRaise(.10)                    # Run this object's giveRaise\n        print(obj)                            # Run the common __repr__\nHere’s the resulting output, showing just its new parts (and the accumulated\neffects of two raises per person):\n…\n--All three--\n[Person: Bob Smith $0]\n[Person: Sue Jones $121,000]\n[Person: Pat Jones $72,000]\nIn the added code, object is either a Person or a Manager, and Python runs the\nappropriate giveRaise automatically—our original version in Person for bob\nand sue, and our customized version in Manager for pat. Trace the method calls\nyourself to see how Python selects the right giveRaise method for each object.\nThis is just Python’s notion of polymorphism, which we met earlier in the book,\nat work again—what giveRaise does depends on what you do it to. Here, it’s\nmade all the more obvious when it selects from code we’ve written ourselves in\nclasses. The practical effect in this code is that sue gets another 10% but pat\ngets another 20% because giveRaise is dispatched based on the object’s type.\nAs we’ve seen, polymorphism is at the heart of Python’s flexibility. Passing any\nof our three objects to a function that calls a giveRaise method, for example,\nwould have the same effect: the appropriate version would be run automatically,\ndepending on which type of object was passed.\nOn the other hand, printing runs the same __repr__ for all three objects,\nbecause it’s coded just once in Person. Manager both specializes and applies the\ncode we originally wrote in Person. Although this example is small, it’s already\nleveraging OOP’s talent for code customization and reuse; with classes, this",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1055,
      "chapter": null,
      "content": "almost seems automatic at times.\nInherit, Customize, and Extend\nIn fact, classes can be even more flexible than our example implies. In general,\nclasses can inherit, customize, or extend existing code in superclasses. For\nexample, although we’re focused on customization here, we can also add unique\nmethods to Manager that are not present in Person, if Managers require\nsomething different. The following abstract snippet illustrates. Here, giveRaise\nredefines a superclass’s method to customize it, but someThingElse defines\nsomething new to extend:\nclass Person:\n    def lastName(self): …\n    def giveRaise(self): …\n    def __repr__(self): …\nclass Manager(Person):                 # Inherit\n    def giveRaise(self, …): …          # Customize\n    def someThingElse(self, …): …      # Extend\npat = Manager()\npat.lastName()           # Inherited verbatim\npat.giveRaise()          # Customized version\npat.someThingElse()      # Extension here\nprint(pat)               # Inherited overload method\nExtra methods like this code’s someThingElse augment the existing software\nand are available on Manager objects only, not on Persons. For the purposes of\nthis tutorial, however, we’ll limit our scope to customizing some of Person’s\nbehavior by redefining it, not adding to it.\nOOP: The Big Idea\nAs is, our code may be small, but it’s fairly functional. And really, it already\nillustrates the main point behind OOP in general: in OOP, we program by\ncustomizing what has already been done, rather than copying or changing\nexisting code. This isn’t always an obvious win to newcomers at first glance,\nespecially given the extra coding requirements of classes. But overall, the\nprogramming style implied by classes can cut development time radically",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1056,
      "chapter": null,
      "content": "compared to other approaches.\nFor instance, in our example we could theoretically have implemented a custom\ngiveRaise operation without subclassing, but none of the other options yield\ncode as optimal as ours:\nAlthough we could have simply coded Manager from scratch as new,\nindependent code, we would have had to reimplement all the behaviors\nin Person that are the same for Managers.\nAlthough we could have simply changed the existing Person class in\nplace for the requirements of Manager’s giveRaise, doing so would\nbreak code that still needs the original Person behavior.\nAlthough we could have simply copied the Person class in its entirety,\nrenamed the copy to Manager, and changed its giveRaise, doing so\nwould introduce code redundancy that would double our work in the\nfuture—changes made to Person in the future would not be picked up\nautomatically, but would have to be manually propagated to Manager’s\ncode. As usual, the cut-and-paste approach may seem quick now, but it\ndoubles your work in the future.\nThe customizable hierarchies we can build with classes provide a much better\nsolution for software that will evolve over time. No other tools in Python support\nthis development mode. Because we can tailor and extend our prior work by\ncoding new subclasses, we can leverage what we’ve already done, rather than\nstarting anew each time, breaking what already works, or introducing multiple\nredundant copies. When done right, OOP is a powerful ally.\nStep 5: Customizing Constructors, Too\nOur code works as it is, but if you study the current version closely, you may be\nstruck by something a bit odd—it seems pointless to have to provide a mgr job\nname for Manager objects when we create them: this is already implied by the\nclass itself. It would be better if we could somehow fill in this value\nautomatically when a Manager is made.",
      "content_length": 1849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1057,
      "chapter": null,
      "content": "The trick we need to improve on this turns out to be the same as the one we\nemployed in the prior section: we want to customize the constructor logic for\nManagers in such a way as to provide a job name automatically. In terms of\ncode, we want to redefine an __init__ method in Manager that provides the mgr\nstring for us. And as in giveRaise customization, we also want to run the\noriginal __init__ in Person by calling through the class name, so it still\ninitializes our objects’ state information attributes.\nThe mods in Example 28-10 will do the job—we’ve coded the new Manager\nconstructor and changed the call that creates pat to not pass in the mgr job name.\nExample 28-10. person_10.py (add constructor customization in a subclass)\nclass Person:\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\n   def lastName(self):\n       return self.name.split()[-1]\n   def giveRaise(self, percent):\n       self.pay = int(self.pay * (1 + percent))\n   def __repr__(self):\n       return f'[Person: {self.name} ${self.pay:,}]'\nclass Manager(Person):\n   def __init__(self, name, pay):                     # Redefine constructor\n       Person.__init__(self, name, 'mgr', pay)        # Run original with 'mgr'\n   def giveRaise(self, percent, bonus=.10):\n       Person.giveRaise(self, percent + bonus)\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob)\n   print(sue)\n   print(bob.lastName(), sue.lastName())\n   sue.giveRaise(.10)\n   print(sue)\n   pat = Manager('Pat Jones', 50000)                   # Job name set by class\n   pat.giveRaise(.10)\n   print(pat.lastName())\n   print(pat)\nAgain, we’re using the same technique to augment the __init__ constructor\nhere that we used for giveRaise earlier—running the superclass version by",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1058,
      "chapter": null,
      "content": "calling through the class name directly and passing the self instance along\nexplicitly. Although the constructor has a strange name, the effect is identical.\nBecause we need Person’s construction logic to run too (to initialize instance\nattributes), we really have to call it this way; otherwise, instances would not\nhave any attributes attached.\nCalling superclass constructors from redefinitions this way turns out to be a very\ncommon coding pattern in Python. By itself, Python uses inheritance to look for\nand call only one __init__ method at construction time—the lowest one in the\nclass tree. If you need higher __init__ methods to be run at construction time\n(and you usually do), you must call them manually, and usually through the\nsuperclass’s name. The upside to this is that you can be explicit about which\nargument to pass up to the superclass’s constructor and can choose to not call it\nat all: not calling the superclass constructor allows you to replace its logic\naltogether, rather than augmenting it.\nThe output of this file’s self-test code is the same as before—we haven’t changed\nwhat this example does, we’ve simply restructured it to get rid of some logical\nredundancy:\n$ python3 person_10.py\n[Person: Bob Smith $0]\n[Person: Sue Jones $100,000]\nSmith Jones\n[Person: Sue Jones $110,000]\nJones\n[Person: Pat Jones $60,000]\nPer the sidebar “The super Alternative”, an implicit super().__init__(…) sans\nself would run the superclass constructor too, and this is a common role for this\nbuilt-in in single-inheritance class trees. For all the reasons noted earlier, though,\nlet’s stick to the explicit and general for now.\nOOP Is Simpler Than You May Think\nIn this complete form, and despite their relatively small sizes, our classes capture\nnearly all the important concepts in Python’s OOP machinery:\nInstance creation—filling out instance attributes",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1059,
      "chapter": null,
      "content": "Behavior methods—encapsulating logic in a class’s methods\nOperator overloading—providing behavior for built-in operations like\nprinting\nCustomizing behavior—redefining methods in subclasses to specialize\nthem\nCustomizing constructors—adding initialization logic to superclass\nsteps\nMost of these concepts are based upon just three simple ideas: the inheritance\nsearch for attributes in object trees, the special self argument in methods, and\noperator overloading’s automatic dispatch to methods.\nAlong the way, we’ve also made our code easy to change in the future, by\nharnessing the class’s propensity for factoring code to reduce redundancy. For\nexample, we wrapped up logic in methods and called back to superclass methods\nfrom extensions to avoid having multiple copies of the same code. Most of these\nsteps were a natural outgrowth of the structuring power of classes.\nBy and large, that’s all there is to OOP in Python. Classes certainly can become\nlarger than this, and there are some more advanced class concepts, such as\ndecorators and metaclasses, which we will explore in later chapters. In terms of\nthe basics, though, our classes already do it all. In fact, if you’ve grasped the\nworkings of the classes we’ve written, most OOP Python code should now be\nwithin your reach.\nOther Ways to Combine Classes: Composites\nHaving said that, it should be noted that although the basic mechanics of OOP\nare simple in Python, some of the art in larger programs lies in the way that\nclasses are put together. We’re focusing on inheritance in this tutorial because\nthat’s the mechanism the Python language provides, but programmers sometimes\ncombine classes in other ways, too.\nFor example, a common coding pattern involves nesting objects inside each\nother to build up composites. We’ll explore this pattern in more detail in\nChapter 31, which is really more about design than about Python. As a quick",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1060,
      "chapter": null,
      "content": "example, though, we could use this composition idea to code our Manager\nextension by embedding a Person, instead of inheriting from it.\nThe alternative Manager in Example 28-11 does so in part by using the\n__getattr__ operator-overloading method to intercept undefined attribute\nfetches, and the getattr built-in to delegate them to the embedded object:\nThe __getattr__ method isn’t covered in full until Chapter 30, but its\nusage is simple enough to employ here—it’s automatically run for all\nfetches of undefined attributes (those not found in the instance by\nnormal inheritance search). And yes, this is the same name used for the\nnonclass module-attributes hook of Chapter 25.\nThe getattr call was introduced in Chapter 25—it’s the same as X.Y\nattribute fetch notation and thus performs inheritance, but the attribute\nname Y is evaluated to yield a string, not a name interpreted literally.\nMore specifically here, the constructor makes the embedded object with job\nname; giveRaise customizes by changing the argument passed along to the\nembedded object; and lastName triggers __getattr__ to reroute to the\nembedded object. In effect, Manager becomes a controller layer that passes calls\ndown to the embedded object, rather than up to superclass methods.\nExample 28-11. person-composite.py (embedding-based Manager alternative)\nfrom person_10 import Person                        # Example 28-10's Person\nclass Manager:\n   def __init__(self, name, pay):\n       self.person = Person(name, 'mgr', pay)      # Embed a Person object\n   def giveRaise(self, percent, bonus=.10):\n       self.person.giveRaise(percent + bonus)      # Intercept and delegate\n   def __getattr__(self, attr):\n       return getattr(self.person, attr)           # Delegate all other attrs\n   def __repr__(self):\n       return str(self.person)                     # Must overload again per ahead\nif __name__ == '__main__':\n   pat = Manager('Pat Jones', 50000)               # Embed a Person\n   pat.giveRaise(.10)                              # Run Manager.giveRaise",
      "content_length": 2038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1061,
      "chapter": null,
      "content": "print(pat.lastName())                           # Delegate to embedded\n   print(pat)                                      # Run Manager.__repr__\nThe output of this version tests just its Manager class (the imported Example 28-\n10 tests its own code). As before, a 10% raise is munged to 20% by the custom\ngiveRaise here, but other calls are handled by the embedded Person, by either\ndirect calls or generic attribute rerouting:\n$ python3 person-composite.py\nJones\n[Person: Pat Jones $60,000]\nThe more important point here is that this Manager alternative is representative\nof a general coding pattern usually known as delegation—a composite-based\nstructure that manages a wrapped object and propagates method calls to it.\nThis pattern works in our example, but it requires about twice as much code and\nis less well suited than inheritance to the kinds of direct customizations we\nmeant to express. In fact, reasonable Python programmers would almost\ncertainly not code this example this way in practice. Manager isn’t really a\nPerson here, so we need extra code to manually dispatch method calls to the\nembedded object; operator-overloading methods like __repr__ must be\nredefined for reasons explained in the sidebar “Delegating Built-ins—or Not”;\nand adding new Manager behavior is less straightforward since state information\nis one level removed.\nStill, object embedding, and design patterns based upon it, can be a good fit\nwhen embedded objects require more limited interaction with the container than\ndirect customization implies. A controller layer, or proxy, like this alternative\nManager, for example, might come in handy if we want to adapt a class to an\nexpected interface it does not support, or trace or validate calls to another\nobject’s methods (indeed, we will use a nearly identical coding pattern when we\nstudy class decorators later in the book).\nMoreover, a Department class prototype like that in Example 28-12 could\naggregate other objects in order to treat them as a set.\nExample 28-12. person-department.py (aggregate embedded objects into a\ncomposite)",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1062,
      "chapter": null,
      "content": "from person_10 import Person, Manager           # Example 28-10's classes\nclass Department:\n   def __init__(self, *args):\n       self.members = list(args)               # Manage an objects list\n   def addMember(self, person):\n       self.members.append(person)\n   def giveRaises(self, percent):              # Apply methods to all objects\n       for person in self.members:\n           person.giveRaise(percent)\n   def showAll(self):                          # Display all nested objects\n       for person in self.members:\n           print(person)\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   pat = Manager('Pat Jones', 50000)\n   development = Department(bob, sue)          # Embed objects in a composite\n   development.addMember(pat)\n   development.giveRaises(.10)                 # Runs embedded objects' giveRaise\n   development.showAll()                       # Runs embedded objects' __repr__\nWhen run, the department’s showAll method lists all of its contained objects\nafter updating their state in true polymorphic fashion with giveRaises:\n$ python3 person-department.py\n[Person: Bob Smith $0]\n[Person: Sue Jones $110,000]\n[Person: Pat Jones $60,000]\nInterestingly, this code uses both inheritance and composition—Department is a\ncomposite that embeds and controls other objects to aggregate, but the embedded\nPerson and Manager objects themselves use inheritance to customize. As\nanother example, a GUI might similarly use inheritance to customize the\nbehavior or appearance of labels and buttons, but also composition to build up\nlarger packages of embedded widgets, such as input forms, calculators, and text\neditors. The class structure to use depends on the objects you are trying to model\n—in fact, the ability to model real-world entities this way is one of OOP’s\nstrengths.",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1063,
      "chapter": null,
      "content": "Design issues like composition are explored in Chapter 31, so we’ll postpone\nfurther investigations for now. But again, in terms of the basic mechanics of\nOOP in Python, our Person and Manager classes already tell the entire story.\nNow that you’ve mastered the basics of OOP, though, developing general tools\nfor applying it more easily in your scripts is often a natural next step—and the\ntopic of the next section.\nDELEGATING BUILT-INS—OR NOT\nNotice how the delegation-based Manager class of Example 28-11 redefines\nthe __repr__ run by print, instead of allowing it to be caught by\n__getattr__ like other attributes. In general, classes cannot intercept and\ndelegate operator-overloading method attributes used by built-in operations\nwithout redefining them this way. Although we know that __repr__ is the\nonly such name used in our specific example, this is a broader issue for\ndelegation-based classes.\nRecall that built-in operations like printing and addition implicitly invoke\noperator-overloading methods such as __repr__ and __add__. Built-in\noperations like these, however, do not route their implicit attribute fetches\nthrough generic attribute managers: neither __getattr__ (run for undefined\nattributes, and used here) nor __getattribute__ (run for all attributes, and\ncovered later) is invoked. Moreover, the implicit object superclass at the\ntop of all class trees defines defaults that can preclude __getattr__ too.\nHence, Manager must redefine __repr__ redundantly, in order to route\nprinting to the embedded Person object. Comment out this class’s __repr__\nmethod to see this live—the Manager instance then prints with a default\ninstead of our custom display, because __getattr__ is never run for\nprinting:\n$ python3 person-composite.py\nJones\n<__main__.Manager object at 0x10a292c30>\nTechnically, built-in operations begin their implicit search for method names\nat the class, skipping the instance entirely. By contrast, explicit by-name",
      "content_length": 1955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1064,
      "chapter": null,
      "content": "attribute fetches are always routed to the instance first. Classes also inherit a\ndefault __repr__ from their automatic object superclass that would prevent\n__getattr__ from running too, but the more absolute __getattribute__\ndoesn’t intercept the name either, and other methods not in object also\nwon’t be caught by __getattr__, like __add__ for +.\nThis was a mod in Python 3.X, but isn’t a showstopper: delegation-based\nclasses can still redefine operator-overloading methods to delegate them to\nwrapped objects, either manually or via tools or superclasses. It’s also too\nadvanced to explore further in this tutorial, but watch for it to crop up in\nChapter 38, be solved in Chapter 39, and make a cameo appearance as a\nspecial case in the formal inheritance definition of Chapter 40.\nStep 6: Using Introspection Tools\nLet’s make one final tweak before we throw our objects onto a database. As they\nare, our classes are complete and demonstrate most of the basics of OOP in\nPython. They still have two remaining issues we probably should iron out,\nthough, before we go live with them:\nFirst, if you look at the display of the objects as they are right now,\nyou’ll notice that when you print pat the Manager, the display labels it\nas a Person. That’s not technically incorrect, since Manager is a kind of\ncustomized and specialized Person. Still, it would be more accurate to\ndisplay an object with the most specific (that is, lowest) class possible:\nthe one an object is made from.\nSecond, and perhaps more importantly, the current display format\nshows only the attributes we include in our __repr__, and that might\nnot account for future goals. For example, we can’t yet verify that pat’s\njob name has been set to mgr correctly by Manager’s constructor,\nbecause the __repr__ we coded for Person does not print this field.\nWorse, if we ever expand or otherwise change the set of attributes\nassigned to our objects in __init__, we’ll have to remember to also\nupdate __repr__ for new names to be displayed, or it will become out",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1065,
      "chapter": null,
      "content": "of sync over time.\nThe last point means that, yet again, we’ve made potential extra work for\nourselves in the future by introducing redundancy in our code. Because any\ndisparity in __repr__ will be reflected in the program’s output, this redundancy\nmay be more obvious than the other forms we addressed earlier; still, avoiding\nextra work in the future is a generally good thing.\nSpecial Class Attributes\nWe can address both issues with Python’s introspection tools—special attributes\nand functions that give us access to some of the internals of objects’\nimplementations. These tools are somewhat advanced and generally used more\nby people writing tools for other programmers to use than by programmers\ndeveloping applications. Even so, a basic knowledge of some of these tools is\nuseful because they allow us to write code that processes classes in generic\nways. In our code, for example, there are two hooks that can help us out, both of\nwhich were introduced near the end of the preceding chapter and used in earlier\nexamples:\nThe built-in instance.__class__ attribute provides a link from an\ninstance to the class from which it was created. Classes in turn have a\n__name__, just like modules, and a __bases__ sequence that provides\naccess to superclasses. We can use these here to print the name of the\nclass from which an instance is made rather than one we’ve hardcoded.\nThe built-in object.__dict__ attribute provides a dictionary with one\nkey/value pair for every attribute attached to a namespace object\n(including modules, classes, and instances). Because it is a dictionary,\nwe can fetch its keys list, index by key, iterate over its keys, and so on,\nto process all attributes generically. We can use this here to print every\nattribute in any instance, not just those we hardcode in custom displays,\nmuch as we did in Chapter 25’s module tools.\nWe met the first of these categories in the prior chapter, but here’s a quick review\nat Python’s interactive prompt with the latest versions of our person.py classes\n(Example 28-10). Notice how we load Person at the interactive prompt with a",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1066,
      "chapter": null,
      "content": "from statement here—class names live in and are imported from modules,\nexactly like function names and other variables:\n>>> from person import Person\n>>> pat = Person('Pat Jones')\n>>> pat                                        # Run pat's __repr__\n[Person: Pat Jones $0] \n>>> pat.__class__                              # Show pat's class and its name\n<class 'person.Person'>\n>>> pat.__class__.__name__\n'Person'\n>>> list(pat.__dict__.keys())                  # Attributes are dict keys\n['name', 'job', 'pay']\n>>> for key in pat.__dict__:\n        print(key, '=>', pat.__dict__[key])    # Index manually: no inheritance\nname => Pat Jones\njob => None\npay => 0\n>>> for key in pat.__dict__:                   # obj.attr, but attr is a string\n        print(key, '=>', getattr(pat, key))    # Runs attr inheritance\n…same as prior output…\nAs noted briefly in the prior chapter, some attributes accessible from an instance\nmight not be stored in the __dict__ dictionary if the instance’s class defines a\n__slots__: an optional and relatively obscure feature of classes that stores\nattributes sequentially in the instance; may preclude an instance __dict__\naltogether; and which we won’t study in full until Chapter 32. Since slots really\nbelong to classes instead of instances, and since they are rarely used in any\nevent, we can reasonably ignore them here and focus on the normal __dict__.\nAs we do, though, keep in mind that some programs may need to catch\nexceptions for a missing __dict__, or use built-in functions like hasattr,\ngetattr, and dir if its users might deploy slots. As you’ll learn in Chapter 32,\nthe next section’s code won’t fail if used by a class with slots (its lack of them is\nenough to guarantee a __dict__) but slots—and other “virtual” attributes—\nwon’t be reported as instance data.",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1067,
      "chapter": null,
      "content": "A Generic Display Tool\nWe can put these interfaces to work in a superclass that displays accurate class\nnames and formats all attributes of an instance of any class. This superclass is\ncoded in Example 28-13—it’s a new, independent module named classtools.py.\nBecause its __repr__ display overload uses generic introspection tools, it will\nwork on any instance, regardless of the instance’s attributes set. And because\nthis is a class, it automatically becomes a general formatting tool: thanks to\ninheritance, it can be mixed into any class that wishes to use its display format.\nAs an added bonus, if we ever want to change how instances are displayed we\nneed only change this class—every class that inherits its __repr__ will\nautomatically pick up the new format when next used by a program.\nExample 28-13. classtools.py (new)\n\"Assorted class utilities and tools\"\nclass AttrDisplay:\n   \"\"\"\n   Provides an inheritable display overload method that shows\n   instances with their class names and a name=value pair for\n   each attribute stored on the instance itself (but not attrs\n   inherited from its classes). Can be mixed into any class,\n   and will work on any instance.\n   \"\"\"\n   def gatherAttrs(self):\n       attrs = []\n       for key in sorted(self.__dict__):\n           attrs.append(f'{key}={getattr(self, key)}')\n       return ', '.join(attrs)\n   def __repr__(self):\n       return f'[{self.__class__.__name__}: {self.gatherAttrs()}]'\nif __name__ == '__main__':\n   class TopTest(AttrDisplay):\n       count = 0\n       def __init__(self):\n           self.attr1 = TopTest.count\n           self.attr2 = TopTest.count+1\n           TopTest.count += 2\n   class SubTest(TopTest):\n       pass",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1068,
      "chapter": null,
      "content": "X, Y = TopTest(), SubTest()      # Make two instances\n   print(X)                         # Show all instance attrs\n   print(Y)                         # Show lowest class name\nNotice the docstrings here—because this is a general-purpose tool, we want to\nadd some functional documentation for potential users to read. As we saw in\nChapter 15, docstrings can be placed at the top of simple functions and modules,\nand also at the start of classes and any of their methods; the help function and\nthe PyDoc tool extract and display these automatically. We’ll revisit docstrings\nfor classes in Chapter 29.\nWhen run directly, this module’s self-test makes two instances and prints them;\nthe __repr__ defined here shows the instance’s class, and all its attributes’\nnames and values, in sorted attribute name order:\n$ python3 classtools.py\n[TopTest: attr1=0, attr2=1]\n[SubTest: attr1=2, attr2=3]\nAnother design note here: because this class uses __repr__ instead of __str__\nits displays are used in all contexts, but its clients also won’t have the option of\nproviding an alternative low-level display—they can still add a __str__, but this\napplies to print and str only. In a more general tool, using __str__ instead\nlimits a display’s scope, but leaves clients the option of adding a __repr__ for a\nsecondary display at interactive prompts and nested appearances. We’ll follow\nthis alternative policy when we code expanded versions of this class in\nChapter 31; for this demo, we’ll stick with the all-inclusive __repr__.\nInstance Versus Class Attributes\nIf you study the classtools module’s self-test code long enough, you’ll notice\nthat its class displays only instance attributes, attached to the self object at the\nbottom of the inheritance tree; that’s what self’s __dict__ contains. As an\nintended consequence, we don’t see attributes inherited by the instance from\nclasses above it in the tree.\nFor example, the attribute TopTest.count used as an instance counter in this",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1069,
      "chapter": null,
      "content": "file’s self-test code is a class attribute that lives only on the class: it’s assigned in\nthe class block like methods, and referenced in methods by explicit class name—\nmuch like explicit calls to customized class methods. It’s also inherited by\ninstances, but inherited class attributes are attached to the class only, not copied\ndown to instances. There’s more on such nonmethod class attributes in the next\nchapter; the main point here is that count won’t be displayed by AttrDisplay.\nIf you ever do wish to include inherited attributes too, you can climb the\n__class__ link to the instance’s class, use the __dict__ there to fetch class\nattributes, and then iterate through the class’s __bases__ attribute to climb to\neven higher superclasses, repeating as necessary. If you’re a fan of simple code,\nrunning a built-in dir call on the instance instead of using __dict__ and\nclimbing would have much the same effect, since dir results include inherited\nnames in sorted results lists (along with built-in __X__ methods that are easy to\nfilter out as usual):\n$ python3\n>>> from person_10 import Person\n>>> pat = Person('Pat Jones')\n>>> list(pat.__dict__)\n['name', 'job', 'pay']\n>>> [a for a in dir(pat) if not a.startswith('__')]\n['giveRaise', 'job', 'lastName', 'name', 'pay']\nIn the interest of space, we’ll leave optional display of inherited class attributes\nwith either tree climbs or dir as suggested experiments for now. For more hints\non this front, though, watch for the classtree.py inheritance-tree climber we will\nwrite in Chapter 29, and the lister.py attribute listers and climbers we’ll code in\nChapter 31.\nName Considerations in Tool Classes\nOne last subtlety here: because our AttrDisplay class in the classtools\nmodule is a general tool designed to be mixed into other arbitrary classes, we\nhave to be aware of the potential for unintended name collisions with client\nclasses. As is, the code assumes that client subclasses may want to use both its",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1070,
      "chapter": null,
      "content": "__repr__ and gatherAttrs, but the latter of these may be more than a subclass\nexpects—if a subclass innocently defines a gatherAttrs name of its own, it will\nlikely break our class, because the lower version in the subclass will be used\ninstead of ours.\nTo see this for yourself, add a gatherAttrs to TopTest in the file’s self-test\ncode; unless the new method is identical, or intentionally customizes the\noriginal, our tool class will no longer work as planned—self.gatherAttrs\nwithin AttrDisplay searches anew from the TopTest instance:\n    class TopTest(AttrDisplay):\n        …\n        def gatherAttrs(self):         # Replaces method in AttrDisplay!\n            return 'Oops'\nThis isn’t necessarily bad—sometimes we want other methods to be available to\nsubclasses, either for direct calls or for customization this way. If we really\nmeant to provide a __repr__ only, though, this is less than ideal.\nTo minimize the chances of name collisions like this, Python programmers often\nprefix methods not meant for external use with a single underscore:\n_gatherAttrs in our case. This isn’t foolproof (what if another class defines its\nown “private” _gatherAttrs, too?), but it’s usually sufficient, and it’s a\ncommon Python naming convention for methods internal to a class.\nA better solution would be to use two underscores at the front of the method\nname only: __gatherAttrs for us. Python automatically expands such names to\ninclude the enclosing class’s name, which makes them truly unique when looked\nup by the inheritance search. This is a feature usually called pseudoprivate class\nattributes, which we’ll expand on in Chapter 31 and deploy in an expanded\nversion of this class there. For now, we’ll make both our methods available.\nOur Classes’ Final Form\nNow, to use the preceding section’s generic display tool in our classes, all we\nneed to do is import it from its module, mix it in by inheritance in our top-level\nclass, and get rid of the more specific __repr__ we coded before. The new\ndisplay overload method will be inherited by instances of Person, as well as",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1071,
      "chapter": null,
      "content": "Manager; Manager gets __repr__ from Person, which now obtains it from the\nAttrDisplay coded in another module. Example 28-14 codes the final version\nof our person.py file with these changes applied, and imports and uses the\nseparate Example 28-13.\nExample 28-14. person_14.py (final)\n\"\"\"\nRecord and process information about people.\nRun this file directly to test its classes.\n\"\"\"\nfrom classtools import AttrDisplay                    # Use generic display tool\nclass Person(AttrDisplay):                            # Mix in a repr at this level\n   \"\"\"\n   Create and process person records\n   \"\"\"\n   def __init__(self, name, job=None, pay=0):\n       self.name = name\n       self.job  = job\n       self.pay  = pay\n   def lastName(self):                               # Assumes last is last\n       return self.name.split()[-1]\n   def giveRaise(self, percent):                     # Percent must be 0..1\n       self.pay = int(self.pay * (1 + percent))\nclass Manager(Person):\n   \"\"\"\n   A customized Person with special requirements\n   \"\"\"\n   def __init__(self, name, pay):\n       Person.__init__(self, name, 'mgr', pay)       # Job name is implied\n   def giveRaise(self, percent, bonus=.10):\n       Person.giveRaise(self, percent + bonus)\nif __name__ == '__main__':\n   bob = Person('Bob Smith')\n   sue = Person('Sue Jones', job='dev', pay=100000)\n   print(bob)\n   print(sue)\n   print(bob.lastName(), sue.lastName())\n   sue.giveRaise(.10)\n   print(sue)\n   pat = Manager('Pat Jones', 50000)\n   pat.giveRaise(.10)\n   print(pat.lastName())",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1072,
      "chapter": null,
      "content": "print(pat)\nAs this is the final revision, we’ve added a few comments here to document our\nwork—docstrings for functional descriptions and # for smaller notes, per best-\npractice conventions, as well as blank lines between methods for readability—a\ngenerally good style choice when classes or methods grow large, which has been\nresisted earlier for these small classes, in part to save space and keep the code\nmore compact.\nWhen we run this code now, we see all the attributes of our objects, not just the\nones we hardcoded in the original __repr__. And our final issue is resolved:\nbecause AttrDisplay takes class names off the self instance directly, each\nobject is shown with the name of its closest (lowest) class—pat displays as a\nManager now, not a Person, and we can finally verify that job name is correctly\nfilled in by the Manager constructor:\n$ python3 person_14.py\n[Person: job=None, name=Bob Smith, pay=0]\n[Person: job=dev, name=Sue Jones, pay=100000]\nSmith Jones\n[Person: job=dev, name=Sue Jones, pay=110000]\nJones\n[Manager: job=mgr, name=Pat Jones, pay=60000]\nThis is the more useful display we were after. From a larger perspective, though,\nour attribute display class has become a general tool, which we can mix into any\nclass by inheritance to leverage the display format it defines. Further, all its\nclients will automatically pick up future changes in our tool. Later in the book,\nwe’ll explore even more powerful class tool concepts, such as decorators and\nmetaclasses; along with Python’s many introspection tools, they allow us to\nwrite code that augments and manages classes in structured and maintainable\nways.\nStep 7 (Final): Storing Objects in a Database\nAt this point, our work is almost complete. We now have a two-module system\nthat not only implements our original design goals for representing people but\nalso provides a general attribute display tool we can use in other programs in the",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1073,
      "chapter": null,
      "content": "future. By coding functions and classes in module files, we’ve ensured that they\nnaturally support reuse. And by coding our software as classes, we’ve ensured\nthat it naturally supports extension.\nAlthough our classes work as planned, though, the objects they create are not\nreal database records. That is, if we kill Python or our program, our instances\nwill disappear—they’re transient objects in memory and are not stored in a more\npermanent medium like a file, so they won’t be available in future program runs.\nIt turns out that it’s easy to make instance objects more permanent, with a\nPython feature called object persistence—making objects live on after the\nprogram that creates them exits. As a final step in this tutorial, let’s make our\nobjects permanent.\nPickles and Shelves\nObject persistence is implemented by three standard-library modules, installed\nwith Python itself:\npickle\nSerializes arbitrary Python objects to and from a string of bytes\ndbm\nImplements an access-by-key filesystem for storing strings\nshelve\nUses the other two modules to store Python objects on a file by key\nWe used pickle and noted shelve briefly in Chapter 9 when we studied file\nbasics. They provide simple yet useful data-storage options. Although we can’t\ndo them complete justice in this tutorial or book, they are straightforward\nenough that a brief introduction should suffice to get you started.\nThe pickle module\nThe pickle module is a general formatting and deformatting tool: given a nearly\narbitrary Python object in memory, it converts the object to a string of bytes,",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1074,
      "chapter": null,
      "content": "which it can use later to reconstruct the original object in memory. The pickle\nmodule can handle most Python objects—including lists, dictionaries, nested\ncombinations thereof, and class instances. The latter are especially useful things\nto pickle because they provide both data (attributes) and behavior (methods); the\ncombination is roughly equivalent to “records” and “programs.”\nBecause pickle is so general, it can replace extra code you might otherwise\nwrite to create and parse custom text-file representations for your objects. By\nstoring an object’s pickle string on a file, you effectively make it persistent:\nsimply load and unpickle it later to re-create the original object.\nThe shelve module\nAlthough it’s easy to use pickle by itself to store objects in simple flat files and\nload them from there later, the shelve module provides an extra layer of\nstructure that allows you to store pickled objects by key. shelve translates an\nobject to its pickled string with pickle and stores that string under a key in a\ndbm file; when later loading, shelve fetches the pickled string by key and re-\ncreates the original object in memory with pickle. To your script a shelf of\npickled objects looks like a dictionary—you index by key to fetch, assign to\nkeys to store, and use dictionary tools such as len, in, and dict.keys to get\ninformation. Shelves automatically map most dictionary operations to pickled\nobjects stored in a file.2\nIn fact, to your script, the main coding difference between shelves and normal\ndictionaries is that you must open shelves initially and must close them after\nmaking changes. The net effect is that shelve provides a simple database for\nstoring and fetching native Python objects by keys, and thus makes them\npersistent across program runs. It does not support query tools such as SQL (but\nPython’s sqlite3 standard-library module does) and does not have some\nadvanced features found in enterprise-level databases such as true transaction\nprocessing (but other Python database tools do). It does, however, store native\nPython objects that may be processed with the full power of the Python language\nonce they are fetched back by key.\nStoring Objects on a shelve Database",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1075,
      "chapter": null,
      "content": "Pickles and shelves are somewhat advanced topics, and we won’t go into all\ntheir details here; you can read more about them in the standard-library manuals.\nThis is all simpler in code than narrative, though, so let’s jump right in.\nFirst up is a new script to store objects of our classes on a shelf. Since this is a\nnew file, we’ll need to import our classes in order to make instances to store. We\nused from to load a class earlier, but as noted in the prior chapter, there are two\nways to get a class from a file (class names are variables like any other, and not\nat all magic in this context). As an abstract refresher:\nimport person                                    # Load class with import\nbob = person.Person(…)                           # Go through module name\nfrom person import Person                        # Load class with from\nbob = Person(…)                                  # Use name directly\nWe’ll use from to load in our script, just because it’s less to type. To also keep\nthis simple, we’ll duplicate some of the self-test lines from person.py that make\ninstances of our classes, so we have something to store (we won’t fret over the\ntest-code redundancy in this simple demo). Once we have some instances, it’s\nalmost trivial to store them on a shelf. We simply import the shelve module,\nopen a new shelf with an external filename, assign the objects to keys in the\nshelf, and close the shelf when we’re done because we’ve made changes—all\nper Example 28-15.\nExample 28-15. makedb.py (store Person objects in a shelve database)\nfrom person_14 import Person, Manager            # Load our classes\nbob = Person('Bob Smith')                        # Re-create objects to be stored\nsue = Person('Sue Jones', job='dev', pay=100000)\npat = Manager('Pat Jones', 50000)\nimport shelve\ndb = shelve.open('persondb')                     # Filename where objects are stored\nfor obj in (bob, sue, pat):                      # Use object's name attr as key\n   db[obj.name] = obj                           # Store object in shelf by key\ndb.close()                                       # Close after making changes\nNotice how we assign objects to the shelf using their own names as keys. This is\njust for convenience; in a shelf, the key can be any string, including one we\nmight create to be unique using tools such as process IDs and timestamps\n(available in the os and time standard-library modules). The only rule is that the",
      "content_length": 2437,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1076,
      "chapter": null,
      "content": "keys must be strings and should be unique since we can store just one object per\nkey—though that object can be a list, dictionary, or other object containing many\nobjects itself.\nIn fact, the values we store under keys can be Python objects of almost any sort:\nbuilt-in types like strings, lists, tuples, and dictionaries, as well as user-defined\nclass instances, and nested combinations of all of these and more. For example,\nthe name and job attributes of our objects could be nested dictionaries and lists\nas in earlier incarnations in this book (though this would require a bit of redesign\nto the current code).\nThat’s all there is to it—if this script has no output when run, it means it\nprobably worked; we’re not printing anything, just creating and storing objects\nin a file-based database:\n$ python3 makedb.py\nExploring Shelves Interactively\nAt this point, there are one or more files in the current directory whose names all\nstart with persondb. The actual files created can vary, and just as in the built-in\nopen function, the filename in shelve.open() is relative to the current working\ndirectory (CWD) unless it includes a directory path. Wherever they are stored,\nthese files implement a keyed-access file that contains the pickled representation\nof our three Python objects. Don’t delete these files—they are your database and\nare what you’ll need to copy or transfer when you back up or move your storage.\nYou can inspect the shelf’s files either from a file explorer, console shell, or\nPython REPL, but they are binary hash files, and most of their content makes\nlittle sense outside the context of the shelve module. For example, Python’s\nstandard-library glob module allows us to get directory listings to verify the\nshelf (it’s just one file on macOS, persondb.db), and we can open it in binary\nmode to explore stored bytes:\n>>> import glob\n>>> glob.glob('persondb*')\n['persondb.db']\n>>> print(open('persondb.db', 'rb').read())\nb'\\x00\\x06\\x15a\\x00\\x00\\x00\\x02\\x00\\x00\\x04\\xd2\\x00\\x00…much more omitted…",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1077,
      "chapter": null,
      "content": "This content can vary, but it’s nearly impossible to decipher here, and doesn’t\nexactly qualify as a user-friendly database interface. To verify better, we can\nwrite another script, or poke around our shelf at the interactive prompt. Because\nshelves are Python objects containing Python objects, we can process them with\nnormal Python syntax and development modes. Here, the REPL effectively\nbecomes a database client:\n$ python3\n>>> import shelve\n>>> db = shelve.open('persondb')                 # Reopen the shelf\n>>> len(db)                                      # Three objects stored\n3\n>>> list(db.keys())                              # keys is the index\n['Bob Smith', 'Sue Jones', 'Pat Jones']\n>>> pat = db['Pat Jones']                        # Fetch an object by key\n>>> pat.lastName()                               # Runs lastName from Person        \n'Jones'\n>>> pat                                          # Runs __repr__ from AttrDisplay\n[Manager: job=mgr, name=Pat Jones, pay=50000]\n>>> for key in sorted(db):                       # Iterate, sort, fetch, print\n        print(key, '=>', db[key])\n \nBob Smith => [Person: job=None, name=Bob Smith, pay=0]\nPat Jones => [Manager: job=mgr, name=Pat Jones, pay=50000]\nSue Jones => [Person: job=dev, name=Sue Jones, pay=100000]\nNotice that we don’t have to import our Person or Manager classes here in order\nto load or use our stored objects. For example, we can call pat’s lastName\nmethod freely, and get its custom print display format automatically, even though\nwe don’t have the Person class in scope here. This works because when Python\npickles a class instance, it records the instance’s self attributes, along with the\nnames of the class it was created from and the module where that class lives.\nWhen an instance is later fetched from the shelf and unpickled, Python\nautomatically reimports the class by name and makes a new instance of it having\nthe previously stored instance attributes.\nThe upshot of this scheme is that class instances automatically acquire all their\nclass behavior when they are loaded in the future. We have to import our classes\nonly to make new instances, not to process existing ones. Although a deliberate",
      "content_length": 2193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1078,
      "chapter": null,
      "content": "feature, this scheme has somewhat mixed consequences:\nThe downside is that classes and their module’s files must be importable\nwhen an instance is later loaded. That is, picklable classes must be\ncoded at the top level of a module file that is accessible from a directory\nlisted on the sys.path module search path (and shouldn’t live in the\ntopmost script files’ module __main__ unless they’re always in that\nmodule when used). Because of this external file requirement, some\nprograms pickle simpler objects such as dictionaries or lists, especially\nif they are to be transferred across networks.\nThe upside is that changes in a class’s source code file are automatically\npicked up when instances of the class are loaded again. There is often\nno need to update stored objects themselves since updating their class’s\ncode changes their behavior.\nShelves have other well-known limitations covered in Python’s library manual.\nFor simple object storage, though, shelves and pickles are easy-to-use tools for\npersonal databases, program configurations, and more.\nUpdating Objects on a Shelf\nOne last script: let’s write a program that updates an instance (record) each time\nit runs, to show that our objects really are persistent—that their current values\nare available every time a Python program runs. The file coded in Example 28-\n16 prints the database and gives a raise to one of our stored objects on each run.\nIf you trace through what’s going on here, you’ll notice that we’re getting a lot\nof utility “for free”—printing our objects automatically employs the general\n__repr__ overloading method, and we give raises by calling the giveRaise\nmethod we wrote earlier. This all just works for objects based on OOP’s\ninheritance model, even when they live in a file.\nExample 28-16. updatedb.py (modify Person object in a shelve database)\nimport shelve\ndb = shelve.open('persondb')               # Reopen shelf with same filename\nfor key in sorted(db):                     # Iterate to display database objects\n   print(key, '\\t=>', db[key])            # Prints with custom format",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1079,
      "chapter": null,
      "content": "sue = db['Sue Jones']                      # Index by key to fetch\nsue.giveRaise(.10)                         # Update in memory using class's method\ndb['Sue Jones'] = sue                      # Assign to key to update in shelf\ndb.close()                                 # Close after making changes\nBecause this script prints the database when it starts up, we have to run it at least\ntwice to see our objects change. Here it is in action, displaying all records and\nincreasing sue’s pay each time it is run (it’s a pretty good script for sue;\nsomething to schedule to run regularly as a cron job perhaps?):\n$ python3 updatedb.py\nBob Smith     => [Person: job=None, name=Bob Smith, pay=0]\nPat Jones     => [Manager: job=mgr, name=Pat Jones, pay=50000]\nSue Jones     => [Person: job=dev, name=Sue Jones, pay=100000]\n$ python3 updatedb.py\nBob Smith     => [Person: job=None, name=Bob Smith, pay=0]\nPat Jones     => [Manager: job=mgr, name=Pat Jones, pay=50000]\nSue Jones     => [Person: job=dev, name=Sue Jones, pay=110000]\n$ python3 updatedb.py\nBob Smith     => [Person: job=None, name=Bob Smith, pay=0]\nPat Jones     => [Manager: job=mgr, name=Pat Jones, pay=50000]\nSue Jones     => [Person: job=dev, name=Sue Jones, pay=121000]\nAgain, what we see here is a product of the shelve and pickle tools we get\nfrom Python, and of the behavior we coded in our classes ourselves. And once\nagain, we can verify our script’s work at the interactive prompt—the shelf’s\nequivalent of a database client:\n$ python3\n>>> import shelve\n>>> db = shelve.open('persondb')             # Reopen database\n>>> rec = db['Sue Jones']                    # Fetch object by key\n>>> rec\n[Person: job=dev, name=Sue Jones, pay=133100]\n>>> rec.lastName(), rec.pay\n('Jones', 133100)\nFor another example of object persistence in this book, watch for the sidebar\n“Why You Will Care: Classes and Persistence”. It stores a somewhat larger\ncomposite object in a flat file with pickle instead of shelve, but the effect is\nsimilar. For more details and examples of pickles, see also Chapter 9 (file basics)",
      "content_length": 2066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1080,
      "chapter": null,
      "content": "and Chapter 37 (string tools and Unicode), as well as Python’s manuals.\nFuture Directions\nAnd that’s a wrap for this chapter’s tutorial. At this point, you’ve seen all the\nbasics of Python’s OOP machinery in action, and you’ve learned ways to avoid\nredundancy and its associated maintenance issues in your code. You’ve built\nfull-featured classes that do real work. As an added bonus, you’ve made them\nreal database records by storing them in a Python shelf, so their information lives\non persistently.\nThere is much more we could explore here, of course. For example, we could\nextend our classes to make them more realistic, add new kinds of behavior to\nthem, and so on. Giving a raise, for instance, should in practice verify that pay\nincrease rates are between zero and one—an extension we’ll add when we meet\ndecorators later in this book. You might also mutate this example into a personal\ncontacts database, by changing the state information stored on objects, as well as\nthe classes’ methods used to process it. We’ll leave this a suggested exercise\nopen to your imagination.\nWe could also expand our scope to use tools that either come with Python or are\nfreely available in the open source world. For instance, GUIs, websites, or apps\nwould make our database more accessible to nonprogrammers, and other\ndatabase interfaces like sqlite3 and content-representation schemes like JSON\noffer additional possibilities.\nWhile this chapter has hopefully sparked your interest for future exploration,\nsuch topics are of course beyond the scope of this tutorial and this book at large.\nIf you want to explore any of them on your own, see the web, Python’s standard-\nlibrary manuals, and follow-up application texts. First, though, let’s return to\nclass fundamentals and finish up the rest of the Python core-language story.",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1081,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we explored all the fundamentals of Python classes and OOP in\naction, by building upon a simple but real example, step by step. We added\nconstructors, methods, operator overloading, customization with subclasses, and\nintrospection-based tools, and we met concepts such as composition, delegation,\nand polymorphism along the way.\nIn the end, we took objects created by our classes and made them persistent by\nstoring them on a shelve object database—a simple system for saving and\nretrieving native Python objects by key. While exploring class basics, we also\nencountered multiple ways to factor our code to reduce redundancy and\nminimize future maintenance costs.\nIn the next chapters of this part of the book, we’ll resume our study of the details\nbehind Python’s class model and investigate its application to some of the design\nconcepts used to combine classes in larger programs. Before we move ahead,\nthough, let’s work through this chapter’s quiz to review what we covered here.\nSince we’ve already done a lot of hands-on work in this chapter, we’ll close with\na set of mostly theory-oriented questions designed to make you trace through\nsome of the code and ponder some of the bigger ideas behind it.\nTest Your Knowledge: Quiz\n1. When we fetch a Manager object from the shelf and print it, where does\nthe display format logic come from?\n2. When we fetch a Person object from a shelf without importing its\nmodule, how does the object know that it has a giveRaise method that\nwe can call?\n3. Why is it so important to move processing into methods, instead of\nhardcoding it outside the class?\n4. Why is it better to customize by subclassing rather than copying the\noriginal and modifying?",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1082,
      "chapter": null,
      "content": "5. Why is it better to call back to a superclass method to run default\nactions, instead of copying and modifying its code in a subclass?\n6. Why is it better to use tools like __dict__ that allow objects to be\nprocessed generically than to write more custom code for each type of\nclass?\n7. In general terms, when might you choose to use object embedding and\ncomposition instead of inheritance?\n8. What would you have to change if the objects coded in this chapter used\na dictionary for names and a list for jobs, as in similar examples earlier\nin this book?\n9. How might you modify the classes in this chapter to implement a\npersonal contacts database in Python?\nTest Your Knowledge: Answers\n1. In the final version of our classes, Manager ultimately inherits its\n__repr__ printing method from AttrDisplay in the separate\nclasstools module, and two levels up in the class tree. Manager\ndoesn’t have one itself, so the inheritance search climbs to its Person\nsuperclass; because there is no __repr__ there either, the search climbs\nhigher and finds it in AttrDisplay. The class names listed in\nparentheses in a class statement’s header line provide the links to\nhigher superclasses.\n2. Shelves (really, the pickle module they use) automatically relink an\ninstance to the class it was created from when that instance is later\nloaded back into memory. Python reimports the class from its module\ninternally and creates a new instance with the previously stored\nattributes. This way, loaded instances automatically obtain all their\noriginal methods (like lastName, giveRaise, and __repr__), even if\nwe have not imported the instance’s class into the loading scope.\n3. It’s important to move processing into methods so that there is only one",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1083,
      "chapter": null,
      "content": "copy to change in the future, and so that the methods can be run on any\ninstance. This is Python’s notion of encapsulation—wrapping up logic\nbehind interfaces, to better support future code maintenance. If you\ndon’t do so, you create code redundancy that can multiply your work\neffort as the code evolves in the future.\n4. Customizing with subclasses reduces development effort. In OOP, we\ncode by customizing what has already been done, rather than copying or\nchanging existing code. This is the real “big idea” in OOP—because we\ncan easily extend our prior work by coding new subclasses, we can\nleverage what we’ve already done. This is much better than either\nstarting from scratch each time, or introducing multiple redundant\ncopies of code that may all have to be updated in the future.\n5. Copying and modifying code doubles your potential work effort in the\nfuture, regardless of the context. If a subclass method needs to perform\ndefault actions coded in a superclass method, it’s much better to call\nback to the original through the superclass’s name (or the super built-\nin) than to copy its code. This also holds true for superclass\nconstructors. Again, copying code creates redundancy, which is a major\nissue as code evolves.\n6. Generic tools can avoid hardcoded solutions that must be kept in sync\nwith the rest of the class as it evolves over time. A generic __repr__\nprint method, for example, need not be updated each time a new\nattribute is added to instances in an __init__ constructor. In addition, a\ngeneric print method inherited by all classes appears and need be\nmodified in only one place—changes in the generic version are picked\nup by all classes that inherit from the generic class. Again, eliminating\ncode redundancy cuts future development effort; that’s one of the\nprimary assets classes bring to the table.\n7. Inheritance is best at coding extensions based on direct customization\n(like our Manager specialization of Person). Composition is well suited\nto scenarios where multiple objects are aggregated into a whole and\ndirected by a controller-layer class. Inheritance passes calls up to reuse,\nand composition passes down to delegate. Inheritance and composition",
      "content_length": 2195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1084,
      "chapter": null,
      "content": "are not mutually exclusive; often, the objects embedded in a controller\nare themselves customizations based upon inheritance.\n8. Not much since this was really a first-cut prototype, but the lastName\nmethod would need to be updated for the new name format; the Person\nconstructor would have to change the job default to an empty list; and\nthe Manager class would probably need to pass along a job list in its\nconstructor instead of a single string (self-test code would change as\nwell, of course). The good news is that these changes would need to be\nmade in just one place—in our classes, where such details are\nencapsulated. Apart from their object-construction calls, the database\nscripts should work as is, as shelves support arbitrarily nested data.\n9. The classes in this chapter could be used as boilerplate “template” code\nto implement a variety of types of databases. Essentially, you can\nrepurpose them by modifying the constructors to record different\nattributes and providing whatever methods are appropriate for the target\napplication. For instance, you might use attributes such as name,\naddress, birthday, phone, email, and so on for a contacts database,\nand methods appropriate for this purpose. A method named sendmail,\nfor example, might use Python’s standard-library smtplib module to\nsend an email to one of the contacts automatically when run (see\nPython’s manuals or application-level books for more details on such\ntools). The AttrDisplay tool we wrote here could be used verbatim to\nprint your objects because it is intentionally generic. Most of the\nshelve database code here can be used to store your objects, too, with\nminor changes.\n1  And no offense to any managers in the audience, of course. This joke was once delivered at a Python\nclass in New Jersey, and nobody laughed. The organizers later revealed that the attendees were all\nmanagers evaluating Python. Hence the silence.\n2  Terminology note: this book now uses the noun “shelf” for the object storage managed by module\nshelve, and its plural “shelves” for more than one “shelf” managed by shelve. In the past, the\nmodule’s verb name “shelve” was also confusingly used as the noun—much to the chagrin of this\nbook’s editors over the years, both electronic and human.",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1085,
      "chapter": null,
      "content": "Chapter 29. Class Coding Details\nIf you haven’t quite grasped all of Python OOP yet, don’t worry—now that\nwe’ve taken a first pass, we’re going to dig a bit deeper and study the concepts\nintroduced earlier in further detail. In this and the following chapter, we’ll take\nanother look at class mechanics. Here, we’ll study classes, methods, and\ninheritance, formalizing and expanding on some of the coding ideas introduced\nin Chapter 27 and demoed in Chapter 28. Because the class is our last\nnamespace tool, we’ll summarize Python’s namespace and scope concepts as\nwell.\nIf you’ve been reading linearly, some of this chapter will be partly review and\nsummary of topics introduced in the preceding chapter’s case study, revisited\nhere by language topics with self-contained examples that may help readers new\nto OOP. While you may be tempted to skip some material here, it includes extra\ndetails worth a browse and unveils more subtleties in Python’s class model along\nthe way.\nThe next chapter continues this in-depth second pass over class mechanics by\ncovering one specific aspect: operator overloading. First, though, let’s fill in\nmore of the Python OOP picture.\nThe class Statement\nAlthough the Python class statement may seem similar to tools in other OOP\nlanguages on the surface, on closer inspection, it is quite different from what\nsome programmers may be used to.\nFor example, as in C++, the class statement is Python’s main OOP tool, but\nunlike in C++, Python’s class is not a declaration. Like a def, a class\nstatement is an object builder and an implicit assignment—when run, it\ngenerates a class object and stores a reference to it in the name used in the\nheader. Also like a def, a class statement is true executable code—your class\ndoesn’t exist until Python reaches and runs the class statement that defines it.",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1086,
      "chapter": null,
      "content": "This typically occurs while importing the module it is coded in, but not before.\nGeneral Syntax and Usage\nAs we’ve seen, class is a compound statement with a body of statements\ntypically indented under the header. In the header, superclasses are listed in\nparentheses after the class name, separated by commas. Listing more than one\nsuperclass leads to multiple inheritance, which we’ll discuss more formally in\nChapter 31 (in brief, the left-to-right order of superclasses in parentheses gives\nthe search order). Here is the statement’s general form and usage:\nclass name(superclass,…):             # Assign to name\n    attr = value                      # Shared class data\n    def method(self,…):               # Methods\n        self.attr = value             # Per-instance data\nx = name(…)                           # Make an instance\nx.method(…)                           # Call a method\nWithin the class statement, any assignments generate class attributes (both data\nitems and callable functions known as methods); specially named methods\nimplement built-in operations (e.g., a function named __init__ is run at\ninstance-object construction time if defined); and calling the class after its class\nhas run makes instances of it.\nExample: Class Attributes\nAs we’ve also seen, classes are mostly just namespaces—tools for defining\nnames (i.e., attributes) that export data and logic to clients. Just as in a module\nfile, the statements nested in a class statement body create its namespace and\nattributes. When Python reaches and runs a class statement, it runs all the\nstatements nested in its body, from top to bottom. Assignments that happen\nduring this process create names in the class’s local scope, which become\nattributes in the associated class object. Because of this, classes resemble both\nmodules and functions:\nLike functions, class statements are local scopes where names created\nby nested assignments live.",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1087,
      "chapter": null,
      "content": "Like modules, names assigned in a class statement become attributes\nin a class object.\nThe main distinction for classes is that their namespaces are also the basis of\ninheritance in Python: referenced attributes that are not found in a class or\ninstance object may be fetched from other classes.\nBecause class is a compound statement, any sort of statement can be nested\ninside its body—print, assignments, if, def, and so on. All the statements\ninside the class statement run when the class statement itself runs (not when\nthe class is later called to make an instance). Typically, assignment statements\ninside the class statement make data attributes and nested defs make method\nattributes. In general, though, any type of name assignment at the top level of a\nclass statement creates a same-named attribute in the resulting class object.\nFor example, assignments of simple nonfunction objects to class attributes\nproduce data attributes shared by all instances. In the REPL of your choosing:\n>>> class SharedData:\n        attr = 16          # Generates a class data attribute\n>>> x = SharedData()       # Make two instances\n>>> y = SharedData()\n>>> x.attr, y.attr         # They inherit and share 'attr' (a.k.a. SharedData.attr)\n(16, 16)\nHere, because the name attr is assigned at the top level of a class statement, it\nis attached to the class—which means it will be shared by all instances via the\nusual inheritance search from instance to class. We can change it by going\nthrough the class name, and we can refer to it through either instances or the\nclass:\n>>> SharedData.attr = 32\n>>> x.attr, y.attr, SharedData.attr\n(32, 32, 32)\nSuch class attributes can be used to manage information that spans all the\ninstances—a counter of the number of instances generated, for example (an idea\nwe’ll expand on by example in Chapter 32). Now, watch what happens if we",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1088,
      "chapter": null,
      "content": "assign the name attr through an instance instead of the class:\n>>> x.attr = 64\n>>> x.attr, y.attr, SharedData.attr\n(64, 32, 32)\nAssignments to instance attributes create or change the names in the instance,\nnot the shared class. More generally, inheritance searches occur only on attribute\nreferences, not on attribute assignments: assigning to an object’s attribute always\nchanges that object and no other (subject to the note ahead). For example,\ny.attr is still looked up in the class by inheritance, but the assignment to\nx.attr attaches a name to x itself and so replaces the version in the class.\nReaders who’ve done OOP before may recognize class attributes like\nSharedData.attr as similar to other languages’ “static” data members—values\nthat are stored in the class, independent of instances. In Python, it’s nothing\nspecial: all class attributes are just names assigned in the class statement,\nwhether they happen to reference functions or something else. When they are\nfunctions (a.k.a. methods), they simply receive an instance when called through\none.\nHere’s a more comprehensive example of this behavior that stores the same\nname in two places. Suppose we run the following class in a REPL:\n>>> class MixedNames:                            # Define class\n        data = 'text'                            # Assign class attr\n        def __init__(self, value):               # Assign method name\n            self.data = value                    # Assign instance attr\n        def display(self):\n            print(self.data, MixedNames.data)    # Instance attr, class attr\nThis class contains two defs, which assign class attributes to method functions.\nIt also contains a top-level = assignment statement; because this assignment\nassigns the name data inside the class, it lives in the class’s local scope and\nbecomes an attribute of the class object. Like all class attributes, this data is\ninherited and shared by all instances of the class that don’t have data attributes\nof their own.\nWhen we make instances of this class, though, the name data is also attached to",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1089,
      "chapter": null,
      "content": "those instances by the assignment to self.data in the __init__ method run\nautomatically at instance-construction time:\n>>> x = MixedNames(1)           # Make two instance objects\n>>> y = MixedNames(2)           # Each has its own data\n>>> x.display(); y.display()    # self.data differs, MixedNames.data is the same\n1 text\n2 text\nThe net result is that data lives in two places: in the instance objects (created by\nthe self.data assignment in __init__) and in the class from which they inherit\nnames (created by the data assignment in the class). The class’s display\nmethod prints both versions by first qualifying the self instance and then the\nclass.\nBy using these techniques to store attributes in different objects, we determine\ntheir scope of visibility. When attached to classes, names are shared. When\nattached to instances, names record per-instance data, not shared behavior or\ndata. Although inheritance searches look up names for us, we can always get to\nan attribute anywhere in a tree by accessing the desired object directly. The\nobject from which an attribute is requested focuses and limits search.\nIn the preceding example, for instance, specifying x.data or self.data will\nreturn an instance name, which normally hides the same name in the class.\nHowever, MixedNames.data grabs the class’s version of the name explicitly.\nThe next section describes another common role for such through-the-class\ncoding patterns and explains more about the way we deployed class-level fetches\nin the prior chapter.\nNOTE\nAssignment-rule exceptions: Assigning to an object’s attribute always changes only that object\n—unless, that is, the object inherits from a class that has redefined attribute assignment to do\nsomething unique with the __setattr__ operator-overloading method (discussed in\nChapter 30) or uses advanced attribute-management tools such as properties and descriptors\n(discussed in Chapters 32 and 38). Much of this chapter presents the normal case, which\nsuffices at this point in the book and for most Python code. As you’ll see later, though, Python\nclasses are, well, richly endowed with hooks that allow programs to deviate from the norm—\nand render simple rules fanciful.",
      "content_length": 2194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1090,
      "chapter": null,
      "content": "Methods\nBecause you already know about functions, you also know about methods in\nclasses. As you’ve learned, methods are just function objects created by def\nstatements nested in a class statement’s body. From an abstract perspective,\nmethods provide behavior for instance objects to inherit. From a programming\nperspective, methods work in exactly the same way as simple functions, with\none crucial exception: a method’s first argument receives the instance object that\nis the implied subject of the method call.\nBy way of review from the last chapter, a method call made through an instance\nlike this:\ninstance.method(args…)\nis automatically translated into a call of method function in a class like this:\nclass.method(instance, args…)\nwhere Python determines the class to use by locating the method name using the\ninheritance search procedure. In fact, both call forms are valid in Python: in the\nsecond, the class name narrows the method search, and the instance is provided\nexplicitly, but the net result is the same for the same method.\nBesides the inheritance of method names, the special first argument is the only\nreal magic behind method calls. In a class’s method, the first argument is usually\ncalled self by convention (technically, only its position is significant, not its\nname). This argument provides methods with a hook back to the instance that is\nthe subject of the call—because classes generate many instance objects, they use\nself to manage per-instance data.\nIn Python, self is always explicit in your code: methods must always both list\nand use self to fetch or change attributes of the instance being processed by the\ncurrent method call. This is by design—the presence of this name makes it\nobvious that you are using instance attribute names in your script, not names in\nthe local or global scope.",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1091,
      "chapter": null,
      "content": "Method Example\nTo solidify these concepts, let’s turn to an example. Define the following class\nby running its code in a REPL:\n>>> class NextClass:                        # Define class\n        def printer(self, text):            # Define method\n            self.message = text             # Change instance\n            print(self.message)             # Access instance\nThe name printer references a normal function object; because it’s assigned in\nthe class statement’s scope, it becomes a class-object attribute and is inherited\nby every instance made from the class—and earns the title method. Normally,\nbecause methods like printer are designed to process instances, we call them\nthrough instances:\n>>> x = NextClass()                         # Make instance\n>>> x.printer('instance call')              # Call its method\ninstance call\n>>> x.message                               # Instance changed\n'instance call'\nWhen we call the method by qualifying an instance like this, printer is first\nlocated by inheritance, and then its self argument is automatically assigned the\ninstance object (x); the text argument gets the string passed at the call\n('instance call'). Notice that because Python automatically passes the first\nargument to self for us, we can (and must) pass in just one argument. Inside\nprinter, the name self is used to access or set per-instance data because it\nrefers back to the instance currently being processed.\nAs we’ve seen, though, methods may be called in one of two ways—through an\ninstance or through the class itself. For example, we can also call printer by\ngoing through the class name, provided we pass an instance to the self\nargument explicitly:\n>>> NextClass.printer(x, 'class call')      # Direct class call\nclass call\n>>> x.message                               # Instance changed again\n'class call'",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1092,
      "chapter": null,
      "content": "Calls routed through the instance and the class have the exact same effect—as\nlong as we pass the same instance object ourselves in the class form. In fact, you\nget an error message if you try to call our method without any instance:\n>>> NextClass.printer('bad call')\nTypeError: NextClass.printer() missing 1 required positional argument: 'text'\nReally, class methods are just functions assigned to class attributes, some of\nwhich happen to expect an instance that Python provides automatically only\nwhen methods are called through an instance. Moreover, calling a method\nthrough a class this way uses the same pattern we coded previously to fetch\nnonfunction class attributes. This same expression, class.attribute, works the\nsame, whether the result is a callable object or not. It’s a general tool.\nOther Method-Call Possibilities\nThis pattern of calling methods through a class is the general basis of extending\n—instead of completely replacing—inherited method behavior. It requires an\nexplicit instance to be passed because all methods do by default. Technically,\nthis is because methods called through instances are instance methods in the\nabsence of any special code.\nIn Chapter 32, we’ll also study a less common option, static methods, that allows\nus to code methods that do not expect instance objects in their first arguments,\neven when called through an instance. Such methods can act like simple\ninstanceless functions, with names that are local to the classes in which they are\ncoded, and may be used to manage class data. A related concept we’ll explore in\nthe same chapter, class methods receive a class when called instead of an\ninstance and can be used to manage per-class data, and are implied in\nmetaclasses—yet another topic we’ll reach later.\nThese are all advanced, optional, and atypical extensions, though. Normally, an\ninstance must always be passed to a method—whether automatically when it is\ncalled through an instance, or manually when you call through a class.\nInheritance",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1093,
      "chapter": null,
      "content": "Of course, the whole point of the namespace created by the class statement is to\nsupport name inheritance. This section expands on some of the mechanisms and\nroles of attribute inheritance in Python.\nAs we’ve seen, in Python, inheritance happens when an object is qualified, and it\ninvolves searching an attribute definition tree—one or more namespaces. Every\ntime you use an expression of the form object.attr where object is an\ninstance or class object, Python searches the namespace tree from bottom to top,\nbeginning with object, and looking for the first attr it can find. This also\nhappens for references to self attributes in your methods. Because lower\ndefinitions in the tree override higher ones, inheritance forms the basis of\nspecialization.\nAttribute Tree Construction\nFigure 29-1 summarizes the way namespace trees are constructed and populated\nwith names. Generally:\nInstance attributes are generated by assignments to self attributes in\nmethods.\nClass attributes are created by statements (assignments) nested in class\nstatements.\nSuperclass links are made by listing classes in parentheses in a class\nstatement header.",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1094,
      "chapter": null,
      "content": "Figure 29-1. Program code creates a tree of objects searched by attribute inheritance\nThe net result is a tree of attribute namespaces that leads from an instance to the\nclass it was generated from to all the superclasses listed in the class header.\nPython searches upward in this tree—from instances to superclasses, and left to\nright through multiple superclasses—each time you fetch an attribute name from\nan instance object.\nInheritance Fine Print\nTechnically speaking, the preceding description isn’t complete because we can\nalso create instance and class attributes by assigning them to objects outside of\nclass statements. In Python, all attributes are always accessible by default, and\nprivacy is an add-on (we’ll talk about attribute privacy in Chapter 30 when we\nstudy __setattr__, in Chapter 31 when we meet __X names, and again in\nChapter 39 when we implement it with a class decorator). Even so, changes\noutside of a class are uncommon and error-prone: classes work best when they\nmanage their instances.\nAlso technically speaking, as hinted in Chapter 27, the full inheritance story\ngrows more convoluted when advanced topics we haven’t yet met are added to\nthe mix. Metaclasses, diamond-pattern MROs, and descriptors, for example,",
      "content_length": 1245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1095,
      "chapter": null,
      "content": "may all play a role in some programs. Because of this, we’ll begin formalizing\nthe inheritance algorithm in Chapter 31 but won’t finish it until Chapter 40. In\nthe vast majority of Python code, though, inheritance is a simple way to redefine,\nand hence customize, behavior coded in classes—as the next section demos.\nSpecializing Inherited Methods\nThe tree-searching model of inheritance just described turns out to be a great\nway to specialize systems. Because inheritance finds names in subclasses before\nit checks superclasses, subclasses can replace default behavior by redefining\ntheir superclasses’ attributes. In fact, you can build entire systems as hierarchies\nof classes, which you extend by adding new external subclasses rather than\ncopying existing logic or changing it in place.\nThe idea of redefining inherited names leads to a variety of specialization\ntechniques. For instance, subclasses may replace inherited attributes completely,\nprovide attributes that a superclass expects to find, and extend superclass\nmethods by calling back to the superclass from an overridden method. We’ve\nalready seen some of these patterns in action; here’s a self-contained example of\nextension at work:\n>>> class Super:\n        def method(self):\n            print('in Super.method')\n>>> class Sub(Super):\n        def method(self):                    # Override method\n            print('starting Sub.method')     # Add actions here\n            Super.method(self)               # Run default action\n            print('ending Sub.method')\nDirect superclass method calls are the crux of the matter here. The Sub class\nreplaces Super’s method function with its own specialized version, but within\nthe replacement, Sub calls back to the version exported by Super to carry out the\ndefault behavior. In other words, Sub.method just extends Super.method’s\nbehavior rather than replacing it completely:\n>>> x = Super()              # Make a Super instance\n>>> x.method()               # Runs Super.method",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1096,
      "chapter": null,
      "content": "in Super.method\n>>> x = Sub()                # Make a Sub instance\n>>> x.method()               # Runs Sub.method, calls Super.method\nstarting Sub.method\nin Super.method\nending Sub.method\nPerhaps the most common places that superclass-method calls show up are in\nconstructors. The __init__ method, like all attributes, is looked up by\ninheritance. This means that at construction time, Python locates and calls just\none __init__, not one in every superclass. If subclass constructors need to\nensure that superclass construction-time logic runs too, they must call the\nsuperclass’s __init__ method explicitly. Calling it through the class name\nleverages the same general coding pattern we’ve been using in multiple roles:\n>>> class Super:\n        def __init__(self, x):\n            print('default code')\n \n>>> class Sub(Super):\n        def __init__(self, x, y):\n            Super.__init__(self, x)        # Run superclass __init__\n            print('custom code')           # Do my extra init actions\n \n>>> I = Sub(1, 2)\ndefault code\ncustom code\nThis is one of the few contexts in which your code is likely to call an operator-\noverloading method directly. Naturally, you should call the superclass\nconstructor this way only if you really want it to run—without the call, the\nsubclass replaces it completely. For a more realistic illustration of this technique\nin action, see the Manager class example in the prior chapter’s tutorial.\nOn a related note, readers with prior OOP experience may also be interested to\nknow that redefining the constructor with differing argument lists in the same\nclass means that only the last is used—later defs simply reassign the method\nname in Python. Starred arguments can be used in this role, but rarely are. We’ll\nexplore this phenomenon more fully in Chapter 31’s coverage of polymorphism\n(short story: it’s about interfaces, not call signatures).",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1097,
      "chapter": null,
      "content": "NOTE\nThe super reminder: Per the sidebar “The super Alternative”, Python also has a super built-in\nfunction that allows calling back to a superclass’s methods more generically, but we’re\ndeferring its coverage until Chapter 32 due to its downsides and complexities. As a preview,\nthough, the prior section’s first of the following can also be coded as the second—which\nessentially automates the self argument via deep magic beyond our scope here (note its\nlowercase):\nSuper.method(self)           # Explicit, general tool\nsuper().method()             # Implicit, special case\nLikewise, the same equivalence goes for the constructor calls of this section:\nSuper.__init__(self, x)      # Explicit fundamental\nsuper().__init__(x)          # Implicit alternative\nPer the aforementioned sidebar, though, super has well-known trade-offs in basic usage and\nan esoteric advanced use case that requires universal deployment to be most effective. Because\nof such issues, this book prefers to call superclasses by explicit name instead of super. If\nyou’re new to Python, consider following the same policy, especially for your first pass over\nOOP. Learn the simple and general now so you can weigh it against the complicated and\nnarrow later.\nClass Interface Techniques\nBroadly speaking, the prior section’s extension is only one way to interface with\na superclass. The file listed in Example 29-1, specialize.py, defines multiple\nclasses that illustrate a variety of common techniques:\nSuper\nDefines a method function and a delegate that expects an action in a\nsubclass\nInheritor\nDoesn’t provide any new names, so it gets everything defined in Super\nReplacer",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1098,
      "chapter": null,
      "content": "Overrides Super’s method with a version of its own\nExtender\nCustomizes Super’s method by overriding and calling back to run the default\nProvider\nImplements the action method expected by Super’s delegate method\nStudy each of these subclasses to get a feel for the various ways they customize\ntheir common superclass.\nExample 29-1. specialize.py\nclass Super:\n   def method(self):\n       print('in Super.method')             # Default behavior\n   def delegate(self):\n       self.action()                        # Expected to be defined\nclass Inheritor(Super):                      # Inherit method verbatim\n   pass\nclass Replacer(Super):                       # Replace method completely\n   def method(self):\n       print('in Replacer.method')\nclass Extender(Super):                       # Extend method behavior\n   def method(self):\n       print('starting Extender.method')\n       Super.method(self)                   # Or: super().method()\n       print('ending Extender.method')\nclass Provider(Super):                       # Fill in a required method\n   def action(self):\n       print('in Provider.action')\nif __name__ == '__main__':\n   for klass in (Inheritor, Replacer, Extender):\n       print('\\n' + klass.__name__ + '...')\n       klass().method()\n   print('\\nProvider...')\n   x = Provider()\n   x.delegate()",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1099,
      "chapter": null,
      "content": "Two things are worth pointing out here. First, notice how the self-test code at the\nend of this example creates instances of three different classes in a for loop.\nBecause classes, like functions, are first-class objects, you can store them in a\ntuple and create instances generically with no extra syntax. Second, classes also\nhave a built-in __name__ attribute, like modules; it’s preset to a string containing\nthe name in the class header. Here’s what happens when we run the file:\n$ python3 specialize.py\nInheritor...\nin Super.method\nReplacer...\nin Replacer.method\nExtender...\nstarting Extender.method\nin Super.method\nending Extender.method\nProvider...\nin Provider.action\nTrace through the code to see how each of these outputs is produced.\nAbstract Superclasses\nOf the prior example’s classes, Provider may be one of the most crucial to\nunderstand. When we call the delegate method through a Provider instance,\ntwo independent inheritance searches occur:\n1. On the initial x.delegate call, Python finds the delegate method in\nSuper by searching the Provider instance and above. The instance x is\npassed into the method’s self argument as usual.\n2. Inside the Super.delegate method, self.action invokes a new,\nindependent inheritance search of self and above. Because self\nreferences a Provider instance, the action method is located in the\nProvider subclass.",
      "content_length": 1363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1100,
      "chapter": null,
      "content": "This “filling in the blanks” sort of coding structure is typical of OOP\nframeworks. In a more realistic context, the method filled in this way might\nhandle an event in a GUI, provide data to be rendered as part of a web page,\nprocess a tag’s text in an XML file, and so on—your subclass provides specific\nactions, but the framework handles the rest of the overall job and runs your\nactions when needed.\nAt least in terms of the delegate method, the superclass in this example is what\nis sometimes called an abstract superclass—a class that expects parts of its\nbehavior to be provided by its subclasses. If an expected method is not defined\nin a subclass, Python raises an undefined name exception when the inheritance\nsearch fails.\nClass coders sometimes make such subclass requirements more obvious with\nassert statements or by raising the built-in NotImplementedError exception\nwith raise statements. We’ll study statements that may trigger exceptions in\ndepth in the next part of this book; as a quick preview, here’s the assert scheme\nin action:\n>>> class Super:\n        def delegate(self):\n            self.action()\n        def action(self):\n            assert False, 'action must be defined!'      # Error if called\n>>> X = Super()\n>>> X.delegate()\nAssertionError: action must be defined!\nWe’ll study assert in Chapters 33 and 34; in short, if its first expression\nevaluates to false, it raises an exception with the provided error message. Here,\nthe expression is always false so as to trigger an error message if a method is not\nredefined, and inheritance locates the stub version here. Alternatively, some\nclasses simply raise a NotImplementedError exception directly in such method\nstubs to signal the mistake:\n>>> class Super:\n        def delegate(self):\n            self.action()\n        def action(self):",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1101,
      "chapter": null,
      "content": "raise NotImplementedError('action must be defined!')\n>>> X = Super()\n>>> X.delegate()\nNotImplementedError: action must be defined!\nFor instances of subclasses, we still get the exception unless the subclass\nprovides the expected method to replace the default in the superclass:\n>>> class Sub(Super): pass\n>>> X = Sub()\n>>> X.delegate()\nNotImplementedError: action must be defined!\n>>> class Sub(Super):\n        def action(self): print('okay')\n>>> X = Sub()\n>>> X.delegate()\nokay\nFor a somewhat more realistic example of this section’s concepts in action, see\nthe “Zoo animal hierarchy” exercise (Exercise 8) in “Test Your Knowledge: Part\nVI Exercises” and its solution in Appendix B. Such taxonomies are a traditional\nway to introduce OOP, but they’re a bit removed from most developers’ job\ndescriptions (with apologies to any readers who happen to work at the zoo).\nPreview: Abstract superclasses with library tools\nThe preceding abstract superclasses (a.k.a. “abstract base classes”), which\nrequire methods to be filled in by subclasses, may also be implemented with\nspecial class syntax and a library module. This is coded with a keyword\nargument in a class header, along with special @ decorator syntax, both of\nwhich we’ll study later in this book. While necessarily a preview in part, here is\nthe special syntax equivalent of the preceding example:\n>>> from abc import ABCMeta, abstractmethod\n>>> class Super(metaclass=ABCMeta):\n        def delegate(self):\n            self.action()\n        @abstractmethod\n        def action(self):",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1102,
      "chapter": null,
      "content": "pass\nThe net effect more rigidly prevents instance creation unless the method is\ndefined lower in the class tree:\n>>> X = Super()\nTypeError: Can't instantiate abstract class Super without an implementation \nfor abstract method 'action'\n>>> class Sub(Super): pass\n>>> X = Sub()\nTypeError: Can't instantiate abstract class Sub without an implementation \nfor abstract method 'action'\n>>> class Sub(Super):\n        def action(self): print('okay')\n>>> X = Sub()\n>>> X.delegate()\nokay\nCoded this way, a class with an abstract method cannot be instantiated (that is,\nwe cannot create an instance by calling it) unless all of its abstract methods have\nbeen defined in subclasses. Although this requires more code and extra\nknowledge, the potential advantage of this approach is that errors for missing\nmethods are issued when we attempt to make an instance of the class, not later\nwhen we try to call a missing method. This scheme may also be used to define\nan expected interface, automatically verified in client classes.\nUnfortunately, this scheme also relies on two advanced language tools we have\nnot mastered yet—function decorators, introduced in Chapter 32 and covered in\ndepth in Chapter 39, as well as metaclass declarations, mentioned in Chapter 32\nand covered in Chapter 40—so we will postpone other facets of this option here.\nSee Python’s standard manuals for more on this, as well as precoded abstract\nsuperclasses Python provides.\nNamespaces: The Conclusion\nNow that we’ve examined class and instance objects, the Python namespace\nstory is complete. For reference, this section summarizes all the rules used to",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1103,
      "chapter": null,
      "content": "resolve names and extends them to classes. The first things you need to\nremember are that qualified and unqualified names are treated differently, and\nthat some scopes serve to initialize object namespaces:\nUnqualified names (e.g., X) deal with scopes.\nQualified attribute names (e.g., object.X) use object namespaces.\nSome scopes initialize object namespaces (for modules and classes).\nThese concepts sometimes interact—in object.X, for example, object is first\nlooked up per scopes, and then X is looked up in the located object. Since scopes\nand namespaces are essential to understanding Python code, let’s flesh out the\nrules in more detail.\nSimple Names: Global Unless Assigned\nAs we’ve seen, unqualified simple names follow the LEGB lexical scoping rule\noutlined when we explored functions in Chapter 17:\nAssignment (X = value)\nMakes names local by default: creates or changes the name X in the current\nlocal scope, unless declared global or nonlocal in that scope. These\ndeclarations work in both def for functions and class for classes.\nReference (X)\nLooks for the name X in the current local scope (L), then any and all\nenclosing functions from inner to outer (E), then the current global-scope\nmodule (G), then the built-ins module (B)—per the LEGB rule. Notably\nabsent here, enclosing classes are not searched; class names are referenced as\nobject attributes instead.\nAlso per Chapter 17, some special-case constructs localize names further (e.g.,\nvariables in some comprehensions and some try statement clauses), and nested",
      "content_length": 1535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1104,
      "chapter": null,
      "content": "class scopes currently have some peculiarities that reflect longstanding bug\nreports and are too obscure to merit coverage here. The vast majority of names,\nhowever, follow the LEGB rule.\nNew here, the class statement allows global and nonlocal to modify\nassignment rules the same as def, though we have to nest it to see how the latter\nof these come online:\n>>> gvar = 111\n>>> class C:\n        global gvar            # Change name gvar in enclosing module\n        gvar = 222             # Else it would be class attribute C.gvar\n \n>>> gvar\n222\n>>> def outer():\n        nvar = 111\n        class C:\n            nonlocal nvar      # Change name nvar in enclosing function\n            nvar = 222         # Else it would be class attribute C.nvar\n        print(nvar)\n \n>>> outer()\n222\nThough rare, namespace declarations in class map assignments to outer scopes,\ninstead of making class attributes—the same way they prevent assignments from\nmaking local variables in functions. There’s more on how nested classes interact\nwith scopes in default cases later in this section.\nAttribute Names: Object Namespaces\nWe’ve also seen that qualified attribute names refer to attributes of specific\nobjects and obey the rules for modules and classes. For class and instance\nobjects, the reference rules are augmented to include the inheritance search\nprocedure:\nAssignment (object.X = value)\nCreates or alters the attribute name X in the namespace of the object being",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1105,
      "chapter": null,
      "content": "qualified, and none other. Inheritance-tree climbing happens only on attribute\nreference, not on attribute assignment.\nReference (object.X)\nFor class-based objects, searches for the attribute name X in object, then in\nall accessible classes above it, using the inheritance search procedure. For\nnonclass objects such as modules, fetches X from object directly.\nAs noted earlier, the preceding captures the normal case for typical code, but\nthese attribute rules can vary in classes that utilize more advanced tools you’ll\nmeet later. For example, reference inheritance can be richer than implied here\nwhen metaclasses are deployed, and classes that leverage attribute management\ntools such as properties, descriptors, and __setattr__ can intercept and route\nattribute assignments arbitrarily.\nIn fact, some inheritance is run on assignment, too, to locate descriptors with a\n__set__ method; such tools override the normal rules for both reference and\nassignment. We’ll explore attribute management tools in depth in Chapter 38 and\nformalize inheritance and its use of descriptors in Chapter 40. For now, most\nreaders should focus on the normal rules given here, which cover most Python\napplication code you’re likely to read, write, or run.\nThe “Zen” of Namespaces: Assignments Classify Names\nWith distinct search procedures for qualified and unqualified names, and\nmultiple lookup layers for both, it can sometimes be difficult to tell where a\nname will wind up going. In Python, the place where you assign a name is\ncrucial—it fully determines the scope or object in which a name will reside. The\nfile in Example 29-2, manynames.py, illustrates how this principle translates to\ncode and summarizes the namespace ideas we have seen throughout this book\n(sans obscure special-case scopes like comprehensions):\nExample 29-2. manynames.py (first half)\nX = 11                       # Global (module) X (manynames.X post import)",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1106,
      "chapter": null,
      "content": "def f():\n   print(X)                 # Access global X per LEGB lookup\ndef g():\n   X = 22                   # Local (function) X (hides module X)\n   print(X)\nclass C:\n   X = 33                   # Class attribute C.X (self.X pre self.m())\n   def m(self):\n       X = 44               # Local (function) X in method (unused here)\n       self.X = 55          # Instance attribute self.X (hides class X)\nThis file assigns the same name, X, five times—illustrative, though not exactly\nbest practice! Because this name is assigned in five different locations, though,\nall five Xs in this program are completely different variables. From top to\nbottom, the assignments to X here generate a module attribute (11), a local\nvariable in a function (22), a class attribute (33), a local variable in a method\n(44), and an instance attribute (55). Although all five are named X, the fact that\nthey are all assigned at different places in the source code or to different objects\nmakes all of these unique variables.\nYou should study this example carefully because it collects ideas we’ve been\nexploring throughout the last few parts of this book. When it makes sense to you,\nyou will have achieved Python namespace enlightenment. Or you can run the\ncode and see what happens—Example 29-3 lists the remainder of the source file\nin Example 29-2, with self-test code that makes an instance and prints all the Xs\nthat it can fetch.\nExample 29-3. manynames.py (second half)\nif __name__ == '__main__':\n   print(X)                 # 11: module (a.k.a. manynames.X outside file)\n   f()                      # 11: global\n   g()                      # 22: local\n   print(X)                 # 11: module name unchanged\n   I = C()                  # Make instance\n   print(I.X)               # 33: class name inherited by instance\n   I.m()                    # Attach attribute name X to instance now\n   print(I.X)               # 55: instance\n   print(C.X)               # 33: class (a.k.a. I.X if no X in I)\n   #print(C.m.X)            # FAILS: only visible in method\n   #print(g.X)              # FAILS: only visible in function",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1107,
      "chapter": null,
      "content": "The outputs that are printed when the file is run are noted in the comments in the\ncode; trace through them to see which variable named X is being accessed each\ntime. Notice in particular that we can go through the class to fetch its attribute\n(C.X), but we can never fetch local variables in functions or methods from\noutside their def statements. Locals are visible only to other code within the\ndef, and, in fact, only live in memory while a call to the function or method is\nexecuting.\nSome of the names defined by this file are visible outside the file to other\nmodules, too, but recall that we must always import before we can access names\nin another file—name segregation is the main point of modules, after all.\nExample 29-4 shows how names appear outside the module in Example 29-3,\nagain with expected outputs in comments:\nExample 29-4. manynames-client.py\nimport manynames\nX = 66\nprint(X)                     # 66: the global here\nprint(manynames.X)           # 11: globals become attributes after imports\nmanynames.f()                # 11: manynames's X, not the one here!\nmanynames.g()                # 22: local in other file's function\nprint(manynames.C.X)         # 33: attribute of class in other module\nI = manynames.C()\nprint(I.X)                   # 33: still from class here\nI.m()\nprint(I.X)                   # 55: now from instance!\nNotice here how manynames.f() prints the X in manynames, not the X assigned\nin this file—scopes are always determined by the position of assignments in your\nsource code (i.e., lexically) and are never influenced by what imports what or\nwho imports whom. Also, notice that the instance’s own X is not created until we\ncall I.m()—attributes, like all variables, spring into existence when assigned,\nand not before. Normally, we create instance attributes by assigning them in\nclass __init__ constructor methods, but this isn’t the only option.\nFinally, as covered in Chapter 17, it’s also possible for a function to change\nnames outside itself with global and nonlocal statements—these statements\nprovide write access, but also modify assignment’s namespace binding rules.",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1108,
      "chapter": null,
      "content": "Example 29-5 provides a refresher on these points.\nExample 29-5. funcscope.py\nX = 11                       # Global in module\ndef g1():\n   print(X)                 # Reference global in module (11)\ndef g2():\n   global X\n   X = 22                   # Change global in module\ndef h1():\n   X = 33                   # Local in function\n   def nested():\n       print(X)             # Reference local in enclosing scope (33)\ndef h2():\n   X = 33                   # Local in function\n   def nested():\n       nonlocal X\n       X = 44               # Change local in enclosing scope\nOf course, you generally shouldn’t use the same name for every variable in your\nscript—but as this example demonstrates, even if you do, Python’s namespaces\nwill work to keep names used in one context from accidentally clashing with\nthose used in another.\nNested Classes: The LEGB Scopes Rule Revisited\nThe preceding example summarized the effect of nested functions on scopes,\nwhich we studied in Chapter 17. As we saw briefly near the start of this section,\nclasses can be nested, too—a useful coding pattern in some types of programs.\nThis has scope implications that follow naturally from what you already know,\nbut that may not be obvious on first encounter. This section illustrates the\nconcept by example.\nThough they are normally coded at the top level of a module, classes also appear\nnested in functions that generate them—a variation on the “factory function”\n(a.k.a. closure) theme in Chapter 17, with similar state retention roles. There, we\nnoted that class statements introduce new local scopes, much like function def\nstatements, which follow the same LEGB scope lookup rule as function",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1109,
      "chapter": null,
      "content": "definitions.\nThis rule applies both to the top level of the class itself as well as to the top level\nof method functions nested within it. Both form the L layer in this rule—they are\nlocal scopes with access to their names, names in any enclosing functions,\nglobals in the enclosing module, and built-ins. Like modules, the class’s local\nscope morphs into an attribute namespace after the class statement is run, but\nits top-level code is a local scope while the class runs.\nImportantly, though, although classes have access to enclosing functions’ scopes,\nthey do not themselves act as enclosing scopes to code nested within the class—\nPython searches enclosing functions for referenced names but never any\nenclosing classes. That is, a class is a local scope and has access to enclosing\nlocal scopes, but it does not serve as an enclosing local scope to further nested\ncode. Because the search for names used in method functions skips the enclosing\nclass, class attributes must be fetched as object attributes using inheritance.\nFor example, in the nester function of Example 29-6, all references to X are\nrouted to the global scope except the last, which picks up a local-scope\nredefinition in method2 (the output of each example in this section is described\nin its last two comments).\nExample 29-6. classscope1.py\nX = 1\ndef nester():\n  print(X)                 # Global: 1\n  class C:\n      print(X)             # Global: 1\n      def method1(self):\n          print(X)         # Global: 1\n      def method2(self):\n          X = 3            # Hides global\n          print(X)         # Local: 3\n  I = C()\n  I.method1()\n  I.method2()\nprint(X)                    # Global: 1\nnester()                    # Rest: 1, 1, 1, 3\nWatch what happens, though, when we reassign the same name in nested",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1110,
      "chapter": null,
      "content": "function layers in Example 29-7: the redefinitions of X create locals that hide\nthose in enclosing scopes, just as for simple nested functions; the enclosing class\nlayer does not change this rule, and in fact is irrelevant to it.\nExample 29-7. classscope2.py\nX = 1\ndef nester():\n  X = 2                    # Hides global\n  print(X)                 # Local: 2\n  class C:\n      print(X)             # In enclosing def (nester): 2\n      def method1(self):\n          print(X)         # In enclosing def (nester): 2\n      def method2(self):\n          X = 3            # Hides enclosing (nester)\n          print(X)         # Local: 3\n  I = C()\n  I.method1()\n  I.method2()\nprint(X)                    # Global: 1\nnester()                    # Rest: 2, 2, 2, 3\nFinally, Example 29-8 shows what happens when we reassign the same name at\nmultiple stops along the way: assignments in the local scopes of both functions\nand classes hide globals or enclosing function locals of the same name,\nregardless of the nesting involved.\nExample 29-8. classscope3.py\nX = 1\ndef nester():\n  X = 2                    # Hides global\n  print(X)                 # Local: 2\n  class C:\n      X = 3                # Class local hides nester's: C.X or I.X (not scoped)\n      print(X)             # Local: 3\n      def method1(self):\n          print(X)         # In enclosing def (not 3 in class!): 2\n          print(self.X)    # Inherited class local: 3\n      def method2(self):\n          X = 4            # Hides enclosing (nester, not class)\n          print(X)         # Local: 4\n          self.X = 5       # Hides class's\n          print(self.X)    # Located in instance: 5",
      "content_length": 1643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1111,
      "chapter": null,
      "content": "I = C()\n  I.method1()\n  I.method2()\nprint(X)                    # Global: 1\nnester()                    # Rest: 2, 3, 2, 3, 4, 5\nMost importantly, the lookup rules for simple names like X never search\nenclosing class statements—just defs, modules, and built-ins (it’s the LEGB\nrule, not LCEGB!). In method1, for example, X is found in a def outside the\nenclosing class that has the same name in its local scope. To get to names\nassigned in the class (e.g., methods), we must fetch them as class or instance\nobject attributes, via self.X in this case.\nBelieve it or not, you’ll see valid roles for this nested-classes coding pattern later\nin this book, especially in some of Chapter 39’s decorators. In this role, the\nenclosing function usually both serves as a class or instance factory and provides\nretained state for later use in the enclosed class or its methods.\nNamespace Dictionaries: Review\nIn Chapter 23, we saw that module namespaces have a concrete implementation\nas dictionaries, exposed with the built-in __dict__ attribute. In Chapters 27 and\n28, we saw that the same holds true for class and instance objects—attribute\nqualification is mostly a dictionary indexing operation internally, and attribute\ninheritance is largely a matter of searching linked dictionaries. In fact, within\nPython, instance and class objects are mostly just dictionaries with links between\nthem. Python exposes these dictionaries, as well as their links, for use in\nadvanced roles.\nWe put some of these tools to work in the prior chapter, but to summarize and\nhelp you better understand how attributes work internally, let’s work through an\ninteractive session that traces the way namespace dictionaries grow when classes\nare involved. Now that we know more about methods and superclasses, we can\nalso embellish the coverage here for a better look. First, let’s define a superclass\nand a subclass with methods that will store data in their instances:\n>>> class Super:\n        def hello(self):\n            self.data1 = 'hack'",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1112,
      "chapter": null,
      "content": ">>> class Sub(Super):\n        def hola(self):\n            self.data2 = 'code'\nWhen we make an instance of the subclass, the instance starts out with an empty\nnamespace dictionary, but it has links back to the class for the inheritance search\nto follow. In fact, the inheritance tree is explicitly available in special attributes,\nwhich you can inspect. Instances have a __class__ attribute that links to their\nclass, and classes have a __bases__ attribute that is a tuple containing links to\nhigher superclasses:\n>>> X = Sub()\n>>> X.__dict__                            # Instance namespace dict\n{}\n>>> X.__class__                           # Class of instance\n<class '__main__.Sub'>\n>>> Sub.__bases__                         # Superclasses of class\n(<class '__main__.Super'>,)\n>>> Super.__bases__                       # Implied above top-levels\n(<class 'object'>,)\nAs classes assign to self attributes, they populate the instance objects—that is,\nattributes wind up in the instances’ attribute namespace dictionaries, not in the\nclasses’. An instance object’s namespace records data that can vary from\ninstance to instance, and self is a hook into that namespace:\n>>> Y = Sub()\n>>> X.hello()\n>>> X.__dict__\n{'data1': 'hack'}\n>>> X.hola()\n>>> X.__dict__\n{'data1': 'hack', 'data2': 'code'}\n>>> list(Sub.__dict__.keys())\n['__module__', 'hola', '__doc__'] \n>>> list(Super.__dict__.keys())\n['__module__', 'hello', '__dict__', '__weakref__', '__doc__'] \n>>> Y.__dict__\n{}",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1113,
      "chapter": null,
      "content": "Notice the extra underscore names in the class dictionaries; Python sets these\nautomatically, and we can filter them out with the generator expressions we\ncoded in Chapters 27 and 28 omitted here for space. Most are not used in typical\nprograms but may be used by tools (e.g., __doc__ holds the docstrings discussed\nin Chapter 15).\nAlso, observe that Y, a second instance made at the start of this series, still has an\nempty namespace dictionary at the end, even though X’s dictionary has been\npopulated by assignments in methods. Again, each instance has an independent\nnamespace dictionary, which starts out empty and can record completely\ndifferent attributes than those recorded by the namespace dictionaries of other\ninstances of the same class.\nBecause instance attributes are actually dictionary keys inside Python, there are\nreally two ways to fetch and assign their values—by qualification or by key\nindexing:\n>>> X.data1, X.__dict__['data1']\n('hack', 'hack')\n>>> X.data3 = 'docs'\n>>> X.__dict__\n{'data1': 'hack', 'data2': 'code', 'data3': 'docs'}\n>>> X.__dict__['data3'] = 'apps'\n>>> X.data3\n'apps'\nThis equivalence applies only to attributes actually attached to the instance,\nthough. Because attribute fetch qualification also performs an inheritance search,\nit can access inherited attributes that namespace dictionary indexing cannot. The\ninherited attribute X.hello, for instance, cannot be accessed by\nX.__dict__['hello'].\nExperiment with these special attributes on your own to get a better feel for how\nnamespaces actually do their attribute business. Also, try running these objects\nthrough the dir function we met in the prior two chapters—dir(X) is similar to\nX.__dict__.keys(), but dir sorts its list and includes inherited attributes. Even\nif you will never use these in the kinds of programs you write, seeing how\nattributes are stored can help solidify namespaces in general.",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1114,
      "chapter": null,
      "content": "NOTE\nThe slots exception: In Chapter 32, you’ll learn about slots, an advanced class tool that stores\nattributes in instances but not in their namespace dictionaries. It’s tempting to treat these as\nclass attributes, and indeed, they appear in class namespaces where they manage per-instance\nvalues. As you’ll find, though, slots may prevent a __dict__ from being created in the\ninstance—a potential that generic tools must sometimes account for by using storage-neutral\nbuilt-ins like dir to list and getattr to fetch. The good news is that slots are used very rarely\n—as they should be!\nNamespace Links: A Tree Climber\nThe prior section demonstrated the special __class__ and __bases__ instance\nand class attributes without really explaining why you might care about them. In\nshort, these attributes allow you to inspect inheritance hierarchies within your\nown code. For example, they can be used to display a class tree, as coded in the\nmodule of Example 29-9.\nExample 29-9. classtree.py\n\"\"\"\nclasstree.py: Climb inheritance trees using namespace links,\ndisplaying higher superclasses with indentation for height\n\"\"\"\ndef classtree(cls, indent):\n   print('.' * indent + cls.__name__)     # Print class name here\n   for supercls in cls.__bases__:         # Recur to all superclasses\n       classtree(supercls, indent+3)      # May visit super > once\ndef instancetree(inst):\n   print('Tree of', inst)                 # Show instance\n   classtree(inst.__class__, 3)           # Climb to its class\ndef selftest():\n   class A:      pass\n   class B(A):   pass\n   class C(A):   pass\n   class D(B,C): pass\n   class E:      pass\n   class F(D,E): pass\n   instancetree(B())\n   instancetree(F())\nif __name__ == '__main__': selftest()",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1115,
      "chapter": null,
      "content": "The classtree function in this script is recursive—it prints a class’s name using\n__name__, then climbs up to the superclasses by calling itself. This allows the\nfunction to traverse arbitrarily shaped class trees; the recursion climbs to the top\nand stops at root superclasses that have empty __bases__ attributes. As explored\nin Chapter 19, when using recursion, each active level of a function gets its own\ncopy of the local scope. Here, this means that cls and indent are different at\neach classtree level.\nMost of this file is self-test code. When run standalone, it builds an empty class\ntree, makes two instances from it, and prints their class tree structures. The trees\ninclude the implied object superclass that is automatically added above\nstandalone root (i.e., topmost) classes; there’s more on object in Chapter 32:\n$ python3 classtree.py\nTree of <__main__.selftest.<locals>.B object at 0x10733a000>\n...B\n......A\n.........object\nTree of <__main__.selftest.<locals>.F object at 0x10733a000>\n...F\n......D\n.........B\n............A\n...............object\n.........C\n............A\n...............object\n......E\n.........object\nHere, indentation marked by periods is used to denote class tree height. Of\ncourse, we could improve on this output format and perhaps even sketch it in a\nGUI display. Even as is, though, we can import these functions anywhere we\nwant a quick display of a physical class tree:\n$ python3\n>>> class Employee: pass\n>>> class Person(Employee): pass\n>>> pat = Person()\n>>> import classtree\n>>> classtree.instancetree(pat)",
      "content_length": 1551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1116,
      "chapter": null,
      "content": "Tree of <__main__.Person object at 0x1072a1b80>\n...Person\n......Employee\n.........object\nRegardless of whether you will ever code or use such tools, this example\ndemonstrates one of the many ways that you can make use of special attributes\nthat expose interpreter internals. You’ll see others when we code general-\npurpose class display tools in “Multiple Inheritance and the MRO”—there, we\nwill extend this technique to also display attributes in each object in a class tree\nand function as a reusable superclass.\nIn the last part of this book, we’ll revisit such tools in the context of Python tool\nbuilding at large, to code tools that implement attribute privacy, argument\nvalidation, and more. While not in every Python programmer’s job description,\naccess to internals enables powerful development tools.\nDocumentation Strings Revisited\nThe last section’s example includes a docstring for its module, but remember\nthat docstrings can be used for class components as well. Docstrings, which we\ncovered in detail in Chapter 15, are string literals that show up at the top of\nvarious structures and are automatically saved by Python in the corresponding\nobjects’ __doc__ attributes. This works for module files, function defs, and\nclasses and methods.\nNow that we’ve seen more about classes and methods, Example 29-10, a.k.a.\ndocstr.py, provides a quick but comprehensive example that summarizes the\nplaces where docstrings can show up in your code. All of these can be triple-\nquoted blocks or simpler one-liner literals like those here.\nExample 29-10. docstr.py\n\"I am: docstr.__doc__\"\ndef func(args):\n   \"I am: docstr.func.__doc__\"\n   pass\nclass Klass:\n   \"I am: Klass.__doc__ or docstr.Klass.__doc__ or self.__doc__\"",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1117,
      "chapter": null,
      "content": "def method(self):\n       \"I am: Klass.method.__doc__ or self.method.__doc__\"\n       print(self.__doc__)\n       print(self.method.__doc__)\nThe main advantage of documentation strings is that they stick around at\nruntime. Thus, if it’s been coded as a docstring, you can qualify an object with\nits __doc__ attribute to fetch its documentation (calling print on the result\ninterprets line breaks if it’s a multiline string):\n$ python3\n>>> import docstr\n>>> docstr.__doc__\n'I am: docstr.__doc__'\n>>> docstr.func.__doc__\n'I am: docstr.func.__doc__'\n>>> docstr.Klass.__doc__\n'I am: Klass.__doc__ or docstr.Klass.__doc__ or self.__doc__'\n>>> docstr.Klass.method.__doc__\n'I am: Klass.method.__doc__ or self.method.__doc__'\n>>> x = docstr.Klass()\n>>> x.method()\nI am: Klass.__doc__ or docstr.Klass.__doc__ or self.__doc__\nI am: Klass.method.__doc__ or self.method.__doc__\nA discussion of the PyDoc tool, which knows how to format all these strings in\nreports and web pages, appears in Chapter 15. Here it is running its help\nfunction on our code:\n>>> help(docstr)\nHelp on module docstr:\nNAME\n    docstr - I am: docstr.__doc__\nCLASSES\n    builtins.object\n        Klass\n    class Klass(builtins.object)\n     |  I am: Klass.__doc__ or docstr.Klass.__doc__ or self.__doc__\n     |\n     |  Methods defined here:\n     |",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1118,
      "chapter": null,
      "content": "|  method(self)\n     |      I am: Klass.method.__doc__ or self.method.__doc__\n     |\n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |\n     |  __dict__\n     |      dictionary for instance variables\n     |\n     |  __weakref__\n     |      list of weak references to the object\nFUNCTIONS\n    func(args)\n        I am: docstr.func.__doc__\nFILE\n    /…/LP6E/Chapter29/docstr.py\nDocumentation strings are available at runtime, but they are less flexible\nsyntactically than # comments, which can appear anywhere in a program. Both\nforms are useful, and any program documentation is good (as long as it’s\naccurate, of course!). As stated before, the Python “best practice” rule of thumb\nis to use docstrings for higher-level functional documentation and hash-mark\ncomments for more fine-grained coding documentation.\nClasses Versus Modules\nFinally, let’s wrap up this chapter by briefly comparing the topics of this book’s\nlast two parts: modules and classes. Because they’re both about namespaces, the\ndistinction can be confusing. In short:\nModules\nImplement data+logic packages\nAre created with Python files or other-language extensions",
      "content_length": 1204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1119,
      "chapter": null,
      "content": "Are used by being imported\nForm the top level in Python program structure\nClasses\nImplement new full-featured objects\nAre created with class statements\nAre used by being called\nAlways live within a module\nClasses also support extra features that modules don’t, such as operator\noverloading, multiple instance generation, and inheritance. Although both\nclasses and modules are namespaces, you should be able to tell by now that they\nare very different things. We need to move ahead to see just how unique classes\ncan be.",
      "content_length": 519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1120,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter took us on a second, more in-depth tour of the OOP mechanisms of\nthe Python language. We learned more about classes, methods, and inheritance,\nand we wrapped up the namespaces and scopes story in Python by extending it\nto cover its application to classes. Along the way, we encountered core OOP\nconcepts such as abstract superclasses, class data attributes, namespace links,\nand manual calls to superclass methods and constructors.\nNow that we’ve explored all the basic mechanics of coding classes in Python,\nthe next chapter turns to a specific facet of those mechanics: operator\noverloading. After that, we’ll explore common design patterns, looking at some\nof the ways that classes are commonly used and combined to optimize code\nreuse. Before you read ahead, though, be sure to work through the usual chapter\nquiz to review what we’ve covered here.\nTest Your Knowledge: Quiz\n1. What is an abstract superclass?\n2. What happens when a simple assignment statement appears at the top\nlevel of a class statement?\n3. Why might a class need to manually call the __init__ method in a\nsuperclass?\n4. How can you augment, instead of completely replacing, an inherited\nmethod?\n5. How does a class’s local scope differ from that of a function?\nTest Your Knowledge: Answers\n1. An abstract superclass is a class that calls a method, but does not inherit\nor define it—it expects the method to be filled in by a subclass. This is\noften used as a way to generalize classes when behavior cannot be",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1121,
      "chapter": null,
      "content": "predicted until a more specific subclass is coded. OOP frameworks also\nuse this as a way to dispatch to client-defined, customizable operations.\n2. When a simple assignment statement (X = Y) appears at the top level of\na class statement, it attaches a data attribute to the class (Class.X).\nLike all class attributes, this will be shared by all instances that do not\nhave the same attribute. Methods are generally created instead by def\nstatements nested in a class.\n3. A class must manually call the __init__ method in a superclass if it\ndefines an __init__ constructor of its own and still wants the\nsuperclass’s construction code to run (and it often will). Python itself\nautomatically runs just one constructor—the lowest one in the\ninheritance tree. Superclass constructors are often called through the\nclass name, passing in the self instance manually:\nSuperclass.__init__(self, …); they may also be called by\nsuper().__init__(…), though we haven’t yet studied this form in full.\n4. To augment instead of completely replacing an inherited method,\nredefine it in a subclass, but call back to the superclass’s version of the\nmethod manually from the new version of the method in the subclass.\nThat is, pass the self instance to the superclass’s version of the method\nmanually: Superclass.method(self, …); or do so implicitly with\nsuper().method(…). The prior answer is really just a special case of\nthis one.\n5. A class is a local scope and has access to enclosing local scopes, but it\ndoes not serve as an enclosing local scope to further nested code. Like\nmodules, the class local scope morphs into an attribute namespace after\nthe class statement is run.",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1122,
      "chapter": null,
      "content": "Chapter 30. Operator Overloading\nThis chapter continues our in-depth survey of class mechanics by focusing on\noperator overloading. We looked briefly at operator overloading in prior\nchapters. Here, we’ll fill in more details and explore a handful of commonly\nused overloading methods, most of which we haven’t yet encountered. Although\nwe don’t have space to demonstrate each of the many operator-overloading\nmethods available, those we will code here are a representative sample large\nenough to uncover the possibilities of this Python class feature.\nThe Basics\nReally “operator overloading” simply means intercepting built-in operations in a\nclass’s methods—Python automatically invokes your methods when instances of\nthe class appear in built-in operations, and your method’s return value becomes\nthe result of the corresponding operation. Here’s a review of the key ideas\nbehind overloading:\nOperator overloading lets classes intercept normal Python operations.\nClasses can overload all Python built-in expression operators.\nClasses can also overload other built-in operations such as printing,\nfunction calls, and attribute access.\nOverloading is implemented by providing specially named methods in a\nclass.\nPython predefines the special method names that correspond to built-in\noperations.\nIn other words, when methods of predefined special names are provided in a\nclass, Python automatically calls them when instances of the class appear in their\nassociated built-in operations or expressions. Your class provides the behavior of\nthe corresponding operation for instance objects created from it.",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1123,
      "chapter": null,
      "content": "As you’ve learned, operator-overloading methods are never required and\ngenerally don’t have defaults (apart from a handful that all classes get from the\nimplied object root class). If you don’t code or inherit an overloading method,\nit just means that your class does not support the corresponding operation. When\nused, though, these methods allow classes to emulate the interfaces of built-in\nobjects, which makes them consistent, and compatible with more code.\nConstructors and Expressions: __init__ and __sub__\nAs a warm-up, consider the simple class in Example 30-1: its Number class,\ncoded in module file number.py, provides a method to intercept instance\nconstruction (__init__), as well as one for catching subtraction expressions\n(__sub__). Special methods like these are the hooks that let you tie into built-in\noperations.\nExample 30-1. number.py\nclass Number:\n   def __init__(self, start):                  # On Number(start)\n       self.data = start\n   def __sub__(self, other):                   # On instance - other\n       return Number(self.data - other)        # Result is a new instance\nAs we’ve already learned, the __init__ constructor method seen in this code is\nthe most commonly used operator-overloading method in Python; it’s present in\nmost classes and used to initialize the newly created instance object using any\narguments passed to the class name. The __sub__ method plays the binary-\noperator role that __add__ did in Chapter 27’s introduction, intercepting\nsubtraction expressions and returning a new instance of the class as its result\n(and running __init__ along the way):\n>>> from number import Number                   # Fetch class from module\n>>> X = Number(5)                               # Number.__init__(X, 5)\n>>> Y = X - 2                                   # Number.__sub__(X, 2)\n>>> Y.data                                      # Y is new Number instance\n3\nWe’ve already studied __init__ and basic binary operators like __sub__ in\nsome depth, so we won’t rehash their usage further here. In this chapter, we will\ntour some other tools available in this domain and look at example code that",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1124,
      "chapter": null,
      "content": "applies them in common use cases.\nNOTE\nConstruction convolution: Technically, instance creation first triggers the __new__ method,\nwhich creates and returns the new instance object, which is then passed into __init__ for\ninitialization. Since __new__ has a built-in implementation and is redefined in only very\nlimited roles, though, nearly all Python classes initialize by defining an __init__ method.\nWe’ll explore one use case for __new__ when we study metaclasses in Chapter 40; though\nrare, it is sometimes also used to customize creation of instances of immutable types.\nCommon Operator-Overloading Methods\nJust about everything you can do to built-in objects such as integers and lists has\na corresponding specially named method for overloading in classes. Table 30-1\nlists a few of the most common; there are many more. In fact, many overloading\nmethods come in multiple versions (e.g., __add__, __radd__, and __iadd__ for\naddition), which is one reason there are so many. See the Python language\nreference manual for an exhaustive list of the special method names available.\nTable 30-1. Common operator-overloading methods\nMethod\nImplements\nCalled for\n__init__\nConstructor\nObject creation: X = Class(args)\n__del__\nDestructor\nObject reclamation of X\n__add__\nOperator + (among\nothers)\nX + Y, X += Y if no __iadd__\n__or__\nOperator | (bitwise\nOR)\nX | Y, X |= Y if no __ior__\n__repr__, __str_\n_\nPrinting,\nconversions\nprint(X), X, repr(X), str(X), f'{X!r}'\n__call__\nFunction calls\nX(*pargs, **kargs)",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1125,
      "chapter": null,
      "content": "__getattr__\nAttribute fetch\nX.undefined\n__setattr__\nAttribute\nassignment\nX.any = value\n__delattr__\nAttribute deletion\ndel X.any\n__getattribute__\nAttribute fetch\nX.any\n__getitem__\nIndexing, slicing,\niteration\nX[i], X[i:j], for and other iterations if\nno __iter__\n__setitem__\nIndex and slice\nassignment\nX[i] = value,\nX[i:j] = iterable\n__delitem__\nIndex and slice\ndeletion\ndel X[i], del X[i:j]\n__len__\nLength\nlen(X), truth tests if no __bool__\n__bool__\nBoolean tests\nbool(X), truth tests\n__lt__, __gt__,\n__le__, __ge__,\n__eq__, __ne__\nComparisons\nX < Y, X > Y, \nX <= Y, X >= Y, \nX == Y, X != Y\n__radd__\nRight-side operators\nother + X\n__iadd__\nIn-place augmented\noperators\nX += Y (or else __add__)\n__iter__, __next\n__\nIteration tools\nI=iter(X), next(I), for and other\niterations, in if no __contains__\n__contains__\nMembership test\nitem in X\n__index__\nInteger value\nhex(X), bin(X), oct(X), O[X], O[X:]\nwith obj as var:",
      "content_length": 913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1126,
      "chapter": null,
      "content": "__enter__, __exi\nt__\nContext manager\n(Chapter 34)\n__get__, __set__,\n__delete__\nDescriptor attributes\n(Chapter 38)\nX.attr, X.attr = value, del X.attr\n__new__\nCreation\n(Chapter 40)\nObject creation, before __init__\nAll overloading methods have names that start and end with two underscores to\nkeep them distinct from other names you define in your classes. The mappings\nfrom special method names to expressions or operations are predefined by the\nPython language and documented in full in its standard language manual. For\nexample, the name __add__ always maps to + expressions by Python language\ndefinition, regardless of what an __add__ method’s code actually does; it’s\nlargely just a dispatch mechanism.\nOperator-overloading methods may be inherited from superclasses if not\ndefined, just like any other methods. Operator-overloading methods are also all\noptional—if you don’t code or inherit one, that operation is simply unsupported\nby your class, and attempting it will raise an exception. Some built-in operations,\nlike printing, have defaults inherited from the implied object root class, but\nmost built-ins fail for class instances if no corresponding operator-overloading\nmethod is present.\nAs we go along here, keep in mind that most overloading methods are used only\nin advanced programs that require objects to behave like built-ins, though the\n__init__ constructor we’ve already met tends to appear in most classes. With\nthat qualifier, let’s explore some of the additional methods in Table 30-1 by\nexample.\nNOTE\nMeasure twice, post once: Although expressions trigger operator methods, be careful not to\nassume that there is a speed advantage to cutting out the middleperson and calling the operator\nmethod directly. In fact, calling the operator method directly might be twice as slow,\npresumably because of the overhead of a function call, which Python avoids or optimizes in\nbuilt-in cases.",
      "content_length": 1905,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1127,
      "chapter": null,
      "content": "Here’s the story for len and __len__ using Chapter 21’s timing techniques on Python 3.12 and\nmacOS. Calling __len__ directly takes twice as long (and has since Python 2.X):\n$ python3 -m timeit -n 10000 -r 10 \\\n             -s \"L = list(range(100))\" \"x = L.__len__()\"\n10000 loops, best of 10: 53.4 nsec per loop\n$ python3 -m timeit -n 10000 -r 10 \\\n             -s \"L = list(range(100))\" \"x = len(L)\"\n10000 loops, best of 10: 25.3 nsec per loop\nThis is not as contrived as it may seem—recommendations for using the slower alternative in\nthe name of speed have been known to crop up in venues that shall remain nameless here.\nIndexing and Slicing: __getitem__ and\n__setitem__\nOur first new method set allows your classes to mimic some of the behaviors of\nsequences and mappings. If defined in a class (or inherited by it), the\n__getitem__ method is called automatically for instance-indexing operations.\nWhen an instance X appears in an indexing expression like X[i], Python calls\nthe __getitem__ method inherited by the instance, passing X to the first\nargument and the index i in brackets to the second argument.\nFor example, the following class returns the square of an index value—atypical\nperhaps but illustrative of the mechanism in general:\n>>> class Indexer:\n        def __getitem__(self, index):\n            return index ** 2\n>>> X = Indexer()\n>>> X[2]                                # X[i] calls X.__getitem__(i)\n4\n>>> for i in range(5):\n        print(X[i], end=' ')            # Runs __getitem__(X, i) each time\n0 1 4 9 16\nIt’s up to your class to define what this expression means, though it should",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1128,
      "chapter": null,
      "content": "generally imitate a sequence index or mapping key fetch; returning the index\nsquared as done here works but probably won’t qualify as “best practice.”\nIntercepting Slices\nSurprisingly, in addition to indexing, __getitem__ is also called for slice\nexpressions. Formally speaking, built-in object types handle slicing the same\nway. For example, the following demos slicing at work on a built-in list, using\nupper and lower bounds, omitted parts, and a stride (see Chapter 7 if you need a\nrefresher on slicing):\n>>> L = [5, 6, 7, 8, 9]\n>>> L[2:4]                              # Slice with slice syntax: 2..(4-1)\n[7, 8]\n>>> L[1:]\n[6, 7, 8, 9]\n>>> L[:-1]\n[5, 6, 7, 8]\n>>> L[::2]\n[5, 7, 9]\nReally, though, slicing bounds are bundled up into a slice object and passed to\nthe list’s implementation of indexing. In fact, you can always pass a slice object\nmanually—slice syntax is mostly syntactic sugar for indexing with a slice object:\n>>> L[slice(2, 4)]                      # Slice with slice objects\n[7, 8]\n>>> L[slice(1, None)]\n[6, 7, 8, 9]\n>>> L[slice(None, -1)]\n[5, 6, 7, 8]\n>>> L[slice(None, None, 2)]\n[5, 7, 9]\nThis matters in classes with a __getitem__ method—this method will be called\nboth for basic indexing (with an index or key) and for slicing (with a slice\nobject). Our previous class won’t handle slicing because its math assumes\ninteger indexes are passed, but the following class will. When called for\nindexing, the argument is an integer as before:\n>>> class Indexer:",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1129,
      "chapter": null,
      "content": "def __init__(self, data):\n            self.data = data\n        def __getitem__(self, index):    # Called for index or slice\n            print('getitem:', index)\n            return self.data[index]      # Perform index or slice\n>>> X = Indexer([5, 6, 7, 8, 9])\n>>> X[0]                                 # Indexing sends __getitem__ an integer\ngetitem: 0\n5\n>>> X[1]\ngetitem: 1\n6\n>>> X[-1]\ngetitem: −1\n9\nWhen called for slicing, though, the method receives a slice object, which is\nsimply passed along to the embedded list indexer in a new index expression:\n>>> X[2:4]                             # Slicing sends __getitem__ a slice object\ngetitem: slice(2, 4, None)\n[7, 8]\n>>> X[1:]\ngetitem: slice(1, None, None)\n[6, 7, 8, 9]\n>>> X[:-1]\ngetitem: slice(None, −1, None)\n[5, 6, 7, 8]\n>>> X[::2]\ngetitem: slice(None, None, 2)\n[5, 7, 9]\nWhere needed, __getitem__ can test the type of its argument, and extract slice\nobject bounds—slice objects have attributes start, stop, and step, any of\nwhich can be None if omitted:\n>>> class Indexer:\n        def __getitem__(self, index):\n            if isinstance(index, int):               # Test usage mode\n                print('indexing', index)\n            else:\n                print('slicing', index.start, index.stop, index.step)\n>>> X = Indexer()\n>>> X[99]",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1130,
      "chapter": null,
      "content": "indexing 99\n>>> X[1:99:2]\nslicing 1 99 2\n>>> X[1:]\nslicing 1 None None\nRun a help(slice) in a REPL for more info on this very special-case built-in\nobject type, and see the section “Membership: __contains__, __iter__, and\n__getitem__” for another example of slice interception at work.\nIntercepting Item Assignments\nIf used, the __setitem__ index assignment method similarly intercepts both\nindex and slice assignments—it receives a slice object for the latter, which may\nbe passed along in another index assignment or used directly in the same way:\n>>> class IndexSetter:\n        def __init__(self, data):\n            self.data = data\n        def __setitem__(self, index, value):    # Catch index or slice assignment\n            print('setitem:', index)\n            self.data[index] = value            # Assign index or slice\n>>> X = IndexSetter([5, 6, 7, 8, 9])\n>>> X[0] = 555\nsetitem: 0\n>>> X[-2:] = [888, 999, 111]\nsetitem: slice(-2, None, None)\n>>> X.data\n[555, 6, 7, 888, 999, 111]\nIn fact, __getitem__ may be called automatically in even more contexts than\nindexing and slicing—it’s also an iteration fallback option, as you’ll see in a\nmoment. First, though, let’s clear up a potential point of confusion in this\ncategory.\nBut __index__ Means As-Integer\nDon’t mistake the (perhaps unfortunately named) __index__ method for\n__getitem__ index interception. The __index__ method returns an integer",
      "content_length": 1402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1131,
      "chapter": null,
      "content": "value for an instance when one is needed—and in retrospect, might have been\nbetter named __asindex__. For example, it’s used by built-ins that convert to\ndigit strings:\n>>> class C:\n        def __index__(self):\n            return 255\n>>> X = C()\n>>> hex(X)               # Integer value\n'0xff'\n>>> bin(X)\n'0b11111111'\n>>> oct(X)\n'0o377'\nAlthough this method does not intercept instance indexing like __getitem__, it\nis also used in contexts that require an integer—including indexing and slicing:\n>>> eds = [f'LP{i}e' for i in range(256)]\n>>> eds[255]\n'LP255e'\n>>> X = C()\n>>> eds[X]               # As index (not X[i]!)\n'LP255e'\n>>> eds[X:]              # As index (not X[i:]!)\n['LP255e']\nThough arguably misnamed, there is a rich history of former methods that\n__index__ subsumes, which we’ll mercifully omit here. The __getitem__\nmethod also subsumes former tools but retains a fallback role up next.\nIndex Iteration: __getitem__\nOur next hook isn’t always obvious to beginners but turns out to be surprisingly\nuseful. In the absence of the more specific iteration methods we’ll get to in the\nnext section, the for statement works by repeatedly indexing an object from\nzero to higher indexes, until an out-of-bounds IndexError exception is detected.\nBecause of that, __getitem__ also turns out to be one way to overload iteration\nin Python—if only this method is defined, for loops call the class’s",
      "content_length": 1401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1132,
      "chapter": null,
      "content": "__getitem__ each time through, with successively higher offsets.\nIt’s a case of “code one, get one free”—any built-in or user-defined object that\nresponds to indexing also responds to for loop iteration:\n>>> class StepperIndex:\n        def __getitem__(self, i):\n            return self.data[i]\n>>> X = StepperIndex()                # X is a StepperIndex object\n>>> X.data = 'hack'\n>>>\n>>> X[1]                              # Indexing calls __getitem__\n'a'\n>>> for item in X:                    # for loops call __getitem__\n        print(item, end=' ')          # for indexes items 0..N\nh a c k\nIn fact, it’s really a case of “code one, get a bunch free.” Any class that supports\nfor loops automatically supports all iteration tools in Python, many of which\nwe’ve explored in earlier chapters (e.g., iteration tools were presented in\nChapter 14). For instance, the in membership test, list comprehensions, the map\nbuilt-in, list and tuple assignments, and some type constructors will also call\n__getitem__ automatically to iterate if it’s defined:\n>>> 'k' in X                          # All call __getitem__ too\nTrue\n>>> [c for c in X]                    # Comprehension\n['h', 'a', 'c', 'k']\n>>> list(map(str.upper, X))           # map calls\n['H', 'A', 'C', 'K']\n>>> (a, b, c, d) = X                  # Sequence assignments\n>>> a, d\n('h', 'k')\n>>> list(X), tuple(X), ''.join(X)     # And so on...\n(['h', 'a', 'c', 'k'], ('h', 'a', 'c', 'k'), 'hack') \n>>> X\n<__main__.StepperIndex object at 0x10c4bcc20>",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1133,
      "chapter": null,
      "content": "In practice, this technique can be used to create objects that provide a sequence\ninterface and to add logic to built-in sequence type operations; we’ll revisit this\nidea when extending built-in types in Chapter 32.\nIterable Objects: __iter__ and __next__\nAlthough the __getitem__ technique of the prior section works, it’s really just a\nlegacy fallback for iteration. Today, all iteration tools in Python will try the\n__iter__ method first before trying __getitem__. That is, they prefer the\niteration protocol we learned about in Chapter 14 over repeatedly indexing an\nobject; only if the object does not support the iteration protocol is indexing\nattempted instead. Generally speaking, you should prefer __iter__ too—it\nsupports general iteration tools better than __getitem__ can.\nFor a review of this model’s essentials, see Figure 14-1 in Chapter 14. In brief,\niteration tools work by running an iterable object’s __iter__ method to fetch an\niterator object. If this works as planned, Python then repeatedly calls this iterator\nobject’s __next__ method to produce items until it raises a StopIteration\nexception. Built-in functions iter and next are also available as a conveniences\nfor manual iterations—iter(X) is the same as X.__iter__() and next(I) is\nthe same as I.__next__(), and Python internals may vary.\nThis iterable-object interface is given priority and attempted first. Only if no\nsuch __iter__ method is found, Python falls back on the __getitem__ scheme\nand repeatedly indexes by offsets as before until an IndexError exception is\nraised.\nUser-Defined Iterables\nIn the __iter__ scheme, classes implement user-defined iterables by simply\nimplementing the iteration protocol introduced in Chapter 14 and elaborated in\nChapter 20. For example, the code in Example 30-2 uses a class to define a user-\ndefined iterable that generates squares on demand, instead of all at once.\nExample 30-2. squares.py\nclass Squares:\n   def __init__(self, start, stop):    # Save state when created",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1134,
      "chapter": null,
      "content": "self.value = start - 1\n       self.stop  = stop\n   def __iter__(self):                 # Return iterator object\n       return self                     # Also called by iter() built-in\n   def __next__(self):                 # Return a square on each iteration\n       if self.value == self.stop:     # Also called by next() built-in\n           raise StopIteration\n       self.value += 1\n       return self.value ** 2\nWhen imported, its instances can appear in iteration tools just like built-ins:\n$ python3\n>>> from squares import Squares\n>>> for i in Squares(1, 5):             # for calls __iter__\n        print(i, end=' ')               # Each iteration calls __next__\n1 4 9 16 25\nHere, the iterator object returned by __iter__ is simply the instance self\nbecause the __next__ method is part of this class itself. In more complex\nscenarios, the iterator object may be defined as a separate class and object with\nits own state information to support multiple active iterations over the same\ninstance data (we’ll code an example of this in a moment).\nThe end of the iteration is signaled with a Python raise statement—introduced\nin Chapter 29 and covered in full in the next part of this book, but which simply\nraises an exception as if Python itself had done so. Because of all this, manual\niterations work the same on user-defined iterables as they do on built-in objects:\n>>> X = Squares(1, 5)                   # Iterate manually: what loops do\n>>> I = iter(X)                         # iter calls __iter__\n>>> next(I)                             # next calls __next__\n1\n>>> next(I)\n4\n…more omitted…\n>>> next(I)\n25\n>>> next(I)                             # Can catch this in try statement\nStopIteration",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1135,
      "chapter": null,
      "content": "An equivalent coding of this iterable with __getitem__ might be less natural\nbecause the for would then iterate through all offsets zero and higher; the offsets\npassed in would be only indirectly related to the range of values produced (0…N\nwould need to map to start…stop). Because __iter__ objects retain explicitly\nmanaged state between next calls, they can be more general than __getitem__.\nOn the other hand, iterables based on __iter__ can sometimes be more complex\nand less functional than those based on __getitem__. They are really designed\nfor iteration, not random indexing—in fact, they don’t overload the indexing\nexpression at all, though you can collect their items in a sequence such as a list\nto enable other operations:\n>>> X = Squares(1, 5)\n>>> X[1]\nTypeError: 'Squares' object is not subscriptable\n>>> list(X)[1]\n4\nSingle versus multiple scans\nThe __iter__ scheme is also the implementation for all the other iteration tools\nwe saw in action for the __getitem__ method—membership tests, type\nconstructors, sequence assignment, and so on. Unlike our prior __getitem__\nexample, though, we also need to be aware that a class’s __iter__ may be\ndesigned for a single traversal only, not many. Classes can choose either\nbehavior explicitly in their code.\nFor example, because the current Squares class’s __iter__ always returns self\nwith just one copy of iteration state, it is a single-scan iteration; once you’ve\niterated over an instance of that class, it’s empty. Calling __iter__ again on the\nsame instance returns self again—in whatever state it may have been left. As\ncoded, you generally need to make a new iterable instance object for each new\niteration:\n>>> X = Squares(1, 5)                   # Make an iterable with state\n>>> [n for n in X]                      # Exhausts items: __iter__ returns self\n[1, 4, 9, 16, 25]\n>>> [n for n in X]                      # Now it's empty: __iter__ returns same self\n[]",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1136,
      "chapter": null,
      "content": ">>> [n for n in Squares(1, 5)]          # Make a new iterable object\n[1, 4, 9, 16, 25]\n>>> list(Squares(1, 3))                 # A new object for each new __iter__ call\n[1, 4, 9]\nTo support multiple iterations more directly, we could also recode this example\nwith an extra class or other technique, as we will in a moment. As is, though, by\ncreating a new instance for each iteration, you get a fresh copy of iteration state:\n>>> 36 in Squares(1, 10)                # Other iteration tools\nTrue\n>>> a, b, c = Squares(1, 3)             # Each calls __iter__ and then __next__\n>>> a, b, c\n(1, 4, 9)\n>>> ':'.join(map(str, Squares(1, 5)))\n'1:4:9:16:25'\nJust like single-scan built-ins such as map, converting to a list supports multiple\nscans as well but adds time and space performance costs, which may or may not\nbe significant to a given program:\n>>> X = Squares(1, 5)\n>>> tuple(X), tuple(X)                  # Iterator exhausted in second tuple()\n((1, 4, 9, 16, 25), ())\n>>> X = list(Squares(1, 5))\n>>> tuple(X), tuple(X)\n((1, 4, 9, 16, 25), (1, 4, 9, 16, 25))\nWe’ll improve this to support multiple scans more directly ahead after a bit of\ncompare and contrast.\nClasses versus generators\nNotice that the Squares iterable of Example 30-2 that we’ve been using so far\nwould probably be simpler if it was coded with generator functions or\nexpressions—tools introduced in Chapter 20 that automatically produce iterable\nobjects and retain local variable state between iterations:\n>>> def gsquares(start, stop):                   # Generator function\n        for i in range(start, stop + 1):",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1137,
      "chapter": null,
      "content": "yield i ** 2\n>>> for i in gsquares(1, 5):\n        print(i, end=' ')\n1 4 9 16 25\n>>> for i in (x ** 2 for x in range(1, 6)):      # Generator expression\n        print(i, end=' ')\n1 4 9 16 25\nUnlike classes, generator functions and expressions implicitly save their state\nand create the methods required to conform to the iteration protocol—with\nobvious advantages in code conciseness for simpler examples like these. That is,\ngenerators’ automatic and implicit __iter__ and __next__ suffice here.\nOn the other hand, the class’s more explicit attributes and methods, extra\nstructure, inheritance hierarchies, and support for multiple behaviors may be\nbetter suited for richer use cases.\nOf course, for this artificial example, you could in fact skip both techniques and\nsimply use a for loop, map, or a list comprehension to build the list all at once.\nBarring performance data to the contrary, the best and fastest way to accomplish\na task in Python is often also the simplest:\n>>> [x ** 2 for x in range(1, 6)]\n[1, 4, 9, 16, 25]\nThat said, classes are better at modeling more complex iterations, especially\nwhen they can benefit from the assets of classes in general. An iterable that\nproduces items in a complex database or web service result, for example, might\nbe able to take fuller advantage of classes—and leverage more flexible coding\nstructures like that of the next section.\nMultiple Iterators on One Object\nEarlier, it was mentioned in passing that the iterator object (with a __next__)\nproduced by an iterable may be defined as a separate class with its own state\ninformation to more directly support multiple active iterations over the same",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1138,
      "chapter": null,
      "content": "data. To understand this better, consider what happens when we step across a\nbuilt-in type like a string:\n>>> S = 'ace'\n>>> for x in S:\n        for y in S:\n            print(x + y, end=' ')\naa ac ae ca cc ce ea ec ee\nHere, the outer loop grabs an iterator from the string by calling iter, and each\nnested loop does the same to get an independent iterator. Because each active\niterator has its own state information, each loop can maintain its own position in\nthe string, regardless of any other active loops. Moreover, we’re not required to\nmake a new string or convert to a list each time; the single string object itself\nsupports multiple scans.\nWe saw related examples earlier, in Chapters 14 and 20. For instance, generator\nfunctions and expressions, as well as built-ins like map and zip, proved to be\nsingle-iterator objects, thus supporting a single active scan. By contrast, the\nrange built-in, and other built-in types like lists, support multiple active iterators\nwith independent positions.\nWhen we code user-defined iterables with classes, it’s up to us to decide whether\nwe will support a single active iteration or many. To achieve the multiple-iterator\neffect, __iter__ simply needs to define a new stateful object for the iterator\ninstead of returning self for each iterator request.\nFor example, the class SkipObject in Example 30-3 defines an iterable object\nthat skips every other item on iterations. Because its iterator object is created\nanew from a supplemental class for each iteration, it supports multiple active\nloops directly.\nExample 30-3. skipper.py\nclass SkipObject:\n   def __init__(self, wrapped):                  # Save item to be used\n       self.wrapped = wrapped\n   def __iter__(self):\n       return SkipIterator(self.wrapped)         # New iterator each time",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1139,
      "chapter": null,
      "content": "class SkipIterator:\n   def __init__(self, wrapped):\n       self.wrapped = wrapped                    # Iterator state information\n       self.offset  = 0\n   def __next__(self):\n       if self.offset >= len(self.wrapped):      # Terminate iterations\n           raise StopIteration\n       else:\n           item = self.wrapped[self.offset]      # else return and skip\n           self.offset += 2\n           return item\nif __name__ == '__main__':\n   alpha = 'abcdef'\n   skipper = SkipObject(alpha)                   # Make container object\n   I = iter(skipper)                             # Make an iterator on it\n   print(next(I), next(I), next(I))              # Visit offsets 0, 2, 4\n   for x in skipper:                # for calls __iter__ automatically\n       for y in skipper:            # Nested fors call __iter__ again each time\n           print(x + y, end=' ')    # Each iterator has its own state, offset\nWhen run, this example works like the earlier nested loops with built-in strings.\nEach active loop has its own position in the string because each obtains an\nindependent iterator object that records its own state information:\n$ python3 skipper.py\na c e\naa ac ae ca cc ce ea ec ee\nBy contrast, our earlier Squares class of Example 30-2 supports just one active\niteration unless we call Squares again in nested loops to obtain new objects\n(else the outer for loop would run just once). Here, there is just one SkipObject\niterable, with multiple iterator objects created from it.\nClasses versus slices\nAs before, we could achieve similar results with built-in tools—for example,\nslicing with a third bound to skip items:\n>>> S = 'abcdef'\n>>> for x in S[::2]:\n        for y in S[::2]:             # New objects on each iteration\n            print(x + y, end=' ')",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1140,
      "chapter": null,
      "content": "aa ac ae ca cc ce ea ec ee\nThis isn’t quite the same, though, for two reasons. First, each slice expression\nhere will physically store the result list all at once in memory; iterables, on the\nother hand, produce just one value at a time, which can save substantial space\nand startup time for large result lists.\nSecond, slices produce new objects, so we’re not really iterating over the same\nobject in multiple places here. To be closer to the class, we would need to make\na single object to step across by slicing ahead of time:\n>>> S = 'abcdef'\n>>> S = S[::2]\n>>> S\n'ace'\n>>> for x in S:\n        for y in S:                  # Same object, new iterators\n            print(x + y, end=' ')\naa ac ae ca cc ce ea ec ee\nThis is more similar to our class-based solution, but it still stores the slice result\nin memory all at once (there is no generator form of built-in slicing today), and\nit’s only equivalent for this particular case of skipping every other item.\nBecause user-defined iterables coded with classes can do anything a class can\ndo, they are much more general than this example may imply. Though such\ngenerality is not required in all applications, user-defined iterables are a\npowerful tool—they allow us to make arbitrary objects look and feel like the\nother sequences and iterables we have met in this book. We could use this\ntechnique with a database object, for example, to support iterations over large\ndatabase fetches, with multiple cursors into the same query result.\nCoding Alternative: __iter__ Plus yield\nNow for something more implicit—but potentially useful nonetheless. In some\napplications, it’s possible to minimize coding requirements for user-defined\niterables by combining the __iter__ method we’re exploring here and the\nyield generator function statement we studied in Chapter 20. Because generator",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1141,
      "chapter": null,
      "content": "functions automatically save local-variable state and create required iterator\nmethods, they fit this role well and complement the state retention and other\nutility we get from classes.\nAs a quick review, recall that any function that contains a yield statement is\nturned into a generator function. When called, it returns a new generator object\nwith automatic retention of local scope and code position; an automatically\ncreated __iter__ method that simply returns itself; and an automatically created\n__next__ method that starts the function or resumes it where it last left off:\n>>> def gen(x):\n       for i in range(x): yield i ** 2\n>>> G = gen(5)               # Create a generator with __iter__ and __next__\n>>> G.__iter__() is G        # Both methods exist on the same object\nTrue\n>>> I = iter(G)              # Runs __iter__: generator returns itself\n>>> next(I), next(I)         # Runs __next__\n(0, 1)\n>>> list(gen(5))             # Iteration tools automatically run iter and next\n[0, 1, 4, 9, 16]\nThis is still true even if the generator function with a yield happens to be a\nmethod named __iter__ . Whenever invoked by an iteration tool, such a\nmethod will return a new generator object with the requisite __next__. As an\nadded bonus, generator functions coded as methods in classes have access to\nsaved state in both instance attributes and local-scope variables.\nFor example, the class in Example 30-4 is equivalent to the initial Squares user-\ndefined iterable we coded earlier in Example 30-2, but noticeably shorter (4\nlines, for anyone counting).\nExample 30-4. squares_yield.py\nclass Squares:                                 # __iter__ + yield generator\n   def __init__(self, start, stop):           # __next__ is automatic/implied\n       self.start = start\n       self.stop  = stop\n   def __iter__(self):\n       for value in range(self.start, self.stop + 1):\n           yield value ** 2",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1142,
      "chapter": null,
      "content": "As before, for loops and other iteration tools iterate through instances of this\nclass automatically:\n$ python3\n>>> from squares_yield import Squares\n>>> for i in Squares(1, 5): print(i, end=' ')      # Runs __iter__, then __next__\n1 4 9 16 25\nAnd as always, we can also look under the hood to see how this actually works\nin iteration tools like for. Running our class instance through iter obtains the\nresult of calling __iter__ as usual. In this case, though, the result is a generator\nobject with an automatically created __next__ of the same sort we always get\nwhen calling a generator function that contains a yield. The only difference\nhere is that the generator function is automatically called on iter. Invoking the\nresult object’s next interface produces results on demand:\n>>> S = Squares(1, 5)          # Runs __init__: class saves instance state\n>>> S\n<squares_yield.Squares object at 0x109e30b90> \n>>> I = iter(S)                # Runs __iter__: returns a generator\n>>> I\n<generator object Squares.__iter__ at 0x109ecb3e0>\n>>> next(I)\n1\n>>> next(I)                    # Runs generator's __next__\n4\n…etc…\n>>> next(I)                    # Generator has both instance and local scope state\nStopIteration\nIt may also help to notice that we could name the generator method something\nother than __iter__ and call manually to iterate—Squares(…).gen(), for\nexample. Using the __iter__ name invoked automatically by iteration tools\nsimply skips a manual attribute fetch and call step. Example 30-5 demos the\nidea.\nExample 30-5. squares_yield_manual.py\nimport squares_yield                      # Reuse prior example's __init__",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1143,
      "chapter": null,
      "content": "class Squares(squares_yield.Squares):     # Non __iter__ equivalent \n   def gen(self):\n       for value in range(self.start, self.stop + 1):\n           yield value ** 2\nThe example also imports Example 30-4 to inherit its constructor. When run, its\nresults are the same, but we must call the gen method explicitly to fetch an\niterable—a step that __iter__ automates and obviates:\n$ python3\n>>> from squares_yield_manual import Squares\n>>> for i in Squares(1, 5).gen(): print(i, end=' ')\n…same results…\n>>> S = Squares(1, 5)\n>>> I = iter(S.gen())          # Call generator manually for iterable/iterator\n>>> next(I)\n…same results…\nCoding the generator as __iter__ instead cuts out the middleperson in your\ncode, though both schemes ultimately wind up creating a new generator object\nfor each iteration:\nWith __iter__, iteration triggers __iter__, which returns a new\ngenerator with __next__.\nWithout __iter__, your code calls to make a generator, which returns\nitself for __iter__.\nSee Chapter 20 for more on yield and generators if this is puzzling, and\ncompare it with the more explicit __next__ version in Example 30-2 earlier. If\nyou do, you’ll notice that the squares_yield.py version is 4 lines shorter (7 versus\n11, not counting whitespace). In a sense, this scheme reduces class coding\nrequirements much like the closure functions of Chapter 17, but in this case does\nso with a combination of functional and OOP techniques instead of an alternative\nto classes. For example, the generator method still leverages self attributes.\nThis may also seem like one too many levels of magic to some observers—it\nrelies on both the iteration protocol and the object creation of generators, both of\nwhich are highly implicit (in contradiction of longstanding Python goals).\nOpinions aside, it’s important to understand the non-yield flavor of class",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1144,
      "chapter": null,
      "content": "iterables too, because it’s explicit, general, and sometimes broader in scope.\nStill, the __iter__/yield technique may prove effective in cases where it\napplies. It also comes with a substantial advantage—as the next section explains.\nMultiple iterators with yield\nBesides its code conciseness, the user-defined class iterable of the prior section\nbased upon the __iter__/yield combination has an important added bonus—it\nalso supports multiple active iterators automatically. This naturally follows from\nthe fact that each call to __iter__ is a new call to a generator function, which\nreturns a new generator with its own copy of the local scope for state retention.\nUsing Example 30-4 again:\n$ python3\n>>> from squares_yield import Squares   # Using the __iter__/yield Squares\n>>> S = Squares(1, 5)\n>>> I = iter(S)\n>>> next(I); next(I)\n1\n4\n>>> K = iter(S)                         # With yield, multiple iterators automatic\n>>> next(K)\n1\n>>> next(I)                             # I is independent of K: own local state\n9\nAlthough generator functions are single-scan iterables by nature, the implicit\ncalls to __iter__ in iteration tools make new generators supporting new\nindependent scans:\n>>> S = Squares(1, 3)\n>>> for i in S:                         # Each \"for\" calls __iter__\n        for j in S:\n            print(f'{i}:{j}', end=' ')\n1:1 1:4 1:9 4:1 4:4 4:9 9:1 9:4 9:9\nTo do the same without yield requires a supplemental class that stores iterator\nstate explicitly and manually, using techniques of the preceding section. Per\nExample 30-6, this grows to 15 lines: 8 more than with yield.",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1145,
      "chapter": null,
      "content": "Example 30-6. squares_nonyield.py\nclass Squares:\n   def __init__(self, start, stop):                 # Non-yield generator\n       self.start = start                           # Multiscans: extra object\n       self.stop  = stop\n   def __iter__(self):\n       return SquaresIter(self.start, self.stop)\nclass SquaresIter:\n   def __init__(self, start, stop):\n       self.value = start - 1\n       self.stop  = stop\n   def __next__(self):\n       if self.value == self.stop:\n           raise StopIteration\n       self.value += 1\n       return self.value ** 2\nThis works the same as the yield multiscan version, but with more—and more\nexplicit—code:\n$ python3\n>>> from squares_nonyield import Squares\n>>> for i in Squares(1, 5): print(i, end=' ')\n1 4 9 16 25\n>>> S = Squares(1, 5)\n>>> I = iter(S)\n>>> next(I); next(I)\n1\n4\n>>> K = iter(S)                          # Multiple iterators without yield\n>>> next(K)\n1\n>>> next(I)\n9\n>>> S = Squares(1, 3)\n>>> for i in S:                          # Each \"for\" calls __iter__\n        for j in S:\n            print(f'{i}:{j}', end=' ')\n1:1 1:4 1:9 4:1 4:4 4:9 9:1 9:4 9:9",
      "content_length": 1102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1146,
      "chapter": null,
      "content": "Finally, the generator-based approach could similarly remove the need for an\nextra iterator class in the prior item-skipper example, skipper.py of Example 30-\n3, thanks to its automatic methods and local variable state retention.\nExample 30-7 codes the mod, which checks in at 9 lines versus the original’s 16,\nsans the original’s self-test.\nExample 30-7. skipper_yield.py\nclass SkipObject:                           # Another __iter__ + yield generator\n   def __init__(self, wrapped):            # Instance scope retained normally\n       self.wrapped = wrapped              # Local scope state saved auto\n   def __iter__(self):\n       offset = 0\n       while offset < len(self.wrapped):\n           item = self.wrapped[offset]\n           offset += 2\n           yield item\nThis works the same as the non-yield multiscan version, but with less—and less\nexplicit—code:\n$ python3\n>>> from skipper_yield import SkipObject\n>>> skipper = SkipObject('abcdef')\n>>> I = iter(skipper)\n>>> next(I); next(I); next(I)\n'a'\n'c'\n'e'\n>>> for x in skipper:               # Each \"for\" calls __iter__: new auto generator\n        for y in skipper:\n            print(x + y, end=' ')\naa ac ae ca cc ce ea ec ee\nOf course, these are all artificial examples that could be replaced with simpler\ntools like comprehensions, and their code may or may not scale up in kind to\nmore realistic tasks. Study these alternatives to see how they compare. As so\noften in programming, the best tool for the job will likely be the best tool for\nyour job.\nMembership: __contains__, __iter__, and",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1147,
      "chapter": null,
      "content": "__getitem__\nThe iteration story is even richer than told thus far. Operator overloading is often\nlayered: classes may provide specific methods or more general alternatives used\nas fallback options. We’ve already seen this for general iteration (__iter__ or\nelse __getitem__), and will encounter another example ahead when we meet\nBoolean values.\nAlso in the iterations domain, classes can implement the in membership\noperator as an iteration, using either the __iter__ or __getitem__ methods. To\nsupport more specific membership, though, classes may code a __contains__\nmethod—when present, this method is preferred over __iter__, which is\npreferred over __getitem__. The __contains__ method should define\nmembership as applying to keys for a mapping (and can use quick lookups) and\nas a search for sequences.\nConsider the class Iters in Example 30-8. It codes all three methods and tests\nmembership and various iteration tools applied to an instance. To demo, its\nmethods print trace messages when called, its self-test code cycles through tests\ncoded with match to call them out, and its methods’ traces show up before those\nof its tests.\nExample 30-8. contains.py\ndef trace(msg, end=''):\n   print(f'{msg} ', end=end)                 # print sans newline\nclass Iters:\n   def __init__(self, value):\n       self.data = value\n   def __getitem__(self, i):                 # Fallback for iteration\n       trace(f'@get[{i}]')                   # Also for index, slice\n       return self.data[i]\n   def __iter__(self):                       # Preferred for iteration\n       trace('@iter')                        # Allows only one active iterator\n       self.ix = 0\n       return self\n   def __next__(self):\n       trace('@next')\n       if self.ix == len(self.data): raise StopIteration",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1148,
      "chapter": null,
      "content": "item = self.data[self.ix]\n       self.ix += 1\n       return item\n   def __contains__(self, x):                # Preferred for 'in' membership\n       trace('@contains')\n       return x in self.data\ndef self_test(Iters):\n   X = Iters([1, 2, 3, 4])                            # Make one instance\n   tests = 'In', 'For', 'Comp', 'Map', 'Manual'\n   for test in tests:\n       trace(test.ljust(max(map(len, tests)) + 1))\n       match test:\n           case 'In':\n               trace(3 in X)                          # Membership\n           case 'For':\n               for i in X:                            # for-loop iteration\n                   trace(i, end='| ')\n           case 'Comp':\n               trace([i ** 2 for i in X])             # Other Iteration tools\n           case 'Map':\n               trace(list(map(bin, X)))\n           case 'Manual':\n               I = iter(X)                            # Manual iteration\n               while True:\n                   try:\n                       trace(next(I), end='| ')\n                   except StopIteration:\n                       break\n       print()\nif __name__ == '__main__': self_test(Iters)            # Test Iters here\nAs is, the class in this file has an __iter__ that supports only a single active\nscan at any point in time (e.g., nested loops won’t work) because each iteration\nattempt resets the scan cursor to the front. Now that you know about yield in\niteration methods, you should be able to tell that Example 30-9 is equivalent but\nallows multiple active scans—and judge for yourself whether its more implicit\nnature is worth the nested-scan support (and 5 lines shaved).\nExample 30-9. contains-yield.py\nfrom contains import *\nclass ItersYield(Iters):\n   def __iter__(self):                    # Preferred for iteration\n       trace('@iter @next')               # Allows multiple active iterators",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1149,
      "chapter": null,
      "content": "for x in self.data:                # Implicit generator alternative\n           yield x\n           trace('@next')\nif __name__ == '__main__': self_test(ItersYield)      # Test Iters here\nWhen either version of this file runs, its output is as follows—the specific\n__contains__ intercepts membership, the general __iter__ catches other\niteration tools such that __next__ (whether explicitly coded or implied by\nyield) is called repeatedly, and __getitem__ is never called:\n$ python3 contains.py\nIn      @contains True\nFor     @iter @next 1 | @next 2 | @next 3 | @next 4 | @next\nComp    @iter @next @next @next @next @next [1, 4, 9, 16]\nMap     @iter @next @next @next @next @next ['0b1', '0b10', '0b11', '0b100']\nManual  @iter @next 1 | @next 2 | @next 3 | @next 4 | @next\nWatch, though, what happens to this code’s output if we comment out its\n__contains__ method—membership is now routed to the general __iter__\ninstead (add triple quotes above and below the method to test live):\n$ python3 contains.py\nIn      @iter @next @next @next True\nFor     @iter @next 1 | @next 2 | @next 3 | @next 4 | @next\nComp    @iter @next @next @next @next @next [1, 4, 9, 16]\nMap     @iter @next @next @next @next @next ['0b1', '0b10', '0b11', '0b100']\nManual  @iter @next 1 | @next 2 | @next 3 | @next 4 | @next\nAnd finally, here is the output if both __contains__ and __iter__ are\ncommented out—the indexing __getitem__ fallback is called with successively\nhigher indexes until it raises IndexError, for membership and other iteration\ntools:\n$ python3 contains.py\nIn      @get[0] @get[1] @get[2] True\nFor     @get[0] 1 | @get[1] 2 | @get[2] 3 | @get[3] 4 | @get[4]\nComp    @get[0] @get[1] @get[2] @get[3] @get[4] [1, 4, 9, 16]\nMap     @get[0] @get[1] @get[2] @get[3] @get[4] ['0b1', '0b10', '0b11', '0b100']\nManual  @get[0] 1 | @get[1] 2 | @get[2] 3 | @get[3] 4 | @get[4]\nAs we’ve seen, the __getitem__ method does other work too: besides iterations,",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1150,
      "chapter": null,
      "content": "it also intercepts explicit indexing as well as slicing. Slice expressions trigger\n__getitem__ with a slice object containing bounds, both for built-in types and\nuser-defined classes, so slicing is automatic in our class. With __iter__ enabled\nor not:\n$ python3\n>>> from contains import Iters\n>>> X = Iters('hack')\n>>> X[0]                             # Indexing: __getitem__(0)\n@get[0] 'h' \n>>> X[1:]                            # Slicing: __getitem__(slice(…))\n@get[slice(1, None, None)] 'ack'\n>>> X[:-1]\n@get[slice(None, -1, None)] 'hac'\nIterations, though, are more selective—we get the first of the following if\n__iter__ is still commented out and the second if it’s not (be sure to restart or\nreload after the file mod either way):\n>>> list(X) \n@get[0] @get[1] @get[2] @get[3] @get[4] ['h', 'a', 'c', 'k']\n>>> list(X)\n@iter @next @next @next @next @next ['h', 'a', 'c', 'k']\nIn more realistic iteration use cases that are not sequence-oriented, though, the\n__iter__ method may be easier to write since it must not manage an integer\nindex, and __contains__ allows for membership optimization as a special case.\nWhile iteration is a rich topic, it’s time to move on to the next stop on our\noverloading tour.\nAttribute Access: __getattr__ and __setattr__\nIn Python, classes can also intercept basic attribute access (a.k.a. qualification)\nwhen needed or useful. Specifically, for an object created from a class, the dot\noperator expression object.attribute can be implemented by your code too,\nfor reference, assignment, and deletion contexts. We explored a limited example\nin this category which rerouted attribute fetches in the tutorial of Chapter 28, but",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1151,
      "chapter": null,
      "content": "will review and expand on the topic here.\nAttribute Reference\nThe __getattr__ method intercepts attribute references. It’s called with the\nattribute name as a string whenever you try to qualify an instance with an\nundefined (nonexistent) attribute name. It is not called if Python can find the\nattribute using its inheritance tree search procedure.\nBecause of its behavior, __getattr__ is useful as a hook for responding to\nattribute requests in a generic fashion. It’s commonly used to delegate calls to\nembedded (or “wrapped”) objects from a proxy controller object—of the sort\nintroduced in Chapter 28’s introduction to delegation. This method can also be\nused to adapt classes to an interface or add accessors for data attributes after the\nfact—logic in a method that validates or computes an attribute after it’s already\nbeing used with simple dot notation (possibly after a rename of the original).\nThe basic mechanism underlying these goals is straightforward—the following\nclass catches attribute references, computing the value for one dynamically, and\ntriggering an error for others unsupported with the raise statement described\nearlier in this chapter for iterators (and again, fully covered in Part VII):\n>>> class Empty:\n        def __getattr__(self, attrname):           # On self.undefined\n            if attrname == 'age':\n                return 40\n            else:\n                raise AttributeError(attrname)\n>>> X = Empty()\n>>> X.age                  # Becomes X.__getattr__('age')\n40\n>>> X.name                 # Unsupported attribute\n…error text omitted…\nAttributeError: name\nHere, the Empty class and its instance X have no real attributes of their own, so\nthe access to X.age gets routed to the __getattr__ method; self is assigned\nthe instance (X), and attrname is assigned the undefined attribute name string\n('age'). The class makes age look like a real attribute by returning a real value",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1152,
      "chapter": null,
      "content": "as the result of the X.age qualification expression (40). In effect, age becomes a\ndynamically computed attribute—its value is formed by running code, not\nfetching an object.\nFor attributes that the class doesn’t know how to handle, __getattr__ raises the\nbuilt-in Attribute Er⁠ror exception to tell Python that these are bona fide\nundefined names; asking for X.name triggers the error. You’ll see __getattr__\nagain when we explore delegation and properties at work in the next two\nchapters; let’s move on to related tools here.\nAttribute Assignment and Deletion\nIn the same department, the __setattr__ intercepts all attribute assignments. If\nthis method is defined or inherited, self.attr = value becomes\nself.__setattr__('attr', value). Like __getattr__, this allows your\nclass to catch attribute changes and validate or transform as desired.\nThis method is a bit trickier to use, though, because assigning to any self\nattributes within __setattr__ calls __setattr__ again, potentially causing an\ninfinite recursion loop (and a fairly quick stack overflow exception!). In fact,\nthis applies to all self attribute assignments anywhere in the class—all are\nrouted to __setattr__, even those in other methods, and those to names other\nthan that which may have triggered __setattr__ in the first place. So be\nwarned: this catches all attribute assignments.\nIf you wish to use this method, you can avoid loops by coding instance attribute\nassignments as assignments to attribute dictionary keys. That is, use\nself.__dict__['name'] = x, not self.name = x; because you’re not\nassigning to __dict__ itself, this avoids the loop:\n>>> class Accesscontrol:\n        def __setattr__(self, attr, value):\n            if attr == 'age':\n                self.__dict__[attr] = value + 10      # Not self.name=val or setattr\n            else:\n                raise AttributeError(attr + ' not allowed')\n>>> X = Accesscontrol()\n>>> X.age = 40                # Becomes X.__setattr__('age', 40)",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1153,
      "chapter": null,
      "content": ">>> X.age                     # Found in __dict__ as usual\n50\n>>> X.name = 'Pat'            # Unsupported attribute\n…text omitted…\nAttributeError: name not allowed\nIf you change the __dict__ assignment within this class to either of the\nfollowing, it triggers the infinite recursion loop and exception—both dot\nnotation and its setattr built-in function equivalent (the assignment analog of\ngetattr) fail when age is assigned outside the class:\nself.age = value + 10                            # Loops!\nsetattr(self, attr, value + 10)                  # Loops! (attr is 'age')\nAn assignment to any other self attribute within the class triggers a recursive\n__setattr__ call too, though in this class ends less dramatically in the manual\nAttributeError exception:\nself.other = 99                                  # Recurs + fails, but doesn't loop\nIt’s also possible to avoid recursive loops in a class that uses __setattr__ by\nrerouting any attribute assignments to a higher superclass with a call, instead of\nassigning keys in __dict__:\nobject.__setattr__(self, attr, value + 10)       # OK: doesn't loop (preview)\nThis uses an explicit-class call to the implied object superclass above all\ntopmost classes (and has a super().__setattr__(…) equivalent sans self per\nthe sidebar “The super Alternative”). In fact, this alternative may be required in\nsome classes per the upcoming note, though this is rare in practice.\nA third attribute-management method, __delattr__, is passed the attribute\nname string and invoked on all attribute deletions (i.e., del object.attr). Like\n__setattr__, it must avoid recursive loops by running attribute deletions within\nthe class using __dict__, or rerouting them to a superclass.\nNOTE",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1154,
      "chapter": null,
      "content": "Attribute outliers: Preceding chapters mentioned that attributes coded with advanced class\ntools such as slots and properties are not physically stored in the instance’s __dict__\nnamespace dictionary—and slots may even preclude a __dict__ altogether. As noted, dir and\ngetattr might be needed for listing and fetching attributes in classes using these tools, but\nassignment is similarly impacted: to support such “virtual” attributes, __setattr__ may need\nto use the object.__setattr__ scheme shown here, not self.__dict__ indexing. You’ll\nlearn much more about these attribute tools in upcoming chapters.\nOther Attribute-Management Tools\nThe three attribute-access overloading methods we’ve met so far allow you to\ncontrol or specialize access to attributes in your objects. They tend to play highly\nspecialized roles, some of which we’ll explore later in this book. For another\nexample of __getattr__ at work, see Chapter 28’s person-composite.py\n(Example 28-11).\nAnd for future reference, keep in mind that there are other ways to manage\nattribute access in Python:\nThe __getattribute__ method intercepts all attribute fetches, not just\nthose that are undefined; like __setattr__, it must avoid loops for\nother attribute fetches in the class, usually with object rerouting.\nThe property built-in function allows us to associate methods with\nfetch and set operations on a specific class attribute; it cannot catch\naccesses generically but can define what some do.\nDescriptors provide a protocol for associating __get__ and __set__\nmethods of a class with accesses to a specific instance or class attribute;\nthey are as focused as property and, in fact, are used to implement it.\nSlots attributes are declared in classes but create implicit storage in each\ninstance; if present, generic tools may need to list, fetch, and assign\nwith schemes described in the preceding note.\nBecause these are all advanced tools that are not of interest to every Python\nprogrammer, we’ll defer the details of other attribute-management techniques\nuntil Chapter 32, and await their focused coverage in Chapter 38.",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1155,
      "chapter": null,
      "content": "Emulating Privacy for Instance Attributes: Part 1\nAs another use case for attribute tools, the code in Example 30-10—file\nprivate0.py—generalizes the previous example, to allow each subclass to have\nits own list of private names that cannot be assigned to its instances (and raises a\nbuilt-in exception with raise, which you’ll have to take on faith until Part VII).\nExample 30-10. private0.py\nclass Privacy:\n   def __setattr__(self, attr, value):             # On self.attr = value\n       if attr in self.privates:\n           raise NameError(f'{attr!r} for {self}') \n       else:\n           self.__dict__[attr] = value             # Avoid loops by using dict key\nclass Test1(Privacy):\n   privates = ['age']\nclass Test2(Privacy):\n   privates = ['name', 'pay']\n   def __init__(self):\n       self.__dict__['name'] = 'Pat'               # To do better, see Chapter 39\nif __name__ == '__main__':\n   x = Test1()\n   x.name = 'Sue'      # Works\n   print(x.name)\n  #x.age = 40          # Fails\n   y = Test2()\n   y.age = 30          # Works\n   print(y.age)\n  #y.name = 'Bob'      # Fails\nThis is a first-cut solution for an implementation of attribute privacy in Python\n—disallowing changes to attribute names outside a class. Although Python\ndoesn’t support private declarations per se, techniques like this can emulate\nmuch of their purpose.\nThis particular attempt, though, is a partial—and even clumsy—solution. To\nmake it more effective, we must augment it to allow classes to set their private\nattributes more naturally, without having to go through __dict__ each time, as\nthe constructor must do here to avoid triggering __setattr__ and an exception.",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1156,
      "chapter": null,
      "content": "A better and more complete approach might require a wrapper (“proxy”) class to\ncheck for private attribute accesses made outside the class only and a\n__getattr__ to validate attribute fetches too.\nWe’ll postpone a more complete solution to attribute privacy until Chapter 39,\nwhere we’ll use class decorators to intercept and validate attributes more\ngenerally. Even though privacy can be emulated this way, though, it almost never\nis in practice. Python programmers are able to write large OOP frameworks and\napplications without private declarations—an interesting finding about access\ncontrols in general that is beyond the scope of our purposes here.\nStill, catching attribute references and assignments is generally a useful\ntechnique; it supports delegation, a design technique that allows controller\nobjects to wrap up embedded objects, add new behaviors, and route other\noperations back to the wrapped objects. Because they involve design topics,\nwe’ll revisit delegation and wrapper classes in the next chapter. Here, it’s time to\nmove ahead in the operator overloading domain.\nString Representation: __repr__ and __str__\nOur next methods deal with display formats—a topic we’ve already explored in\nprior chapters but will summarize and formalize here. To serve as a guinea pig,\nthe following codes the __init__ constructor and the __add__ overload\nmethod, both of which we’ve already seen (+ is an in-place operation here, just\nto show that it can be; this may be better coded as a named method per\nChapter 27, or the in-place __iadd__ covered ahead). As we’ve learned, the\ndefault display of instance objects for a class like this is neither generally useful\nnor aesthetically pretty:\n>>> class adder:\n        def __init__(self, value=0):\n            self.data = value                    # Initialize data\n        def __add__(self, other):\n            self.data += other                   # Add other in place\n>>> x = adder()                                  # Default displays:\n>>> print(x)                                     # str or else repr\n<__main__.adder object at 0x106110c80> \n>>> x                                            # repr",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1157,
      "chapter": null,
      "content": "<__main__.adder object at 0x106110c80>\nBut coding or inheriting string representation methods allows us to customize\nthe display—as in the following, which defines a __repr__ method in a subclass\nthat returns a string representation for its instances:\n>>> class addrepr(adder):                        # Inherit __init__, __add__\n        def __repr__(self):                      # Add string representation\n            return f'addrepr({self.data})'       # Convert to as-code string\n>>> x = addrepr(2)                              \n>>> x + 1                           \n>>> x                                    # Runs __repr__\naddrepr(3)\n>>> print(x)                             # Runs __repr__\naddrepr(3)\n>>> str(x), repr(x)                      # Runs __repr__ for both\n('addrepr(3)', 'addrepr(3)')\nIf defined, __repr__ (or its close relative, __str__) is called automatically\nwhen class instances are printed or converted to strings. These methods allow\nyou to define a better display format for your objects than the default instance\ndisplay. Here, __repr__ uses basic string formatting to convert the managed\nself.data object to a more human-friendly string for display.\nWhy Two Display Methods?\nSo far, this section has largely been review. But while these methods are\ngenerally straightforward to use, their roles and behavior have some subtle\nimplications for both design and coding. In particular, Python provides its two\ndisplay methods to support alternative displays for different audiences:\n__str__ is tried first for the print operation and the str built-in\nfunction (the internal equivalent of which print runs). It generally\nshould return a user-friendly display.\n__repr__ is used in all other contexts: for interactive echoes, the repr\nfunction, and nested appearances, as well as by print and str if no\n__str__ is present. It should generally return an as-code string that\ncould be used to re-create the object or a detailed display for",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1158,
      "chapter": null,
      "content": "developers.\nThat is, __repr__ is used everywhere, except by print and str when a\n__str__ is defined. This means you can code a __repr__ to define a single\ndisplay format used everywhere and may code a __str__ to either support\nprint and str exclusively or to provide an alternative display for them.\nGeneral tools may also prefer __str__ to leave other classes the option of\nadding an alternative __repr__ display for use in other contexts, as long as\nprint and str displays suffice for the tool. Conversely, a general tool that codes\na __repr__ still leaves clients the option of adding alternative displays with a\n__str__ for print and str. In other words, if you code either, the other is\navailable for an additional display. In cases where the choice isn’t clear, use\n__str__ for higher-level displays and __repr__ for lower-level displays and all-\ninclusive roles.\nLet’s write some code to illustrate these two methods’ distinctions in more\nconcrete terms. The prior example in this section showed how __repr__ is used\nas the fallback option in many contexts. However, while printing falls back on\n__repr__ if no __str__ is defined, the inverse is not true—other contexts, such\nas interactive echoes, use __repr__ only and don’t try __str__ at all:\n>>> class addstr(adder):\n        def __str__(self):                       # __str__ but no __repr__\n            return f'[Value: {self.data}]'       # Convert to nice string\n>>> x = addstr(3)\n>>> x + 1\n>>> x                                            # Default __repr__ (in object)\n<__main__.addstr object at 0x106111b20> \n>>> print(x)                                     # Runs __str__ (in addstr)\n[Value: 4]\n>>> str(x), repr(x)\n('[Value: 4]', '<__main__.addstr object at 0x106111b20>')\nBecause of this, __repr__ may be best if you want a single display for all\ncontexts. By defining both methods, though, you can support different displays\nin different contexts—for example, a class’s end-user display with __str__, and\na class’s developer display with __repr__. In effect, __str__ simply overrides",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1159,
      "chapter": null,
      "content": "__repr__ for more user-friendly display contexts:\n>>> class addboth(adder):\n        def __str__(self):\n            return f'[Value: {self.data}]'       # User-friendly string\n        def __repr__(self):\n            return f'addboth({self.data})'       # As-code string\n>>> x = addboth(4)\n>>> x + 1\n>>> x                                            # Runs __repr__\naddboth(5)\n>>> print(x)                                     # Runs __str__\n[Value: 5]\n>>> str(x), repr(x)\n('[Value: 5]', 'addboth(5)')\nBonus: your classes’ __str__ and __repr__ are also run automatically by all\nthree string-formatting tools of Chapter 7. This may not be a shocker, given\nthese tools are defined to work like str and repr in this role, but display\noverloading is not just about REPL echoes and print:\n>>> f'{x!s} {x!r}',  '{!s} {!r}'.format(x, x),  '%s %r' % (x, x)\n('[Value: 5] addboth(5)', '[Value: 5] addboth(5)', '[Value: 5] addboth(5)')\nDisplay Usage Notes\nThough generally simple to use, three points regarding these display methods are\nworth calling out here. First, keep in mind that __str__ and __repr__ must\nboth return strings; other result types are not converted and raise errors, so be\nsure to run them through a to-string converter (e.g., str or f'…') if needed.\nSecond, depending on a container’s string-conversion logic, the user-friendly\ndisplay of __str__ might only apply when objects appear at the top level of a\nprint operation; objects nested in larger objects might still print with their\n__repr__ or its default. The following illustrates both of these points:\n>>> class Printer:\n        def __init__(self, val):\n            self.val = val\n        def __str__(self):                  # Used for instance itself\n            return str(self.val)            # Convert to a string result",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1160,
      "chapter": null,
      "content": ">>> objs = [Printer(2), Printer(3)]\n>>> for x in objs: print(x)                 # __str__ run when instance printed\n                                            # But not when instance is in a list!\n2\n3\n>>> print(objs)\n[<__main__.Printer object at 0x106110c80>, <__main__.Printer object at 0x1060d2570>]\n>>> objs\n[<__main__.Printer object at 0x106110c80>, <__main__.Printer object at 0x1060d2570>]\nTo ensure that a custom display is run in all contexts regardless of the container,\ncode __repr__, not __str__; the former is run in all cases if the latter doesn’t\napply, including nested appearances:\n>>> class Printer:\n        def __init__(self, val):\n            self.val = val\n        def __repr__(self):                 # __repr__ used by print if no __str__\n            return str(self.val)            # __repr__ used if echoed or nested\n>>> objs = [Printer(2), Printer(3)]\n>>> for x in objs: print(x)                 # No __str__: runs __repr__\n2\n3\n>>> print(objs)                             # Runs __repr__, not __str__\n[2, 3]\n>>> objs\n[2, 3]\nThird, and perhaps most subtle, the display methods also have the potential to\ntrigger infinite recursion loops in rare contexts—because some objects’ displays\ninclude displays of other objects, it’s not impossible that a display may trigger a\ndisplay of an object being displayed, and thus loop. This is rare and obscure\nenough to skip here but watch for an example of this looping potential to appear\nfor these methods in a note near the end of the next chapter about its\nlistinherited.py class of Example 31-12, where __repr__ can loop.\nIn practice, __str__ and its more inclusive relative, __repr__, seem to be the\nsecond most commonly used operator-overloading methods in Python scripts,\nbehind __init__. Anytime you can print an object and see a custom display, one\nof these two tools is probably in use. For additional examples of these tools at",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1161,
      "chapter": null,
      "content": "work and the design trade-offs they imply, see Chapter 28’s case study and\nChapter 31’s class-lister mix-ins, as well as their role in Chapter 35’s exception\nclasses, where __str__ is required over __repr__.\nRight-Side and In-Place Ops: __radd__ and\n__iadd__\nOur next group of overloading methods extends the functionality of binary\noperator methods such as __add__ and __sub__ (called for + and –), which\nwe’ve already seen. As mentioned earlier, part of the reason there are so many\noperator-overloading methods is that they come in multiple flavors—for every\nbinary expression, we can implement a left, right, and in-place variant. Though\ndefaults are also applied if you don’t code all three, your objects’ roles dictate\nhow many variants you’ll need to code.\nRight-Side Addition\nFor instance, the __add__ methods coded so far technically do not support the\nuse of instance objects on the right side of the + operator:\n>>> class Adder:\n       def __init__(self, value=0):\n           self.data = value\n       def __add__(self, other):\n           return self.data + other\n>>> x = Adder(5)\n>>> x + 2\n7\n>>> 2 + x\nTypeError: unsupported operand type(s) for +: 'int' and 'Adder'\nTo implement more general expressions and hence support commutative-style\noperators, code the __radd__ method as well. Python calls __radd__ only when\nthe object on the right side of the + is your class instance, but the object on the\nleft is not an instance of your class. The __add__ method for the object on the\nleft is called instead in all other cases. As a demo, consider the class in\nExample 30-11 (which we’ll be adding to in a moment).",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1162,
      "chapter": null,
      "content": "Example 30-11. commuter.py (start)\nclass Commuter1:\n   def __init__(self, val):\n       self.val = val\n   def __add__(self, other):\n       print('add', self.val, other)\n       return self.val + other\n   def __radd__(self, other):\n       print('radd', self.val, other)\n       return other + self.val\nBecause this defines both left- and right-side overloads for +, instances can\nappear on either side, or both:\n>>> from commuter import Commuter1\n>>> x = Commuter1(88)\n>>> y = Commuter1(99)\n>>> x + 1                      # __add__: instance + noninstance\nadd 88 1\n89\n>>> 1 + y                      # __radd__: noninstance + instance\nradd 99 1\n100\n>>> x + y                      # __add__: instance + instance => triggers __radd__\nadd 88 <commuter.Commuter1 object at 0x1011b63f0>\nradd 99 88\n187\nNotice how the order is reversed in __radd__: self is really on the right of the\n+, and other is on the left. Also note that x and y are instances of the same class\nhere; when instances of different classes appear mixed in an expression, Python\nprefers the class of the one on the left. When we add the two instances of this\nclass together, Python runs __add__, which in turn triggers __radd__ by\nsimplifying the left operand and re-adding.\nReusing __add__ in __radd__\nFor truly commutative operations that do not require special-casing by position,\nit is also sometimes sufficient to reuse __add__ for __radd__, either by calling\n__add__ directly; by swapping order and re-adding to trigger __add__",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1163,
      "chapter": null,
      "content": "indirectly; or by simply assigning __radd__ to be an alias for __add__ at the top\nlevel of the class statement (i.e., in the class’s scope). The alternatives in\nExample 30-12 implement all three of these schemes and return the same results\nas the original—though the last saves an extra call or dispatch and hence may be\nquicker (in all, __radd__ is run when self is on the right side of a +).\nExample 30-12. commuter.py (continued)\nclass Commuter2:\n   def __init__(self, val):\n       self.val = val\n   def __add__(self, other):\n       print('add', self.val, other)\n       return self.val + other\n   def __radd__(self, other):\n       return self.__add__(other)              # Call __add__ explicitly\nclass Commuter3:\n   def __init__(self, val):\n       self.val = val\n   def __add__(self, other):\n       print('add', self.val, other)\n       return self.val + other\n   def __radd__(self, other):\n       return self + other                     # Swap order and re-add\nclass Commuter4:\n   def __init__(self, val):\n       self.val = val\n   def __add__(self, other):\n       print('add', self.val, other)\n       return self.val + other\n   __radd__ = __add__                          # Alias: cut out the middleperson\nIn all these, right-side instance appearances trigger the single, shared __add__\nmethod, passing the right operand to self, to be treated the same as a left-side\nappearance. Run these on your own for more insight; their names differ, but their\nusage and returned values are the same as the original Commuter1 of\nExample 30-11.",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1164,
      "chapter": null,
      "content": "Propagating class type\nIn more realistic classes where the class type may need to be propagated in\nresults, things can become trickier: type testing may be required to tell whether\nit’s safe to convert and thus avoid nesting. For instance, without the built-in\nisinstance test in Example 30-13, we could wind up with a Commuter5 whose\nval is another Commuter5 when two instances are added and __add__ triggers\n__radd__.\nExample 30-13. commuter.py (continued)\nclass Commuter5:                                # Propagate class type in results\n   def __init__(self, val):\n       self.val = val\n   def __add__(self, other):\n       if isinstance(other, Commuter5):        # Type test to avoid object nesting\n           other = other.val\n       return Commuter5(self.val + other)      # Else + result is another Commuter\n   def __radd__(self, other):\n       return Commuter5(other + self.val)\n   def __repr__(self):\n       return f'Commuter5({self.val})'\nWhen this version is run, + results retain the Commuter5 type for future\noperations:\n>>> from commuter import Commuter5\n>>> x = Commuter5(88)\n>>> y = Commuter5(99)\n>>> x\nCommuter5(88)\n>>> x + 1                      # Result is another Commuter instance\nCommuter5(89)\n>>> 1 + y\nCommuter5(100) \n>>> z = x + y                  # Not nested: doesn't recur to __radd__\n>>> z\nCommuter5(187)\n>>> z + 10\nCommuter5(197)\n>>> z + z\nCommuter5(374)",
      "content_length": 1384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1165,
      "chapter": null,
      "content": ">>> z + z + 1\nCommuter5(375)\nThe need for the isinstance type test here is very subtle—uncomment, run, and\ntrace to see why it’s required. If you do, you’ll see that the last part of the\npreceding test winds up differing and nesting objects—which still do the math\ncorrectly but kick off pointless recursive calls to simplify their values and extra\nconstructor calls to build results:\n>>> z = x + y                  # With isinstance test+action commented-out       \n>>> z\nCommuter5(Commuter5(187))\n>>> z + 10\nCommuter5(Commuter5(197))\n>>> z + z\nCommuter5(Commuter5(Commuter5(Commuter5(374))))\n>>> z + z + 1\nCommuter5(Commuter5(Commuter5(Commuter5(375))))\nAnother way out of this dilemma is to test and simplify in the constructor\ninstead, per Example 30-14.\nExample 30-14. commuter.py (continued)\nclass Commuter6:                                # Propagate class type in results\n   def __init__(self, val):\n       if isinstance(val, Commuter6):          # Type test to avoid object nesting\n           self.val = val.val\n       else:\n           self.val = val\n   def __add__(self, other):\n       return Commuter6(self.val + other)\n   def __radd__(self, other):\n       return Commuter6(other + self.val)\n   def __repr__(self):\n       return f'Commuter6({self.val})'\nThis last version works the same as the non-nesting Commuter5. To test all of\nthis section’s classes, the rest of commuter.py in Example 30-15 looks and runs\nlike this—like functions, classes can again be used in tuples naturally because\nthey are first-class objects.",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1166,
      "chapter": null,
      "content": "Example 30-15. commuter.py (conclusion)\nif __name__ == '__main__':\n   for klass in (Commuter1, Commuter2, Commuter3, Commuter4, Commuter5, Commuter6):\n       print('-' * 50)\n       x = klass(88)\n       y = klass(99)\n       print(x + 1)\n       print(1 + y)\n       print(x + y)\n$ python3 commuter.py\n--------------------------------------------------\nadd 88 1\n89\nradd 99 1\n100\nadd 88 <__main__.Commuter1 object at 0x101edc2f0>\nradd 99 88\n187\n--------------------------------------------------\n…etc…\n--------------------------------------------------\nCommuter6(89)\nCommuter6(100)\nCommuter6(187)\nExperiment with these classes on your own for more insight. Aliasing __radd__\nto __add__ in Commuter5 and Commuter6, for example, works and saves a line\nbut doesn’t prevent object nesting without these classes’ isinstance tests. See\nalso Python’s manuals for a discussion of other options in this domain; for\nexample, classes may also return the special NotImplemented object for\nunsupported operands to influence method selection (this is treated as though the\nmethod were not defined—and differs from the prior chapter’s\nNotImplementedError).\nIn-Place Addition\nTo also implement += in-place augmented addition, code either an __iadd__ or\nan __add__. The latter is used if the former is absent. In fact, the prior section’s\nCommuter classes already support += for this reason—Python runs __add__ and\nassigns the result manually. The __iadd__ method, though, allows for more\nefficient in-place changes to be coded where applicable; its return value is",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1167,
      "chapter": null,
      "content": "assigned to the target on the left of the +=:\n>>> class Number:\n        def __init__(self, val):\n            self.val = val\n        def __iadd__(self, other):             # __iadd__ explicit: x += y\n            self.val += other                  # Usually returns self\n            return self                        # Else None is returned+assigned\n>>> x = Number(5)\n>>> x += 1\n>>> x += 1\n>>> x.val\n7\nFor mutable objects, this method can often specialize for quicker in-place\nchanges:\n>>> y = Number([1])                            # In-place change faster than +\n>>> y += [2]\n>>> y += [3]\n>>> y.val\n[1, 2, 3]\nThe normal __add__ method is run as a fallback, but may not be able to\noptimize in-place cases:\n>>> class Number:\n        def __init__(self, val):\n            self.val = val\n        def __add__(self, other):              # __add__ fallback: x = (x + y)\n            return Number(self.val + other)    # Propagates class type\n>>> x = Number(5)\n>>> x += 1\n>>> x += 1                                     # And += does concatenation here\n>>> x.val\n7\nThough we’ve focused on + here, keep in mind that every binary operator has\nsimilar right-side and in-place overloading methods that work the same (e.g.,\n__mul__, __rmul__, and __imul__). Still, these methods tend to be uncommon\nin practice; you only code right-side methods for either-side roles and in-place",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1168,
      "chapter": null,
      "content": "methods for code economy or speed—and only if you need to support such\noperators at all. For instance, a Vector class may use these tools, but a Button\nclass probably would not. Button presses, though, may run the next section’s\nmethod.\nCall Expressions: __call__\nOn to our next overloading method: the __call__ method is called when your\ninstance is called. No, this isn’t a circular definition—if defined, Python runs a\n__call__ method for function-call expressions applied to your instances,\npassing along whatever positional or keyword arguments were sent. This allows\ninstances to conform to a function-based API:\n>>> class Callee:\n        def __call__(self, *pargs, **kargs):       # Intercept instance calls\n            print(f'Called: {pargs=} {kargs=}')    # Accept arbitrary arguments\n>>> C = Callee()\n>>> C(1, 2, 3)                                     # C is a \"callable\" object\nCalled: pargs=(1, 2, 3) kargs={}\n>>> C(1, 2, 3, x=4, y=5)\nCalled: pargs=(1, 2, 3) kargs={'x': 4, 'y': 5}\nMore formally, all the argument-passing modes we explored in Chapter 18 are\nsupported by the __call__ method—whatever is passed to the instance is\npassed to this method, along with the usual implied instance argument self. For\nexample, the method definitions:\nclass C:\n    def __call__(self, a, b, c=5, d=6): ...        # Normals and defaults\nclass C:\n    def __call__(self, *pargs, **kargs): ...       # Collect arbitrary arguments\nclass C:\n    def __call__(self, *pargs, d=6, **kargs): ...  # 3.X keyword-only argument\nall match all the following instance calls after assigning X to C():\nX(1, 2)                                            # Omit defaults",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1169,
      "chapter": null,
      "content": "X(1, 2, 3, 4)                                      # Positionals\nX(a=1, b=2, d=4)                                   # Keywords\nX(*[1, 2], **dict(c=3, d=4))                       # Unpack arbitrary arguments\nX(1, *(2,), c=3, **dict(d=4))                      # Mixed modes\nSee Chapter 18 for a refresher on function arguments. The net effect is that\nclasses and instances with a __call__ support the exact same argument syntax\nand semantics as normal functions and methods.\nIntercepting call expression like this allows class instances to emulate the look\nand feel of things like functions, but also retain state information for use during\ncalls. The following, for example, defines callable objects with per-call info\nspecified by explicit attribute assignments:\n>>> class Prod:\n        def __init__(self, value):                 # Accept just one argument\n            self.value = value\n        def __call__(self, other):\n            return self.value * other\n>>> x = Prod(2)                                    # \"Remembers\" 2 in state\n>>> x(3)                                           # 3 (passed) * 2 (state)\n6\n>>> x(4)\n8\nIn this example, the __call__ may seem a bit gratuitous at first glance. A simple\nmethod can provide similar utility:\n>>> class Prod:\n        def __init__(self, value):\n            self.value = value\n        def comp(self, other):\n            return self.value * other\n>>> x = Prod(3)\n>>> x.comp(3)\n9\nHowever, __call__ can become more beneficial when using APIs (i.e.,\nlibraries) that expect functions—it allows us to code objects that conform to an\nexpected function-call interface but also retain state information and other class\nassets such as inheritance. In fact, it may be the third most commonly used",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1170,
      "chapter": null,
      "content": "operator-overloading method, behind the __init__ constructor and the __str__\nand __repr__ display-format alternatives (qualitatively speaking).\nFunction Interfaces and Callback-Based Code\nAs an example, Python’s tkinter GUI module lets you register functions as\nevent handlers (a.k.a. callbacks)—when events occur, tkinter calls the\nregistered objects. If you want an event handler to retain state between events,\nyou can register a class instance that conforms to the expected interface with\n__call__. Chapter 17’s closure functions can achieve similar effects but don’t\nprovide as much support for multiple operations or customization.\nTo demo the concept, here’s a hypothetical example of __call__ applied to the\nGUI domain. The following class defines an object that supports a function-call\ninterface but also has state information that remembers the color a button should\nchange to when it is later pressed:\nclass Callback:\n    def __init__(self, color):               # Function + state information\n        self.color = color\n    def __call__(self):                      # Support calls with no arguments\n        print('turn', self.color)\nIn the context of a GUI, we can register instances of this class as event handlers\nfor buttons, even though the GUI expects to be able to invoke event handlers as\nsimple functions with no arguments:\ncb1 = Callback('blue')                       # Remember blue\ncb2 = Callback('green')                      # Remember green\nB1 = Button(command=cb1)                     # Register handlers\nB2 = Button(command=cb2)\nWhen the button is later pressed, the instance object is called as a simple\nfunction with no arguments, exactly like in the following calls. Because it retains\nstate as instance attributes, though, it remembers what to do—it becomes a\nstateful function object:\ncb1()                                        # On event: prints 'turn blue'",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1171,
      "chapter": null,
      "content": "cb2()                                        # On event: prints 'turn green'\nIn fact, some consider such classes to be the best way to retain state information\nin the Python language. With OOP, the state remembered is made explicit with\nattribute assignments. This is different than other state retention techniques (e.g.,\nglobal variables, enclosing function scope references, and default mutable\narguments), which rely on more limited or implicit behavior. Moreover, the\nadded structure and customization in classes goes beyond state retention.\nOn the other hand, tools such as closure functions are useful in basic state\nretention roles too, and the nonlocal statement makes enclosing scopes a viable\nalternative in more programs. We’ll revisit such trade-offs when we start coding\nsubstantial decorators in Chapter 39, but here’s a quick closure equivalent:\ndef callback(color):                         # Enclosing scope versus attrs\n    def oncall():\n        print('turn', color)\n    return oncall\ncb3 = callback('yellow')                     # Handler to be registered\ncb3()                                        # On event: prints 'turn yellow'\nBefore we move on, there are two other ways that Python programmers\nsometimes tie information to a callback function like this. One option is to use\ndefault arguments in lambda functions:\ncb4 = (lambda color='red':                   # Defaults retain state too\n           print('turn', color))             # lambda defers code till call\ncb4()                                        # On event: prints 'turn red'\nThe other is to use bound methods of a class—covered in the next chapter but\nsimple enough to preview here. A bound-method object is created for instance\nmethods referenced but not called and remembers both the instance and the\nreferenced function. This object may therefore be called later as a simple\nfunction without an instance:\nclass Callback:\n    def __init__(self, color):               # Class with state information\n        self.color = color\n    def changeColor(self):                   # A normally named method",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1172,
      "chapter": null,
      "content": "print('turn', self.color)\ncb1 = Callback('blue')\ncb2 = Callback('yellow')\nB1 = Button(command=cb1.changeColor)         # Bound method: reference, not call\nB2 = Button(command=cb2.changeColor)         # Remembers instance + function pair\nIn this case, when this button is later pressed it’s as if the GUI does the\nfollowing, which invokes the instance’s changeColor method to process the cb1\nobject’s state information instead of calling the instance itself:\ncb1 = Callback('blue')\ncmd = cb1.changeColor                        # Registered event handler\ncmd()                                        # On event: prints 'turn blue'\nNote that a lambda is not required here unless extra arguments must be passed,\nbecause a bound method reference by itself already defers a call until later. This\ntechnique doesn’t require overloading calls with __call__, but either scheme\nmay be preferred for a given program. Again, watch for more about bound\nmethods in the next chapter.\nYou’ll also see another __call__ example in Chapter 32, where we will use it to\nimplement a function decorator—a callable object often used to add a layer of\nlogic on top of an embedded function. Because __call__ allows us to attach\nstate information to a callable object, it’s a natural implementation technique for\na function that must remember to call another function when called itself. For\nmore __call__ examples, also watch for the more advanced decorators and\nmetaclasses of Chapters 39 and 40.\nComparisons: __lt__, __gt__, and Others\nOur next batch of overloading methods supports object comparisons. As\nsuggested in Table 30-1, classes can define methods to catch all six comparison\noperators: <, >, <=, >=, ==, and !=. These methods are generally straightforward\nto use, but keep the following qualifications in mind:\nUnlike the __add__/__radd__ pairings discussed earlier, there are no",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1173,
      "chapter": null,
      "content": "right-side variants of comparison methods. Instead, reflective methods\nare used when only one operand supports comparison (e.g., __lt__ and\n__gt__ are each other’s reflection).\nThere are no implicit relationships among the comparison operators.\nThe truth of == does not imply that != is false, for example, so both\n__eq__ and __ne__ should be defined to ensure that both operators\nbehave correctly.\nWe don’t have space for an in-depth exploration of comparison methods, but as a\nquick introduction, consider the following class and tests:\n>>> class Vetter:\n        data = 'hack'\n        def __gt__(self, other):\n            print(f'gt: {self=} {other=}')\n            return self.data > other\n        def __lt__(self, other):\n            print(f'lt: {self=} {other=}')\n            return self.data < other\n \n>>> X = Vetter()\n>>> X > 'code', X < 'code'\ngt: self=<__main__.Vetter object at 0x10898f650> other='code'\nlt: self=<__main__.Vetter object at 0x10898f650> other='code'\n(True, False)\n>>> 'code' < X, 'code' > X\ngt: self=<__main__.Vetter object at 0x10898f650> other='code'\nlt: self=<__main__.Vetter object at 0x10898f650> other='code'\n(True, False)\n>>> X < X\nlt: self=<__main__.Vetter object at 0x10898f650> other=<__main__.Vetter …etc…>\ngt: self=<__main__.Vetter object at 0x10898f650> other='hack'\nFalse\nWhen run, the class’s methods intercept and implement comparison expressions\nas noted by their trace outputs. Importantly, __lt__ is also used for sorts—both\nthe list method and built-in function:\n>>> class Order:\n        def __init__(self, data):\n            self.data = data",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1174,
      "chapter": null,
      "content": "def __lt__(self, other):\n            return self.data < other.data\n        def __repr__(self):\n            return f'Order({self.data})'\n \n>>> sorted(Order(i) for i in [3, 1, 4, 2])\n[Order(1), Order(2), Order(3), Order(4)]\nConsult Python’s manuals for more details in this category. As you’ll find there,\nthe __eq__ method run for value equality is coupled with the __hash__ method\nrun for as-key and set-object roles (in ways that should send most readers\nscreaming into the night); and comparison methods can also return\nNotImplemented for unsupported arguments (but again, not\nNotImplementedError, an exception with a similar name but very different\nroles).\nBoolean Tests: __bool__ and __len__\nThe next set of methods is truly useful (pun intended). As you’ve learned, every\nobject is inherently true or false in Python. When you code classes, you can\ndefine what this means for your objects by coding methods that give the True or\nFalse values of instances on request.\nIn Boolean contexts, Python first tries __bool__ to obtain a direct Boolean\nvalue; if that method is missing, Python tries __len__ to infer a truth value from\nthe object’s length—a length of zero means empty, which is always false. The\nfirst of these generally uses object state or other information to produce a\nBoolean result:\n>>> class Truth:\n       def __bool__(self): return True\n>>> X = Truth()\n>>> if X: print('yes!')\nyes!\n>>> class Truth:\n       def __bool__(self): return False\n>>> X = Truth()",
      "content_length": 1474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1175,
      "chapter": null,
      "content": ">>> bool(X)\nFalse\nIf this method is missing, Python falls back on length because a nonempty object\nis considered true. That is, a nonzero length is taken to mean the object is true,\nand a zero length means it is false—just as for built-in objects:\n>>> class Truth:\n       def __len__(self): return 0            # Empty means false too\n>>> X = Truth()\n>>> if not X: print('no!')\nno!\nIf both methods are present Python prefers __bool__ over __len__, because it is\nmore specific:\n>>> class Truth:\n       def __bool__(self): return True        # Preferred over length\n       def __len__(self): return 0            # Object length: fallback\n>>> if Truth(): print('yes!')\nyes!\nIf neither truth method is defined, the object is vacuously considered true\n(though any existential implications of this are strictly out of scope here):\n>>> class Truth:\n        pass\n>>> X = Truth()\n>>> bool(X)\nTrue\nBut now that we’ve managed to cross over into the realm of philosophy, let’s\nmove on to look at one last overloading context: object demise.\nObject Destruction: __del__",
      "content_length": 1056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1176,
      "chapter": null,
      "content": "It’s time to close out this chapter—and learn how to do the same for our class\nobjects. You’ve seen how the __init__ constructor is called whenever an\ninstance is generated (and noted how __new__ is run first to make the object). Its\ncounterpart, the destructor (less commonly known as finalizer) method __del__,\nis run automatically when an instance’s space is being reclaimed (i.e., at\ngarbage-collection time):\n>>> class Life:\n        def __init__(self, name):\n            print('Hello', name)\n            self.name = name\n        def live(self):\n            print(self.name + '...')\n        def __del__(self):\n            print('Goodbye', self.name)\n>>> pat = Life('Pat')\nHello Pat\n>>> pat.live()\nPat...\n>>> pat = 'end'\nGoodbye Pat\nHere, when pat is assigned a string at the end, we lose the last reference to the\nLife instance and so trigger its destructor method. This works, and it may be\nuseful for implementing some cleanup activities, such as terminating a server\nconnection. However, destructors are not as commonly used in Python as in\nsome OOP languages, for a number of reasons that the next section describes.\nDestructor Usage Notes\nThe destructor method works as documented, but it has some well-known\ncaveats and a few outright dark corners that make it somewhat rare to see in\nPython code:\nNeed\nFor one thing, destructors may not be as useful in Python as they are in some\nother OOP languages. Because Python automatically reclaims all memory\nspace held by an instance when the instance is reclaimed, destructors are not\nnecessary for space management. In the current CPython implementation of",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1177,
      "chapter": null,
      "content": "Python, you also don’t need to close file objects held by the instance in\ndestructors because they are automatically closed when reclaimed. As\nmentioned in Chapter 9, though, it’s still sometimes best to run file close\nmethods anyhow because this autoclose behavior may vary in alternative\nPython implementations.\nPredictability\nFor another, you cannot always easily predict when an instance will be\nreclaimed. In some cases, there may be lingering references to your objects\nin system tables that prevent destructors from running when your program\nexpects them to be triggered. Python also does not guarantee that destructor\nmethods will be called for objects that still exist when the interpreter exits.\nExceptions\nIn fact, __del__ can be tricky to use for even more subtle reasons.\nExceptions raised within it, for example, simply print a warning message to\nsys.stderr (the standard error stream) rather than triggering an exception\nevent, because of the unpredictable context under which it is run by the\ngarbage collector—it’s not always possible to know where such an exception\nshould be delivered.\nCycles\nIn addition, cyclic (a.k.a. circular) references among objects may prevent\ngarbage collection from happening when you expect it to. An optional cycle\ndetector, enabled by default, can automatically collect such objects\neventually (including objects with __del__ methods, as of Python 3.4).\nSince this is relatively obscure, we’ll ignore further details here; see Python’s\nstandard manuals’ coverage of both __del__ and the gc garbage collector\nmodule for more information.\nBecause of these downsides, it’s often better to code termination activities in an\nexplicitly called method (e.g., shutdown). As described in the next part of the\nbook, the try/finally statement also supports termination actions, as does the\nwith statement for objects that support its context-manager model.",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1178,
      "chapter": null,
      "content": "Chapter Summary\nThat’s as many overloading examples as we have space for here. Most of the\nother operator-overloading methods work similarly to the ones we’ve explored,\nand all are just hooks for intercepting built-in type operations. Some overloading\nmethods, for example, have unique argument lists or return values, but the\ngeneral usage pattern is the same. You’ll see a few others in action later in the\nbook:\nChapter 34 uses __enter__ and __exit__ in with statement context\nmanagers.\nChapter 38 uses the __get__ and __set__ class descriptor fetch/set\nmethods.\nChapter 40 uses the __new__ object creation method in the context of\nmetaclasses.\nIn addition, some of the methods we’ve studied here, such as __call__ and\n__str__, will be employed by later examples in this book. For complete\ncoverage, though, it must defer to other documentation sources—see Python’s\nlanguage manual or other reference resources for details on additional\noverloading methods.\nIn the next chapter, we leave the realm of class mechanics behind to explore\ndesign—the ways that classes are commonly used and combined to optimize\ncode reuse. After that, we’ll survey a gumbo of advanced class topics and move\non to exceptions, the last core subject of this book. Before you read on, though,\ntake a moment to work through the chapter quiz below to review the concepts\nwe’ve covered here.\nTest Your Knowledge: Quiz\n1. What two operator-overloading methods can you use to support\niteration in your classes?",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1179,
      "chapter": null,
      "content": "2. What two operator-overloading methods handle printing, and in what\ncontexts?\n3. How can you intercept slice operations in a class?\n4. How can you catch in-place addition in a class?\n5. When should you provide operator overloading?\nTest Your Knowledge: Answers\n1. Classes can support iteration by defining (or inheriting) __getitem__\nor __iter__. In all iteration tools, Python tries to use __iter__ first,\nwhich returns an object that supports the iteration protocol with a\n__next__ method: if no __iter__ is found by inheritance search,\nPython falls back on the __getitem__ indexing method, which is called\nrepeatedly, with successively higher indexes. If used, the yield\nstatement can create the __next__ method automatically.\n2. The __str__ and __repr__ methods implement object print displays.\nThe former is called by the print and str built-in functions; the latter\nis called by print and str if there is no __str__, and always by the\nrepr built-in, interactive echoes, and nested appearances. That is,\n__repr__ is used everywhere, except by print and str when a\n__str__ is defined. A __str__ is usually used for user-friendly\ndisplays; __repr__ gives extra details or the object’s as-code form.\nString formatting runs these methods too.\n3. Slicing is caught by the __getitem__ indexing method: it is called with\na slices object instead of a simple integer index, and slice objects may\nbe passed on or inspected as needed.\n4. In-place addition tries __iadd__ first, and __add__ with an assignment\nsecond. The same policy holds true for all binary operators. The\n__radd__ method is also available for right-side addition.\n5. When a class naturally matches, or needs to emulate, a built-in type’s",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1180,
      "chapter": null,
      "content": "interfaces. For example, collections might imitate sequence or mapping\ninterfaces, and callables might be coded for use with an API that\nexpects a function. You generally shouldn’t implement expression\noperators if they don’t naturally map to your objects naturally and\nlogically, though—use normally named methods instead.",
      "content_length": 323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1181,
      "chapter": null,
      "content": "Chapter 31. Designing with\nClasses\nSo far in this part of the book, we’ve concentrated on using Python’s OOP tool,\nthe class. But OOP is also about design—that is, how to use classes to model\nuseful objects. Toward this end, this chapter codes common OOP design patterns\nin Python, such as inheritance, composition, delegation, and factories. Along the\nway, we’ll also investigate some design-focused class concepts, such as\npseudoprivate attributes, multiple inheritance, and bound methods. Because\nmultiple inheritance is dependent on the MRO search order, we’ll finally explore\nthat here too.\nOne note up front: some of the design terms mentioned here require more\ncoverage than this book can provide. If this material sparks your curiosity, you\nmay want to consider exploring a text on OOP design or design patterns as a\nnext step. As you’ll see, the good news is that Python makes many traditional\ndesign patterns almost trivial.\nPython and OOP\nLet’s begin with a review—Python’s implementation of OOP can be summarized\nby three ideas:\nInheritance\nInheritance is based on attribute lookup in Python (in X.name expressions).\nPolymorphism\nIn X.method, the meaning of method depends on the type (class) of subject\nobject X.\nEncapsulation",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1182,
      "chapter": null,
      "content": "Methods and operators implement behavior, though data hiding is a\nconvention by default.\nBy now, you should have a good feel for what basic inheritance is all about in\nPython, so we’ll take it as a given here. As you’ve learned, it’s the mechanism\nbehind flexible code customization.\nWe’ve also talked about Python’s polymorphism a few times already. It flows\nfrom Python’s lack of type declarations (per Chapter 6, the optional, unused, and\nparadoxical “type hinting” doesn’t qualify). Because attributes are always\nresolved at runtime, objects that implement the same interfaces are automatically\ninterchangeable; clients don’t need to know what sorts of objects are\nimplementing the methods they call.\nNewer here, encapsulation in Python means packaging—that is, hiding\nimplementation details behind an object’s interface. It does not mean enforced\nprivacy, though that can be partly implemented with code, as you’ll see in\nChapter 39. Encapsulation is available and useful in Python nonetheless: it\nallows the implementation of an object’s interface to be changed without\nimpacting the users of that object.\nPolymorphism Means Interfaces, Not Call Signatures\nSome OOP languages also define polymorphism to mean overloading functions\nbased on the type signatures of their arguments—the number passed and/or their\ntypes. Because there are no real type declarations in Python, this concept doesn’t\napply; as we’ve seen, polymorphism in Python is based on object interfaces, not\ntypes.\nIf you’re pining for your C++ days, you can try to overload methods by their\nargument lists, like this:\nclass C:\n    def meth(self, x):\n        …\n    def meth(self, x, y, z):\n        …",
      "content_length": 1670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1183,
      "chapter": null,
      "content": "Such code will run without error, but because the def simply assigns an object to\na name in the class’s scope, the last definition of the method function is the only\none that will be retained. Put another way, it’s just as if you say X = 1 and then X\n= 2; at the end, X will be 2. Hence, there can be only one definition of a method\nname. Per Chapter 29, this includes __init__ constructors, which are only\nspecial because of when they are run.\nIf call-signature dispatch is truly required, you can always code type-based\nselections using the type-testing ideas we met in Chapters 4 and 9, or the\nargument-list tools introduced in Chapter 18:\nclass C:\n    def meth(self, *args):\n        if len(args) == 1:              # Branch on number arguments\n            …\n        elif type(arg[0]) == int:       # Branch on argument types (or isinstance())\n            …\nYou normally shouldn’t do this, though—it’s not the “Python way.” As explained\nin Chapter 16, you should write your code to expect only an object interface, not\na specific object type. That way, it will be useful for a broader category of types\nand applications, both now and in the future:\nclass C:\n    def meth(self, x):\n        x.operation()                   # Assume x does the right thing\nIt’s also generally considered better to use distinct method names for distinct\noperations rather than relying on call signatures (no matter what language you\ncode in).\nBut enough review. Although Python’s object model is straightforward, much of\nthe art in OOP is in the way we combine classes to achieve a program’s goals.\nThe next section begins a tour of some of the ways larger programs use classes\nto their advantage.\nOOP and Inheritance: “Is-a” Relationships\nWe’ve explored the mechanics of inheritance in depth already; let’s turn to an",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1184,
      "chapter": null,
      "content": "example of how it can be used to model real-world relationships. From a\nprogrammer’s point of view, inheritance is kicked off by attribute qualifications,\nwhich trigger searches for names in instances, their classes, and then any\nsuperclasses. From a designer’s point of view, inheritance is a way to specify set\nmembership: a class defines a set of properties that may be inherited and\ncustomized by more specific sets (i.e., subclasses).\nTo illustrate, let’s put that pizza-making robot we talked about at the start of this\npart of the book to work. Suppose we’ve decided to explore alternative career\npaths and open a pizza restaurant (not bad, as career paths go). One of the first\nthings we’ll need to do is hire employees to serve customers, prepare the food,\nand so on. Being engineers at heart, we’ve decided to build a robot to make the\npizzas, but being politically and cybernetically correct, we’ve also decided to\nmake our robot a full-fledged employee with a salary.\nOur pizza shop team can be simulated by the four classes in Example 31-1,\nemployees.py. The most general class, Employee, is a takeoff on Chapter 28’s\ndemo. It provides common behavior such as bumping up salaries (giveRaise)\nand printing (__repr__). There are two kinds of employees, and so there are two\nsubclasses of Employee—Chef and Server. Both override the inherited work\nmethod to print more specific messages. Finally, our pizza robot is modeled by\nan even more specific class—PizzaRobot is a kind of Chef, which is a kind of\nEmployee. In OOP terms, we call these relationships “is-a” links: a robot is a\nchef, which is an employee.\nExample 31-1. employees.py\nclass Employee:\n   def __init__(self, name, salary=0):\n       self.name   = name\n       self.salary = salary\n   def giveRaise(self, percent):\n       self.salary += self.salary * percent\n   def work(self):\n       print(self.name, 'does stuff')\n   def __repr__(self):\n       return (f'<{self.__class__.__name__}: '\n               f'name=\"{self.name}\", salary={self.salary:,.2f}>')\nclass Chef(Employee):\n   def __init__(self, name):\n       Employee.__init__(self, name, 50000)",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1185,
      "chapter": null,
      "content": "def work(self):\n       print(self.name, 'makes food')\nclass Server(Employee):\n   def __init__(self, name):\n       Employee.__init__(self, name, 40000)\n   def work(self):\n       print(self.name, 'interfaces with customer')\nclass PizzaRobot(Chef):\n   def __init__(self, name):\n       Chef.__init__(self, name)\n   def work(self):\n       print(self.name, 'makes pizza')\nif __name__ == '__main__':\n   pat = PizzaRobot('pat')       # Make a robot named pat\n   print(pat)                    # Run inherited __repr__\n   pat.work()                    # Run type-specific action\n   pat.giveRaise(0.20)           # Give pat a 20% raise\n   print(pat); print()\n   for klass in Employee, Chef, Server, PizzaRobot:\n       object = klass(klass.__name__)\n       object.work()\nWhen we run the self-test code included in this module, we create a pizza-\nmaking robot named pat, which inherits names from three classes: PizzaRobot,\nChef, and Employee. For instance, printing and giving a raise to pat runs the\nEmployee class’s __repr__ and giveRaise methods two levels up,\nrespectively, simply because that’s where the inheritance search finds these\nmethods:\n$ python3 employees.py\n<PizzaRobot: name=\"pat\", salary=50,000.00>\npat makes pizza\n<PizzaRobot: name=\"pat\", salary=60,000.00>\nEmployee does stuff\nChef makes food\nServer interfaces with customer\nPizzaRobot makes pizza\nIn a class hierarchy like this, you can usually make instances of any of the\nclasses, not just the ones at the bottom. For instance, the for loop in this",
      "content_length": 1507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1186,
      "chapter": null,
      "content": "module’s self-test code creates instances of all four classes; each responds\ndifferently when asked to work because the work method is different in each.\npat the robot, for example, gets work from the most specific (i.e., lowest)\nPizzaRobot class.\nOf course, these classes just simulate real-world objects; work prints a message\nfor the time being, but it could be expanded to do real work later (see Python’s\ninterfaces to devices such as serial ports, Arduino boards, and the Raspberry Pi if\nyou’re taking this section much too literally!).\nNOTE\nThe super re-reminder: Notice how Example 31-1 uses explicit class calls to run superclass\nconstructors; this is how PizzaRobot salaries are set. Per the sidebar “The super Alternative”,\nthese could also use the super().__init__(…) form explored in the next chapter. As you’ll\nfind, this call avoids having to pass self along in single-inheritance trees like those here, but\nit’s much more complex in the multiple-inheritance contexts you’ll meet ahead. If you’re\nanxious to see what this looks like now, see the alternative employees-super.py in the examples\npackage.\nOOP and Composition: “Has-a” Relationships\nThe notion of composition was introduced in Chapters 26 and 28. From a\nprogrammer’s perspective, composition involves embedding other objects in a\ncontainer object and activating them to implement container methods. To a\ndesigner, composition is another way to represent relationships in a problem\ndomain. But, rather than set membership, composition has to do with\ncomponents—parts of a whole.\nComposition also reflects the relationships between parts, called “has-a”\nrelationships. Some OOP design texts refer to composition as aggregation or\ndistinguish between the two terms by using aggregation to describe a weaker\ndependency between container and contained. In this text, a “composition”\nsimply refers to a collection of embedded objects. The composite class generally\nprovides an interface all its own and implements it by directing the embedded\nobjects.\nNow that we’ve implemented our employees, let’s put them in the pizza shop",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1187,
      "chapter": null,
      "content": "and let them get busy. Our pizza shop is a composite object: it has an oven, and it\nhas employees like servers and chefs. When a customer enters and places an\norder, the components of the shop spring into action—the server takes the order,\nthe chef makes the pizza, and so on. Example 31-2—file pizzashop.py—\nsimulates all the objects and relationships in this scenario.\nExample 31-2. pizzashop.py\nfrom employees import PizzaRobot, Server    # From Example 31-1\nclass Customer:\n   def __init__(self, name):\n       self.name = name\n   def order(self, server):\n       print(self.name, 'orders from', server)\n   def pay(self, server):\n       print(self.name, 'pays for item to', server)\nclass Oven:\n   def bake(self):\n       print('oven bakes')\nclass PizzaShop:\n   def __init__(self):\n       self.server = Server('Jan')         # Embed other objects\n       self.chef   = PizzaRobot('Pat')     # A robot named Pat\n       self.oven   = Oven()\n   def order(self, name):\n       customer = Customer(name)           # Activate other objects\n       customer.order(self.server)         # Customer orders from server\n       self.chef.work()\n       self.oven.bake()\n       customer.pay(self.server)\nif __name__ == '__main__':\n   scene = PizzaShop()                     # Make the composite\n   scene.order('Sue')                      # Simulate Sue's order\n   print('...')\n   scene.order('Bob')                      # Simulate Bob's order\nThe PizzaShop class is a container and controller; its constructor makes and\nembeds instances of the employee classes we wrote in the prior section, as well\nas an Oven class defined here. When this module’s self-test code calls the\nPizzaShop’s order method, the embedded objects are asked to carry out their\nactions in turn.",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1188,
      "chapter": null,
      "content": "Notice that we make a new Customer object for each order, and we pass on the\nembedded Server object to Customer methods; customers come and go, but the\nserver is part of the pizza shop composite. Also notice that employees are still\ninvolved in an inheritance relationship; composition and inheritance are\ncomplementary tools.\nWhen we run this module, our pizza shop handles two orders—one from Sue\nand then one from Bob (overlapping orders with the async coroutines of\nChapter 20 is explicitly out of scope here):\n$ python3 pizzashop.py\nSue orders from <Server: name=\"Jan\", salary=40,000.00>\nPat makes pizza\noven bakes\nSue pays for item to <Server: name=\"Jan\", salary=40,000.00>\n...\nBob orders from <Server: name=\"Jan\", salary=40,000.00>\nPat makes pizza\noven bakes\nBob pays for item to <Server: name=\"Jan\", salary=40,000.00>\nAgain, this is mostly just a toy simulation, but the objects and interactions are\nrepresentative of composites at work. As a rule of thumb, classes can represent\njust about any objects and relationships you can express in a sentence; just\nreplace nouns with classes (e.g., Oven) and verbs with methods (e.g., bake), and\nyou’ll have a first cut at a design.\nStream Processors Revisited\nFor a composition example that may be a bit more tangible than pizza-making\nrobots, recall the generic data-stream processor function we partially coded in\nthe introduction to OOP in Chapter 26, repeated here for ease:\ndef processor(reader, converter, writer):\n    while True:\n        data = reader.read()\n        if not data: break\n        data = converter(data)\n        writer.write(data)\nRather than using a simple function here, we might code this as a class that uses",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1189,
      "chapter": null,
      "content": "composition to do its work in order to provide more structure and support\ninheritance. Example 31-3, file streams.py, demonstrates one way to code the\nclass (it also mutates one method name, readline, because we’re actually going\nto run this code here).\nExample 31-3. streams.py\nclass Processor:\n   def __init__(self, reader, writer):\n       self.reader = reader\n       self.writer = writer\n   def process(self):\n       while True:\n           data = self.reader.readline()\n           if not data: break\n           data = self.converter(data)\n           self.writer.write(data)\n   def converter(self, data):\n       assert False, 'converter must be defined'       # Or raise exception\nThis class defines a converter method that it expects subclasses to fill in; it’s an\nexample of the abstract superclass model we outlined in Chapter 29 (again,\nmore on assert in Part VII—it simply raises an exception if its test is false).\nCoded this way, reader and writer objects are embedded within the class\ninstance (composition), and we supply the conversion logic in a subclass rather\nthan passing in a converter function (inheritance). The file in Example 31-4,\nconverters.py, shows how.\nExample 31-4. converters.py\nfrom streams import Processor\nclass Uppercase(Processor):\n   def converter(self, data):\n       return data.upper()\nif __name__ == '__main__':\n   import sys\n   obj = Uppercase(open('trihack.txt'), sys.stdout)\n   obj.process()\nHere, the Uppercase class inherits the stream-processing loop logic of process\n(and anything else that may be coded in its superclasses). It needs to define only",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1190,
      "chapter": null,
      "content": "what is unique about it—the data conversion logic. When this file is run, it\nmakes and runs an instance that reads from the file trihack.txt in the current\ndirectory and writes the uppercase equivalent of that file to the stdout stream\n(which usually means the window you’re working in):\n$ cat trihack.txt        # Use \"type\" on Windows\nhack\nHack\nHACK!\n$ python3 converters.py\nHACK\nHACK\nHACK!\nTo process different sorts of streams, pass in different sorts of objects to the class\nconstruction call. Here, we use an output file instead of a stream:\n$ python3\n>>> import converters\n>>> scan = converters.Uppercase(open('trihack.txt'), open('trihackup.txt', 'w'))\n>>> scan.process()\n$ cat trihackup.txt\nHACK\nHACK\nHACK!\nBut, as suggested earlier, we could also pass in arbitrary objects coded as classes\nthat define the required input and output method interfaces. Here’s a simple\nexample that passes in a writer class that wraps up the text inside HTML tags—\nlines are read from a file, run through uppercase conversion, and then printed\nwith HTML tags:\n$ python3\n>>> from converters import Uppercase\n>>> class HTMLize:\n         def write(self, line):\n            print(f'<PRE>{line.rstrip()}</PRE>')\n>>> Uppercase(open('trihack.txt'), HTMLize()).process()\n<PRE>HACK</PRE>\n<PRE>HACK</PRE>",
      "content_length": 1285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1191,
      "chapter": null,
      "content": "<PRE>HACK!</PRE>\nIf you trace through this example’s control flow, you’ll see that we get both\nuppercase conversion (by inheritance) and HTML formatting (by composition),\neven though the core processing logic in the original Processor superclass\nknows nothing about either step. The processing code only cares that writers\nhave a write method and that a method named converter is defined; it doesn’t\ncare what those methods do when they are called. Such polymorphism and\nencapsulation of logic are behind much of the power of classes in Python.\nAs is, the Processor superclass only provides a file-scanning loop. In more\nrealistic work, we might extend it to support additional programming tools for its\nsubclasses and, in the process, turn it into a full-blown application framework.\nCoding such a tool once in a superclass enables you to reuse it in all your\nprograms. Even in this simple example, because so much is packaged and\ninherited with classes, all we had to code was the HTML formatting step; the rest\nwas free.\nFor another example of composition at work, see exercise 9 in “Test Your\nKnowledge: Part VI Exercises” and its solution in Appendix B; it’s similar to the\npizza shop example. We’ve focused on inheritance in this book because that is\nthe main tool that the Python language itself provides for OOP. But, in practice,\ncomposition may be used as much as inheritance as a way to structure classes,\nespecially in larger systems. As we’ve seen, inheritance and composition are\noften complementary (and sometimes alternative) techniques. Because\ncomposition is a design issue outside the scope of the Python language and this\nbook, though, we’ll defer to other resources for more on this topic.\nWHY YOU WILL CARE: CLASSES AND PERSISTENCE\nWe’ve explored Python’s pickle and shelve object persistence earlier in\nthis part of the book because it works especially well with class instances. In\nfact, these tools are often compelling enough to motivate the use of classes\nin general—by pickling or shelving a class instance, we store both data and\nlogic.\nFor example, besides allowing us to simulate real-world interactions, the",
      "content_length": 2139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1192,
      "chapter": null,
      "content": "pizza shop classes developed in this chapter could also be used as the basis\nof a restaurant database. Pickling instances of such classes to a file makes\nthem persistent across Python program executions:\n>>> from pizzashop import PizzaShop\n>>> shop = PizzaShop()\n>>> shop.chef\n<PizzaRobot: name=\"Pat\", salary=50,000.00>\n>>> import pickle\n>>> pickle.dump(shop, open('shopfile.pkl', 'wb'))\nThis stores an entire composite shop object in a file all at once. To bring it\nback later in another session or program, a single step suffices as well.\nObjects restored this way retain both state and behavior:\n>>> import pickle\n>>> shop = pickle.load(open('shopfile.pkl', 'rb'))\n>>> shop.chef\n<PizzaRobot: name=\"Pat\", salary=50,000.00>\n>>> shop.order('sue')\nsue orders from <Server: name=\"Jan\", salary=40,000.00>\nPat makes pizza\noven bakes\nsue pays for item to <Server: name=\"Jan\", salary=40,000.00>\nThis is just a prototype as is, but we might extend the shop to keep track of\ninventory, revenue, and so on—saving it to its file after changes would retain\nits updated state. See the standard-library manual and related coverage in\nChapters 9, 28, and 37 for more on pickles and shelves.\nOOP and Delegation: “Like-a” Relationships\nBesides inheritance and composition, object-oriented programmers often speak\nof delegation, which usually implies controller objects that embed other objects\nto which they pass off operation requests. The controllers can take care of\nadministrative activities, such as logging or validating accesses, adding extra\nsteps to interface components, or monitoring active instances.\nIn a sense, delegation is a special form of composition, with a single embedded",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1193,
      "chapter": null,
      "content": "object managed by a proxy (sometimes called a wrapper) class that retains most\nor all of the embedded object’s interface. The notion of proxies sometimes\napplies to other mechanisms, too, such as function calls; in delegation, we’re\nconcerned with proxies for all of an object’s behavior, including method calls\nand other operations.\nThis concept was introduced by example in Chapter 28, and in Python is often\nimplemented with the __getattr__ method hook we studied in Chapter 30.\nBecause this operator-overloading method intercepts accesses to nonexistent\nattributes, a wrapper class can use __getattr__ to route arbitrary accesses to a\nwrapped object. Because this method allows attribute requests to be routed\ngenerically, the wrapper class retains the interface of the wrapped object and\nmay add additional operations of its own.\nBy way of review, consider the file trace.py in Example 31-5.\nExample 31-5. trace.py\nclass Wrapper:\n   def __init__(self, object):\n       self.wrapped = object                    # Save object\n   def __getattr__(self, attrname):\n       print('Trace: ' + attrname)              # Trace fetch\n       return getattr(self.wrapped, attrname)   # Delegate fetch\nRecall from Chapter 30 that __getattr__ gets the attribute name as a string.\nThis code makes use of the getattr built-in function to fetch an attribute from\nthe wrapped object by name string—getattr(X,N) is like X.N, except that N is\nan expression that evaluates to a string at runtime, not a variable. In fact,\ngetattr(X,N) is similar to X.__dict__[N], but the former also performs an\ninheritance search, like X.N, while the latter does not (see Chapters 23 and 29 for\nmore on the __dict__ attribute).\nYou can use the approach of this module’s wrapper class to manage access to\nany object with attributes—lists, dictionaries, and even classes and instances.\nHere, the Wrapper class simply prints a trace message on each attribute access\nand delegates the attribute request to the embedded wrapped object:\n>>> from trace import Wrapper\n>>> x = Wrapper([1, 2, 3])                       # Wrap a list\n>>> x.append(4)                                  # Delegate to list method",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1194,
      "chapter": null,
      "content": "Trace: append\n>>> x.wrapped                                    # Print my member\n[1, 2, 3, 4]\n>>> x = Wrapper({'a': 1, 'b': 2})                # Wrap a dictionary\n>>> list(x.keys())                               # Delegate to dictionary method\nTrace: keys\n['a', 'b']\nThe net effect is to augment the entire interface of the wrapped object with\nadditional code in the Wrapper class. We can use this to log our method calls,\nroute method calls to extra or custom logic, adapt a class to a new interface, and\nso on.\nIn the next chapter, we’ll revive the notions of wrapped objects and delegated\noperations as one way to extend built-in types. If you are interested in the\ndelegation design pattern, also watch for the discussions in Chapters 32 and 39\nof function decorators, a strongly related concept designed to augment a specific\nfunction or method call rather than the entire interface of an object, as well as\nclass decorators, which serve as a way to automatically add such delegation-\nbased wrappers to all instances of a class.\nNOTE\nDelegation reminder: As noted in the sidebar “Delegating Built-ins—or Not”, general proxies\nlike the Wrapper example here cannot directly intercept and delegate calls to operator-\noverloading methods run by built-in operations. The list’s __add__, for instance, is not caught\nand fails:\n>>> [1, 2, 3] + [4, 5]\n[1, 2, 3, 4, 5]\n>>> [1, 2, 3].__add__([4, 5])\n[1, 2, 3, 4, 5]\n>>> Wrapper([1, 2, 3]) + [4, 5]\nTypeError: unsupported operand type(s) for +: 'Wrapper' and 'list'\nExplicit-name attribute fetches are routed to __getattr__, but built-in operations differ in\nways that impact some delegation-based tools. We’ll return to this issue and see it live in\nChapters 38 and 39, in the context of managed attributes and decorators. For now, keep in\nmind that delegation proxies may need to redefine operator-overloading methods by code,\ntools, or superclasses if those methods are used by embedded objects and should be routed to\nthem.",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1195,
      "chapter": null,
      "content": "Pseudoprivate Class Attributes\nBesides larger structuring goals, class designs often must address name usage\ntoo. In Chapter 28’s case study, for example, we noted that methods defined\nwithin a general tool class might be modified by subclasses if exposed, and\nnoted the trade-offs of this policy—while it supports method customization and\ndirect calls, it’s also open to accidental replacements.\nIn Part V, we learned that every name assigned at the top level of a module file\nis exported. By default, the same holds for classes—data hiding is a convention,\nand clients may fetch or change attributes in any class or instance to which they\nhave a reference. All attributes are all “public” and “virtual,” in C++ terms:\nthey’re accessible everywhere and are looked up dynamically at runtime. In fact,\nit’s even possible to change or delete a class’s method at runtime, though this is\nrarely done in practical programs. As a scripting language, Python is about\nenabling, not restricting.\nAll that being said, Python today does support the notion of name “mangling”\n(i.e., expansion) to associate some names with their classes. Mangled names are\nsometimes misleadingly called “private attributes,” but really this is just a way to\nlocalize a name to the class that created it—name mangling does not prevent\naccess by code outside the class. This feature is mostly intended to avoid\nnamespace collisions in instances, not to restrict access to names in general;\nmangled names are therefore better called “pseudoprivate” than “private.”\nPseudoprivate names are an advanced and entirely optional feature, and you\nprobably won’t find them very useful until you start writing general tools or\nlarger class hierarchies for use in multiprogrammer projects. In fact, they are not\nalways used even when they probably should be—more commonly, Python\nprogrammers code internal names with a single underscore (e.g., _X), which is\njust an informal convention to let you know that a name shouldn’t generally be\nchanged, but this means nothing to Python itself.\nBecause you may see this feature in other people’s code, though, you need to be\nsomewhat aware of it, even if you don’t use it yourself. And once you learn its\nadvantages and contexts of use, you may find this feature to be more useful in\nyour own code than some programmers realize.",
      "content_length": 2328,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1196,
      "chapter": null,
      "content": "Name Mangling Overview\nHere’s how name mangling works: within a class statement only, any names\nthat start with two underscores but do not end with two underscores are\nautomatically expanded to include the name of the enclosing class at their front.\nFor instance, a name like __X within a class named Hack is changed to _Hack__X\nautomatically: the original name is prefixed with a single underscore and the\nenclosing class’s name. Because the modified name contains the name of the\nenclosing class, it’s generally unique; it won’t clash with similar names created\nby other classes in a hierarchy.\nName mangling happens only for names that appear inside a class statement’s\ncode and then only for names that begin with two leading underscores. It works\nfor every name preceded with double underscores, though—both class attributes\n(including method names) and instance attribute names assigned to self. For\nexample, in a class named Hack, a method named __meth is mangled to\n_Hack__meth, and an instance attribute reference self.__X is transformed to\nself._Hack__X.\nDespite the mangling, as long as the class uses the double-underscore version\neverywhere it refers to the name, all its references will still work. Because more\nthan one class may add attributes to an instance, though, this mangling helps\navoid clashes—but we need to move on to an example to see how.\nWhy Use Pseudoprivate Attributes?\nOne of the main issues that the pseudoprivate attribute feature is meant to\nalleviate has to do with the way instance attributes are stored. In Python,\ninstance attributes normally wind up in the single instance object at the bottom\nof the class tree and are shared by class-level method functions the instance is\npassed into. This is different from the C++ model, where each class gets its own\nspace for data members it defines.\nWithin a class’s method in Python, whenever a method assigns to a self\nattribute (e.g., self.attr = value), it changes or creates an attribute in the\ninstance (recall that inheritance searches happen only on reference, not on",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1197,
      "chapter": null,
      "content": "assignment). Because this is true even if multiple classes in a hierarchy assign to\nthe same attribute, collisions are possible.\nFor example, suppose that when a programmer codes a class, it is assumed that\nthe class owns the attribute name X in the instance. In this class’s methods, the\nname is set and later fetched:\nclass C1:\n    def meth1(self): self.X = 88         # I assume X is mine\n    def meth2(self): print(self.X)\nSuppose further that another programmer, working in isolation, makes the same\nassumption in another class:\nclass C2:\n    def metha(self): self.X = 99         # Me too\n    def methb(self): print(self.X)\nBoth of these classes work by themselves. The problem arises if the two classes\nare ever mixed together in the same class tree, using the multiple inheritance\nwe’ll expand on ahead:\nclass C3(C1, C2): ...\nI = C3()                                 # But only 1 X in I!\nNow, the value that each class gets back when it says self.X will depend on\nwhich class assigned it last. Because all assignments to self.X refer to the same\nsingle instance, there is only one X attribute—I.X—no matter how many classes\nuse that attribute name.\nThis isn’t a problem if it’s expected, and indeed, this is how classes normally\ncommunicate—the instance is shared memory. To guarantee that an attribute\nbelongs to the class that uses it, though, prefix the name with double underscores\neverywhere it is used in the class, as in Example 31-6, pseudoprivate.py.\nExample 31-6. pseudoprivate.py\nclass C1:\n   def meth1(self): self.__X = 88       # Now X is mine\n   def meth2(self): print(self.__X)     # Becomes _C1__X in I\nclass C2:",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1198,
      "chapter": null,
      "content": "def metha(self): self.__X = 99       # Me too\n   def methb(self): print(self.__X)     # Becomes _C2__X in I\nclass C3(C1, C2): pass\nI = C3()                                 # Two X names in I\nI.meth1(); I.metha()                     # Set names\nprint(I.__dict__)                        # Actual storage\nI.meth2(); I.methb()                     # Fetch names\nWhen thus prefixed, the X attributes will be expanded to include the names of\ntheir classes before being added to the instance. If you run a dir call on I or\ninspect its namespace dictionary after the attributes have been assigned, you’ll\nsee the expanded names, _C1__X and _C2__X, but not X. Because the expansion\nmakes the names more unique within the instance, the classes’ coders can be\nfairly safe in assuming that they truly own any names that they prefix with two\nunderscores:\n$ python3 pseudoprivate.py\n{'_C1__X': 88, '_C2__X': 99}\n88\n99\nThis trick can avoid potential name collisions in the instance, but note that it\ndoes not amount to true privacy. If you know the name of the enclosing class,\nyou can still access either of these attributes anywhere you have a reference to\nthe instance by using the fully expanded name (e.g., I._C1__X = 77).\nMoreover, names could still collide if unknowing programmers use the expanded\nnaming pattern explicitly (unlikely, but not impossible). On the other hand, this\nfeature makes it much less likely that you will accidentally step on a class’s\nnames.\nPseudoprivate attributes are also useful in larger frameworks or tools, both to\navoid introducing new method names that might accidentally hide definitions\nelsewhere in the class tree and to reduce the chance of internal methods being\nreplaced by names defined lower in the tree. If a method is intended for use only\nwithin a class that may be mixed into other classes, the double underscore prefix\nvirtually ensures that the method won’t interfere with other names in the tree,\nespecially in multiple-inheritance scenarios:",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1199,
      "chapter": null,
      "content": "class Super:\n    def method(self): …                    # A real application method\nclass Tool:\n    def __method(self): …                  # Becomes _Tool__method\n    def other(self): self.__method()       # Use my internal method\nclass Sub1(Tool, Super): …\n    def actions(self): self.method()       # Runs Super.method as expected\nclass Sub2(Tool):\n    def __init__(self): self.method = 99   # Doesn't break Tool.__method\n    def method(self): …                    # Ditto\nWe met multiple inheritance briefly in Chapter 26 and will explore it in more\ndetail later in this chapter. Recall that superclasses are searched according to\ntheir left-to-right order in class header lines. Here, this means Sub1 prefers\nTool attributes to those in Super. Although in this example we could force\nPython to pick the application class’s methods first by switching the order of the\nsuperclasses listed in the Sub1 class header, pseudoprivate attributes resolve the\nissue altogether. Pseudoprivate names also prevent subclasses from accidentally\nredefining the internal method’s names, as in Sub2.\nAgain, this feature tends to be of use primarily for larger multiprogrammer\nprojects and then only for selected names. Don’t be tempted to clutter your code\nunnecessarily; only use this feature for names that truly need to be controlled by\na single class. Although useful in some general class-based tools, for simpler\nprograms, it’s probably overkill.\nFor more examples that make use of the __X naming feature, see the lister.py\nmix-in classes introduced later in this chapter’s multiple inheritance section, as\nwell as the later class decorators mentioned in the following note.\nNOTE\nPrivate matters: If you’re interested in more binding forms of privacy, you may want to review\nthe emulation of private instance attributes coded in “Attribute Access: __getattr__ and\n__setattr__” in Chapter 30 and watch for the broader Private class decorator we’ll build with\ndelegation in Chapter 39. Although it’s possible to add name-access controls in Python classes,\nthis is rarely done in practice—even for large systems that solve real-world problems. Go\nfigure?",
      "content_length": 2143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1200,
      "chapter": null,
      "content": "Method Objects: Bound or Not\nMethods in general, and bound methods in particular, simplify the\nimplementation of many design goals in Python. We met bound methods briefly\nwhile studying __call__ in Chapter 30. The full story, which we’ll flesh out\nhere, turns out to be more general and flexible than you might expect.\nIn Chapter 19, we learned how functions can be processed as normal objects.\nMethods are a kind of object too, and can be used generically in much the same\nway as other objects—they can be assigned to names, passed to functions, stored\nin data structures, and so on—and like simple functions, modules, and classes,\nqualify as first-class objects. Because a class’s methods can be accessed from an\ninstance or a class, though, they come in two flavors:\nBound methods: when a method is referenced through an instance\nAccessing a function attribute of a class by qualifying an instance returns a\nbound method object. This object automatically packages the instance with\nthe function as a pair. When a bound method is later called, the instance is\nautomatically passed to the function’s self argument.\nPlain functions: when a method is referenced through a class\nAccessing a function attribute of a class by qualifying a class returns a plain\nfunction object. To call this function later, you must provide an instance\nobject explicitly to the self argument—if the function expects one.\nBoth kinds of methods are full-fledged objects; they can be transferred around a\nprogram at will, just like strings and numbers. Bound methods simply remember\nan instance, but plain functions do not. This is why we’ve had to pass in an\ninstance explicitly when calling superclass methods from subclass methods in\nprevious examples (including this chapter’s employees.py in Example 31-1);\ntechnically, such calls produce plain functions along the way.\nWhen calling a bound method object, though, Python provides the instance\nargument for us—the instance that was used to create the bound method object.\nThis means that bound method objects are usually interchangeable with simple",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1201,
      "chapter": null,
      "content": "function objects and makes them especially useful for interfaces originally\nwritten for functions.\nNOTE\nBlast from the past: Python once required all methods in a class to have an instance argument\nand termed methods fetched directly from a class unbound methods—implying that they\nrequired an instance argument when later called. Today, method functions in a class are really\njust plain functions; their only special quality is that they are bound to an instance when\nfetched through the instance. Unbound methods, however, are dead; long live plain functions!\nBound Methods in Action\nTo illustrate in simple terms, suppose we define the following class in the REPL\nof our choosing:\n>>> class Hack:\n        def doit(self, message):\n            print(message)\nNow, in normal operation, we make an instance and call its method in a single\nstep to print the passed-in argument:\n>>> inst = Hack()\n>>> inst.doit('hello')     # Typical method calls\nhello\nReally, though, a bound method object is generated along the way—just before\nthe method call’s parentheses. In fact, we can fetch a bound method without\nactually calling it. An object.name expression evaluates to an object as all\nexpressions do. In the following, it returns a bound method object that packages\nthe instance (inst) with the method function (Hack.doit). The net effect is that\nwe can assign this bound method pair to another name and then call it as though\nit were a simple self-less function:\n>>> inst = Hack()\n>>> meth = inst.doit       # Bound method object: instance+function\n>>> meth('hola')           # Same effect as inst.doit('...')\nhola",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1202,
      "chapter": null,
      "content": "In fact, if you know where to look, you can see the instance/function pair that the\nbound method packages:\n>>> meth\n<bound method Hack.doit of <__main__.Hack object at 0x108f2e8a0>>\n>>> meth.__self__\n<__main__.Hack object at 0x108f2e8a0>\n>>> meth.__func__\n<function Hack.doit at 0x108f37ce0>\nOn the other hand, if we qualify the class to get to doit, we get back a plain\nfunction object with no associated instance. To call this type of method, we\nusually must pass in an instance as the leftmost argument—there isn’t one in the\nexpression otherwise, and the method in this demo expects it:\n>>> inst = Hack()\n>>> meth = Hack.doit       # Plain function: requires self\n>>> meth(inst, 'ciao')     # Pass in instance (if the method expects one)\nciao\nThis time, though, the method is substantially more mundane because it’s a plain\nfunction:\n>>> meth\n<function Hack.doit at 0x108f37ce0>\nBy extension, the same rules apply within a class’s method if we reference self\nattributes that name functions in a class. A self.method expression is a bound\nmethod object because self is an instance object, though a class.method is a\nsimple function that may require a self when run:\n>>> class Hack2(Hack):\n        def doit2(self):\n            meth = self.doit        # Bound method object: instance+function\n            meth('bonjour')         # Looks like a simple function\n            meth = Hack.doit\n            meth(self, 'privit')    # Plain function: requires self\n \n>>> Hack2().doit2()\nbonjour\nprivit",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1203,
      "chapter": null,
      "content": "Most of the time, you call methods immediately after fetching them with\nattribute qualification, so you don’t notice the method objects generated along\nthe way. But if you start writing code that calls objects generically, you need to\nbe careful to treat nonbound methods specially—they normally require an\nexplicit instance object to be passed in.\nImplied by this model, classes can also code methods that do not require a self\ninstance, and are called normally when fetched from their class; apart from\ninheritance, this is similar to a function in a module file:\n>>> class Hack3:\n        def doit3(message):        # A self-less method (function)\n            print(message)\n \n>>> Hack3.doit3('guten tag')       # No \"self\" is passed\nguten tag\nThis falls down, though, if we try to call such a self-less method function\nthrough an instance: because the bound method made along the way packages\nand passes an instance, the call sends one too many arguments:\n>>> x = Hack3()\n>>> x.doit3('namaste')\nTypeError: Hack3.doit3() takes 1 positional argument but 2 were given\nIn other words, you can code self-less functions in a class, and can call them\nnormally through a class without an instance. To call them though an instance,\ntoo, though, you’ll have to stay tuned for the next chapter’s coverage of static\nand class methods—methods marked specially to suppress an automatic instance\nargument in all contexts. Also in the next chapter, you’ll see that the super built-\nin binds an instance with a method, too, but is substantially more convoluted\nthan the bound method pairs we’ve met here.\nFinally, to demo how flexible bound methods can be, the following stores four of\nthem in a list and calls them generically, with normal call expressions:\n>>> class Number:\n        def __init__(self, base):\n            self.base = base\n        def double(self):",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1204,
      "chapter": null,
      "content": "return self.base * 2\n        def triple(self):\n            return self.base * 3\n>>> x, y, z = Number(2), Number(3), Number(4)           # Class instance objects\n>>> x.double()                                          # Normal immediate calls\n4\n>>> acts = [x.double, y.double, z.double, z.triple]     # List of bound methods\n>>> for act in acts:                                    # Calls are deferred\n        print(act(), end=' ')                           # Call as though functions\n4 6 8 12\nIn the end, calls to act here run methods in the class to process instances—both\nsaved previously in bound methods.\nWHY YOU WILL CARE: BOUND METHOD CALLBACKS\nBecause bound methods automatically pair an instance with a class’s method\nfunction, you can use them anywhere a simple function is expected. One of\nthe most common places you’ll see this idea put to work is in code that\nregisters methods to handle event callbacks run by GUI interfaces, like\nPython’s tkinter standard-library module. As review, here’s the simple\ncase:\ndef handler():\n    …use globals or closure scopes for state…\n…\nwidget = Button(text='Tap', command=handler)\nWith tkinter, to register a handler for button click events, we usually pass a\ncallable object that takes no arguments to the command keyword argument.\nFunction names (and lambdas) work here, and so do class-level methods—\nthough they must be bound methods if they expect an instance when called:\nclass MyGui:\n    def handler(self):\n        …use self.attr for state…\n    def makewidgets(self):\n        b = Button(text='Tap', command=self.handler)",
      "content_length": 1575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1205,
      "chapter": null,
      "content": "Here, the event handler is self.handler—a bound method object that\nremembers both self and MyGui.handler. Because self will refer to the\noriginal instance when handler is later invoked on events, the method will\nhave access to instance attributes that can retain state between events, as well\nas class-level methods. With simple functions, state normally must be\nretained in global variables or enclosing function scopes (a.k.a. closures)\ninstead.\nSee also the discussion of __call__ operator overloading in Chapter 30 for\nanother way to make classes compatible with function-based APIs, as well\nas lambda in Chapter 19 for another tool often used in callback roles. As\nnoted in the former of these, you don’t need to wrap a bound method in a\nlambda unless extra arguments must be added to the call; because the bound\nmethod in the preceding example already defers the call (there are no\nparentheses to trigger one!), adding a lambda here would otherwise be\npointless.\nClasses Are Objects: Generic Object Factories\nSometimes, class-based designs require objects to be created in response to\nconditions that can’t be predicted when a program is written. The factory design\npattern allows such a deferred approach. Due in large part to Python’s flexibility,\nfactories can take multiple forms, some of which don’t seem special at all.\nBecause classes are also first-class objects (in the Chapter 19 sense), it’s easy to\npass them around a program, store them in data structures, and so on. You can\nalso pass classes to functions that generate arbitrary kinds of objects; such\nfunctions are sometimes called factories in OOP design circles. Factories can be\na major undertaking in a statically typed language such as C++ but are almost\ntrivial to implement in Python.\nFor example, the call syntax we studied in Chapter 18 can call any class with any\nnumber of positional or keyword constructor arguments in one step to generate\nany sort of instance. Example 31-7 demos the underlying code.\nExample 31-7. factory.py",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1206,
      "chapter": null,
      "content": "def factory(aClass, *pargs, **kargs):        # Varargs tuple, dict\n   return aClass(*pargs, **kargs)           # Call aClass\nclass Hack:\n   def doit(self, message):\n       print(message)\nclass Person:\n   def __init__(self, name, job=None):\n       self.name = name\n       self.job  = job\nobject1 = factory(Hack)                      # Make a Hack object\nobject2 = factory(Person, 'Sue', 'dev')      # Make a Person object\nobject3 = factory(Person, name='Bob')        # Ditto, with keywords and default\nThis code’s factory is passed a class object, along with zero or more arguments\nfor the class’s constructor. When called, it uses star syntax to collect and unpack\narguments and calls the class to return an instance. Really, factory can invoke\nany callable object, including functions, classes, and methods, but we’re using it\nfor classes here.\nThe rest of the example simply defines two classes and generates instances of\nboth by passing them to the factory function. And that’s the only factory\nfunction you may ever need to write in Python; it works for any class and any\nconstructor arguments. If you run this example live, your objects will look like\nthis:\n>>> from factory import *\n>>> object1.doit(99)\n99\n>>> object2.name, object2.job\n('Sue', 'dev')\n>>> object3.name, object3.job\n('Bob', None)\nBy now, you should know that everything is a first-class object in Python—\nincluding classes, which are usually just compiler input in languages like C++.\nIt’s natural to pass them around this way. As mentioned at the start of this part of\nthe book, though, only objects derived from classes do full OOP in Python.\nWhy Factories?",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1207,
      "chapter": null,
      "content": "So, what good is the factory function (besides providing an excuse to illustrate\nfirst-class class objects in this book)? Unfortunately, it’s difficult to show\napplications of this design pattern without listing much more code than we have\nspace for here. In general, though, such a factory might allow code to be\ninsulated from the details of dynamically configured object construction.\nFor instance, recall the Processor class presented as a composition demo earlier\nin Example 31-3. It accepts reader and writer objects for processing arbitrary\ndata streams. The original abstract version of this example in Chapter 26\nmanually passed in instances of specialized classes like FileWriter and\nSocketReader to customize the data streams being processed; later, we passed\nin hardcoded file, stream, and formatter objects. In a more dynamic scenario,\nexternal devices such as configuration files or GUIs might be used to configure\nthe streams.\nIn such a dynamic world, we might not be able to hardcode the creation of\nstream interface objects in our scripts but might instead create them at runtime\naccording to the contents of a configuration file.\nSuch a file might simply give the string name of a stream class to be imported\nfrom a module, plus an optional constructor call argument. Factory-style\nfunctions or code might come in handy here because they would allow us to\nfetch and pass in classes that are not hardcoded in our program ahead of time.\nIndeed, those classes might not even have existed at all when we wrote our code.\nHypothetically:\nclassname = …parse from config file…\nclassarg  = …parse from config file…\nimport streamtypes                           # Customizable code\naclass = getattr(streamtypes, classname)     # Fetch from module\nreader = factory(aclass, classarg)           # Or aclass(classarg)\nprocessor(reader, …)\nHere, the getattr built-in is again used to fetch a module attribute given a\nstring name (it’s like saying obj.attr, but attr is a string). This code snippet\ndoesn’t strictly need factory—it could make an instance with just\naclass(classarg). A separate function, though, may prove more useful when\nextra work is required at instance creation time, such as caching objects for",
      "content_length": 2217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1208,
      "chapter": null,
      "content": "reuse. However they are coded, factories are almost trivial with Python’s\ndynamic typing and universal first-class object model.\nMultiple Inheritance and the MRO\nOur last design pattern is one of the most useful and will serve as a subject for\nmore realistic examples to wrap up this chapter. As a bonus, the code we’ll write\nhere may be useful tools.\nMost of our examples so far have used single inheritance—class trees in which\neach class has just one superclass. This suffices for simple hierarchies and\nenables customization. In our pizza shop demo of Example 31-1, for example,\neach worker belonged to just one category and inherited names from only one\nbranch of the class tree. Abstractly:\nclass B: …\nclass A(B): …\nI = A()                # Use attributes from I, A, and B - in that order\nMany class-based designs, however, call for combining disparate sets of\nmethods. As we’ve seen, in a class statement, more than one superclass can be\nlisted in parentheses in the header line. When you do this, you leverage multiple\ninheritance—the class and its instances inherit names from all the listed\nsuperclasses:\nclass C: …\nclass B: …\nclass A(B, C): …\nI = A()                # Use attributes from I, A, B, and C – in that order\nIn general, multiple inheritance is good for modeling objects that belong to more\nthan one set. For instance, a person may be an engineer, a writer, a musician, and\nso on and inherit properties from all such sets. With multiple inheritance, objects\nobtain the union of the behavior in all their superclasses. As you’ll see ahead,\nmultiple inheritance also allows classes to function as general packages of\nmixable attributes known as mix-in classes.\nAlthough it’s a useful tool, multiple inheritance adds another dimension to",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1209,
      "chapter": null,
      "content": "attribute inheritance:\nSingle inheritance searches only depth first—from instance, to class,\nand to each superclass from lowest to highest. This order is determined\nby the class from which an instance is made and the superclass listed in\nparentheses in each class statement’s header (which is mirrored by\nclass objects’ __bases__ attributes).\nMultiple inheritance also searches left to right—according to the order\nof classes listed in parentheses in class headers. This is a nested\ncomponent, pursued only after branches further to the left have reached\nthe top of the tree and have been exhausted of their own right-branch\ncandidates.\nWe call the combination of these two DFLR, for depth first, and then left to\nright. This search suffices when an attribute name shows up in just one branch of\na class tree, which is a typical case. When the same name appears in multiple\nbranches, though, DFLR alone is subpar: a lower (and hence more specialized)\nsubclass to the right isn’t able to redefine a name in one of its higher (and hence\nmore general) superclasses reached through a branch to the left.\nBecause of this, multiple inheritance requires a slightly different search order\nknown as MRO, for method resolution order (though it’s used for all attributes,\nnot just methods). In brief, MRO order works the same as DFLR in typical trees\nbut proceeds across by tree levels before moving up in a more breadth-first\nfashion when multiple classes in a tree share a common superclass, forming\nwhat’s called a diamond pattern—after the tree’s square-on-its-corner shape.\nThis MRO search order is run for all class trees but differs from DFLR when\nmultiple-inheritance diamonds are present. It’s designed to visit a common\nsuperclass just once, and after all its subclasses. While user-defined classes don’t\noften form diamonds in Python, the built-in object class automatically added\nabove all root (topmost) classes makes every multiple-inheritance tree a\ndiamond; without MRO, object’s defaults may hide user-defined versions.\nTo illustrate what diamonds are all about and how MRO search runs across\nbefore up in this one somewhat atypical case, let’s turn to some code.",
      "content_length": 2169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1210,
      "chapter": null,
      "content": "How Multiple Inheritance Works\nWe’ll get more formal about MROs in the next section, but it’s easy to demo\nlive. Let’s start with the simple and typical case. In the following, class A inherits\nfrom both B and C using multiple inheritance, and B and C are root (topmost)\nclasses that define unique attributes. When we make an instance I of A and ask\nfor its attributes, inheritance searches I, A, B, and C—and in that order. Hence,\nattr1 is located in B, and attr2 in C (if you’re working along in a REPL, type a\nblank line after each class, omitted here for brevity):\n>>> class C:       attr2 = 'C2'\n>>> class B:       attr1 = 'B1'\n>>> class A(B, C): pass\n>>> I = A()\n>>> I.attr1, I.attr2\n('B1', 'C2')\nAs you might already expect, B wins if it has the same name as C due to the left-\nto-right component of the search, and lower classes like A still beat their supers\nas before:\n>>> class C:       attr2 = 'C2'\n>>> class B:       attr2 = 'B2'; attr1 = 'B1'\n>>> class A(B, C): attr1 = 'A1'\n>>> I = A()\n>>> I.attr1, I.attr2\n('A1', 'B2')\nNext, let’s add a higher superclass above B: in the following, A inherits from both\nB and C as before, but B also inherits from D, and C is a root class with no supers.\nPer the DFLR order (depth first and then left to right), attr2 is found in D by\nvirtue of searching I, A, B, and D—and before C is even checked:\n>>> class D:       attr2 = 'D2' \n>>> class C:       attr2 = 'C2'\n>>> class B(D):    attr1 = 'B1'\n>>> class A(B, C): pass\n>>> I = A()",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1211,
      "chapter": null,
      "content": ">>> I.attr1, I.attr2\n('B1', 'D2')\nWatch what happens, though, if B and C both inherit from D, and C has the same\nattribute as D—per the MRO, the lower C class’s attribute wins and effectively\nreplaces that attribute in D:\n>>> class D:       attr2 = 'D2'\n>>> class C(D):    attr2 = 'C2'\n>>> class B(D):    attr1 = 'B1'\n>>> class A(B, C): pass\n \n>>> I = A()\n>>> I.attr1, I.attr2\n('B1', 'C2')\nThis last case is what is meant by a diamond pattern of inheritance: multiple\nclasses, B and C, both inherit from the same superclass. When this happens, the\nlower class’s version of an attribute is used instead of a same-named version in a\nsuperclass—even if that superclass could be reached first by depth alone.\nImportantly, this differs only in diamonds: nondiamonds still choose higher\nsuperclasses over lower classes with the same attribute name as in the third\nexample in this section. The MRO allows a class to override attributes in a\nhigher class from which it inherits; in the last example, C can replace names in D\neven if D is first per DFLR.\nTo summarize: in nondiamonds, the search proceeds all the way to the top, then\nbacks up and starts searching to the right (DFLR); in diamonds, the search\nchecks classes to the right before climbing up to a common superclass (MRO).\nStrictly speaking, all four examples in this section are implicit diamonds because\nroot classes inherit from the built-in object class automatically. We’ve seen that\nthis class provides defaults for some methods, such as print displays. Because of\nthe MRO, the object built-in’s defaults never hide methods in user-defined\nclasses below it in a class tree. To see how object is ruled out this way, let’s get\na bit more precise on the MRO’s order.\nHow the MRO Works",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1212,
      "chapter": null,
      "content": "Formally speaking, the MRO inheritance search order works as if all classes are\nlisted per the DFLR, and then all but the rightmost duplicate of each class is\nremoved. In more detail, it’s computed as follows:\n1. List all the classes from which an instance inherits using the DFLR\nlookup order, and include a class multiple times if it’s visited more than\nonce.\n2. Scan the resulting list for duplicate classes, removing all but the last\n(rightmost) occurrence of duplicates in the list.\nThe resulting MRO sequence for a given class includes the class, its\nsuperclasses, and all higher superclasses up to and including the implicit or\nexplicit object root class above the tops of the tree. It’s ordered such that each\nclass appears before its parents, and multiple parents retain the order in which\nthey appear in the class header.\nBecause common parents in diamonds appear only at the position of their last\nvisitation in the MRO, lower classes are searched first when the MRO list is\nused later by attribute inheritance (making it more breadth-first than depth-first\nin diamonds only), and each class is included and thus visited just once, no\nmatter how many classes lead to it.\nConsider the nondiamond class tree of Example 31-8, whose shape is sketched\nas “ASCII art” in its comments.\nExample 31-8. mro_nondiamond.py\nclass E:       attr = 'E'     #   D     E\nclass D:       attr = 'D'     #   |     |\nclass C(E):    attr = 'C'     #   B     C\nclass B(D):    pass           #    \\   /\nclass A(B, C): pass           #      A\n                             #      |\nX = A()                       #      X\nprint(X.attr)  # D\nTo compute the MRO inheritance search order through this tree, Python first\nenumerates all classes accessible per DFLR and then removes duplicates\n(remember that the built-in object is implicitly above all root classes):\nDFLR => [X, A, B, D, object, C, E, object]",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1213,
      "chapter": null,
      "content": "MRO  => [X, A, B, D, C, E, object]\nIn this tree, the net result for both DFLR and MRO ordering is the same: the\noutput is “D” by taking attr from class D at the top left. Technically, the MRO\ndiffers because the built-in object appears just once at the end after its\nduplicates are removed; this is irrelevant to our classes because they don’t\nredefine an object default (like print strings). When a user-defined diamond is\ncoded in Example 31-9, however, the MRO’s difference is more striking.\nExample 31-9. mro_diamond.py\nclass D:       attr = 'D'     #      D\nclass C(D):    attr = 'C'     #    /   \\\nclass B(D):    pass           #   B     C\nclass A(B, C): pass           #    \\   /\n                             #      A\nX = A()                       #      |\nprint(X.attr)  # C            #      X\nFor this tree, the common user-defined class D is reached twice by DFLR.\nBecause the MRO keeps only the last (rightmost) D, the lower C’s definition of\nattr now wins over D’s. Hence, the output is now “C” instead of “D”:\nDFLR => [X, A, B, D, object, C, D, object]\nMRO  => [X, A, B, C, D, object]\nIn fact, you can view the MRO of any class with its built-in __mro__ attribute.\nThis tuple gives the inheritance search order followed for instances of the class\n(after the instance itself). It’s set to the result of the mro class method at class\ncreation time (which can technically be customized for roles too obscure to\ncover here). It’s also a lot to look at unless we select class names as in the\nfollowing, which inspects classes in the nondiamond tree of Example 31-8:\n>>> from mro_nondiamond import *\nD\n>>> [c.__name__ for c in A.__mro__]\n['A', 'B', 'D', 'C', 'E', 'object']\n>>> [c.__name__ for c in C.__mro__]\n['C', 'E', 'object']\nAnd here’s the case for the diamond class tree in Example 31-9:",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1214,
      "chapter": null,
      "content": ">>> from mro_diamond import *\nC\n>>> [c.__name__ for c in A.__mro__]\n['A', 'B', 'C', 'D', 'object']\n>>> [c.__name__ for c in C.__mro__]\n['C', 'D', 'object']\nThe full __mro__ is the classes used by inheritance at a given class, and\n__bases__ is just supers there:\n>>> A.__mro__\n(<class 'mro_diamond.A'>, <class 'mro_diamond.B'>, <class 'mro_diamond.C'>, \n<class 'mro_diamond.D'>, <class 'object'>)\n>>> X.__class__.__mro__\n(<class 'mro_diamond.A'>, <class 'mro_diamond.B'>, <class 'mro_diamond.C'>, \n<class 'mro_diamond.D'>, <class 'object'>)\n>>> A.__bases__\n(<class 'mro_diamond.B'>, <class 'mro_diamond.C'>)\n>>> D.__bases__\n(<class 'object'>,)\nExperiment with __mro__ on your own for more fidelity. Especially if you’re\nunsure how the MRO handles a given class tree, this attribute can be consulted\nto see how inheritance will truly search.\nAttribute Conflict Resolution\nThough a useful pattern, multiple inheritance’s chief downside is that it can pose\na conflict when the same method (or other attribute) name is defined in more\nthan one branch of the class tree. When this occurs, the conflict is resolved either\nautomatically by the inheritance search order or manually in your code:\nDefault\nBy default, inheritance chooses the first occurrence of an attribute it finds\nwhen an attribute is referenced normally (e.g., by self.attr). In this mode,\nPython chooses the first appearance located while scanning the MRO of an\ninstance’s class, from left to right.",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1215,
      "chapter": null,
      "content": "Explicit\nIn some class models, you may need to select an attribute explicitly by\nreferencing it through its class name (e.g., by superclass.attr). Your code\nresolves the conflict this way and overrides the search’s default to select an\noption other than the inheritance search’s default.\nBy default, multiple inheritance assumes that names on the left of a class tree\nshould override the same names on the right, and the MRO further assumes that\nlower classes should override same-named attributes in common superclasses of\ndiamonds. Of course, the problem with assumptions is that they assume things.\nIn Example 31-9, what if you need some same-named attributes from B but also\nsome others from C?\nLuckily, there is an inheritance escape hatch. If the default search order doesn’t\nwork, or if you simply want more control over the search process, you can\nalways force the selection of an attribute from anywhere in the tree by assigning\nor otherwise naming the one you want at the place where classes are mixed\ntogether.\nIn Example 31-9, for instance, any of the following selections are allowed, but\nthe latter two make the choice explicit rather than relying on the implicit and\nsubtle “magic” of MRO inheritance:\nclass A(B, C): pass              # Use the default MRO choice (C)\nclass A(B, C): attr = B.attr     # Choose attr from the left branch (D)\nclass A(B, C): attr = C.attr     # Choose attr from the right branch (C)\nNaturally, attributes picked this way can also be methods—which are normal,\nassignable attributes that happen to reference callable function objects.\nMoreover, the choice can be made in a call:\nclass A(B, C):\n    def …:\n        self.method(…)           # Use the default MRO choice\nclass A(B, C):\n    def …:\n        B.method(self, …)        # Choose method from the left branch",
      "content_length": 1805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1216,
      "chapter": null,
      "content": "class A(B, C):  \n    def …:              \n        C.method(self, …)        # Choose method from the right branch\nSuch calls can be used to override the MRO, but also to kick calls up the tree\nwhen the class’s own version must be skipped. As you’ll see in the next chapter,\nsuch calls might also use super().method(), which selects the class following\nthe call’s host on self’s MRO; though simple in single inheritance, this can be\nstunningly implicit in multiple inheritance and doesn’t provide as much control\nas explicit class names.\nHowever they are coded, such manual overrides are required only when the\nsame name appears in multiple superclasses, and you do not wish to use the first\none inherited. For example, manual overrides allow you to unambiguously\nchoose among a set of same names in both left and right branches, while\ninheritance would choose just names on the left. They can also be used to make\nchoices explicit in general, though, even in nondiamonds.\nBecause this isn’t as common an issue in typical Python code as it may sound,\nwe’ll defer details on this topic until we study the super built-in in the next\nchapter and revisit this as a “gotcha” at the end of that chapter. First, though, the\nnext section demonstrates a practical use case for the multiple-inheritance design\npattern.\nNOTE\nThe inheritance finale: Despite the MRO’s complexity, there are still a few inheritance hurdles\nleft to clear. Its complete algorithm is more complex than the model sketched here,\nincorporating special cases for metaclasses, descriptors, and built-ins. In Chapter 32, we’ll\nexpand inheritance for the metaclass tree and apply its MRO in the super built-in, and in\nChapter 38, we’ll study its special case for built-in operations, but we won’t be able to\nformalize inheritance in full until Chapter 40 after we’ve studied all these tools in more depth.\nThe good news is that the remaining hurdles almost never trip up application programs; for\nmost coders, the “finale” is a fully optional read.\nExample: “Mix-in” Attribute Listers\nPerhaps the most common way multiple inheritance is used is to “mix in”\ngeneral-purpose methods from superclasses. Such superclasses are usually called",
      "content_length": 2194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1217,
      "chapter": null,
      "content": "mix-in classes—they provide methods you add to application classes by\ninheritance. In a sense, mix-in classes are similar to modules: they provide\npackages of methods for use in their client subclasses. Unlike simple functions\nin modules, though, methods in mix-in classes also can participate in inheritance\nhierarchies and have access to the self instance for using state information and\nother methods in their trees.\nFor example, we’ve seen that Python’s default way to print a class instance\nobject isn’t incredibly useful:\n>>> class Hack:\n        def __init__(self, what):               # No __repr__ or __str__\n            self.data1 = what\n>>> X = Hack('code')\n>>> print(X)                                    # Default: class name + address (id)\n<__main__.Hack object at 0x10ba464e0>\nAs you learned in both Chapter 28’s case study and Chapter 30’s operator-\noverloading coverage, classes can provide a __str__ or __repr__ method to\nimplement custom displays. But rather than coding one of these in each class\nyou wish to print, why not code it once in a general-purpose tool class and\ninherit it in all your other classes?\nThat’s what mix-ins are for. Defining a display method in a mix-in superclass\nonce enables us to reuse it anywhere we want to see a custom display format—\neven in classes that may already have another superclass. We’ve already\nexplored tools that do related work:\nChapter 28’s AttrDisplay class, of Example 28-13, formatted instance\nattributes in a generic __repr__ method, but it did not climb class trees\nand was utilized in single-inheritance mode only.\nChapter 29’s classtree.py module, of Example 29-9, defined functions\nfor climbing and sketching class trees, but it did not display object\nattributes along the way and was not architected as an inheritable class.\nHere, we’re going to revisit these examples’ techniques and expand upon them to\ncode a set of three mix-in classes that serve as generic display tools for listing\ninstance attributes, inherited attributes, and attributes on all objects in a class",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1218,
      "chapter": null,
      "content": "tree, respectively. We’ll also use our tools in multiple-inheritance mode and\ndeploy coding techniques that make classes better suited to use as generic tools.\nUnlike Chapter 28, we’ll code this with a __str__ instead of a __repr__. This\nis partially a style issue and limits their role to print and str, but the displays\nwe’ll be developing are meant to be user-friendly, not imitative of code. This\npolicy also leaves client classes the option of coding an alternative lower-level\ndisplay for interactive echoes and nested appearances with a __repr__ and has\nbuilt-in immunity from __repr__ looping perils covered ahead.\nListing instance attributes with __dict__\nLet’s get started with the simple case—listing attributes attached to an instance.\nExample 31-10, coded in the file listinstance.py, defines a mix-in called\nListInstance that overloads the __str__ method for all classes that include it\nin their header lines. Because this is coded as a class, ListInstance is a generic\ntool whose formatting logic can be used for instances of any subclass client.\nExample 31-10. listinstance.py\nclass ListInstance:\n   \"\"\"\n   Mix-in class that provides a formatted print() or str() of instances via\n   inheritance of __str__ coded here.  Displays instance attrs only; self is\n   instance of lowest class; __X naming avoids clashing with client's attrs.\n   Works for classes with slots: a __dict__ is ensured by lack of slots here.\n   \"\"\"\n   def __attrnames(self):\n       result = '\\n'\n       for attr in sorted(self.__dict__):                      # Slots okay\n           result += f'\\t{attr}={self.__dict__[attr]!r}\\n'     # Repr for quotes\n       return result\n   def __str__(self):\n       return (f'<Instance of {self.__class__.__name__}, '     # My class's name\n               f'address {id(self):#x}:'                       # My address (hex)\n               f'{self.__attrnames()}>')                       # name=value list\nif __name__ == '__main__':\n   import testmixin\n   testmixin.tester(ListInstance)      # Test class in this module\nThe __attrnames method here exhibits a classic comprehension pattern, and\nyou might save some program real estate by implementing it more concisely",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1219,
      "chapter": null,
      "content": "with a generator expression triggered by a ''.join() call; we’ll leave this as a\nsuggested exercise. As coded, ListInstance uses some previously explored\ntechniques to extract the instance’s class name and attributes:\nIt uses a self.__class__.__name__ expression to fetch the name of an\ninstance’s class. Recall that each instance has a built-in __class__\nattribute that references the class from which it was created, and each\nclass has a __name__ attribute that references the class’s name given in\nits header line.\nIt does most of its work by simply scanning the instance’s attribute\ndictionary (remember, it’s available in __dict__) to build up a string\nshowing the names and values of all instance attributes. The\ndictionary’s keys are sorted by name to be human-friendly, instead of\nrelying on the dictionary’s insertion order.\nIn these respects, ListInstance is similar to Chapter 28’s attribute display; in\nfact, it’s largely just a variation on a theme. Our class here, though, uses two\nadditional techniques:\nIt displays the instance’s memory address by calling the id built-in\nfunction, which returns any object’s address. By definition, this is a\nunique object identifier, which will be useful in later mutations of this\ncode.\nIt uses the pseudoprivate naming pattern for its worker method:\n__attrnames. As we saw earlier in this chapter, Python automatically\nlocalizes any such name to its enclosing class by expanding the attribute\nname to include the class name; in this case, it becomes\n_ListInstance__attrnames. This holds true for both class attributes\nlike methods and instance attributes attached to self. As first noted in\nChapter 28, this helps in a general tool like this, as it ensures that its\nnames won’t clash with any names used in its client subclasses.\nBecause ListInstance defines a __str__ operator-overloading method,\ninstances derived from this class display their attributes automatically when\nprinted, giving a bit more information than a simple address. Here is this tool",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1220,
      "chapter": null,
      "content": "class in action in single-inheritance mode, mixed in to the previous section’s\nclass:\n>>> from listinstance import ListInstance\n>>> class Hack(ListInstance):                    # Inherit a __str__ method\n        def __init__(self, what):\n            self.data1 = what\n>>> X = Hack('code')\n>>> print(X)                                     # print() and str() run __str__\n<Instance of Hack, address 0x10c890b90:\n        data1='code'\n>\nYou can also get the listing display as a string with str without printing it (use\nprint later to apply escapes), and interactive echoes and nesting still use the\ndefault format (we’ve left __repr__ as an option for clients):\n>>> str(X)\n\"<Instance of Hack, address 0x10c890b90:\\n\\tdata1='code'\\n>\"\n>>> X\n<__main__.Hack object at 0x10c890b90>\nThe ListInstance class is useful for any classes you write—even classes that\nalready have one or more superclasses. This is where multiple inheritance comes\nin handy: by adding ListInstance to the list of superclasses in a class header\n(i.e., mixing it in), you get its __str__ “for free” while still inheriting from the\nexisting superclass(es). The file testmixin.py in Example 31-11 codes test classes\nthat prove the point.\nExample 31-11. testmixin.py\n\"\"\"\nGeneric lister-mixin tester: similar to transitive reloader in\nChapter 25, but passes a class object to tester (not function),\nand testByNames adds loading of both module and class by name\nstrings here, in keeping with Chapter 31's factories pattern.\n\"\"\"\nimport importlib\ndef tester(listerclass, sept=False):   \n   \"Pass any lister class to listerclass\"\n   class Super:",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1221,
      "chapter": null,
      "content": "def __init__(self):                 # Superclass __init__\n           self.data1 = 'code'             # Create instance attrs\n       def method1(self):\n           pass\n   class Sub(Super, listerclass):          # Mix in method1 and a __str__\n       def __init__(self):                 # Listers have access to self\n           Super.__init__(self)            # Or super().__init__()\n           self.data2 = 'Python'           # More instance attrs\n           self.data3 = 3.12\n       def method2(self):                  # Define another method here\n           pass\n   instance = Sub()                        # Build instance with lister's __str__\n   print(instance)                         # Run mixed-in __str__ (or via str(x))\n   if sept: print(f'\\n{'-' * 80}\\n')       # Python 3.12+ f-string\ndef testByNames(modname, classname, sept=False):\n   modobject   = importlib.import_module(modname)    # Import mod by namestring\n   listerclass = getattr(modobject, classname)       # Fetch attr by namestring\n   tester(listerclass, sept)\nif __name__ == '__main__':\n   testByNames('listinstance',  'ListInstance',  True)      # Test all three here\n   testByNames('listinherited', 'ListInherited', True)      # See others ahead...\n   testByNames('listtree',      'ListTree',      False)\nThis file is instrumented to allow us to test a variety of listers. To this end, it is\npassed a lister class, listerclass, to be mixed in with test classes nested in a\nfunction. Again, classes are passable first-class objects, and we need to make the\nlister class a parameter to allow it to vary per test. This file also has tools to fetch\nclasses by name strings, which we’ll set aside for the moment.\nMore important here is the self-test code in Example 31-10: it imports the\ntester function in Example 31-11 and passes in ListInstance to be tested\nhere. Hence, Sub here inherits names from both Super and ListInstance; it’s a\ncomposite of its own names and names in both its superclasses. When you make\na Sub instance and print it, you automatically get the custom representation\nmixed in from ListInstance:\n$ python3 listinstance.py \n<Instance of Sub, address 0x10caae300:\n        data1='code'\n        data2='Python'\n        data3=3.12",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1222,
      "chapter": null,
      "content": ">\nThe ListInstance class responsible for this display works in any class it’s\nmixed into because self refers to an instance of the subclass that pulls this class\nin, whatever that may be. Again, in a sense, mix-in classes are the class\nequivalent of modules—packages of methods useful in a variety of clients.\nBesides the utility they provide, mix-ins optimize code maintenance, as all\nclasses do. For example, if you later decide to extend ListInstance’s __str__\nto also print all the class attributes that an instance inherits, you’re covered;\nbecause it’s an inherited method, changing __str__ automatically updates the\ndisplay of each subclass that imports the class and mixes it in. And since it’s now\nofficially “later,” let’s move on to the next section to see what such an extension\nmight look like.\nListing inherited attributes with dir\nAs it is, our ListInstance mix-in displays instance attributes only—names\nattached to the instance object itself. It’s nearly trivial, though, to extend the\nclass to display all the attributes accessible from an instance—both its own and\nthose it inherits from its classes. The trick is to use the dir built-in function\ninstead of scanning the instance’s __dict__ dictionary; the latter holds instance\nattributes only, but the former also collects all inherited attributes.\nThe mutation in Example 31-12, listinherited.py, codes this scheme. This is\nlocated in its own module to facilitate testing, but if existing clients were to use\nthis version instead, they would pick up the new display automatically (and\nrecall from Chapter 25 that a from import’s as clause can rename a new version\nto a prior name being used).\nExample 31-12. listinherited.py\nclass ListInherited:\n   \"\"\"\n   Use dir() to collect both instance attrs and names inherited from\n   its classes.  This includes default names inherited from the implied \n   'object' superclass above topmost classes.  getattr() can fetch\n   inherited names not in self.__dict__.  \n   Caution: use __str__, not __repr__, or else this loops when printing \n   bound methods that may be returned for some attributes by getattr().",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1223,
      "chapter": null,
      "content": "This will normally fail for class \"slots\" attributes not yet assigned.\n   \"\"\"\n   def __attrnames(self, unders=False):\n       result = '\\n'\n       for attr in dir(self):                                   # Instance dir()\n           if attr[:2] == '__' and attr[-2:] == '__':           # Built-in names\n               result += f'\\t{attr}\\n' if unders else ''        # Skip built-ins?\n           else:\n               result += f'\\t{attr}={getattr(self, attr)!r}\\n'\n       return result\n   def __str__(self):\n       return (f'<Instance of {self.__class__.__name__}, '      # My class's name\n               f'address {id(self):#x}:'                        # My address (hex)\n               f'{self.__attrnames()}>')                        # name=value list\nif __name__ == '__main__':\n   import testmixin\n   testmixin.tester(ListInherited)      # Test class in this module\nNotice that this code, by default, skips all __X__ names for brevity. There are 27\nsuch names in the test case, and most of these are internal names that we don’t\ngenerally care about in a generic listing like this. Some are user-defined\noperator-overloading methods like our __str__, though most reflect defaults in\nthe implicit object root class, and there’s no easy way to determine an\nattribute’s class of origin here (but stay tuned).\nBecause dir results may include names from anywhere in a class tree, this\nversion also must use the getattr built-in function to fetch attributes by name\nstring instead of indexing the instance’s __dict__ attribute dictionary. getattr\nruns inheritance search, and some of the names we’re listing here are not stored\non the instance itself.\nTo test the new version, run its file directly—it passes the ListInherited class\nit defines to the testmixin.py file’s test function in Example 31-11 to be mixed in\nwith a subclass in the function. Here’s the output of this test and lister class;\nnotice the extra names inherited from classes and the name mangling at work in\nthe lister’s method name:\n$ python3 listinherited.py\n<Instance of Sub, address 0x101d76600:\n        _ListInherited__attrnames=<bound method ListInherited.__attrnames of <…>>",
      "content_length": 2147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1224,
      "chapter": null,
      "content": "data1='code'\n        data2='Python'\n        data3=3.12\n        method1=<bound method tester.<locals>.Super.method1 of <testmixin.tester…>>\n        method2=<bound method tester.<locals>.Sub.method2 of <testmixin.tester…>>\n>\nThe display of bound methods in this was truncated to fit this page; here’s what\nthe first two look like with an added line break (as usual, run on your own for the\nfull and up-to-date picture):\n        _ListInherited__attrnames=<bound method ListInherited.__attrnames of\n            <testmixin.tester.<locals>.Sub object at 0x101d76600>>\n        method1=<bound method tester.<locals>.Super.method1 of \n            <testmixin.tester.<locals>.Sub object at 0x101d76600>>\nDisplay formatting is an open-ended task (e.g., Python’s standard pprint “pretty\nprinter” module may offer options here too), so we’ll leave further polishing as a\nsuggested exercise. The tree lister of the next section may be more useful in any\nevent, so let’s move on.\nNOTE\nLooping in __repr__: Now that we’re displaying inherited methods too, we must use __str__\ninstead of __repr__ to overload printing. With __repr__, this code (and code like it) will fall\ninto recursive loops—its getattr returns a bound method; whose display includes the\ninstance; which triggers __repr__ again to display the instance; and so on, quickly triggering a\nstack-overflow exception. Subtle, but true! Change __str__ to __repr__ to see this live. One\nway to avoid such __repr__ loops is to skip getattr results for which isinstance\ncomparisons to the standard library’s types.MethodType are true. Using __str__ instead is\nsimpler.\nListing attributes per object in class trees\nLet’s code one last extension. As it is, our latest lister includes inherited names\nbut doesn’t give any sort of designation of the classes from which the names are\nacquired. As we saw in the classtree.py example near the end of Chapter 29,\nthough, it’s straightforward to climb class inheritance trees in code.",
      "content_length": 1965,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1225,
      "chapter": null,
      "content": "The mix-in class in Example 31-13, coded in file listtree.py, makes use of this\nsame technique to display attributes grouped by the classes in which they live—\nit sketches the full physical class tree, displaying attributes attached to each\nobject along the way. The reader must still infer attribute inheritance (and we’ll\naddress this in the next section’s final demo), but this version gives substantially\nmore detail than a simple flat list of attributes, inherited or not.\nExample 31-13. listtree.py\nclass ListTree:\n   \"\"\"\n   Mix-in that returns a __str__ trace of the entire class tree and all\n   its objects' attrs at and above the self instance.  The display is\n   run by print() automatically; use str() to fetch as a string.  This:\n   -Uses __X pseudoprivate attr names to avoid conflicts with clients\n   -Recurses to superclasses explicitly in DLFR (though not MRO) order\n   -Uses __dict__ instead of dir() because attrs are listed per object\n   -Supports classes with slots: lack of slots here ensures a __dict__\n   \"\"\"\n   def __attrnames(self, obj, indent, unders=True):\n       spaces = ' ' * (indent + 1)\n       result = ''\n       for attr in sorted(obj.__dict__):\n           if attr.startswith('__') and attr.endswith('__'):\n               if unders: result += f'{spaces}{attr}\\n'\n           else:\n               result += f'{spaces}{attr}={getattr(obj, attr)!r}\\n'\n       return result\n   def __listclass(self, aClass, indent):\n       dots = '.' * indent\n       preamble = (f'\\n{dots}'\n                   f'<Class {aClass.__name__}'\n                   f', address {id(aClass):#x}')\n       if aClass in self.__visited:\n           return preamble + ': (see above)>\\n'                # Already listed\n       elif aClass is object:\n           self.__visited[aClass] = True\n           return preamble + ': (see dir(object))>\\n'          # Skip object's 24\n       else:\n           self.__visited[aClass] = True\n           here  = self.__attrnames(aClass, indent)            # My attrs + supers\n           above = ''\n           for Super in aClass.__bases__:\n               above += self.__listclass(Super, indent + 4)",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1226,
      "chapter": null,
      "content": "return preamble + f':\\n{here}{above}{dots}>\\n'\n   def __str__(self):\n       self.__visited = {}\n       here  = self.__attrnames(self, 0)                       # My attrs\n       above = self.__listclass(self.__class__, 4)             # My supers tree\n       return (f'<Instance of {self.__class__.__name__}'       # My class's name\n               f', address {id(self):#x}'                      # My address (hex)\n               f':\\n{here}{above}>')                           # attrs + supers\nif __name__ == '__main__':\n   import testmixin\n   testmixin.tester(ListTree)      # Test class in this module\nThis class achieves its goal by traversing the inheritance tree—from an\ninstance’s __class__ to its class, and then from the class’s __bases__ to all\nsuperclasses recursively, scanning each object’s attribute __dict__ along the\nway to enumerate attributes. Ultimately, it concatenates each tree portion’s string\nas the recursion unwinds.\nIt can take a few moments to understand recursive programs like this, but given\nthe arbitrary shape and depth of class trees, we really have no choice here (apart\nfrom explicit stack equivalents of the sorts we met in Chapters 19 and 25, which\ntend to be no simpler, and which we’ll omit here for space and time). This class\nis coded to keep its business as explicit as possible, though, to maximize clarity.\nTo test, run this class’s module file as before; it passes the ListTree class to\ntestmixin.py of Example 31-11 again, to be mixed in with a subclass in the test\nfunction. The file’s tree-sketcher output is as follows:\n$ python3 listtree.py\n<Instance of Sub, address 0x10a45f980:\n _ListTree__visited={}\n data1='code'\n data2='Python'\n data3=3.12\n....<Class Sub, address 0x7fb26a448530:\n     __doc__\n     __init__\n     __module__\n     method2=<function tester.<locals>.Sub.method2 at 0x10a482ac0>\n........<Class Super, address 0x7fb26a445c00:\n         __dict__",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1227,
      "chapter": null,
      "content": "__doc__\n         __init__\n         __module__\n         __weakref__\n         method1=<function tester.<locals>.Super.method1 at 0x10a4823e0>\n............<Class object, address 0x10a01b100: (see dir(object))>\n........>\n........<Class ListTree, address 0x7fb26a443d20:\n         _ListTree__attrnames=<function ListTree.__attrnames at 0x10a480f40>\n         _ListTree__listclass=<function ListTree.__listclass at 0x10a480fe0>\n         __dict__\n         __doc__\n         __module__\n         __str__\n         __weakref__\n............<Class object, address 0x10a01b100: (see above)>\n........>\n....>\n>\nSome points to notice about this example:\nThe __visited table’s name is mangled in the instance’s attribute\ndictionary for pseudoprivacy; unless we’re very unlucky, this won’t\nclash with other data there. Some of the lister class’s methods are\nmangled as well.\nTo minimize displays, __X__ attributes are listed by name only,\nskipping their values. The built-in object class implied above all\ntopmost classes is also singled out to simply refer readers to its dir\nresult; object comes with 24 attributes today, which wouldn’t be useful\nto repeat in every display.\nThe attributes that were bound methods in the prior version are now\nplain functions. This reflects the fact that this version fetches methods\nfrom classes instead of the instance—the getattr here is run on the\ncurrent tree object whose __dict__ is being scanned (getattr and\n__dict__ indexing are equivalent in this context). Again, class methods\nare just functions, which are bound only when fetched from an instance.",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1228,
      "chapter": null,
      "content": "To avoid listing a class object more than once, a table records classes\nvisited so far. A dictionary works in this role because class objects are\nhashable and thus may be dictionary keys; a set would work similarly.\nOn the last point, cycles are not generally possible in class inheritance trees—a\nclass must already have been defined to be named as a superclass, and Python\nraises an exception as it should if you attempt to create a cycle later by\n__bases__ changes. The visited mechanism here is still needed, though, to avoid\nrelisting a class twice:\n>>> class C: pass\n>>> class B(C): pass\n>>> C.__bases__ = (B,)        # Dark magic\nTypeError: a __bases__ item causes an inheritance cycle\nFor more fun, try mixing this class into something more substantial, like the\nButton class of Python’s tkinter GUI-toolkit module. In general, you’ll want\nto name ListTree first (leftmost) in a class header, so its __str__ is picked up;\nButton has one, too, and the leftmost superclass is searched first in multiple\ninheritance’s default:\n>>> from tkinter import Button\n>>> from listtree import ListTree\n>>> class ButtonPlus(ListTree, Button): pass       # ListTree's str, not Button's\n>>> print(ButtonPlus())\n…our class's display…\n>>> class ButtonPlus(Button, ListTree): pass       # Mix-in order can matter!\n>>> print(ButtonPlus())\n.!buttonplus2\nOrder matters in multiple inheritance, though the manual overrides we explored\nearlier can force the issue:\n>>> class ButtonPlus(Button, ListTree): __str__ = ListTree.__str__\n…our class's display…\nYou might also try running testmixin.py of Example 31-11 directly; its self-test\ncode that we shelved earlier runs each of our three lister classes in turn, using",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1229,
      "chapter": null,
      "content": "their module and class name strings—a trivial class factory in action:\n$ python3 testmixin.py\n…all three listers' results…\nWhile our tree lister works as planned, it doesn’t really follow Python’s MRO\nordering through class trees with diamonds. In fact, it really just sketches the\nDFLR order and leaves it to readers to determine from which class a given\nattribute is inherited. To do better, let’s move on to a final example to close out\nthis chapter.\nExample: Mapping Attributes to Inheritance Sources\nThis section wraps up with an example that demos an application for the MRO\nin programming tools. As coded, the preceding section’s tree lister gave the\nphysical locations of attributes in a class tree. However, by mapping the list of\ninherited attributes in a dir result to the linear MRO sequence, such tools can\nmore directly associate attributes with the classes from which they are actually\ninherited—also a useful relationship for programmers.\nWe won’t recode our tree lister in full here, but as a first major step, Example 31-\n14, file mapattrs.py, implements tools that can be used to associate attributes\nwith their inheritance source. As an added bonus, its mapattrs function\ndemonstrates how inheritance actually searches for attributes in class tree objects\n—though the MRO is largely automated for us with the built-in __mro__ class\nattribute we met earlier.\nExample 31-14. mapattrs.py\n\"\"\"\nMain tool: mapattrs() maps all attributes on or inherited by an\ninstance to the instance or class from which they are inherited.\nAlso here: assorted dictionary tools using comprehensions.\nAssumes dir() gives all attributes of an instance.  To emulate \ninheritance, this uses the class's __mro__ tuple, which gives the\nMRO search order for classes in Python 3.X.  A recursive tree\ntraversal for the DFLR order of classes is included but unused.\n\"\"\"\nimport pprint\ndef trace(label, X, end='\\n'):",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1230,
      "chapter": null,
      "content": "print(f'{label}\\n{pprint.pformat(X)}{end}')   # Print nicely\ndef filterdictvals(D, V):\n   \"\"\"\n   dict D with entries for value V removed.\n   filterdictvals(dict(a=1, b=2, c=1), 1) => {'b': 2}\n   \"\"\"\n   return {K: V2 for (K, V2) in D.items() if V2 != V}\ndef invertdict(D):\n   \"\"\"\n   dict D with values changed to keys (grouped by values).\n   Values must all be hashable to work as dict/set keys.\n   invertdict(dict(a=1, b=2, c=1)) => {1: ['a', 'c'], 2: ['b']}\n   \"\"\"\n   def keysof(V):\n       return sorted(K for K in D.keys() if D[K] == V)\n   return {V: keysof(V) for V in set(D.values())}\ndef dflr(cls):\n   \"\"\"\n   Depth-first left-to-right order of class tree at cls.\n   Cycles not possible: Python disallows on __bases__ changes.\n   \"\"\"\n   here = [cls]\n   for sup in cls.__bases__:\n       here += dflr(sup)\n   return here\ndef inheritance(instance):\n   \"\"\"\n   Inheritance order sequence: MRO or DFLR.\n   DFLR alone is no longer used in Python 3.X.\n   \"\"\"\n   if hasattr(instance.__class__, '__mro__'):\n       return (instance,) + instance.__class__.__mro__\n   else:\n       return [instance] + dflr(instance.__class__)\ndef mapattrs(instance, withobject=False, bysource=False):\n   \"\"\"\n   dict with keys giving all inherited attributes of instance,\n   with values giving the object that each is inherited from.\n   withobject: False=remove object built-in class attributes.\n   bysource:   True=group result by objects instead of attributes.\n   Supports classes with slots that preclude __dict__ in instances.\n   \"\"\"\n   attr2obj = {}\n   inherits = inheritance(instance)",
      "content_length": 1563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1231,
      "chapter": null,
      "content": "for attr in dir(instance):\n       for obj in inherits:\n            if hasattr(obj, '__dict__') and attr in obj.__dict__:    # Slots okay\n              attr2obj[attr] = obj\n              break\n   if not withobject:\n       attr2obj = filterdictvals(attr2obj, object)\n   return attr2obj if not bysource else invertdict(attr2obj)\nif __name__ == '__main__':\n   class D:         attr2 = 'D'\n   class C(D):      attr2 = 'C'\n   class B(D):      attr1 = 'B'\n   class A(B, C):   pass\n   I = A()\n   I.attr0 = 'I'\n   print(f'Py=>{I.attr0=}, {I.attr1=}, {I.attr2=}\\n')    # Python's search\n   trace('INHERITANCE', inheritance(I))                  # [Inheritance order]\n   trace('ATTRIBUTES',  mapattrs(I))                     # {Attr => Source}\n   trace('SOURCES',     mapattrs(I, bysource=True))      # {Source => [Attrs]}\nThis module’s main mapattrs function uses dir to collect all the attributes that\nan instance inherits. For each, it maps the attribute to its source by scanning the\nMRO order available in the __mro__ of the instance’s class, searching each\nobject’s namespace __dict__ along the way. The net effect replicates Python’s\ntrue inheritance search for each attribute accessible from the instance passed in.\nThis file’s self-test code applies its tools to a diamond multiple-inheritance tree\nsimilar to those we studied earlier. It uses Python’s pprint standard-library\nmodule to display lists and dictionaries nicely—pprint.pprint is its basic call,\nand its pformat returns a print string. Notably, attr2, whose value is given on\nthe first line and whose name appears in later function results, is inherited from\nclass C per the MRO order we’ve studied:\n$ python3 mapattrs.py\nPy=>I.attr0='I', I.attr1='B', I.attr2='C'\nINHERITANCE\n(<__main__.A object at 0x10cc33e00>,\n <class '__main__.A'>,\n <class '__main__.B'>,\n <class '__main__.C'>,",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1232,
      "chapter": null,
      "content": "<class '__main__.D'>,\n <class 'object'>)\nATTRIBUTES\n{'__dict__': <class '__main__.D'>,\n '__doc__': <class '__main__.A'>,\n '__module__': <class '__main__.A'>,\n '__weakref__': <class '__main__.D'>,\n 'attr0': <__main__.A object at 0x10cc33e00>,\n 'attr1': <class '__main__.B'>,\n 'attr2': <class '__main__.C'>}\nSOURCES\n{<__main__.A object at 0x10cc33e00>: ['attr0'],\n <class '__main__.D'>: ['__dict__', '__weakref__'],\n <class '__main__.C'>: ['attr2'],\n <class '__main__.B'>: ['attr1'],\n <class '__main__.A'>: ['__doc__', '__module__']}\nAlthough this module was not designed to be a mix-in class itself, listers may\nindex its mapattrs function’s dictionary results to obtain an attribute’s source or\na source’s attributes. Moreover, it’s easy to adapt this module’s results to be a\nmix-in by wrapping them in a __str__. Here it is listing attributes’ sources in\nthe test classes of the prior section’s Example 31-11:\n$ python3\n>>> import pprint\n>>> from mapattrs import mapattrs\n>>> class ListAttr2Source:\n        def __str__(self):\n            return pprint.pformat(mapattrs(self))\n>>> from testmixin import tester\n>>> tester(ListAttr2Source)\n{'__dict__': <class 'testmixin.tester.<locals>.Super'>,\n '__doc__': <class 'testmixin.tester.<locals>.Sub'>,\n '__init__': <class 'testmixin.tester.<locals>.Sub'>,\n '__module__': <class 'testmixin.tester.<locals>.Sub'>,\n '__str__': <class '__main__.ListAttr2Source'>,\n '__weakref__': <class 'testmixin.tester.<locals>.Super'>,\n 'data1': <testmixin.tester.<locals>.Sub object at 0x102a8fa40>,\n 'data2': <testmixin.tester.<locals>.Sub object at 0x102a8fa40>,\n 'data3': <testmixin.tester.<locals>.Sub object at 0x102a8fa40>,\n 'method1': <class 'testmixin.tester.<locals>.Super'>,\n 'method2': <class 'testmixin.tester.<locals>.Sub'>}",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1233,
      "chapter": null,
      "content": "Listing sources’ attributes is just as easy (see also Python’s docs for pprint\noptions like compact and width):\n>>> class ListSource2Attr:\n        def __str__(self):\n            return pprint.pformat(mapattrs(self, bysource=True))\n \n>>> tester(ListSource2Attr)\n{<testmixin.tester.<locals>.Sub object at 0x102ad12e0>: ['data1',\n                                                         'data2',\n                                                         'data3'],\n <class 'testmixin.tester.<locals>.Super'>: ['__dict__',\n                                             '__weakref__',\n                                             'method1'],\n <class 'testmixin.tester.<locals>.Sub'>: ['__doc__',\n                                           '__init__',\n                                           '__module__',\n                                           'method2'],\n <class '__main__.ListSource2Attr'>: ['__str__']}\nStudy this example’s code for more insight. As callouts, notice how it uses\nhasattr to check whether an object has a __dict__ attribute dictionary before\ntrying to index it. Though rare, some instances may not have a __dict__ if they\nuse the class extension known as slots noted earlier. The prior section’s slot story\nis varied: as mix-ins, ListTree and ListInstance work as is for classes with\nslots because their lack of slots ensures an instance __dict__, but\nListInherited can fail for slots not yet assigned—findings to be clarified in the\nnext chapter.\nAdditionally, both slots and other “virtual” instance attributes like properties and\ndescriptors live at the class instead of the instance and hence may require\ngeneric handling—dir enumeration, and either getattr fetches, tree climbs, or\nMRO scans. This example and the prior section’s listers accommodate this, but\nunevenly: some such names will be associated with the classes in which their\nimplementations live, not the instance through which they are accessed.\nMoreover, no lister can show attribute names dynamically computed in full by\nmethods like __getattr__ because these names have no physical basis. Classes\nimplementing such dynamic names can also define a __dir__ method to provide\nan attribute result list for dir calls, but general tools like our listers and mapper",
      "content_length": 2246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1234,
      "chapter": null,
      "content": "cannot depend on this optional and relatively uncommon interface being present.\nFinally, all the attribute listers and mappers in this chapter work in full for\nnormal instances but don’t support classes. For the latter, prints run a default\ndisplay instead of any of the three listers, and mapattrs strangely attributes most\nnames to a mystery class called “type.” The lister skips stem from the fact that\nbuilt-ins like print skip the “instance,” as we’ve noted before. The mapattrs\noddity reflects the fact that classes acquire names from both their own superclass\ntree (and MRO), and a secondary tree (and MRO) formed by “metaclasses” that\nwe have yet to meet.\nBut to understand both the inheritance bifurcation of metaclasses, as well as\nethereal attributes like slots, properties, and descriptors, we need to move on to\nthe next chapter.\nOther Design-Related Topics\nIn this chapter, we’ve studied an assortment of design patterns used to combine\nclasses in Python programs, along with the mechanism behind some of them.\nWe’ve really only scratched the surface here in the design patterns domain,\nthough. Elsewhere in this book, you’ll find coverage of other design-related\ntopics, such as:\nAbstract superclasses (Chapter 29)\nDecorators (Chapters 32 and 39)\nType subclasses (Chapter 32)\nStatic and class methods (Chapter 32)\nManaged attributes (Chapters 32 and 38)\nMetaclasses (Chapters 32 and 40)\nFor even more details on design patterns, this book must delegate to other\nresources on OOP at large. Although patterns are important in OOP work and are\noften more natural in Python than other languages, they are not specific to\nPython itself and a subject that’s often best acquired by experience.",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1235,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we sampled common ways to use and combine classes to\noptimize their reusability and factoring benefits—what are usually considered\ndesign issues, which are often independent of any particular programming\nlanguage (though Python can make them easier to implement). We studied\ninheritance (acquiring behavior from other classes), composition (controlling\nembedded objects), and delegation (wrapping objects in proxy classes), as well\nas the related topics of pseudoprivate attributes, bound methods, factories,\nmultiple inheritance, and the MRO.\nThe next chapter concludes our look at classes and OOP by surveying class-\nrelated topics that are more esoteric than most of what we’ve already seen. Some\nof its material may be of more interest to tool writers than application\nprogrammers, but it still merits a review by most people who will do OOP in\nPython—if not for your code, then for others’ code you may need to understand\nand reuse. First, though, here’s another quick chapter quiz to review.\nTest Your Knowledge: Quiz\n1. What is multiple inheritance?\n2. What is composition?\n3. What is delegation?\n4. What are bound methods?\n5. What are pseudoprivate attributes used for?\n6. How does the MRO inheritance search order differ from DFLR?\nTest Your Knowledge: Answers\n1. Multiple inheritance occurs when a class inherits from more than one\nsuperclass; it’s useful for mixing together multiple packages of class-",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1236,
      "chapter": null,
      "content": "based code. The left-to-right order in class statement headers\ndetermines the general order of attribute searches, and the MRO\nspecializes search for diamonds with common superclasses.\n2. Composition is a technique whereby a controller class embeds and\ndirects a number of objects and provides an interface all its own; it’s a\nway to build up larger structures with classes.\n3. Delegation involves wrapping an object in a proxy class, which adds\nextra behavior and passes other operations to the wrapped object. The\nproxy generally retains the interface of the wrapped object.\n4. Bound methods combine an instance and a method function; you can\ncall them without passing in an instance object explicitly because the\noriginal instance is still available in the instance+function pair.\n5. Pseudoprivate attributes, whose names begin but do not end with two\nleading underscores (e.g., __X), are used to localize names to the\nenclosing class. This includes both class attributes, like methods defined\ninside the class statement, and self instance attributes assigned inside\nthe class’s methods. Such names are expanded to include the class\nname, which makes them generally unique among all classes in an\ninheritance tree.\n6. The MRO selects same-named attributes in a lower subclass over those\nin a higher common superclass in multiple-inheritance “diamond” trees\n—effectively searching across before up in this specific case. The\nDFLR is otherwise the same; in fact, the MRO is defined by starting\nwith the DFLR order and then removing all but the last (rightmost)\nappearances of classes that are visited more than once. This differs from\nDFLR only when there are duplicates, which arise only in diamonds\nthat have common superclasses. That said, the built-in object makes\nevery multiple-inheritance tree a diamond, so this is a common, if\nimplicit, occurrence.",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1237,
      "chapter": null,
      "content": "Chapter 32. Class Odds and Ends\nThis chapter concludes our look at OOP in Python by presenting a collage of\nmore advanced class topics. We will survey customizing built-in types, the\nrelationship of classes and types, attribute tools like slots and properties, the\nspecial-case static and class methods, decorators and metaclasses, and the super\ncall’s complete story. Some of these are introduced here but resumed by focused\nchapters in this book’s Part VIII, “Advanced Topics”.\nAs we’ve seen, Python’s OOP model is, at its core, relatively simple, and some\nof the topics presented in this chapter are so advanced and optional that you may\nnot encounter them very often in your Python applications-programming career.\nIn the interest of completeness, though—and because you never know when an\n“advanced” topic may crop up in code you use—we’ll round out our discussion\nof classes with a brief look at these advanced tools for OOP work.\nAs usual, because this is the last chapter in this part of the book, it ends with a\nsection on class-related “gotchas” and a set of lab exercises for this part to help\ncement the ideas we’ve studied here. Beyond these exercises, studying larger\nOOP Python projects or starting some of your own is heartily recommended as a\nsupplement to this book. As with much in life and computing, the benefits of\nOOP tend to become more apparent with practice.\nNOTE\nBlast from the past: Python 3.X launched with a mandatory “new-style” class model that could\nbe enabled in 2.X as an option; 2.X’s own model was dubbed “classic.” At least in terms of its\nOOP support, new-style classes transformed Python into a different language altogether—one\nthat borrows much more from, and is often as complex as, other languages in this domain. The\nlast chapter’s MRO and most topics in this chapter were part of this package. Because this\nbook is now focused on 3.X only, the term “new style” is moot and unused here—all its classes\nqualify.\nExtending Built-in Object Types",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1238,
      "chapter": null,
      "content": "Besides implementing new kinds of objects, classes are sometimes used to\nextend the functionality of Python’s built-in object types to support more exotic\ndata structures. For instance, to add queue insert and delete methods to lists, you\ncan code classes that wrap (embed) a list object and augment it with insert and\ndelete methods that process the list specially, using the delegation technique we\nstudied in Chapter 31. You can also use simple inheritance to customize built-in\ntypes for such custom roles. The next two sections show both techniques in\naction.\nExtending Types by Embedding\nDo you remember those set functions we wrote in Chapters 16 and 18? Here’s\nwhat they look like brought back to life as a Python class. Example 32-1 (file\nsetwrapper.py) implements a new set object type by moving set functions to\nmethods and adding some basic operator overloading. For the most part, this\nclass just wraps a Python list with extra set operations. But because it’s a class, it\nalso supports multiple instances and customization by inheritance in subclasses.\nUnlike our earlier functions, using classes here allows us to make multiple self-\ncontained set objects with preset data and behavior rather than passing lists into\nfunctions manually.\nExample 32-1. setwrapper.py\nclass Set:\n  def __init__(self, value = []):    # Constructor\n      self.data = []                 # Manages a list\n      self.concat(value)             # Removes duplicates\n  def intersect(self, other):        # other is any iterable\n      res = []                       # self is the subject\n      for x in self.data:\n          if x in other:             # Pick common items\n              res.append(x)\n      return Set(res)                # Return a new Set\n  def union(self, other):            # other is any iterable\n      res = self.data[:]             # Copy of my list\n      for x in other:                # Add items in other\n          if not x in res:\n              res.append(x)\n      return Set(res)\n  def concat(self, value):           # value: list, Set...",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1239,
      "chapter": null,
      "content": "for x in value:                # Removes duplicates\n         if not x in self.data:\n              self.data.append(x)\n  def __len__(self):          return len(self.data)            # len(self), if self\n  def __getitem__(self, key): return self.data[key]            # self[i], self[i:j]\n  def __and__(self, other):   return self.intersect(other)     # self & other\n  def __or__(self, other):    return self.union(other)         # self | other\n  def __repr__(self):         return f'Set({self.data!r})'     # print(self),...\n  def __iter__(self):         return iter(self.data)           # for x in self,...\nTo use this class, we make instances, call methods, and run defined operators as\nusual:\n$ python3\n>>> from setwrapper import Set\n>>> x = Set([1, 3, 5, 7, 3])\n>>> x.union(Set([1, 4, 7]))\nSet([1, 3, 5, 7, 4])\n>>> x | Set([1, 4, 6, 4])\nSet([1, 3, 5, 7, 4, 6])\nOverloading operations such as indexing and iteration also enables instances of\nour Set class to often masquerade as real lists. Because you will interact with\nand extend this class in an exercise at the end of this chapter, we’ll put this code\non the back burner until its solution in Appendix B.\nExtending Types by Subclassing\nWhile the prior section’s embedding works, Python’s built-in types can also be\nsubclassed directly. In fact, type-conversion functions such as list, str, dict,\nand tuple are really built-in type names; although transparent to your script, a\ntype-conversion call (e.g., list('text')) is really an invocation of a type’s\nconstructor.\nThis allows you to customize or extend the behavior of built-in types with user-\ndefined class statements: simply subclass the type names to customize them.\nInstances of your type subclasses can generally be used anywhere that the\noriginal built-in type can appear. For example, suppose you have trouble getting\nused to the fact that Python list offsets begin at 0 instead of 1. Not to worry—\nyou can always code your own subclass that customizes this core behavior of",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1240,
      "chapter": null,
      "content": "lists, and Example 32-2 shows how.\nExample 32-2. typesubclass.py\n\"\"\"\nSubclass built-in list type/class.\nMap 1..N to 0..N-1, call back to built-in version.\n\"\"\"\nclass MyList(list):\n   def __getitem__(self, offset):\n       print(f'<indexing {self} at {offset}>')\n       return list.__getitem__(self, offset - 1)\nif __name__ == '__main__':\n   print(list('abc'))\n   x = MyList('abc')               # __init__ inherited from list\n   print(x)                        # __str__/__repr__ inherited from list\n   print(x[1])                     # MyList.__getitem__\n   print(x[3])                     # Customizes list superclass method\n   x.append('hack!'); print(x)     # Attributes from list superclass\n   x.reverse();       print(x)\nIn this file, the MyList subclass extends the built-in list’s __getitem__ indexing\nmethod only, to map indexes 1 to N back to the required 0 to N−1. Really, all it\ndoes is decrement the submitted index and call back to the superclass’s version\nof indexing, but it’s enough to do the trick:\n$ python3 typesubclass.py\n['a', 'b', 'c']\n['a', 'b', 'c']\n<indexing ['a', 'b', 'c'] at 1>\na\n<indexing ['a', 'b', 'c'] at 3>\nc\n['a', 'b', 'c', 'hack!']\n['hack!', 'c', 'b', 'a']\nThis output also includes tracing text the class prints on indexing. Of course,\nwhether changing indexing this way is a good idea, in general, is another issue\n—users of your MyList class may very well be confused by such a core\ndeparture from Python sequence behavior. The ability to customize built-in types\nthis way can be a powerful asset, though.",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1241,
      "chapter": null,
      "content": "For instance, this coding pattern gives rise to an alternative way to code a set—\nas a subclass of the built-in list type rather than a standalone class that manages\nan embedded list object, as shown in the prior section. As discussed in\nChapter 5, Python today comes with a powerful built-in set object, along with\nliteral and comprehension syntax for making new sets. Coding one yourself,\nthough, is still a great way to learn about type subclassing in general.\nThe code in Example 32-3, file setsubclass.py, customizes lists to add just\nmethods and operators related to set processing. Because all other behavior is\ninherited from the built-in list superclass, this makes for a shorter and simpler\nalternative—everything not defined here is routed to list directly.\nExample 32-3. setsubclass.py\nclass Set(list):\n   def __init__(self, value = []):      # Constructor\n       list.__init__(self)              # Customizes list\n       self.concat(value)               # Copies mutable defaults\n   def intersect(self, other):          # other is any iterable\n       res = []                         # self is the subject\n       for x in self:\n           if x in other:               # Pick common items\n               res.append(x)\n       return Set(res)                  # Return a new Set\n   def union(self, other):              # other is any iterable\n       res = Set(self)                  # Copy me and my list\n       res.concat(other)\n       return res\n   def concat(self, value):             # value: list, Set, etc.\n       for x in value:                  # Removes duplicates\n           if not x in self:\n               self.append(x)\n   def __and__(self, other): return self.intersect(other)\n   def __or__(self, other):  return self.union(other)\n   def __repr__(self):       return f'Set({list.__repr__(self)})'\nif __name__ == '__main__':\n   x = Set([1, 3, 5, 7])\n   y = Set([2, 1, 4, 5, 6])\n   print(x, y, len(x))\n   print(x.intersect(y), y.union(x))\n   print(x & y, x | y)\n   x.reverse(); print(x)",
      "content_length": 2008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1242,
      "chapter": null,
      "content": "Here is the output of the self-test code at the end of this file. Because subclassing\ncore types is a somewhat advanced feature with a limited audience, we’ll end\nthis topic here, but you’re invited to trace through these results in the code to\nstudy its behavior:\n$ python3 setsubclass.py\nSet([1, 3, 5, 7]) Set([2, 1, 4, 5, 6]) 4\nSet([1, 5]) Set([2, 1, 4, 5, 6, 3, 7])\nSet([1, 5]) Set([1, 3, 5, 7, 2, 4, 6])\nSet([7, 5, 3, 1])\nSubtleties: some inherited list operations may introduce duplicates to our Set,\nand there are more efficient ways to implement sets with dictionaries in Python,\nwhich replace the nested linear search scans in the set implementations shown\nhere with more direct dictionary index operations (hashing) and so run much\nquicker. If you’re interested in sets, also take another look at the set object type\nwe explored in Chapter 5; this type provides extensive set operations as built-in\ntools. Set implementations are fun to experiment with but not strictly required in\nPython today.\nMore important here is the question of why we can subclass built-in types like\nlist at all. The next section solves the mystery—at least as much as this chapter\ncan.\nThe Python Object Model\nThe reason we could subclass built-in types in the prior section is that types and\nclasses are largely one and the same—a unification that came with the “new-\nstyle” model alluded to at the start of this chapter. For built-ins, some instances\ncan uniquely be coded with literal syntax like [], 'lp6e', and 3.12 instead of\nclass calls like list(), str(), and float(), but they are instances of a class,\nnonetheless.\nIn fact, built-in types and user-defined classes are both classes and are both\nthemselves instances of the built-in type class. The type object generates\nclasses as its instances, classes generate instances of themselves, and classes are\nreally just user-defined types. And on top of all this, the built-in object class",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1243,
      "chapter": null,
      "content": "provides defaults for every object.\nClasses Are Types Are Classes\nWhile you probably shouldn’t ponder the preceding definitions before operating\nheavy machinery, it’s easy to see all this in code. The type built-in with one\nargument returns any object’s type, which is normally the same as the object’s\n__class__, and isinstance checks whether an object inherits from another.\nHere’s the story for user-defined classes (a.k.a. types):\n>>> class Hack: pass         # A humble user-defined class\n>>> I = Hack()               # Make an instance by calling the class\n>>> type(I)                  # Type is the user-defined class of origin\n<class '__main__.Hack'>\n>>> type(Hack)               # User-defined classes are instances of type\n<class 'type'>\n>>> I.__class__, Hack.__class__\n(<class '__main__.Hack'>, <class 'type'>)\n>>> isinstance(I, object), isinstance(Hack, object)\n(True, True)\nThis works the same for built-in types (a.k.a. classes), but there is also literal\nsyntax for generating instances:\n>>> I = 'hack'               # Make an instance by literal syntax, or str()\n>>> type(I)                  # Built-in objects are instances of classes\n<class 'str'>\n>>> type(str)                # Built-in classes are instances of type\n<class 'type'>\n>>> I.__class__, str.__class__\n(<class 'str'>, <class 'type'>)\n>>> isinstance(I, object), isinstance(str, object)\n(True, True)\nIn fact, type itself reports in as a class, though it has no type but itself—",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1244,
      "chapter": null,
      "content": "circularly capping the chain:\n>>> type(type)                 # The type class ends the chain\n<class 'type'>\n>>> type(type(type))           # Hmm...the top of the chain\n<class 'type'>\nThis model may seem academic (and to some extent is), but it allows us to\nspecialize built-in types with normal user-defined classes and bears on type-\ntesting code: you must know what a type is to test it accurately.\nSome Instances Are More Equal Than Others\nIt’s tempting to simply take away from the foregoing that classes and types are\nthe same, but this story is richer than that may imply: the instances we make\nfrom these objects diverge in both functionality and inheritance.\nTo truly understand how, we have to briefly factor in metaclasses—classes that\ngenerate other classes. The type built-in itself is a metaclass and may be\ncustomized with user-defined subclasses. These subclasses are coded with\nnormal class statements, selected with special syntax in class headers, and\ndesigned to play metaclass roles, but they won’t be covered in full until\nChapter 40.\nIn brief, though, the complete relationship between instances, classes, and types\nis as follows:\nInstances are created from classes—both built-in and user-defined.\nClasses themselves are created from the built-in type class or one of its\nsubclasses.\nThe object built-in class is a superclass to every object—instance,\nclass, or both.\nAlthough everything is ultimately an “instance” in Python, there are two\nfundamentally different kinds of instances, and conflating these only serves to\nmask the true complexity of the model. It doesn’t help to distinguish these as\ninstances of user-defined classes or not: metaclasses may be user-defined classes\ntoo. Nor is this about being a subclass of a type: the real fork in this model is",
      "content_length": 1785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1245,
      "chapter": null,
      "content": "that classes are created from a type class specially.\nAs you’ll learn in full later, classes define their types with optional metaclass\nsyntax that defaults to type if omitted, but other instances define their types by\nthe class calls or literal syntax we’ve used so far:\nclass C(metaclass=Meta): …     # Class creation: metaclass defaults to \"type\"\nI = C(…)                       # Instance creation: user-defined classes\nX = [1, 2]                     # Instance creation: built-in classes\nWhile both produce instances in some sense, these different syntaxes create very\ndifferent kinds of instances—class and nonclass—with fundamentally different\nbehaviors:\nNonclass instances do not make instances\nClasses are created from type (or another metaclass), similar to the way\ninstances are created from classes. Once created, though, the analogy fails:\nclasses create instances of their own, but nonclass instances do not.\nClasses have an extra inheritance search\nThere are really two inheritance trees and searches in Python, which are\ndistinct but not entirely disjoint. The secondary tree is formed by type and\nits subclasses and is searched only for classes, not nonclass instances.\nIn other words, nonclass instances seal off the instantiation chain, and\ninheritance differs for nonclass instances and classes themselves—even though\nthe latter are also instances of type. All of this boils down to different creation\nsyntax that makes different kinds of objects, which are often confusingly lumped\ntogether as “instances”:\n>>> class C: pass                                # A type instance\n>>> I = C()                                      # A nonclass instance\n>>> isinstance(I, type), isinstance(C, type)     # Only classes are types\n(False, True)",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1246,
      "chapter": null,
      "content": "While types and classes may be synonymous, the instances we create from them\nvary per creation code.\nThe Inheritance Bifurcation\nThough we can’t get into full details here, the inheritance search used for classes\n(a.k.a. types) differs from what we’ve seen so far and may be their most\nprofound distinction. In short, inheritance is always based on the MRO (method\nresolution order) we studied in “Multiple Inheritance and the MRO”, but varies\nas follows:\nNonclass inheritance\nAs we’ve seen, inheritance run on a nonclass instance searches the __dict__\nattributes of instance, class, and superclasses, per the MRO order we studied\nin the prior chapter. This works by first checking the instance, then following\nthe instance’s __class__ to its class, and finally following each class’s\n__bases__ to superclasses. Technically, __bases__ are used to make an\n__mro__ at __class__, which inheritance scans.\nClass inheritance\nInheritance run on a class directly, though, first searches the __dict__ of the\nclass and all its supers available from __bases__ as usual, but then also\nsearches the separate class tree formed by the type class and its metaclass\nsubclasses. The second part of this works by following the class’s own\n__class__ to its type class tree and using __bases__ and MROs there, too\n—but only as a last resort and only for inheritance run on classes.\nIn fact, if you know where to look, you can inspect the inheritance sources that\ndiffer for nonclass instances like I and classes like C in the prior example—\nthough the underscores and displays aren’t pretty:\n>>> isinstance(I, C), type(I)",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1247,
      "chapter": null,
      "content": "(True, <class '__main__.C'>)\n>>> I, I.__class__, I.__class__.__bases__\n(<__main__.C object at 0x101265d60>, <class '__main__.C'>, (<class 'object'>,))\n>>> C, C.__bases__, C.__class__, C.__class__.__bases__\n(<class '__main__.C'>, (<class 'object'>,), <class 'type'>, (<class 'object'>,))\nBut due to the way MROs are computed from __bases__ and scanned, it’s more\naccurate to think of inheritance’s different search orders for nonclass instances\nand classes as follows—where each __mro__ is a flattened tree:\n>>> I, I.__class__.__mro__\n(<__main__.C object at 0x101265d60>, (<class '__main__.C'>, <class 'object'>))\n>>> C.__mro__, C.__class__.__mro__\n((<class '__main__.C'>, <class 'object'>), (<class 'type'>, <class 'object'>))\nAnd because each item’s __dict__ is checked, the ordered set of candidates\nsearched by inheritance for nonclass instances I and classes C is ultimately and\nrespectively as follows—with two MRO scans of flattened trees for classes only,\nand ignoring the fact that some kinds of descriptors, introduced ahead, take\nprecedence in both trees as you’ll learn in Chapter 40:\n[I.__dict__] + [x.__dict__ for x in I.__class__.__mro__]\n[x.__dict__ for x in C.__mro__] + [x.__dict__ for x in C.__class__.__mro__]\nWait—there’s a second tree in inheritance? Well, yes, though it doesn’t come\ninto play in the vast majority of application code. The type/metaclass tree is used\nin advanced class-management roles and, even then, is often limited to class\ncustomization at class creation time.\nStill, this secondary tree, along with the descriptors’ special cases omitted here,\nbifurcates and convolutes the inheritance story, especially compared to its prior\nforms. It also explains why some class attributes like __bases__ are not\ninherited by nonclass instances—they’re located in the secondary tree (i.e.,\nMRO) searched only for classes:\n>>> '__bases__' in I.__dict__                # Not in instance",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1248,
      "chapter": null,
      "content": "False\n>>> '__bases__' in C.__dict__                # Not in instance's class\nFalse\n>>> '__bases__' in C.__class__.__dict__      # In instance's class's class\nTrue\nBecause of the two-tree inheritance model, such names inherited by classes are\nnot inherited by their instances:\n>>> C.__bases__                              # Instance does not inherit!\n(<class 'object'>,)\n>>> I.__bases__\nAttributeError: 'C' object has no attribute '__bases__'. Did you mean: '__class__'?\nThe Metaclass/Class Dichotomy\nSo, where does this odd tale of type/class unification leave us? Types indeed\nbehave as classes, and this allows us to extend them with normal class syntax in\nboth the primary and metaclass trees. But it also comes with noticeable seams,\nincluding special-case syntax for class instantiation, an extra type-tree search for\nclasses only, two very different kinds of instances, and unique semantics for\nmetaclasses that customize types (a.k.a. classes), which we’ll uncover later.\nIn fact, a reasonable argument can be made that the type/class dichotomy of\nearlier Pythons may simply have morphed into one of metaclass/class—which\ntrades a straightforward distinction for all the seams just enumerated and\nmuddles inheritance and the fundamental meaning of names in Python\neverywhere to support what in the end is a very rare use case. As usual, the net\nmerit of the morph is yours to weigh.\nTo be fair, some of the widespread confusion this model has spawned may stem\nfrom type itself: it’s overloaded to either return a sole argument’s type, or\ngenerate a new instance of itself for multiple arguments—just like other\nconstructors and equivalent to what a class statement does to make a class\nobject:\ntype(object)                                     # Fetch object type\ntype(classname, superclasses, attributedict)     # Make a class/type\nThe first of these roles might have been better named “typeof,” but the second",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1249,
      "chapter": null,
      "content": "will have to await the metaclass preview later in this chapter and the extended\ncoverage in Chapter 40.\nAnd One “object” to Rule Them All\nTo round out this topic, keep in mind that because topmost classes inherit from\nthe built-in class object, every object derives (i.e., inherits) from it, whether\ndirectly or through a superclass—and whether you code object or not:\n>>> class C: pass\n>>> class D(object): pass\n>>> dir(C) == dir(D)\nTrue\n>>> C.__bases__, D.__bases__\n((<class 'object'>,), (<class 'object'>,))\nIn fact, the type class inherits from the object class, and object inherits from\ntype, even though the two are different objects—a circular relationship that\ncrowns the object model and may make your cranium catch fire (to avoid\ncombustion, keep in mind that isinstance is true for either a subclass\nrelationship or creation source, though this is based on inheritance through the\nsecondary type-class tree for classes):\n>>> type is object\nFalse\n>>> type(type), type(object)\n(<class 'type'>, <class 'type'>)\n>>> isinstance(type, object), isinstance(object, type)\n(True, True)\n>>> type.__bases__, object.__bases__\n((<class 'object'>,), ())\nStrange though it may seem, this has a number of practical consequences. For\none thing, it means that we sometimes must be aware of the method defaults that\ncome with the implicit (or explicit) object root class. As we noted in earlier\nchapters, for instance, the object class comes with a __repr__ for display:\n>>> class C: pass                     # All classes inherit object defaults\n>>> X = C()",
      "content_length": 1549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1250,
      "chapter": null,
      "content": ">>> X.__repr__\n<method-wrapper '__repr__' of C object at 0x1091a7920>\nFor another, this also allows us to write code that can safely assume and use an\nobject superclass. As an example, we can rely on it to be a call-chain “anchor”\nin some super built-in roles described ahead and can reroute method calls to it\nfrom attribute-interceptor methods to invoke higher default behavior. Per\nChapter 30:\nobject.__setattr__(self, attr, value)\nWe’ll code examples of such rerouting later in the book; for now, let’s move on\nto something a bit more tangible and our next topic in this OOP jamboree.\nAdvanced Attribute Tools\nAlong with the normal class and instance attributes we’ve been using so far,\nPython’s OOP support includes attribute tools of narrower scope—slots,\nproperties, descriptors, and more. Slots, for example, are an optimization option,\nand properties and descriptors allow classes to augment access. None of these\ntools are required, but as for most topics in this chapter, all are fair game in\nPython code you may someday use. Most of these tools get extended coverage in\nChapter 38, but slots get full coverage here, and others are presented in\nabbreviated form.\nSlots: Attribute Declarations\nFirst off, we’ve noted the implications of slots several times in this part of the\nbook. In short, by assigning an iterable of attribute name strings to a special\n__slots__ class attribute, we can enable a class to both limit the set of legal\nattributes that instances of the class will have and optimize memory usage and\npossibly program speed. As you’ll find, though, slots should be used only in\napplications that clearly warrant the added complexity. They will complicate\nyour code, may complicate or break code you may use, and rigidly require\nuniversal deployment to be effective.",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1251,
      "chapter": null,
      "content": "Slot basics\nTo declare slots, assign an iterable (e.g., list) of string names to the special\n__slots__ variable and attribute at the top level of a class statement: only\nthose names in __slots__ can be assigned as instance attributes. This doesn’t\nchange the way these attributes work in general, though; like all names in\nPython, instance attribute names must always be assigned before they can be\nreferenced, even if they’re listed in __slots__. Here are the basics:\n>>> class Limiter(object):\n        __slots__ = ['age', 'name', 'job']                # Slots \"declaration\"\n>>> I = Limiter()\n>>> I.age                                                 # Must assign before use\nAttributeError: 'Limiter' object has no attribute 'age' \n>>> I.age = 40                                            # Looks like instance data\n>>> I.age\n40\n>>> I.ape = 1000                                          # Fails: not in __slots__\nAttributeError: 'Limiter' object has no attribute 'ape'\nThis feature is advertised as both a way to catch typo errors like this\n(assignments to illegal attribute names not in __slots__ are detected instead of\nsilently assigned), as well as an optimization mechanism that saves memory.\nAllocating a namespace dictionary for every instance object can be expensive in\nterms of memory if many instances are created and only a few attributes are\nrequired. To save space, instead of allocating a dictionary for each instance,\nPython reserves just enough space in each instance to hold a value for each slot\nattribute, along with inherited attributes in the common class to manage slot\naccess. This might additionally speed execution, though this benefit may vary\nper program, platform, and Python version (spoiler: the speedup is trivial today,\nas we’ll prove ahead).\nYou shouldn’t normally use slots\nSlots are a fairly major break with Python’s core dynamic nature, which dictates\nthat any name may be created by assignment (and frankly, tend to appeal most to\npeople with backgrounds in draconian languages). In fact, they partly imitate\nC++ for efficiency at the expense of flexibility and even have the potential to",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1252,
      "chapter": null,
      "content": "break some programs.\nAs you’ll see, slots also come with a plethora of special-case usage rules. Per\nPython’s own manual, they should not be used except in clearly warranted cases\n—they are difficult to deploy correctly, complicate your code badly, and are best\nlimited to very rare memory-critical programs that produce an extremely large\nnumbers of instances.\nIn other words, this is yet another feature that should be used only if clearly\njustified. Unfortunately, slots seem to be showing up in Python code much more\noften than they should; their obscurity seems to be a draw in itself. Slots are\nactually used by Python, unlike type hinting, their declaration cousin of\nChapter 6, but they are similarly paradoxical and restrictive. As usual,\nknowledge is your best ally in such things, so let’s take a deeper look here.\nSlots and namespace dictionaries\nPotential benefits aside, slots can complicate a class model—and code that relies\non it—substantially. In fact, some instances with slots may not have a __dict__\nattribute namespace dictionary at all, and others will have data attributes that this\ndictionary does not include. To be clear, this is a major incompatibility with the\ntraditional class model—one that can impact any code that accesses attributes\ngenerically and may even cause some to fail altogether.\nFor instance, programs that list or access instance attributes by name string may\nneed to use more storage-neutral interfaces than __dict__ if slots may be used.\nBecause an instance’s data may include class-level names such as slots—either\nin addition to or instead of namespace dictionary storage—both attribute sources\nmay need to be queried for completeness, and some roles may be rendered\nimpossible.\nLet’s see what this means in terms of code and explore more about slots along\nthe way. First off, when slots are used, instances do not normally have an\nattribute dictionary—instead, Python uses the class descriptors feature\nintroduced ahead to allocate and manage space reserved for slot attributes in the\ninstance:\n>>> class C:\n        __slots__ = ['a', 'b']           # __slots__ means no __dict__ by default",
      "content_length": 2140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1253,
      "chapter": null,
      "content": ">>> I = C()\n>>> I.a = 1\n>>> I.a\n1\n>>> I.__dict__\nAttributeError: 'C' object has no attribute '__dict__'. Did you mean: '__dir__'?\nHowever, we can still fetch and set slot-based attributes by name string using\nstorage-neutral tools such as getattr and setattr (which look beyond the\ninstance __dict__ and thus include class-level names like slots) and list them\nwith dir (which collects all inherited names of any kind throughout a class tree):\n>>> getattr(I, 'a')\n1\n>>> setattr(I, 'b', 2)                   # But getattr() and setattr() still work\n>>> I.b\n2\n>>> 'a' in dir(I)                        # And dir() finds slot attributes too\nTrue\n>>> 'b' in dir(I)                        # Though __dict__ access will fail\nTrue\n>>> I.__dict__\nAttributeError: 'C' object has no attribute '__dict__'. Did you mean: '__dir__'?\nAlso keep in mind that without an attribute namespace dictionary, it’s not\npossible to assign new names to instances that are not names in the slots list:\n>>> class D:                             \n        __slots__ = ['a', 'b']\n        def __init__(self):\n            self.d = 4                   # Cannot add new names if no __dict__\n>>> I = D()\nAttributeError: 'D' object has no attribute 'd'\nWe can still accommodate extra attributes, though, by including __dict__\nexplicitly in __slots__ in order to create an attribute namespace dictionary in\naddition to slots:\n>>> class D:\n        __slots__ = ['a', 'b', '__dict__']    # Name __dict__ to include one too\n        c = 3                                 # Class attrs work normally",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1254,
      "chapter": null,
      "content": "def __init__(self):\n            self.d = 4                        # d stored in __dict__, a is a slot\n>>> I = D()\n>>> I.d\n4\n>>> I.c\n3\n>>> I.a                          # All instance attrs undefined until assigned\nAttributeError: 'D' object has no attribute 'a' \n>>> I.a = 1\n>>> I.b = 2\nIn this case, both storage mechanisms are used. This renders __dict__ too\nlimited for code that wishes to treat slots as instance data, but generic tools such\nas getattr still allow us to process both storage forms as a single set of\nattributes:\n>>> I.__dict__                   # Some objects have both __dict__ and slot names\n{'d': 4}                         # getattr() can fetch either type of attr\n>>> I.__slots__\n['a', 'b', '__dict__']\n>>> getattr(I, 'a'), getattr(I, 'c'), getattr(I, 'd')    # Fetches all 3 forms\n(1, 3, 4)\nBecause dir also returns all inherited attributes, though, it might be too broad in\nsome contexts; it also includes class-level methods and even all object defaults.\nCode that wishes to list just instance attributes may, in principle, still need to\nallow for both storage forms explicitly. We might at first naively code this as\nfollows:\n>>> for attr in list(I.__dict__) + I.__slots__:          # Wrong...\n        print(attr, '=>', getattr(I, attr))\nSince either can be omitted, we may more correctly code this as follows, using\ngetattr to allow for defaults—a noble but nonetheless inaccurate approach, as\nthe next section will explain:\n>>> for attr in list(getattr(I, '__dict__', [])) + getattr(I, '__slots__', []):\n        print(attr, '=>', getattr(I, attr))\nd => 4",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1255,
      "chapter": null,
      "content": "a => 1                                                   # Less wrong...\nb => 2\n__dict__ => {'d': 4}\nMultiple __slot__ lists in superclasses\nThe preceding code works in this specific case, but in general, it’s not entirely\naccurate. Specifically, this code addresses only slot names in the lowest\n__slots__ attribute inherited by an instance, but slot lists may appear more than\nonce in a class tree. That is, a name’s absence in the lowest __slots__ list does\nnot preclude its existence in a higher __slots__. Because slot names become\nclass-level attributes, instances acquire the union of all slot names anywhere in\nthe tree by the normal inheritance rule:\n>>> class E:\n        __slots__ = ['c', 'd']            # Superclass has slots\n>>> class D(E):\n        __slots__ = ['a', '__dict__']     # But so does its subclass\n>>> I = D()                               # The instance gets the union of each\n>>> dir(I)\n[…names omitted…, 'a', 'c', 'd']\n>>> I.a = 1; I.b = 2; I.c = 3             # slots: a, c, __dict__: b\n>>> I.a, I.c\n(1, 3)\nBut inspecting just the inherited slots list won’t pick up slots defined higher in a\nclass tree:\n>>> E.__slots__                           # But __slots__ not concatenated\n['c', 'd']\n>>> D.__slots__\n['a', '__dict__']\n>>> I.__slots__                           # Instance inherits *lowest* __slots__\n['a', '__dict__']\n>>> I.__dict__                            # And has its own attr dict\n{'b': 2}\n>>> for attr in list(getattr(I, '__dict__', [])) + getattr(I, '__slots__', []):\n        print(attr, '=>', getattr(I, attr))\nb => 2                                    # Other superclass slots missed!\na => 1",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1256,
      "chapter": null,
      "content": "__dict__ => {'b': 2}\n>>> dir(I)                                # But dir() includes all slot names\n[…names omitted…, 'a', 'b', 'c', 'd']\nIn other words, in terms of listing instance attributes generically, one __slots__\nisn’t always enough—they are potentially subject to the full inheritance search\nprocedure. If multiple classes in a class tree may have their own __slots__\nattributes, tools must develop other policies for listing attributes—as the next\nsection explains.\nHandling slots and other “virtual” attributes generically\nThe prior chapter concluded with a brief summary of the slots policies of its\nattribute lister tools—a prime example of why generic programs may need to\ncare about slots. Such tools that attempt to list instance data attributes\ngenerically must account for slots and perhaps other such “virtual” instance\nattributes like properties and descriptors introduced ahead—names that similarly\nreside in classes but may provide attribute values for instances on request. Slots\nare the most data-centric of these but are representative of a larger category.\nSuch attributes require inclusive approaches, special handling, or general\navoidance—the latter of which becomes unsatisfactory as soon as any\nprogrammer uses slots in subject code. Really, class-level instance attributes like\nslots probably necessitate a redefinition of the term instance data—as locally\nstored attributes, the union of all inherited attributes, or some subset thereof.\nFor example, some programs might classify slot names as attributes of classes\ninstead of instances; these attributes do not exist in instance namespace\ndictionaries, after all. Alternatively, as shown earlier, programs can be more\ninclusive by relying on dir to fetch all inherited attribute names and getattr to\nfetch their corresponding values—without regard to their physical location or\nimplementation. If you must support slots as instance data, this may be the most\nrobust way to proceed:\n>>> class Slotful:\n        __slots__ = ['a', 'b', '__dict__']\n        def __init__(self, data):\n            self.c = data",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1257,
      "chapter": null,
      "content": ">>> I = Slotful(3)\n>>> I.a, I.b = 1, 2\n>>> I.a, I.b, I.c                            # Normal attribute fetch\n(1, 2, 3)\n>>> I.__dict__                               # Both __dict__ and slots storage\n{'c': 3}\n>>> [x for x in dir(I) if not x.startswith('__')]\n['a', 'b', 'c']\n>>> I.__dict__['c']                          # __dict__ is only one attr source\n3\n>>> getattr(I, 'c'), getattr(I, 'a')         # dir+getattr is broader than __dict__\n(3, 1)                                       # applies to slots, properties, descrip\n>>> for a in (x for x in dir(I) if not x.startswith('__')):\n        print(a, '=>', getattr(I, a))\na 1\nb 2\nc 3\nUnder this dir/getattr model, you can still map attributes to their inheritance\nsources and filter them more selectively by source or type, if needed, by\nscanning the MRO—as we did in the prior chapter’s mapattrs.py (Example 31-\n14). As a bonus, such tools and policies for handling slots will potentially apply\nautomatically to properties and descriptors too, though these attributes are more\nexplicitly computed values, and less obviously instance-related data than slots.\nAlso keep in mind that this is not just a tools issue. Class-based instance\nattributes like slots also impact the traditional coding of the __setattr__\noperator-overloading method we met in Chapter 30. Because slots and some\nother attributes are not stored in the instance __dict__, and may even imply its\nabsence, classes must instead generally run attribute assignments by rerouting\nthem to the object superclass.\nSlot usage rules\nSlot declarations can appear in multiple classes in a class tree, but when they do,\nthey are subject to a number of constraints that are somewhat difficult to\nrationalize unless you understand the implementation of slots as class-level\ndescriptors for each slot name that are inherited by the instances in which the",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1258,
      "chapter": null,
      "content": "managed space is reserved (again, you’ll meet descriptors briefly ahead). Here\nare the main constraints that slots impose:\nSlots in subs are pointless when absent in supers. If a subclass\ninherits from a superclass without a __slots__, the instance __dict__\nattribute created for the superclass will always be accessible, making a\n__slots__ in the subclass largely pointless. The subclass still manages\nits slots but doesn’t compute their values in any way and doesn’t avoid a\ndictionary—the main reason to use slots.\nSlots in supers are pointless when absent in subs. Similarly, because\nthe meaning of a __slots__ declaration is limited to the class in which\nit appears, subclasses will produce an instance __dict__ if they do not\ndefine a __slots__, rendering a __slots__ in a superclass largely\npointless.\nRedefinition renders super slots pointless. If a class defines the same\nslot name as a superclass, its redefinition hides the slot in the superclass\nper normal inheritance. You can access the version of the name defined\nby the superclass slot only by fetching its descriptor directly from the\nsuperclass.\nSlots prevent class-level defaults. Because slots are implemented as\nclass-level descriptors (along with per-instance space), you cannot use\nclass attributes of the same name to provide defaults as you can for\nnormal instance attributes: assigning the same name in the class\noverwrites the slot descriptor.\nSlots cannot be combined in multiple inheritance. Multiple\ninheritance cannot be used if more than one of the classes mixed\ntogether have nonempty slots lists—even if their slots define the same\nnames. You’ll get an error when running the class that does the mixing.\nEmpty slots lists allow the mixer to define slots or not, as desired.\nSlots can impact __dict__. As shown earlier, __slots__ preclude\nboth an instance __dict__ and assigning names not listed, unless\n__dict__ is listed explicitly too. Slots similarly preclude a\n__weakref__ attribute used to support instance “weak references”",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1259,
      "chapter": null,
      "content": "covered briefly in Chapter 6, but these are rare enough to soft-pedal\nhere.\nWe’ve already seen the last of these in action. It’s easy to demonstrate how the\nnew rules here translate to actual code—most crucially, a namespace dictionary\nis created when any class in a tree omits slots, thereby negating the memory\noptimization benefit but also supporting classes that require a __dict__ when\nmixed in with others:\n>>> class C: pass                        # Bullet 1: slots in sub but not super\n>>> class D(C): __slots__ = ['a']        # Makes instance dict for nonslots\n>>> I = D()                              # But slot name still managed in class\n>>> I.a = 1; I.b = 2\n>>> I.__dict__\n{'b': 2}\n>>> D.__dict__.keys()\ndict_keys([… '__slots__', 'a', …])\n>>> class C: __slots__ = ['a']           # Bullet 2: slots in super but not sub\n>>> class D(C): pass                     # Makes instance dict for nonslots\n>>> I = D()                              # But slot name still managed in class\n>>> I.a = 1; I.b = 2\n>>> I.__dict__\n{'b': 2}\n>>> C.__dict__.keys()\ndict_keys([… '__slots__', 'a', …])\n>>> class C: __slots__ = ['a']           # Bullet 3: only lowest slot accessible\n>>> class D(C): __slots__ = ['a']        # Superclass slot 'a' is pointless\n>>> class C: __slots__ = ['a']; a = 99   # Bullet 4: no class-level defaults\nValueError: 'a' in __slots__ conflicts with class variable\n>>> class C: __slots__ = ['a']           # Bullet 5: only one nonempty in mixins\n>>> class D: __slots__ = ['a']           # Use empty slots or omit in all but one\n>>> class E(C, D): pass\nTypeError: multiple bases have instance lay-out conflict\nIn other words, besides their program-breaking potential, slots essentially require\nboth universal and careful deployment to be effective—because slots do not\ncompute values dynamically like properties (coming up in the next section), they\nare largely pointless unless each class in a tree uses them and is cautious to\ndefine only new slot names not defined by other classes. It’s an all-or-nothing\nfeature—an unfortunate property shared by the super call discussed ahead:",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1260,
      "chapter": null,
      "content": ">>> class C: __slots__ = ['a']           # Assumes universal use, differing names\n>>> class D(C): __slots__ = ['b']\n>>> I = D()                              # And may break code and tools you use\n>>> I.a = 1; I.b = 2\n>>> I.__dict__\nAttributeError: 'D' object has no attribute '__dict__'. Did you mean: '__dir__'?\n>>> C.__dict__.keys(), D.__dict__.keys()\n(dict_keys([… '__slots__', 'a', …]), dict_keys([… '__slots__', 'b', …]))\nSuch rules—and others omitted here for space—are part of the reason slots are\nnot widely used and are not generally recommended except in pathological cases\nwhere their space reduction is significant. Even then, their potential to\ncomplicate or break code should be ample cause to carefully consider the trade-\noffs. Not only must they be spread almost neurotically throughout a framework,\nbut they may also break tools you rely on.\nExample impacts of slots: ListTree and mapattrs\nAs a more realistic example of slots’ effects, due to the first bullet in the prior\nsection, Chapter 31’s ListTree class (Example 31-13) does not fail when mixed\ninto a class that defines __slots__, even though it scans instance namespace\ndictionaries without verifying their presence. This lister class’s own lack of slots\nis enough to ensure that the instance will still have a __dict__ and hence not\ntrigger an exception when fetched or indexed.\nFor example, both of the following single-inheritance trees display without error\n—the second also allows names not in the slots list to be assigned as instances\nattributes, including any required by the superclass:\nclass C(ListTree): pass\nI = C()                                        # OK: no __slots__ used\nprint(I)\nclass C(ListTree): __slots__ = ['a', 'b']      # OK: superclass produces __dict__\nI = C()\nI.c = 3\nprint(I)                                       # Displays c at I, a and b at C\nThe following multiple-inheritance classes display correctly as well—any\nnonslot class like ListTree generates an instance __dict__ and can thus safely\nassume its presence. Although it renders subclass slots pointless, this is a",
      "content_length": 2082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1261,
      "chapter": null,
      "content": "positive side effect for tool classes like ListTree and its Chapter 28\npredecessor:\nclass A: __slots__ = ['a']                     # Both OK by bullet 1 above\nclass B(A, ListTree): pass\nprint(B())\nclass A: __slots__ = ['a']\nclass B(A, ListTree): __slots__ = ['b']        # Displays b at B, a at A\nprint(B())\nIn general, though, tools may need to catch exceptions when __dict__ is absent\nor use a hasattr or getattr to test or provide defaults if slot usage may\npreclude an instance namespace dictionary. For instance, Chapter 31’s\nmapattrs.py module (Example 31-14) must check for __dict__ presence\nexplicitly because it is not a class mixed into others, and so cannot assume this\nattribute. Like ListTree, this example also associates slots with their classes.\nRun these examples on your own for more info. Slots’ impacts may be onerous,\nbut knowledge is your best defense.\nWhat about slots speed?\nFinally, while slots primarily optimize memory use, their speed impact is less\nclear-cut. Example 32-4 codes a simple test script using the timeit techniques\nwe studied in Chapter 21. For both the slots and nonslots (instance dictionary)\nstorage models, it makes 1,000 instances, assigns and fetches 4 attributes on\neach, and repeats 1,000 times—for both models taking the best of 5 runs that\neach exercise a total of 8M attribute operations.\nExample 32-4. slots-test.py\nimport timeit\nbase = \"\"\"\nIs = []\nfor i in range(1000):\n   I = C()\n   I.a = 1; I.b = 2; I.c = 3; I.d = 4\n   t = I.a + I.b + I.c + I.d\n   Is.append(I)\n\"\"\"\nstmt = \"\"\"",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1262,
      "chapter": null,
      "content": "class C:\n   __slots__ = ['a', 'b', 'c', 'd']\n\"\"\" + base\nprint('Slots   =>', end=' ')\nprint(min(timeit.repeat(stmt, number=1000, repeat=5)))\nstmt = \"\"\"\nclass C:\n   pass\n\"\"\" + base\nprint('Nonslots=>', end=' ')\nprint(min(timeit.repeat(stmt, number=1000, repeat=5)))\nAt least for this code, on the macOS test host, and using CPython 3.12, the best\ntimes imply that slots are only slightly quicker, though this says little about\nmemory space and is prone to change arbitrarily in the future (PyPy 7.3\nstruggled on this test with times 10x slower than CPython, presumably due to\ndynamic class creation, but relatively similar):\n$ python3 slots-test.py\nSlots   => 0.17895982996560633\nNonslots=> 0.18887511501088738\nFor more on slots in general, see the Python standard manual set. Also, watch for\nthe Private decorator case study of Chapter 39—an example that naturally\nallows for attributes based on both __slots__ and __dict__ storage, by using\ndelegation and storage-neutral accessor tools like getattr.\nProperties: Attribute Accessors\nOur next attribute-related topic is properties—a mechanism that provides\nanother way for classes to define methods called automatically for access or\nassignment to instance attributes. This feature is similar to “getters” and “setters”\nin languages like Java and C#, but in Python is generally best used sparingly as a\nway to add accessors to attributes after the fact as needs evolve and warrant.\nWhere needed, though, properties allow attribute values to be computed\ndynamically without requiring method calls at the point of access.\nThough properties cannot support generic attribute routing goals, at least for\nspecific attributes, they are an alternative to some traditional uses of the\n__getattr__ and __setattr__ overloading methods we first studied in",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1263,
      "chapter": null,
      "content": "Chapter 30. Properties can have a similar effect to these two methods but, by\ncontrast, incur an extra method call only for accesses to names that require\ndynamic computation—other nonproperty names are accessed normally with no\nextra calls. Although __getattr__ is invoked only for undefined names, the\n__setattr__ method is instead called for assignment to every attribute.\nProperties and slots are related, too, but serve different goals. Both implement\ninstance attributes that are not physically stored in instance namespace\ndictionaries—a sort of “virtual” attribute—and both are based on the notion of\nclass-level attribute descriptors. In contrast, slots manage instance storage, while\nproperties intercept access and compute values arbitrarily. Because their\nunderlying descriptor implementation tool is too advanced for us to cover here,\nproperties and descriptors both get full treatment in Chapter 38.\nProperty basics\nAs a brief introduction, though, a property is a type of object assigned to a class\nattribute name. You can generate a property by calling the property built-in\nfunction, passing in up to three accessor methods—handlers for get, set, and\ndelete operations—as well as an optional docstring for the property. If any\nargument is passed as None or omitted, that operation is not supported.\nThe resulting property object is typically assigned to a name at the top level of a\nclass statement as a class attribute (e.g., name=property(…)), and a special @\ndecorator syntax you’ll meet later is available to automate this step. When thus\nassigned, later accesses to the class property name itself as an object attribute\n(e.g., obj.name) are automatically routed to one of the accessor methods passed\ninto the property call.\nFor example, we’ve seen how the __getattr__ operator-overloading method\nallows classes to intercept undefined attribute references:\n>>> class WithOperators:\n        def __getattr__(self, name):   # On undefined attr\n            if name == 'age':\n                return 40\n            else:\n                raise AttributeError(name)\n>>> x = WithOperators()",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1264,
      "chapter": null,
      "content": ">>> x.age                         # Runs __getattr__\n40\n>>> x.name                        # Runs __getattr__\nAttributeError: name\nHere is the same example, coded with properties instead:\n>>> class WithProperties:\n        def getage(self):\n            return 40\n        age = property(getage)    # (get?, set?, del?, docs?), or @\n>>> x = WithProperties()\n>>> x.age                         # Runs getage\n40\n>>> x.name                        # Normal fetch\nAttributeError: 'WithProperties' object has no attribute 'name'\nFor some coding tasks, properties can be less complex and quicker to run than\nthe traditional techniques. For example, when we add attribute assignment\nsupport, properties become more attractive—there’s less code to type, and no\nextra method calls are incurred for assignments to attributes we don’t wish to\nmanage or compute dynamically:\n>>> class WithProperties:    \n        def getage(self):\n            print('get age')            \n            return 40\n        def setage(self, value):\n            print('set age:', value)\n            self._age = value\n        age = property(getage, setage)\n>>> x = WithProperties()\n>>> x.age                         # Runs getage\nget age\n40\n>>> x.age = 42                    # Runs setage\nset age: 42\n>>> x._age                        # Normal fetch:  no getage call\n42\n>>> x.job = 'hacker'              # Normal assign: no setage call\n>>> x.job                         # Normal fetch:  no getage call\n'hacker'",
      "content_length": 1468,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1265,
      "chapter": null,
      "content": "The equivalent class based on operator overloading incurs extra method calls for\nassignments to attributes not being managed and needs to route attribute\nassignments through the attribute dictionary to avoid loops (or to the object\nsuperclass’s __setattr__ to better support “virtual” attributes such as slots and\nproperties coded in other classes):\n>>> class WithOperators:\n        def __getattr__(self, name):            # On undefined attr\n            if name == 'age':\n                print('get age')\n                return 40\n            else:\n                raise AttributeError(name)\n        def __setattr__(self, name, value):     # On all assignments\n            print('set:', name, value)\n            if name == 'age':\n                self.__dict__['_age'] = value   # Or object.__setattr__(self, ...)\n            else:\n                self.__dict__[name] = value\n>>> x = WithOperators()\n>>> x.age                         # Runs __getattr__\nget age\n40\n>>> x.age = 41                    # Runs __setattr__\nset: age 41\n>>> x._age                        # Defined: no __getattr__ call\n41\n>>> x.job = 'coder'               # Runs __setattr__ again\nset: job coder\n>>> x.job                         # Defined: no __getattr__ call\n'coder'\nProperties seem like a win for this simple example. However, some applications\nof __getattr__ and __setattr__ still require more dynamic or generic\ninterfaces than properties directly provide.\nFor example, the set of attributes to be managed might be unknown when a class\nis coded and may not even exist in a tangible form (e.g., when delegating\narbitrary attribute references to a wrapped and embedded object generically). In\nsuch contexts, a generic attribute handler like __getattr__ with a passed-in\nattribute name may be preferable. Because such generic handlers can also\nsupport simpler cases, properties may be a redundant extension—albeit one that",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1266,
      "chapter": null,
      "content": "may avoid extra calls on assignments and one that some programmers may\nprefer when applicable.\nFor more details on both options, tune in to Chapter 38. As you’ll see there, it’s\nalso possible to code properties using the @ symbol function decorator syntax—a\ntopic introduced in brief later in this chapter and an equivalent and automatic\nalternative to manual assignment in the class scope:\nclass WithProperties:\n    @property                     # Coding properties with decorators: ahead\n    def age(self):                # On instance.age\n        …\n    @age.setter\n    def age(self, value):         # On instance.age = value\n        …\nTo make sense of this decorator syntax, though, we must move ahead.\n__getattribute__ and Descriptors: Attribute\nImplementations\nTo complete our attribute-tools collection, the __getattribute__ operator-\noverloading method intercepts all attribute references, not just undefined\nreferences. This makes it more potent than its __getattr__ cousin we used in\nthe prior section, but also trickier to use—it’s prone to loops much like\n__setattr__, but in different ways.\nFor more specialized attribute interception goals, in addition to properties and\noperator-overloading methods, Python provides attribute descriptors—classes\nwith __get__ and __set__ methods, assigned to class attributes and inherited by\ninstances, that intercept read and write accesses to specific attributes. As a\npreview, here’s one of the simplest descriptors you’re likely to encounter:\n>>> class AgeDesc:\n        def __get__(self, instance, owner): return 40\n        def __set__(self, instance, value): instance._age = value\n>>> class WithDescriptors:\n        age = AgeDesc()           # Assign descriptor instance\n>>> x = WithDescriptors()",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1267,
      "chapter": null,
      "content": ">>> x.age                         # Runs AgeDesc.__get__\n40\n>>> x.age = 42                    # Runs AgeDesc.__set__\n>>> x._age                        # Normal fetch: no AgeDesc call\n42\nDescriptors have access to state-information attributes in instances of themselves\nas well as their client class and are, in a sense, a more general form of properties.\nIn fact, properties are a simplified way to define a specific type of descriptor—\none that runs functions on access. Descriptors are also used to implement the\nslots feature we met earlier, among other Python tools, and are afforded special\ncases in attribute inheritance alluded to earlier in this chapter.\nBecause __getattribute__ and descriptors are too substantial to present here,\nwe’ll defer the rest of their coverage, as well as much more on properties, to\nChapter 38. We’ll also employ them in examples in Chapter 39 and study how\nthey factor into inheritance in Chapter 40. Here, the topics tour is moving on.\nStatic and Class Methods\nBeyond the usual methods we’ve been using so far, classes can define two kinds\nof methods called without an instance: static methods work roughly like simple\ninstance-less functions inside a class no matter how they’re called, and class\nmethods are passed a class instead of an instance. Both are similar to tools in\nother languages (e.g., C++ static methods). The prior chapter’s bound method\ncoverage noted these briefly, but we’ll finish the story here.\nTo enable these special method modes, you call built-in functions named\nstaticmethod and classmethod within the class or invoke them with the\nspecial @name decoration syntax you’ll meet later in this chapter. The\nclassmethod call is required to enable its mode; staticmethod is not required\nfor instance-less methods called only through a class name but is required if such\nmethods are called through instances.\nWhy the Special Methods?\nAs we’ve seen, a class’s method is normally passed an instance object in its first\nargument to serve as the implied subject of the method call—that’s the “object”",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1268,
      "chapter": null,
      "content": "in “object-oriented programming.” Though much less common, there are two\nformal ways to temper this model. Before we get to the syntax, let’s clarify why\nthis might matter to you.\nSometimes, programs need to process data associated with classes instead of\ninstances. Consider keeping track of the number of instances created from a\nclass or maintaining a list of all of a class’s instances that are currently in use.\nThis type of information and its processing are associated with the class rather\nthan its instances. That is, the information is usually stored on the class itself and\nprocessed apart from any instance.\nFor such tasks, simple functions coded outside a class might suffice—because\nthey can access class attributes through the class name, they have access to class\ndata, and never require access to an instance. However, to better associate such\ncode with a class and to allow such processing to be customized with inheritance\nas usual, it would be better to code these types of functions inside the class itself.\nTo make this work, we need methods in a class that are not passed, and do not\nexpect, a self instance argument.\nPer the prior chapter, methods accessed through the class are plain functions that\nmeet some of this need but fail if accessed through an instance: the resulting\nbound method passes an instance in calls, even if the plain function doesn’t\nexpect one. To address, Python provides static methods—plain functions that are\nnested in a class and never expect nor receive an automatic self argument,\nregardless of how they are called. They’re optional for methods only ever\naccessed through classes but needed for access through instances.\nAlthough less commonly used, Python also supports class methods—methods of\na class that are passed a class object in their first argument instead of an instance,\nregardless of whether they are called through an instance or a class. Such\nmethods can access class data through their class argument—what we’ve called\nself thus far—even if called through an instance. Normal methods, sometimes\ncalled instance methods, still receive a subject instance when called; static and\nclass methods do not.\nPlain-Function Methods\nTo demo the preceding ideas, let’s suppose that we want to use class attributes to",
      "content_length": 2274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1269,
      "chapter": null,
      "content": "count how many instances are generated from a class. Example 32-5, hack1.py,\nmakes a first attempt—its class has a counter stored as a class attribute, a\nconstructor that bumps up the counter by one each time a new instance is\ncreated, and a method that displays the counter’s value. Remember, class\nattributes are stored just once on a class and shared by all instances; storing the\ncounter this way ensures that it effectively spans all instances.\nExample 32-5. hack1.py\nclass Hack:\n   numInstances = 0\n   def __init__(self):\n       Hack.numInstances += 1\n   def printNumInstances():\n       print('Number of instances created:', Hack.numInstances)\nThe printNumInstances method is designed to process class data, not instance\ndata—it’s about all the instances, not any one in particular. Because of that, we\nwant to be able to call it without having to pass an instance. Indeed, we don’t\nwant to make an instance to fetch the number of instances because this would\nchange the number of instances we’re trying to fetch! In other words, we want a\nself-less “static” method.\nWhether this code’s printNumInstances works or not, though, depends on\nwhich way you call the method—through the class or through an instance. Calls\nto self-less methods made through classes work because they produce plain\nfunctions, but calls from instances produce bound methods and fail:\n$ python3\n>>> from hack1 import Hack\n>>> a, b, c = Hack(), Hack(), Hack()     # Make three instances\n>>> Hack.printNumInstances()             # Okay to call from instance – only!\nNumber of instances created: 3\n>>> a.printNumInstances()\nTypeError: Hack.printNumInstances() takes 0 positional arguments but 1 was given\nCalls to instance-less methods like printNumInstances made through the class\nwork, but calls made through an instance fail because an instance is\nautomatically passed to a method that does not have an argument to receive it. If\nyou’re able to stick with calling self-less methods through classes only, you",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1270,
      "chapter": null,
      "content": "already have a static method. However, to allow self-less methods to be called\nthrough instances, you need to either adopt other designs or mark such methods\nas special. Let’s look at both options in turn.\nStatic Method Alternatives\nShort of marking a self-less method as special, you can sometimes achieve\nsimilar results with different coding structures. For example, if you just want to\ncall functions that access class members without an instance, perhaps the\nsimplest idea is to use normal functions outside the class, not class methods.\nThis way, an instance isn’t expected in the call. The mutation in Example 32-6\nillustrates.\nExample 32-6. hack2.py\ndef printNumInstances():\n   print('Number of instances created:', Hack.numInstances)\nclass Hack:\n   numInstances = 0\n   def __init__(self):\n       Hack.numInstances += 1\nBecause the class name is accessible to the simple function as a global variable,\nthis works fine. Also, note that the name of the function becomes global, but\nonly to this single module; it will not clash with names in other files:\n>>> import hack2 as hack\n>>> a = hack.Hack()\n>>> b = hack.Hack()\n>>> c = hack.Hack()\n>>> hack.printNumInstances()           # But function may be too far removed\nNumber of instances created: 3         # And cannot be changed via inheritance\n>>> hack.Hack.numInstances\n3\nPrior to static methods in Python, this structure was the general prescription.\nBecause Python already provides modules as a namespace-partitioning tool, one\ncould argue that there’s not typically any need to package functions in classes\nunless they implement object behavior. Simple functions within modules like the\none here do much of what instance-less class methods could and are already\nassociated with the class because they live in the same module.",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1271,
      "chapter": null,
      "content": "This approach, though, may be subpar. For one thing, it adds to this file’s scope\nan extra name that is used only for processing a single class. For another, the\nfunction is not directly associated with the class by structure; in fact, its def\ncould be hundreds of lines away. Worse, simple functions like this cannot be\ncustomized by inheritance since they live outside a class’s namespace:\nsubclasses cannot directly replace or extend such a function by redefining it.\nWe might also try to make this example work by simply using a normal method\nand always calling it through an instance, as usual. Unfortunately, such an\napproach is completely unworkable if we don’t have an instance available, and\nmaking an instance changes the class data, as noted earlier. A better solution\nwould be to somehow mark a method inside a class as never requiring an\ninstance. The next section shows how.\nUsing Static and Class Methods\nTo designate a self-less method that may be called through either the class or\nits instances, classes can simply call the built-in functions staticmethod and\nclassmethod. Both mark a function object as special—requiring no instance for\nthe former and requiring a class argument for the latter. Example 32-7 shows\nhow.\nExample 32-7. allmethods.py\nclass Methods:\n   def imeth(self, x):            # Instance method: passed a self\n       print([self, x])           # Always expects a self instance\n   def smeth(x):                  # Static method: no instance passed\n       print([x])                 # Also a plain function from the class\n   def cmeth(cls, x):             # Class method: gets class, not instance\n       print([cls, x])            # Always expects a class, not instance\n   smeth = staticmethod(smeth)    # Make smeth a static method (or use @: ahead)\n   cmeth = classmethod(cmeth)     # Make cmeth a class method (or use @: ahead)\nNotice how the last two assignments in this code simply reassign (a.k.a. rebind)\nthe method names smeth and cmeth. Attributes are created and changed by any\nassignment in a class statement, so these final assignments simply overwrite the\nassignments made earlier by the defs. As you’ll see in a few moments, the",
      "content_length": 2178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1272,
      "chapter": null,
      "content": "special @ decorator syntax works here as an alternative to this just as it does for\nproperties—but makes little sense unless you first understand the assignment\nform here that it automates.\nTechnically, Python supports three kinds of class-related methods with differing\nargument protocols:\nInstance methods, passed a self instance object (the default)\nStatic methods, passed no extra instance object (via staticmethod)\nClass methods, passed a class object (via classmethod, and inherent in\nmetaclasses)\nMoreover, simple functions in a class also serve the role of static methods\nwithout requiring any extra protocol when called through a class object only. The\nallmethods.py module illustrates all three method types, so let’s expand on these\nin turn.\nInstance methods are the normal and default case that we’ve used in this book so\nfar. An instance method must always be called with an instance object. When\nyou call it through an instance, Python passes the instance to the first (leftmost)\nargument automatically; when you call it through a class, you must pass along\nthe instance manually:\n>>> from allmethods import Methods     # Normal instance methods\n>>> obj = Methods()                    # Callable through instance or class\n>>> obj.imeth(1)                       # Becomes imeth(obj, 1)\n[<allmethods.Methods object at 0x1015a79b0>, 1] \n>>> Methods.imeth(obj, 2)\n[<allmethods.Methods object at 0x1015a79b0>, 2]\nStatic methods, by contrast, are called without an instance argument. Unlike\nsimple functions outside a class, their names are local to the scopes of the classes\nin which they are defined, and they may be looked up by inheritance. Instance-\nless functions can be called through a class normally, but using the\nstaticmethod built-in allows such methods to also be called through an\ninstance. That is, the first of the following works without the staticmethod in\nthe class but the second does not:",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1273,
      "chapter": null,
      "content": ">>> Methods.smeth(3)                   # Static method: call through class\n[3]                                    # No instance passed or expected\n>>> obj.smeth(4)                       # Static method: call through instance\n[4]                                    # Instance not passed – requires staticmethod\nClass methods are similar, but Python automatically passes the class (not an\ninstance) to a class method’s first (leftmost) argument, whether it is called\nthrough a class or an instance:\n>>> Methods.cmeth(5)                   # Class method: call through class\n[<class 'allmethods.Methods'>, 5]      # Becomes cmeth(Methods, 5)\n>>> obj.cmeth(6)                       # Class method: call through instance\n[<class 'allmethods.Methods'>, 6]      # Becomes cmeth(Methods, 6)\nIn Chapter 40, you’ll also find that metaclass methods—an advanced and\ntechnically distinct method type used in the secondary class trees of types—\nbehave similarly to the explicitly declared class methods we’re exploring here.\nCounting Instances with Static Methods\nNow, given these built-ins, Example 32-8 codes the static method equivalent of\nthis section’s instance-counting example—it marks the method as special, so it\nwill never be passed an instance automatically.\nExample 32-8. hack_static.py\nclass Hack:\n   numInstances = 0                         # Use static method for class data\n   def __init__(self):\n       Hack.numInstances += 1\n   def printNumInstances():\n       print('Number of instances:', Hack.numInstances)\n   printNumInstances = staticmethod(printNumInstances)\nUsing the static method built-in, our code now allows the self-less method to be\ncalled through the class or any instance of it:\n>>> from hack_static import Hack\n>>> a, b, c = Hack(), Hack(), Hack()\n>>> Hack.printNumInstances()                 # Call as simple function\nNumber of instances: 3\n>>> a.printNumInstances()                    # Instance argument not passed\nNumber of instances: 3",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1274,
      "chapter": null,
      "content": "Compared to simply moving printNumInstances outside the class, as\nprescribed earlier, this version requires an extra staticmethod call (or an @ line\nyou’ll meet ahead). However, it also localizes the function name in the class\nscope (so it won’t clash with other names in the module); moves the function\ncode closer to where it is used (inside the class statement); and allows\nsubclasses to customize the static method with inheritance—a more convenient\nand powerful approach than importing functions from the files in which\nsuperclasses are coded. The following subclass illustrates (this continues the\nprior session, so the count is already 3 at the start):\n>>> class Sub(Hack):\n        def printNumInstances():             # Override a static method\n            print('Extra stuff...')          # But call back to original\n            Hack.printNumInstances()\n        printNumInstances = staticmethod(printNumInstances)\n \n>>> a, b = Sub(), Sub()\n>>> a.printNumInstances()                    # Call from subclass instance\nExtra stuff...\nNumber of instances: 5\n>>> Sub.printNumInstances()                  # Call from subclass itself\nExtra stuff...\nNumber of instances: 5\n>>> Hack.printNumInstances()                 # Call original version\nNumber of instances: 5\nMoreover, classes can inherit the static method without redefining it—it is run\nwithout an instance, regardless of where it is defined in a class tree:\n>>> class Other(Hack): pass                  # Inherit static method verbatim\n>>> c = Other()\n>>> c.printNumInstances()\nNumber of instances: 6\nNotice how this also bumps up the superclass’s instance counter because its\nconstructor is inherited and run—a behavior that begins to encroach on the next\nsection’s subject.\nCounting Instances with Class Methods",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1275,
      "chapter": null,
      "content": "Interestingly, a class method can do similar work here—Example 32-9 has the\nsame behavior as the static method version listed earlier, but it uses a class\nmethod that receives the instance’s class in its first argument. Rather than\nhardcoding the class name, the class method uses the automatically passed class\nobject generically.\nExample 32-9. hack_class.py\nclass Hack:\n   numInstances = 0                         # Use class method instead of static\n   def __init__(self):\n       Hack.numInstances += 1\n   def printNumInstances(cls):\n       print('Number of instances:', cls.numInstances)\n   printNumInstances = classmethod(printNumInstances)\nThis class is used in the same way as the prior versions, but its\nprintNumInstances method receives the Hack class, not the instance, when\ncalled from either the class or an instance:\n>>> from hack_class import Hack\n>>> a, b = Hack(), Hack()\n>>> a.printNumInstances()                    # Passes class to first argument\nNumber of instances: 2\n>>> Hack.printNumInstances()                 # Also passes class to first argument\nNumber of instances: 2\nWhen using class methods, though, keep in mind that they receive the most\nspecific (i.e., lowest) class of the call’s subject. This has some subtle\nimplications when trying to update class data through the passed-in class. To\ndemo, Example 32-10 subclasses to customize the same way we did for static\nmethods in the prior section, and augments Hack.printNumInstances to also\ntrace its cls argument.\nExample 32-10. hack_class2.py\nclass Hack:\n   numInstances = 0                         # Trace class passed in\n   def __init__(self):\n       Hack.numInstances += 1\n   def printNumInstances(cls):\n       print('Number of instances:', cls.numInstances, cls)\n   printNumInstances = classmethod(printNumInstances)\nclass Sub(Hack):",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1276,
      "chapter": null,
      "content": "def printNumInstances(cls):              # Override a class method\n       print('Extra stuff...', cls)         # But call back to original\n       Hack.printNumInstances()\n   printNumInstances = classmethod(printNumInstances)\nclass Other(Hack): pass                      # Inherit class method verbatim\nRunning this in a REPL reveals that the lowest class is passed in whenever a\nclass method is run—even for subclasses that have no class methods of their\nown:\n>>> from hack_class2 import Hack, Sub, Other\n>>> x = Sub()\n>>> y = Hack()\n>>> x.printNumInstances()                           # Call from subclass instance\nExtra stuff... <class 'hack_class2.Sub'>\nNumber of instances: 2 <class 'hack_class2.Hack'>\n>>> Sub.printNumInstances()                         # Call from subclass itself\nExtra stuff... <class 'hack_class2.Sub'>\nNumber of instances: 2 <class 'hack_class2.Hack'>\n>>> y.printNumInstances()                           # Call from superclass instance\nNumber of instances: 2 <class 'hack_class2.Hack'>\nIn the first call here, a class method call is made through an instance of the Sub\nsubclass, and Python passes the lowest class, Sub, to the class method. All is\nwell in this case—since Sub’s redefinition of the method calls the Hack\nsuperclass’s version explicitly, the superclass method in Hack receives its own\nclass in its first argument. But watch what happens for an object that inherits the\nclass method verbatim:\n>>> z = Other()                                     # Call from lower sub's instance\n>>> z.printNumInstances()\nNumber of instances: 3 <class 'hack_class2.Other'>\nThis last call here passes Other to Hack’s class method. This works in this\nexample because fetching the counter finds it in Hack by class inheritance. If this\nmethod tried to assign to the passed class’s data, though, it would update Other,\nnot Hack! In this specific case, Hack is probably better off hardcoding its own\nclass name to update its data if it means to count instances of all its subclasses,",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1277,
      "chapter": null,
      "content": "too, rather than relying on the passed-in class argument.\nCounting instances per class with class methods\nIn fact, because class methods always receive the lowest class in an instance’s\ntree:\nStatic methods and explicit class names may be a better solution for\nprocessing data local to a class.\nClass methods may be better suited to processing data that may differ\nfor each class in a hierarchy.\nCode that needs to manage per-class instance counters, for example, might be\nbest off leveraging class methods. To illustrate, the top-level superclass in\nExample 32-11 uses a class method to manage state information that varies for\nand is stored on each class in the tree—similar in spirit to the way instance\nmethods manage state information that varies per class instance.\nExample 32-11. hack_class3.py\nclass Hack:\n   numInstances = 0\n   def count(cls):                    # Per-class instance counters\n       cls.numInstances += 1          # cls is lowest class above instance\n   def __init__(self):\n       self.count()                   # Passes self.__class__ to count\n   count = classmethod(count)\nclass Sub(Hack):\n   numInstances = 0\n   def __init__(self):                # Redefines __init__ (to demo)\n       Hack.__init__(self)\nclass Other(Hack):                     # Inherits __init__\n   numInstances = 0\nWhen run, the Hack class keeps track of each of its subclasses’ instances, using a\ncounter on each subclass:\n>>> from hack_class3 import Hack, Sub, Other\n>>> x = Hack()\n>>> y1, y2 = Sub(), Sub()\n>>> z1, z2, z3 = Other(), Other(), Other()\n>>> x.numInstances, y1.numInstances, z1.numInstances             # Per-class data!",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1278,
      "chapter": null,
      "content": "(1, 2, 3)\n>>> Hack.numInstances, Sub.numInstances, Other.numInstances\n(1, 2, 3)\nStatic and class methods have additional advanced roles, which we will skip\nhere; see other resources for more use cases. In later Python versions, though,\nthe static and class method designations became even simpler with the advent of\nfunction decoration syntax—a way to apply one function to another that has\nroles well beyond the static method use case that was one of its initial\nmotivations. This syntax also allows us to augment classes—to initialize data\nlike the numInstances counter in the last example, for instance. The next\nsection explains how.\nNOTE\nThe methods finale: For a postscript on Python’s method types, be sure to watch for coverage\nof metaclass methods in Chapter 40—because these are designed to process a class that is an\ninstance of a metaclass, they turn out to be very similar to the class methods defined here but\nrequire no classmethod declaration, and apply only to the shadowy metaclass realm\npreviewed next.\nDecorators and Metaclasses\nBecause the staticmethod and classmethod call technique described in the\nprior section initially seemed obscure to some observers, a device was eventually\nadded to make the operation simpler. Python decorators—similar to the notion\nand syntax of annotations in Java—both address this specific need and provide a\ngeneral tool for adding logic that manages functions and classes or later calls to\nthem.\nThis is called a “decoration,” but in more concrete terms is really just a way to\nrun extra processing steps at function and class definition time with explicit\nsyntax. It comes in two flavors:\nFunction decorators: The initial entry, augment function definitions at\ndef statements. They specify operation modes for both simple functions\nand classes’ methods by wrapping them in an extra layer of logic",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1279,
      "chapter": null,
      "content": "implemented as another function. That function is often called a\nmetafunction, though this is just terminology.\nClass decorators: A later extension, augment class definitions at class\nstatements. They wrap classes in a similar way, adding support for\nmanagement of whole objects and their interfaces instead of a single\nfunction.\nWe met decorators very briefly in Chapter 19 in relation to simple functions, but\nthey are more general than earlier implied: they can also be used for class\nmethods and classes and can add nearly arbitrary logic to functions and classes\nthat go well beyond that the static- and class-method roles used as a segue here.\nFor instance, function decorators may be used to augment functions with code\nthat logs calls made to them, checks argument types during debugging, times\ncalls, and so on, and can be used to manage either functions themselves or later\ncalls to them. In the latter mode, function decorators are similar to the delegation\ndesign pattern we explored in Chapter 31, but they are designed to augment a\nspecific function or method call, not an entire object interface.\nPython provides a few built-in function decorators for operations, such as\nmarking static and class methods and defining properties (as sketched earlier, the\nproperty built-in works as a decorator automatically), but programmers can\nalso code arbitrary decorators of their own. Although they are not strictly tied to\nclasses, user-defined function decorators are often coded as classes to save the\noriginal functions for later dispatch, along with other data as state information.\nThis proved such a useful hook that it was eventually extended—class\ndecorators bring augmentation to classes, too, and are more directly tied to the\nclass model. Like their function cohorts, class decorators may manage classes\nthemselves or later instance-creation calls and often employ delegation of entire\ninterfaces in the latter mode. As you’ll find, their roles also often overlap with\nmetaclasses but are a more lightweight way to achieve some goals.\nFunction Decorator Basics\nSyntactically, a function decorator is a sort of runtime declaration about the\nfunction that follows it. A function decorator is coded on a line by itself just",
      "content_length": 2237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1280,
      "chapter": null,
      "content": "before the def statement that defines a function or method. It consists of the @\nsymbol, followed by a metafunction—a plain function (or other callable object)\nof one argument, which is passed and manages another function. The code\nfollowing the @ is usually the name of a metafunction with an optional arguments\nlist, but as of Python 3.9, it can be any expression returning a one-argument\nfunction. Listing multiple decorators on consecutive lines allows them to nest, as\nyou’ll see later in this book.\nFor example, the prior section’s methods may be coded with decorator syntax\nlike this:\nclass C:\n   @staticmethod                    # Function decoration syntax\n   def meth():\n       …\nInternally, this syntax has the same effect as the following—passing the function\nthrough the decorator and assigning the result back to the original name:\nclass C:\n   def meth():\n       …\n   meth = staticmethod(meth)        # Name rebinding equivalent\nDecoration rebinds the method name to the decorator’s result. The net effect is\nthat calling the method function’s name later actually triggers the result of its\nstaticmethod decorator first. Because a decorator can return any sort of object,\nthis allows the decorator to insert a layer of logic to be run on every later call.\nThe decorator function is free to return either the original function itself or a new\nproxy object that saves the original function passed to the decorator to be\ninvoked indirectly after the extra logic layer runs.\nWith this addition, Example 32-12 is a better way to code our static method code\nof Example 32-8.\nExample 32-12. hack_static_deco.py\nclass Hack:\n   numInstances = 0\n   def __init__(self):\n       Hack.numInstances += 1",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1281,
      "chapter": null,
      "content": "@staticmethod\n   def printNumInstances():\n       print('Number of instances:', Hack.numInstances)\nHere is this example in action as before:\n>>> from hack_static_deco import Hack\n>>> a, b, c = Hack(), Hack(), Hack()\n>>> Hack.printNumInstances()              # Calls from classes and instances work\nNumber of instances: 3\n>>> a.printNumInstances()\nNumber of instances: 3\nBecause they also accept and return functions, the classmethod and property\nbuilt-in functions may be used as decorators in the same way—as in\nExample 32-13, which demos all three built-in decorators previewed earlier.\nExample 32-13. alldecorators.py\nclass Methods:\n   def imeth(self, x):            # Normal instance method: passed a self\n       print([self, x])\n   @staticmethod\n   def smeth(x):                  # Static: no instance passed\n       print([x])\n   @classmethod\n   def cmeth(cls, x):             # Class: gets class, not instance\n       print([cls, x])\n   @property                      # Property: computed on fetch\n   def name(self):\n       return 'Pat ' + self.__class__.__name__\nRunning this live in a REPL proves the point:\n>>> from alldecorators import Methods\n>>> obj = Methods()\n>>> obj.imeth(1)\n[<alldecorators.Methods object at 0x10d2839b0>, 1]\n>>> obj.smeth(2)\n[2]\n>>> obj.cmeth(3)\n[<class 'alldecorators.Methods'>, 3]\n>>> obj.name\n'Pat Methods'",
      "content_length": 1341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1282,
      "chapter": null,
      "content": "Bear in mind that staticmethod and its kin here are still built-in functions; they\nmay be used in decoration syntax just because they take a function as an\nargument and return a callable to which the original function name can be\nrebound. In fact, any such function can be used in this way—even user-defined\nfunctions we code ourselves, as the next section explains.\nA First Look at User-Defined Function Decorators\nAlthough Python provides a handful of built-in functions that can be used as\ndecorators, we can also write custom decorators of our own. Because of their\nwide utility, we’re going to devote an entire chapter to coding decorators in the\nfinal part of this book. As a quick example, though, let’s look at a simple user-\ndefined decorator at work.\nRecall from Chapter 30 that the __call__ operator-overloading method\nimplements a function-call interface for class instances. Example 32-14 uses this\nto code a call proxy class that saves the decorated function in the instance and\ncatches calls to the original name. Because this is a class, it also has state\ninformation—a counter of calls made.\nExample 32-14. tracer1.py\nclass tracer:\n   def __init__(self, func):          # Remember original, init counter\n       self.calls = 0\n       self.func  = func\n   def __call__(self, *args):         # On later calls: add logic, run original\n       self.calls += 1\n       print(f'call {self.calls} to {self.func.__name__}')\n       return self.func(*args)\n@tracer                                # Same as hack = tracer(hack)\ndef hack(a, b, c):                     # Wrap hack in a decorator object\n   return a + b + c\nif __name__ == '__main__':\n   print(hack(1, 2, 3))               # Really calls the tracer wrapper object\n   print(hack('a', 'b', 'c'))         # Invokes __call__ in class\nBecause the hack function is run through the tracer decorator, when the\noriginal hack name is called, it actually triggers the __call__ method in the\nclass. This method counts and logs the call and then dispatches it to the original",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1283,
      "chapter": null,
      "content": "wrapped function. Note how the *name argument syntax is used to pack and\nunpack the passed-in arguments; because of this, this decorator can be used to\nwrap any function with any number of positional arguments.\nThe net effect, again, is to add a layer of logic to the original hack function.\nWhen run, the first output line comes from the tracer class, and the second\ngives the return value of the hack function itself:\n$ python3 tracer1.py\ncall 1 to hack\n6\ncall 2 to hack\nabc\nTrace through this example’s code for more insight. As it is, this decorator works\nfor any function that takes positional arguments, but it does not handle keyword\narguments and cannot decorate class-level method functions (in short, for\nmethods, its __call__ would be passed a tracer instance only). As you’ll learn\nin Part VIII, there are a variety of ways to code function decorators, including\nnested def statements, and some of the alternatives are better suited to methods\nthan the version shown here.\nFor example, by using nested functions with enclosing scopes for state instead of\ncallable class instances with attributes, function decorators often become more\nbroadly applicable to class-level methods too. We’ll postpone the full details on\nthis, but Example 32-15 provides a brief look at this closure-based coding\nmodel; it uses function attributes for counter state for portability but could also\nleverage variables and nonlocal instead.\nExample 32-15. tracer2.py\ndef tracer(func):                      # Remember original\n   def oncall(*args):                 # On later calls\n       oncall.calls += 1\n       print(f'call {oncall.calls} to {func.__name__}')\n       return func(*args)\n   oncall.calls = 0\n   return oncall\nclass C:\n   @tracer\n   def hack(self, a, b, c): return a + b + c",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1284,
      "chapter": null,
      "content": "if __name__ == '__main__':\n   x = C()\n   print(x.hack(1, 2, 3))\n   print(x.hack('a', 'b', 'c'))\nThe example’s output is the same as its predecessor but reflects a decorated class\nmethod; more on this later.\nA First Look at Class Decorators and Metaclasses\nPython later generalized decorators, allowing them to be applied to classes as\nwell as functions. In short, class decorators are similar to function decorators,\nbut they are run at the end of a class statement to rebind a class name to a\ncallable. As such, they can be used to either manage classes just after they are\ncreated or insert a layer of wrapper logic to manage instances when they are later\ncreated. Symbolically, the code structure:\ndef decorator(aClass): …\n@decorator                       # Class decoration syntax\nclass C: …\nis mapped to the following equivalent:\ndef decorator(aClass): …\nclass C: …                       # Name rebinding equivalent\nC = decorator(C)\nThe class decorator is free to augment the class itself or return a proxy object that\nintercepts later instance construction calls. For example, in the code of\n“Counting instances per class with class methods”, we could use this hook to\nautomatically augment the classes with instance counters and any other data\nrequired:\ndef count(aClass):\n    aClass.numInstances = 0\n    return aClass                # Return class itself instead of a wrapper\n@count",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1285,
      "chapter": null,
      "content": "class Hack: …                    # Same as Hack = count(Hack)\n@count\nclass Sub(Hack): …               # numInstances = 0 not needed here\nIn fact, as coded, this decorator can be applied to classes or functions—it\nhappily returns the object being defined in either context after initializing the\nobject’s attribute:\n@count\ndef hack(): pass                 # Like hack = count(hack)\n@count\nclass Hack: pass                 # Like Hack = count(Hack)\nhack.numInstances                # Both are set to zero\nHack.numInstances\nThough this decorator manages a function or class itself, as detailed later in this\nbook, class decorators can also manage an object’s entire interface by\nintercepting construction calls and wrapping the new instance object in a proxy\nthat deploys attribute accessor tools to intercept later requests—a multilevel\ncoding technique we’ll use to implement class attribute privacy in Chapter 39.\nHere’s a preview of the model:\ndef decorator(cls):                             # On @ decoration\n    class Proxy:\n        def __init__(self, *args):              # On instance creation: make a cls\n            self.wrapped = cls(*args)\n        def __getattr__(self, name):            # On attribute fetch: extra ops here\n            return getattr(self.wrapped, name)\n    return Proxy\n@decorator\nclass C: …          # Like C = decorator(C)\nX = C()             # Makes a Proxy that wraps a C, and catches later X.attr\nFinally, metaclasses, mentioned briefly earlier in this chapter, are a similarly\nadvanced class-based tool whose roles often intersect with those of class\ndecorators. They provide an alternate model, which routes the creation of a class\nobject to a subclass of the top-level type class (normally), at the conclusion of a\nclass statement:",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1286,
      "chapter": null,
      "content": "class Meta(type):\n    def __new__(meta, classname, supers, classdict):\n        …extra logic + class creation via type call…\nclass C(metaclass=Meta):\n    …my creation routed to Meta…            # Like C = Meta('C', (), {…})\nPython calls a class’s metaclass to create the new class object, passing in the data\ndefined during the class statement’s run; if omitted, the metaclass simply\ndefaults to the type class we explored earlier. Abstractly speaking, here’s what\nhappens at the end of class statements having explicit metaclasses like the\npreceding:\n classname = Meta(classname, superclasses, attributedict)\nTo manage the creation or initialization of a new class object, a metaclass\ngenerally redefines the __new__ or __init__ method of the type class that\nintercepts this call by default. The net effect, as with class decorators, is to define\ncode to be run automatically at class creation time. Here, this binds the class\nname to the result of a call to a user-defined metaclass. In fact, a metaclass need\nnot be a class at all—a possibility we’ll explore later that blurs some of the\ndistinction between this tool and decorators and even qualifies the two as\nfunctionally equivalent in some roles.\nBoth schemes, class decorators and metaclasses, are free to augment a class or\nreturn an arbitrary object to replace it—a hook with almost limitless class-based\ncustomization possibilities. As you’ll learn later, metaclasses may also define\nmethods that process their instance classes rather than normal instances of them\n—a technique that’s similar (if not redundant) to class methods and might be\nemulated by methods and data in class decorator proxies, or even a class\ndecorator that returns a metaclass instance.\nSuch mind-bending concepts, however, require Chapter 40’s conceptual\ngroundwork (and quite possibly sedation).\nFor More Details\nNaturally, there’s more to the decorator and metaclass stories than shown here.\nAlthough they are a general mechanism whose usage may be required by some",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1287,
      "chapter": null,
      "content": "packages, coding new user-defined decorators and metaclasses is an advanced\ntopic of interest primarily to tool writers, not application programmers. Because\nof this, this book defers additional coverage until its final and optional part:\nChapter 38 shows how to code properties using function decorator\nsyntax in more depth.\nChapter 39 focuses on decorators, and includes more comprehensive\nexamples.\nChapter 40 covers metaclasses, and more on the class and instance\nmanagement story.\nAlthough these chapters cover advanced topics, they’ll also provide us with a\nchance to see Python at work in substantial examples. For now, let’s move on to\nour final class-related topic.\nThe super Function\nTo close out this chapter, we turn to super: a built-in function that can be used to\nboth reference superclass attributes implicitly without naming a superclass\nexplicitly and route method calls coherently in multiple-inheritance trees.\nSo far, super has been called out in the sidebar “The super Alternative”, as well\nas multiple notes along the way, but has not appeared in code. This was by\ndesign: super comes with substantial complexities and downsides that make it\ndifficult to recommend to learners. In short, it’s an all-or-nothing tool with\nseveral arduous coding requirements and relies on special-case and wildly\nimplicit semantics that run counter to Python norms.\nThe super call also tends to be abused for “Java-fication” of Python code.\nNewcomers with backgrounds in Java often rush to use Python’s super simply\nbecause of its similarity to a Java tool but are unaware of its much more subtle\nimplications in Python’s multiple inheritance—until adding superclasses breaks\ntheir programs.\nNevertheless, this call has grown pervasive in Python code and merits further\nelaboration, especially for those opting to use it naively. Hence, this section both",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1288,
      "chapter": null,
      "content": "covers super usage and notes its pitfalls along the way. Ultimately, though, the\nmerit of this call, like everything else presented in this chapter and book, is\nultimately yours to decide.\nThe super Basics\nFirst off, let’s review the explicit alternative that’s more closely in step with\nPython’s own idioms. As we’ve seen in this book so far, it’s always possible to\nreference a superclass’s attributes—whether method or data—by naming their\ndesired source class explicitly:\n>>> class C:\n        def act(self):\n            print('hack')\n>>> class D(C):\n        def act(self):\n            C.act(self)         # Name superclass explicitly, pass self\n            print('code')\n>>> I = D()\n>>> I.act()\nhack\ncode\nIn single-inheritance trees like this one, the super alternative seems relatively\nstraightforward at first glance: its most common form in the following\nautomatically selects the calling class’s superclass generically and implicitly\nwhen an attribute is later fetched. An explicit call like class.method(self), for\nexample, becomes an implicit super().method(), as in our example:\n>>> class C: \n        def act(self):\n            print('hack')\n>>> class D(C):\n        def act(self):\n            super().act()       # Reference superclass implicitly, omit self\n            print('code')\n>>> I = D()\n>>> I.act()\nhack",
      "content_length": 1323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1289,
      "chapter": null,
      "content": "code\nThis works as advertised and may minimize work—you don’t need to list self\nin the call, don’t need to update the call if D’s superclass changes in the future,\nand don’t need to code long superclass names or package-import paths.\nThe super Details\nIf you study the preceding code closely, though, you’ll realize that there’s\nsomething odd going on here. The super call somehow knows about the class,\nits superclass, and the self instance, even though none are present in its call.\nThe backstory involves MROs, a proxy, and an algorithm that are required\nreading for super aspirants of all kinds.\nA “magic” proxy\nTo understand how super works, you first need to be fluent in the MRO\nalgorithm covered in “Multiple Inheritance and the MRO”—and you should\nreview that now if you gave it a pass. The MRO is both a firm prerequisite and\nnested component of super. Given the complexity and artificial nature of the\nMRO, some may rule this a first strike against super.\nOnce you’ve mastered the MRO, the simplest description of super is this: when\nused in a class method, super returns a proxy object that will locate an attribute\nin a class following that of the containing class in the MRO of the self\ninstance’s class. The net effect finds an attribute in a superclass or other relative\nof the class containing the call.\nThis works as expected in single-inheritance trees because the superclass\nnaturally follows the containing class on self’s MRO. Really, though, this relies\non deep magic. Apart from the MRO itself, super works by inspecting:\nThe runtime call stack info for the calling method’s arguments\nThe __class__ variable internally added to the __closure__ of\nmethods that call super\nThe combo automatically locates both the self argument and the class",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1290,
      "chapter": null,
      "content": "containing the super call and then pairs the two in a special proxy object that\nroutes later attribute fetches to a superclass’s version of a name.\nIn fact, the common no-argument super form is equivalent to manually passing\nin the class containing the super call, along with the self instance. That is,\nwithin a class’s method function, the following forms work the same, though the\nsecond can be used outside a method, too, and its first argument can be\n__class__ inside a method:\nsuper()\nsuper(class-containing-the-super-call, method-self-argument)\nBoth forms can be used in your code, and manual arguments may be handy in\nsome roles. Because the second may be harder to code and maintain than\nexplicit class-name references, though, Python roots out the class and instance\nfor you behind the scenes. Here’s the equivalent manual version in our example:\nclass D(C):\n    def act(self):\n        super(D, self).act()    # Works the same as super().act()\n        print('code')           # And D is available as __class__\nIf that all sounds complicated and strange, it’s because it is. Due to its unusual\nsemantics, the no-argument super call form doesn’t work at all outside the\ncontext of a class’s method:\n>>> super                       # A \"magic\" proxy object that routes later calls\n<class 'super'>\n>>> super()                     # This form has no meaning outside a method\nRuntimeError: super(): no arguments\nAnd where it does work, its implicit pairing of class and instance is nowhere to\nbe found in your code:\n>>> class E(C):\n        def method(self):\n            proxy = super()     # self is implicit in super - only!\n            return proxy\n \n>>> prx = E().method()          # The normally hidden proxy object",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1291,
      "chapter": null,
      "content": ">>> prx\n<super: <class 'E'>, <E object>>\n>>> prx.act()    # Find act on MRO past hidden E, bind with hidden self, call (!)\nHack\nTo be sure, this call’s semantics resemble nothing else in Python—it’s neither a\nbound nor nonbound method and fills in a class and self even though you omit\nboth in the call. This deviates from Python’s explicit self policy, which holds\ntrue everywhere else. As we’ve seen, class methods list and use self explicitly\nto make instance references apparent. Operator overloading, including\nconstructors, implies a self, but this is trivial by comparison.\nBy hiding the instance, super violates this fundamental Python idiom for a\nsingle role. While that may be comfortable to those accustomed to other OOP\nlanguages, it may also qualify as a strike two to others.\nAttribute-fetch algorithm\nIn a single-inheritance tree like the preceding example, super is straightforward\nbecause there’s just one obvious follower on the MRO—the superclass of the\nclass containing the super call. In fact, in this simple case, the immediate\nsuperclass can be had from an instance at __class__.__bases__ without\napplying MROs at all:\n>>> E.__mro__\n(<class '__main__.E'>, <class '__main__.C'>, <class 'object'>)\n>>> E().__class__.__bases__[0]\n<class '__main__.C'>\nIn the more complex class trees of multiple inheritance, though, you must\nunderstand super’s full algorithm to know what it will choose in a given tree.\nHere’s how this works. The proxy object that super returns—created\n“magically” from runtime info as described in the prior section—uses its saved\ninstance and class containing the call to resolve attribute references as follows:\n1. Fetch the MRO of the saved self instance’s class, available at\nself.__class__.__mro__.",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1292,
      "chapter": null,
      "content": "2. Scan this MRO from left to right to find the saved containing class, and\nskip it.\n3. Search the namespace dictionaries of each remaining class in the MRO\nfrom left to right until the requested attribute is found.\n4. If the attribute was found and is a method, bind it with the saved self\ninstance.\nThis procedure is run for each attribute fetch and is wholly based on MRO\nordering. It must start with self’s MRO because the containing class’s own\nMRO won’t apply if it has been mixed with other classes; class-tree shape and\nhence MRO may be arbitrary for instances made from lower classes (and may\neven change dynamically in rare cases).\nYou can’t ignore these underlying mechanics except in very simple class trees.\nUnlike in Java, the utility of mix-in classes in Python makes multiple inheritance\nfrom disjoint and independent superclasses a common occurrence in realistic\ncode. And once you add multiple superclasses, you’ve kicked super up to a\nwhole new level.\nUniversal deployment\nLet’s illustrate with code. Suppose you’ve written the following classes that\nhappily deploy super in simple single-inheritance mode to implicitly invoke a\nmethod one level up from C:\n>>> class A:\n        def act(self): print('A')\n>>> class B:\n        def act(self): print('B')\n>>> class C(B):\n        def act(self):\n            super().act()         # super applied to a single-inheritance tree\n>>> C().act()                     # Make an instance and call its method\nB\nIf such classes later grow to use more than one superclass, though, super’s",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1293,
      "chapter": null,
      "content": "effects might be surprising—it does not raise an exception when the same name\nappears in more than one superclass of a multiple inheritance tree but will\nnaively pick just the leftmost superclass having the method being run (really, the\nfirst per the class tree’s flattened MRO). This may or may not be the class that\nyou want, and is completely wrong if you want both:\n>>> class C(B, A):                # Add an A mix-in class with the same method\n        def act(self):\n            super().act()         # Doesn't fail on conflicts - picks just one\n>>> C().act()\nB\n>>> class C(A, B):\n        def act(self):\n            super().act()         # If A is listed first, B.act() is no longer run\n>>> C().act()\nA\nThis silently masks a source of OOP errors so common that it shows up again in\nthis part’s “Gotchas” ahead. Explicit calls are one way to solve this dilemma.\nWith explicit class names, you can choose either the left class, the right class, or\nboth, and with substantially less drama. If you might need to be explicit later,\nwhy not use this form earlier too?\n>>> class C(A, B):                # Explicit form\n        def act(self):            # You probably want to be more explicit here\n            A.act(self)           # This handles both single and multiple \ninheritance\n            B.act(self)           # So why use the super special case at all?\n>>> C().act()\nA\nB\nTechnically speaking, this example’s explicit class inheritance also searches the\nmetaclass type tree for names not defined in superclasses, per “The Inheritance\nBifurcation”. This secondary-tree search differs from super, which always\nsearches just a tail portion of the instance’s MRO (and hence just the superclass\ntree), but is completely moot for defined names and unlikely to matter for\nundefined names.",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1294,
      "chapter": null,
      "content": "The real underlying issue with super here, though, is that it requires itself to be\ncalled in every class’s method in order to propagate the call chain. Without this\nuniversal deployment, the call dies in the first class that doesn’t call super—as\nin our A and B. While we can add super to these classes, too, this is the first of\nthat handful of arduous super coding requirements and quickly leads to another\nissue, as the next section explains.\nCall-chain anchors\nSince both A and B of the prior section’s example are somewhere on C’s MRO,\nwe might be tempted to make both classes’ methods run by propagating the call\nwith added super calls in both.\nThis scheme is called cooperative method dispatch: each class in a tree runs\nsuper to hand the call off to the next class on the MRO that cares about it. This\nautomatically routes calls through each calling class just once and avoids\nrunning a method in a diamond’s common superclass more than once (it appears\njust once in an MRO). It assumes that the MRO’s order makes sense for your\nmethod calls, but explicit class-name calls are a fallback if not.\nArmed with that info, here’s the mod in our example:\n>>> class A:\n        def act(self):\n            print('A')\n            super().act()         # Add a super here too\n>>> class B:\n        def act(self):\n            print('B')\n            super().act()         # Add a super here too\n \n>>> class C(B, A):\n        def act(self):\n            super().act()         # Hope this winds up running all methods \n \n>>> C().act()\nB\nA\nAttributeError: 'super' object has no attribute 'act'\nImmediately, though, we’re in trouble here, and the reason requires inspecting",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1295,
      "chapter": null,
      "content": "the MRO of an instance’s class:\n>>> I = C()\n>>> I.__class__.__mro__\n(<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>)\nHere’s the subtle problem. The super in C will select act in B, the next on this\nMRO; the super in B will then select act in A, its follower on self’s MRO; but\nthen there are no more act definitions to be had: the super in A looks for act in\nobject and beyond, and of course fails. We say that there is no call-chain\nanchor—no end point for the call propagation. Hence, the last super call in A\ndies with an exception.\nIn fact, you’ve just met a possible strike three for this call. While this code\nworks if you omit the super in A, this policy won’t help in general: classes like A\nand B are probably designed to be mixed into other classes, too, and it wouldn’t\nmake sense to specialize their code just for the C class’s use case. To propagate\nsuper method calls, all classes must define super, too, and there must be an\nanchor to catch and end the chain somewhere.\nAlthough we can add an anchor in a pointless superclass that defines the method\nbut does not call super again, this is another of those arduous coding\nrequirements—and substantially more effort than simply running the explicit\nclass-name calls alternative shown earlier:\n>>> class X:                          # Code a bogus class, just to appease super\n        def act(self):\n            print('anchor')\n>>> class A: …same…\n \n>>> class B: …same…\n>>> class C(B, A, X):                 # Add a final anchor, just to appease super\n         def act(self):\n             super().act()\n>>> C().act()\nB\nA\nanchor",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1296,
      "chapter": null,
      "content": ">>> [c.__name__ for c in C().__class__.__mro__]\n['C', 'B', 'A', 'X', 'object']\nThis works because X precedes object on the MRO as shown, and hence stops\nthe act call chain. Adding the anchor class X as a common superclass to both A\nand B in a diamond would work, too, because the resulting MRO is the same\n(remember, the MRO removes all but the last [rightmost] appearance of a class\nfrom the DFLR order):\n>>> class X: …same…\n         \n>>> class A(X): …same…\n \n>>> class B(X): …same…\n>>> class C(B, A): …same…\n>>> C().act()\nB\nA\nanchor\n>>> [c.__name__ for c in C().__class__.__mro__]\n['C', 'B', 'A', 'X', 'object']\nAgain, though, if you have to code a special class just to appease super, why not\njust use explicit class names? Similarity to other languages isn’t a very good\nreason, especially in contexts that other languages don’t support.\nAlso, keep in mind that the class selected by super for an attribute reference\nmay not be a superclass at all and may vary per tree that a class is mixed into.\nFor instance, the super in B in our example dispatches to A—the next on the\nMRO, but a sibling, not a superclass. This may be moot in most programs, but if\nyou must be sure that an immediate superclass’s method is run, you again must\nuse explicit class names instead of super.\nSame argument lists\nWhile super always demands a call-chain anchor, you may occasionally get one\nfor free. As a special case, object defines a constructor that can be relied on to\nanchor some chains (recall that the __init__ constructor method is a class",
      "content_length": 1533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1297,
      "chapter": null,
      "content": "attribute like any other, despite its odd name and automatic invocation):\n>>> class A:\n        def __init__(self):\n            print('A')\n            super().__init__()       # Propagate constructor calls\n \n>>> class B:\n        def __init__(self):\n            print('B')\n            super().__init__()       # Propagate constructor calls\n \n>>> class C(B, A):\n        def __init__(self):\n            super().__init__()       # Assume object anchors constructor chain\n>>> I = C()\nB\nA\nThis propagates the constructor call through C, B, A, and object per the I\ninstance’s MRO via cooperative method dispatch as before. This also fails,\nhowever, for constructors that take any arguments because that of object takes\nnone except self:\n>>> class A:\n        def __init__(self, name):         # Add an argument to the method\n            print('A')\n            super().__init__(name)\n \n>>> class B:\n        def __init__(self, name):\n            print('B')\n            super().__init__(name)\n \n>>> class C(B, A):\n        def __init__(self, name):\n            super().__init__(name)        # But object's arguments list differs\n \n>>> I = C('Pat')\nB\nA\nTypeError: object.__init__() takes exactly one argument (the instance to initialize)\nAnd now you’ve run into another one of those other arduous coding",
      "content_length": 1289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1298,
      "chapter": null,
      "content": "requirements: super generally assumes that all the methods in a call chain use\nthe same arguments list because the MRO’s ordering of method calls can vary\nwith class-tree shape: an arbitrary change in inheritance may change call order\narbitrarily.\nThis limits flexibility inherently. While you may be able to ensure same\narguments for classes used only in a single program and can sometimes fudge it\nwith starred-argument collectors for generality, neither policy will apply to code\nmeant to be reused in multiple contexts—which is really one of the main points\nbehind Python programming.\nNoncalls and operator overloading\nTo close, here are two more super oddities. First, keep in mind that super, like\nthe MRO, is not just about methods, despite their prevalence in its jargon. It can\nalso fetch the class data attributes we met in Chapter 29 and the bound methods\nwe met in Chapter 31:\n>>> class C:\n        attr1 = 'hack'          # super also fetches data attributes\n        def attr2(self):        # And returns bound methods sans calls\n            return 'code'\n \n>>> class D(C):\n        def act(self):\n            return super().attr1, super().attr2\n>>> I = D()\n>>> I.act()\n('hack', <bound method C.attr2 of <__main__.D object at 0x103993200>>)\n>>> I.act()[1]()\n'code'\nWhen you access a method like attr2 from a super proxy, the proxy binds it\nwith the saved instance to produce a bound method. Fetching a method as a\nplain function, however, may require an explicit class name, like C.attr2. This\nleads down a rabbit hole too deep to plumb here, but it’s another way that super\nclashes with normal semantics.\nSecond, super also doesn’t fully work in the presence of __X__ operator-\noverloading methods. If you study the following code, you’ll see that explicit",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1299,
      "chapter": null,
      "content": "named calls to overloading methods in the superclass work normally, but using\nthe super result in an expression fails to dispatch to the superclass’s method:\n>>> class C:\n        def __getitem__(self, ix):      # Indexing overload method\n            print('C index')\n>>> class D(C):\n        def __getitem__(self, ix):      # Redefine to extend here\n            print('D index')\n            C.__getitem__(self, ix)     # Explicit call form works\n            super().__getitem__(ix)     # Direct name calls work too\n            super()[ix]                 # But operators do not! (__getattribute__)\n>>> I = C()\n>>> I[99]\nC index\n>>> I = D()\n>>> I[99]\nD index\nC index\nC index\nTypeError: 'super' object is not subscriptable\nThis behavior is due to the same limitation described in the sidebar “Delegating\nBuilt-ins—or Not”—because the proxy object returned by super uses the\n__getattribute__ method we met earlier to catch and dispatch later attribute\nrequests, it fails to intercept the automatic __X__ method invocations run by\nbuilt-in operations including expressions, as these begin their search in the class\ninstead of the instance.\nThis may seem less severe than the other limitations we’ve met, but operators\nshould generally work the same as the equivalent method call, especially for a\nbuilt-in like this. Not supporting this adds another exception for super users to\nconfront and remember. Other languages’ mileage may vary, but in Python, self\nis explicit, multiple-inheritance mix-ins and operator overloading are common,\nand superclass name changes are rare enough to pass as a red herring.\nThe super Wrap-Up\nSo there you have it: a brief tutorial on the super built-in, for which you can",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1300,
      "chapter": null,
      "content": "find copious supplements in all the standard places, some of which seem as\nfocused on defending super as on documenting it. Hopefully, the coverage here\nhas given you a balanced view of this tool’s trade-offs while introducing its\nfundamentals.\nAs we’ve just seen, in single-inheritance class trees, the super call may be used\nto refer to parent superclasses generically without naming them explicitly. In\nmultiple-inheritance trees, this call can also be used to implement cooperative\nmethod dispatch that propagates calls through a tree. The latter role may be\nespecially useful in diamonds, as a conforming method call chain visits each\nsuperclass just once.\nWhile these are clear upsides in some contexts, it’s important to know that super\ncan also yield highly implicit behavior, which for some programs may not\ninvoke superclasses as expected or required.\nTo summarize, the super method-dispatch technique generally imposes three\nmain coding requirements:\nAnchors: the method called by super must exist—which requires extra\ncode and calls if no call-chain anchor is present, and mix-in classes\ncan’t be specialized for a single tree’s context.\nArguments: the method called by super must have the same argument\nsignature across the entire class tree—which can impair flexibility,\nespecially for implementation-level methods like constructors.\nDeployment: every appearance of the method called by super but the\nlast must use super itself—which can make it difficult to use existing\ncode, change call ordering, override methods, and code self-contained\nclasses.\nIn addition, super builds upon the already complex MRO, can mask problems\nwhen single-inheritance trees become multiple-inheritance trees, may select a\nclass other than a superclass in multiple-inheritance trees, and constitutes yet\nanother special case for attribute inheritance, which we’ll revisit in Chapter 40.\nIn the end, super is easy to use and relatively harmless in single-inheritance\nroles, but its unusual semantics, rigid requirements, and questionable net reward",
      "content_length": 2041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1301,
      "chapter": null,
      "content": "make it a mixed bag. Python programmers, especially those learning Python\nanew, might be better served by the more general and transparent coding\nparadigm of explicit class-name references.\nBut you should judge all this for yourself in an OOP Python program near you.\nClass Gotchas\nWe’ve reached the end of the primary OOP coverage in this book. After\nexceptions up next, we’ll explore additional class-related examples and topics in\nthe last part of the book, but that part mostly just gives expanded coverage to\nconcepts introduced here. As usual, let’s wrap up this part with the standard\nwarnings about pitfalls to avoid.\nMost class issues can be boiled down to namespace issues—which makes sense,\ngiven that classes are largely just namespaces with a handful of extra tricks.\nSome of the items in this section are more like class usage pointers than\nproblems, but even experienced class coders have been known to stumble on a\nfew.\nChanging Class Attributes Can Have Side Effects\nTheoretically speaking, classes (and class instances) are mutable objects. As\nwith built-in lists and dictionaries, you can change them in place by assigning to\ntheir attributes—and as with lists and dictionaries, this means that changing a\nclass or instance object may impact multiple references to it.\nThat’s usually what we want and is how objects change their state in general, but\nawareness of this issue becomes especially critical when changing class\nattributes. Because all instances generated from a class share the class’s\nnamespace, any changes at the class level are reflected in all instances unless\nthey have their own versions of the changed class attributes.\nIn Python, we can normally change any attribute in any object to which we have\na reference. Consider the following class. Inside the class body, the assignment\nto the name a generates an attribute X.a, which lives in the class object at\nruntime and will be inherited by all of X’s instances:",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1302,
      "chapter": null,
      "content": ">>> class X:\n        a = 1       # Class attribute\n>>> I = X()\n>>> I.a             # Inherited by instance\n1\n>>> X.a             # Accessible through class\n1\nSo far, so good—this is the normal case. But notice what happens when we\nchange the class attribute dynamically outside the class statement: it also\nchanges the attribute in every object that inherits from the class. Moreover, new\ninstances created from the class during this session or program run also get the\ndynamically set value, regardless of what the class’s source code says:\n>>> X.a = 2         # May change more than X\n>>> I.a             # I changes too\n2\n>>> J = X()         # J inherits from X's runtime values\n>>> J.a             # (but assigning to J.a changes a in J, not X or I)\n2\nIs this a useful feature or a dangerous trap? You be the judge. As discussed in\nChapter 27, you can actually get work done by changing class attributes without\never making a single instance—a technique that can simulate the use of\n“records” or “structs” in other languages. As a refresher, consider the following\nunusual but legal Python program:\nclass X: pass                       # Make a few attribute namespaces\nclass Y: pass\nX.a = 1                             # Use class attributes as variables\nX.b = 2                             # No instances anywhere to be found\nX.c = 3\nY.a = X.a + X.b + X.c\nfor X.i in range(Y.a): print(X.i)   # Prints 0..5\nHere, the classes X and Y work like “fileless” modules—namespaces for storing\nvariables we don’t want to clash. This is a perfectly legal Python programming\ntrick, but it’s less appropriate when applied to classes written by others; you\ncan’t always be sure that class attributes you change aren’t critical to the class’s",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1303,
      "chapter": null,
      "content": "internal behavior. If you’re out to simulate a C struct, you may be better off\nchanging instances than classes, as that way, only one object is affected:\nclass Record: pass\nX = Record()\nX.name = 'pat'\nX.job  = 'Pizza maker'\nChanging Mutable Class Attributes Can Have Side Effects,\nToo\nThis gotcha is really an extension of the prior. Because class attributes are shared\nby all instances, if a class attribute references a mutable object, changing that\nobject in place from any instance impacts all instances at once:\n>>> class C:\n        shared = []                 # Class attribute\n        def __init__(self):\n            self.perobj = []        # Instance attribute\n>>> x, y = C(), C()                 # Two instances\n>>> y.shared, y.perobj              # Implicitly share class attrs\n([], [])\n>>> x.shared.append('hack')         # Impacts y's view too!\n>>> x.perobj.append('code')         # Impacts x's data only\n>>> x.shared, x.perobj\n(['hack'], ['code']) \n>>> y.shared, y.perobj              # y sees change made through x\n(['hack'], [])\n>>> C.shared                        # Stored on class and shared\n['hack']\nThis effect is no different than many we’ve seen in this book already: mutable\nobjects are shared by simple variables, globals are shared by functions, module-\nlevel objects are shared by multiple importers, and mutable function arguments\nare shared by the caller and the callee. All of these are cases of general behavior\n—multiple references to a mutable object—and all are impacted if the shared\nobject is changed in place from any reference.\nHere, this occurs in class attributes shared by all instances via inheritance, but",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1304,
      "chapter": null,
      "content": "it’s the same phenomenon at work. It may be made more subtle by the different\nbehavior of assignments to instance attributes themselves:\nx.shared.append('hack')    # Changes shared object attached to class - in place\nx.shared = 'hack'          # Changed or creates instance attribute attached to x\nBut again, this is not a problem, it’s just something to be aware of; shared\nmutable class attributes can have many valid uses in Python programs.\nMultiple Inheritance: Order Matters\nThis may be obvious by now, but it’s worth underscoring one last time: if you\nuse multiple inheritance, the order in which superclasses are listed in the class\nstatement header can be critical. Python always searches superclasses from left\nto right, according to their order in the header line.\nFor instance, in the multiple inheritance example we studied in Chapter 31,\nimagine that the Super class implemented a __str__ method, too:\nclass ListTree:\n    def __str__(self): …\nclass Super:\n    def __str__(self): …\nclass Sub(ListTree, Super):    # Get ListTree's __str__ by listing it first\nx = Sub()                      # Inheritance searches ListTree before Super\nWhich class would we inherit it from—ListTree or Super? As inheritance\nsearches generally proceed from left to right, we would get the method from\nwhichever class is listed first (leftmost) in Sub’s class header. Presumably, we\nwould list ListTree first because its whole purpose is its custom __str__.\nIndeed, we had to do this in Chapter 31 when mixing this class with a\ntkinter.Button that had a __str__ of its own.\nBut now, suppose Super and ListTree have their own versions of other same-\nnamed attributes, too. If we want one name from Super and another from\nListTree, the order in which we list them in the class header won’t help—we",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1305,
      "chapter": null,
      "content": "will have to override inheritance by manually assigning to the attribute name in\nthe Sub class:\nclass ListTree:\n    def __str__(self): …\n    def other(self): …\nclass Super:\n    def __str__(self): …\n    def other(self): …\nclass Sub(ListTree, Super):    # Get ListTree's __str__ by listing it first\n    other = Super.other        # But explicitly pick Super's version of other\n    def __init__(self):\n        …\nx = Sub()                      # Inheritance searches Sub before ListTree/Super\nHere, the assignment to other within the Sub class creates Sub.other—a\nreference back to the Super.other object. Because it is lower in the tree,\nSub.other effectively hides ListTree.other, the attribute that the inheritance\nsearch would normally find. Similarly, if we listed Super first in the class\nheader to pick up its other, we would need to select ListTree’s method\nexplicitly:\nclass Sub(Super, ListTree):               # Get Super's other by order\n    __str__ = ListTree.__str__            # Explicitly pick ListTree.__str__\nFor another example of the technique shown here in action, see the discussion of\nexplicit conflict resolution in “Attribute Conflict Resolution”. Ultimately,\nmultiple inheritance is an advanced tool. Even if you understood the last\nparagraph, it’s still a good idea to use it sparingly and carefully. Otherwise, the\nmeaning of a name may come to depend on the order in which classes are mixed\nin an arbitrarily far-removed subclass.\nAs a rule of thumb, multiple inheritance works best when your mix-in classes are\nas self-contained as possible—because they may be used in a variety of contexts,\nthey should not make assumptions about names related to other classes in a tree.\nThe pseudoprivate __X attributes feature we studied in Chapter 31 can help by\nlocalizing names that a class relies on owning and limiting the names that mix-in",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1306,
      "chapter": null,
      "content": "classes add to the mix. In this example, for instance, if ListTree only means to\nexport its custom __str__, it can name its other method __other to avoid\nclashing with like-named classes in the tree.\nScopes in Methods and Classes\nWhen working out the meaning of names in class-based code, it helps to\nremember that classes introduce local scopes, just as functions do, and methods\nare simply further nested functions. In the following example, the generate\nfunction returns an instance of the nested Hack class. Within its code, the class\nname Hack is assigned in the generate function’s local scope and hence is\nvisible to any further nested functions, including code inside method; it’s in the\nE enclosing-function layer of the LEGB scope lookup rule:\ndef generate():\n    class Hack:                  # Hack is a name in generate's local scope\n        count = 1\n        def method(self):\n            print(Hack.count)    # Visible in generate's scope, per LEGB rule (E)\n    return Hack()\ngenerate().method()\nThis example works because the local scopes of all enclosing function defs are\nautomatically visible to nested defs—including nested method defs, as in this\nexample.\nEven so, keep in mind that method defs cannot see the local scope of the\nenclosing class; they can see only the local scopes of enclosing defs. That’s why\nmethods must go through the self instance or the class name to reference\nmethods and other attributes defined in the enclosing class statement. For\nexample, code in the method must use self.count or Hack.count, not just\ncount.\nTo avoid nesting, we could restructure this code such that the class Hack is\ndefined at the top level of the module: the nested method function and the top-\nlevel generate will then both find Hack in their global scopes; it’s not localized\nto a function’s scope, but is still local to a single module:",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1307,
      "chapter": null,
      "content": "def generate():\n    return Hack()\nclass Hack:                    # Define at top level of module\n    count = 1\n    def method(self):\n        print(Hack.count)      # Found in global scope (enclosing module)\ngenerate().method()\nCode tends to be simpler in general if you avoid nesting classes and functions.\nOn the other hand, class nesting is useful in closure contexts, where the\nenclosing function’s scope retains state used by the class or its methods. In the\nfollowing, the nested method has access to its own scope, the enclosing\nfunction’s scope (for label), the enclosing module’s global scope, anything\nsaved in the self instance by the class, and the class itself via its nonlocal name:\n>>> def generate(label):       # Returns a class instead of an instance\n        class Hack:\n            count = 1\n            def method(self):\n                print(f'{label}={Hack.count}')\n        return Hack\n>>> aclass = generate('Gotchas')\n>>> I = aclass()\n>>> I.method()\nGotchas=1\nMiscellaneous Class Gotchas\nHere’s a handful of additional class-related warnings, mostly as review:\nChoose per-instance or class storage wisely. On a similar note, be\ncareful when you decide whether an attribute should be stored on a class\nor its instances: the former is shared by all instances, and the latter will\ndiffer per instance. In a GUI program, for instance, if you want\ninformation to be shared by all of the window class objects your\napplication will create (e.g., the last directory used for a Save operation\nor an already entered password), it might be best stored as class-level\ndata; if stored in the instance as self attributes, it will vary per window",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1308,
      "chapter": null,
      "content": "or be missing entirely when looked up by inheritance.\nYou usually want to call superclass constructors. Remember that\nPython runs only one __init__ constructor method when an instance is\nmade—the first it finds by inheritance. It does not automatically run the\nconstructors of all superclasses higher up. Because constructors\nnormally perform required startup work, you’ll usually need to run a\nsuperclass constructor from a subclass constructor—using either an\nexplicit call through the superclass’s name or super, passing along\nwhatever arguments are required—unless you mean to replace the\nsuper’s constructor altogether, or the superclass doesn’t have or inherit a\nconstructor at all.\nStay tuned for a fix for __getattr__ and built-ins. Another\nreminder: as noted in Chapter 28 and elsewhere, classes that use the\n__getattr__ operator-overloading method to delegate attribute fetches\nto wrapped objects may fail unless operator-overloading methods are\nredefined in the wrapper class. The names of operator-overloading\nmethods implicitly fetched by built-in operations are not routed through\ngeneric attribute-interception methods. To work around this, you must\nredefine such methods in wrapper classes, either manually, with tools,\nor by definition in superclasses; you’ll learn how in Chapter 39.\n“Overwrapping-itis”\nFinally, when used well, the code reuse features of OOP make it excel at cutting\ndevelopment time. Sometimes, though, OOP’s abstraction potential can be\nabused to the point of making code difficult to understand. If classes are layered\ntoo deeply, code can become obscure; you may have to search through many\nclasses to discover what an operation does.\nImagine, for example, a framework with hundreds of classes and a dozen levels\nof inheritance (this is a true story, but details have been omitted to protect the\ninnocent). Deciphering method calls in such a complex system may be a\nmonumental task: multiple classes might have to be consulted for even the most\nbasic of operations. In fact, the logic of such a system can be so deeply wrapped\nthat understanding a piece of code in some cases may require days of wading",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1309,
      "chapter": null,
      "content": "through related files. This obviously isn’t ideal for programmer productivity.\nThe most general rule of thumb of Python programming applies here, too: don’t\nmake things complicated unless they truly must be. Wrapping your code in\nmultiple layers of classes to the point of incomprehensibility is always a bad\nidea. Abstraction is the basis of polymorphism and encapsulation, and it can be a\nvery effective tool when used well. However, you’ll simplify debugging and aid\nmaintainability if you make your class interfaces intuitive, avoid making your\ncode overly abstract, and keep your class hierarchies short and small unless there\nis a good reason to do otherwise. Remember: code you write is generally code\nthat others must read.",
      "content_length": 731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1310,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter presented an assortment of class-related topics, including\nsubclassing built-in types, the relationship of types and classes, slots, properties,\nstatic methods, decorators, and super. Most are optional extensions to the OOP\ntoolbox in Python but may become more useful as you start writing larger\nobject-oriented programs, and all are fair game if they appear in code you must\nunderstand. As noted earlier, some of these topics are continued in the final part\nof this book; be sure to look ahead for more info on properties, descriptors,\ndecorators, and metaclasses.\nThis is the end of the class part of this book, so you’ll find the usual lab\nexercises at the end of the chapter: be sure to work through them to get some\npractice coding real classes. In the next chapter, we’ll begin our look at our last\ncore language topic, exceptions—Python’s mechanism for communicating errors\nand other conditions to your code. This is a relatively lightweight topic but it\nwas saved for last because new exceptions must be coded as classes. Before we\ntackle that final core subject, though, take a look at this chapter’s quiz and the\nlab exercises.\nTest Your Knowledge: Quiz\n1. Name two ways to extend a built-in object type.\n2. What are function and class decorators used for?\n3. How are normal and static methods different?\n4. Are tools like __slots__ and super valid to use in your code?\nTest Your Knowledge: Answers\n1. You can embed a built-in object in a wrapper class, or subclass the built-\nin type directly. The latter approach tends to be simpler, as most original\nbehavior is automatically inherited. This works because types are",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1311,
      "chapter": null,
      "content": "classes, though the way you create an instance from a type/class\ndetermines its functionality.\n2. Function decorators are generally used to manage a function or method\nor add to it a layer of logic that is run each time the function or method\nis called. They can be used to log or count calls to a function, check its\nargument types, and so on. They are also used to “declare” static\nmethods (simple functions in a class that are not passed an instance,\nhowever they are called), as well as class methods and properties. Class\ndecorators are similar but manage whole objects and their interfaces\ninstead of a function call.\n3. Normal (instance) methods receive a self argument (the implied\ninstance), but static methods do not. Static methods are simple functions\nnested in class objects. To make a method static, it must either be run\nthrough a special built-in function or be decorated with decorator\nsyntax. Python also allows simple functions in a class to be called\nthrough the class without this step, but calls through instances still\nrequire static-method declaration.\n4. Of course, but you shouldn’t use advanced tools automatically without\ncarefully considering their implications. Slots, for example, can break\ncode; super can mask later problems when used for single inheritance,\nand in multiple inheritance brings with it substantial complexity for an\nisolated use case; and both require universal deployment to be most\nuseful. Evaluating new or advanced tools is a primary task of any\nengineer, and this is why we explored trade-offs in this chapter. This\nbook’s goal is not to tell you which tools to use but to underscore the\nimportance of objectively analyzing them—a task often given too low a\npriority in the software field. In engineering, as in life in general, we\nshouldn’t let other people make choices for us.",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1312,
      "chapter": null,
      "content": "Test Your Knowledge: Part VI Exercises\nThese exercises ask you to write a few classes and experiment with some\nexisting code. Of course, the problem with existing code is that it must be\nexisting. To work with the set class in exercise 5, either copy/paste Example 32-\n1 from emedia, find it in this book’s examples package (see the Preface for\npointers), or type it up by hand (mildly tedious but a great way to make syntax\nmore concrete). These programs are growing sophisticated, so be sure to check\nthe solutions at the end of the book for pointers. You’ll find them in Appendix B,\nunder “Part VI, Classes and OOP”.\n1. Inheritance: Write a class called Adder that exports a method\nadd(self, x, y), which prints a “Not Implemented” message. Then,\ndefine two subclasses of Adder that implement the add method:\nListAdder\nWith an add method that returns the concatenation of its two list\narguments\nDictAdder\nWith an add method that returns a new dictionary containing the\nitems in both its two dictionary arguments (any definition of\ndictionary addition will do; see dictionary union in Chapter 8 for\ntips)\nExperiment by making instances of all three of your classes\ninteractively and calling their add methods.\nNow, extend your Adder superclass to save an object in the instance\nwith a constructor (e.g., assign self.data a list or a dictionary), and\noverload the + operator with an __add__ method to automatically\ndispatch to your add methods (e.g., X + Y triggers X.add(X.data,Y)).",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1313,
      "chapter": null,
      "content": "Where is the best place to put the constructors and operator-overloading\nmethods (i.e., in which classes)? What sorts of objects can you add to\nyour class instances?\nIn practice, you might find it easier to code your add methods to accept\njust one real argument (e.g., add(self,y)) and add that one argument\nto the instance’s current data (e.g., self.data + y). Does this make\nmore sense than passing two arguments to add? Would you say this\nmakes your classes more “object-oriented”?\n2. Operator overloading: Write a class called MyList that shadows\n(“wraps”) a Python list: it should overload most list operators and\noperations, including +, indexing, iteration, slicing, and list methods\nsuch as append and sort. See the Python reference manual or other\ndocumentation for a list of all possible methods to support. Also,\nprovide a constructor for your class that takes an existing list (or a\nMyList instance) and copies its components into an instance attribute.\nExperiment with your class interactively. Things to explore:\nWhy is copying the initial value important here?\nCan you use an empty slice (e.g., start[:]) to copy the initial\nvalue if it’s a MyList instance?\nIs there a general way to route list method calls to the wrapped\nlist?\nCan you add a MyList and a regular list? How about a list and\na MyList instance?\nWhat type of object should operations like + and slicing return?\nWhat about indexing operations?\nYou may implement this sort of wrapper class by embedding a\nreal list in a standalone class or by extending the built-in list\ntype with a subclass. Which is easier, and why?\n3. Subclassing: Make a subclass of MyList from exercise 2 called\nMyListSub, which extends MyList to print a message to stdout before",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1314,
      "chapter": null,
      "content": "each call to the + overloaded operation and counts the number of such\ncalls. MyListSub should inherit basic method behavior from MyList.\nAdding a sequence to a MyListSub should print a message, increment\nthe counter for + calls, and perform the superclass’s method. Also,\nintroduce a new method that prints the operation counters to stdout\n(i.e., your console window) and experiment with your class\ninteractively. Do your counters count calls per instance or per class (for\nall instances of the class)? How would you program the other option?\n(Hint: it depends on which object the count members are assigned to:\nclass members are shared by instances, but self members are per-\ninstance data.)\n4. Attribute methods: Write a class called Attrs with methods that\nintercept every attribute qualification (both fetches and assignments),\nand print messages listing their arguments to stdout. Create an Attrs\ninstance and experiment with qualifying it interactively. What happens\nwhen you try to use the instance in expressions? Try adding, indexing,\nand slicing the instance of your class. (Note: a fully generic approach\nbased upon __getattr__ requires Chapter 39’s workarounds for\nreasons noted in Chapter 28 and later and summarized in the solution to\nthis exercise.)\n5. Set objects: Experiment with the set class of Example 32-1 and\ndescribed in “Extending Types by Embedding”. Run commands to do\nthe following sorts of operations:\nCreate two sets of integers, and compute their intersection and\nunion by using & and | operator expressions.\nCreate a set from a string, and experiment with indexing your\nset. Which methods in the class are called?\nTry iterating through the items in your string set using a for\nloop. Which methods run this time?\nTry computing the intersection and union of your string set and\na simple Python string. Does it work?",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1315,
      "chapter": null,
      "content": "Now, extend your set by subclassing to handle arbitrarily many\noperands using the *args argument form. (Hint: see the\nfunction versions of these algorithms in Chapter 18.) Compute\nintersections and unions of multiple operands with your set\nsubclass. How can you intersect three or more sets, given that &\nhas only two sides?\nHow would you go about emulating other list operations in the\nset class? (Hint: __add__ can catch concatenation, and\n__getattr__ can pass most named list method calls like\nappend to the wrapped list.)\n6. Class tree links: In“Namespaces: The Conclusion” and in “Multiple\nInheritance and the MRO”, we learned that classes have a __bases__\nattribute that returns a tuple of their superclass objects (the ones listed in\nparentheses in the class header). Use __bases__ to extend any or all\nthree of the listing mix-in classes we wrote in Chapter 31 so that they\nprint the names of the immediate superclasses of the instance’s class.\nModding Example 31-10 first may be easiest. When you’re done, the\nfirst line of the string representation should look like this (your hex\naddresses will almost certainly vary):\n<Instance of Sub(Super, Lister), address 0x…:\n7. Composition: Simulate a fast-food ordering scenario by defining four\nclasses:\nLunch\nA container and controller class\nCustomer\nThe actor who buys food\nEmployee\nThe actor from whom a customer orders",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1316,
      "chapter": null,
      "content": "Food\nWhat the customer buys\nTo get you started, here are the classes and methods you’ll be defining:\nclass Lunch:\n    def __init__(self)               # Make/embed Customer and Employee\n    def order(self, foodName)        # Start a Customer order simulation\n    def result(self)                 # Ask the Customer what Food it has\nclass Customer:\n    def __init__(self)                        # Initialize my food to None\n    def placeOrder(self, foodName, employee)  # Place order with an \nEmployee\n    def printFood(self)                       # Print the name of my food\nclass Employee:\n    def takeOrder(self, foodName)    # Return a Food, with requested name\nclass Food:\n    def __init__(self, name)         # Store food name\nThe order simulation should work as follows:\nThe Lunch class’s constructor should make and embed an\ninstance of Customer and an instance of Employee, and it\nshould export a method called order. When called, this order\nmethod should ask the Customer to place an order by calling\nits placeOrder method. The Customer’s placeOrder method\nshould, in turn, ask the Employee object for a new Food object\nby calling Employee’s takeOrder method.\nFood objects should store a food name string (e.g., “burritos”),\npassed down from Lunch.order, to Customer.placeOrder, to\nEmployee.takeOrder, and finally to Food’s constructor. The",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1317,
      "chapter": null,
      "content": "top-level Lunch class should also export a method called\nresult, which asks the customer to print the name of the food\nit received from the Employee via the order (this can be used to\ntest your simulation).\nNote that Lunch needs to pass either the Employee or itself to the\nCustomer to allow the Customer to call Employee methods.\nExperiment with your classes interactively by importing the Lunch\nclass, calling its order method to run an interaction, and then calling its\nresult method to verify that the Customer got what it ordered. If you\nprefer, you can also simply code test cases as self-test code in the file\nwhere your classes are defined, using the module __name__ trick of\nChapter 25. In this simulation, the Customer is the active agent; how\nwould your classes change if Employee were the object that initiated\ncustomer/employee interaction instead?\n8. Zoo animal hierarchy: Consider the class tree shown in Figure 32-1.\nFigure 32-1. A zoo hierarchy composed of classes linked into an inheritance tree\nCode a set of six class statements to model this taxonomy with Python\ninheritance. Then, add a speak method to each of your classes that\nprints a unique message and a reply method in your top-level Animal",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1318,
      "chapter": null,
      "content": "superclass that simply calls self.speak to invoke the category-specific\nmessage printer in a subclass below (this will kick off an independent\ninheritance search from self). Finally, remove the speak method from\nyour Hacker class so that it picks up the default above it. When you’re\nfinished, your classes should work this way:\n$ python3\n>>> from zoo import Cat, Hacker\n>>> spot = Cat()\n>>> spot.reply()                   # Animal.reply: calls Cat.speak\nmeow\n>>> data = Hacker()                # Animal.reply: calls Primate.speak\n>>> data.reply()\nHello world!\nWHY YOU WILL CARE: OOP BY THE MASTERS\nAlmost invariably, when teaching live Python classes, about halfway through\nthe OOP section, people who have used OOP in the past are following along\nintensely, while people who have not are beginning to glaze over (or nod off\ncompletely). The point behind the technology just isn’t apparent.\nA book like this has the luxury of slowly presenting material like the\noverview in Chapter 26, and the gradual tutorial of Chapter 28—in fact, you\nshould probably review those sections again if you’re starting to feel like\nOOP is just some computer science mumbo-jumbo. Though OOP adds more\nstructure than the generators we met earlier, it similarly relies on some magic\n(inheritance search and a special first argument) that beginners can\nunderstandably find difficult to rationalize.\nIn real classes, however, to help get the newcomers on board (and keep them\nawake), it often helps to stop and ask the experts in the audience why they\nuse OOP. The answers they’ve given might help shed some light on the\npurpose of OOP if you’re new to the subject.\nHere, then, with only a few embellishments, are the most common reasons to\nuse OOP, as cited by students over the years:",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1319,
      "chapter": null,
      "content": "Code reuse\nThis one’s easy and is the main reason for using OOP. By supporting\ninheritance, classes make it natural to program by customization instead\nof starting each project from scratch.\nEncapsulation\nWrapping up implementation details behind object interfaces insulates\nusers of a class from code changes.\nStructure\nClasses provide new local scopes, which minimizes name clashes. They\nalso provide a natural place to write and look for implementation code\nand to manage object state.\nMaintenance\nClasses naturally promote code factoring, which allows us to minimize\nredundancy. Thanks to both the structure and code reuse support of\nclasses, usually only one copy of the code needs to be changed.\nConsistency\nClasses and inheritance allow you to implement common interfaces and\nhence create a common look and feel in your code; this eases debugging,\ncomprehension, and maintenance.\nPolymorphism\nThis is more a property of OOP than a reason for using it, but by\nsupporting code generality, polymorphism makes code more flexible and\nwidely applicable and hence more reusable.\nOther",
      "content_length": 1084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1320,
      "chapter": null,
      "content": "And, of course, the number one reason students gave for using OOP: it\nlooks good on a résumé! (OK, this one was added as a joke, but it is\nimportant to be familiar with OOP if you plan to work in the software\nfield today.)\nFinally, keep in mind the guidance given multiple times in this part of this\nbook: you won’t fully appreciate OOP until you’ve used it for a while. Pick a\nproject, study larger examples, work through the exercises. Do whatever it\ntakes to get your feet wet with OOP code. It’s optional stuff and may even be\noverkill in some contexts, but it’s generally worth the effort.",
      "content_length": 594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1321,
      "chapter": null,
      "content": "Part VII. Exceptions",
      "content_length": 20,
      "extraction_method": "OCR"
    },
    {
      "page_number": 1322,
      "chapter": null,
      "content": "Chapter 33. Exception Basics\nThis part of the book deals with exceptions—events that signal conditions and\nmodify the flow of control through a program. In Python, exceptions are\ntriggered automatically on errors, and they can be both triggered and intercepted\nby your code. They are processed by four statements we’ll study here, the first of\nwhich comes in multiple flavors that qualify as different statement forms by\nsome measures:\ntry/except/else/finally\nCatch and recover from exceptions raised by Python, or by you\nraise\nTrigger an exception manually in your code\nassert\nConditionally trigger an exception in your code\nwith\nUse context managers that automate exception handling\nWe’ve met some of these briefly before, but full coverage of this topic was saved\nuntil the end of the main part of this book because you need to know about\nclasses to code exceptions of your own. Still, with a few exceptions (pun\nintended), you’ll find that exception handling is simple in Python because it’s\nintegrated into the language itself as another high-level tool. Before we dig into\nthe “how,” though, let’s get clear on the “why.”\nWhy Use Exceptions?\nIn a nutshell, exceptions let us jump out of arbitrarily large chunks of a program.",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1323,
      "chapter": null,
      "content": "Consider the hypothetical pizza-making robot we discussed earlier in the book.\nSuppose we took the idea seriously and actually built such a machine. To make a\npizza, our culinary automaton would need to execute a plan, which we would\nimplement as a Python program: it would take an order, prepare the dough, add\ntoppings, bake the pie, and so on.\nNow, suppose that something goes very wrong during the “bake the pie” step.\nPerhaps the oven is broken, or perhaps our robot miscalculates its reach and\nspontaneously combusts. Clearly, we want to be able to jump to code that\nhandles such unusual states quickly. As we have no hope of finishing the pizza\ntask in such unusual cases, we might as well abandon the entire plan.\nThat’s exactly what exceptions let your programs do: they can jump to an\nexception handler in a single step, abandoning all activity begun since the\nexception handler was entered. Code in the exception handler can then respond\nto the raised exception as appropriate (by calling the fire department, for\ninstance!).\nOne way to think of an exception is as a sort of structured “go-to.” An exception\nhandler (try statement) leaves a marker and executes some code. Somewhere\nfurther ahead in the program, an exception is raised that makes Python jump\nback to that marker, abandoning any code that was started and functions that\nwere called after the marker was left. The net effect unwinds the program’s\ncontrol flow back to the marker and resumes there.\nThis protocol provides a coherent way to respond to unusual events. Moreover,\nbecause Python jumps to the handler statement immediately, your code is\nsimpler—there is usually no need to check status codes after every operation and\nfunction call that could possibly fail. Instead, we catch errors only where we\nneed to recover from them.\nException Roles\nIn less hypothetical programs, exceptions serve a variety of purposes. Here are\nsome of their most common roles:\nError handling\nPython raises exceptions whenever it detects errors in programs at runtime.",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1324,
      "chapter": null,
      "content": "You can catch and respond to the errors in your code, or ignore the\nexceptions that are raised. If an error is ignored, Python’s default exception-\nhandling behavior kicks in: it stops the program and prints an error message.\nIf you don’t want this default behavior, code a try statement to catch and\nrecover from the exception—Python will jump to your try handler when the\nerror is detected in the statement’s code, and your program will resume\nexecution after the try.\nEvent notification\nExceptions can also be used to signal valid conditions without you having to\npass result flags around a program or test them explicitly. For instance, a\nsearch routine might raise an exception on failure, rather than returning an\ninteger result code—and hoping that the code will never be a valid result.\nSpecial-case handling\nSometimes a condition may occur so rarely that it’s hard to justify\nconvoluting your code to handle it in multiple places. You can often\neliminate special-case code by handling unusual cases in exception handlers\nin higher levels of your program. An assert can similarly be used to check\nthat conditions are as expected during development.\nTermination actions\nAs you’ll see, the finally option in a try statement allows you to guarantee\nthat required closing-time operations will be performed, regardless of the\npresence or absence of exceptions in your programs. The with statement\noffers an alternative in this department for objects that support its expected\nmethod-call protocol.\nUnusual control flows",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1325,
      "chapter": null,
      "content": "Finally, because exceptions are a sort of high-level and structured “go-to,”\nyou can use them as the basis for implementing exotic control flows. For\ninstance, although the language does not explicitly support backtracking, you\ncan implement it in Python by using exceptions and logic to unwind\nassignments.1 There is no “go to” statement in Python (thankfully) and no\nbuilt-in backtracking (today), but exceptions can sometimes serve similar\nroles; a raise, for instance, can be used to jump out of multiple loops in\nways that break cannot.\nWe saw some of these roles briefly earlier and will study typical exception use\ncases in action later in this part of the book. For now, let’s get started with a look\nat Python’s exception-processing tools.\nExceptions: The Short Story\nCompared to some other core language topics we’ve explored in this book,\nexceptions are a fairly lightweight tool in Python. Because they are so simple,\nlet’s jump right into some code.\nDefault Exception Handler\nSuppose we write the following function in the interactive REPL of our choice:\n>>> def fetcher(obj, index):\n        return obj[index]\nThere’s not much to this function—it simply indexes an object on a passed-in\nindex. In normal operation, it returns the result of a legal index:\n>>> food = 'pizza'\n>>> fetcher(food, 4)                        # Like x[4], last item\n'a'\nHowever, if we ask this function to index off the end of the string, an exception",
      "content_length": 1439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1326,
      "chapter": null,
      "content": "will be triggered when the function tries to run obj[index]. Python detects out-\nof-bounds indexing for sequences and reports it by raising (triggering) the built-\nin IndexError exception:\n>>> fetcher(food, 5)                        # Default handler – console interface\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 2, in fetcher\nIndexError: string index out of range\nBecause our code does not explicitly catch this exception, it filters back up to the\ntop level of the program and invokes the default exception handler, which\nsimply prints the standard error message shown here.\nBy this point in the book, you’ve probably seen your share of standard error\nmessages. They include the exception that was raised, along with a stack trace—\na list of all the lines and functions that were active when the exception occurred,\nwhich has been largely omitted in this book for space and brevity.\nThe error message text here was printed by Python 3.12 in a console. It can vary\nslightly per release, and even per interactive REPL, so you shouldn’t rely upon\nits exact form—in either this book or your code. When you’re coding\ninteractively in a console interface, the filename may be just “<stdin>,” meaning\nthe standard input stream.\nWhen working in the IDLE GUI’s interactive shell today, though, the filename is\n“<pyshell…>,” and source lines are displayed, too. Either way, file line numbers\nare not very meaningful when there is no file (you’ll see more interesting error\nmessages later in this part of the book):\n>>> fetcher(food, 5)                        # Default handler – IDLE GUI interface\nTraceback (most recent call last):\n  File \"<pyshell#3>\", line 1, in <module>\n    fetcher(food, 5)\n  File \"<pyshell#0>\", line 2, in fetcher\n    return obj[index]\nIndexError: string index out of range\nIn a more realistic program launched outside the interactive REPL, after printing\nan error message the default handler at the top also terminates the program\nimmediately. That course of action makes sense for simple scripts; errors often",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1327,
      "chapter": null,
      "content": "should be fatal, and the best you can do when they occur is inspect the standard\nerror message.\nCatching Exceptions\nSometimes, though, program termination on exceptions isn’t what you want.\nServer programs, for instance, typically need to remain active even after internal\nerrors. If you don’t want the default exception behavior, wrap the call in a try\nstatement to catch exceptions yourself (copy/pasters: omit the “...” here per the\nnote ahead):\n>>> try:\n...     fetcher(food, 5)\n... except IndexError:                      # Catch and recover\n...     print('got exception')\n...\ngot exception\n>>>\nNow, Python automatically jumps to your handler—the block under the except\nclause that names the exception raised—when an exception is triggered while\nthe try block is running. The net effect is to wrap a nested block of code in an\nerror handler that intercepts the block’s exceptions.\nWhen working interactively like this, after the except clause runs, we wind up\nback at the Python prompt. In a more realistic program, try statements not only\ncatch exceptions but also recover from them:\n>>> def catcher():\n        try:\n            fetcher(food, 5)\n        except IndexError:\n            print('got exception')          # Catch and recover more\n        print('continuing')\n>>> catcher()\ngot exception\ncontinuing\n>>>\nThis time, after the exception is caught and handled, the program resumes",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1328,
      "chapter": null,
      "content": "execution after the entire try statement that caught it—which is why we get the\n“continuing” message here. We don’t see the standard error message, and the\nprogram continues on its way normally.\nNotice, though, that there’s no way in Python to go back to the code that\ntriggered the exception (short of rerunning the code that reached that point all\nover again, of course). Once you’ve caught the exception, control continues after\nthe entire try that caught the exception, not after the statement that kicked off\nthe exception. In fact, Python clears the memory of any functions that were\nexited as a result of the exception, like fetcher in our example; their variables\nare discarded, and they’re not resumable. The try both catches exceptions and is\nwhere the program resumes.\nPython does not, however, undo any work done by the try block up to the point\nwhere the exception occurred—any changes made to referenced mutable objects\nand accessible global names live on. This isn’t a problem if it’s known:\n>>> L, S = [], 'text'\n>>> def modder():\n        L.append('added')                   # Change a mutable\n        global S; S = 'changed'             # Change a global\n        fetcher(food, 5)                    # Trigger an exception\n \n>>> try:\n...     modder()\n... except IndexError:\n...     print('got exception')\n... \ngot exception\n>>> L, S                                    # Changes retained\n(['added'], 'changed')\nAs you’ll see later in this part of the book, the try can also use except* clauses\nto process multiple exceptions, but this is a convoluted extension with narrow\nscope that doesn’t play well with others, and you can safely defer studying until\nyou’ve mastered the fundamentals.\nNOTE\nPresentation note: The interactive REPL’s “...” continuation prompt reappears in this part for\nsome top-level try statements, because their code won’t work if copied and pasted unless",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1329,
      "chapter": null,
      "content": "nested in a function or class (the except and other lines must align with the try, and not have\nextra preceding spaces that are needed to illustrate their indentation structure here). To run,\nsimply type or paste statements with “...” prompts one line at a time, and without their leading\n“...” prompts.\nRaising Exceptions\nSo far, we’ve been letting Python raise exceptions for us by making mistakes (on\npurpose this time!), but our scripts can raise exceptions too—that is, exceptions\ncan be raised by Python or by your program, and can be caught or not. To trigger\nan exception manually, simply run a raise statement. User-triggered exceptions\nare caught the same way as those Python raises. The following may not be the\nmost useful Python code ever penned, but it makes the point—raising the built-\nin IndexError exception:\n>>> try:\n...     raise IndexError                    # Trigger exception manually\n... except IndexError:\n...     print('got exception')\n...\ngot exception\nAs usual, if they’re not caught, user-triggered exceptions are propagated up to\nthe top-level default exception handler and terminate the program with a\nstandard error message:\n>>> raise IndexError\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError\nAs you’ll see in the next chapter, the assert statement can be used to trigger\nexceptions, too—it’s a conditional raise predicated on a test, used mostly for\ndebugging purposes and sanity checks during development:\n>>> assert 1 < 0, 'Not in this universe!'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAssertionError: Not in this universe!",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1330,
      "chapter": null,
      "content": "Also in the next chapter, you’ll learn that raise can use a from clause to “chain”\nexceptions; generally speaking, this is not common, but can be used to give more\ncontext where it’s useful.\nUser-Defined Exceptions\nThe raise statement demos in the prior section raised IndexError, a built-in\nexception defined in Python’s built-in scope. As you’ll learn later in this part of\nthe book, you can also define new exceptions of your own that are specific to\nyour programs. User-defined exceptions are coded with classes, which inherit\nfrom a built-in exception class—usually, the class named Exception:\n>>> class Combust(Exception): pass          # User-defined exception\n>>> def makePizza():\n        raise Combust()                     # Raise an instance\n>>> try:\n...     makePizza()\n... except Combust:                         # Catch class name\n...     print('got exception')\n...\ngot exception\n>>>\nAs you’ll see in upcoming chapters, exception classes allow scripts to build\nexception categories, which can inherit behavior and have attached state\ninformation and methods, and an as clause on an except can gain access to the\nexception object itself. Exception classes can also customize their message text\ndisplayed if they’re not caught:\n>>> class Combust(Exception):\n        def __str__(self): \n            return 'Call the fire department!...'\n>>> raise Combust()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nCombust: Call the fire department!...\n>>>",
      "content_length": 1482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1331,
      "chapter": null,
      "content": "Termination Actions\nFinally, try statements can say “finally”—that is, they may include finally\nblocks. These look like except handlers for exceptions, but the try/finally\ncombination specifies termination actions that always execute “on the way out,”\nregardless of whether an exception occurs in the try block or not. Continuing\nour REPL session:\n>>> try:\n...     fetcher(food, 4)\n... finally:                                # Termination actions\n...     print('after fetch')\n...\n'a'\nafter fetch\n>>>\nHere, if the try block finishes without an exception, the finally block will run,\nand the program will resume after the entire try. In this case, this statement\nseems a bit silly—we might as well have simply typed the print right after a\ncall to the function, and skipped the try altogether:\nfetcher(food, 4)\nprint('after fetch')\nThere is a problem with coding this way, though: if the function call raises an\nexception, the print will never be reached. The try/finally combination\navoids this pitfall—when an exception does occur in a try block, finally\nblocks are executed while the program is being unwound:\n>>> def after():\n        try:\n            fetcher(food, 5)\n        finally:\n            print('after fetch')            # Always run\n        print('after try?')                 # Run only if no exception\n>>> after()\nafter fetch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>",
      "content_length": 1412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1332,
      "chapter": null,
      "content": "File \"<stdin>\", line 3, in after\n  File \"<stdin>\", line 2, in fetcher\nIndexError: string index out of range\n>>>\nHere, we don’t get the “after try?” message because control does not resume\nafter the try statement when an exception occurs. Instead, Python jumps back to\nrun the finally action and then propagates the exception up to a prior handler\n(in this case, to the default handler at the top). If we change the call inside this\nfunction so as not to trigger an exception, the finally code still runs, but the\nprogram continues after the try:\n>>> def after():\n        try:\n            fetcher(food, 4)\n        finally:\n            print('after fetch')            # Both run if no exception\n        print('after try?')\n>>> after()\nafter fetch\nafter try?\n>>>\nIn practice, except clauses in a try are useful for catching and recovering from\nexceptions, and finally clauses come in handy to guarantee that termination\nactions will fire regardless of any exceptions that may occur in the try block’s\ncode. For instance, you might use try/except combinations to catch errors\nraised by code that you import from a third-party library, and try/finally\ncombos to ensure that calls to close files or terminate server connections are\nalways run. We’ll code some such practical examples later in this part of the\nbook.\nAlthough they serve conceptually distinct purposes, you can also mix except\nand finally clauses in the same try statement—the finally is run on the way\nout regardless of whether an exception was raised, and regardless of whether the\nexception was caught by an except clause. Such combos have rules you’ll meet\nin the next chapter.\nAs you’ll also learn in the next chapter, Python provides an alternative to the",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1333,
      "chapter": null,
      "content": "try/finally mix when using some types of built-in and user-defined objects.\nThe with statement runs a context manager object’s methods to guarantee that\ntermination actions occur, irrespective of any exceptions in its nested block:\n>>> with open('pizzarobot.txt', 'w') as file:        # Always close file on exit\n        file.write('Catch fire!\\n')\nAlthough this option requires fewer lines of code, it’s applicable only when\nprocessing certain object types, so try/finally is a more general termination\nstructure, and is often simpler than coding a class in cases where with is not\nalready supported. On the other hand, with may also run startup actions too, and\nsupports user-defined context management code with access to Python’s full\nOOP toolset. To see how, let’s move on to the next chapter.",
      "content_length": 798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1334,
      "chapter": null,
      "content": "Chapter Summary\nAnd that is the majority of the exception story; exceptions really are a simple\ntool.\nTo summarize, Python exceptions are a control-flow device. They may be raised\nby Python, or by your own programs. In both cases, they may be ignored (to\ntrigger the default error handler), or caught by try statements (to be processed\nby your code). The try statement comes in logically distinct forms that can be\ncombined—one that handles exceptions, and one that runs finalization code\nregardless of whether exceptions occur or not. Python’s raise and assert\nstatements trigger exceptions on demand—both built-ins and new exceptions we\ndefine with classes—and the with statement is an alternative way to ensure that\ntermination actions are carried out for objects that support it.\nIn the rest of this part of the book, we’ll fill in some of the details about the\nstatements involved, examine the other sorts of clauses that can appear under a\ntry (spoiler: it also allows an else for the no-exception case), and discuss class-\nbased exception objects. The next chapter begins our tour by taking a closer look\nat the statements we introduced here. Before you turn the page, though, here are\na few quiz questions to review.\nTest Your Knowledge: Quiz\n1. Name three things that exception processing is good for.\n2. What happens to an exception if you don’t do anything special to handle\nit?\n3. How can your script recover from an exception?\n4. Name two ways to trigger exceptions in your script.\n5. Name two ways to specify actions to be run at termination time,\nwhether an exception occurs or not.",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1335,
      "chapter": null,
      "content": "Test Your Knowledge: Answers\n1. Exception processing is useful for error handling, termination actions,\nand event notification. It can also simplify the handling of special cases\nand can be used to implement alternative control flows as a kind of\nstructured “go-to” operation. In general, exception processing also cuts\ndown on the amount of error-checking code your program may require\n—because all errors filter up to handlers, you may not need to test the\noutcome of every operation (see this chapter’s sidebar “Why You Will\nCare: Error Checks” for an illustration).\n2. Any uncaught exception eventually filters up to the default exception\nhandler Python provides at the top of your program. This handler prints\nthe familiar error message and shuts down your program.\n3. If you don’t want the default message and shutdown, you can code try\nstatements with except clauses to catch and recover from exceptions\nthat are raised within its nested code block. Once an exception is\ncaught, the exception is terminated and your program continues after\nthe try.\n4. The raise and assert statements can be used to trigger an exception,\nexactly as if it had been raised by Python itself. In principle, you can\nalso raise an exception by making a programming mistake, but that’s\nnot usually an explicit goal!\n5. The try statement with a finally clause can be used to ensure actions\nare run after a block of code exits, regardless of whether the block raises\nan exception or not. The with statement can also be used to ensure\ntermination actions are run, but only when processing object types that\nsupport it.\nWHY YOU WILL CARE: ERROR CHECKS\nOne way to see how exceptions are useful is to compare coding styles in\nPython and languages without exceptions. For instance, if you want to write\nrobust programs in the C language, you generally have to test return values",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1336,
      "chapter": null,
      "content": "or status codes after every operation that could possibly go astray, and\npropagate the results of the tests as your programs run:\ndoStuff()\n{                                   # C program\n    if (doFirstThing() == ERROR)    # Detect errors everywhere\n        return ERROR;               # even if not handled here\n    if (doNextThing() == ERROR)\n        return ERROR;\n    ...\n    return doLastThing();\n}\nmain()\n{\n    if (doStuff() == ERROR)\n        badEnding();\n    else\n        goodEnding();\n}\nIn fact, realistic C programs often have as much code devoted to error\ndetection as to doing actual work. But in Python, you don’t have to be so\nmethodical (and neurotic!). You can instead wrap arbitrarily vast pieces of a\nprogram in exception handlers and simply write the parts that do the actual\nwork, assuming all is normally well:\ndef doStuff():          # Python code\n    doFirstThing()      # We don't care about exceptions here,\n    doNextThing()       # so we don't need to detect them\n    ...\n    doLastThing()\nif __name__ == '__main__':\n    try:\n        doStuff()       # This is where we care about results,\n    except:             # so it's the only place we must check\n        badEnding()\n    else:               # See the next chapter for else\n        goodEnding()\nBecause control jumps immediately to a handler when an exception occurs,\nthere’s no need to instrument all your code to guard for errors, and there’s no\nextra performance overhead to run all the tests. Moreover, because Python",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1337,
      "chapter": null,
      "content": "detects errors automatically, your code often doesn’t need to check for errors\nin the first place. The upshot is that exceptions let you largely ignore the\nunusual cases and avoid error-checking code that can distract from your\nprogram’s purpose.\n1  For any computer scientists in the audience, true backtracking is not part of the Python language.\nBacktracking undoes computations before it jumps back, but Python exceptions do not: local\nvariables in open function calls run by the try are simply discarded, but changes made to globals and\nobjects are retained (see the demo ahead). Even the generator functions and expressions we met in\nChapter 20 don’t do full backtracking—they simply respond to next(G) requests by restoring saved\nstate and resuming. For more on backtracking, try books on AI or the Prolog or Icon programming\nlanguages.",
      "content_length": 843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1338,
      "chapter": null,
      "content": "Chapter 34. Exception Coding\nDetails\nThe prior chapter provided a quick look at exception-related statements in\naction. Here, we’re going to dig a bit deeper—this chapter provides fuller\ncoverage of exception-processing syntax in Python. Specifically, we’ll explore\nthe details behind the try, raise, assert, and with statements. Although these\nstatements are mostly straightforward, you’ll find that they offer powerful tools\nfor dealing with exceptional conditions in Python code.\nThe try Statement\nFirst up, the try statement is how your code catches exceptions. In short, if an\nexception occurs while running this statement’s main block, the program jumps\nback to run one of the statement’s handlers and continues from there. Its\nhandlers may be specified by except, else, finally, and except* clauses\nnested in the try, and separate rules apply to these clauses’ syntax, and their\nvalid combinations.\nThis is a simple model on the surface, but the try statement’s handler clauses\nhave disjoint purposes, and its rules for valid combinations mean that it comes in\ndistinct flavors. Because of this, we’ll approach this subject by exploring the\ntry’s common roles in isolation first and putting their pieces together later as a\ncombined statement. This parallels the fact that try really was separate\nstatements in Python’s dim past, but our focus here is on its unified present.\nAlthough technically part of the try, we’ll also defer the except* clause until\nthe next chapter, partly because it encroaches on that chapter’s exception-object\ntopic, but mostly because this is a tool that complicates the try story\nsubstantially for an extension that’s rarely useful in practice. Our priority here is\nlearning the fundamentals.",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1339,
      "chapter": null,
      "content": "try Statement Clauses\nWhen you write a try statement, a variety of clauses can appear after and below\nthe try header. Table 34-1 summarizes all the possible forms as both reference\nand preview. We’ve already seen that except clauses catch exceptions and\nfinally clauses run on the way out. New here, else clauses run if no\nexceptions are encountered, and except* clauses process exception groups and\nsupport all except forms except the empty.\nFormally, a try must use at least one of the clauses in Table 34-1. There may be\nany number of except clauses, but you can code else only if there is at least\none except, and there can be only one else and one finally. A finally can\nappear in the same statement as except and else, with ordering rules given later\nin this chapter.\nTable 34-1. try statement clauses and forms\nClause form\nInterpretation\nexcept:\nCatch all (or all other) exception types\nexcept name:\nCatch a specific exception only\nexcept name as var:\nCatch the listed exception and assign its instance\nexcept (name1, name2):\nCatch any of the exceptions listed in a tuple\nexcept (name1, name2) as var:\nCatch any listed exception and assign its instance\nelse:\nRun if no exceptions are raised in the try block\nfinally:\nAlways perform this block on exit, exception or\nnot\nexcept* …nonempty except forms…\n:\nCatch multiple exceptions in a group (Chapter 35)\nWe’ll explore the as var part available in some of Table 34-1’s clauses in more",
      "content_length": 1439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1340,
      "chapter": null,
      "content": "detail when we meet the raise statement later in this chapter because it provides\naccess to the object raised as an exception via var. Before all that, let’s get\nstarted by examining the more common clauses of Table 34-1 more closely.\nThe except and else Clauses\nSyntactically, the try is a compound, multipart statement. It starts with a try\nheader line, followed by a block of (usually) indented statements, which is\nfollowed by clauses that each identify a condition to be handled and give a block\nof statements to handle it. In its most common form, try is coded with one or\nmore except clauses and an optional else clause at the end. You associate the\nwords try, except, and else by indenting them to the same level (i.e., lining\nthem up vertically), like this:\ntry:\n    statements              # Run this main action first\nexcept name1:\n    statements              # Run if name1 is raised during the try block\nexcept (name2, name3):\n    statements              # Run if name2 or name3 occur in the try\nexcept name4 as var:\n    statements              # Run if name4 is raised, and assign it to var\nexcept:\n    statements              # Run for all other exceptions raised\nelse:\n    statements              # Run if no exception was raised in the try block\nSemantically, the block under the try header in this statement represents the\nmain action of the statement—the code you’re trying to run, and wrapping in\nexception handlers. The rest of the statement defines the handlers themselves:\nexcept clauses give handlers for exceptions raised during the try block, and the\noptional else clause gives a handler run if no exceptions occur in the try block.\nWithin a try, each except names exceptions to catch: a single exception catches\njust that exception, a tuple catches any exception in the tuple, and an except that\nomits the exception altogether matches all (or all other) exceptions. Each\nnonempty except can also give a variable name after as to be assigned the\nexception object raised by Python or raise statements; again, we’ll explore this\noption ahead.",
      "content_length": 2066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1341,
      "chapter": null,
      "content": "How try statements work\nOperationally, here’s how try statements are run. When a try statement is\nentered, Python records the current program context so it can return to it if an\nexception occurs. The statements nested under the try header are run first. What\nhappens next depends on whether an exception is raised while the try block’s\nstatements are running, and whether a raised exception matches any of those that\nthe try is watching for:\nException and match\nIf an exception occurs while the try block’s statements are running, and the\nexception matches one that the statement names, Python jumps back to the\ntry and runs the statements under its topmost except clause that matches\nthe raised exception, after assigning the raised exception object to the\nvariable named by as in the clause (if present). After the except block runs,\ncontrol resumes below the entire try statement. If the except block itself\nraises another exception, the propagation process is started anew from this\npoint in the code.\nException and no match\nIf an exception occurs while the try block’s statements are running, but the\nexception does not match one that the statement names, the exception is\npropagated up to the next most recently entered try statement that matches\nthe exception; if no such matching try statement can be found and the\nsearch reaches the top level of the program, Python prints a default error\nmessage and terminates the program (unless it’s the REPL).\nNo exception\nIf an exception does not occur while the try block’s statements are running,\nPython runs the statements under the else clause (if present), and control",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1342,
      "chapter": null,
      "content": "then resumes below the entire try statement. If the else block itself raises\nanother exception, it kicks off the propagation process again.\nIn sum, except clauses catch any matching exceptions that happen while the\ntry block is running, and the else clause runs only if no exceptions happen\nwhile the try block runs. Exceptions raised are matched to exceptions named in\nexcept clauses by class relationships we’ll explore both ahead and in the next\nchapter (brief: a subclass matches its superclass), and the empty except clause\nwith no exception name matches any exception.\nIn effect, except clauses are focused exception handlers—they catch exceptions\nthat occur only within the statements in the associated try block. However, as\nthe try block’s statements can call functions coded elsewhere in a program, the\nsource of an exception may very well be outside the code of the try statement\nitself.\nIn fact, a try block might invoke arbitrarily large amounts of program code—\nincluding code that may have try statements of its own, which will be searched\nfirst when exceptions occur. In other words, because try statements can nest at\nruntime, where an exception goes depends on the code run before it, a\nphenomenon we’ll explore in Chapter 36.\nIf a finally clause is added to a try, its code block is run for all three of the\ncases listed previously, as you’ll see ahead. First, though, let’s take a look at\nsome common variations of exception-catching clauses.\nCatching many exceptions with a tuple\nPer the fourth and fifth entries in Table 34-1, except clauses that list many\nexceptions in a parenthesized tuple catch any of the listed exceptions. Because\nPython looks for a match within a given try by inspecting the except clauses\nfrom top to bottom, the tuple version has the same effect as listing each\nexception in its own except clause, but you have to code the common statement\nbody associated with each only once.\nHere’s a partial example of multiple except clauses at work, which demos just",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1343,
      "chapter": null,
      "content": "how specific your handlers can be:\ntry:\n    …\nexcept NameError:\n    …\nexcept IndexError:\n    …\nexcept (AttributeError, TypeError, SyntaxError):\n    …\nIf an exception is raised while this try block is running, Python returns to the\ntry and searches for the first except that names the exception raised. It inspects\nclauses from top to bottom—and left to right along the way—and runs the\nstatements under the first clause that matches. If none match, the exception is\npropagated past this try.\nNote that parentheses are required around the tuple in the “any” form, and using\nan as in this form lets you check which exception occurred when you listed\nmany:\ntry:\n    …\nexcept (AttributeError, TypeError, SyntaxError) as What:\n    …and check What…\nTo learn more about both as, as well as what happens when no except matches,\nwe must move on.\nCatching all exceptions with empties and Exception\nPer the first entry in Table 34-1, except clauses that list no exception name\ncatch all exceptions not previously listed in the try statement. That is, if you\nwant to code a general “catchall” handler to be run when no other except clause\nmatches the exception raised, an empty except does the trick:\ntry:\n    …\nexcept NameError:\n    …                   # Handle NameError\nexcept IndexError:",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1344,
      "chapter": null,
      "content": "…                   # Handle IndexError\nexcept:\n    …                   # Handle all other exceptions\nelse:\n    …                   # Handle the no-exception case (preview)\nThe empty except clause is a sort of wildcard feature—because it catches\neverything, adding it to the mix allows your handlers to be as general or specific\nas you like. In some scenarios, this form may be more convenient than listing all\npossible exceptions in a try—especially when working interactively in a REPL,\nor writing code that must recover no matter what occurs. For example, the\nfollowing catches everything by not listing anything:\ntry:\n    …\nexcept:\n    …                   # Catch all possible exceptions\nThat being shown, the empty except can also cause problems. It may catch\nunexpected exceptions, and intercept events unrelated to your code and required\nby another handler. For example, even system exit calls and Ctrl+C key-\ncombination interrupts in Python work by triggering exceptions, and you usually\nwant these to pass.\nPerhaps worse, the empty except may also catch genuine programming\nmistakes for which you probably want to see an error message. Otherwise, you\nmay not even know that a bug exists until it’s too late to avoid a user’s report.\nWe’ll revisit this as a gotcha at the end of this part of the book. For now, the\nstandard “use with care” applies.\nPython provides an alternative that solves at least one of these problems—\ncatching the built-in Exception has almost the same effect as an empty except,\nbut won’t catch exceptions related to system exits and Ctrl+C:\ntry:\n    …\nexcept Exception:\n    …                   # Catch all possible exceptions - except exits\nWe’ll explore how this form does its magic formally in the next chapter when we",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1345,
      "chapter": null,
      "content": "study exception classes. In short, it works because exceptions match if they are a\nsubclass of one named in an except clause, and Exception is a superclass of all\nthe exceptions you should generally catch this way. This form has most of the\nsame convenience of the empty except without the risk of catching exit events\nand also allows you to check the exception raised via as. While better, though, it\nalso has some of the same risks—it may still mask and silently ignore\nprogramming errors.\nThe opening snippet of this section deliberately listed an else clause, to call out\nthat it is not a catchall like the empty except—an understandable source of\nconfusion for try newcomers. The next section dissects the difference.\nCatching the no-exception case with else\nAll told, else can be used in three places in Python: in if selections, for and\nwhile loops, and try exception handlers. In the latter, else is run when no\nexception occurs—not for unmatched exceptions (that’s what the prior section’s\nempty except is for). This else role may seem different than in if and loops,\nbut this context differs.\nThe need for an else clause in try is not always obvious to Python beginners.\nWithout it, though, there is no direct way to tell whether the flow of control has\nproceeded past a try statement because no exception was raised, or because an\nexception occurred and was handled. Either way, we wind up after the try:\ntry:\n    …run code…\nexcept IndexError:\n    …handle exception…\n# Did we get here because the try failed or not?\nOf course, we could initialize, set, and check a Boolean flag to know what\nhappened, which adds lines of admin code. Much like the way else clauses in\nloops make the exit cause more apparent (for exits sans break), the else clause\nprovides syntax in try that makes the outcome unambiguous with minimal extra\ncode:\ntry:",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1346,
      "chapter": null,
      "content": "…run code…\nexcept IndexError:\n    …handle exception…\nelse:\n    …no exception occurred…\nYou can almost emulate an else clause by moving its code into the try block:\ntry:\n    …run code…\n    …no exception occurred…\nexcept IndexError:\n    …handle exception…\nThis can lead to incorrect exception classifications, though. If the “no exception\noccurred” action itself causes an IndexError, it will register as a failure of the\ntry block and erroneously trigger the exception handler below the try (unlikely\nperhaps, but true). By using an explicit else clause instead, you make the logic\nmore obvious and guarantee that except handlers will run only for real failures\nin the code you’re wrapping in a try, not for failures in the else no-exception\ncase’s action.\nExample: Default behavior\nBecause the control flow through a program may be easier to capture in Python\nthan in English, let’s run some simple examples that further illustrate exception\nbasics with real code in files.\nAs noted, exceptions not caught by try statements percolate up to the top level\nof the Python process and run Python’s default exception-handling logic (i.e.,\nPython terminates the running program and prints a standard error message). To\nillustrate, module file crashware.py coded in Example 34-1 generates a divide-\nby-zero exception—by design.\nExample 34-1. crashware.py\ndef gobad(x, y):\n   return x / y\ndef gosouth(x):\n   print(gobad(x, 0))\nif __name__ == '__main__': gosouth(1)",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1347,
      "chapter": null,
      "content": "Because the program ignores the exception it triggers, Python kills the program\nand prints a message (edited here to condense paths and drop some interfaces’\ncode-pointer lines for space):\n$ python3 crashware.py\nTraceback (most recent call last):\n  File \"/…/LP6E/Chapter34/crashware.py\", line 7, in <module>\n    if __name__ == '__main__': gosouth(1)\n  File \"/…/LP6E/Chapter34/crashware.py\", line 5, in gosouth\n    print(gobad(x, 0))\n  File \"/…/LP6E/Chapter34/crashware.py\", line 2, in gobad\n    return x / y\nZeroDivisionError: division by zero\nThis message consists of a stack trace (“Traceback”) and the name of and details\nabout the exception that was raised. The stack trace lists all lines active when the\nexception occurred, from oldest to newest. Note that because this code was\nwritten in a file instead of a REPL, the file and line-number information is more\nuseful here. For example, we can see that the bad divide happens at the last entry\nin the trace—line 2 of the file crashware.py, a return statement.\nBecause Python detects and reports all errors at runtime by raising exceptions\nlike this, exceptions are intimately bound up with the ideas of error handling and\ndebugging in general. If you’ve worked through this book’s examples, you’ve\nundoubtedly seen an exception or two along the way—even typos usually\ngenerate a SyntaxError or other exception when a file is imported or executed\n(that’s when the code compiler is run).\nBy default, programming errors generate a useful error display like the one just\nshown, which helps you track down the problem. In some interfaces, this\nmessage today even comes with pointers to offending expressions, as well as\nspeculative but mandatory “Did you?” tips (whose merit you may wish to\nreweigh later in your Python career):\n$ python3\n>>> import crashware\n>>> crashware.gobad(1, 0)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/…/LP6E/Chapter34/crashware.py\", line 2, in gobad",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1348,
      "chapter": null,
      "content": "return x / y\n           ~~^~~\nZeroDivisionError: division by zero\nOften, this standard error message is all you need to resolve problems in your\ncode. For more heavy-duty debugging jobs, you can catch exceptions with try\nstatements, or use debugging tools introduced in Chapter 3, such as the pdb\nstandard-library module. See “Debugging Python Code” for more related tips.\nExample: Catching built-in exceptions\nPython’s error checking and default exception handling is often exactly what you\nwant: especially for code in a top-level script file, an error often should terminate\nyour program immediately. For many programs, there is no need to be more\nspecific about errors in your code.\nSometimes, though, you’ll want to catch errors and recover from them instead. If\nyou don’t want your program terminated when Python raises an exception,\nsimply catch it by wrapping the program logic in a try. This is an important\ncapability for programs such as network servers, which must keep running\npersistently. For example, the code in Example 34-2, file kaboom.py, catches and\nrecovers from the TypeError Python raises immediately when you try to\nconcatenate a list and a string (remember, the + operator expects the same\nsequence type on both sides).\nExample 34-2. kaboom.py\ndef kaboom(x, y):\n   print(x + y)                        # Trigger TypeError\ndef serve(n=2):                         # Simulate long-running task\n   for i in range(n):\n       try:\n           kaboom([1, 2], 'hack')\n       except TypeError:               # Catch and recover here\n           print('Hello world!')\n       print('Resuming here...')       # Continue here if exception or not\nif __name__ == '__main__': serve()\nWhen the exception occurs in the function kaboom, control jumps to the try\nstatement’s except clause, which prints a message. Since an exception is “dead”",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1349,
      "chapter": null,
      "content": "after it’s been caught like this, the program continues executing below the try\nrather than being terminated by Python. In effect, the code processes and clears\nthe error, and your script recovers:\n$ python3 kaboom.py\nHello world!\nResuming here...\nHello world!\nResuming here...\nKeep in mind that once you’ve caught an error, control resumes at the place\nwhere you caught it (i.e., after the try); there is no direct way to go back to the\nplace where the exception occurred (here, in the function kaboom). This makes\nexceptions more like simple jumps than function calls—there is no way to\n“return” to the scene of the crime.\nThe finally Clause\nThe other main flavor of the try statement is for coding finalization (a.k.a.\ntermination) actions and is really related to exceptions only incidentally. If a\nfinally clause is included in a try, Python will always run its block of\nstatements “on the way out” of the try statement—whether an exception\noccurred while the try block was running or not. That is, this clause doesn’t\ncatch exceptions, it works around them.\nWhen used in isolation, this flavor’s general form is this:\ntry:\n    statements           # Run this action first\nfinally:\n    statements           # Always run this code on the way out\nWhen a finally appears in try, Python begins by running the statement block\nassociated with the try header line as usual. What happens next depends on\nwhether an exception occurs during the try block, and what other clauses are\npresent:\nException and match",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1350,
      "chapter": null,
      "content": "If an exception occurs during the try block’s run and is matched by an\nexcept clause, Python first runs the matching except block and then runs\nthe finally block. After both finish, the program then resumes below the\nentire try statement. The finally is also run if the except raises a new\nexception.\nException and no match\nIf an exception occurs during the try block’s run but is not caught by an\nexcept, Python still comes back and runs the finally block, but it then\npropagates the exception up to a previously entered try or the top-level\ndefault handler. That is, finally is run even if an exception is raised and\nuncaught, but unlike an except, the finally does not terminate the\nexception—it continues being raised after the finally block runs.\nNo exception\nIf an exception does not occur while the try block is running, Python first\nruns the else block (if present) and then runs the finally block. After both\nfinish, the program then resumes below the entire try statement. The\nfinally is also run if the else raises a new exception.\nThe try/finally form is useful when you want to be completely sure that an\naction will happen after some code runs, regardless of the exception behavior of\nthe code. In practice, it allows you to specify cleanup actions that always must\noccur, such as file closes and server disconnects where required.\nTechnically speaking, finally can appear in the same statement as except and\nelse, so there is really a single try statement with many optional clauses.\nBecause of its distinct role, though, as well as its ordering rules we will meet in a\nmoment, the finally clause may be best thought of as a distinct tool. Whether\nmixed or not, finally serves the same purpose—to specify cleanup actions that",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1351,
      "chapter": null,
      "content": "must always be run, regardless of any exceptions.\nAs you’ll also see later in this chapter, the with statement and its context\nmanagers provide an object-based way to do similar work for exit actions.\nUnlike finally, this statement also supports entry actions, but it is limited in\nscope to objects that implement the context-manager protocol it employs.\nExample: Coding termination actions with try/finally\nWe coded some simple try/finally examples in the prior chapter. Example 34-\n3 lists a more tangible example that illustrates a typical role for this statement.\nExample 34-3. closer.py\nclass MyError(Exception): pass\ndef stuff(file):\n   file.write('Hello?')             # May be delayed in file buffer\n   raise MyError()                  # <= Enable or disable me with a #\nif __name__ == '__main__':\n   file = open('temp.txt', 'w')     # Open an output file (this can fail too)\n   try:\n       stuff(file)                  # Raises exception\n   finally:\n       file.close()                 # Always close file to flush output buffers\n   print('Am I reached?')           # Continue here only if no exception\nWhen the function in this code raises its exception, the control flow jumps back\nand runs the finally block to close the file. The exception is then propagated\non to either another try or the default top-level handler, which prints the\nstandard error message and shuts down the program. Hence, the statement after\nthis try is never reached:\n$ python3 closer.py \nTraceback (most recent call last):\n  File \"/…/LP6E/Chapter34/closer.py\", line 10, in <module>\n    stuff(file)                      # Raises exception\n  File \"/…/LP6E/Chapter34/closer.py\", line 5, in stuff\n    raise MyError()                  # <= Enable or disable me with a #\nMyError\nIf the function here did not raise an exception (e.g., by disabling its raise line\nwith an added #), the program would still execute the finally block to close the",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1352,
      "chapter": null,
      "content": "file, but it would then continue below the entire try statement:\n$ python3 closer.py \nAm I reached?\nIn this specific case, we’ve wrapped a call to a file-processing function in a try\nwith a finally clause to make sure that the file is always closed, and thus\nfinalized, whether the function triggers an exception or not. This way, later code\ncan be sure that the file’s output buffer’s content has been flushed from memory\nto disk. A similar code structure can guarantee that server connections are\nclosed, GUI windows are closed, and so on.\nAs we learned in Chapter 9, file objects are automatically closed on garbage\ncollection in standard Python (CPython); this is especially useful for temporary\nfiles that we don’t assign to variables. However, it’s not always easy to predict\nwhen garbage collection will occur, especially in larger programs or alternative\nPython implementations with differing garbage collection policies. The try\nstatement makes file closes more explicit and predictable: it ensures that the file\nwill be closed on block exit, regardless of whether an exception occurs or not.\nThis particular example’s function isn’t all that useful (it always raises an\nexception!), but wrapping calls in try/finally statements is a good way to\nensure that your closing-time termination activities always run. All bets are off if\nPython itself crashes completely, of course, but this is exceedingly rare; because\nit detects errors as a program runs, hard crashes are usually caused by linked-in\nC extension code, outside of Python’s scope.\nAs a preview, notice how the user-defined exception in Example 34-3 is defined\nwith a class; as you’ll learn more formally in the next chapter, exceptions must\nall be class instances, for reasonably good causes.\nCombined try Clauses\nFor the first 15 years of Python’s tenure (more or less), the try statement came\nin two flavors and was two separate statements—we could either use a finally\nto ensure that cleanup code was always run, or write except blocks to catch and\nrecover from specific exceptions and optionally specify an else clause for when",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1353,
      "chapter": null,
      "content": "exceptions occurred.\nThat is, the finally clause could not be mixed with except and else. This was\npartly because of implementation issues, and partly because the meaning of\nmixing the two seemed obscure—catching and recovering from exceptions\nseemed a disjoint concept from performing cleanup actions.\nFor better or worse, the two statements eventually merged. Today, we can mix\nfinally, except, and else clauses in the same statement—in part because of\nsimilar utility in the Java language (alas, many a programming-language mod\nowes to imitation). That is, the try statement in its most complete form looks\nlike this:\ntry:                       # Combined try statement\n    main-action\nexcept Exception1:         # Catch specific exceptions\n    handler1\nexcept Exception2:         \n    handler2\nexcept:                    # Catch all (other) exceptions\n    handler3\nelse:                      # No-exception handler\n    handler4\nfinally:                   # The finally encloses all\n    finally-block\nThe code in this statement’s main-action block is executed first, as usual. If\nthat code raises an exception, all the except blocks are tested, one after another,\nfor a match to the exception raised: handler1 is run for Exception1, handler2\nfor Exception2, and handler3 for all others. If no exception is raised,\nhandler4 is run.\nNo matter what’s happened previously, the finally-block is executed once,\nafter the main action block is exited and any handler block has been run. In fact,\nthe code in the finally-block will be run even if an error or raise in an\nexcept or else block causes a new exception to be raised.\nAs outlined earlier, even in mixed usage like this, the finally clause does not\nend the exception—if an exception is active when the finally-block is\nexecuted, it continues to be propagated after the finally-block runs, and",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1354,
      "chapter": null,
      "content": "control jumps somewhere else in the program (to an earlier try, or to the default\ntop-level handler). If no exception is active when the finally is run, control\nresumes after the entire try statement.\nThe net effect is that the finally is always run, regardless of whether:\nAn exception occurred in the main action and was handled.\nAn exception occurred in the main action and was not handled.\nNo exceptions occurred in the main action.\nA new exception was triggered in one of the handlers.\nAgain, the finally serves to specify cleanup actions that must always occur on\nthe way out of the try, regardless of what exceptions have been raised or\nhandled.\nCombined-clause syntax rules\nWhen combined like this, the try statement must have either an except or a\nfinally, and the order of its parts must be like this (where “->” means “is\nfollowed by”):\ntry -> except -> else -> finally\nIn this, the else and finally are optional, and there may be zero or more\nexcepts, but there must be at least one except if an else appears. Really, the\ntry statement consists of two parts: excepts with an optional else, and/or the\nfinally.\nIn fact, it’s more accurate to describe the combined try statement’s syntactic by\nthe following two alternative formats (where square brackets mean optional and\nstar means any number of what precedes it):\ntry:                             # Format 1\n    statements\nexcept [type [as value]]:\n    statements\n[except [type [as value]]:\n    statements]*",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1355,
      "chapter": null,
      "content": "[else:\n    statements]\n[finally:\n    statements]\ntry:                             # Format 2\n    statements\nfinally:\n    statements\nBecause of these rules, the else can appear only if there is at least one except,\nand it’s always possible to mix except and finally, regardless of whether an\nelse appears or not. It’s also possible to mix finally and else, but only if an\nexcept appears too (though the except can omit an exception name to catch\neverything and run a raise statement, described later, to reraise the current\nexception). If you violate any of these (arguably intricate!) ordering rules,\nPython will raise a syntax error exception before your code runs.\nCombining finally and except by nesting\nIt may help to realize that it’s also possible to combine finally and except\nclauses in a try by syntactically nesting a try/except in the try block of a\ntry/finally statement. We’ll explore this technique more fully in Chapter 36,\nbut the following has the same effect as the combined form shown at the start of\nthis section:\ntry:                         # Nested equivalent to combined form\n    try:\n        main-action\n    except Exception1:\n        handler1\n    except Exception2:\n        handler2\n    except:\n        handler3\n    else:\n        handler4\nfinally:\n    finally-block\nAgain, the finally block is always run on the way out, regardless of what\nhappened in the main action and regardless of any exception handlers run in the",
      "content_length": 1445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1356,
      "chapter": null,
      "content": "nested try (trace through the four cases listed previously to see how this works\nthe same). Since an else always requires an except, this nested form even\nsports the same mixing constraints of the combined form outlined in the\npreceding section.\nHowever, this nested equivalent seems more obscure to some people and\nrequires more code than the new merged form—though just one four-character\nline plus extra indentation. Mixing finally into the same statement might make\nyour code easier to write and read, though this also might depend on who you\nask.\nCombined-clauses example\nTo demo the effect of mixing finally with other try clauses, the script listed in\nExample 34-4, trycombos.py, codes four common scenarios, with print\nstatements that describe the meaning of each.\nExample 34-4. trycombos.py\nsep = '-' * 45 + '\\n'\nprint(sep + 'EXCEPTION RAISED AND CAUGHT')\ntry:\n   x = 'hack'[99]\nexcept IndexError:\n   print('except run')\nfinally:\n   print('finally run')\nprint('after run')\nprint(sep + 'EXCEPTION NOT RAISED')\ntry:\n   x = 'hack'[3]\nexcept IndexError:\n   print('except run')\nfinally:\n   print('finally run')\nprint('after run')\nprint(sep + 'EXCEPTION NOT RAISED, WITH ELSE')\ntry:\n   x = 'hack'[3]\nexcept IndexError:",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1357,
      "chapter": null,
      "content": "print('except run')\nelse:\n   print('else run')\nfinally:\n   print('finally run')\nprint('after run')\nprint(sep + 'EXCEPTION RAISED BUT NOT CAUGHT')\ntry:\n   x = 1 / 0\nexcept IndexError:\n   print('except run')\nfinally:\n   print('finally run')\nprint('after run')\nWhen this code is run, the following output is produced. Trace through the code\nto see how exception handling produces the output of each of the four tests here:\n$ python3 trycombos.py\n---------------------------------------------\nEXCEPTION RAISED AND CAUGHT\nexcept run\nfinally run\nafter run\n---------------------------------------------\nEXCEPTION NOT RAISED\nfinally run\nafter run\n---------------------------------------------\nEXCEPTION NOT RAISED, WITH ELSE\nelse run\nfinally run\nafter run\n---------------------------------------------\nEXCEPTION RAISED BUT NOT CAUGHT\nfinally run\nTraceback (most recent call last):\n  File \"/…/LP6E/Chapter34/trycombos.py\", line 38, in <module>\n    x = 1 / 0\nZeroDivisionError: division by zero\nThis example uses built-in operations in the main action to trigger exceptions (or\nnot), and it relies on the fact that Python always checks for errors as code is\nrunning. The next section shows how to raise exceptions manually instead.",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1358,
      "chapter": null,
      "content": "The raise Statement\nTo trigger exceptions explicitly, code raise statements. Their general form is\nsimple—a raise statement consists of the word raise, optionally followed by\nthe class to be raised, or an instance of it:\nraise instance           # Raise an instance of a class\nraise class              # Make and raise an instance of a class\nraise                    # Reraise the most recent exception\nAs mentioned earlier, exceptions are always instances of classes today. Hence,\nthe first raise form here is the most common—we provide an instance directly,\neither created before the raise or within the raise statement itself. If we pass a\nclass instead, Python calls the class with no constructor arguments, to create an\ninstance to be raised; this form is equivalent to adding parentheses after the class\nreference. The last form reraises the most recently raised exception; it’s\ncommonly used in exception handlers to propagate exceptions that have been\ncaught.\nNOTE\nBlast from the past: Long ago and far away (well, before Python 2.6), exceptions could be\nidentified as simple string objects, with an optional associated data item in raise. This was\nreplaced with classes to support added functionality and categories, as you’ll see in the next\nchapter. Still, strings were a simpler model for simpler roles, didn’t require newcomers to learn\nclasses and OOP before exceptions, and didn’t force some Python books to put exceptions on\nhold until Part VII!\nRaising Exceptions\nTo make raise more concrete, let’s turn to some examples. With built-in\nexceptions, the following two forms are equivalent—both raise an instance of\nthe exception class named, but the first creates the instance implicitly:\nraise IndexError             # Class (instance created)\nraise IndexError()           # Instance (created in statement)\nWe can also create the instance ahead of time—because the raise statement",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1359,
      "chapter": null,
      "content": "accepts any kind of object reference, the following two examples raise\nIndexError just like the prior two:\nexc = IndexError()           # Create instance ahead of time\nraise exc\nexcs = [IndexError, TypeError]\nraise excs[0]\nIn fact, the instance provided to raise can be had in the try that catches it too,\nper the next section.\nThe except as hook\nWhen an exception is raised, Python sends the raised instance along with the\nexception. If a try includes an except with an as clause per Table 34-1, the\nvariable it gives will be assigned the instance raised by raise or Python:\ntry:\n    …\nexcept IndexError as X:      # X assigned the raised instance object\n    …\nThe as is optional in a try handler (if it’s omitted, the instance is simply not\nassigned to a name), but including it allows the handler to access both data in the\ninstance and methods in the exception class.\nThis model works the same for user-defined exceptions we code with classes—\nthe following, for example, passes to the exception class constructor arguments\nthat become available in the handler through the assigned instance:\nclass MyExc(Exception): pass\ntry:\n    raise MyExc('oops')      # Exception class with constructor args\nexcept MyExc as X:           # Instance attributes available in handler\n    print(X.args)            # Prints ('oops',)\nBecause this encroaches on the next chapter’s topic, though, we’ll defer further\ndetails until then.",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1360,
      "chapter": null,
      "content": "Regardless of their source, exceptions are always identified by class instance\nobjects, and at most one is active at any given time (sans the except* groups of\nChapter 35). Once caught by an except clause anywhere in the program, an\nexception ends and won’t propagate to another try, unless it’s reraised by\nanother raise statement or error.\nScopes and except as\nWe’ll study exception objects in more detail in the next chapter. Now that we’ve\nseen the as variable in action, though, we can finally clarify the related scope\nissue summarized back in Chapter 17. As mentioned in that chapter, the variable\nused to access an exception in the as clause of an except is localized to the\nexcept block—the variable is not available after the block exits, much like a\ntemporary loop variable in comprehension expressions:\n$ python3\n>>> try:\n...     1 / 0\n... except Exception as X:            # The \"as\" localizes names to except block\n...     print(X)\n...\ndivision by zero\n>>> X\nNameError: name 'X' is not defined\nUnlike comprehension loop variables, though, this variable is removed after the\nexcept block exits. This is done because the variable would otherwise retain a\nreference to the runtime call stack, which would defer garbage collection and\nthus retain excess memory space. This removal occurs, though, even if you’re\nusing the name for other purposes in the surrounding scope, and is a much more\nextreme policy than that used for comprehensions:\n>>> X = 99\n>>> {X for X in 'hack'}               # Comprehensions localize but don't remove\n{'a', 'c', 'k', 'h'}\n>>> X\n99\n>>> X = 99\n>>> try:",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1361,
      "chapter": null,
      "content": "...     1 / 0\n... except Exception as X:            # But \"as\" localizes _and_ removes on exit!\n...     print(X)\n...\ndivision by zero\n>>> X                                 # Where did my X go? – an odd boundary case\nNameError: name 'X' is not defined\nBecause of this, you should generally use unique variable names in your try\nstatement’s except clauses, even if they are localized by scope. If you do need\nto reference the exception instance after the try statement, simply assign it to\nanother name that won’t be automatically removed:\n>>> try:\n...     1 / 0\n... except Exception as X:            # Python removes this reference\n...     print(X)\n...     saveit = X                    # Assign exc to retain exc if needed\n...\ndivision by zero\n>>> X\nNameError: name 'X' is not defined\n>>> saveit\nZeroDivisionError('division by zero',)\nPropagating Exceptions with raise\nThe raise statement is a bit more feature-rich than we’ve seen thus far. For\nexample, a raise that does not list an exception to raise simply reraises the\ncurrently active exception. This form is typically used if you need to catch and\nhandle an exception but don’t want the exception to die in your handler:\n>>> try:\n...     raise IndexError('code')         # Exceptions remember arguments\n... except IndexError:\n...     print('propagating')\n...     raise                            # Reraise most recent exception\n...\npropagating\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nIndexError: code",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1362,
      "chapter": null,
      "content": "Running a raise this way reraises the exception and propagates it to a higher\nhandler (or the default handler at the top, which stops the program with a\nstandard error message). Notice how the argument we passed to the exception\nclass shows up in the error messages; you’ll learn why this happens in the next\nchapter.\nException Chaining: raise from\nExceptions can sometimes be triggered in response to other exceptions—both\ndeliberately and by new program errors. To support full disclosure in such cases,\nPython also allows raise statements to have an optional from clause:\nraise newexception from otherexception\nWhen the from is used in an explicit raise request, the expression following\nfrom specifies another exception class or instance to attach to the __cause__\nattribute of the new exception being raised. If the raised exception is not caught,\nPython prints both exceptions as part of the standard error message:\n>>> try:\n...     1 / 0\n... except Exception as E:\n...     raise TypeError('Bad') from E             # Explicitly chained exceptions\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nZeroDivisionError: division by zero\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\nTypeError: Bad\nWhen an exception is raised implicitly by a program error inside an exception\nhandler, a similar procedure is followed automatically: the previous exception is\nattached to the new exception’s __context__ attribute and is again displayed in\nthe standard error message if the exception goes uncaught:\n>>> try:",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1363,
      "chapter": null,
      "content": "...     1 / 0\n... except:\n...     badname                                   # Implicitly chained exceptions\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nZeroDivisionError: division by zero\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\nNameError: name 'badname' is not defined\nIn both cases, because the original exception objects thus attached to new\nexception objects may themselves have attached causes, the causality chain can\nbe arbitrarily long, and is displayed in full in error messages. That is, error\nmessages might give more than two exceptions. The net effect in both explicit\nand implicit chaining contexts is to allow programmers to know all exceptions\ninvolved when one exception triggers another:\n>>> try:\n...     try:\n...         raise IndexError()\n...     except Exception as E:\n...         raise TypeError() from E\n... except Exception as E:\n...     raise SyntaxError() from E\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 3, in <module>\nIndexError\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"<stdin>\", line 5, in <module>\nTypeError\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"<stdin>\", line 7, in <module>\nSyntaxError: None\nCode like the following similarly displays three exceptions, though implicitly",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1364,
      "chapter": null,
      "content": "triggered by handler errors (its separator lines are “During handling of the above\nexception…” instead of “The above exception was the direct cause…”):\ntry:\n    try:\n        1 / 0\n    except:\n        badname\nexcept:\n    open('nonesuch')\nException chains impact error displays, but do not affect the way that exceptions\nare named and caught in try statements: chains are simply recorded in\nexception-object attributes which may be inspected as usual where useful.\nLike the combined try, chained exceptions are similar to utility in other\nlanguages (including Java and C#) though it’s not clear which languages were\nborrowers. In Python, it’s not unusual to see exception chains in error messages,\nbut it is uncommon to create them explicitly with raise, so we’ll defer to\nPython’s manuals for more details.\nAs a footnote on this topic, though, Python also provides a way to stop\nexceptions from chaining: a raise from None allows the display of the chained\nexception context to be disabled when needed. This makes for less cluttered\nerror messages in applications that convert between exception types while\nprocessing exception chains.\nThe assert Statement\nAs a somewhat special case for debugging purposes, Python also includes the\nassert statement in its exceptions toolset. It is mostly just syntactic shorthand\nfor a common raise usage pattern, and an assert can be thought of as a\nconditional raise statement. A statement of the form:\nassert test, data              # The data part is optional\nworks like the following code:",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1365,
      "chapter": null,
      "content": "if __debug__:\n    if not test:\n        raise AssertionError(data)\nIn other words, if the test evaluates to false, Python raises an exception: the\ndata item (if it’s provided) is used as the exception’s constructor argument. Like\nall exceptions, the built-in AssertionError exception will kill your program if\nit’s not caught with a try, and the data item shows up as part of the standard\nerror message:\n>>> language = 'Java'\n>>> assert language.startswith('Py'), \"You're using the wrong language!\"\nAssertionError: You're using the wrong language!\nAs an added feature, assert statements are removed from a compiled program’s\nbytecode—and hence not run—if the -O Python command-line flag is used to\noptimize the program. The __debug__ flag is a built-in and unchangeable name\nthat is automatically set to True unless the -O flag is used. When __debug__ is\nFalse for -O, any code predicated on it being True is removed, including\nasserts.\nHence, to disable (and omit) asserts, run code with a command line like python\n–O file.py, or generate optimized bytecode before program runs with similar\noptions in the compileall standard-library module or compile built-in function.\nSee Python’s manuals for details on the module and function.\nExample: Trapping Constraints (but Not Errors!)\nHere’s a less politically charged example of assert in action. Assertions are\ntypically used to verify program conditions during development. When\ndisplayed, their error message text automatically includes source code line\ninformation and the value listed in the assert statement. Consider the file\nasserter.py in Example 34-5.\nExample 34-5. asserter.py\ndef f(x):\n   assert x < 0, 'x must be negative'\n   return x ** 2",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1366,
      "chapter": null,
      "content": "Running this normally triggers the assertion error for positive numbers, but\nrunning with -O does not:\n$ python3\n>>> import asserter\n>>> asserter.f(-3)\n9\n>>> asserter.f(3)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/…/LP6E/Chapter34/asserter.py\", line 2, in f\n    assert x < 0, 'x must be negative'\nAssertionError: x must be negative\n$ python3 -O\n>>> import asserter\n>>> asserter.f(3)\n9\nIt’s important to keep in mind that assert is mostly intended for trapping user-\ndefined constraints, not for catching genuine programming errors. Because\nPython traps programming errors itself, there is usually no need to code assert\nto catch things like out-of-bounds indexes, type mismatches, and zero divides:\ndef reciprocal(x):\n    assert x != 0              # A generally useless assert!\n    return 1 / x               # Python checks for zero automatically\nSuch assert use cases are usually superfluous—because Python raises\nexceptions on errors automatically, you might as well let it do the job for you. As\na rule, you normally don’t need to do error checking explicitly in your own code.\nOf course, there are exceptions to most rules. As suggested earlier in the book, if\na function has to perform long-running or unrecoverable actions before it\nreaches the place where an exception will be triggered, you still might want to\ntest for errors. Even in this case, though, be careful not to make your tests overly\nspecific or restrictive, or you will limit your code’s utility.\nFor another example of common assert usage, see the abstract superclass\nexample in Chapter 29; there, we used assert to make calls to undefined\nmethods fail with a message. It’s a rare but useful tool.",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1367,
      "chapter": null,
      "content": "The with Statement and Context Managers\nIn addition to the tools we’ve seen so far, Python includes another that delegates\nexception-related tasks to objects. The with statement is designed to work with\ncontext manager objects that support a method-based protocol. The combination\nis similar in spirit to the way that iteration tools like for work with methods of\nthe iteration protocol.\nThe with statement is also similar to a “using” statement in the C# language.\nAlthough a somewhat optional and advanced tools-oriented topic (and once a\ncandidate for the next part of this book), context managers are lightweight and\nuseful enough to group with the rest of the exception toolset here.\nIn short, the with statement is designed to be an alternative to a common\ntry/finally usage idiom: like that statement, with is in large part intended for\nspecifying termination-time or “cleanup” activities that must run regardless of\nwhether an exception occurs during the execution of a block of code.\nUnlike try/finally, the with statement is based upon a method-call protocol\nfor specifying actions to be run around a block of code. This makes with less\ngeneral, qualifies it as redundant in termination roles, and requires coding\nclasses for objects that do not support its protocol. On the other hand, with also\nhandles entry actions, can reduce code size where supported, and allows code\ncontexts to be managed with full OOP.\nPython enhances some built-in tools with context managers, such as files that\nautomatically close themselves, thread locks that automatically lock and unlock,\nand async-function tools that automatically await results per Chapter 20, but\nprogrammers can code context managers of their own with classes, too. Let’s\ntake a brief look at the statement and its implicit protocol.\nBasic with Usage\nThe basic format of the with statement looks like this, with an optional part in\nsquare brackets here:\nwith expression [as variable]:\n    with-block",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1368,
      "chapter": null,
      "content": "The statements of the nested with-block are the main action to be run here. The\nexpression is assumed to return an object that supports the context-\nmanagement protocol (more on this protocol in a moment). This object may also\nreturn a value that will be assigned to the name variable if the optional as\nclause is present.\nNote that the variable is not necessarily assigned the result of the expression;\nthe result of the expression is the object that supports the context protocol, and\nthe variable may be assigned something else intended to be used inside the\nwith-block. The object returned by the expression may then run startup code\nbefore the block is started, as well as termination code after the block is done—\nwhether the block raised an exception or not.\nAs noted, some built-in Python objects have been augmented to support the\ncontext-management protocol, and so can be used with the with statement. For\nexample, file objects (covered in Chapter 9) have a context manager that\nautomatically closes the file after the with block regardless of whether an\nexception is raised, and regardless of if or when the version of Python running\nthe code may close automatically. In abstract code:\nwith open('somefile.txt') as myfile:\n    for line in myfile:\n        print(line)\nHere, the call to open returns a simple file object that is assigned to the name\nmyfile. We can use myfile with the usual file tools—in this case, the file\niterator reads line by line in the for loop.\nHowever, the open result also supports the context-management protocol used\nby the with statement. After this with statement has run, the context\nmanagement machinery guarantees that the file object referenced by myfile is\nautomatically closed, even if the for loop raised an exception while processing\nthe file.\nAlthough file objects may be automatically closed on garbage collection, it’s not\nalways straightforward to know when that will occur, especially when using\nalternative Python implementations. The with statement in this role is an",
      "content_length": 2023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1369,
      "chapter": null,
      "content": "alternative that allows us to be sure that the close will occur automatically after\nexecution of a specific block of code.\nAs covered earlier, we can achieve a similar effect with the more general and\nexplicit try/finally idiom, but it requires three more lines of administrative\ncode in this case (four instead of just one):\nmyfile = open('somefile.txt')\ntry:\n    for line in myfile:\n        print(line)\nfinally:\n    myfile.close()\nOf course, we could skip both statements, but our file may not be closed if an\nexception is raised during the for loop, and this can matter in long-running\nprograms (we’ll revisit such trade-offs in Chapter 36).\nAs another example, we won’t cover Python’s multithreading modules in this\nbook, but the lock and condition synchronization objects they define may also be\nused with the with statement because they support the context-management\nprotocol—in this case adding both entry and exit actions around a block. After\nimporting threading:\nlock = threading.Lock()   \nwith lock:\n    …access shared resources…\nHere, the context management machinery guarantees that the lock is\nautomatically acquired before the block is executed and released once the block\nis complete, regardless of exception outcomes.\nFinally, the decimal module introduced in Chapter 5 also uses context managers\nto simplify saving and restoring the current decimal context, which specifies the\nprecision and rounding characteristics for calculations:\nwith decimal.localcontext() as ctx:\n    ctx.prec = 2\n    x = decimal.Decimal('1.00') / decimal.Decimal('3.00')",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1370,
      "chapter": null,
      "content": "After this statement runs, the current thread’s context manager state is\nautomatically restored to what it was before the statement began. To do the same\nwith a try/finally, we would need to save the context before and restore it\nmanually after the nested block.\nThe Context-Management Protocol\nAlthough some built-in types come with context managers, we can also write\nnew ones of our own. To implement context managers, classes use special\nmethods that fall into the operator-overloading category to tap into the with\nstatement. The interface expected of objects used in with statements is\nsomewhat complex, and most programmers only need to know how to use\nexisting context managers. For tool builders who might want to write new\napplication-specific context managers, though, let’s take a quick look at what’s\ninvolved.\nHere’s how the with statement actually works:\n1. The expression is evaluated, resulting in an object known as a context\nmanager that must have __enter__ and __exit__ methods.\n2. The context manager’s __enter__ method is called. The value it returns\nis assigned to the variable in the as clause if present, or simply\ndiscarded otherwise.\n3. The code in the nested with block is executed.\n4. If the with block raises an exception, the context manager’s\n__exit__(type, value, traceback) method is called with the\nexception details. These are the same three values returned by\nsys.exc_info, described in the Python manuals and later in this part of\nthe book. If this method returns a false value, the exception is reraised;\notherwise, the exception is terminated. The exception should normally\nbe reraised so that it is propagated outside the with statement after\n__exit__ returns.\n5. If the with block does not raise an exception, the __exit__ method is\nstill called, but its type, value, and traceback arguments are all",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1371,
      "chapter": null,
      "content": "passed in as None, and its return value is ignored.\nLet’s look at a quick demo of the protocol in action. The file withas.py in\nExample 34-6 defines a context-manager object that simply traces the entry and\nexit of the with block in any with statement it is used for.\nExample 34-6. withas.py\n\"A context manager that traces entry and exit of any with statement's block\"\nclass TraceBlock:\n   def message(self, arg):\n       print('running ' + arg)\n   def __enter__(self):\n       print('[starting with block]')\n       return self\n   def __exit__(self, exc_type, exc_value, exc_tb):\n       if exc_type is None:\n           print('[exited normally]\\n')\n       else:\n           print(f'[propagating exception: {exc_type}]')\n           return False\nif __name__ == '__main__':\n   with TraceBlock() as action:\n       action.message('test 1')\n       print('reached')\n   with TraceBlock() as action:\n       action.message('test 2')\n       raise TypeError\n       print('not reached')\nNotice that this class’s __exit__ method returns False to propagate the\nexception; deleting the return statement would have the same effect, as the\ndefault None return value of functions is false by definition, but explicit is\ngenerally better in coding. Also notice that the __enter__ method returns self\nas the object to assign to the as variable; in other use cases, this might return a\ncompletely different object instead.\nWhen run, this module’s self-test code uses its context manager to trace the entry\nand exit of two with statement blocks. The net effect automatically invokes the\nmanager’s __enter__ and __exit__ methods:",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1372,
      "chapter": null,
      "content": "$ python3 withas.py\n[starting with block]\nrunning test 1\nreached\n[exited normally]\n[starting with block]\nrunning test 2\n[propagating exception: <class 'TypeError'>]\nTraceback (most recent call last):\n  File \"/…/LP6E/Chapter34/withas.py\", line 25, in <module>\n    raise TypeError\nTypeError\nContext managers can also utilize OOP state information and inheritance, but are\nsomewhat advanced devices meant for tool builders, so we’ll skip additional\ndetails here. See Python’s standard manuals for the full story—including its\ncoverage of the contextlib standard module that provides additional tools for\ncoding context managers.\nAlso remember that the try/finally combination provides support for\ntermination-time activities too, and is generally sufficient in roles that don’t\nwarrant coding classes to support the with statement’s protocol.\nMultiple Context Managers\nThe with statement has one last card to turn over: it may also specify multiple\n(sometimes called “nested”) context managers with comma syntax. For example,\nin the following code snippet that selects lines by substring, both files’ exit\nactions are automatically run to close the files when the statement block exits,\nregardless of exception outcomes:\nwith open('lines.txt') as input, open('matches.txt', 'w') as output:\n    for line in input:\n        if 'somekey' in line:\n            output.write(line)\nAny number of context manager items may be listed, and multiple items work\nthe same as nested with statements. That is, the following hypothetical code:\nwith A() as a, B() as b:",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1373,
      "chapter": null,
      "content": "statements\nis equivalent to (and possibly simpler than) the following:\nwith A() as a:\n    with B() as b:\n        statements\nThe net effect is that each context manager’s entry and exit method is run in turn\non block entry and exit, and exceptions in the block are caught automatically and\npossibly reraised on with exit at the discretion of the outermost manager’s exit\nmethod. Multiple file context managers, for instance, will all be run to open files\non entry, and close them on exit—exception or not.\nThe Termination-Handlers Shoot-Out\nYou can find more info on context managers in Python’s docs. Rather than\ngetting more detailed here, let’s close out this chapter with a quick look at this\nextension in action and a vetting of its roles. Using newer and redundant tools\nlike with doesn’t in and of itself prove intelligence, and it’s important to\nunderstand the trade-offs such options imply.\nFirst up, the following codes a parallel lines scan of files located in this book’s\nexamples package. It uses with to open two files at once and then reads and zips\ntogether their next-line pairs on each iteration of a for loop. Thanks to the file\nobject’s context manager, there’s no need to manually catch exceptions or close\nfiles when finished:\n>>> with open('lines1.txt') as file1, open('lines2.txt') as file2:\n        for pair in zip(file1, file2):\n            print(pair)\n('hack\\n', 'HACK\\n')\n('code\\n', 'GOOD\\n')\n('well\\n', 'CODE\\n')\nYou might also use this coding structure to do a lines comparison of two text\nfiles. The following simply replaces the former’s print with an if for a\ncomparison operation, and adds an enumerate for automatic line numbers:",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1374,
      "chapter": null,
      "content": ">>> with open('lines1.txt') as file1, open('lines2.txt') as file2:\n        for (linenum, (line1, line2)) in enumerate(zip(file1, file2)):\n            if line1.lower() != line2.lower():\n                print(f'{linenum} => {line1!r} != {line2!r}')\n \n1 => 'code\\n' != 'GOOD\\n'\n2 => 'well\\n' != 'CODE\\n'\nThat said, with isn’t all that useful in the preceding examples when using\nCPython, because input file objects don’t require a buffer flush, and file objects\nare closed automatically when garbage collected if still open. Moreover,\nexceptions in the with block are still propagated outside the statement if not\nexplicitly caught. Hence, the temporary files would be auto-closed immediately\nand exception behavior would be the same for simpler code like this:\nfor pair in zip(open('lines1.txt'), open('lines2.txt')):     # Same if auto close\n    print(pair)                                              # Ditto for != test\nOn the other hand, some of the alternative Pythons of Chapter 2 may use\ndifferent garbage collectors that require direct closes, to avoid taxing system\nresources. In addition, output files may require closes to ensure that any buffered\ncontent is transferred to disk so it’s available for opens in later code.\nThe following lines filter code addresses both concerns, by automatically closing\nfiles on statement exit, exception or not (it also uses parentheses and line splits\nafter with, available as of Python 3.10, and omits write counts for brevity):\n>>> with (open('lines1.txt') as input, \n          open('uppers.txt', 'w') as output):\n        for line in input:\n            output.write(line.upper())\n   \n>>> print(open('uppers.txt').read())          # File content is available here\nHACK\nCODE\nWELL\nStill, in simple scripts, we can often just open files in separate statements and\nclose after processing if needed. There’s no point in catching an exception if it\nmeans your program is out of business anyhow, and closes are required to flush\noutput buffers only if files will be reopened by later code—which may never be",
      "content_length": 2047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1375,
      "chapter": null,
      "content": "reached after exceptions anyhow:\ninput  = open('lines1.txt')\noutput = open('uppers.txt', 'w')\nfor line in input:                            # Same effect if files auto close,\n    output.write(line.upper())                # and file is not reopened ahead\nNevertheless, in programs that must both continue after exceptions and close\noutput files for later use in the same program (or REPL) run, the with avoids an\nequivalent try/finally combination that may be more obvious to some readers,\nbut also requires noticeably more code—eight lines instead of four,\nquantitatively speaking:\ninput  = open('lines1.txt')\noutput = open('uppers.txt', 'w')\ntry:                                         # Same but explicit close on errors\n    for line in input:\n        output.write(line.upper())\nfinally:\n    input.close()                            # Ensure output file is complete\n    output.close()                           # Whether exception occurs or not\nEven so, the try/finally is a single tool that applies to all finalization cases and\nmakes code explicit. The with can be more concise for context-manager users,\nbut applies only to objects that implement its complex protocol, relies on\nimplicit “magic” that obscures meaning, and adds redundancy that doubles the\nrequired knowledge base of programmers. As usual, you’ll have to weigh these\ntools’ trade-offs for yourself.",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1376,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we took a more detailed look at exception processing by\nexploring the statements related to exceptions in Python: try to catch them,\nraise to trigger them, assert to raise them conditionally, and with to wrap\ncode blocks in context managers that automate entry and exit actions.\nUp to this point, exceptions may seem like a fairly lightweight tool (apart from\nthe with protocol, that is). The most complex thing about them may be how they\nare identified—a topic the next chapter will address by showing how exception\nobjects are made. As you’ll see there, classes allow you to code new exceptions\nspecific to your programs. Before we move ahead, though, let’s work through\nthe following short quiz on the basics covered here.\nTest Your Knowledge: Quiz\n1. What is the try statement for?\n2. What are the two common variations of the try statement?\n3. What is the raise statement for?\n4. What is the assert statement designed to do, and what other statement\nis it like?\n5. What is the with statement designed to do, and what other statement is\nit like?\nTest Your Knowledge: Answers\n1. The try statement catches and recovers from exceptions—it specifies a\nblock of code to run and one or more handlers for exceptions that may\nbe raised during the block’s execution.\n2. The two common variations on the try statement are try/except/else",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1377,
      "chapter": null,
      "content": "(for catching exceptions) and try/finally (for specifying cleanup\nactions that must occur whether an exception is raised or not). Despite\nthese logically distinct roles, the except and finally blocks may be\nmixed in the same statement, so the two forms are really part of the\nsingle try statement. Even when mixed with except, though, the\nfinally is still run on the way out of the try, regardless of what\nexceptions may have been raised or handled. In fact, the combined form\nis equivalent to nesting a try/except/else in a try/finally.\n3. The raise statement raises (triggers) an exception. Python raises built-\nin exceptions on errors internally, but your scripts can trigger built-in or\nuser-defined exceptions too with raise.\n4. The assert statement raises an AssertionError exception if a\ncondition is false. It’s similar to a conditional raise statement wrapped\nup in an if statement, and can be disabled with a –O command switch.\n5. The with statement is designed to automate startup and termination\nactivities that must occur around a block of code. It is roughly like a\ntry/finally combination in that its exit actions run whether an\nexception occurred or not, but it employs an object-based protocol for\nspecifying entry and exit actions and may reduce code size for context-\nmanger users. Still, it’s not quite as general, as it applies only to objects\nthat support its protocol; try with finally clauses can handle more use\ncases.",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1378,
      "chapter": null,
      "content": "Chapter 35. Exception Objects\nSo far, this book has been somewhat vague about what an exception actually is.\nThis chapter clears up the mystery by disclosing the facts behind exception\nobjects—both built-in and user-defined. As suggested in the preceding chapters,\nexceptions are identified by class instance objects. This is what is raised and\npropagated along by exception processing, and the source of the class matched\nagainst except clauses in try statements.\nAlthough this means you must use object-oriented programming to define new\nexceptions in your programs—and introduces a knowledge dependency\nlamented in the prior chapter’s note—basing exceptions on classes and OOP\noffers a number of benefits. Among them, class-based exceptions support:\nFlexible exception categories\nException classes allow code to choose specificity and ease future changes.\nAdding new exception subclasses, for example, need not require changes in\ntry statements.\nState information and behavior\nException classes provide a natural place to store context for use in the try\nhandler. Both attributes and methods, for example, are available on the raised\ninstance.\nReuse by inheritance\nExceptions classes can participate in inheritance hierarchies to obtain and\ncustomize common behavior. Inherited error displays, for example, can\nprovide a common look and feel.\nBecause of these advantages, class-based exceptions support program evolution\nand larger systems well. As you’ll learn here, all built-in exceptions are\nidentified by classes and are organized into an inheritance tree for the reasons\njust listed. You can do the same with user-defined exceptions of your own.",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1379,
      "chapter": null,
      "content": "In fact, the built-in exceptions we’ll study here turn out to be integral to new\nexceptions you define. Because Python largely requires user-defined exceptions\nto inherit from built-in exception classes that provide useful defaults for printing\nand state, the task of coding user-defined exceptions also involves understanding\nthe roles of these built-ins.\nException Classes\nWhether built-in or user-defined, exceptions work much of their magic by\nsuperclass relationships: a raised exception matches an except clause if that\nclause names the exception’s class or any superclass of it. Put another way, a try\nstatement’s except matches both the class it lists, as well as all of that class’s\nsubclasses lower in the class tree.\nThe net effect is that class exceptions naturally support the construction of\nexception hierarchies: superclasses become category names, and subclasses\nbecome specific kinds of exceptions within a category. By naming a general\nexception superclass, an except clause can catch an entire category of\nexceptions—any more specific subclass will match.\nIn addition to this category idea, class-based exceptions support exception state\ninformation and allow exceptions to inherit common behaviors, as noted. To see\nhow all these assets come together in code, let’s turn to an example.\nCoding Exceptions Classes\nIn the file listed in Example 35-1, categoric.py, we define a superclass called\nGeneral and two subclasses called Specific1 and Specific2. This example\nillustrates the notion of exception categories—General is a category name, and\nits two subclasses are specific types of exceptions within the category. Handlers\nthat catch General will also catch any subclasses of it, including Specific1 and\nSpecific2.\nExample 35-1. categoric.py\nclass General(Exception): pass\nclass Specific1(General): pass\nclass Specific2(General): pass",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1380,
      "chapter": null,
      "content": "def raiser0():\n   X = General()            # Raise superclass instance\n   raise X\ndef raiser1():\n   X = Specific1()          # Raise subclass instance\n   raise X\ndef raiser2():\n   X = Specific2()          # Raise different subclass instance\n   raise X\nfor func in (raiser0, raiser1, raiser2):\n   try:\n       func()\n   except General:          # Match General or any subclass of it\n       import sys\n       print('caught:', sys.exc_info()[0])\nWhen this example runs, its try statement catches and reports instances of all\nthree of its classes because the except clause names their common superclass:\n$ python3 categoric.py\ncaught: <class '__main__.General'>\ncaught: <class '__main__.Specific1'>\ncaught: <class '__main__.Specific2'>\nThis code is mostly straightforward, but here are a few points to notice:\nException superclass\nClasses used to build exception category trees have very few requirements—\nin fact, in this example, they are mostly empty, with bodies that do nothing\nbut pass. Notice, though, how the top-level class here inherits from the built-\nin Exception class. This is required: classes that don’t inherit from a built-in\nexception class won’t work in most exception contexts. The built-in\nsuperclass is normally Exception, the root for nonexit exceptions, but may\nalso be BaseException, the root for all exceptions, or other. Although we\ndon’t employ it here, Exception provides behavior you’ll meet later that\nmakes inheriting from it useful, required or not.",
      "content_length": 1478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1381,
      "chapter": null,
      "content": "Raising instances\nIn this code, we call classes to make instances for the raise statements\n(notice the parentheses). In the class exception model, we always raise and\ncatch a class instance object. If we list a class name without parentheses in a\nraise, Python makes an instance for us by calling the class with no\nconstructor arguments. Exception instances can be created before the raise,\nas done here, or within the raise statement itself.\nCatching categories\nThis code includes functions that raise instances of all three of our classes as\nexceptions, as well as a top-level try that calls the functions and catches\nGeneral exceptions. The same try also catches the two specific exceptions\nbecause they are subclasses of General—that is, members of its category.\nException details\nThe exception handler here uses the sys.exc_info call, which is one way to\nfetch the exception being handled in a generic fashion. As you’ll see in more\ndetail in the next chapter, the first item in this call’s result tuple is the class of\nthe exception raised, and the second is the actual instance raised. In a general\nexcept clause like the one here that catches all classes in a category,\nsys.exc_info can be used to determine exactly what has occurred.\nThe last point merits elaboration. When an exception is caught, we can be sure\nthat the instance raised is an instance of the class listed in the except or one of\nits subclasses. Because of that, the specific kind of exception raised can also be\nhad via the type result or __class__ attribute of the instance, regardless of how\nthe instance is obtained.\nTo demo, the variant in Example 35-2 works the same as the prior example but",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1382,
      "chapter": null,
      "content": "uses the as extension in its except clause to directly assign a variable to the\ninstance raised, from which type yields the exception’s kind.\nExample 35-2. categoric2.py\nclass General(Exception): pass\nclass Specific1(General): pass\nclass Specific2(General): pass\ndef raiser0(): raise General()\ndef raiser1(): raise Specific1()\ndef raiser2(): raise Specific2()\nfor func in (raiser0, raiser1, raiser2):\n   try:\n       func()\n   except General as X:                 # X is the raised instance\n       print('caught:', type(X))        # Same as sys.exc_info()[0], X.__class__\nBecause the except’s as can be used to access the exception directly this way,\nsys.exc_info is more useful for empty except clauses that do not otherwise\nhave a way to access the instance or its class. More importantly, well-designed\nprograms usually should not have to care about which specific exception was\nraised at all—calling methods of the exception instance should automatically\ndispatch to behavior tailored for the exception raised.\nThere’s more on this and sys.exc_info and its ilk in the next chapter. Also, see\nChapter 29 and Part VI at large if you’ve forgotten what __class__ means in an\ninstance, and the prior chapter for a review of the as used here.\nWhy Exception Hierarchies?\nBecause there are only three possible exceptions in the prior section’s examples,\nit doesn’t really do justice to the utility of class exceptions. In principle, we\ncould achieve the same effects by coding a list of exception names in a\nparenthesized tuple within the except clause:\ntry:\n    func()\nexcept (General, Specific1, Specific2):     # Catch any of these\n    …",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1383,
      "chapter": null,
      "content": "This approach may work for smaller, self-contained code. For large or high\nexception hierarchies, however, it will probably be easier to catch categories\nusing class-based categories than to list every member of a category in a single\nexcept clause. Perhaps more importantly, you can extend exception hierarchies\nas software needs evolve by adding new subclasses without breaking existing\nhandler code.\nSuppose, for example, you code a numeric programming library in Python to be\nused by a large number of people. While you are writing your library, you\nidentify two things that can go wrong with numbers in your code—division by\nzero and numeric overflow. You document these as the two standalone\nexceptions that your library may raise:\n# mathlib.py\nclass Divzero(Exception): pass\nclass Oflow(Exception): pass\ndef func():\n    …\n    raise Divzero()\n…and so on…\nNow, when people use your library, they typically wrap calls to your functions or\nclasses in try statements that catch your two exceptions; after all, if they do not\ncatch your exceptions, exceptions from your library will kill their code:\n# client.py\nimport mathlib\ntry:\n    mathlib.func()\nexcept (mathlib.Divzero, mathlib.Oflow):\n    …handle and recover…\nThis works fine, and lots of people start using your library. Six months down the\nroad, though, you revise it (as programmers are prone to do). Along the way, you\nidentify a new thing that can go wrong—underflow, perhaps—and add that as a\nnew exception:\n# mathlib.py\nclass Divzero(Exception): pass",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1384,
      "chapter": null,
      "content": "class Oflow(Exception): pass\nclass Uflow(Exception): pass\nUnfortunately, when you re-release your code, you create a maintenance\nproblem for your users. If they’ve listed your exceptions explicitly, they now\nhave to go back and change every place they call your library to include the\nnewly added exception name:\n# client.py\ntry:\n    mathlib.func()\nexcept (mathlib.Divzero, mathlib.Oflow, mathlib.Uflow):\n    …handle and recover…\nThis may not be the end of the world. If your library is used only in-house, you\ncan make the changes yourself. You might also ship a Python script that tries to\nfix such code automatically (it would probably be only a few dozen lines, and it\nwould guess right at least some of the time). If many people have to change all\ntheir try statements each time you alter your exception set, though, this is not\nexactly the politest of upgrade policies.\nYour users might try to avoid this pitfall by coding empty except clauses to\ncatch all possible exceptions:\n# client.py\ntry:\n    mathlib.func()\nexcept:                            # Catch everything here (or catch Exception)\n    …handle and recover…\nBut as noted in the prior chapter, this workaround might catch more than they\nbargained for—things like running out of memory, keyboard interrupts (Ctrl+C),\nsystem exits, and even typos in their own try block’s code will all trigger\nexceptions, and such things should pass, not be caught and erroneously classified\nas library errors. Catching the Exception superclass improves on this but still\nintercepts—and thus may mask—program errors.\nAnd really, in this scenario, users want to catch and recover from only the\nspecific exceptions the library is defined and documented to raise. If any other\nexception occurs during a library call, it’s likely a genuine bug in the library (and",
      "content_length": 1807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1385,
      "chapter": null,
      "content": "it’s probably time to contact the vendor). As a rule of thumb, it’s usually better to\nbe specific than general in exception handlers—an idea we’ll revisit as a\n“gotcha” in the next chapter.\nSo what to do, then? In principle again, the library module could provide a tuple\nobject that contains all the exceptions it can possibly raise. The client could then\nimport the tuple and name it in an except clause to catch all the library’s\nexceptions (recall that using a tuple catches any of its exceptions). This would\nwork and support mods, but you’d need to keep the tuple up-to-date with library\nexceptions, and that’s both error-prone and tedious.\nClass exception hierarchies solve this dilemma better. Rather than defining your\nlibrary’s exceptions as a set of autonomous classes, arrange them into a class tree\nwith a common superclass to encompass the entire category:\n# mathlib.py\nclass NumErr(Exception): pass\nclass Divzero(NumErr): pass\nclass Oflow(NumErr): pass\ndef func():\n    …\n    raise DivZero()\n…and so on…\nThis way, users of your library simply need to list the common superclass (i.e.,\ncategory) to catch all of your library’s exceptions—both now and in the future:\n# client.py\nimport mathlib\ntry:\n    mathlib.func()\nexcept mathlib.NumErr:\n    …handle and recover…\nWhen you go back and hack (update) your code again now, you can add new\nexceptions as new subclasses of the common superclass:\n# mathlib.py\n…\nclass Uflow(NumErr): pass",
      "content_length": 1445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1386,
      "chapter": null,
      "content": "The end result is that user code that catches your library’s exceptions will keep\nworking, unchanged. In fact, you are free to add, delete, and change exceptions\narbitrarily in the future—as long as clients name the superclass, and that\nsuperclass remains intact, they are insulated from changes in your exceptions set.\nIn other words, class exceptions provide a better answer to maintenance issues\nthan other solutions can.\nClass-based exception hierarchies also support state retention and inheritance in\nways that make them ideal in larger programs. To understand these roles, though,\nwe first need to see how user-defined exception classes relate to the built-in\nexceptions from which they inherit.\nBuilt-in Exception Classes\nThe prior section’s example wasn’t really pulled out of thin air. All built-in\nexceptions that Python itself may raise are predefined class objects. Moreover,\nthey are organized into a shallow hierarchy with general superclass categories\nand specific subclass types, much like the prior section’s final exceptions class\ntree.\nAll the familiar exceptions you’ve seen (e.g., SyntaxError) are really just\npredefined classes, available as built-in names in the module named builtins.\nIn addition, Python organizes the built-in exceptions into a hierarchy to support a\nvariety of catching modes. For example:\nBaseException: topmost root, with printing and constructor defaults\nThe top-level root superclass of exceptions. This class is not supposed to be\ndirectly inherited by user-defined classes (use Exception instead). It\nprovides default printing and state retention behavior inherited by subclasses.\nIf the str built-in is called on an instance of this class (e.g., by print), the\nclass returns the display strings of the constructor arguments passed when the\ninstance was created (or an empty string if there were no arguments). In\naddition, unless subclasses replace this class’s constructor, all of the\narguments passed to this class at instance construction time are stored in its",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1387,
      "chapter": null,
      "content": "args attribute as a tuple.\nException: root of user-defined exceptions\nThe top-level root superclass of application-related exceptions. This is an\nimmediate subclass of BaseException and is a superclass to every other\nbuilt-in exception, except the system exit event classes (SystemExit,\nKeyboardInterrupt, and GeneratorExit) and an exception-group class\nwe’ll ignore here. Nearly all user-defined classes should inherit from this\nclass, not BaseException. When this convention is followed, naming\nException in a try statement’s handler ensures that your program will catch\neverything but system exit events, which should normally be allowed to pass.\nIn effect, Exception becomes a catchall in try statements but is more\naccurate than an empty except.\nArithmeticError: root of numeric errors\nA subclass of Exception, and the superclass of all numeric errors. Its\nsubclasses identify specific numeric errors: OverflowError,\nZeroDivisionError, and FloatingPointError.\nLookupError: root of indexing errors\nA subclass of Exception, and the superclass category for indexing errors for\nboth sequences and mappings: IndexError and KeyError.\nOSError: root of IO and other system-function errors, with details\nA subclass of Exception, with attributes for error details (e.g., errno,\nstrerror, and filename), and subclasses for specific errors:\nFileNotFoundError, PermissionError, TimeoutError, and more.",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1388,
      "chapter": null,
      "content": "And so on—because the built-in exception set is prone to frequent changes, this\nbook doesn’t document it exhaustively. You can read further about its contents\nand structure in the Python library manual.\nBuilt-in Exception Categories\nThe built-in class tree allows you to choose how specific or general your\nhandlers will be. For example, because the built-in exception ArithmeticError\nis a superclass for more specific exceptions such as OverflowError and\nZeroDivisionError:\nBy listing ArithmeticError in a try, you will catch any kind of\nnumeric error raised.\nBy listing ZeroDivisionError, you will intercept just that specific type\nof error and no others.\nSimilarly, because Exception is the superclass of all application-level\nexceptions, you can generally use it as a catchall—as outlined in the prior\nchapter, the effect is much like an empty except, but it allows system exit\nexceptions to pass and propagate as they usually should:\ntry:\n    …\nexcept Exception:                               # Exits not caught here\n    …handle all application exceptions…\nelse:\n    …handle no-exception case…\nThis technique is reliable because Python requires all classes to derive from\nbuilt-in exceptions. Still, this scheme suffers most of the same potential pitfalls\nas the empty except, as described in the prior chapter—it might intercept\nexceptions intended for elsewhere, and it might mask genuine programming\nerrors. Since this is such a common issue, we’ll revisit it one more time as a\n“gotcha” in the next chapter.\nWhether or not you will leverage the categories in the built-in class tree, it serves\nas a good example. By using similar techniques for class exceptions in your own\ncode, you can provide exception sets that are flexible and easily modified.",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1389,
      "chapter": null,
      "content": "Default Printing and State\nBuilt-in exceptions also provide default print displays and state retention, which\nis often as much logic as user-defined classes require. Unless you redefine the\nconstructors your classes inherit from built-ins, any constructor arguments you\npass to these classes are automatically saved in the instance’s args tuple\nattribute and are automatically displayed when the instance is printed. An empty\ntuple and display string are used if no constructor arguments are passed, and a\nsingle argument displays as itself (not as a tuple) and serves as message details.\nThis explains why arguments passed to built-in exception classes show up in\nerror messages—any constructor arguments are attached to the instance and\ndisplayed when the instance is printed:\n>>> raise IndexError                     # Same as IndexError(): no arguments\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError\n>>> raise IndexError('bad')              # Constructor argument attached, printed\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: bad\n \n>>> i = IndexError('bad', 'stuff')       # Available in object attribute \"args\"\n>>> i.args\n('bad', 'stuff')\n>>> print(i)                             # Displays args when printed manually\n('bad', 'stuff')\n>>> i                                    # Uses repr for echo, str for print\nIndexError('bad', 'stuff')\nThe same holds true for user-defined exceptions because they inherit the\nconstructor and display methods present in their built-in superclasses:\n>>> class E(Exception): pass\n \n>>> raise E\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nE\n>>> raise E('bad')\nTraceback (most recent call last):",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1390,
      "chapter": null,
      "content": "File \"<stdin>\", line 1, in <module>\nE: bad\n>>> i = E('bad', 'stuff')\n>>> i.args\n('bad', 'stuff')\n>>> print(i)\n('bad', 'stuff')\n>>> i\nE('bad', 'stuff')\nWhen intercepted in a try statement, the exception instance object gives access\nto both the original constructor arguments and the display method:\n>>> try:\n...     raise E('bad')                     # Displays + saves constructor args\n... except E as X:\n...     print(f'{X} - {X.args} - {X!r}')\n...\nbad - ('bad',) - E('bad')\n>>> try:\n...     raise E('bad', 'stuff')            # Multiple args save/display a tuple\n... except E as X:\n...     print(f'{X} - {X.args} - {X!r}')\n...\n('bad', 'stuff') - ('bad', 'stuff') - E('bad', 'stuff')\nNote that exception instance objects are not strings themselves, but use the\n__str__ and __repr__ operator-overloading methods we studied in Chapter 30\nto provide display strings for print and other contexts. To concatenate with real\nstrings, perform manual conversions: str(X), '%s' % X, f'{X}', and the like.\nAlthough this automatic state and display support is useful by itself, for more\nspecific display and state retention needs, you can always redefine inherited\nmethods such as __str__ and __init__ in Exception subclasses—as the next\nsection shows.\nCustom Print Displays\nAs we saw in the preceding section, by default, instances of class-based\nexceptions display whatever you passed to the class constructor when they are",
      "content_length": 1414,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1391,
      "chapter": null,
      "content": "caught and printed:\n>>> class MyBad(Exception): pass\n>>> try:\n...     raise MyBad('Sorry--my mistake!')\n... except MyBad as X:\n...     print(X)\n...\nSorry--my mistake!\nThis inherited default display model is also used if the exception is displayed as\npart of an error message when the exception is not caught:\n>>> raise MyBad('Sorry--my mistake!')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nMyBad: Sorry--my mistake!\nFor many roles, this is sufficient. To provide a more custom display, though, you\ncan define one of two string-representation overloading methods in your class\n(__repr__ or __str__) to return the string you want to display for your\nexception. The string the method returns will be displayed if the exception either\nis caught and printed or reaches the default handler:\n>>> class MyBad(Exception):\n        def __str__(self):\n            return 'Stuff happens...'\n>>> try:\n...     raise MyBad()\n... except MyBad as X:\n...     print(X)\n...\nStuff happens...\n>>> raise MyBad()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nMyBad: Stuff happens...\nWhatever your method returns is included in error messages for uncaught\nexceptions and used when exceptions are printed explicitly. The method returns",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1392,
      "chapter": null,
      "content": "a hardcoded string here to illustrate, but it can also perform arbitrary text\nprocessing, possibly using state information attached to the instance object. The\nnext section looks at state information options.\nFirst, though, one fine point: you generally must redefine __str__ for exception\ndisplay purposes because the built-in exception superclasses already have a\n__str__ method, and __str__ is preferred to __repr__ in some contexts—\nincluding error-message displays. If you define a __repr__, printing will\nhappily call the built-in superclass’s __str__ instead:\n>>> class Oops(Exception):\n        def __repr__(self): return 'Custom display not used'\n>>> raise Oops(\"Nobody's perfect\")\n…\nOops: Nobody's perfect\nBut a custom __str__ is used if defined:\n>>> class Oops(Exception):\n        def __str__(self): return 'Custom display used'\n>>> raise Oops(\"Nobody's perfect\")\n…\nOops: Custom display used\nSee Chapter 30 for more details on these special operator-overloading methods.\nCustom State and Behavior\nBesides supporting flexible hierarchies, exception classes also provide storage\nfor extra state information as instance attributes. As discussed earlier, built-in\nexception superclasses provide a default constructor that automatically saves\nconstructor arguments in an instance tuple attribute named args. Although the\ndefault constructor is adequate for many cases, for more custom needs we can\nprovide a constructor of our own. In addition, classes may define methods for\nuse in handlers that provide precoded exception processing logic.",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1393,
      "chapter": null,
      "content": "Providing Exception Details\nWhen an exception is raised, it may cross arbitrary file boundaries—the raise\nstatement that triggers an exception and the try statement that catches it may be\nin completely different module files. It is not generally feasible to store extra\ndetails in global variables because the try statement might not know which file\nthe globals reside in. Passing extra state information along in the exception itself\nallows the try statement to access it more reliably.\nWith classes, this is nearly automatic. As we’ve seen, when an exception is\nraised, Python passes the class instance object along with the exception. Code in\ntry statements can access the raised instance by listing an extra variable after the\nas keyword in an except handler. This provides a natural hook for supplying\ndata and behavior to the handler. Generic instance-access tools like\nsys.exc_info used earlier enable the same interfaces.\nFor example, a program that parses data files might signal a formatting error by\nraising an exception instance that is filled out with extra details about the error\n(the input file here isn’t real because this demo dies before reading it!):\n>>> class FormatError(Exception):\n        def __init__(self, line, file):            # Custom constructor\n            self.line = line\n            self.file = file\n>>> def parser(file):                              # Parse the file first\n        raise FormatError(62, file=file)           # When an error is found\n>>> try:\n...     parser('code.py')\n... except FormatError as X:\n...     print(f'Error at: {X.file} #{X.line}')     # Custom state info\n...\nError at: code.py #62\nIn the except clause here, the variable X is assigned a reference to the instance\nthat was generated when the exception was raised. This gives access to the\nattributes attached to the instance by the custom constructor. Although we could\nrely on the default state retention of built-in superclasses, it’s less relevant to our\napplication (and doesn’t support the keyword arguments used in the prior\nexample):",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1394,
      "chapter": null,
      "content": ">>> class FormatError(Exception): pass             # Inherited constructor\n>>> def parser(file):\n        raise FormatError(file, 62)                # No keywords allowed!\n>>> try:\n...     parser('code.py')\n... except FormatError as X:\n...     print(f'Error at: {X.args[0]} #{X.args[1]}')     # Generic state info\n...\nError at: code.py #62\nProviding Exception Methods\nBesides enabling application-specific state and display, classes also support extra\nbehavior for exception objects. That is, the exception class can also define\nunique methods to be called in handlers. The file parsely.py in Example 35-3, for\nexample, adds a method that uses exception custom state information to log\nerrors to a file automatically.\nExample 35-3. parsely.py\nfrom time import asctime\nclass FormatError(Exception):\n   logfile = 'parser-errors.txt'\n   def __init__(self, line, file):\n       self.line = line\n       self.file = file\n   def logerror(self):\n       with open(self.logfile, 'a') as log:\n           print(f'Error at: {self.file} #{self.line} [{asctime()}]', file=log)\ndef parser(file):\n   # Parse a file here...\n   raise FormatError(line=62, file=file)\nif __name__ == '__main__':\n   try:\n       parser('code.py')\n   except FormatError as exc:\n       exc.logerror()\nWhen run, this script appends its error message to a file in response to method\ncalls in the exception handler (use type instead of the Unix cat on Windows,",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1395,
      "chapter": null,
      "content": "and see Python manuals for time.asctime):\n$ python3 parsely.py\n$ python3 parsely.py\n$ cat parser-errors.txt\nError at: code.py #62 [Sat Jul 13 12:22:19 2024]\nError at: code.py #62 [Sat Jul 13 12:22:25 2024]\nIn such a class, methods (like logerror) may also be inherited from\nsuperclasses, and instance attributes (like line and file) provide a place to save\nstate information that provides extra context for use in later method calls.\nMoreover, exception classes are free to customize and extend inherited behavior:\nclass CustomFormatError(FormatError):\n    def logerror(self):\n        …something unique here…\n…\nraise CustomFormatError(…)\n…\ntry:\n    …\nexcept FormatError as exc:\n    exc.logError()               # Runs the raised class's version\nIn other words, because they are defined with classes, all the benefits of OOP\nthat we studied in Part VI are available for use with exceptions in Python.\nTwo final notes here: first, the raised instance object assigned to exc in this code\nis also available generically as the second item in the result tuple of the\nsys.exc_info() call used earlier—a tool that returns information about the\nexception being handled. This call can be used if you do not list an exception\nname in an except clause but still need access to the exception that occurred, or\nto any of its attached state information or methods. And second, although our\nclass’s logerror method appends a custom message to a logfile, it could also\ngenerate Python’s standard error message with stack trace using tools in the\ntraceback standard-library module, which uses traceback objects.\nTo learn more about sys.exc_info and tracebacks, though, we need to move\nahead to the next chapter.",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1396,
      "chapter": null,
      "content": "Exception Groups: Yet Another Star!\nBut wait—just when you thought it was safe to put exceptions in the win\ncolumn, the exceptions story has recently sprouted yet another wild plot twist,\nwhich is sufficiently limited and arcane to pass as an optional follow-up topic for\nmost Python learners (and was deferred until now for this reason). Lest it crop\nup in one of those silly job interviews that favor the inane over the practical,\nthough, here’s a quick peek.\nLet’s get right to the code. As we’ve seen, try statements normally run at most\none matching handler clause, plus an optional finally on exit:\n>>> try:\n...     raise IndexError()\n... except IndexError:\n...     print('Got IE')\n... except (SyntaxError, TypeError):\n...     print('Got SE')\n... \nGot IE\nPython 3.11, though, adds the ability to trigger multiple matching handlers in a\nsingle try statement by wrapping them in an exception group and catching them\nwith except* clauses:\n>>> try:\n...     raise ExceptionGroup('Many', [IndexError(), SyntaxError()])\n... except* IndexError:\n...     print('Got IE')\n... except* (SyntaxError, TypeError):\n...     print('Got SE')\n... \nGot IE\nGot SE\nIn a nutshell, each except* clause can process and consume one exception, or\none batch of them, in the group. Here, the first clause runs for IndexError and\nthe second for SyntaxError (a tuple means “any” as before). The group is\nsimply an exception object made by calling a built-in exception class provided\nfor this role, passing a message-string label used in displays, along with a\nsequence of exceptions to be matched by except* clauses in a try.",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1397,
      "chapter": null,
      "content": "Syntactically, an empty except* is not allowed, and a basic except cannot be\nmixed with except*—but an else and finally can. Moreover, except* cannot\nhost a break, continue, or return—but except can. Like the awkward\nexcept/else/finally mixing rules before it, these special cases probably\nqualify except* as a distinct statement form; adding it to the mix makes try an\noverloaded jumble of semi-related functionality.\nSemantically, each except* clause executes at most once, and consumes all\nmatching exceptions in the group. In addition, each exception in the group is\nhandled by at most one except* clause—the topmost clause that matches it;\noptional as variables are assigned exception groups—with attributes like\nexceptions that expose their contents; and any unmatched exceptions in the\ngroup are reraised after the try statement processes matches—with a top-level\nmessage that denotes those unmatched:\n>>> excs = ExceptionGroup('Many', [IndexError(), SyntaxError(), TypeError()])\n>>> try:\n...     raise excs\n... except* IndexError as E:\n...     print(f'Got IE: {E} => {E.exceptions}')\n... except* SyntaxError as E:\n...     print(f'Got SE: {E} => {E.exceptions}')\n... \nGot IE: Many (1 sub-exception) => (IndexError(),)\nGot SE: Many (1 sub-exception) => (SyntaxError(),)\n  + Exception Group Traceback (most recent call last):\n  |   File \"<stdin>\", line 2, in <module>\n  | ExceptionGroup: Many (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | TypeError\n    +------------------------------------\nWhen the group has multiple exceptions of the same type, a matching except*\nconsumes them all and can process them in normal iteration code. As also shown\nnext, spaces around the * are allowed and ignored—despite all the except*\nlabels in docs, and more reflective of the wildcard nature of these clauses:\n>>> try:\n...     raise ExceptionGroup('Dups', [IndexError(), TypeError(1), TypeError(2)])\n... except *IndexError:",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1398,
      "chapter": null,
      "content": "...     print('Got IE')\n... except *TypeError as E:\n...     print(f'Got TE: {E} => {E.exceptions}')\n... \nGot IE\nGot TE: Dups (2 sub-exceptions) => (TypeError(1), TypeError(2))\nAs usual, a group’s exception matches an except* that names its class or one of\nits superclasses. Because of this, the ordering of clauses is both subtle and\nimportant—the first match wins and removes exceptions from the group. In the\nfollowing, for example, the first clause gobbles IndexError via its\nLookupError superclass, along with the two TypeErrors in the group (but\nreversing the clauses’ order would handle IndexError separately):\n>>> try:\n...     raise ExceptionGroup('Dups', [IndexError(), TypeError(1), TypeError(2)])\n... except* (TypeError, LookupError) as E:\n...     print('Got1:', E)\n... except* IndexError as E:\n...     print('Got2:', E)\n... \nGot1: Dups (3 sub-exceptions)\nThe except* can also match basic individual exceptions, which are\nautomatically wrapped in a group to appease group-based code in the handler:\n>>> try:\n...     raise IndexError\n... except* IndexError as E:\n...    print(f'Got IE: {E} => {E.exceptions}')\n... \nGot IE:  (1 sub-exception) => (IndexError(),)\nAnd a basic except can catch a group as a collective and process it manually,\nbut an except* cannot catch a group because it would be ambiguous (a schism\nof the sort that’s usually a hallmark of an ad hoc extension):\n>>> try:\n...     raise ExceptionGroup('Lots', [IndexError(), SyntaxError()])\n... except Exception as E:\n...     print(f'Got group: {E} => {E.exceptions}')\n...     for exc in E.exceptions:",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1399,
      "chapter": null,
      "content": "...         print('With exc:', type(exc))\n... \nGot group: Lots (2 sub-exceptions) => (IndexError(), SyntaxError())\nWith exc: <class 'IndexError'>\nWith exc: <class 'SyntaxError'>\nFinally, for comparison, here’s a catchall in both models—though there’s no\nreason to use except* to catch a single exception (unless overly complicated\ncode is your thing):\n>>> try:\n...     raise ExceptionGroup('Dups', [IndexError()])\n... except* Exception as E:\n...     print(type(E.exceptions[0]))\n... \n<class 'IndexError'>\n>>> try:\n...     raise IndexError()\n... except Exception as E:\n...     print(type(E))\n... \n<class 'IndexError'>\nFor another exception-groups example, see the next chapter’s Example 36-2,\nwhich demos how runtime nesting consumes items in groups (short story: groups\npropagate until empty, then die like individual exceptions).\nDesign concerns aside, the “why” of except* is even more elusive than the\n“how.” While it’s conceivable that some programs may wish to collect a set of\nexceptions and send them to a try statement as a batch, it’s harder to understand\nwhy these wildly rare programs could not be expected to package with normal\nexception objects and process with normal iteration code—instead of\nconvoluting the Python language for everyone.\nBecause exception groups are an obscure tool with very narrow roles, we’ll defer\nto Python’s manuals for more info on this esoteric try extension that, like many\na Python mod, seems to blow up complexity radically to address a purported\nneed that somehow managed to go unnoticed for all of Python’s first three\ndecades+. How did we live?",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1400,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we explored both built-in exceptions and ways to code\nexceptions of our own. As we learned, exceptions are implemented as class\ninstance objects. Exception classes support the concept of exception hierarchies\nthat ease maintenance, allow data and behavior to be attached to exceptions as\ninstance attributes and methods, and allow exceptions to inherit tools from\nsuperclasses as usual in OOP.\nWe saw that in a try statement, catching a superclass catches that class as well\nas all subclasses below it in the class tree—superclasses become exception\ncategory names, and subclasses become more specific exception types within\nthose categories. We also saw that the built-in exception superclasses we must\ninherit from provide usable defaults for printing and state retention, which we\ncan override if desired.\nFinally, armed with our new knowledge of exception objects, we also peeked at\nexception groups and the except* clause, used to run multiple handlers in a try.\nWe questioned whether this extension’s convolution of try statements is\njustified by its perceived roles; it’s a lot to ask of most Python users, but this\nquestion is ultimately yours to answer.\nThe next chapter wraps up this part of the book by exploring some common use\ncases for exceptions and surveying tools commonly used by Python\nprogrammers. Before we get there, though, here’s this chapter’s quiz.\nTest Your Knowledge: Quiz\n1. What are the two main constraints on user-defined exceptions in\nPython?\n2. How are raised exceptions matched to except handler clauses?\n3. Name two ways that you can attach context information to exception\nobjects.\n4. Name two ways that you can specify the error-message text for",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1401,
      "chapter": null,
      "content": "exception objects.\n5. What do except* clauses do in a try statement?\nTest Your Knowledge: Answers\n1. Exceptions must be defined by classes (that is, a class instance object is\nraised and caught). In addition, exception classes must be derived from\nthe built-in class BaseException or one of its subclasses; most\nprograms inherit from its Exception subclass to support catchall\nhandlers for normal kinds of exceptions.\n2. Exceptions match by superclass relationships: naming a superclass in an\nexception handler will catch instances of that class, as well as instances\nof any of its subclasses lower in the class tree. Because of this, you can\nthink of superclasses as general exception categories and subclasses as\nmore specific types of exceptions within those categories.\n3. You can attach context information to exceptions with either custom or\ndefault constructors. A custom constructor can fill out attributes in a\nraised instance object that are specific to the program. For simpler\nneeds, built-in exception superclasses provide a default constructor that\nstores its arguments on the instance automatically as tuple attribute\nargs. Handlers can list a variable to be assigned to the raised instance,\nthen go through this name to access attached state information and call\nany methods defined in the class.\n4. The error-message text in exceptions can be specified with a custom\n__str__ operator-overloading method. For simpler needs, built-in\nexception superclasses automatically display anything you pass to the\nclass constructor. Operations like print and str automatically fetch the\ndisplay string of an exception object when it is printed either explicitly\nor as part of an error message.\n5. In a try, except* is used to run possibly multiple handlers for\nexceptions raised as part of a group. The except* also comes with\nheavy semantics, has special-case syntax and rules, does not combine",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1402,
      "chapter": null,
      "content": "with basic except, and is rarely useful in most Python programs.\nNevertheless, there it is.",
      "content_length": 91,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1403,
      "chapter": null,
      "content": "Chapter 36. Exception Odds and\nEnds\nThis chapter rounds out this part of the book with the usual collection of stray\ntopics, common-usage examples, and design concepts, followed by this part’s\ngotchas and exercises. Because this chapter also closes out the fundamentals\nportion of the book at large, it includes a brief overview of concepts and\ndevelopment tools to help as you make the transition from Python language\nbeginner to Python application developer.\nNesting Exception Handlers\nMost of our examples so far have used only a single try to catch exceptions, but\nwhat happens if one try is physically nested inside another? For that matter,\nwhat does it mean if a try calls a function that runs another try? Technically,\ntry statements can nest in terms of both syntax and the runtime control flow\nthrough your code. This was mentioned earlier in brief but merits clarification\nhere.\nBoth of these cases can be understood if you realize that Python stacks try\nstatements at runtime. When an exception is raised, Python returns to the most\nrecently entered try statement with a matching except clause. Because each\ntry statement leaves a marker on the top of a LIFO stack, Python can jump back\nto earlier trys by inspecting the stacked markers. This nesting of active handlers\nis what we mean when we talk about propagating exceptions up to “higher”\nhandlers—such handlers are simply try statements entered earlier in the\nprogram’s execution flow.\nFigure 36-1 illustrates what occurs when try statements with except clauses\nnest at runtime. The amount of code that goes into a try block can be\nsubstantial, and it may contain function calls that invoke other code watching for\nthe same exceptions. When an exception is eventually raised, Python jumps back",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1404,
      "chapter": null,
      "content": "to the most recently entered try statement that names that exception, runs that\nstatement’s except clause, and then resumes execution after that try.\nFigure 36-1. Nested try/except combinations\nOnce the exception is caught, its life is over—control does not jump back to all\nmatching trys that name the exception; only the first (i.e., most recent) one is\ngiven the opportunity to handle it. In Figure 36-1, for instance, the raise\nstatement in the function func2 sends control back to the handler in func1, and\nthen the program continues within func1.\nBy contrast, when try statements that contain only finally clauses are nested,\neach finally block is run in turn when an exception occurs—Python continues\npropagating the exception up to other trys, and eventually perhaps to the top-\nlevel default handler (the standard error-message printer). As Figure 36-2\nillustrates, the finally clauses do not kill the exception—they just specify code\nto be run on the way out of each try during the exception propagation process.\nIf there are many try/finally combos active when an exception occurs, they\nwill all be run unless an except clause catches the exception somewhere along\nthe way.\nFigure 36-2. Nested try/finally combinations\nIn other words, where the program goes when an exception is raised depends",
      "content_length": 1304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1405,
      "chapter": null,
      "content": "entirely upon where it has been—it’s a function of the runtime flow of control\nthrough the script, not just its syntax. The propagation of an exception\nessentially proceeds backward through time to try statements that have been\nentered but not yet exited. This propagation stops as soon as control is unwound\nto a matching except clause, but not as it passes through finally clauses on the\nway.\nThe prior chapter’s except* clauses don’t change this story—if they consume\nevery exception in the raised group, the aggregate exception ends in the try as\nusual. As we saw, unmatched excepts are reraised after the except* clauses\nhave their chance and are propagated on to other try statements or the top-level\nhandler, but this is not fundamentally different from exceptions unmatched by an\nexcept.\nExample: Control-Flow Nesting\nLet’s turn to examples to make this nesting concept more concrete. The module\nfile in Example 36-1 defines three functions: action1 wraps a call to action2 in\na try handler, action2 does likewise for a call to action3, and action3 is\ncoded to trigger a built-in TypeError exception (you can’t add numbers and\nsequences).\nExample 36-1. nested_exc_normal.py\ndef action3():\n   print(1 + [])              # Generate TypeError\ndef action2():\n   try:                       # Most recent matching try\n       action3()\n   except TypeError:          \n       print('Inner try')     # Match kills the exception\n       raise                  # Unless manually reraised\ndef action1():\n   try:\n       action2()\n   except TypeError:\n       print('Outer try')     # Run only if action2 reraises\nif __name__ == '__main__': action1()",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1406,
      "chapter": null,
      "content": "Notice, though, that when action3 triggers the exception, there will be two\nactive try statements—the older one in action1 and the newer one in action2.\nPython picks and runs just the most recent try with a matching except—which\nin this case is the try inside action2. In this demo, action2 also manually\nreraises the TypeError with raise to trigger the try in action1, but the\nexception would otherwise die in action2:\n$ python3 nested_exc_normal.py\nInner try\nOuter try\nThe same happens for an exception group, though the exception doesn’t die until\nthe entire group has been matched. In Example 36-2, for instance, action2 picks\noff IndexError, action1 consumes TypeError, and SyntaxError propagates\nto the top-level default handler (or an earlier matching try, if one had been run).\nExample 36-2. nested_exc_group.py\ndef action3():\n   raise ExceptionGroup('Nest*', [IndexError(1), TypeError(2), SyntaxError(3)])\ndef action2():\n   try:\n       action3()\n   except* IndexError:        # Consume matches, rest propagate\n       print('Got IE')\ndef action1():\n   try:\n       action2()\n   except* TypeError:         # Consume matches, rest propagate\n       print('Got TE')\nif __name__ == '__main__': action1()\nWhen run, each function’s try consumes exceptions it matches, and the top-\nlevel handler prints an error message for the last remaining unmatched item in\nthe group:\n$ python3 nested_exc_group.py \nGot IE\nGot TE\n  + Exception Group Traceback (most recent call last):",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1407,
      "chapter": null,
      "content": "…etc…\n  | ExceptionGroup: Nest* (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | SyntaxError: 3\n    +------------------------------------\nWhether groups or individual exceptions are raised, the place where an\nexception winds up jumping to depends on the control flow through the program\nat runtime. Because of this, to know where you will go, you need to know where\nyou’ve been. In other words, routing for exceptions nested at runtime is more a\nfunction of control flow than of statement syntax. That said, we can also nest\nexception handlers syntactically—an equivalent case we turn to next.\nExample: Syntactic Nesting\nAs discussed when we studied clause combinations of the try statement in\nChapter 34, it is also possible to nest try statements syntactically by their\nposition in your source code:\n>>> from nested_exc_normal import action3\n>>> try:\n...     try:\n...         action3()\n...     except TypeError:        # Most-recent matching try\n...         print('Inner try')\n...         raise\n... except TypeError:            # Here, only if nested handler reraises\n...     print('Outer try')\n... \nInner try\nOuter try\nReally, though, this code just sets up the same handler-nesting structure as, and\nbehaves identically to, the try statements in Example 36-1. In fact, syntactic\nnesting works just like the cases sketched in Figures 36-1 and 36-2. The only\ndifference is that the nested handlers are physically embedded in a try block,\nnot coded elsewhere in functions that are called from the try block. For\nexample, nested finally handlers all fire on an exception, whether they are\nnested syntactically or by means of the runtime flow through physically",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1408,
      "chapter": null,
      "content": "separated parts of your code:\n>>> try:\n...     try:\n...         action3()\n...     finally:\n...         print('Inner try')\n... finally:\n...     print('Outer try')\n...\nInner try\nOuter try\nTraceback (most recent call last):\n…etc…\nTypeError: unsupported operand type(s) for +: 'int' and 'list'\nSee Figure 36-2 for a graphic illustration of this code’s operation; the effect is\nthe same, but the function logic has been inlined as nested statements here. As a\nmore comprehensive example of syntactic nesting at work, consider the file\nlisted in Example 36-3.\nExample 36-3. except-finally.py\ndef raise1():  raise IndexError\ndef noraise(): return\ndef raise2():  raise SyntaxError\nfor func in (raise1, noraise, raise2):\n   print(f'<{func.__name__}>')\n   try:\n       try:\n           func()\n       except IndexError:\n           print('caught IndexError')\n   finally:\n       print('finally run')\n   print('...')\nThis code catches an exception if a matching one is raised and performs a\nfinally termination-time action regardless of whether an exception occurs. This\nmay take a few moments to digest, but the effect is the same as combining an\nexcept and a finally clause in a single try statement:\n$ python3 except-finally.py\n<raise1>\ncaught IndexError",
      "content_length": 1241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1409,
      "chapter": null,
      "content": "finally run\n...\n<noraise>\nfinally run\n...\n<raise2>\nfinally run\nTraceback (most recent call last):\n…etc…\nSyntaxError: None\nAs we saw in Chapter 34, except and finally clauses can be mixed in the same\ntry statement. While this, along with multiple except clauses, makes the\nsyntactic nesting shown in this section largely academic, the equivalent runtime\nnesting is common in larger Python programs. Moreover, syntactic nesting can\nmake the disjoint roles of except and finally explicit and might be useful for\nimplementing alternative exception-handling behaviors.\nException Idioms\nWe’ve seen the mechanics behind exceptions. Now, let’s survey the ways they\nare typically used. Some of these are reviews of roles we’ve explored in earlier\nchapters, collected here as part of a referable set.\nBreaking Out of Multiple Nested Loops: “go to”\nAs mentioned at the start of this part of the book, exceptions can often be used to\nserve the same roles as other languages’ “go-to” statements to implement more\narbitrary control transfers. Exceptions, however, provide a more structured\noption that localizes the jump to a specific block of nested code.\nIn this role, raise is like “go to,” and except clauses and exception names take\nthe place of program labels. You can only jump out of code wrapped in a try\nthis way, but that’s a crucial feature—truly arbitrary “go to” statements can make\ncode extraordinarily difficult to understand and maintain (“spaghetti code” in\ndeveloper lingo).\nFor example, Python’s break statement exits just the single closest enclosing\nloop, but we can always use exceptions to break out of more than one loop level",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1410,
      "chapter": null,
      "content": "if needed, as in Example 36-4.\nExample 36-4. breaker.py\nclass Exitloop(Exception): pass\ntry:\n   while True:\n       while True:\n           for i in range(10):\n                if i > 3: raise Exitloop          # break exits just one level\n                print('loop3: %s' % i)            # raise can exit many \n           print('loop2')\n       print('loop1')\nexcept Exitloop:\n   print('continuing')                            # Or just pass, to move on\nprint(f'{i=}')                                     # Loop variable not undone\nWhen run, the raise in the for breaks out of three nested loops immediately:\n$ python3 breaker.py\nloop3: 0\nloop3: 1\nloop3: 2\nloop3: 3\ncontinuing\ni=4\nIf you change the raise in this to break, you’ll get an infinite loop because\nyou’ll break only out of the most deeply nested for loop, and wind up in the\nsecond-level while loop nesting. The code would then print “loop2” and start\nthe for again. Make the mod to see for yourself—but get ready to type Ctrl+C to\nstop the code!\nAlso, notice that variable i is still what it was in for after the try statement\nexits. As previously noted, variable assignments made in a try are not undone in\ngeneral, though as we’ve seen, exception instance variables listed in except\nclause as headers are localized to that clause, and the local variables of any\nfunctions that are exited as a result of a raise are discarded. Technically, active\nfunctions’ local variables are popped off the call stack, and the objects they\nreference may be garbage-collected as a result, but this is an automatic step.",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1411,
      "chapter": null,
      "content": "Exceptions Aren’t Always Errors\nIn Python, all errors are exceptions, but not all exceptions are errors. For\ninstance, we saw in Chapter 9 that file object read methods return an empty\nstring at the end of a file. In contrast, the built-in input function—which we first\nmet in Chapter 3 and deployed in an interactive loop in Chapter 10—reads a line\nof text from the standard input stream, sys.stdin, on each call and raises the\nbuilt-in EOFError at end-of-file.\nUnlike file methods, this function does not return an empty string—an empty\nstring from input means an empty line. Despite its name, though, the EOFError\nexception is just a signal in this context, not an error. Because of this behavior,\nunless the end-of-file should terminate a script, input often appears wrapped in\na try handler and nested in a loop, as in the following code:\nwhile True:\n    try:\n        line = input()           # Read line from stdin\n    except EOFError:\n        break                    # Exit loop at end-of-file\n    else:\n        …process next line here…\nSeveral other built-in exceptions are similarly signals, not errors—for example,\ncalling sys.exit() and pressing Ctrl+C on your keyboard raise SystemExit\nand KeyboardInterrupt, respectively.\nPython also has a set of built-in exceptions that represent warnings rather than\nerrors; some of these are used to signal the use of deprecated (soon to be phased\nout) language features. See the standard-library manual’s description of built-in\nexceptions for more information, and consult the warnings module’s\ndocumentation for more on exceptions raised as warnings.\nFunctions Can Signal Conditions with raise\nUser-defined exceptions can also signal nonerror conditions. For instance, a\nsearch routine can be coded to raise an exception when a match is found instead\nof returning a status flag for the caller to interpret. In the following abstract code,\nthe try/except/else exception handler does the work of an if/else return-",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1412,
      "chapter": null,
      "content": "value tester:\nclass Found(Exception): pass\ndef searcher():\n    if …success…:\n        raise Found()            # Raise exceptions instead of returning flags\n    else:\n        return\ntry:\n    searcher()\nexcept Found:                    # Exception if item was found\n    …success…\nelse:                            # else returned: not found\n    …failure…\nMore generally, such a coding structure may also be useful for any function that\ncannot return a sentinel value to designate success or failure. In a widely\napplicable function, for instance, if all objects are potentially valid return values,\nit’s impossible for any return value to signal a failure condition. Exceptions\nprovide a way to signal results without a return value:\nclass Failure(Exception): pass\ndef searcher():\n    if …success…:\n        return founditem\n    else:\n        raise Failure()\ntry:\n    item = searcher()\nexcept Failure:\n    …not found…\nelse:\n    …use item here…\nBecause Python is dynamically typed and polymorphic to the core, exceptions,\nrather than sentinel return values, are the generally preferred way to signal such\nconditions.\nClosing Files and Server Connections",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1413,
      "chapter": null,
      "content": "We encountered examples in this category in Chapter 34. As a review, exception\nprocessing tools are also commonly used to ensure that system resources are\nfinalized, regardless of whether an error occurs during processing or not.\nFor example, some servers require connections to be closed in order to terminate\na session. Similarly, output files may require close calls to flush their buffers to\ndisk for waiting consumers; input files may consume file descriptors if not\nclosed; and CPython closes open files when garbage-collecting them, but this\nisn’t always predictable or reliable.\nAs we saw in Chapter 34, the most general and explicit way to guarantee\ntermination actions for a specific block of code is the try/finally combination:\nmyfile = open('somefile', 'w')\ntry:\n    …process myfile…\nfinally:\n    myfile.close()\nAs we also saw, some objects make this potentially easier by providing context\nmanagers that terminate or close resources for us automatically when run by the\nwith statement:\nwith open('somefile', 'w') as myfile:\n    …process myfile…\nIf you want to know which option is better, flip back to “The Termination-\nHandlers Shoot-Out”—and draw your own conclusions.\nDebugging with Outer try Statements\nYou can also make use of exception handlers to replace Python’s default top-\nlevel exception-handling behavior. By wrapping an entire program (or a call to\nit) in an outer try in your top-level code, you can catch any exception that may\noccur while your program runs, thereby subverting the default program\ntermination.\nIn the following, the empty except clause catches any uncaught exception raised\nwhile the program runs. To get hold of the actual exception that occurred in this",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1414,
      "chapter": null,
      "content": "mode, fetch the exc_info function call result from the built-in sys module; it\nreturns a tuple whose first two items contain the currently handled exception’s\nclass and the instance object raised (more on sys.exc_info in a moment):\ntry:\n    …run program…\nexcept:                         # All uncaught exceptions come here\n    import sys\n    print('uncaught!', sys.exc_info()[0], sys.exc_info()[1])\nThis structure is commonly used during development to keep programs active\neven after errors occur. It’s also used when testing other program code, as\ndescribed in the next section: coded within a loop, this structure allows you to\nrun additional tests without having to restart.\nRunning In-Process Tests\nSome of the coding patterns we’ve just seen can be combined in a test-driver\nscript that tests other code imported and run within the same process (i.e.,\nprogram run). The following partial and abstract code sketches the general\nmodel:\nimport sys\nlog = open('testlog', 'a')\nfrom testapi import moreTests, runNextTest, testName\ndef testdriver():\n    while moreTests():\n        try:\n            runNextTest()\n        except:\n            print('FAILED', testName(), sys.exc_info()[:2], file=log)\n        else:\n            print('PASSED', testName(), file=log)\ntestdriver()\nThe testdriver function here cycles through a series of test calls. Because an\nuncaught exception in any of them would normally kill this test driver, tests are\nwrapped in a try to continue the testing process if a test fails. The empty except\ncatches any uncaught exception generated by a test case and uses sys.exc_info\nto log the exception to a file. The else clause is run when no exception occurs—",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1415,
      "chapter": null,
      "content": "the test success case.\nSuch boilerplate code is typical of systems that test imported functions, modules,\nand classes. In practice, though, testing can be much more sophisticated. For\ninstance, to test external programs, you could instead check status codes or\noutputs generated by program-launching tools such as os.system and\nos.popen, which were used earlier in this book and are covered in Python’s\nstandard-library manual. Such tools do not generally raise exceptions for errors\nin the external programs—in fact, the test cases may run in parallel with the test\ndriver.\nAt the end of this chapter, we’ll also briefly explore more complete testing\nframeworks provided by Python, such as doctest and PyUnit, which provide\ntools for comparing expected outputs with actual results.\nMore on sys.exc_info\nThe sys.exc_info result used in the last two sections allows an exception\nhandler to generically gain access to the exception being handled. This is\nespecially useful when using the empty except clause to catch everything\nblindly because it allows you to determine what was raised:\ntry:\n    …\nexcept:\n    # sys.exc_info()[0:2] are the exception class and instance\nIf no exception is being handled, this call returns a tuple containing three None\nvalues. Otherwise, the values returned are (type, value, traceback), where:\ntype is the class of the exception being handled.\nvalue is the class instance that was raised.\ntraceback is a traceback object that represents the call stack at the\npoint where the exception originally occurred, and may be used by the\ntraceback module to generate error messages.\nAs we saw in Chapter 35, sys.exc_info can also sometimes be useful to",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1416,
      "chapter": null,
      "content": "determine the specific exception type when catching exception category\nsuperclasses. As we’ve also learned, though, because in this case you can also\nget the exception type by fetching the __class__ attribute or type result of the\ninstance obtained with the as clause, sys.exc_info is rarely useful outside the\nempty except:\ntry:\n    …\nexcept General as instance:\n    # instance.__class__ or type(instance) is the exception class\n    # but instance.method() does the right thing for this instance\nAs we’ve seen, using Exception for the General exception name here would\ncatch all nonexit exceptions; it’s similar to an empty except but less extreme\nand still gives access to the exception instance and its class. Even so, leveraging\npolymorphism by calling the instance’s methods is often a better approach than\ntesting exception types.\nThe sys.exception alternative—and diss\nAs yet another option, a new call added in Python 3.11, sys.exception, returns\njust the exception instance raised—the same object assigned to the variable\nlisted after as in except clauses, and equivalent to the second item in the\nsys.exc_info result (i.e., sys.exc_info()[1]). Hence, the following work the\nsame in a try:\nexcept …:\n    print('uncaught!', sys.exc_info()[0], sys.exc_info()[1])\nexcept …:\n    print('uncaught!', type(sys.exception()), sys.exception())\nMore generally, there are now three ways to obtain the same information about a\ncaught exception in try (two of which in the following are nested in a tuple to\nmatch exc_info and display with repr instead of str):\n>>> class E(Exception): pass\n... \n>>> try:",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1417,
      "chapter": null,
      "content": "...     raise E('info')\n... except E as X:\n...     print((type(X), X))\n...     print(sys.exc_info()[:2])\n...     print((type(sys.exception()), sys.exception()))\n... \n(<class '__main__.E'>, E('info'))\n(<class '__main__.E'>, E('info'))\n(<class '__main__.E'>, E('info'))\nWhen using sys.exception, the exception class is available from the instance\nvia __class__ or type (as shown), and the traceback is normally present in the\nexception instance’s __traceback__ object. Though largely trivial, the new call\navoids an index or slice when only the instance is needed.\nLess pleasantly, with the addition of sys.exception, the sys.exc_info call has\nalso been branded “old-style” in Python’s docs, but doing this for the sake of a\nredundant call added just over a year ago seems both opinionated and divisive—\nif not software ageism. Naturally, you’re welcome and encouraged to use either\ncall in your code, but this book generally recommends tools that are traditional,\ncommon, and inclusive.\nDisplaying Errors and Tracebacks\nFinally, the exception traceback object available in the prior section’s\nsys.exc_info data is also used by the standard library’s traceback module to\ngenerate the standard error message and stack display manually. This module has\na handful of interfaces that support wide customization, which we don’t have\nspace to cover usefully here, but the basics are simple. Consider the\n(judgmentally named) file in Example 36-5, badly.py.\nExample 36-5. badly.py\nimport traceback\ndef inverse(x):\n   return 1 / x\ntry:\n   inverse(0)\nexcept Exception:\n   traceback.print_exc(file=open('badly.txt', 'w'))\nprint('Bye')",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1418,
      "chapter": null,
      "content": "This code uses the print_exc convenience function in the traceback module,\nwhich internally uses sys.exc_info data (technically, it was changed to use the\nsys.exception component as part of the prior section’s subjective purge).\nWhen run, the script prints the standard error message to a file—useful in\nprograms that need to catch errors but still record them in full (again, type is the\nWindows equivalent of Unix cat here):\n$ python3 badly.py\nBye\n$ cat badly.txt\nTraceback (most recent call last):\n  File \"/…/LP6E/Chapter36/badly.py\", line 7, in <module>\n    inverse(0)\n  File \"/…/LP6E/Chapter36/badly.py\", line 4, in inverse\n    return 1 / x\n           ~~^~~\nZeroDivisionError: division by zero\nFor much more on traceback objects, the traceback module that uses them, and\nrelated topics, consult your favorite Python reference resources.\nException Design Tips and Gotchas\nThis chapter is lumping design tips and gotchas together because it turns out that\nthe most common exception gotchas stem from design issues. By and large,\nexceptions are easy to use in Python. The real art behind them is in deciding how\nspecific or general your except clauses should be and how much code to wrap\nup in try statements. Let’s address the latter of these choices first.\nWhat Should Be Wrapped\nIn principle, you could wrap every statement in your script in its own try, but\nthat would just be silly (the try statements would then need to be wrapped in\ntry statements!). What to wrap is really a design issue that goes beyond the\nlanguage itself, and it will become more apparent with use. But as a summary,\nhere are a few rules of thumb:",
      "content_length": 1627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1419,
      "chapter": null,
      "content": "Operations that commonly fail should generally be wrapped in try\nstatements. For example, operations that interface with system state (file\nopens, socket calls, and the like) are prime candidates for try.\nUnless they should fail—in a simple script, you may want failures to\nkill your program instead of being caught and ignored, especially if the\nfailure is a showstopper. Failures in Python normally generate useful\nerror messages instead of hard crashes, and this is the best outcome\nsome programs could hope for.\nCleanup actions that must be run regardless of exception outcomes\nshould generally be run with a try/finally combination unless a\ncontext manager is available as a with option.\nWrapping the call to a function in a single try statement often makes\nfor less code than wrapping operations in the function itself. That way,\nall exceptions in the function percolate up to the single try around the\ncall.\nThe types of programs you write will probably influence the amount of\nexception handling you code as well. Servers, test runners, and GUIs, for\ninstance, must generally catch and recover from exceptions. Simpler one-shot\nscripts, though, will often ignore exception handling completely because failure\nat any step requires shutdown.\nIn all cases, keep in mind that failures in Python normally generate useful error\nmessages instead of hard crashes. Even without try, this is often a better\noutcome than some programs could hope for.\nCatching Too Much: Avoid Empty except and Exception\nAs we’ve learned, Python lets us pick and choose which exceptions to catch, but\nit’s important not to be too inclusive. For example, we’ve seen that an empty\nexcept clause catches every exception. That’s easy to code and sometimes\ndesirable, but it may also wind up intercepting an error that’s expected by a try\nelsewhere:\ndef func():",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1420,
      "chapter": null,
      "content": "try:\n        …                  # IndexError is raised in here\n    except:\n        …                  # But everything comes here and dies!\ntry:\n    func()\nexcept IndexError:         # Exception should be processed here\n    …\nPerhaps worse, such code might also catch unrelated critical exceptions. Even\nthings like memory errors, program typos, iteration stops, keyboard interrupts,\nand system exits raise exceptions in Python. Unless you’re writing a debugger or\nsimilar tool, such exceptions should not usually be intercepted in your code.\nFor example, scripts normally exit when control falls off the end of the top-level\nfile, but Python also provides a built-in sys.exit(statuscode) call to allow\nearly terminations. This works by raising a built-in SystemExit exception to end\nthe program so that try/finally handlers run on the way out and tools can\nintercept the event. Because of this, a try with an empty except might\nunknowingly prevent an exit, as in Example 36-6.\nExample 36-6. exiter.py\nimport sys\ndef bye():\n   sys.exit(62)           # Crucial error: abort now!\ntry:\n   bye()\nexcept:\n   print('Got it')        # Oops--we ignored the exit\nprint('Continuing...')\nWhen run, the script happily keeps going after a call to shut it down:\n$ python3 exiter.py\nGot it\nContinuing...\nYou simply might not expect all the kinds of exceptions that could occur during\nan operation. Per the prior chapter, using the built-in Exception superclass can",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1421,
      "chapter": null,
      "content": "help because it is not a superclass of SystemExit:\ntry:\n    bye()\nexcept Exception:          # Won't catch exits, but _will_ catch many others\n    …\nIn some cases, though, this scheme is no better than an empty except clause—\nbecause Exception is a superclass above all built-in exceptions except system-\nexit events, it still has the potential to catch exceptions meant for elsewhere in\nthe program. Worse, using either the empty except or Exception will also catch\nprogramming errors, which should usually be allowed to pass. In fact, these two\ntechniques can effectively turn off Python’s error-reporting machinery, making it\ndifficult to notice mistakes in your code. Consider this code, for example:\nmydictionary = {…}\n…\ntry:\n    x = myditctionary[key]       # Oops: misspelled name\nexcept:\n    x = None                     # Assume we got KeyError – only - here\n…continue here with x…\nThe coder here assumes that the only sort of error that can happen when\nindexing a dictionary is a missing key error. But because the name\nmyditctionary is misspelled, Python raises a NameError instead for the\nundefined name reference, which the handler will silently catch and ignore.\nHence, the event handler will incorrectly fill in a None default for the dictionary\naccess, masking the program error.\nMoreover, catching Exception here will not help—it would have the exact same\neffect as an empty except, silently filling in a default and hiding an error you\nwill probably want to know about. If this happens in code that is far removed\nfrom the place where the fetched values are used, it might make for an\ninteresting debugging task!\nAs a rule of thumb, be as specific in your handlers as you can be—empty except\nclauses and Exception catchers are handy but potentially error-prone. In the last\nexample, for instance, you would be better off listing KeyError in the except to",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1422,
      "chapter": null,
      "content": "avoid intercepting unrelated events. In simpler scripts, the potential for problems\nmight not be significant enough to outweigh the convenience of a catchall, but in\ngeneral, general handlers are generally trouble.\nNOTE\nMore closers: Python’s atexit standard-library module allows programs to handle program\nshutdowns without recovery from them, and its sys.excepthook can be used to customize\nwhat the top-level exception handler does. A related call, os._exit, ends a program like\nsys.exit, but via immediate termination—it skips cleanup actions, including any registered\nwith atexit, and cannot be intercepted with try/except or try/finally. It is usually used\nonly in spawned child processes, a topic beyond this book’s scope. See Python’s library\nmanual for more details.\nCatching Too Little: Use Class-Based Categories\nBeing too specific in exception handlers can be just as perilous as being too\ngeneral. When you list specific exceptions in a try, you catch only what you\nactually list. This isn’t necessarily a bad thing, but if a system evolves to raise\nother exceptions in the future, you may need to go back and add them to\nexception lists elsewhere in your code.\nWe saw this phenomenon at work in the prior chapter. By way of review,\nbecause the following handler is written to treat only MyExcept1 and MyExcept2\nas cases of interest, a future MyExcept3 won’t apply:\ntry:\n    …\nexcept (MyExcept1, MyExcept2):    # Breaks if you add a MyExcept3 later\n    …\nCareful use of class-based exceptions can make this code maintenance trap go\naway completely. By catching a general superclass, new exceptions don’t imply\nexcept-clause changes:\ntry:\n    …\nexcept CommonCategoryName:        # OK if you add a MyExcept3 subclass later\n    …",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1423,
      "chapter": null,
      "content": "In other words, a little design goes a long way. The moral of the story is to be\ncareful to be neither too general nor too specific in exception handlers and to\npick the granularity of your try statement wrappings wisely. Especially in larger\nsystems, exception policies should be a part of the overall design.\nCore Language Wrap-Up\nCongratulations! This concludes your voyage through the fundamentals of the\nPython programming language. If you’ve gotten this far, you’ve become a fully\noperational Python programmer. There’s more optional reading in the advanced\ntopics part ahead described in a moment. In terms of the essentials, though, the\nPython story—and this book’s main journey—is now complete.\nAlong the way, you’ve seen just about everything there is to see in the language\nitself and in enough depth to apply to most of the code you are likely to\nencounter in the Python “wild.” You’ve studied built-in types, statements, and\nexceptions, as well as tools used to build up the larger program units of\nfunctions, modules, and classes.\nYou’ve also explored important software design issues, the complete OOP\nparadigm, functional programming tools, program architecture concepts,\nalternative tool trade-offs, and more—compiling a skill set now qualified to be\nturned loose on the task of developing real applications.\nThe Python Toolset\nFrom this point forward, your future Python career will largely consist of\nbecoming proficient with the toolset available for application-level Python\nprogramming. You’ll find this to be an ongoing task. The standard library, for\nexample, contains hundreds of modules, and the public domain offers still more\ntools. It’s possible to spend decades seeking proficiency with all these tools—\nespecially as new ones are constantly appearing to address new technologies.\nSpeaking generally, Python provides a hierarchy of toolsets:\nBuilt-in tools\nBuilt-in types like strings, lists, and dictionaries make it easy to write simple",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1424,
      "chapter": null,
      "content": "programs fast.\nPython-coded extensions\nFor more demanding tasks, you can write your own functions, modules, and\nclasses in Python itself.\nOther-language extensions\nAlthough we don’t cover this topic in this book, Python can also be extended\nwith code written in an external language like C, C++, or Java.\nBecause Python layers its toolsets, you can decide how deeply your programs\nneed to delve into this hierarchy for any given task—you can use built-ins for\nsimple scripts, add Python-coded extensions for larger systems, and code other\nextensions for advanced work. We’ve only covered the first two of these\ncategories in this book, and that’s plenty to get you started doing substantial\nprogramming in Python.\nBeyond this, there are tools, resources, and precedents for using Python in nearly\nany computer domain you can imagine. For pointers on where to go next, see\nChapter 1’s overview of Python applications and users. You’ll likely find that\nwith a powerful open source language like Python, common tasks are often\nmuch easier, and even enjoyable, than you might expect.\nDevelopment Tools for Larger Projects\nMost of the examples in this book have been fairly small and self-contained.\nThey were written that way on purpose to help you master the basics. But now\nthat you know all about the core language, it’s time to start learning how to use\nPython’s built-in and third-party interfaces to do real work.\nIn practice, Python programs can become substantially larger than the examples\nyou’ve experimented with so far in this book. Even in Python, thousands of lines\nof code are not uncommon for nontrivial and useful programs once you add up\nall the individual modules in the system. Though Python’s basic program\nstructuring tools, such as modules and classes, help much to manage this",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1425,
      "chapter": null,
      "content": "complexity, other tools can sometimes offer additional support.\nFor developing larger systems, you’ll find such support available in both Python\nand the public domain. You’ve seen some of these in action, and others have\nbeen noted in passing. This category morphs constantly, so we can’t get too\ndetailed here, but to help you with your next steps, here is a quick tour and\nsummary of tools in this domain:\nDocumentation tools\nPyDoc’s help function and HTML interfaces were introduced in Chapter 15.\nPyDoc provides a documentation system for your modules and objects,\nintegrates with Python’s docstrings syntax, and is a standard part of the\nPython system. See Chapters 15 and 4 for more documentation source hints.\nError-checking tools\nBecause Python is such a dynamic language, some programming errors are\nnot reported until your program runs (even syntax errors are not caught until\na file is run or imported). This isn’t a big drawback—as with most languages,\nit just means that you have to test your Python code before shipping it. With\nPython, you essentially trade a compile phase for an initial testing phase.\nFurthermore, Python’s dynamic nature, automatic error checking and\nreporting messages, and exception model make it easier and quicker to find\nand fix errors than it is in some other languages. Unlike C, for example,\nPython does not crash completely on errors.\nStill, tools can help here too. As representative examples, the PyChecker,\nPylint, and Pyflakes third-party systems provide support for catching\ncommon errors before your script runs. They serve similar roles to the lint\nprogram in C development. Some Python developers run their code through\nsuch tools prior to testing or delivery to catch any lurking potential problems.\nIn fact, it’s not a bad idea to try this when you’re first starting out—some of\nthese tools’ warnings may help you learn to spot and avoid common Python\nmistakes.\nTesting tools",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1426,
      "chapter": null,
      "content": "In Chapter 25, we learned how to add self-test code to a Python file by using\nthe __name__ == '__main__' trick at the bottom of the file—a simple unit-\ntesting protocol. For more advanced testing purposes, Python comes with\ntwo testing tools. The first, PyUnit (called unittest in the standard-library\nmanual), provides an object-oriented class framework for specifying and\ncustomizing test cases and expected results. It mimics the JUnit framework\nfor Java and is a sophisticated class-based unit testing system.\nThe doctest standard-library module provides a second and simpler\napproach to regression testing based upon Python’s docstrings feature.\nRoughly, to use doctest, you cut and paste a log of an interactive testing\nsession into the docstrings of your source files. doctest then extracts your\ndocstrings, parses out the test cases and results, and reruns the tests to verify\nthe expected results. See the library manual for more on both testing tools.\nIDEs\nWe discussed IDEs for Python briefly in Chapter 3. IDEs such as PyCharm\nand Python’s own IDLE provide a graphical environment for editing,\nrunning, debugging, and browsing your Python programs. Some advanced\nIDEs listed in Chapter 3 may support additional development tasks,\nincluding source control integration, code refactoring, project management\ntools, and more. Though aimed at roles other than general software\ndevelopment, Jupyter notebooks may qualify as a kind of IDE too. See\nChapter 3, the text editors page at python.org, and your favorite web search\nengine for more on available IDEs for Python.\nProfilers\nAs we’ve seen, because Python is both dynamic and fluid, intuitions about\nperformance gleaned from experience with other languages usually don’t\napply to Python code. To truly isolate performance bottlenecks in your code\nand compare coding alternatives’ speed, you need to add timing logic with\nclock tools in the time or timeit modules or run your code under the",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1427,
      "chapter": null,
      "content": "profile module. We saw examples of the timing modules at work when\ncomparing the speed of iteration tools and Pythons in Chapter 21.\nProfiling is often your first optimization step—code for clarity, then profile\nto isolate bottlenecks, and then time alternative codings of the slow parts of\nyour program. For the second of these steps, profile and its optimized\ncProfile relative are standard-library modules that implement source code\nprofiling for Python. After running code you provide, they print a report that\ngives performance statistics too detailed for us to cover here. See Python’s\nlibrary manual for more on profilers, as well as the pstats module used to\nanalyze results.\nDebuggers\nWe discussed debugging options both in this part and in Chapter 3 (see the\nlatter’s sidebar “Debugging Python Code”). As a review, most development\nIDEs for Python support GUI-based debugging, and the Python standard\nlibrary also includes a source code debugger module called pdb. This module\nprovides a command-line interface and works much like common C\nlanguage debuggers (e.g., dbx, gdb), and is detailed in Python’s library\nmanual.\nBecause IDEs such as IDLE also include point-and-click debugging\ninterfaces, pdb is more useful when a GUI isn’t available or when more\ncontrol is desired. See Chapter 3 for tips on using IDLE’s debugging GUI\ninterfaces. As also noted in Chapter 3, though, neither pdb nor IDEs seem to\nbe used much in practice: most programmers simply either read Python’s\nerror messages or insert print statements to add beacon displays and rerun\n—not the most high-tech of solutions, perhaps, but the practical tends to win\nthe day in the Python world.\nShipping options\nIn “Standalone Executables”, we surveyed common tools for packaging\nPython programs. A variety of systems package program bytecode and the\nPython Virtual Machine into standalone executables, which don’t require that\nPython be installed on the host machine. In addition, we’ve learned that",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1428,
      "chapter": null,
      "content": "Python programs may be shipped in their source (.py) or bytecode (.pyc)\nforms, and .zip files may act like package folders. When your code is ready\nto go live as an open source tool, also see the web for resources on Python’s\npip installer system.\nOptimization options\nWhen speed counts, there are numerous ways to optimize your Python\nprograms, as enumerated in Chapter 2. For instance, the PyPy system\ndemoed in Chapter 21 provides an automatic speed boost today, and others\nlike Shed Skin and Cython offer different routes to faster programs. Although\nPython’s -O command-line flag noted in Chapter 34 (and to be deployed in\nChapter 39) optimizes bytecode, it yields a very modest performance boost,\nand is not commonly used except to remove debugging code and asserts.\nThough a last resort, you can also move parts of your program to a compiled\nlanguage such as C to boost performance; see Python’s manuals for more on\nC extensions. In addition, Python’s speed tends to improve over time, so\nupgrading to later releases may boost speed too—once you verify that they\nare faster for your code, that is (though long since fixed, Python 3.X’s early\nreleases were radically slower than 2.X in some roles).\nInstallation management\nIf you need to install and segregate multiple sets of Python extensions on\nyour machine, you may also wish to use virtual environments—noted briefly\nin Chapter 22 and implemented by Python’s standard-library module venv.\nThis module allows you to create multiple virtual environments, each of\nwhich has its own independent set of Python packages, is contained in a\ndirectory, and is activated and deactivated by console commands. When a\nvirtual environment is activated, tools such as pip install Python packages\ninto that environment, and search paths are tailored for that environment’s\ninstalls. See Python’s library manual for more info.",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1429,
      "chapter": null,
      "content": "Other hints for larger projects\nWe’ve also studied a variety of core-language topics in this text that may\ngrow more useful once you start coding larger projects. These include\nmodule packages (Chapter 24), exceptions classes (Chapter 34),\npseudoprivate class attributes (Chapter 31), documentation strings\n(Chapter 15), module data hiding (Chapter 25), and all the design and usage\nguidelines we’ve explored along the way for objects, statements, functions,\nmodules, classes, and exceptions. If you’ve read this far, you’re already well-\nequipped to level up.\nTo learn about these and many other larger-scale Python development tools,\nbrowse the PyPI website, python.org, and the web at large. Applying Python\nmay be a larger topic than learning Python, but it is also one we’ll have to\ndelegate to follow-up resources here.",
      "content_length": 825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1430,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter wrapped up the exceptions part of this book with a survey of design\nconcepts, a look at common exception use cases, and a brief summary of\ncommonly used development tools.\nThis chapter also wrapped up the core material of this book. At this point,\nyou’ve been exposed to the full subset of Python that most programmers use—\nand probably much more. In fact, by virtue of reaching these words, you should\nfeel free to consider yourself an official Python programmer. Be sure to pick up\na t-shirt or laptop sticker the next time you’re online (and don’t forget to add\nPython to your résumé the next time you dig it out).\nThe next and final part of this book is a collection of chapters dealing with topics\nthat are advanced but still in the core-language category. These chapters are all\noptional reading, or at least deferrable reading, because not every Python\nprogrammer must delve into their subjects, and others can postpone these\nchapters’ topics until they are needed. Indeed, many of you can stop here and\nbegin exploring Python’s roles in your application domains. Frankly, application\nlibraries tend to be more important in practice than advanced—and, to some,\nesoteric—language features.\nOn the other hand, if you do need to care about things like Unicode or binary\ndata (and you probably do!); have to deal with API-building tools such as\ndescriptors, decorators, and metaclasses; or just want to dig a bit further in\ngeneral, the next part of the book will help you get started. The larger examples\nin the final part will also give you a chance to see the concepts you’ve already\nlearned being applied in more realistic ways.\nAs this is the end of the core material of this book, though, you get a break on\nthe chapter quiz—just one question this time. As always, be sure to work\nthrough this part’s closing exercises to cement what you’ve learned in the past\nfew chapters; because the next part is optional reading, this is the final end-of-\npart exercises session. If you want to see some examples of how what you’ve\nlearned comes together in real scripts drawn from common applications, be sure\nto check out the “solution” to this part’s exercise 4 in Appendix B.",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1431,
      "chapter": null,
      "content": "And if this is where you’ll be disembarking from this book’s voyage, be sure to\nalso see “Encore: Print Your Own Completion Certificate!” at the end of\nChapter 41, the very last chapter in this book (for the sake of readers continuing\non to the Advanced Topics part, this chapter won’t spill the beans here).\nTest Your Knowledge: Quiz\n1. What’s up with the mouse on the cover of this book?\nTest Your Knowledge: Answers\n1. OK, this was never mentioned and is hardly a fair question, but for the\nrecord: the mouse—really, a wood rat, Neotoma muridae—was chosen\nfor this book’s first edition by its publishing company in the 1990s,\nbased on the fact that this animal is common food for a python. The\nidea was that the wood rat must learn about the python to avoid being\neaten by it. Clever, to be sure, but this also came with a subtler tie-in\nabout Neotoma being pack rats attracted to shiny objects that\ncompulsively collect whatever they come across, which seems an apt\nmetaphor for Python’s history of language-feature accumulation.\nSo enjoy the shiny objects, but don’t get eaten by the constricting\nreptiles along the way.",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1432,
      "chapter": null,
      "content": "Test Your Knowledge: Part VII Exercises\nAs we’ve reached the end of this part of the book, it’s time for a few exception\nexercises to give you a chance to practice the basics. Exceptions really are\nsimple tools; if you’re able to work through these exercises, you’ve probably\nmastered the exceptions domain. See “Part VII, Exceptions” in Appendix B for\nthe solutions.\n1. try/except: Write a function called oops that explicitly raises an\nIndexError exception when called. Then, write another function that\ncalls oops inside a try/except statement to catch the error. What\nhappens if you change oops to raise a KeyError instead of an\nIndexError? Where do the names KeyError and IndexError come\nfrom? (Hint: recall that all unqualified names generally come from one\nof four scopes.)\n2. Exception objects and lists: Change the oops function you just wrote to\nraise an exception you define yourself, called MyError. Identify your\nexception with a class of your own. Then, extend the try statement in\nthe catcher function to catch this exception and its instance in addition\nto IndexError, and print the instance you catch.\n3. Error handling: Write a function called safe(func, *pargs,\n**kargs) that runs any function with any number of positional and/or\nkeyword arguments by using the * arbitrary arguments header and call\nsyntax, catches any exception raised while the function runs, and prints\nthe exception using the exc_info call in the sys module. Then use your\nsafe function to run your oops function from exercise 1 or 2. Put safe\nin a module file called exctools.py, and pass it the oops function\ninteractively. What kind of error messages do you get? Finally, expand\nsafe to also print a Python stack trace when an error occurs by calling\nthe built-in print_exc function in the standard-library traceback\nmodule; see earlier in this chapter, and consult the Python library\nreference manual for usage details. We could probably code safe as a",
      "content_length": 1946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1433,
      "chapter": null,
      "content": "function decorator per the Chapter 19 and 32 introductions, but we’ll\nhave to move on to the next part of the book to learn fully how (see the\nsolutions for a preview).\n4. Self-study examples: At the end of Appendix B in “Part VII,\nExceptions”, this book lists a handful of example scripts developed as\ngroup exercises in live Python classes for you to study on your own in\nconjunction with Python’s standard manual set. These are not described,\nand they use tools in the Python standard library that you’ll have to\nresearch yourself. Still, for many readers, it helps to see how the\nconcepts we’ve discussed in this book come together in real programs.\nIf these pique your interest for more, you can find a wealth of larger and\nmore realistic application-level Python program examples in follow-up\nbooks and on the web: pick your domain, and start exploring!",
      "content_length": 859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1434,
      "chapter": null,
      "content": "Part VIII. Advanced Topics",
      "content_length": 26,
      "extraction_method": "OCR"
    },
    {
      "page_number": 1435,
      "chapter": null,
      "content": "Chapter 37. Unicode and Byte\nStrings\nIn Chapter 7, the Python string story was watered down on purpose to help you\nget started with the fundamentals. Now that you’ve learned the basics, this\nchapter moves on to extend them to include the full Unicode-text and binary-\ndata string tales in Python.\nThis extension was more optional in earlier editions of this book because\nUnicode was an afterthought in Python 2.X. Python 3.X elevates it to required\nreading because its normal strings simply are Unicode. Still, how much you\nneed to care about this topic depends in large part upon which of the following\ncategories you fall into:\nIf you deal with non-ASCII Unicode text—for instance, in the context\nof internet content, internationalized applications, XML parsers, and\nsome GUIs—you will find direct and seamless support for text\nencodings in both Python’s all-Unicode str object, as well as its\nUnicode-aware text files.\nIf you deal with binary data—for example, in the form of image or\naudio files, network transfers, or packed data shared with lower-level\ntools—you will need to understand Python’s bytes object and its sharp\ndistinction between text and binary data and files.\nIf you fall into neither of the prior two categories, you may be able to\ndefer this topic and use strings as you did in Chapter 7: with the general\nstr object, text files, and all the familiar string operations. Your strings\nwill be encoded and decoded using your platform’s default Unicode\nencoding, but you won’t notice—until, as you’ll see, you encounter\ncontent or platforms that use a different default!\nTo be sure, if text is always ASCII in your corner of the software world, you\nmight be able to get by with simple string objects and text files and can avoid",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1436,
      "chapter": null,
      "content": "much of the story that follows. As you’ll learn in a moment, ASCII is a simple\nkind of Unicode and a subset of other common encodings, so string operations\nand files “just work” if your programs process ASCII text only and will never\ndeviate from this limitation.\nEven if this chapter’s topics seems remote to you today, though, a basic\nunderstanding of Python’s string model can both demystify some of the\nunderlying details now and prepare you for Unicode or binary-data issues that\nmay impact you in the future. Given the prominence of the web in most software\ncareers today, that impact may be more a matter of when than if.\nUnicode Foundations\nBefore jumping into code, let’s begin with a general overview of the Unicode\nmodel and Python’s support for it. To fully understand both, we have to start\nwith a brief look at how characters are actually represented in computers.\nCharacter Representations\nMost programmers think of strings as a series of characters (really, their integer\ncodes) used to represent textual data. That’s still true in the brave new world of\nUnicode, but the way characters are stored in a computer’s memory and files can\nvary, depending on both what sort of characters are recorded and how\nprogrammers choose to record them.\nFor many programmers in the US, ASCII formed their original notion of text\nstrings. ASCII is a standard that defines character codes 0…127 (which always\nmeans an inclusive range in this chapter) and thus allows each character to be\nstored in one 8-bit byte (using 7 bits). For example, the ASCII standard maps\ncharacter a to the integer value 97 (0x61 in hex), which can be stored in a single\nbyte both in computer memory and on files.\nTo witness this for yourself, Python’s built-in ord function shows the integer\ncode of a given character; chr reveals the character of a given integer code; and\nhex gives the code’s byte value as two hex digits, each of which fits a 4-bit\n“nibble.” The first of these, ord, is the value of a character’s representation code\n—and byte—in ASCII:",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1437,
      "chapter": null,
      "content": "$ python3          # Or py -3 on Windows\n>>> ord('a')       # Character => code\n97\n>>> chr(97)        # Code => character\n'a'\n>>> hex(97)        # Byte value: fits 8 bits\n'0x61'\n>>> 0b0111_1111    # Limit of ASCII's 7-bit range\n127\nASCII makes text processing simple, because characters directly correlate to\nbytes. Sometimes, though, this isn’t enough. Accented characters and special\nsymbols, for example, do not fit into the range of character codes defined by\nASCII. To allow for some such extra characters, other standards allow all\npossible values in an 8-bit byte, 0…255, to be used as codes, and assign values\n128…255 to additional characters.\nOne such standard is known as Latin-1 and is widely used in Western Europe. In\nLatin-1, character codes above 127 are assigned to accented and otherwise\nspecial characters. For instance, the character that Latin-1 assigns to code 196\n(a.k.a. byte value 0xc4) is a specially marked and non-ASCII character, Ä. In\nPython:\n>>> chr(196)         # Too big for ASCII\n'Ä'\n>>> ord('Ä')         # Okay for Latin-1\n196\n>>> hex(ord('Ä'))    # Byte value in Latin-1\n'0xc4'\n>>> bin(ord('Ä'))    # Latin-1 uses all 8 bits \n'0b11000100'\nStill, some alphabets define so many characters that it is impossible to represent\nthem as one byte-sized code per character. The integer codes of the symbols and\ncharacters in the following, for example, require more space than a byte—as do\nthose of all the emojis that may not work in some tools but manage to crop up in\nyour emails and texts anyhow:\n>>> ord('\n')\n9758\n>>> hex(ord('\n'))                     # Too big for one byte\n'0x261e'",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1438,
      "chapter": null,
      "content": ">>> [hex(ord(c)) for c in '\nЛ\n']     # Ditto: Unicode required\n['0x771f', '0x41b', '0x21e8']\n>>> [hex(ord(c)) for c in '\n']    # Emojis: > two bytes (16 bits)\n['0x1f642', '0x1f64a', '0x1f44d']\nUnicode provides the generality we need to deal with text containing non-ASCII\ncharacters and symbols like these. In fact, it defines and assigns enough\ncharacter codes to represent almost every natural language in use, plus a large\nset of symbols and emojis. In Unicode speak, these codes that stand for\ncharacters take the form of numbers (integers), and are usually called code\npoints. The code points that Unicode assigns to characters a, Ä, and \n, for\ninstance, are 97, 196, and 128578 (0x61, 0xc4, and 0x1f642 in hex),\nrespectively:\n>>> [f'{c} is {ord(c)} and {hex(ord(c))}' for c in 'aÄ\n']\n['a is 97 and 0x61', 'Ä is 196 and 0xc4', '\n is 128578 and 0x1f642']\nUnicode is sometimes referred to as “wide-character” strings because its range\nof characters is so broad that multiple bytes may be needed to represent\nindividual character codes. Such text is readily stored in computer memory\nbecause each character code can simply span as many bytes as its code-point\nnumber requires (the exact way this is done can vary by programming language\nand isn’t consequential to your Python code).\nOnce text leaves your computer, though, its storage is more constrained: bytes\nare a bad thing to waste on your drives and networks, and text used across\nplatforms must follow the same formatting rules. To allow for this, Unicode also\ndefines standard ways to map character codes to and from bytes for storage and\ntransmission that are both platform- and language-neutral—the encodings we’ll\nexplore in the next section.\nThe takeaway here is that Unicode’s combination of all-encompassing character\ncodes and their predefined encodings make it a portable and flexible model, and\nthe standard way that programs deal with non-English and other text that may\nhave more characters than 8-bit bytes can handle. As an added bonus, earlier\nschemes like ASCII also fall under the Unicode umbrella unchanged, but we\nhave to move on to the next section to see how.",
      "content_length": 2139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1439,
      "chapter": null,
      "content": "Character Encodings\nOne of the keys to understanding how Unicode works lies in the way its integer\ncharacter codes (a.k.a. code points) that represent characters are mapped to their\nencoded forms for efficient storage or transfer. Code points in memory are just\nintegers of arbitrary size, but storage and transfer, by nature, impose constraints\non time, space, and interoperability that warrant extra formatting steps.\nIn the Unicode world, we say that characters are translated to and from raw bytes\nusing an encoding—the rules for translating a Unicode-text string into a\nsequence of bytes and extracting the same string from its sequence of bytes.\nMore procedurally, this translation back and forth between bytes and strings is\ndefined by two terms (the first of which doubles as a noun and verb,\nconfusingly!):\nEncoding is the process of translating a string of characters into its raw-\nbytes form, per any desired encoding that’s broad enough to store the\nstring’s characters.\nDecoding is the process of translating a string of raw bytes into its\ncharacter-string form, per the encoding originally used to create the\nbytes string.\nAs we’ve seen, Unicode defines both character codes and a set of standard\nencodings. For some of the encodings it defines, the translation process is trivial\n—ASCII and Latin-1, for instance, map each character to a single byte, so little\nor no work is required to encode and decode if characters are the same bytes in\nmemory too.\nYou can view this for yourself with the encode method available on all Python\ntext strings, which simply returns the bytes used to encode the string. The\nfollowing means that the ASCII character a occupies just one byte when\nencoded per the ASCII encoding:\n>>> len('a'.encode('ASCII'))      # ASCII 'a' encodes in 1 byte per ASCII\n1\nFor other encodings, the mapping can be more complex and yield multiple bytes\nper character. The widely used UTF-8 encoding, for example, allows more",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1440,
      "chapter": null,
      "content": "characters to be represented by employing a variable-number-of-bytes scheme\nthat’s both general and economical. In fact, because UTF-8 can handle any\nUnicode code point, it’s become a de facto standard of sorts for text.\nIn UTF-8, character codes less than 128 are represented as a single byte; codes\nbetween 128 and 0x7ff (2047) are turned into two bytes, where each byte has a\nvalue between 128 and 255; and codes above 0x7ff are turned into three- or\nfour-byte sequences having values between 128 and 255. This keeps simple\nASCII strings compact, sidesteps byte ordering issues, and avoids null (zero)\nbytes that can cause problems for C libraries and networking. In Python:\n>>> len('a'.encode('UTF-8'))      # ASCII: encodes in 1 byte\n1\n>>> len('Ä'.encode('UTF-8'))      # Non-ASCII: encodes in 2 bytes\n2\n>>> len('\n'.encode('UTF-8'))     # Emoji: encodes in 4 bytes\n4\nDespite such details, it’s important to note that ASCII is a subset of both Latin-1\nand UTF-8. This is true because these encodings encode ASCII characters to\nbytes the same way as ASCII. That, in turn, makes these encodings backward\ncompatible with existing ASCII data: every character string encoded per ASCII\nis also valid according to the Latin-1 and UTF-8 encodings, and every ASCII file\nis a valid Latin-1 and UTF-8 file.\nTechnically, the ASCII encoding is a 7-bit subset of the other two: it’s binary\ncompatible with all character codes less than 128. Latin-1 and UTF-8 simply\nallow for additional characters: Latin-1 for characters mapped to values 128…\n255 within a byte, and UTF-8 for characters that may be represented with\nmultiple bytes. The converse is not true, however: UTF-8 and Latin-1 text is not\ncompatible with the ASCII encoding unless its text’s code-point values are all\nless than 128; otherwise, encoding or decoding per ASCII fails.\nIn Python again, the mapping is easy to observe. Per the following, an ASCII\ncharacter encodes to the same single byte in ASCII, UTF-8, and Latin-1\nencodings, but non-ASCII characters do not, and require more general encodings\nthan ASCII to encode at all (the b'…' here is the Python bytes object, which\nwill soon play a leading role in this chapter):",
      "content_length": 2182,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1441,
      "chapter": null,
      "content": ">>> 'a'.encode('ASCII')          # ASCII encodes to the same byte \nb'a'\n>>> 'a'.encode('UTF-8')\nb'a'\n>>> 'a'.encode('Latin-1')\nb'a'\n>>> 'Ä'.encode('UTF-8')          # But non-ASCII requires more bytes\nb'\\xc3\\x84'\n>>> '\n'.encode('UTF-8')         # And more inclusive encodings\nb'\\xf0\\x9f\\x99\\x82'\n>>> '\n'.encode('ASCII')\nUnicodeEncodeError: 'ascii' codec can't encode character '\\U0001f642'…\n>>> '\n'.encode('Latin-1')\nUnicodeEncodeError: 'latin-1' codec can't encode character '\\U0001f642'…\nOther encodings support richer character sets in other ways. For instance, UTF-\n16 and UTF-32 use a fixed and larger 2 and 4 bytes per character, respectively,\nthe former with a special surrogate-pair protocol for codes too large for 2 bytes.\nBoth of these, along with UTF-8, may also allow or require a BOM (Byte Order\nMarker) preamble at the start of encoded text, which can designate byte order\nand encoding type, may be present in encoded text stored in files or memory, and\nis automatically handled for text-mode files.\nWe’ll skip further details here for space (watch for the BOM to drop at the end\nof this chapter, along with the thorny topic of Unicode normalization), but keep\nin mind that all of these—ASCII, Latin-1, UTF-8, and others—are simply\nalternative Unicode encodings that yield the same Unicode code-point text when\ndecoded. The net effect ensures that text is portable across all the tools that use it\nin exchange for minor translation costs:\nWhen decoded, character code points may or may not occupy multiple\nbytes in memory, depending on programming-language implementation.\nSome recent Pythons, for example, use a variable-length scheme to\nstore decoded text with 1, 2, or 4 bytes per character, depending on\nstring content. Earlier Pythons instead store each character in a fixed 2\nor 4 bytes, depending on compilation settings.\nWhen encoded, the format of character code points is wholly\ndetermined by the standard Unicode encoding applied. This format is",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1442,
      "chapter": null,
      "content": "the same regardless of which programming language creates or\nprocesses the text, making it ideal for storage and transfer—especially\nin the diverse realm of the internet. This format is often less ideal for\nprograms to use, though, which is why it’s normally decoded when\nloaded.\nTo Python programmers, an encoding is specified as a string containing the\nencoding’s name. Python comes with roughly 100 different encodings out of the\nbox; see the codecs module in the Python Library Reference for a list. Importing\nmodule encodings and asking for help(encodings) shows you many as well.\nSome encodings are implemented in Python, and some in C, and many have\nmultiple names; for example, latin-1, iso_8859_1, and 8859 are all synonyms\nfor the same encoding, Latin-1. We’ll revisit encodings later in this chapter\nwhen we study Unicode coding techniques.\nFor another take on the Unicode backstory, see the Python standard manual at\npython.org. It includes a Unicode HOWTO section, which provides additional\nminutiae that we will skip here to focus on the fundamentals.\nIntroducing Python String Tools\nAt a more concrete level, the Python language provides multiple string data\ntypes to represent content in your script: both textual data—integer code-point\nvalues of decoded Unicode characters in memory—as well as binary data—raw\nbyte values, including text that is in encoded form. All told, Python comes with\nthree string object types:\nstr—for representing Unicode text (decoded code points)\nbytes—for representing binary data (including encoded text)\nbytearray—a mutable flavor of the bytes type\nAll three types support similar operation sets but have very different roles and\ncannot generally be mixed because of this. Moreover, files and other content\ntools reflect the text/binary dichotomy, too, and use specific string types in\ndifferent nodes. The next sections introduce the salient points of this model.",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1443,
      "chapter": null,
      "content": "The str Object\nFirst up, the basic str type (e.g., 'text') is for decoded Unicode text. It’s\nformally defined as an immutable sequence of characters—which means code\npoints that are not necessarily bytes. Its content may contain both simple text,\nsuch as ASCII, whose encoded and decoded forms might yield one byte per\ncharacter, as well as richer Unicode text, whose encoded and decoded forms may\nboth require multiple bytes per character.\nIn memory, a str is just an ordered collection of Unicode code-point integers,\nwhich print as glyphs—visual representations that may vary from host to host—\nof the characters that the code points represent. When transferred to and from\nfiles, a str is automatically encoded to and decoded from a sequence of bytes\nusing either the host platform’s default or a provided encoding name to translate\nwith an explicit scheme. str objects themselves, however, have no notion of an\nencoding; they are just character code points.\nThe bytes Object\nWhile str is great for Unicode text, many programs need to process raw binary\ncontent that is not encoded per any Unicode format—as well as the bytes used to\nstore text when it is encoded. Image files and packed data you might process\nwith Python’s struct module fall into this category. To accommodate this, the\nbytes type supports processing of truly binary data. bytes is just raw bytes, not\nUnicode-text characters, though its content may include the bytes of still-\nencoded text, which always has an implied encoding.\nThe bytes type is formally defined as an immutable sequence of 8-bit integers.\nIts content represents byte values, and it supports almost all the same operations\nthat the str type does; this includes string methods, sequence operations, and\neven re module pattern matching (formatting works on bytes today, too, but\nwas added later in 3.X’s evolution).\nA bytes object really is a sequence of small integers, each of which is in the\nrange 0…255: indexing a bytes returns an int, slicing one returns another\nbytes, and running list on one returns a list of integers, not characters.\nHowever, when processed with operations that assume characters (e.g., the",
      "content_length": 2157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1444,
      "chapter": null,
      "content": "isalpha method), the contents of bytes objects are assumed to be ASCII-\nencoded bytes. Further, bytes items whose values fall in the range of ASCII\ncharacter codes are printed as ASCII-character glyphs instead of integers or their\nhex escapes; this is done for convenience, though it may also confuse the\ndistinction between text and binary data.\nThe bytearray Object\nThough less commonly used, Python also comes with bytearray, a variant of\nbytes that is mutable and so supports in-place changes. The bytearray type\nprovides the usual string operations that str and bytes do but also has many of\nthe same in-place change operations as lists (e.g., append and extend methods\nand assignment to indexes). Assuming your strings can be treated as raw bytes,\nbytearray adds direct in-place mutability for string data—something long\nprohibited by str and bytes.\nText and Binary Files\nBecause file I/O is one of the main benefactors of encodings, it’s also a core\nUnicode tool. As we’ve seen, text is really just decoded integer character codes\nwhen it is in memory; it’s when text is transferred to and from external\ninterfaces like files that Unicode encodings come into play. By contrast, truly\nbinary data may have nothing at all to do with encodings—or text at all. Because\nof this, Python makes a sharp platform-independent distinction between text and\nbinary files accessed with the built-in open function:\nWhen a file is opened in text mode, reading its data automatically\ndecodes its content and returns it as a str, and writing takes a str and\nautomatically encodes it before transferring it to the file. In both cases,\nthe encoding to use is either a platform default or a provided encoding\nargument to open. Text mode files also support universal newline (a.k.a.\nend-of-line) translation, BOMs, and other encoding arguments.\nWhen a file is opened in binary mode by adding a b to the mode string\nargument in the open call, reading its data does not decode it in any way\nand simply returns its content raw and unchanged as a bytes object.",
      "content_length": 2041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1445,
      "chapter": null,
      "content": "Writing similarly takes a bytes object and transfers it to the file\nunchanged, and binary-mode files also accept a bytearray object for\nthe content to be written to the file.\nBecause str and bytes are sharply differentiated by the language this way, you\nmust decide whether your data is text or binary in nature and use str or bytes\nobjects to represent its content in your script, respectively. Ultimately, the mode\nin which you open a file will dictate which type of object your script will use to\nrepresent its content:\nIf you are processing image or audio files, packed data created by other\nprograms whose content you must extract, and some device data\nstreams, chances are good that you will want to deal with it using bytes\nand binary-mode files. You might also opt for bytearray to update the\ndata without making copies of it in memory.\nIf instead you are processing something that is textual in nature, such as\nprogram output, HTML or JSON content, and CVS or XML files, you\nprobably want to use str and text-mode files.\nSubtly, the mode-string argument to open (its mode keyword and second\npositional argument) becomes fairly crucial—its content not only specifies a file\nprocessing mode but also implies a Python object type. By adding a b (lowercase\nonly) to the mode string, you specify a binary mode file and will receive, or\nusually provide, a bytes object to represent the file’s content when reading or\nwriting. Without the b, your file is processed in text mode, and you’ll use str\nobjects to represent its content. For example, modes rb, wb, and rb+ imply\nbytes, but r, w+, and rt (the default: read text) imply str.\nIf you’re anxious to see files in action, watch for the examples ahead, especially\nthose of Unicode text files. To understand file usage in full, though, we first need\nto explore string operations as they extend to Unicode and bytes.\nUsing Text Strings\nLet’s step through a few examples that demo the prior section’s string types live.",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1446,
      "chapter": null,
      "content": "Here, our primary focus is on using these types for text (we’ll explore binary\nroles later). Along the way, you’ll see how literals, conversions, and non-ASCII\ntext are coded in Python.\nLiterals and Basic Properties\nMost Python string objects are born when you call a built-in function such as\nstr or bytes, process a file created by calling open, or code literal syntax in\nyour script. For the latter, the usual '…' makes a str; a unique literal form b'…'\nis used to create a bytes; and bytearray objects may be made by calling the\nsame-named function with a variety of possible arguments.\nMore formally, all the usual string literal forms we met in Chapter 7—'…', \"…\",\nand triple-quoted blocks—generate a str; adding a b or B just before them\ncreates a bytes instead. This b'…' (and equivalently, B'…') bytes literal is\nsimilar in spirit to the r'…' raw string we’ve also met, which suppresses\nbackslash escapes; in fact, the two prefixes can be combined to use backlashes\nverbatim in a bytes. Consider the following:\n>>> B = b'code'                # Make a bytes object: 8-bit bytes\n>>> S = 'hack'                 # Make a str object: Unicode characters\n>>> type(B), type(S)\n(<class 'bytes'>, <class 'str'>)\n>>> B                          # Sequence of ints, prints as ASCII characters\nb'code'\n>>> S                          # Sequence of code points, prints as text glyphs\n'hack'\n>>> B2 = B\"\"\"                  # bytes prefix works on single, double, triple\nxxxx\nyyyy\n\"\"\"\n>>> B2\nb'\\nxxxx\\nyyyy\\n'\n>>> b'A\\nB\\rC', br'A\\nB\\rC', rb'A\\nB\\rC'    # Raw-string combos work too\n(b'A\\nB\\rC', b'A\\\\nB\\\\rC', b'A\\\\nB\\\\rC')\nOnce you have a string, all the usual operations we met earlier work, but their",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1447,
      "chapter": null,
      "content": "results are type specific (bytes is integers and str is characters), and\nimmutability still applies:\n>>> B, S\n(b'code', 'hack')\n>>> B[0], S[0]                 # Indexing returns an int for bytes, str for str\n(99, 'h')\n>>> B[1:], S[1:]               # Slicing makes another bytes or str\n(b'ode', 'ack')\n>>> list(B), list(S)\n([99, 111, 100, 101], ['h', 'a', 'c', 'k'])     # bytes is really ints\n>>> B[0] = 'x'                                  # Both are immutable\nTypeError: 'bytes' object does not support item assignment\n>>> S[0] = 'x'\nTypeError: 'str' object does not support item assignment\nBecause they apply to more than text and have some unique behaviors, we’ll\ndefer bytearray and more about bytes to a dedicated section later in this\nchapter.\nNOTE\nBlast from the past: Python 3.X also recognizes Python 2.X’s Unicode string literals to ease\nmigration of 2.X code: a 2.X u'…' literal in Python 3.X is just a synonym for a 3.X '…' str\nliteral. This makes sense, given that 3.X’s str is all Unicode, and allows some 3.X code to run\non 2.X and vice versa. In today’s 3.X world, though, there’s no compelling reason to use the\nu'…' literal anymore (unless you’re a fan of superfluous prefixes), but it may crop up in\nPython code you’ll encounter in the wild. 2.X had a very long shelf life, after all.\nString Type Conversions\nSyntax aside, the first thing you might notice about Python strings is what they\ncannot do—str and bytes never mix automatically in expressions and generally\nare not converted to one another automatically when passed to functions. A\nfunction that expects an argument to be a str may not accept a bytes (and vice\nversa), and operators are fully rigid:",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1448,
      "chapter": null,
      "content": ">>> 'hack' + b'code'\nTypeError: can only concatenate str (not \"bytes\") to str\nThis is easier to understand if you remember that a text string may be radically\ndifferent in its encoded and decoded forms, and Python has no idea what the\ncontent of a bytes is: if the bytes is encoded text, its encoding is unknown, but\nit may also be binary data (e.g., a loaded audio file) that has nothing to do with\ntext at all.\nBecause of this ambiguity, Python basically requires that you either commit to\none type or the other or perform manual, explicit conversions with the following\ntools (where ? means optional):\nS.encode(encoding?) and bytes(S, encoding)\nEncode a str object S to a new bytes per encoding\nB.decode(encoding?) and str(B, encoding)\nDecode a bytes object B to a new str per encoding\nBoth the preceding S.encode() and B.decode() methods and the file open call\nwe’ll explore ahead use either an explicitly passed-in encoding name or a\ndefault. The methods’ default is always UTF-8 (by contrast, open uses a value in\nthe locale module you’ll meet shortly that may vary per platform, settings, and\nrun and should usually be avoided):\n>>> S = 'hack'\n>>> S.encode()                     # str to bytes: encode text into raw bytes\nb'hack'\n>>> bytes(S, encoding='ascii')     # str to bytes, alternative\nb'hack'\n>>> B = b'code'\n>>> B.decode()                     # bytes to str: decode raw bytes into text\n'code'\n>>> str(B, encoding='ascii')       # bytes to str, alternative\n'code'",
      "content_length": 1478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1449,
      "chapter": null,
      "content": "Putting this together solves our original type error and allows us to mix strings\nand bytes as either encoded or decoded text:\n>>> S, B\n('hack', b'code')\n>>> S.encode('ascii') + B          # bytes + bytes (encoded)\nb'hackcode'\n>>> S + B.decode('ascii')          # str + str (code points)\n'hackcode'\nA few cautions on defaults here. First of all, the encoding argument to bytes is\nnot optional, even though it is in S.encode() (and B.decode()). More subtly,\nalthough str does not require the encoding argument like bytes does, leaving it\noff in str calls does not mean it defaults—instead, due to Python history, a str\nwithout an encoding returns the bytes object’s print string, not its decoded and\nconverted str form (this is usually not what you’ll want!).\nAssuming again that B and S are still as in the prior listing:\n>>> bytes(S)\nTypeError: string argument without an encoding\n>>> str(B)                               # str() works without encoding\n\"b'code'\"                                # But print string, not conversion!\n>>> len(str(B))\n7\n>>> len(str(B, encoding='ascii'))        # Pass encoding to convert to str\n4\nAlso in the defaults department, your platform’s various default encodings are\navailable in the sys and locale modules but aren’t as trustworthy as you might\nthink:\n$ py -3\n>>> import sys, locale\n>>> sys.platform                         # Underlying platform: Windows\n'win32'\n>>> sys.getdefaultencoding()             # Methods default (but not for str()!)\n'utf-8'",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1450,
      "chapter": null,
      "content": ">>> locale.getpreferredencoding(False)   # open() default: a Latin-1 superset\n'cp1252'\nAs shown, the open function’s default file encoding lives in the locale module.\nOn the PC used for Windows examples in this chapter, it’s cp1252—a superset\nof Latin-1 that adds characters like slanted quotes. Defaults may differ, however,\non other platforms and technically can even depend on environment-variable\nsettings, command-line arguments, and settings on individual host machines.\nFor example, here’s the differing case on this chapter’s macOS host—like most\nUnix platforms, its open defaults to UTF-8:\n$ python3\n>>> import sys, locale\n>>> sys.platform                         # Underlying platform: macOS\n'darwin'\n>>> sys.getdefaultencoding()             # The default for methods is same\n'utf-8'\n>>> locale.getpreferredencoding(False)   # But this differs on Unix: be explicit!\n'utf-8'\nBesides such program-host differences, keep in mind that text content you\nreceive from disparate sources might also use any Unicode encoding at all,\nmaking your host’s default a moot point. Hence, your programs shouldn’t\ngenerally rely on open defaults if they may need to care about portability now or\nin the future—always pass an explicit encoding to open when interoperability\ncounts. We’ll revisit encoding defaults and learn how to provide an explicit\nencoding to open when we explore Unicode files later in this chapter.\nHaving said all that, it’s important to also note that encoding and decoding are\nsubstantially more than simple programming-language type conversions; really,\nthey produce very different kinds of data. Encoding returns the bytes that result\nfrom transforming a text string per a Unicode scheme, and decoding returns the\ntext string that is produced by undoing that transformation. While this is a\nconversion of sorts, and the mapping may seem trivial for simple text like\nASCII, Unicode tends to make much more sense if you avoid blurring the\ndistinction—especially for richer types of text like that in the next section.\nCoding Unicode Strings in Python",
      "content_length": 2065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1451,
      "chapter": null,
      "content": "Encoding and decoding grow more meaningful when you start dealing with non-\nASCII Unicode text. To code Unicode characters that may be difficult to type on\nyour keyboard, Python string literals support both:\n\\xNN hex escapes, where two hex digits (NN) specify a character code as\na 1-byte (8-bit) numeric value\n\\uNNNN and \\UNNNNNNNN Unicode escapes, where the first lowercase\nform gives 4 hex digits to denote a 2-byte (16-bit) character code, and\nthe second uppercase form gives 8 hex digits for a 4-byte (32-bit) code\nImportantly, in str objects, all three of these escapes are used to give a Unicode\ncharacter’s code-point value—not its encoded bytes. By contrast, bytes objects\nallow only hex escapes for byte values; for text, this gives its encoded form—not\nits decoded code points.\nLet’s see how this all translates to code. Simple 7-bit ASCII text is formatted\nwith one character per byte under most of the encoding schemes described near\nthe start of this chapter (again, this is why ASCII passes as a binary-compatible\nsubset of many other schemes):\n>>> ord('X')                # Character 'X' has code-point value 88 \n88\n>>> chr(88)                 # Code-point 88 stands for character 'X'\n'X'\n>>> S = 'XYZ'               # str: code points display as their character glyphs\n>>> S\n'XYZ'\n>>> len(S)                  # 3 characters (not necessarily bytes) long\n3\n>>> S.encode('ascii')       # Values 0…127 in 1 byte each (ASCII shown as chars)\nb'XYZ'\n>>> S.encode('latin-1')     # Values 0…255 in 1 byte each\nb'XYZ'\n>>> S.encode('utf-8')       # Values 0…127 in 1 byte, 128…2047 in 2, others 3~4\nb'XYZ'\nBy contrast, the less common UTF-16 and UTF-32 use 2 and 4 bytes for every\ncharacter, respectively, even for simple text like ASCII. This makes these\nencodings’ data fast to process but may consume extra space and bandwidth,",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1452,
      "chapter": null,
      "content": "which renders them subpar in some applications. In the following, ASCII bytes\nprint as characters, non-ASCIIs print as \\xNN escapes, padding bytes follow text,\nand each result has a 2- or 4-byte BOM header at the front whose details we’re\nlargely ignoring here (again, stay tuned for more on BOMs near the end of this\nchapter):\n>>> S\n'XYZ'\n>>> S.encode('utf-16')      # Always 2 or 4 bytes per character, with BOM header\nb'\\xff\\xfeX\\x00Y\\x00Z\\x00'\n>>> S.encode('utf-32')\nb'\\xff\\xfe\\x00\\x00X\\x00\\x00\\x00Y\\x00\\x00\\x00Z\\x00\\x00\\x00'\nTo code non-ASCII characters, you can use hex and Unicode escapes in your\nstrings. The numeric values coded as hexadecimal literals 0xC4 and 0xE8, for\ninstance, are the Unicode code points used to represent two special characters\noutside the 7-bit range of ASCII; we can embed them in str objects anyhow\nbecause str supports Unicode in full:\n>>> chr(0xc4)               # 0xC4 and 0xE8 are accented characters outside ASCII\n'Ä'\n>>> chr(0xe8)\n'è'\n>>> S = '\\xc4\\xe8'          # Hex escapes: code-point values, not encoded bytes\n>>> S\n'Äè'\n>>> S = '\\u00c4\\u00e8'      # Unicode escapes: 16-bits (2-bytes)\n>>> S\n'Äè'\n>>> len(S)                  # 2 characters long (not number of bytes!)\n2\nNow, if we try to encode a non-ASCII string like this to raw bytes as ASCII,\nwe’ll get an error. Encoding as Latin-1 works, though, and allocates 1 byte per\ncharacter; encoding as UTF-8 allocates 2 bytes per character instead. If you\nwrite this string to a text-mode file, the raw bytes shown are what is actually\nstored on the file for the encoding types given:",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1453,
      "chapter": null,
      "content": ">>> S = '\\u00c4\\u00e8' \n>>> S.encode('ascii')\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: \nordinal not in range(128)\n>>> S.encode('latin-1')              # 1 byte per character\nb'\\xc4\\xe8'\n>>> S.encode('utf-8')                # 2 bytes per character\nb'\\xc3\\x84\\xc3\\xa8'\n>>> len(S.encode('latin-1'))         # 2 bytes in latin-1, 4 in utf-8\n2\n>>> len(S.encode('utf-8'))\n4\nYou can also go the other way—from raw bytes back to a Unicode string. You\ncould read raw bytes from a file and decode manually this way, but the encoding\nmode you give to the open call causes this decoding to be done for you\nautomatically (and avoids issues that may arise from reading partial character\nsequences when reading by blocks of bytes):\n>>> B = b'\\xc4\\xe8'\n>>> B\nb'\\xc4\\xe8'\n>>> len(B)                             # 2 raw bytes, 2 characters\n2\n>>> B.decode('latin-1')                # Decode to latin-1 text\n'Äè'\n>>> B = b'\\xc3\\x84\\xc3\\xa8'\n>>> len(B)                             # 4 raw bytes\n4\n>>> B.decode('utf-8')\n'Äè'\n>>> len(B.decode('utf-8'))             # 2 Unicode characters\n2\nWhen needed, you can also specify both 16- and 32-bit Unicode code-point\nvalues for characters in your str strings: use \\u… with 4 hex digits for the former\nand \\U… with 8 hex digits for the latter. As the last example in the following\nshows, you can also build such strings up piecemeal using chr, but it might\nbecome tedious for large strings:",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1454,
      "chapter": null,
      "content": ">>> S = 'A\\u00c4B\\U000000e8C'\n>>> S                                  # A, B, C, and 2 non-ASCII characters\n'AÄBèC'\n>>> len(S)                             # 5 characters long\n5\n>>> S.encode('latin-1')\nb'A\\xc4B\\xe8C'\n>>> len(S.encode('latin-1'))           # 5 bytes in latin-1\n5\n>>> S.encode('utf-8')\nb'A\\xc3\\x84B\\xc3\\xa8C'\n>>> len(S.encode('utf-8'))             # 7 bytes in utf-8\n7\n>>> S.encode('cp500')                  # Two other Western European encodings\nb'\\xc1c\\xc2T\\xc3'\n>>> S.encode('cp850')                  # 5 bytes each\nb'A\\x8eB\\x8aC'\n>>> S = 'code'                         # ASCII text is the same in most\n>>> S.encode('latin-1')\nb'code'\n>>> S.encode('utf-8')\nb'code'\n>>> S.encode('cp500')                  # But not in cp500: IBM ebcdic\nb'\\x83\\x96\\x84\\x85'\n>>> S.encode('cp850')\nb'code'\n>>> S = 'A' + chr(0xC4) + 'B' + chr(0xE8) + 'C'    # str the hard way\n>>> S\n'AÄBèC'\nAnother distinction to keep in mind: Python allows special characters’ code\npoints to be coded with both hex and Unicode escapes in str string literals but\nallows only hex escapes in bytes literals—and prints a warning in its recent\nversions if you violate this rule. In fact, Unicode escape sequences are taken\nverbatim in bytes and not as escapes.\nThis makes sense if you remember that bytes objects hold binary data, both\ntextual and not. When they contain text, they hold characters’ encoded bytes—\nnot their decoded code points. Thus, Unicode code-point escapes simply don’t\napply to bytes, and hex escapes in their literals yield raw byte values, not",
      "content_length": 1541,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1455,
      "chapter": null,
      "content": "characters.\nThis is true even though code-point and encoded-byte values happen to be the\nsame for some characters in some encodings (confusingly!). Because bytes are\nnot code points, they also must be decoded to str to print their non-ASCII\ncharacters properly:\n>>> S = 'A\\xC4B\\xE8C'            # str recognizes hex and Unicode escapes\n>>> S\n'AÄBèC'\n>>> S = 'A\\u00C4B\\U000000E8C'    # 4- and 8-digit Unicode escapes (str only)\n>>> S\n'AÄBèC'\n>>> B = b'A\\xC4B\\xE8C'           # bytes recognizes hex escapes, but not Unicode\n>>> B\nb'A\\xc4B\\xe8C'\n>>> B = b'A\\u00C4B\\U000000E8C'   # Unicode escape sequences taken literally!\n<stdin>:1: SyntaxWarning: invalid escape sequence '\\u'\n>>> B                            # bytes is encoded bytes, not code points\nb'A\\\\u00C4B\\\\U000000E8C'\n>>> B = b'A\\xC4B\\xE8C'           # Use hex escapes for latin-1 bytes\n>>> B                            # Prints non-ASCII bytes as hex \nb'A\\xc4B\\xe8C'\n>>> print(B)                     # For both interactive and print()\nb'A\\xc4B\\xe8C'\n>>> B.decode('latin-1')          # Decode to str to interpret as text \n'AÄBèC'\nFinally, notice that bytes literals assume that textual characters embedded\nwithin them are ASCII and require escapes for byte values > 127. By contrast,\nstr literals in code like that in the following allow embedding any character\nsupported by the source code encoding of the hosting file or GUI (as you’ll learn\nin a moment, the encoding used for code defaults to UTF-8, sans declarations in\nthe code’s file):\n>>> S = 'AÄBèC'                  # Chars from UTF-8 if no encoding declaration\n>>> S                            # Decoded to str when code is read by Py\n'AÄBèC'\n>>> B = b'AÄBèC'",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1456,
      "chapter": null,
      "content": "SyntaxError: bytes can only contain ASCII literal characters.\n>>> B = b'A\\xC4B\\xE8C'           # Chars must be ASCII, or hex escapes\n>>> B                            # Non-ASCIIs are latin-1 encoded bytes\nb'A\\xc4B\\xe8C'\n>>> B.decode('latin-1')\n'AÄBèC'\n>>> S.encode()                   # Source code encoded per UTF-8 by default \nb'A\\xc3\\x84B\\xc3\\xa8C'           # Methods use UTF-8 to encode, unless passed\n>>> S.encode('utf-8')\nb'A\\xc3\\x84B\\xc3\\xa8C'\n>>> B.decode()                   # Raw bytes do not correspond to utf-8\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 1:…\n>>> S = 'AÄBèC'\n>>> S.encode()                   # Method's default utf-8 encoding\nb'A\\xc3\\x84B\\xc3\\xa8C'\n>>>\n>>> T = S.encode('cp500')        # \"Convert\" to EBCDIC bytes\n>>> T\nb'\\xc1c\\xc2T\\xc3'\n>>>\n>>> U = T.decode('cp500')        # Back to Unicode code points\n>>> U\n'AÄBèC'\n>>>\n>>> U.encode()                   # Back to UTF-8 bytes, by default\nb'A\\xc3\\x84B\\xc3\\xa8C'\nNotice how the last part of the preceding code seems to “convert” encodings\nfrom UTF-8 to cp500 and back again. Really, this just creates different encoded\nrepresentations of the same Unicode code points, but this pattern can be used to\ntranslate encoded text when needed. Text in a file, for instance, can be re-\nencoded with a decode (to str) plus an encode (to bytes) combination that\nchanges its stored encoding; as you’ll see ahead, the open function does most of\nthis work for you.\nAlso, note how the preceding code is able to use a str literal 'AÄBèC' with raw\nUnicode characters for its non-ASCII characters. This is noticeably simpler than\ncoding escapes and works as long as your code file (and GUI) support it, as the\nnext section will explain.\nSource-File Encoding Declarations",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1457,
      "chapter": null,
      "content": "Unicode escapes suffice for the occasional Unicode character in string literals,\nbut they can become tedious if you need to code non-ASCII text in your strings\nfrequently. For string literals and other text that you embed in your script files\n(or paste into your coding GUI), Python uses the UTF-8 encoding by default to\nread your code’s text but allows you to change this per file to use an arbitrary\nencoding. With this support, your code can directly embed any unescaped\ncharacters that the chosen encoding supports.\nTo make this work, simply use Python’s default UTF-8 encoding to save your\nsource code file in your text editor, or include a comment that names the\nUnicode encoding that you used for the save if it differs. This special encoding-\ndeclaration comment must appear as either the first or second line in your script\n(e.g., a #! line works before it: see Appendix A) and is usually of the following\nform (see Python’s manuals for other forms it accepts):\n# -*- coding: latin-1 -*-\nWhen present, Python will recognize text in your code represented natively\n(unescaped) in the given encoding. That way, you can edit your script file in a\ntext editor that accepts, displays, and saves accented and other non-ASCII\ncharacters, and Python will correctly decode them when reading your string\nliterals and other program-file text.\nFor example, notice the coding comment at the top of Example 37-1: when this\nfile is saved in the nondefault Latin-1 encoding, it allows Python to recognize\nLatin-1 characters embedded in the string literal to be assigned to myStr1 in the\ntext of the source file. This file also neatly summarizes the various ways to code\nnon-ASCII text in Python.\nExample 37-1. source-encoding-latin1.py\n# -*- coding: Latin-1 -*-\n#----------------------------------------------------------------------------\n# Demo all the ways to code non-ASCII text in Python, plus source encodings.\n#\n# If this file is saved as Latin-1 text, it works as is.  But changing the \n# coding line above to either ASCII or UTF-8 will then fail because the \n# Latin-1 0xc4 and 0xe8 saved in myStr1's value are not valid in either.\n#",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1458,
      "chapter": null,
      "content": "# A UTF-8 line works if this file is also saved as UTF-8 to make its mystr1 \n# text match.  Because UTF-8 is the default for source, the line above is \n# optional if the file is saved as UTF-8 or its text is all UTF-8 compatible\n# (e.g., ASCII, which is a subset of both the Latin-1 and UTF-8 encodings).\n#----------------------------------------------------------------------------\nmyStr1 = 'AÄBèC'                                      # Raw, per source encoding\nmyStr2 = 'A\\xc4B\\xe8C'                                # Hex code-point escapes\nmyStr3 = 'A\\u00c4B\\U000000e8C'                        # Unicode short/long escapes\nmyStr4 = 'A' + chr(0xC4) + 'B' + chr(0xE8) + 'C'      # Concatenated code points\nimport sys, locale\nprint('Sys hosting platform: ', sys.platform)\nprint('Sys default encoding: ', sys.getdefaultencoding())\nprint('Open default encoding:', locale.getpreferredencoding(False))\nfor aStr in (myStr1, myStr2, myStr3, myStr4):\n    print(f'{aStr}, strlen={len(aStr)}', end=', ')    # Decoded text+length\n    bytes1 = aStr.encode()               # Default UTF-8: 2 bytes for accents\n    bytes2 = aStr.encode('latin-1')      # Explicit Latin-1: 1 byte per char \n   #bytes3 = aStr.encode('ascii')        # ASCII fails: outside 0...127 range\n    print(f'byteslen1={len(bytes1)}, byteslen2={len(bytes2)}')   # Encoded length\nAfter saving this file in a text editor with encoding Latin-1 (or its default cp1252\nsuperset on some Windows), running it as a script prints its four strings, their\ncharacter code-point lengths, and their byte lengths in two encodings that work\n—the encode method’s default UTF-8, and an explicit Latin-1 (ASCII is too\nnarrow to use). The Python default encodings it also prints may vary across host\nplatforms, but the rest of the output will not:\n$ python3 source-encoding-latin1.py\nSys hosting platform:  darwin\nSys default encoding:  utf-8\nOpen default encoding: UTF-8\nAÄBèC, strlen=5, byteslen1=7, byteslen2=5\nAÄBèC, strlen=5, byteslen1=7, byteslen2=5\nAÄBèC, strlen=5, byteslen1=7, byteslen2=5 \nAÄBèC, strlen=5, byteslen1=7, byteslen2=5\nTo change this file to use UTF-8 instead, first save it with the UTF-8 encoding in",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1459,
      "chapter": null,
      "content": "your text editor or by running Python code like the following to make its\nembedded literal match (you’ll learn how and why this code works when we\nexplore Unicode text files ahead):\n>>> text = open('source-encoding-latin1.py', encoding='latin1').read()\n>>> open('source-encoding-utf8.py', 'w', encoding='utf8').write(text)\nThen, either mod its first line to name UTF-8, or delete the first line altogether\n(UTF-8 is Python’s default for source code). After you’re done, the only\ndifference in the two versions of the source file will be the way the embedded\nUnicode literal is rendered on your platform; here’s the verdict on the UTF-8-\ncentric macOS using its diff to compare (use fc instead on Windows):\n$ diff source-encoding-latin1.py source-encoding-utf8.py\n1c1\n< # -*- coding: Latin-1 -*-\n---\n> # -*- coding: UTF-8 -*-\n13c13\n< myStr1 = 'A?B?C'\n---\n> myStr1 = 'AÄBèC'\n$ python3 source-encoding-utf8.py    # Same output as Latin-1 version above\nSince most programmers are likely to fall back on the default and general (really,\nuniversal) UTF-8 encoding in Python, we’ll defer to Python’s standard manual\nset for more details on this option, as well as its more advanced and obscure\nUnicode support such as properties and character-name escapes in strings that\nwe’ll skip here.\nNOTE\nUnicode in variable names: Source-file encoding declarations apply to a file’s content in\ngeneral and support arbitrary kinds of text. The rules for variable names within a file’s code,\nhowever, are more stringent.\nAs noted briefly in Chapter 11, Python allows some, but not all, non-ASCII Unicode characters\nto be used for variables in your code. Roughly, number- and letter-like characters work, but\nsymbols and emojis are not allowed. For instance, hÄck is a valid variable, but hÄck\n is not.\nYou can check whether a specific string passes as a variable with the isidentifier method of\nstr, but the rules behind this are complex and best had in Python’s language manual.",
      "content_length": 1960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1460,
      "chapter": null,
      "content": "Also, keep in mind that, even when valid, non-ASCIIs in variables may make your code\ndifficult to use on some keyboards and devices outside a given language’s locale. In fact, all\ncode in the Python standard library must use ASCII-only identifiers for this reason. As usual,\nuse with care.\nUsing Byte Strings\nWe’ll be able to see strings in action again when we study files ahead. First,\nthough, let’s take a brief side trip to dig a bit deeper into the operation sets\nprovided by objects geared for binary data—the bytes type and its bytearray\nmutable kin. While these types can be used to hold encoded text too (as in prior\nsections), their scope is much broader: anything that can be stored as bytes\nworks, and that’s everything digital.\nAs mentioned earlier, Python’s bytes type supports sequence operations and\nmost of the same methods available on str. Even so, because you cannot mix\nand match bytes and str without explicit conversions, you’ll generally use str\nobjects and text files for text data and bytes objects and binary files for binary\ndata. This makes bytes a crucial tool in many roles and worthy of a quick demo\nhere.\nMethods\nIf you really want to see what attributes str has that bytes doesn’t, you can\nalways check their dir results (review: set(X)–set(Y) is items in X but not in\nY). This can also tell you something about the expression operators they support\n(e.g., __mod__ and __rmod__ implement the % operator, and they’re present in\nboth today):\n$ python3\nPython 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024,…\n# Attributes unique to str\n>>> sorted(set(dir('abc')) - set(dir(b'abc')))\n['casefold', 'encode', 'format', 'format_map', 'isdecimal', 'isidentifier', \n'isnumeric', 'isprintable']",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1461,
      "chapter": null,
      "content": "# Attributes unique to bytes\n>>> sorted(set(dir(b'abc')) - set(dir('abc')))\n['__buffer__', '__bytes__', 'decode', 'fromhex', 'hex']\nAs you can see, str and bytes have almost identical functionality; their unique\nattributes are generally methods that don’t apply to the other (format is an\noutlier you’ll meet shortly). For instance, decode translates a raw bytes into its\nstr representation, and encode translates a str into its raw bytes\nrepresentation. Most other methods are shared between the two types. Moreover,\nbytes are immutable just like str:\n>>> B = b'code'                    # b'…' bytes literal\n>>> B.find(b'od')                  # Search for substr offset\n1\n>>> B.replace(b'od', b'XY')        # New bytes with replacement\nb'cXYe'\n>>> B\nb'code'\n>>> B[0] = 'x'\nTypeError: 'bytes' object does not support item assignment\nFor more bytes methods, see the earlier coverage of string fundamentals in\nChapter 7; bytes do most of the same work, though their methods generally\nreturn a new bytes instead of a str.\nSequence Operations\nBesides method calls, all the usual generic sequence operations you know from\nother sequences, like lists, work as expected on both str and bytes. This\nincludes indexing, slicing, concatenation, and so on. As we’ve learned, str is a\nsequence of character code points, and bytes is a sequence of byte-size integers,\nbut their sequence operations’ semantics are the same.\nNotice in the following, though, that indexing bytes returns an integer giving\nthe byte’s binary value; bytes really is a sequence of 8-bit integers in the 0…\n255 range, but when displayed, its components print as either ASCII characters\nif their values fall into ASCII’s 0…127 code-point range, or as hex escapes",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1462,
      "chapter": null,
      "content": "otherwise:\n>>> B = b'code'           # bytes and str are both sequences\n>>> B                     # Bytes in the 0…127 range display as ASCII\nb'code'\n>>> B[0], B[-1]           # Indexing: returns a byte's integer value\n(99, 101)\n>>> 'code'[0]             # But indexing a str returns a 1-item str\n'c'\n>>> chr(B[0])             # Unicode character (code point) for byte's value\n'c'\n>>> list(B)               # But it's really integer bytes\n[99, 111, 100, 101]\n>>> b'A\\x42C\\xFF\\x63'     # That happen to display as ASCII if in 0…127\nb'ABC\\xffc'\n>>> chr(0x63), hex(B[0])  # And accept hex escapes for byte values\n('c', '0x63')\n>>> B[1:], B[:-1]         # Slicing: bytes (that display 0…127 as ASCII)\n(b'ode', b'cod')\n>>> len(B)                # Length: number bytes (not necessarily characters)\n4\n>>> B + b'lmn'            # Concatenation: bytes\nb'codelmn'\n>>> B * 4                 # Repetition: bytes\nb'codecodecodecode'\nFormatting\nOne notable exception to the same-operations rule for strings: string formatting %\nexpressions work on bytes too (as of Python 3.5), but neither the format\nmethod nor f'…' f-string formatting is available for bytes—an odd bifurcation\nthat seems to forget that formatting comes in multiple flavors today:\n>>> b'a %s string' % b'fine'                  # Py 3.5+ formatting for bytes\nb'a fine string'\n>>> b'a %s string' % bytes([0xFF, 0xFE])      # Non-ASCII bytes work too\nb'a \\xff\\xfe string'\n>>> 'a {} string'.format('fine')              # But format method only for str\n'a fine string'\n>>> b'a {} string'.format(b'fine')",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1463,
      "chapter": null,
      "content": "AttributeError: 'bytes' object has no attribute 'format'\n>>> kind = 'fine'\n>>> f'a {kind} string'                        # But f-strings only for str\n'a fine string'\n>>> bf'a {kind} string'\nSyntaxError: invalid syntax\nThere are arguably sound reasons that formatting shouldn’t work on bytes—it’s\njust raw bytes, after all, which happens to accept and print ASCII characters as\ntheir ASCII code-point values, and may use any encoding if it’s text, or none at\nall if it’s not. Plugging ASCII text into an encoded UTF-16 string or loaded\nimage, for instance, makes no sense. But these sound reasons are inconsistently\nviolated for the sake of the % expression alone.\nMoreover, post-operation conversion by encoding isn’t the same as bytes\noperations, and will fail if the default or explicit encoding isn’t inclusive enough\nto handle the text:\n>>> kind = 'fine'\n>>> f'a {kind} string'.encode()               # Encoding != bytes operations\nb'a fine string'\n>>> kind = 'AÄBèC'                            # And narrow encodings may fail\n>>> f'a {kind} string'.encode('ascii')\nUnicodeEncodeError: 'ascii' codec can't encode character '\\xc4' in position 3:…\nThis inconsistency is prone to change over time, but today, the embarrassment of\nriches in the formatting department has also given birth to an embarrassment of\nspecial cases.\nOther Ways to Make Bytes\nSo far in this section, we’ve been making bytes objects with the b'…' literal\nsyntax, but they can also be created by calling the bytes constructor with a str\nand an encoding name, by calling bytes with an iterable of integers representing\nbyte values, or by encoding a str object per the default (or passed-in) encoding.\nWe met some of these earlier in the guise of conversions, but they’re more\ngeneral than previously told.",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1464,
      "chapter": null,
      "content": "For example, encoding takes a str and returns the raw binary bytes value of the\nstring according to its encoding specification; decoding takes a raw bytes\nsequence and encodes it to its string representation—a series of Unicode\ncharacters:\n>>> B = b'abc'\n>>> B\nb'abc'\n>>> B = bytes('abc', 'ascii')\n>>> B\nb'abc'\n>>> ord('a')\n97\n>>> B = bytes([97, 98, 99])\n>>> B\nb'abc'\n>>> B = 'code'.encode()       # Or bytes()\n>>> B\nb'code'\n>>> S = B.decode()            # Or str()\n>>> S\n'code'\nAs we saw earlier, the last two of these operations can also be thought of as tools\nfor converting between str and bytes, as expanded upon in the next section.\nMixing String Types\nWhen we used the replace earlier when sampling bytes methods, you may\nhave noticed that we had to pass in two bytes objects for from and to—str\ntypes won’t work there. More generally, Python requires specific string types in\nsome contexts and expects manual conversions if needed. Here’s the story for\nfunction and method calls:\n>>> B = b'code'\n>>> B.replace('od', 'XY')\nTypeError: a bytes-like object is required, not 'str'",
      "content_length": 1083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1465,
      "chapter": null,
      "content": ">>> B.replace(b'od', b'XY')\nb'cXYe'\n>>> B.replace(bytes('od'), bytes('XY'))\nTypeError: string argument without an encoding\n>>> B.replace(bytes('od', 'ascii'), bytes('XY', 'utf-8'))\nb'cXYe'\nThe same holds for mixed-type expressions: you should try to keep your text and\nbinary data separate, but if you must mix, you generally also must convert:\n>>> b'ab' + 'cd'\nTypeError: can't concat str to bytes\n>>> b'ab'.decode() + 'cd'                   # bytes to str\n'abcd'\n>>> b'ab' + 'cd'.encode()                   # str to bytes\nb'abcd'\n>>> b'ab' + bytes('cd', 'ascii')            # str to bytes\nb'abcd'\nTwo notes here. First, remember that encoding and decoding are more than a\nsimple type conversion; as we learned in the coverage earlier, they create\ndifferent types of data altogether. Second, although you can create bytes objects\nyourself to represent packed binary data, they can also be made automatically by\nreading files opened in binary mode, as we will later in this chapter. First,\nthough, let’s briefly explore bytes’ elusive and changeable colleague.\nThe bytearray Object\nSo far in this chapter, we’ve focused on str and bytes because they will be your\ngo-to string tools. Python, however, has a third string type—bytearray, which\nis essentially a mutable variant of bytes, and thus a mutable sequence of\nintegers in the range 0…255. As such, it supports the same string methods and\nsequence operations as bytes, as well as the mutable in-place-change operations\nfound on lists.\nWe’ve already seen most operations that apply to bytearray, so we’ll just take a",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1466,
      "chapter": null,
      "content": "quick tour here to sample their flavor. First off, you can call bytearray as a\nfunction passing a bytes (not a str) to make a new mutable sequence of small\n(0…255) integers:\n>>> B = b'code'               # A str 'code' does not work: not bytes\n>>> C = bytearray(B)\n>>> C\nbytearray(b'code')\n>>> C[0], chr(C[0])           # ASCII code-point integer for 'c'\n(99, 'c')\nOnce you’ve got a bytearray, you can change it in place using the same sorts of\noperations available to modify a list, but keep in mind that you must assign just\nintegers to its cells, not str text strings, and not arbitrary objects:\n>>> C[0] = 'x'\nTypeError: 'str' object cannot be interpreted as an integer\n>>> C[0] = b'x'\nTypeError: 'bytes' object cannot be interpreted as an integer\n>>> C[0] = ord('x')\n>>> C\nbytearray(b'xode')\n>>> C[1] = b'Y'[0]\n>>> C\nbytearray(b'xYde')\nThe bytearray’s methods set overlaps broadly with both str and bytes\nbecause it’s a kind of string sequence, but it also has the list object’s in-place\nchange methods because it’s mutable too (the second of the following means\nattributes unique to bytearray):\n>>> sorted(set(dir(b'abc')) - set(dir(bytearray(b'abc'))))\n{'__bytes__', '__getnewargs__'}\n \n>>> sorted(set(dir(bytearray(b'abc'))) - set(dir(b'abc')))\n['__alloc__', '__delitem__', '__iadd__', '__imul__', '__release_buffer__',\n'__setitem__', 'append', 'clear', 'copy', 'extend', 'insert', 'pop', 'remove', \n'reverse']",
      "content_length": 1418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1467,
      "chapter": null,
      "content": "Hence, it’s something of a combo platter—you get methods for in-place changes:\n>>> C\nbytearray(b'xYde')\n>>> C.append(b'LMN')\nTypeError: 'bytes' object cannot be interpreted as an integer\n>>> C.append(b'LMN'[0])\n>>> C\nbytearray(b'xYdeL')\n>>> C.append(ord('M'))\n>>> C\nbytearray(b'xYdeLM')\n>>> C.extend(b'NO')\n>>> C\nbytearray(b'xYdeLMNO')\nPlus all the usual sequence operations and string methods:\n>>> C + b'!#'\nbytearray(b'xYdeLMNO!#')\n>>> C[0], chr(C[0])\n(120, 'x')\n>>> C[1:]\nbytearray(b'YdeLMNO')\n>>> len(C)\n8\n>>> C\nbytearray(b'xYdeLMNO')\n>>> C.replace('xY', 'co')\nTypeError: a bytes-like object is required, not 'str'\n>>> C.replace(b'xY', b'co')\nbytearray(b'codeLMNO')\n>>> C\nbytearray(b'xYamLMNO')\n>>> C * 4\nbytearray(b'xYdeLMNOxYdeLMNOxYdeLMNOxYdeLMNO')",
      "content_length": 755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1468,
      "chapter": null,
      "content": "For completeness, this section would be remiss if it didn’t call out that you can\nalso make a bytes or bytearray by passing in any sort of integer sequence or\ngenerator, not just text strings. This also provides a sort of conversion from an\ninteger character code to a bytes, but only if the integer is in the range 0…255,\nand only if it’s embedded in a sequence (else you get a bytes with that many\nzeroes—surprisingly!). See Python’s manuals for more esoteric bits that we\ndon’t have space to cover here:\n>>> bytes([1, 2, 3, 4]), bytearray([1, 2, 3, 4])\n(b'\\x01\\x02\\x03\\x04', bytearray(b'\\x01\\x02\\x03\\x04'))\n>>> bytes([115])        # int => bytes\nb's'\n>>> bytes([115])[0]     # bytes => int\n115\n>>> bytes([999])        # Too big for a byte\nValueError: bytes must be in range(0, 256)\n>>> bytes(115)          # Lots of zeroes!\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00…\n>>> bytes(range(97, 104)), bytes(c + 97 for c in range(7))    # Generators too\n(b'abcdefg', b'abcdefg')\nFinally, by way of summary, the following examples demonstrate how bytes\nand bytearray are sequences of integers, and str is a sequence of characters\n(i.e., decoded Unicode code points); although all three can contain textual\ncontent and support many of the same operations, you should use str for\ndecoded text, bytes for binary data including encoded text, and bytearray for\nbinary data you wish to change in place to avoid the time and space overheads of\ngenerating copies for each change you make:\n>>> B = b'code'                   # Bytes\n>>> list(B)\n[99, 111, 100, 101]\n>>> C = bytearray(b'code')        # Changeable bytes\n>>> list(C)\n[99, 111, 100, 101]",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1469,
      "chapter": null,
      "content": ">>> S = 'code'                    # Unicode text\n>>> list(S)\n['c', 'o', 'd', 'e']\nUsing Text and Binary Files\nNow that we’ve learned all about Python’s string types, let’s return to their roles\nin files—the main context in which most programmers will likely encounter\nUnicode and bytes.\nAs mentioned earlier, the mode in which you open a file is crucial in Python: it\ndetermines both how the file’s content is interpreted as well as the object type\nyou will use to process that content in your script. By way of review, text mode\nimplies str objects and binary mode implies bytes, as follows:\nText-mode files\nInterpret file contents according to an encoding—either the default for your\nplatform or one whose name you pass in to open. By passing in an encoding\nname, you can force conversions for various types of Unicode files. Text-\nmode files may also handle BOM headers for some encodings (deferred till\nthe end of this chapter) and may perform universal newline translations for\nyou or not; by default, all newline forms map to the \\n character in your\nscript, regardless of which platform you are on.\nBinary-mode files\nInstead return file content to you raw as a sequence of integers representing\nbyte values, with no encoding or decoding, no BOM handling, and no\nnewline translations.\nIn terms of code, the second positional argument to open (a.k.a. mode when\npassed by keyword) determines whether you want text or binary processing and\ntypes—adding a b to the mode string implies binary mode. The default mode is\nrt, which is the same as r, and means text input. In addition, the mode argument",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1470,
      "chapter": null,
      "content": "to open also implies an object type for file content representation regardless of\nthe underlying platform—text files return a str for reads and expect one for\nwrites, but binary files return a bytes for reads and expect bytes (or\nbytearray) for writes.\nText-File Basics\nTo demonstrate, let’s review basic file I/O. As long as you’re processing simple\ntext files that adhere to your platform’s default encoding, files look and feel\nmuch as they do in this book’s earlier coverage (for that matter, so do strings in\ngeneral). The next example, for instance, writes one line of text to a file and\nreads it back:\n>>> file = open('temp.txt', 'w')     # Use default encoding on host, mode w=write\n>>> size = file.write('abc\\n')       # Returns number characters written\n>>> file.close()                     # Manual close to flush output buffer\n>>> file = open('temp.txt')          # Default mode is \"r\" == \"rt\": text input\n>>> text = file.read()\n>>> text\n'abc\\n'\nAs a refresher, the first argument to open is the file’s pathname—the address of\na file in the host’s folder hierarchy that’s either absolute or relative to the current\ndirectory. The second argument to open is mode—where w means write text, and\nthe default means read it, and write methods return the number of written items\n—either characters for text mode or bytes for binary mode. Also, the close call\nhere is optional in some contexts (e.g., the widely used CPython auto-closes files\nwhen their objects are garbage collected) but is generally advised to flush\nchanges and avoid memory growth.1\nTechnically, the preceding example writes and reads Unicode text, but it’s hardly\nnoticeable: the ASCII text string is encoded and decoded per the hosting\nplatform’s encoding default. We’re also relying on platform-agnostic newline\nhandling for \\n, but we must move ahead for more on encodings and newlines.\nText and Binary Modes",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1471,
      "chapter": null,
      "content": "Next, let’s write a text file and read it back in both text and binary modes. Notice\nin the following how text mode requires us to provide a str for writing, rb\ndistinguishes binary-mode input, and reading gives us a str or bytes depending\non the mode (opens and transfer operations are strung together here into one-\nliners just for brevity; again, remember to close explicitly in production code,\nand possibly in some IDEs and outside CPython):\n$ py -3                                      # Run on Windows (no \\r on Unix) \n>>> open('temp.txt', 'w').write('abc\\n')     # Text-mode output, provide a str\n4\n>>> open('temp.txt', 'r').read()             # Text-mode input, returns a str\n'abc\\n'\n>>> open('temp.txt', 'rb').read()            # Binary-mode input, returns a bytes\nb'abc\\r\\n'\nObserve how the newline character is always \\n when writing and reading in\ntext mode with str but \\r\\n when reading in binary mode with bytes. This\nreflects the fact that this was run on Windows. Though it has nothing to do with\nUnicode, text-mode files automatically map all \\n in str to and from the host\nplatform’s newline separator: \\r\\n on Windows and just \\n on Unix. When\nreading in binary mode, though, we get what’s actually in the file—with neither\nnewline mapping nor Unicode decoding.\nNow, let’s do the same, but with a binary file. We provide a bytes to write and\nstill get back a str or bytes depending on the input mode, though the \\n isn’t\nexpanded to \\r\\n on Windows this time:\n>>> open('temp.bin', 'wb').write(b'abc\\n')   # Binary-mode output, send a bytes\n4\n>>> open('temp.bin', 'r').read()             # Text-mode input, receive a str\n'abc\\n'\n>>> open('temp.bin', 'rb').read()            # Binary-mode input, bytes sans mapping\nb'abc\\n'\nThis holds true even if the data we’re writing to the binary file is truly binary in\nnature. In the following, the \\x00 is a binary zero byte and not a printable",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1472,
      "chapter": null,
      "content": "character, though it works in the middle of a bytes and qualifies as a text code\npoint in the default encoding (strictly speaking, zero is a character called null or\nNUL in ASCII and its supersets like UTF-8):\n>>> open('temp.bin', 'wb').write(b'a\\x00c')    # Binary data included\n3\n>>> open('temp.bin', 'r').read()               # Text-mode: str with NUL\n'a\\x00c'\n>>> open('temp.bin', 'rb').read()              # Binary mode: bytes\nb'a\\x00c'\nBinary mode files always return contents as a bytes object but accept either a\nbytes or bytearray object for writing. This naturally follows, given that\nbytearray is mostly just a mutable variant of bytes. In fact, most APIs in\nPython that accept a bytes also allow a bytearray (the bytes here are also\nASCII characters too obscure for this note):\n>>> BA = bytearray(b'\\x01\\x02\\x03')\n>>> open('temp.bin', 'wb').write(BA)\n3\n>>> open('temp.bin', 'r').read()\n'\\x01\\x02\\x03'\n>>> open('temp.bin', 'rb').read()\nb'\\x01\\x02\\x03'\nFinally, notice that you can’t get away with violating Python’s str/bytes (i.e.,\ntext/binary) distinction when it comes to files; in the following, we get errors if\nwe try to write a bytes to a text file or a str to a binary file. Remember,\nalthough it is often possible to convert between these two types (as described\nearlier in this chapter), you will usually want to stick to str for text data and\nbytes for binary data:\n>>> open('temp.txt', 'w').write('abc\\n')            # Auto encodes str to bytes\n4\n>>> open('temp.txt', 'w').write(b'abc\\n')           # But bytes != decoded text\nTypeError: write() argument must be str, not bytes",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1473,
      "chapter": null,
      "content": ">>> open('temp.bin', 'wb').write(b'abc\\n')          # Writes raw bytes\n4\n>>> open('temp.bin', 'wb').write('abc\\n')           # But str != raw bytes\nTypeError: a bytes-like object is required, not 'str'\nThis may seem strict, but Python cannot guess how you wish to interpret the\ncontents of a bytes or str when used in the opposite context and wisely refuses\nto convert implicitly (a bytes might be an image, after all). Moreover, because\nstr and bytes operation sets largely intersect, the choice of types won’t be\nmuch of a dilemma for most programs. Watch for the struct module coverage\nahead for another binary-file example.\nUnicode-Text Files\nAnd now for the featured attraction of our files tour: text files with non-ASCII\ntext. Beyond their text/binary distinction, Python files come with additional\nrequirements and tools for dealing with Unicode text. In short, text files allow a\nspecific Unicode encoding-scheme name to be passed in with an encoding\nargument to open and use it to automatically decode and encode text on input\nand output, respectively. As abstract examples, for a file identified by a\npathname string:\nopen(pathname, 'r', encoding='utf8')\nReturns a file object that decodes text from UTF-8 on reads\nopen(pathname, 'w', encoding='latin1')\nReturns a file object that encodes text to Latin-1 on writes\nThe file object returned by the first of the preceding assumes the file’s content is\nencoded per UTF-8 and automatically decodes it to str Unicode code points\nwhen read by the program. Similarly, the result of the second line of code\nencodes str code points to their Latin-1 format as they are output to the file.\nHere’s the text-file story with the universal UTF-8 encoding and three non-\nASCII characters in the content:\n>>> file = open('uni.txt', 'w', encoding='utf8')          # Auto encodes to bytes",
      "content_length": 1830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1474,
      "chapter": null,
      "content": ">>> file.write('\n 2 hÄck \n')\n10\n>>> file.close()\n>>> text = open('uni.txt', 'r', encoding='utf8').read()   # Auto decodes to str\n>>> text\n'\n 2 hÄck \n'\n \n>>> [ord(c) for c in text]                                # Character code points\n[128155, 32, 50, 32, 104, 196, 99, 107, 32, 128013]\n \n>>> raw = open('uni.txt', 'rb').read()                    # No decoding applied\n>>> raw\nb'\\xf0\\x9f\\x92\\x9b 2 h\\xc3\\x84ck \\xf0\\x9f\\x90\\x8d'\nFile transfers raise exceptions whenever a requested encoding doesn’t work, so\nthe encoding you pass must match the data. For example, ASCII is not inclusive\nenough to handle the augmented and emoji characters in the text we’re writing\nhere and fails on both reads and writes:\n>>> open('uni.txt', 'r', encoding='ascii').read()\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 0:…\n>>> open('ascii.txt', 'w', encoding='ascii').write('\n 2 hÄck \n')\nUnicodeEncodeError: 'ascii' codec can't encode character '\\U0001f49b'…\n>>> hex(ord('\n'))    # ASCII will always break your heart?\n'0x1f49b'\nBy contrast, using the broader and more general UTF-16 on both ends handles\nthis text in full, though its encoded bytes stored in the file naturally differ from\nthose of UTF-8:\n>>> file = open('uni2.txt', 'w', encoding='utf16')         # UTF-16 works too \n>>> file.write('\n 2 hÄck \n')\n10\n>>> file.close()\n \n>>> text = open('uni2.txt', 'r', encoding='utf16').read()  # Files encode+decode\n>>> text\n'\n 2 hÄck \n'\n \n>>> [ord(c) for c in text]                                 # Same code points\n[128155, 32, 50, 32, 104, 196, 99, 107, 32, 128013]",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1475,
      "chapter": null,
      "content": ">>> open('uni2.txt', 'rb').read()                          # Different encoding\nb'\\xff\\xfe=\\xd8\\x9b\\xdc \\x002\\x00 \\x00h\\x00\\xc4\\x00c\\x00k\\x00 \\x00=\\xd8\\r\\xdc'\nEven for broader encodings like UTF-8 and UTF-16, though, you cannot mix\nand match: using an incompatible encoding still won’t work because encoded\nbytes in the file differ. Although you can sometimes handle unknown encodings\nwith binary-mode files, open error handlers, and other techniques, your encoding\nmust generally match your file’s data:\n>>> open('uni.txt', 'r', encoding='utf16').read()\nUnicodeDecodeError: 'utf-16-le' codec can't decode byte 0x8d in position 16:…\n>>> open('uni2.txt', 'r', encoding='utf8').read()\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0:…\nIn the absence of an encoding argument, text files still encode and decode per a\nhost- and platform-specific default in locale discussed earlier. But as a\nreminder: although you may not notice these translations if your default and files\nagree, you generally should not rely on the default; it makes your programs\ndependent on the context in which their files were created and can lead to\nportability issues (and nightmares; see the upcoming sidebar “Unicode Defaults\nand UTF-8 Mode”).\nFor instance, a program run on a UTF-8 default platform (the macOS and\nAndroid norm) may have trouble using a file made under a cp1252 default (the\nASCII-superset default on some Windows hosts) and vice versa, and content\nfetched from other devices may be encoded arbitrarily. Use explicit open\nencodings as a rule. This may also help if environment settings are not reliable\n(e.g., when running as a generic user for security in server-side web scripts).\nAll that being said, you’ll probably find files to be easier in practice than the full\ndetails may suggest. Accessing web pages and images, for example, soon\nbecomes second nature and simple. For variety, the following first omits mode\n(again, its default is the same as r) and then passes mode by explicit keyword\n(instead of position), and this example is abstract (substitute pathnames of real\nand accessible files on your device to run live):\n>>> text = open('Websites/about-lp5e.html', encoding='utf8').read()\n>>> text[:79]",
      "content_length": 2226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1476,
      "chapter": null,
      "content": "'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.o'\n>>> image = open('Websites/lp5e-large.jpg', mode='rb').read()    \n>>> image[:20]\nb'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00'\nOnce you’ve loaded content this way, all the str and bytes operations we’ve\nseen in this chapter are at your disposal for wrangling text and binary data, and\nsaving the result follows the same pattern—simply use an encoding for text and\nbinary mode for bytes:\n>>> text = text.replace('2013', '2024')\n>>> open('Websites/about-lp6e.html', mode='w', encoding='utf8').write(text)\n>>> image = some_sort_of_modding(image)\n>>> open('Websites/lp6e-large.jpg', mode='wb').write(image)\nYou might mod images, for example, with the many tools provided by the third-\nparty Pillow (f.k.a. PIL) imaging library for Python or similar. With Python’s\nfiles and strings, content possibilities are largely endless.\nIn the interest of full disclosure, Python’s open accepts additional arguments that\nmodify its behavior. Among them, newline changes newline mapping; errors\nspecifies handling of encoding and decoding errors (e.g., 'surrogateescape'\nreplaces failing bytes with sequences that can be used to restore them on output);\nand buffering alters, well, buffering. Because their defaults are generally what\nyou’ll use, we’ll defer to the Python standard-library manual for the fine print on\nthese and other advanced file options.\nNOTE\nBlast from the past: Python’s codecs module also has an open that returns an auto-\ndecode/encode file object just like the built-in open function. This was used for Python 2.X\nbackward compatibility in times gone by, but there’s no obvious reason to rely on it in new\ncode. Still, you might see it in legacy code (and code that relies on any of its unique behaviors)\nanyhow:\n>>> import codecs\n>>> f = codecs.open('uni.txt', 'r', encoding='utf8')\n>>> f.read()\n'\n 2 hÄck \n'",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1477,
      "chapter": null,
      "content": "Unicode, Bytes, and Other String Tools\nBesides built-in strings and files, many of the popular string-processing tools in\nPython’s standard library installed with Python itself also adhere to the\nstr/bytes dichotomy. We won’t cover any of these application-focused topics in\ndetail in this core-language book, but as a sample, here’s a very brief look at how\nsome of these tools handle the split. See Python’s Library Reference for more on\nthe tools used here if any pique your interest.\nThe re Pattern-Matching Module\nPython’s re pattern-matching module has been generalized to work on objects of\nany string type—str, bytes, and bytearray (a (.*) means any run, saved as a\ngroup):\n>>> import re\n>>> S = '\n is the fastest way to \n!'\n>>> B = b'Python is the fastest way to pizza!'\n>>> re.match('(.*) the (.*) way (.*)', S).groups()       # str + str => str\n('\n is', 'fastest', 'to \n!')\n \n>>> re.match(b'(.*) the (.*) way (.*)', B).groups()      # bytes + bytes => bytes\n(b'Python is', b'fastest', b'to pizza!')\n>>> re.match(b'(.*) the (.*) way (.*)', bytearray(B)).groups()\n(b'Python is', b'fastest', b'to pizza!')\nLike many tools, though, its result types depend on the type of strings you pass\nin, and you can’t mix str and bytes types in its calls’ arguments (sans explicit\nconversions, of course); bytearray is also unusable here as a pattern because\nit’s mutable (and changeable means “unhashable”):\n>>> re.match('(.*) the (.*) way (.*)', B).groups()\nTypeError: cannot use a string pattern on a bytes-like object\n>>> re.match(b'(.*) the (.*) way (.*)', S).groups()\nTypeError: cannot use a bytes pattern on a string-like object",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1478,
      "chapter": null,
      "content": ">>> re.match('(.*) the (.*) way (.*)', bytearray(B)).groups()\nTypeError: cannot use a string pattern on a bytes-like object\n>>> re.match(bytearray(b'(.*) the (.*) way (.*)'), bytearray(B)).groups()\nTypeError: unhashable type: 'bytearray'\nThe struct Binary-Data Module\nThe Python struct module, used to create and extract packed binary data from\nstrings, operates on bytes and bytearray only, not str—which makes sense,\ngiven that it’s intended for processing binary data, not decoded text. This module\nuses a format string to specify the types and sizes of objects to pack to and from\na bytes string, along with an endianness (i.e., significant side of bitstrings)\nspecifier like > for big-endian:\n>>> import struct\n>>> B = struct.pack('>i4sh', 7, b'code', 8)    # int , bytes(4s), int(h) => bytes\n>>> B\nb'\\x00\\x00\\x00\\x07code\\x00\\x08'\n>>> vals = struct.unpack('>i4sh', B)           # Packed data is bytes, not str \n>>> vals\n(7, b'code', 8)\n>>> vals = struct.unpack('>i4sh', B.decode())\nTypeError: a bytes-like object is required, not 'str'\nYou’ll often use this in conjunction with binary-mode files to make the packed\nbytes persistent across program runs (e.g., for saving and restoring app user\nsettings):\n>>> F = open('data.bin', 'wb')                    # Open binary output file\n>>> data = struct.pack('>i4sh', 7, b'code', 8)    # Create packed binary data\n>>> data                                          # bytes, not str\nb'\\x00\\x00\\x00\\x07code\\x00\\x08'\n>>> F.write(data)                                 # Write to the file\n10\n>>> F.close()                                     # Flush changes\n>>> F = open('data.bin', 'rb')                    # Open binary input file\n>>> data = F.read()                               # Read bytes\n>>> data\nb'\\x00\\x00\\x00\\x07code\\x00\\x08'\n>>> values = struct.unpack('>i4sh', data)         # Extract packed binary data",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1479,
      "chapter": null,
      "content": ">>> values                                        # Back to Python objects\n(7, b'code', 8)\nThe pickle and json Serialization Modules\nPython’s pickle module provides another way to save data in files but allows\nsaved data to be nearly arbitrary Python objects instead of values coerced to\nbytes as in struct. We met this module briefly in Chapters 9, 28, and 31. For\ncompleteness here, keep in mind that the pickle module always creates a bytes\nobject, regardless of the default or passed-in protocol (data format level). You\ncan see this by using the module’s dumps call to return an object’s pickle string:\n>>> import pickle                                # dumps() returns pickle string\n>>> pickle.dumps(['code', 4, '\n'])              # Default protocol=binary\nb'\\x80\\x04\\x95\\x15\\x00\\x00\\x00\\x00 …etc… \\x04\\x8c\\x04\\xf0\\x9f\\x90\\x8d\\x94e.'\n>>> pickle.dumps(['code', 4, '\n'], protocol=0)   # ASCII protocol 0, still bytes!\nb'(lp0\\nVcode\\np1\\naI4\\naV\\\\U0001f40d\\np2\\na.'\nThis implies that files used to store pickled objects must always be opened in\nbinary mode because text files use str strings to represent data, not bytes, and\nthe dump call simply attempts to write the pickled byte string to an open output\nfile:\n>>> pickle.dump(['code', 4, '\n'], open('temp.pkl', 'w'))   # bytes+text mode fail\nTypeError: write() argument must be str, not bytes  \n>>> pickle.dump(['code', 4, '\n'], open('temp.pkl', 'w'), protocol=0)\nTypeError: write() argument must be str, not bytes\n>>> pickle.dump(['code', 4, '\n'], open('temp.pkl', 'wb'))  # Always binary mode\n>>> open('temp.pkl', 'r').read()\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0:…\nNotice the last result fails here in text mode on macOS; if it doesn’t fail for you,\nit’s only because the stored binary data is compatible with your platform’s\ndefault decoding (i.e., just by luck!). A Windows host, for example, prints\ngibberish for the last result instead of an error message. Because pickle data is\nnot generally decodable Unicode text, the same rule holds on input as output—",
      "content_length": 2053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1480,
      "chapter": null,
      "content": "correct usage always requires both writing and reading pickle data with binary-\nmode files, whether unpickling or not:\n>>> pickle.dump(['code', 4, '\n'], open('temp.pkl', 'wb'))    # Save to file\n>>> pickle.load(open('temp.pkl', 'rb'))                      # Load from file\n['code', 4, '\n']\n>>> open('temp.pkl', 'rb').read()\nb'\\x80\\x04\\x95\\x15\\x00\\x00\\x00\\x00 …etc… \\x04\\x8c\\x04\\xf0\\x9f\\x90\\x8d\\x94e.'\nThe Python json module, introduced in Chapter 9, is a related tool: it converts a\nnested-object tree to a text string that can be saved on a file to make it persistent.\nUnlike pickle, json doesn’t support arbitrary objects (just basic types and built-\nin containers) but uses a language-neutral scheme that can make it more\ninteroperable with other programs.\nAlso unlike pickle, the json module always produces a str, so files for saves\nand loads should generally use text mode (JSON files are commonly encoded per\nUTF-8 for portability, but any encoding works as long as it’s consistent and\nexpected):\n>>> import json \n>>> vals = ['code', {'app': ('\n', None, 1.23, 99)}]\n>>> json.dumps(vals)\n'[\"code\", {\"app\": [\"\\\\ud83d\\\\ude42\", null, 1.23, 99]}]'\n>>> text = json.dumps(vals)                          # Save to/load from str\n>>> anew = json.loads(text)\n>>> anew\n['code', {'app': ['\n', None, 1.23, 99]}]\n>>> file = open('data.txt', 'w', encoding='utf8')    # Save/load text-mode file\n>>> json.dump(vals, file)\n>>> file.close()\n>>> file = open('data.txt', 'r', encoding='utf8')\n>>> json.load(file)\n['code', {'app': ['\n', None, 1.23, 99]}]\nFilenames in open and Other Filename Tools\nSo far in this chapter, we’ve focused on the content of files, but their names have\na Unicode story too. Its short version is that str is the norm for file pathnames",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1481,
      "chapter": null,
      "content": "in Python and is recommended for portability. If your code, like all the examples\nso far, uses str for filenames and pathnames, they simply work: Python’s file\ntools automatically translate them to and from the encoding used by the host\nplatform and device.\nIt turns out, though, that Python’s file tools also allow you to specify file\npathnames as bytes to skip automatic filesystem encoding in some contexts.\nThis bytes mode may come in handy if you need more control over filename\nencodings in cross-platform code, but it is rarely needed in typical programs. Per\nPython’s manuals, in fact, this mode need be used only on some Unix systems\nwhere undecodable filenames may be present.\nNevertheless, bytes filenames do work in these tools, as shown in the following\ndemo, which runs the same on macOS, Windows, Linux, and Android devices\ntested. As shown, the open function happily accepts text or bytes for a file’s non-\nASCII pathname (this section’s examples were run in subfolder _filenames in the\nbook’s examples package):\n>>> import sys, os, glob\n>>> sys.getfilesystemencoding()          # Filesystem default: macOS and Win11\n'utf-8'\n>>> name1 = 'hÄck\n1'                    # str filenames\n>>> name2 = 'hÄck\n2'\n>>> name2.encode('utf8')                 # bytes equivalent for name2 str\nb'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x822'\n>>> os.listdir()                         # Make files with str and bytes names\n[]\n>>> open(name1, mode='w', encoding='utf8').write('text1')\n5\n>>> open(b'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x822', 'w', encoding='utf8').write('text2')\n5\n>>> os.listdir()                         # Both show up on the filesystem\n['hÄck\n1', 'hÄck\n2']\n>>> open('hÄck\n2').read()               # And can be accessed either way\n'text2'\n>>> open('hÄck\n2'.encode('utf8')).read()\n'text2'\nFor bytes filenames, Python uses UTF-8 encoding on macOS and on Windows",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1482,
      "chapter": null,
      "content": "since 3.6 (with extra translation for the Windows API’s UTF-16). Linux, and by\nextension Android, accepts any sort of bytes for filenames, but Python tries to\nuse UTF-8 when converting to and from str (with surrogate escapes for\nencoding errors: see Python’s manuals).\nWhether bytes filenames are a useful tool or neat parlor trick depends on your\nuse cases, but keep in mind that bytes filenames will only work in open if their\nencoding matches the expectations of Python or the underlying filesystem. To\ndemo, the following passes Latin-1 bytes b'h\\xc4ck' to open: this fails on both\nmacOS and Windows as shown (though with a UnicodeDecodeError on the\nlatter because it’s trapped by Python):\n>>> 'hÄck'.encode('latin-1'), 'hÄck'.encode('utf-8')\n(b'h\\xc4ck', b'h\\xc3\\x84ck')\n>>> b'h\\xc4ck'.decode('utf8')\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 1:…\n>>> open(b'h\\xc4ck', 'w')\nOSError: [Errno 92] Illegal byte sequence: b'h\\xc4ck'\n# open(b'h\\xc4ck'.decode('latin1'), 'w')    # Works: convert to str\n# open(b'h\\xc3\\x84ck', 'w')                 # Works: use UTF-8 encoding\nBy contrast, the preceding code’s open works on Linux and Android—which\nlater decode the Latin-1 name b'h\\xc4ck' to str by UTF-8, as surrogate-\nescaped 'h\\udcc4ck'. Still, this works only on drives using a Linux-native\nfilesystem. For both its bytes and escaped str forms, the Latin-1 name fails in\nopen on Linux and Android when the target is a removable drive formatted as\nexFAT. Hence, the effect of using arbitrary bytes in open varies by both\nplatform and filesystem (that is, don’t do that!).\nThe os standard-library module’s directory-listing listdir similarly accepts\nstr or bytes and returns a folder’s names in the same form, ready to be used in\nother file tools (per the earlier listing, it also defaults to the current directory if\nno folder is passed):\n>>> os.listdir('.')                      # str gives strs, bytes gives bytes\n['hÄck\n1', 'hÄck\n2']",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1483,
      "chapter": null,
      "content": ">>> os.listdir(b'.')\n[b'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x821', b'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x822']\n>>> for name in os.listdir('.'):\n        print(name, '=>', open(name).read())\n \nhÄck\n1 => text1\nhÄck\n2 => text2\n>>> for name in os.listdir(b'.'):\n        print(name, '=>', open(name).read())\nb'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x821' => text1\nb'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x822' => text2\nThe glob module does filename expansion both ways, too (as str or bytes),\nand os.fsdecode decodes filenames per the host’s default automatically:\n>>> glob.glob('h*ck*')                   # str gives str, bytes gives byte\n['hÄck\n1', 'hÄck\n2']\n>>> glob.glob(b'h*ck*')\n[b'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x821', b'h\\xc3\\x84ck\\xf0\\x9f\\x99\\x822']\n>>> for name in glob.glob(b'h*ck*'):\n        print(name.decode(sys.getfilesystemencoding()), os.fsdecode(name))\nhÄck\n1 hÄck\n1\nhÄck\n2 hÄck\n2\nIn addition, tools that create and walk folders are similarly flexible for names\nand paths (demoed on Windows—your path separators and order may vary):\n>>> os.mkdir('sub\n')                         # Make dirs with str or bytes\n>>> os.mkdir('sub\nbytes'.encode('utf8'))     # and walk them with both\n>>> os.mkdir('sub\nbytes/subsub\n')\n>>> os.listdir()\n['hÄck\n1', 'hÄck\n2', 'sub\n', 'sub\nbytes']\n>>> os.listdir('sub\nbytes')\n['subsub\n']\n>>> for (dirhere, subshere, fileshere) in os.walk('.'):\n        print(dirhere, '=>', subshere, fileshere)\n. => ['sub\n', 'sub\nbytes'] ['hÄck\n1', 'hÄck\n2']",
      "content_length": 1418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1484,
      "chapter": null,
      "content": ".\\sub\n => [] []\n.\\sub\nbytes => ['subsub\n'] []\n.\\sub\nbytes\\subsub\n => [] []\n>>>\n>>> for (dirhere, subshere, fileshere) in os.walk(b'.'):\n        print(dirhere, '=>', subshere, fileshere)\nb'.' => [b'sub\\xf0\\x9f\\x99\\x8a', b'sub\\xf0\\x9f\\x99\\x8abytes'] [b'…', b'…']\nb'.\\\\sub\\xf0\\x9f\\x99\\x8a' => [] []\nb'.\\\\sub\\xf0\\x9f\\x99\\x8abytes' => [b'subsub\\xf0\\x9f\\x91\\x8d'] []\nb'.\\\\sub\\xf0\\x9f\\x99\\x8abytes\\\\subsub\\xf0\\x9f\\x91\\x8d' => [] []\nWhile bytes pathnames work as shown and may have some valid roles, it’s\nimportant to stress that you’re almost always better off using str strings to name\nfiles instead. Doing so leverages tools that already go to great lengths to make\npathnames do the right thing and might just avoid at least some portability issues\nthat can arise in apps whose scope must span platforms and devices.\nAnd that’s all the time and space we have for this tools survey. Really, Python’s\nstandard library and third-party domain are large and evolving toolsets that will\nlikely occupy much of your attention after you’ve finished this book and move\non to real programming tasks. Again, be sure to consult the Python Library\nReference soon and often for more info.\nUNICODE DEFAULTS AND UTF-8 MODE\nSo what’s the default encoding, then? This turns out to be a weirdly\nconvoluted story, which we’ve touched on lightly a few times. Now that\nwe’ve seen all the players in action, we can finally summarize and finalize\nthis thread.\nIn short, Python’s default Unicode encoding for both source code and string\nencoding/decoding methods is always UTF-8 everywhere; for filenames can\nbe had with sys.getfilesystemencod⁠ing(); and for file content accessed\nvia open is fetched with locale.getpreferredencod⁠ing(False).\nThe methods default is sys.getdefaultencoding(), but it can no longer be\nchanged as of Python 3.2 (and probably shouldn’t have been changed\nearlier). The filenames default is usually UTF-8, including on Windows as of\nPython 3.6, but is moot if your names are always str (as we learned in the",
      "content_length": 2003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1485,
      "chapter": null,
      "content": "preceding section).\nThe open default for file content is bifurcated most and badly. Contrary to\nPython’s current docs, it’s not simply the result of locale.getencoding()\non all platforms. Rather, it’s either UTF-8 if the UTF-8 mode introduced in\nPython 3.7 is enabled or else getencoding(). This matters on Windows\ntoday, where getencoding() may be a code-page encoding like cp1252;\nbecause Unix is usually UTF-8, an open sans encoding isn’t portable for\nnon-ASCII content.\nUTF-8 mode addresses this, but it must be enabled today by setting the\nenvironment variable PYTHONUTF8 to 1 or using command-line switch -X\nutf8. The result of getencoding(), added in Python 3.11, is not influenced\nby UTF-8 mode the way that open and the older getpreferred\nencod⁠ing(False) in locale are. Hence, open’s conditional default.\nBoth of the locale encoding results may also be influenced by platform,\nenvironment variables, and command-line switches…except on Android,\nwhich is just plain UTF-8, and on Windows in the future, when Python 3.15\nwill turn UTF-8 mode on by default to match other platforms for code that\nlacks explicit encodings—a good idea, though too late to the party to avoid\nmaking a scene.\nSeparately, environment variable PYTHONIOENCODING can be used to give the\nencoding of stdio streams (e.g., sys.stdout) when they are redirected to\nfiles on Windows and others (e.g., > output.txt)…except when you or a\nPython of the future enable 3.7’s UTF-8 mode, which applies to redirected\nstreams too…unless variable PYTHONIOENCODING is also set.\nBeyond all this, the encoding fate of unredirected console streams on\nWindows can be sealed with PYTHONLEGACYWINDOWSSTDIO; filename\nencodings on Windows may also backslide to their code-page roots for the\nbrazenly grandiose PYTHONLEGACYWINDOWSFSENCODING; Unix encodings can\nbe forced to skip UTF-8 similarly with PYTHONCOERCECLOCALE; and UTF-8\nmode modulates additional textual tools omitted here for space (and\nhumanity).\nRight. If you don’t care to remember all that—and prefer to sleep well at",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1486,
      "chapter": null,
      "content": "night—enable UTF-8 mode on Windows today before Python 3.15 does; use\nstr for filenames and pass explicit encodings to open instead of relying on\ntangled and morphing defaults; and see Python’s manuals for the full, if\nfrightening, story.\nThe Unicode Twilight Zone\nTo wrap up, this chapter is going to briefly present two curated topics from the\nUnicode realm—BOMs and normalization—that are too convoluted for full\ncoverage in this book and probably too arcane to matter to most Python\nnewcomers. If and when these become important to your projects, you’ll find\nample resources in both Python’s docs and the web at large to fill in the bits\nglossed over here for space. For now, let’s jump right into the first of these\narguably explosive topics.\nDropping the BOM in Python\nAs noted briefly earlier in this chapter, some encoding schemes store a special\nbyte order marker (BOM) sequence at the start of files to specify data\nendianness (which end of a string of bits is most significant to its value) or\ndeclare the encoding type in general. Python’s text-mode files both skip this\nmarker on input and write it on output if the encoding implies presence, but we\nsometimes must use a specific encoding name to force BOM processing\nexplicitly and may need to accommodate it when encoding manually.\nFor example, in the UTF-16 and UTF-32 encodings, the BOM both identifies the\nencoding and specifies big- or little-endian format. A UTF-8 text file may also\ninclude a BOM, but this isn’t guaranteed and serves only to declare that it is\nUTF-8 in general.\nWhen reading and writing data using these encoding schemes, Python\nautomatically skips or writes the BOM if it is either implied by a general\nencoding name or if you provide a more specific encoding name to force the\nissue. More concretely:\nIn UTF-16, the BOM is always processed for encoding name utf-16,",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1487,
      "chapter": null,
      "content": "and the more specific encoding name utf-16-le denotes little-endian\nformat.\nIn UTF-8, the more specific encoding name utf-8-sig forces Python\nto both skip and write a BOM on input and output, respectively, but the\ngeneral utf-8 does not.\nMaking BOMs in Text Editors\nLet’s make some files with BOMs to see how this works in practice. We’re going\nto do this in Python in a moment, too, but let’s start by creating a file in a text\neditor to underscore that your programs will also have to process content from\nother sources. If you wish to work along, you can use any text editor that’s\nUnicode aware for saves, and most editors on PCs, tablets, and phones are today.\nThis demo uses Windows Notepad just because it’s widespread (and\ndocumenting multiple editors’ usage here is right out).\nWhen you save a text file in Windows Notepad, you can specify its encoding\ntype in a drop-down list—simple text, little- or big-endian UTF-16, or UTF-8\nwith or without a BOM. For instance, if you use Notepad to save the two lines\n“code” and “CODE” in a text file named code.txt with encoding type ANSI, the\nfile is written as simple ASCII text without a BOM. Technically, the save uses\nthe cp1252 default encoding on the Windows host used, but cp1252 is an ASCII\nsuperset, and the content is all ASCII.\nIn Python after the save, when this file is read in binary mode, we can see the\nactual bytes stored in the file, including \\r in newlines. When it’s read as text,\nPython performs newline translation by default, and we can also decode it\nexplicitly as UTF-8 text since ASCII is a subset of both this encoding and\nWindows’ default cp1252:\n$ py -3                                   # On Windows, post ANSI save in Notepad\n>>> import locale\n>>> locale.getpreferredencoding(False)    # open default: ASCII&Latin-1 superset\n'cp1252'\n>>> open('code.txt', 'rb').read()         # ASCII (and cp1252 and UTF-8) bytes\nb'code\\r\\nCODE\\r\\n'\n>>> open('code.txt', 'r').read()          # Text mode also translates newlines\n'code\\nCODE\\n'\n>>> open('code.txt', 'r', encoding='utf-8').read()\n'code\\nCODE\\n'",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1488,
      "chapter": null,
      "content": "Now, if this file is instead saved as UTF-8 with BOM in Notepad (UTF in its\nprior versions), its text is prepended with a three-byte UTF-8 BOM sequence\n(which prints as non-ASCII \\xNN hex escapes in bytes in Python), and we need\nto give the more specific encoding name utf-8-sig to force Python to skip the\nmarker automatically on input (else it prints as a \\uNNNN Unicode code-point\nescapes in str):\n>>> open('code.txt', 'rb').read()       # After resaved: UTF-8 with 3-byte BOM\nb'\\xef\\xbb\\xbfcode\\r\\nCODE\\r\\n'\n>>> open('code.txt', 'r').read()        # Default+utf8 keep BOM, utf-8-sig drops\n'ï»¿code\\nCODE\\n'\n>>> open('code.txt', 'r', encoding='utf-8').read()\n'\\ufeffcode\\nCODE\\n'\n>>> open('code.txt', 'r', encoding='utf-8-sig').read()\n'code\\nCODE\\n'\nAnd if the file is stored as UTF-16 BE in Notepad (Unicode big endian\nformerly), we get UTF-16 big-endian format data in the file, with two bytes (16\nbits) prepended to record a two-byte BOM sequence. The encoding name utf-\n16 in Python skips the BOM because it is implied (since all UTF-16 files have a\nBOM), and utf-16-be handles the big-endian format but does not skip the\nBOM in the input result:\n>>> open('code.txt', 'rb').read()\nb'\\xfe\\xff\\x00c\\x00o\\x00d\\x00e\\x00\\r\\x00\\n\\x00C\\x00O\\x00D\\x00E\\x00\\r\\x00\\n'\n>>> open('code.txt', 'r', encoding='utf-16').read()\n'code\\nCODE\\n'\n>>> open('code.txt', 'r', encoding='utf-16-be').read()\n'\\ufeffcode\\nCODE\\n'\nExperiment with other save/open combinations for more insights. The default\nencoding in the last example, for example, would print garbage characters\nbecause it’s not valid for the text, and UTF-16 little-endian swaps byte order for\nthe BOM and each 2-byte character.\nNOTE",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1489,
      "chapter": null,
      "content": "Notepad flux: Notepad recently changed its encoding options for saves. Today, it offers ANSI\n(the host’s default), UTF-16 in little- and big-endian flavors, and UTF-8 with and without a\nBOM, and defaults to UTF-8. When this book’s prior edition was penned, Notepad had UTF-8\nwith an implied BOM, and its “Unicode” meant UTF-16—which, of course, is just one of the\nvery many kinds of Unicode encoding. The narrative here has been updated to reflect the new\nchoices, but this naturally is just the story today. Because this book staunchly refuses to\nbecome a Notepad doc, translate save options as needed to the Notepad on a PC near you.\nMaking BOMs in Python\nThe preceding section used Python to read files made in an editor, but the same\npatterns apply when Python also makes the files: Python will automatically add a\nBOM when we write a Unicode file in code, but we need to use a more explicit\nencoding name to force the BOM in UTF-8 mode—utf-8 does not write the\nBOM on output but utf-8-sig does, and the same goes for skipping UTF-8\nBOMs on input. When run on Windows (Unix is the same, but doesn’t add \\r to\nnewlines, and usually defaults to UTF-8):\n>>> open('temp.txt', 'w', encoding='utf-8').write('code\\nCODE\\n')\n10\n>>> open('temp.txt', 'rb').read()                         # utf-8: no BOM\nb'code\\r\\nCODE\\r\\n'\n>>> open('temp.txt', 'w', encoding='utf-8-sig').write('code\\nCODE\\n')\n10\n>>> open('temp.txt', 'rb').read()                         # utf-8-sig: adds BOM\nb'\\xef\\xbb\\xbfcode\\r\\nCODE\\r\\n'\n>>> open('temp.txt', 'r').read()                          # Default: bad BOM\n'ï»¿code\\nCODE\\n'\n>>> open('temp.txt', 'r', encoding='utf-8').read()        # utf-8: keeps BOM\n'\\ufeffcode\\nCODE\\n'\n>>> open('temp.txt', 'r', encoding='utf-8-sig').read()    # utf-8-sig: drops BOM\n'code\\nCODE\\n'\nPer this code, although utf-8 does not drop the BOM when one is present, data\nwithout a BOM can be read with both utf-8 and utf-8-sig—which means you\ncan use the latter for input if you’re not sure whether a BOM is present in a file\nor not (and don’t read this paragraph out loud in an airport security line!):\n>>> open('temp.txt', 'w').write('code\\nCODE\\n')           # Default: UTF-8 subset\n10",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1490,
      "chapter": null,
      "content": ">>> open('temp.txt', 'rb').read()                         # No BOM\nb'code\\r\\nCODE\\r\\n'\n>>> open('temp.txt', 'r').read()                          # Default\n'code\\nCODE\\n'\n>>> open('temp.txt', 'r', encoding='utf-8').read()        # Either utf-8 works\n'code\\nCODE\\n'\n>>> open('temp.txt', 'r', encoding='utf-8-sig').read()\n'code\\nCODE\\n'\nFor the encoding name utf-16, the BOM is handled automatically: on output,\ndata is written in the platform’s native endianness, and the BOM is always\nwritten; on input, data is decoded per the BOM, and the BOM is always stripped.\nThis reflects the fact that BOMs are standard and required in the UTF-16\nencoding scheme:\n>>> import sys\n>>> sys.byteorder         # Windows host's default endianness\n'little'\n>>> open('temp.txt', 'w', encoding='utf-16').write('code\\nCODE\\n')\n10\n>>> open('temp.txt', 'rb').read()\nb'\\xff\\xfec\\x00o\\x00d\\x00e\\x00\\r\\x00\\n\\x00C\\x00O\\x00D\\x00E\\x00\\r\\x00\\n\\x00'\n>>> open('temp.txt', 'r', encoding='utf-16').read()\n'code\\nCODE\\n'\nMore specific UTF-16 encoding names can specify different endianness, though\nyou may have to manually write and skip the BOM yourself in some scenarios if\nit is required or present—study the following examples for more BOM-making\ninstructions (sorry):\n>>> open('temp.txt', 'r', encoding='utf-16-le').read()\n'\\ufeffcode\\nCODE\\n'\n>>> open('temp.txt', 'w', encoding='utf-16-be').write('\\ufeffcode\\nCODE\\n')\n11\n>>> open('temp.txt', 'rb').read()\nb'\\xfe\\xff\\x00c\\x00o\\x00d\\x00e\\x00\\r\\x00\\n\\x00C\\x00O\\x00D\\x00E\\x00\\r\\x00\\n'\n>>> open('temp.txt', 'r', encoding='utf-16').read()\n'code\\nCODE\\n'\n>>> open('temp.txt', 'r', encoding='utf-16-be').read()",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1491,
      "chapter": null,
      "content": "'\\ufeffcode\\nCODE\\n'\nThe more specific UTF-16 encoding names by themselves create and work fine\nwith BOM-less files, though utf-16 requires one on input in order to determine\nbyte order:\n>>> open('temp.txt', 'w', encoding='utf-16-le').write('CODE')\n4\n>>> open('temp.txt', 'rb').read()       # Okay if BOM not present or expected\nb'C\\x00O\\x00D\\x00E\\x00'\n>>> open('temp.txt', 'r', encoding='utf-16-le').read()\n'CODE'\n>>> open('temp.txt', 'r', encoding='utf-16').read()\nUnicodeError: UTF-16 stream does not start with BOM\nExperiment with these encodings yourself, or see Python’s library manuals for\nmore details on the BOM. And if you really want to drop the bomb in Python\n(and stretch this section’s silly bit to its breaking point), Unicode emoji\ncharacters do the job:\n>>> open('boms.txt', 'w', encoding='utf-8-sig').write('\n' * 10)\n10\n>>> open('boms.txt', encoding='utf-8-sig').read()\n'\n'\nUnicode Normalization: Whither Standard?\nLast but not least, after devoting dozens of pages to Unicode, we’ll close by\nexplaining one way in which it, at least arguably, falls short. In brief, this\nstandard failed to standardize code points for a handful of characters: it allows\nthe same character to be represented in more than one way, which breaks\nequality testing and can wreak interoperability havoc with text-processing\nprograms. While there may have been valid rationales for this policy, it adds a\nspecial case for programmers and undoubtedly breaks many a tool and app.\nFor example, the character ñ (an n augmented with a tilde, commonly used in\nSpanish) can be represented with two different code-point sequences in the\nUnicode standard:",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1492,
      "chapter": null,
      "content": "As a single character with code point \\u00F1—an augmented n\nAs a two-codepoint sequence \\u006E and \\u0303—a naked n followed\nby a combining tilde\nThe first of these is called the composed form, known as NFC, because it\ncombines letter and accent. The second is called the decomposed form, known as\nNFD, because its parts are split. Despite their glaring difference, the Unicode\nstandard mandates that these two forms represent the same character ñ and must\nbe treated as such by programs.\nIt’s easy to observe these alter egos in Python: the following uses \\u… Unicode\ncode-point escapes to specify the code points for ñ in both forms (Windows\nusers: don’t be alarmed if some characters, especially NFDs, render differently\nin command-line interfaces due to settings that are well beyond this book’s\nscope):\n>>> L = '\\u00F1'            # NFC form ('\\xF1' works too)\n>>> M = '\\u006E\\u0303'      # NFD form \n>>> L, M                    # The same character\n('ñ', 'ñ')\nNor is this character alone in its split personality. Other characters such as Å, è,\né, and \n have both NFC and NFD representations as well:\n>>> '\\u00C5', '\\u0041\\u030A'      # NFD (1 code point), NFC (2 code points)\n('Å', 'Å')\n>>> '\\u00E8', '\\u0065\\u0300'      # But it's the same character for both\n('è', 'è')\n>>> '\\u00E9', '\\u0065\\u0301'\n('é', 'é')\n>>> '\\u03D4', '\\u03D2\\u0308'\n('\n', '\n̈')\nImportantly, this is not about the encoded representation of this character in\nbytes, which naturally varies across different encodings. The two alternative\nrepresentations for each preceding character differ for their decoded, in-memory\nrepresentation as integer code points. Encoded forms (e.g., stored in files) differ,\ntoo, but decode to the same differing code points:",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1493,
      "chapter": null,
      "content": ">>> '\\u00F1'.encode('utf8'), '\\u006E\\u0303'.encode('utf8')\n(b'\\xc3\\xb1', b'n\\xcc\\x83')\n>>> b'\\xc3\\xb1'.decode('utf8'), b'n\\xcc\\x83'.decode('utf8')\n('ñ', 'ñ')\nThe unfortunate consequence of this plurality is that it breaks equality testing:\nbecause the same character can be represented in multiple ways, it’s impossible\nto compare with the usual tools. In Python specifically, the == equal-by-value\noperator, which compares characters (really, code points), won’t suffice in the\npresence of Unicode doppelgängers:\n>>> L = '\\u00F1'\n>>> M = '\\u006E\\u0303'\n>>> L, M                       # The same character\n('ñ', 'ñ')\n \n>>> L == M                     # But equality says no!\nFalse\n \n>>> len(L), len(M)             # Because their code points differ\n(1, 2)\nAll of which might not be a problem in an ideal computing world that settled on\none form as a de facto standard for portability’s sake. Unfortunately, that’s not\nthe world we occupy. Computer vendors, being computer vendors, have opted to\nfavor different forms: broadly speaking, macOS prefers NFD, Windows and\nothers prefer NFC, this can also vary by filesystem, and tolerance of nonnative\nforms is less than complete. The net effect is that the Unicode standard created\nan interoperability problem while trying to fix another!\nSo what to do with a world that just won’t standardize? The trick is that, for\nevery tool that processes file contents or names across divergent platforms, text\nmust be converted to a common form before comparisons. And luckily, Python\nmakes this remarkably easy:\n>>> from unicodedata import normalize             # Python stdlib tool\n>>> L = '\\u00F1'\n>>> M = '\\u006E\\u0303' \n>>> L, M                                          # Same char, diff code points\n('ñ', 'ñ')",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1494,
      "chapter": null,
      "content": ">>> L == M                                        # Equality fails\nFalse\n \n>>> normalize('NFC', L) == normalize('NFC', M)    # Common-form equality works\nTrue\n>>> normalize('NFD', L) == normalize('NFD', M)    # Either suffices, if the same\nTrue\nPrograms that compare filenames sent between platforms, for example, should\nbe careful to normalize this way. Otherwise, files whose names look the same to\nusers (and really are the same per Unicode) will fail to match by simple equality\nand derail searches and syncs. The same goes for text files obtained from\narbitrary sources—like most internet content.\nThere’s more to this story (e.g., the canonical equivalence of NFC and NFD\nnormalized forms is still not the same as compatibility, though most programs\ndon’t need to care). Because we’ve run tight on space, though, we’ll stop short.\nIf you’d like to dig deeper, you’ll find ample follow-up coverage both on the\nweb and in Python’s manual set (see the latter’s Unicode HOWTO and its\ncoverage of the related string casefold method).\nAt the least, though, you now shouldn’t be wholly surprised when your text-\nprocessing code is bitten by this curious choice of the Unicode standard.",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1495,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter explored the advanced string support available in Python for\nprocessing Unicode text and binary data. While some programmers use ASCII\ntext and may get by with the basic string type and its operations, Python’s string\nmodel fully supports both richer Unicode text via the normal str string type and\nbyte-oriented binary data with bytes and bytearray.\nIn addition, we learned how Python’s file object automatically encodes and\ndecodes Unicode text, and deals with byte strings for binary-mode files. Finally,\nwe briefly met some text and binary tools in Python’s library and sampled their\nbehavior with strings, and took a look at the darker Unicode corners of BOMs\nand normalization.\nIn the next chapter, we’ll shift our focus to tool-builder topics, with a survey of\nways to manage access to object attributes by inserting automatically run code.\nBefore we move on, though, here’s a set of questions to review what we’ve\nlearned. This has been a substantial chapter, so be sure to read the quiz answers\neventually for a more in-depth summary.\nTest Your Knowledge: Quiz\n1. What are the names and roles of string object types in Python?\n2. How do Python’s string types differ in terms of operations?\n3. How can you code non-ASCII Unicode characters in a string in Python?\n4. What are the main differences between text- and binary-mode files in\nPython?\n5. How would you read a Unicode text file that contains text in a different\nencoding than the default for your platform?\n6. How can you create a Unicode text file in a specific encoding format?\n7. Why is ASCII text considered to be a kind of Unicode text?",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1496,
      "chapter": null,
      "content": "8. What do BOM and normalization mean in Unicode?\n9. How large an impact does Python’s text/binary string dichotomy have\non your code?\nTest Your Knowledge: Answers\n1. Python has three string types: str (for Unicode text, including ASCII),\nbytes (for binary data with absolute byte values), and bytearray (a\nmutable flavor of bytes). The str type usually represents content\nstored in text files, and the other two types generally represent content\nstored in binary files (including encoded text).\n2. Python’s string types share almost all the same operations: method calls,\nsequence operations, and even larger tools like pattern matching work\nthe same way. On the other hand, only str supports string formatting’s\nformat method and f'…' f-strings, and bytearray has an additional set\nof operations that perform in-place changes. The str and bytes types\nalso have methods for encoding and decoding text, respectively.\n3. Non-ASCII Unicode characters can be coded in a str string with both\nhex (\\xNN) and Unicode (\\uNNNN, \\UNNNNNNNN) escapes for code points,\nboth of which denote character code points. They can also be coded in\ntheir encoded form as bytes using hex escapes and decoded to text. On\nmost devices, non-ASCII characters—accented characters and emojis,\nfor example—can also be typed or pasted directly into code and are\ninterpreted per the UTF-8 default or an encoding-directive comment at\nthe top of a source code file.\n4. Text-mode files assume their content is Unicode text (even if it’s all\nASCII) and automatically decode when reading and encode when\nwriting. With binary-mode files, bytes are transferred to and from the\nfile unchanged. The contents of text-mode files are usually represented\nas str objects in your script, and the contents of binary files are\nrepresented as bytes (or bytearray) objects. Text-mode files also\nhandle BOMs for certain encoding types and automatically translate",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1497,
      "chapter": null,
      "content": "newline sequences to and from the single \\n character on input and\noutput unless this is explicitly disabled; binary-mode files do not\nperform either of these steps.\n5. To read files encoded in a different encoding than the default for your\nplatform, simply pass the name of the file’s encoding to the open built-\nin function; data will be decoded per the specified encoding when it is\nread from the file, and you’ll get back a decoded Unicode-text str\nstring that has no encoding. You can also read in binary mode and\nmanually decode the bytes to a string by giving an encoding name, but\nthis involves extra work and is somewhat error-prone for multibyte\ncharacters (you may accidentally read a partial character sequence when\nloading content by bytes or chunks).\n6. To create a Unicode text file in a specific encoding format, pass the\ndesired encoding name to open; strings will be encoded per the desired\nencoding when they are written to the file. You can also manually\nencode a string to bytes and write it in binary mode, but this is usually\nextra work.\n7. ASCII text is considered to be a kind of Unicode text because its 7-bit\n(0..127) range of values is a subset of many Unicode encodings. For\nexample, valid ASCII text is also valid Latin-1 text (Latin-1 simply\nassigns the remaining possible values in an 8-bit byte to additional\ncharacters) and valid UTF-8 text (UTF-8 uses a variable-byte scheme\nfor representing more characters, but ASCII characters are still\nrepresented with the same values in a single byte). This makes Unicode\nbackward compatible with ASCII, as long as the encodings used\nrepresent ASCII the same way.\n8. The Unicode BOM is a sequence of bytes added to the front of a text\nstring or file in some encodings to both identify the encoding and give\nthe string’s endianness. Unicode normalization converts characters to a\ncommon format to neutralize differences that arise for characters that\nhave multiple code-point values in the Unicode standard and would fail\nto match by simple code-point (character) equality.\n9. The impact of Python’s strings model depends upon the types of strings",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1498,
      "chapter": null,
      "content": "you use. For scripts that use simple ASCII text on platforms with\nASCII-compatible default encodings, the impact is probably minor: the\nstr string sans encodings suffices in this case. Moreover, although\nstring-related tools in the standard library, such as re, struct, pickle,\nand os, may technically use different types in different contexts, the\neffect is largely irrelevant to most programs because Python’s str and\nbytes support almost identical interfaces. On the other hand, if you\nprocess non-ASCII Unicode text, you’ll need to use str and pass\nencodings to open; if you deal with binary data files, you’ll need to deal\nwith content as bytes objects; and if your code must work across many\nplatforms or process content from arbitrary sources, you’ll want to use\nexplicit encodings for text files. In general, Unicode is the way the text\nworld works today and will be a must-know tool for most Python users.\n1  File fine points: most of this chapter’s file examples use filenames relative to, and hence stored in,\nthe current directory, so be sure to run them in a directory (a.k.a. folder) where you have permission\nto create files; cd to one in your shell or IDE if needed. This may matter on platforms with major\nstorage constraints like Android, but you’ll probably already be in a safe folder in most apps. Also,\nfile extensions (e.g., .txt) don’t mean anything to Python and are technically optional; some IDEs\nthat hang on to objects for debugging may require manual close calls to flush changes too; and\npurists take note that file is no longer a built-in name in Python and OK to use as a variable here!",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1499,
      "chapter": null,
      "content": "Chapter 38. Managed Attributes\nThis chapter expands on the attribute interception techniques introduced earlier,\nintroduces another, and employs them in a handful of larger examples. Like\neverything in this part of the book, this chapter is classified as an advanced topic\nand optional reading, because most applications programmers don’t need to care\nabout the material discussed here—they can fetch and set attributes on objects\nwithout concern for attribute implementations.\nEspecially for tools builders, though, managing attribute access can be an\nimportant part of flexible APIs. Moreover, an understanding of the descriptor\nmodel covered here can make related tools such as slots and properties more\ntangible and may even be required reading if it appears in code you must use.\nWhy Manage Attributes?\nObject attributes are central to most Python programs—they are where we often\nstore information about the entities our scripts process. Normally, attributes are\nsimply names for objects; a person’s name attribute, for example, might be a\nsimple string, fetched and set with basic attribute syntax:\nperson.name                 # Fetch attribute value\nperson.name = value         # Change attribute value\nIn most cases, the attribute lives in the object itself or is inherited from a class\nfrom which it derives. That basic model suffices for most programs you will\nwrite in your Python career.\nSometimes, though, more flexibility is required. Suppose you’ve written a\nprogram to use a name attribute directly, but then your requirements change—for\nexample, you decide that names must be validated or mutated with program\nlogic when accessed. It’s straightforward to code methods to manage access to\nthe attribute’s value (valid and transform are abstract and hypothetical here):\nclass Person:",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1500,
      "chapter": null,
      "content": "def getName(self):\n        if not valid():\n            raise TypeError('cannot fetch name')\n        else:\n            return self.name.transform()\n    def setName(self, value):\n         if not valid(value):\n            raise TypeError('cannot change name')\n        else:\n            self.name = transform(value)\nperson = Person()\nperson.getName()\nperson.setName('value')\nThe problem with this is that it also requires changing all the places where\nnames are used in the entire program—a possibly nontrivial task. Moreover, this\napproach requires the program to be aware of how values are exported: as simple\nnames or called methods. If you begin with a method-based interface to data,\nclients are immune to changes; if you do not, changes can become problematic.\nThis issue can crop up more often than you might expect. The value of a cell in a\nspreadsheet-like program, for instance, might begin its life as a simple discrete\nvalue but later mutate into an arbitrary calculation. Since an object’s interface\nshould be flexible enough to support such future changes without breaking\nexisting code, switching to methods later is less than ideal.\nInserting Code to Run on Attribute Access\nA better solution would allow you to run code automatically on attribute access\nif needed. That’s one of the main roles of managed attributes—they provide\nways to add attribute accessor logic after the fact. More generally, they support\narbitrary attribute usage modes that go beyond simple data storage.\nAt various points in this book, we’ve met Python tools that allow our scripts to\ndynamically compute attribute values when fetching them and validate or change\nattribute values when storing them. In this chapter, we’re going to focus more\ndeeply on the tools already introduced, explore other tools in this category, and\nstudy some larger use-case examples in this domain. Specifically, this chapter\npresents four accessor techniques:",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1501,
      "chapter": null,
      "content": "1. The property built-in, for specifying methods to handle access to a\nspecific attribute\n2. The __get__ and __set__ descriptor methods, for handling access to a\nspecific attribute and the basis for other tools such as properties and\nslots\n3. The __getattr__ and __setattr__ methods, for handling undefined\nattribute fetches and all attribute assignments\n4. The __getattribute__ method, for handling all attribute fetches\nWe met these tools in Chapters 30 and 32 briefly, and in some cases, hardly at\nall. As you’ll see here, all four techniques share goals to some degree, and it’s\nusually possible to code a given problem using any one of them.\nThat said, they also differ in some important ways. For example, the last two\ntechniques listed here apply to specific attributes, whereas the first two are\ngeneric enough to be used by delegation-based proxy classes that must route\narbitrary attributes to wrapped objects. As you’ll find, all four schemes also\ndiffer in both complexity and aesthetics in ways you must see in action to judge\nfor yourself.\nBesides studying the specifics behind these four attribute interception\ntechniques, this chapter also presents an opportunity to explore programs larger\nthan most we’ve seen elsewhere in this book. The CardHolder case study at the\nend, for example, should serve as a self-study example of larger classes in\naction. We’ll also be using some of the techniques outlined here in the next\nchapter to code decorators, so be sure you have at least a general understanding\nof these topics before you move on.\nProperties\nUp first, the property protocol allows us to route a specific attribute’s get, set,\nand delete operations to functions or methods we provide, enabling us to insert\ncode to be run automatically on attribute accesses, intercept attribute deletions,\nand provide documentation for attributes if desired.",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1502,
      "chapter": null,
      "content": "As introduced in Chapter 32, properties are created with the property built-in\nand are assigned to class attributes, just like method functions. Accordingly, they\nare inherited by subclasses and instances, like any other class attributes. Their\naccess-interception functions are provided with the self instance argument,\nwhich grants access to state information and class attributes available on the\nsubject instance.\nA property manages a single, specific attribute; although it can’t catch all\nattribute accesses generically, it allows us to control both fetch and assignment\naccesses and enables us to change an attribute from simple data to a computation\nfreely without breaking existing code. As you’ll see, properties are strongly\nrelated to descriptors; in fact, they are essentially a restricted form of them.\nThe Basics\nA property is created by assigning the result of a built-in function to a class\nattribute:\nattribute = property(fget, fset, fdel, doc)\nNone of this built-in’s arguments are required, and all default to None if not\npassed. For the first three, this None means that the corresponding operation is\nnot supported, and attempting it will raise an AttributeError exception\nautomatically.\nWhen these arguments are used, we pass fget a function for intercepting\nattribute fetches, fset a function for assignments, and fdel a function for\nattribute deletions. Technically, all three of these arguments accept any callable,\nincluding a class’s method, having a first argument to receive the instance being\nqualified. When later invoked, the fget function returns the computed attribute\nvalue, fset and fdel return nothing (really, None), and all three may raise\nexceptions to reject access requests.\nThe doc argument receives a documentation string for the attribute if desired. If\nomitted, the property copies the docstring of the fget function, which, as usual,\ndefaults to None.",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1503,
      "chapter": null,
      "content": "This built-in property call returns a property object, which we assign to the\nname of the attribute to be managed in the class scope, where it will be inherited\nby every instance. As you’ll learn ahead, this assignment can be automated by @\ndecorator syntax, though its distributed usage may seem awkward for set and\ndelete methods. However assigned, later accesses to the attribute automatically\ninvoke the property’s handlers.\nA First Example\nTo demonstrate how this translates to working code, the class in Example 38-1\nuses a property to trace access to an attribute named name; the actual stored data\nis named _name so it does not clash with the property.\nExample 38-1. prop-person.py\nclass Person: \n   def __init__(self, name):\n       self._name = name\n   def getName(self):\n       print('fetch...')\n       return self._name\n   def setName(self, value):\n       print('change...')\n       self._name = value\n   def delName(self):\n       print('remove...')\n       del self._name\n   name = property(getName, setName, delName, 'name property docs')\nsue = Person('Sue Jones')           # sue has a managed attribute\nprint(sue.name)                     # Runs getName\nsue.name = 'Susan Jones'            # Runs setName\nprint(sue.name)\ndel sue.name                        # Runs delName\nprint('-'*20)\nbob = Person('Bob Smith')           # bob inherits property too\nprint(bob.name)\nprint(Person.name.__doc__)          # Or help(Person.name)\nThis particular property doesn’t do much—it simply intercepts and traces an",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1504,
      "chapter": null,
      "content": "attribute—but it serves to demonstrate the protocol. When this code is run, two\ninstances inherit the property, just as they would any other attribute attached to\ntheir class. However, accesses to their name attribute are caught and managed by\nthe code we provide:\n$ python3 prop-person.py\nfetch...\nSue Jones\nchange...\nfetch...\nSusan Jones\nremove...\n--------------------\nfetch...\nBob Smith\nname property docs\nLike all class attributes, properties are inherited by both instances and lower\nsubclasses. If we change our example as follows, for instance:\nclass Super:\n    …the original Person class code…\n    name = property(getName, setName, delName, 'name property docs')\nclass Person(Super):\n    pass                            # Properties are inherited (class attrs)\nsue = Person('Sue Jones')\n…rest unchanged…\nthe output is the same—the Person subclass inherits the name property from\nSuper, and the sue instance gets it from Person. In terms of inheritance,\nproperties work the same as normal methods; because they have access to the\nself instance argument, they can access instance state information and methods\nirrespective of subclass depth, as the next section further demonstrates.\nComputed Attributes\nThe example in the prior section simply traces attribute accesses. Usually,\nthough, properties do much more—computing the value of an attribute\ndynamically when fetched, for instance, as Example 38-2 illustrates.",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1505,
      "chapter": null,
      "content": "Example 38-2. prop-computed.py\nclass PropSquare:\n   def __init__(self, start):\n       self.value = start\n   def getX(self):                         # On attr fetch\n       return self.value ** 2\n   def setX(self, value):                  # On attr assign\n       self.value = value\n   X = property(getX, setX)                # No delete or docs\nP = PropSquare(3)       # Two instances of class with property\nQ = PropSquare(32)      # Each has different state information\nprint(P.X)              # 3 ** 2\nP.X = 4\nprint(P.X)              # 4 ** 2\nprint(Q.X)              # 32 ** 2 (1024)\nThis class defines an attribute X that is accessed as though it were simple data,\nbut really runs code to compute its value when fetched. The net effect triggers an\nimplicit method call. When the code is run, the value is stored in the instance as\nstate information, but each time we fetch it via the managed attribute, its value is\nautomatically squared:\n$ python3 prop-computed.py\n9\n16\n1024\nNotice that we’ve made two different instances—because property methods\nautomatically receive a self argument, they have access to the state information\nstored in instances. In our case, this means the fetch computes the square of the\nsubject instance’s own data.\nCoding Properties with Decorators\nAlthough we’re saving additional details until the next chapter, we introduced\nfunction decorator basics earlier, in Chapter 32. Recall that the function\ndecorator syntax:",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1506,
      "chapter": null,
      "content": "@decorator\ndef func(args): …\nis automatically translated to this equivalent by Python to rebind the function\nname to the result of the decorator callable:\ndef func(args): …\nfunc = decorator(func)\nBecause of this mapping, the property built-in can automatically serve as a\ndecorator to define a function that will run automatically when an attribute is\nfetched:\nclass Person:\n    @property\n    def name(self): ...             # Rebinds: name = property(name)\nWhen run, the decorated method is automatically passed to the first argument of\nthe property built-in. This is really just alternative syntax for creating a\nproperty and rebinding the attribute name manually, but may be seen as more\nexplicit in this role:\nclass Person:\n    def name(self): …\n    name = property(name)           # Manual equivalent to @property\nSetter and deleter decorators\nThe preceding works naturally for property get functions, but what about other\naccesses? In full detail, property objects also have getter, setter, and deleter\nmethods that assign the corresponding property accessor methods and return a\ncopy of the property itself. We can use these to specify components of properties\nby decorating normal methods, too, though the getter component (along with\nattributes docs) is usually filled in automatically by the act of creating the\nproperty itself. Example 38-3 demos the basics.\nExample 38-3. prop-person-deco.py\nclass Person:\n   def __init__(self, name):\n       self._name = name",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1507,
      "chapter": null,
      "content": "@property\n   def name(self):                 # name = property(name)\n       'name property docs'\n       print('fetch...')\n       return self._name\n   @name.setter\n   def name(self, value):          # name = name.setter(name)\n       print('change...')\n       self._name = value\n   @name.deleter\n   def name(self):                 # name = name.deleter(name)\n       print('remove...')\n       del self._name\nsue = Person('Sue Jones')           # sue has a managed attribute\nprint(sue.name)                     # Runs name getter (def name 1)\nsue.name = 'Susan Jones'            # Runs name setter (def name 2)\nprint(sue.name)\ndel sue.name                        # Runs name deleter (def name 3)\nprint('-'*20)\nbob = Person('Bob Smith')           # bob inherits property too\nprint(bob.name)\nprint(Person.name.__doc__)          # Or help(Person.name)\nIn fact, this code is equivalent to the first example in this section—decoration is\njust an alternative way to code properties in this case. When it’s run, the results\nare the same:\n$ python3 prop-person-deco.py\nfetch...\nSue Jones\nchange...\nfetch...\nSusan Jones\nremove...\n--------------------\nfetch...\nBob Smith\nname property docs\nCompared to manual assignment of property results, using decorators to\nproperties in this example requires just three extra lines of code—a seemingly",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1508,
      "chapter": null,
      "content": "negligible difference. As is so often the case with alternative tools, though, the\nchoice between the two techniques is largely subjective.\nDescriptors\nVery briefly previewed in Chapter 32, descriptors provide an alternative way to\nintercept attribute access; they are strongly related to the properties discussed in\nthe prior section. Really, a property is a kind of descriptor—technically\nspeaking, the property built-in is just a simplified way to create a specific type\nof descriptor that runs method functions on attribute accesses. In fact, descriptors\nare the underlying implementation mechanism for a variety of class tools,\nincluding both properties and slots, and play other internal roles in Python that\nwe can safely skip here.\nFunctionally speaking, the descriptor protocol allows us to route a specific\nattribute’s get, set, and delete operations to methods of a separate class’s instance\nobject that we provide. This allows us to insert code to be run automatically on\nattribute fetches and assignments, intercept attribute deletions, and provide\ndocumentation for the attributes if desired.\nDescriptors are created as independent classes, and they are assigned to class\nattributes just like method functions. Like any other class attribute, they are\ninherited by subclasses and instances. Their access-interception methods are\nprovided with both a self for the descriptor instance itself as well as the\ninstance of the client class whose attribute references the descriptor object.\nBecause of this, they can retain and use state information of their own, as well as\nstate information of the subject instance. For example, a descriptor may call\nmethods available in the client class, as well as descriptor-specific methods it\ndefines.\nLike a property, a descriptor manages a single, specific attribute; although it\ncan’t catch all attribute accesses generically, it provides control over both fetch\nand assignment accesses and allows us to change an attribute name freely from\nsimple data to a computation without breaking existing code. If this sounds like\nproperties, it’s because it is: as you shall see, properties can be coded as\ndescriptors directly.",
      "content_length": 2171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1509,
      "chapter": null,
      "content": "Unlike properties, though, descriptors provide a more general tool. For instance,\nbecause they are coded as normal classes, descriptors have their own state, may\nparticipate in descriptor inheritance hierarchies, can use composition to\naggregate objects, and provide a natural structure for coding internal methods\nand attribute documentation strings.\nThe Basics\nAs mentioned, descriptors are coded as separate classes and provide specially\nnamed accessor methods for the attribute access operations they wish to\nintercept—get, set, and deletion methods in the descriptor class are automatically\nrun when the attribute assigned to the descriptor class instance is accessed in the\ncorresponding way:\nclass Descriptor:\n    \"docstring goes here\"\n    def __get__(self, instance, owner): …        # Return attr value\n    def __set__(self, instance, value): …        # Return nothing (None)\n    def __delete__(self, instance): …            # Return nothing (None)\nClasses with any of these methods are considered descriptors, and their methods\nare special when one of their instances is assigned to another class’s attribute—\nwhen the attribute is accessed, these methods are automatically invoked.\nIf any of these methods are absent, it generally means that the corresponding\ntype of access is not supported. Unlike properties, however, omitting a __set__\nallows the descriptor attribute’s name to be assigned and thus redefined in an\ninstance, thereby hiding the descriptor—to make an attribute read-only, you\nmust define __set__ to catch assignments and raise an exception.\nDescriptors with __set__ methods also have some special-case implications for\ninheritance that we’ll largely defer until Chapter 40’s coverage of metaclasses\nand the complete inheritance specification. In short, a descriptor with a __set__\nis known formally as a data descriptor and is given precedence over other names\nlocated by normal inheritance rules. The inherited descriptor for attribute\n__class__, for example, overrides the same name in an instance’s namespace\ndictionary. This also works to ensure that data descriptors you code in your own\nclasses take precedence over others.",
      "content_length": 2159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1510,
      "chapter": null,
      "content": "Descriptor method arguments\nBefore we code anything realistic, let’s take a brief look at some fundamentals.\nAll three descriptor methods outlined in the prior section are passed both the\ndescriptor class instance (self) and the instance of the client class to which the\ndescriptor instance is attached (instance).\nThe __get__ access method additionally receives an owner argument,\nspecifying the class to which the descriptor instance is attached. Its instance\nargument is either the instance through which the attribute was accessed (for\ninstance.attr), or None when the attribute is accessed through the owner class\ndirectly (for class.attr). The former of these generally computes a value for\ninstance access, and the latter usually returns self if descriptor object access is\nsupported.\nFor example, in the following REPL session, when X.attr is fetched, Python\nautomatically runs the __get__ method of the Descriptor class instance to\nwhich the Subject.attr class attribute is assigned:\n>>> class Descriptor:                  \n        def __get__(self, instance, owner):\n            print(self, instance, owner, sep='\\n')\n>>> class Subject:                           \n        attr = Descriptor()            # Descriptor instance is class attr\n>>> X = Subject()\n>>> X.attr\n<__main__.Descriptor object at 0x104bc9b20>\n<__main__.Subject object at 0x104b8a570>\n<class '__main__.Subject'> \n>>> Subject.attr\n<__main__.Descriptor object at 0x104bc9b20>\nNone\n<class '__main__.Subject'>\nNotice the arguments automatically passed in to the __get__ method in the first\nattribute fetch—when X.attr is fetched, it’s as though the following translation\noccurs (though the Subject.attr here doesn’t invoke __get__ again as it\nnormally would):",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1511,
      "chapter": null,
      "content": "X.attr  =>  Descriptor.__get__(Subject.attr, X, Subject)\nThe descriptor knows it is being accessed directly when its instance argument is\nNone.\nRead-only descriptors\nAs mentioned earlier, unlike properties, simply omitting the __set__ method in\na descriptor isn’t enough to make an attribute read-only because the descriptor\nname can be assigned in an instance. In the following, the attribute assignment to\nX.a stores a in the instance object X, thereby hiding the descriptor stored in class\nC:\n>>> class D:\n        def __get__(*args): print('get')\n>>> class C:\n        a = D()                     # Attribute \"a\" is a descriptor instance\n>>> X = C()\n>>> X.a                             # Runs inherited descriptor __get__\nget\n>>> C.a\nget\n>>> X.a = 99                        # Stored on X, hiding C.a!\n>>> X.a\n99\n>>> list(X.__dict__.keys())\n['a']\n>>> Y = C()\n>>> Y.a                             # Y still inherits descriptor\nget\n>>> C.a\nget\nThis is the way all instance attribute assignments work in Python, and it allows\nclasses to selectively override class-level defaults in their instances. To make a\ndescriptor-based attribute read-only, catch the assignment in the descriptor class\nand raise an exception to prevent attribute assignment—when assigning an\nattribute that is a descriptor, Python effectively bypasses the normal instance-\nlevel assignment behavior and routes the operation to the descriptor object:",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1512,
      "chapter": null,
      "content": ">>> class D:\n        def __get__(*args): print('get')\n        def __set__(*args): raise AttributeError('cannot set')\n>>> class C:\n        a = D()\n>>> X = C()\n>>> X.a                                 # Routed to C.a.__get__\nget\n>>> X.a = 99                            # Routed to C.a.__set__\nAttributeError: cannot set\nNOTE\nThe deletion trio: Be careful not to confuse the descriptor __delete__ method with the\ngeneral __del__ method. The former is called on attempts to delete the managed attribute\nname on an instance of the owner class; the latter is the general instance destructor method, run\nwhen an instance of any kind of class is about to be garbage-collected. Descriptor __delete__\nis more closely related to the __delattr__ generic attribute deletion method we’ll study later\nin this chapter. See Chapter 30 for more on operator-overloading methods like __del__.\nA First Example\nTo see how this all comes together in more realistic code, let’s get started with\nthe same first example we wrote for properties. Example 38-4 defines a\ndescriptor that intercepts access to an attribute named name in its clients. Its\nmethods use their instance argument to access state information in the subject\ninstance, where the name string is actually stored.\nExample 38-4. desc-person.py\nclass Name:\n   'name descriptor docs'\n   def __get__(self, instance, owner):\n       print('fetch...')\n       return instance._name\n   def __set__(self, instance, value):\n       print('change...')\n       instance._name = value\n   def __delete__(self, instance):\n       print('remove...')",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1513,
      "chapter": null,
      "content": "del instance._name\nclass Person:\n   def __init__(self, name):\n       self._name = name\n   name = Name()                       # Assign descriptor to attr\nsue = Person('Sue Jones')               # sue has a managed attribute\nprint(sue.name)                         # Runs Name.__get__\nsue.name = 'Susan Jones'                # Runs Name.__set__\nprint(sue.name)\ndel sue.name                            # Runs Name.__delete__\nprint('-'*20)\nbob = Person('Bob Smith')               # bob inherits descriptor too\nprint(bob.name)\nprint(Name.__doc__)                     # Or help(Name)\nNotice in this code how we assign an instance of our descriptor class to a class\nattribute in the client class; because of this, it is inherited by all instances of the\nclass, just like a class’s methods. Really, we must assign the descriptor to a class\nattribute like this—it won’t work if assigned to a self instance attribute instead.\nWhen the descriptor’s __get__ method is run, it is passed three objects to define\nits context:\nself is the Name class instance.\ninstance is the Person class instance.\nowner is the Person class.\nWhen this code is run, the descriptor’s methods intercept accesses to the\nattribute, much like the property version. In fact, the output is the same again:\n$ python3 desc-person.py\nfetch...\nSue Jones\nchange...\nfetch...\nSusan Jones\nremove...\n--------------------\nfetch...\nBob Smith",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1514,
      "chapter": null,
      "content": "name descriptor docs\nAlso like in the property example, our descriptor class instance is a class\nattribute and thus is inherited by all instances of the client class and any\nsubclasses. If we change the Person class in our example to the following, for\ninstance, the output of our script is the same:\n…\nclass Super:\n    def __init__(self, name):\n        self._name = name\n    name = Name()\nclass Person(Super):                     # Descriptors are inherited (class attrs)\n   pass\n…\nAlso, note that when a descriptor class is not useful outside the client class, it’s\nperfectly reasonable to embed the descriptor’s definition inside its client\nsyntactically. Here’s what our example looks like if we use a nested class:\nclass Person:\n    def __init__(self, name):\n        self._name = name\n    class Name:                                 # Using a nested class\n        'name descriptor docs'\n        def __get__(self, instance, owner):\n            …same…\n        def __set__(self, instance, value):\n            …same…\n        def __delete__(self, instance):\n            …same…\n    name = Name()\nWhen coded this way, Name becomes a local variable in the scope of the Person\nclass statement, such that it won’t clash with any names outside the class. This\nversion works the same as the original—we’ve simply moved the descriptor",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1515,
      "chapter": null,
      "content": "class definition into the client class’s scope—but the last line of the testing code\nmust change to fetch the docstring from its new location (per unlisted file desc-\nperson-nested.py in the example’s package):\n…\nprint(Person.Name.__doc__)     # Differs: not Name.__doc__ outside class\nComputed Attributes\nAs was the case when using properties, our first descriptor example of the prior\nsection didn’t do much—it simply printed trace messages for attribute accesses\nas a demo. In practice, descriptors can also be used to compute attribute values\neach time they are fetched. Example 38-5 illustrates—it’s a rehash of the same\nexample we coded for properties but uses a descriptor to automatically square an\nattribute’s value each time it is fetched.\nExample 38-5. desc-computed.py\nclass DescSquare:\n   def __init__(self, start):                  # Each desc has own state\n       self.value = start\n   def __get__(self, instance, owner):         # On attr fetch\n       return self.value ** 2\n   def __set__(self, instance, value):         # On attr assign\n       self.value = value                      # No delete or docs\nclass Client1:\n   X = DescSquare(3)          # Assign descriptor instance to class attr\nclass Client2:\n   X = DescSquare(32)         # Another instance in another client class\n                              # Could also code two instances in same class\nc1 = Client1()\nc2 = Client2()\nprint(c1.X)                    # 3 ** 2\nc1.X = 4\nprint(c1.X)                    # 4 ** 2\nprint(c2.X)                    # 32 ** 2 (1024)\nWhen run, the output of this example is the same as that of the original property-\nbased version, but here a descriptor class object is intercepting the attribute\naccesses instead of a property:",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1516,
      "chapter": null,
      "content": "$ python3 desc-computed.py\n9\n16\n1024\nUsing State Information in Descriptors\nIf you closely study the two descriptor examples we’ve written so far, you might\nnotice that they get their information from different places—the first (the name\nattribute example) uses data stored on the client instance, and the second (the\nattribute squaring example) uses data attached to the descriptor object itself\n(a.k.a. self). In fact, descriptors can use both instance state and descriptor state,\nor any combination thereof:\nDescriptor state is used to manage either data internal to the workings\nof the descriptor or data that spans all instances. It can vary per attribute\nappearance (often per client class).\nInstance state records information related to and possibly created by the\nclient class. It can vary per client-class instance (that is, per application\nobject).\nIn other words, descriptor state is per-descriptor data, and instance state is per-\nclient-instance data. As usual in OOP, you must choose state carefully. For\nexample, you would not normally use descriptor state to record employee names\nsince each client instance requires its own value—if stored in the descriptor,\neach client class instance will effectively share the same single copy. On the\nother hand, you would not usually use instance state to record data pertaining to\ndescriptor implementation internals—if stored in each instance, there would be\nmultiple varying copies.\nDescriptor methods may use either state form, but descriptor state sometimes\nmakes it unnecessary to use special naming conventions to avoid name collisions\nin the instance for data that is not instance specific. For example, the descriptor\nin Example 38-6 attaches information to its own instance, so it doesn’t clash\nwith that on the client class’s instance—but also shares that information between\ntwo client instances.\nExample 38-6. desc-state-desc.py",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1517,
      "chapter": null,
      "content": "class DescState:                           # Use descriptor state\n   def __init__(self, value):\n       self.value = value\n   def __get__(self, instance, owner):    # On attr fetch\n       print('DescState get')\n       return self.value * 10\n   def __set__(self, instance, value):    # On attr assign\n       print('DescState set')\n       self.value = value\n# Client class\nclass CalcAttrs:\n   X = DescState(2)                       # Descriptor class attr\n   Y = 3                                  # Class attr\n   def __init__(self):\n       self.Z = 4                         # Instance attr\nobj = CalcAttrs()\nprint(obj.X, obj.Y, obj.Z)                 # X is computed, others are not\nobj.X = 5                                  # X assignment is intercepted\nCalcAttrs.Y = 6                            # Y reassigned in class\nobj.Z = 7                                  # Z assigned in instance\nprint(obj.X, obj.Y, obj.Z)\nobj2 = CalcAttrs()                         # But X uses shared data, like Y!\nprint(obj2.X, obj2.Y, obj2.Z)\nThis code’s internal value information lives only in the descriptor, so there\nwon’t be a collision if the same name is used in the client’s instance. Notice that\nonly the descriptor attribute is managed here—get and set accesses to X are\nintercepted, but accesses to Y and Z are not (Y is attached to the client class and Z\nto the instance). When this code is run, X is computed when fetched, but its value\nis also the same for all client instances because it uses descriptor-level state:\n$ python3 desc-state-desc.py\nDescState get\n20 3 4\nDescState set\nDescState get\n50 6 7\nDescState get\n50 6 4\nIt’s also feasible for a descriptor to store or use an attribute attached to the client",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1518,
      "chapter": null,
      "content": "class’s instance instead of itself. Crucially, unlike data stored in the descriptor\nitself, this allows for data that can vary per client class instance. The descriptor\nin Example 38-7 assumes the instance has an attribute _X attached by the client\nclass and uses it to compute the value of the attribute it represents.\nExample 38-7. desc-state-inst.py\nclass InstState:                           # Using instance state\n   def __get__(self, instance, owner):\n       print('InstState get')             # Assume set by client class\n       return instance._X * 10\n   def __set__(self, instance, value):\n       print('InstState set')\n       instance._X = value\n# Client class\nclass CalcAttrs:\n   X = InstState()                        # Descriptor class attr\n   Y = 3                                  # Class attr\n   def __init__(self):\n       self._X = 2                        # Instance attr\n       self.Z  = 4                        # Instance attr\nobj = CalcAttrs()\nprint(obj.X, obj.Y, obj.Z)                 # X is computed, others are not\nobj.X = 5                                  # X assignment is intercepted\nCalcAttrs.Y = 6                            # Y reassigned in class\nobj.Z = 7                                  # Z assigned in instance\nprint(obj.X, obj.Y, obj.Z)\nobj2 = CalcAttrs()                         # But X differs now, like Z!\nprint(obj2.X, obj2.Y, obj2.Z)\nHere, X is assigned to a descriptor as before that manages accesses. The new\ndescriptor here, though, has no information itself, but it uses an attribute\nassumed to exist in the instance—that attribute is named _X, to avoid collisions\nwith the name of the descriptor itself. When this version is run, the results are\nsimilar, but the value of the descriptor attribute can vary per client instance due\nto the differing state policy:\n$ python3 desc-state-inst.py\nInstState get\n20 3 4\nInstState set\nInstState get",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1519,
      "chapter": null,
      "content": "50 6 7\nInstState get\n20 6 4\nBoth descriptor and instance state have roles. In fact, this is a general advantage\nthat descriptors have over properties—because they have state of their own, they\ncan easily retain data internally without adding it to the namespace of the client\ninstance object. As a summary, the following uses both state sources—its\nself.data retains per-attribute information, while its instance.data can vary\nper client instance:\n>>> class DescBoth:\n        def __init__(self, data):\n            self.data = data\n        def __get__(self, instance, owner):\n            return f'{self.data}, {instance.data}'\n        def __set__(self, instance, value):\n            instance.data = value\n>>> class Client:\n        def __init__(self, data):\n            self.data = data\n        managed = DescBoth('hack')\n>>> I = Client('code')\n>>> I.managed                      # Show both data sources\n'hack, code' \n>>> I.managed = 'HACK'             # Change instance data\n>>> I.managed\n'hack, HACK'\nWe’ll revisit the implications of this choice in a case study later in this chapter.\nBefore we move on, recall from Chapter 32’s coverage of slots that we can\naccess “virtual” attributes like properties and descriptors with tools like dir and\ngetattr, even though they don’t exist in the instance’s namespace dictionary.\nWhether you should access these this way probably varies per program—\nproperties and descriptors may run arbitrary computation and may be less\nobviously instance “data” than slots:\n>>> I.__dict__\n{'data': 'HACK'}\n>>> [x for x in dir(I) if not x.startswith('__')]\n['data', 'managed']",
      "content_length": 1605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1520,
      "chapter": null,
      "content": ">>> getattr(I, 'data')\n'HACK'\n>>> getattr(I, 'managed')\n'hack, HACK'\n>>> for attr in (x for x in dir(I) if not x.startswith('__')):\n        print(f'{attr} => {getattr(I, attr)}')\ndata => HACK\nmanaged => hack, HACK\nThe more generic __getattr__ and __getattribute__ tools we’ll explore\nsoon are not designed to support this functionality: because they have no class-\nlevel attributes, their “virtual” attribute names do not appear in dir results (per\nChapter 31, a __dir__ can provide a dir result, but it’s optional and\nuncommon). In exchange, they are also not limited to specific attribute names\ncoded as properties or descriptors—tools that share even more than this\nbehavior, as the next section explains.\nHow Properties and Descriptors Relate\nAs mentioned earlier, properties and descriptors are strongly related—the\nproperty built-in is just a convenient way to create a descriptor. Now that you\nknow how both work, you should also be able to see that it’s possible to simulate\nthe property built-in with a descriptor class, as demoed by Example 38-8.\nExample 38-8. prop-desc-equiv.py\nclass Property:\n   def __init__(self, fget=None, fset=None, fdel=None, doc=None):\n       self.fget = fget\n       self.fset = fset\n       self.fdel = fdel                                  # Save unbound methods\n       self.__doc__ = doc                                # or other callables\n   def __get__(self, instance, instancetype=None):\n       if instance is None:\n           return self\n       if self.fget is None:\n           raise AttributeError(\"can't get attribute\")\n       return self.fget(instance)                        # Pass instance to self\n                                                         # in property accessors\n   def __set__(self, instance, value):",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1521,
      "chapter": null,
      "content": "if self.fset is None:\n           raise AttributeError(\"can't set attribute\")\n       self.fset(instance, value)\n   def __delete__(self, instance):\n       if self.fdel is None:\n           raise AttributeError(\"can't delete attribute\")\n       self.fdel(instance)\nclass Person:\n   def getName(self): \n       print('getName...')\n   def setName(self, value): \n       print('setName...')\n   name = Property(getName, setName)                     # Use like property()\nx = Person()\nx.name\nx.name = 'Pat'\ndel x.name\nThis Property class catches attribute accesses with the descriptor protocol and\nroutes requests to functions or methods passed in and saved in descriptor state\nwhen the class’s instance is created. Attribute fetches, for example, are routed\nfrom the Person class, to the Property class’s __get__ method, and back to the\nPerson class’s getName. With descriptors, this “just works”:\n$ python3 prop-desc-equiv.py\ngetName...\nsetName...\nAttributeError: can't delete attribute\nNote that this descriptor class equivalent only handles basic property usage,\nthough; to use @ decorator syntax to also specify set and delete operations, we’d\nhave to extend our Property class with setter and deleter methods, which\nwould save the decorated accessor function and return the property object (self\nshould suffice). Since the property built-in already does this, we’ll omit a\nformal coding of this extension here.\nDescriptors and slots and more\nYou can also probably now, at least in part, imagine how descriptors are used to\nimplement Python’s slots extension: instance attribute dictionaries are avoided",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1522,
      "chapter": null,
      "content": "by creating class-level descriptors that intercept slot name access and map those\nnames to sequential storage space in the instance. Unlike the explicit property\ncall, though, much of the magic behind slots is orchestrated at class creation time\nboth automatically and implicitly when a __slots__ attribute is present in a\nclass.\nSee Chapter 32 for more on slots—and why they’re not recommended except in\npathological use cases. Descriptors are also used for other class tools, but we’ll\nomit further internals details here; see Python’s manuals and its open source\ncode for more details.\nNOTE\nDescriptor cliff-hangers: In Chapter 39, we’ll also make use of descriptors to implement\nfunction decorators that apply to both functions and methods. As you’ll see there, because\ndescriptors receive both descriptor and subject class instances they work well in this role,\nthough nested functions are often a conceptually simpler solution. In addition, Chapter 39\ndeploys descriptors as one way to intercept built-in operation method fetches, and Chapter 40\nformalizes data descriptors’ precedence in the full inheritance model noted earlier: with a\n__set__, descriptors override other names and are thus fairly binding—they cannot be hidden\nby names in instance dictionaries.\n__getattr__ and __getattribute__\nSo far, we’ve studied properties and descriptors—tools for managing specific\nattributes. The __getattr__ and __getattribute__ operator-overloading\nmethods provide still other ways to intercept attribute fetches for class instances.\nLike properties and descriptors, they allow us to insert code to be run\nautomatically when attributes are accessed. As shown here, though, these two\nmethods can also be used in more general ways. Because they intercept arbitrary\nnames, they can apply in broader roles, but may also incur extra calls in some\ncontexts, and are too dynamic to register in dir results without help.\nThis form of attribute-fetch interception comes in two flavors, coded with two\ndifferent methods:\n__getattr__ is run for undefined attributes—because it is run only for\nattributes not stored on an instance or inherited from one of its classes,",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1523,
      "chapter": null,
      "content": "its use is straightforward.\n__getattribute__ is run for every attribute—because it is all-\ninclusive, you must be cautious when using this method to avoid\nrecursive loops by passing attribute accesses to a superclass.\nWe met the first of these in Chapter 30. These two methods are representatives\nof a set of attribute interception methods that also includes __setattr__ and\n__delattr__. Because these methods have similar roles, though, we will\ngenerally treat them all as a single topic here.\nUnlike properties and descriptors, these methods are usually considered part of\nPython’s operator-overloading protocol—specially named methods of a class,\ninherited by subclasses, and run automatically when instances are used in the\nassociated built-in operation (here, attribute fetch). Like all normal methods of a\nclass, they each receive a first self argument when called, giving access to both\ninstance state information and other methods of their hosting class.\nThe __getattr__ and __getattribute__ methods are also more generic than\nproperties and descriptors—they can be used to intercept access to any (or even\nall) instance attribute fetches, not just a single specific name. Because of this,\nthese two methods are well suited to general delegation coding patterns—they\ncan implement wrapper (a.k.a. proxy) objects that manage all attribute accesses\nfor an embedded object. By contrast, we must define one property or descriptor\nfor every attribute we wish to intercept. As covered ahead, this delegation role is\nlimited somewhat for built-in operations but still applies to all named methods in\na wrapped object’s interface.\nFinally, these two methods are more narrowly focused than the alternatives we\nconsidered earlier: they intercept attribute fetches only, not assignments. To also\ncatch attribute changes by assignment, we must code a __setattr__ method—\nan operator-overloading method run for every attribute assignment, which must\ntake care to avoid recursive loops by routing attribute assignments through the\ninstance namespace dictionary or a superclass method. Although less common,\nwe can also code a __delattr__ overloading method (which must avoid\nlooping in the same way) to intercept attribute deletions. By contrast, properties\nand descriptors catch get, set, and delete operations by design.",
      "content_length": 2316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1524,
      "chapter": null,
      "content": "__getattr__ and __setattr__ were introduced in Chapters 30 and 32, and\n__getattribute__ was mentioned briefly in Chapter 32. Here, we’ll expand on\ntheir usage and study their roles in larger contexts.\nThe Basics\nIn short, if a class defines or inherits the following methods, they will be run\nautomatically when an instance is used in the operation described by the\ncomments to the right:\ndef __getattr__(self, name):         # On undefined attribute fetch [obj.name]\ndef __getattribute__(self, name):    # On all attribute fetch [obj.name]\ndef __setattr__(self, name, value):  # On all attribute assignment [obj.name=value]\ndef __delattr__(self, name):         # On all attribute deletion [del obj.name]\nIn these, self is the subject instance object as usual, name is the string name of\nthe attribute being accessed, and value is the object being assigned to the\nattribute. The two get methods normally return an attribute’s value, and the other\ntwo return nothing (None). All can raise exceptions to signal prohibited access.\nFor example, to catch every attribute fetch, we can use either of the first two\nprevious methods, and to catch every attribute assignment we can use the third.\nThe following uses __getattr__ for fetches:\nclass Catcher:\n    def __getattr__(self, name):\n        print('Get:', name)\n    def __setattr__(self, name, value):\n        print('Set:', name, value)\nX = Catcher()\nX.job                               # Prints \"Get: job\"\nX.pay                               # Prints \"Get: pay\"\nX.pay = 'bread'                     # Prints \"Set: pay bread\"\nUsing __getattribute__ works exactly the same in this specific case but has\nsubtle looping potential which we’ll take up in the next section:\nclass Catcher:                               # On all attribute fetches\n    def __getattribute__(self, name):        # Works same as getattr here\n        print('Get:', name)                  # But prone to loops in general",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1525,
      "chapter": null,
      "content": "…rest unchanged…\nSuch a coding structure can be used to implement the delegation design pattern\nwe met earlier in Chapter 31. Because all attributes are routed to interception\nmethods generically, we can validate and pass them along to embedded,\nmanaged objects. As a refresher, the following class, borrowed from Chapter 31,\ntraces every attribute fetch made to another object passed to the wrapper (proxy)\nclass:\nclass Wrapper:\n    def __init__(self, object):\n        self.wrapped = object                    # Save object\n    def __getattr__(self, attrname):\n        print('Trace:', attrname)                # Trace fetch\n        return getattr(self.wrapped, attrname)   # Delegate fetch\nX = Wrapper([1, 2, 3])\nX.append(4)                         # Prints \"Trace: append\"\nprint(X.wrapped)                    # Prints \"[1, 2, 3, 4]\"\nThere is no such analog for properties and descriptors, short of coding accessors\nfor every attribute present in every wrapped object. On the other hand, when\nsuch generality is not required, generic accessor methods may incur additional\ncalls for assignments in some contexts—a trade-off described in Chapter 30 and\nmentioned in the context of the case study example we’ll explore at the end of\nthis chapter.\nAvoiding loops in attribute interception methods\nThese methods are generally straightforward to use. Their most complex aspect\nis the potential for looping (a.k.a. recursing). Because __getattr__ is called for\nundefined attributes only, it can freely fetch other attributes within its own code.\nHowever, because __getattribute__ and __setattr__ are run for all\nattributes, their code must be careful when accessing other attributes to avoid\ncalling themselves again and triggering a recursive loop.\nFor example, another attribute fetch run inside a __getattribute__ method’s\ncode like the following will trigger __getattribute__ again—and the code will\nusually loop until memory is exhausted:",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1526,
      "chapter": null,
      "content": "def __getattribute__(self, name):\n        x = self.other                                # LOOPS!\nTechnically, this method is even more loop-prone than this may imply—a self\nattribute reference run anywhere in a class that defines this method will trigger\n__getattribute__ and also has the potential to loop, depending on the class’s\nlogic. This is normally desired behavior—intercepting every attribute fetch is\nthis method’s purpose, after all—but you should be aware that this method\ncatches all attribute fetches wherever they are coded. When coded within\n__getattribute__ itself, this almost always causes a loop.\nTo avoid this loop, route the fetch through a higher superclass instead to skip this\nlevel’s version—because the object class is always a superclass to every class,\nit serves well in this role:\n    def __getattribute__(self, name):\n        x = object.__getattribute__(self, 'other')    # Force higher to avoid me\nFor __setattr__, the situation is similar, as summarized in Chapter 30—\nassigning any attribute inside this method triggers __setattr__ again and may\ncreate a similar loop:\n    def __setattr__(self, name, value):\n        self.other = value                            # Recurs (and might LOOP!)\nHere too, self attribute assignments anywhere in a class defining this method\ntrigger __setattr__ as well, though the potential for looping is much stronger\nwhen they show up in __setattr__ itself. To work around this problem, you\ncan assign the attribute as a key in the instance’s __dict__ namespace\ndictionary instead. This avoids direct attribute assignment:\n    def __setattr__(self, name, value):\n        self.__dict__['other'] = value                # Use attr dict to avoid me\nAlternatively, __setattr__ can also pass its own attribute assignments to a\nhigher superclass to avoid looping, just like __getattribute__. In fact, this\nscheme is sometimes preferred when wrapped classes use slots, properties, or",
      "content_length": 1940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1527,
      "chapter": null,
      "content": "other “virtual” attributes that live on classes instead of instances—and in the\ncase of slots, may preclude __dict__:\n    def __setattr__(self, name, value):\n        object.__setattr__(self, 'other', value)      # Force higher to avoid me\nThis book’s __setattr__ examples often use __dict__ for smaller demos\nanyhow, just because their parameters are known. By contrast, though, we\ncannot use the __dict__ trick to avoid loops in __getattribute__:\n    def __getattribute__(self, name):\n        x = self.__dict__['other']                    # Loops!\nIf this is coded, fetching the __dict__ attribute itself triggers\n__getattribute__ again—causing a recursive loop and an immediate fail.\nStrange but true!\nThe __delattr__ method is less commonly used in practice, but when it is, it is\ncalled for every attribute deletion, just as __setattr__ is called for every\nattribute assignment. When using this method, you must avoid loops when\ndeleting attributes by the same techniques: namespace dictionaries operations or\nsuperclass method calls.\nA First Example\nGeneric attribute management is not nearly as complicated as the prior section\nmay have implied. To see how to put these ideas to work, Example 38-9 is the\nsame first example we used for properties and descriptors in action again, this\ntime implemented with attribute operator-overloading methods. Because these\nmethods are so generic, we test attribute names here to know when a managed\nattribute is being accessed; others are allowed to pass normally.\nExample 38-9. getattr-person.py\nclass Person:\n   def __init__(self, name):               # On [Person()]\n       self._name = name                   # Triggers __setattr__!\n   def __getattr__(self, attr):            # On [obj.undefined]\n       print('get: ' + attr)",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1528,
      "chapter": null,
      "content": "if attr == 'name':                  # Intercept name: not stored\n           return self._name               # Does not loop: real attr\n       else:                               # Others are errors\n           raise AttributeError(attr)\n   def __setattr__(self, attr, value):     # On [obj.any = value]\n       print('set: ' + attr)\n       if attr == 'name':\n           attr = '_name'                  # Set internal name\n       self.__dict__[attr] = value         # Avoid looping here\n   def __delattr__(self, attr):            # On [del obj.any]\n       print('del: ' + attr)\n       if attr == 'name':\n           attr = '_name'                  # Avoid looping here too\n       del self.__dict__[attr]             # but much less common\nsue = Person('Sue Jones')           # sue has a managed attribute\nprint(sue.name)                     # Runs __getattr__\nsue.name = 'Susan Jones'            # Runs __setattr__\nprint(sue.name)\ndel sue.name                        # Runs __delattr__\nprint('-'*20)\nbob = Person('Bob Smith')           # bob's attrs work like sue's\nprint(bob.name)\n#print(Person.name.__doc__)         # No direct equivalent here!\nWhen this code is run, the same sort of output is produced, but this time it\nreflects our generic attribute-interception methods responding to Python’s\nnormal operator-overloading mechanism:\n$ python3 getattr-person.py\nset: _name\nget: name\nSue Jones\nset: name\nget: name\nSusan Jones\ndel: name\n--------------------\nset: _name\nget: name\nBob Smith\nNotice how the attribute assignment in the __init__ constructor triggers\n__setattr__ too—this method catches every instance-attribute assignment,",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1529,
      "chapter": null,
      "content": "even those anywhere within the class itself, and those to underlying attributes\nlike _name. Also note that, unlike with properties and descriptors, there’s no\ndirect notion of specifying documentation for our attribute here; managed\nattributes exist within the code of our interception methods, not as distinct\nobjects.\nUsing __getattribute__\nTo achieve exactly the same results with __getattribute__, replace\n__getattr__ in Example 38-9 with the differing code in Example 38-10.\nBecause it catches all attribute fetches, this version must be careful to avoid\nlooping by passing new fetches to a superclass, and it can’t generally assume\nunknown names are errors.\nExample 38-10. getattribute-person.py (differing part)\n   # Replace just __getattr__ with this\n   def __getattribute__(self, attr):                 # On [obj.any]\n       print('get: ' + attr)\n       if attr == 'name':                            # Intercept all names\n           attr = '_name'                            # Map to internal name\n       return object.__getattribute__(self, attr)    # Avoid looping here\nWhen run with this change, the output is similar, but we get an extra\n__getattribute__ call for the fetch of __dict__ in __setattr__ (the first\ntime originating in __init__):\n$ python3 getattribute-person.py\nset: _name\nget: __dict__\nget: name\nSue Jones\nset: name\nget: __dict__\nget: name\nSusan Jones\ndel: name\nget: __dict__\n--------------------\nset: _name\nget: __dict__\nget: name\nBob Smith",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1530,
      "chapter": null,
      "content": "This example is equivalent to that coded for properties and descriptors, but it’s a\nbit artificial, and it doesn’t really highlight these tools’ assets. Because they are\ngeneric, __getattr__ and __getattribute__ are probably more commonly\nused in delegation-base code (as sketched earlier), where attribute access is\nvalidated and routed to an embedded object. Where just a single attribute must\nbe managed, properties and descriptors might do as well or better, and avoid\nextra calls for unmanaged attributes.\nComputed Attributes\nAs before, our prior example doesn’t really do anything but trace attribute\nfetches; it’s not much more work to compute an attribute’s value when fetched.\nAs for properties and descriptors, Example 38-11 creates a virtual attribute X that\nruns a calculation when fetched.\nExample 38-11. getattr-computed.py\nclass AttrSquare:\n   def __init__(self, start):\n       self.value = start                            # Triggers __setattr__!\n   def __getattr__(self, attr):                      # On undefined attr fetch\n       if attr == 'X':\n           return self.value ** 2                    # value is not undefined\n       else:\n           raise AttributeError(attr)\n   def __setattr__(self, attr, value):               # On all attr assignments\n       if attr == 'X':\n           attr = 'value'\n       self.__dict__[attr] = value\nA = AttrSquare(3)       # 2 instances of class with overloading\nB = AttrSquare(32)      # Each has different state information\nprint(A.X)              # 3 ** 2\nA.X = 4\nprint(A.X)              # 4 ** 2\nprint(B.X)              # 32 ** 2 (1024)\nRunning this code results in the same output that we got earlier when using\nproperties and descriptors, but this script’s mechanics are based on generic\nattribute interception methods:",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1531,
      "chapter": null,
      "content": "$ python3 getattr-computed.py\n9\n16\n1024\nUsing __getattribute__\nAs before, we can achieve the same effect with __getattribute__ instead of\n__getattr__. Example 38-12 replaces the fetch method with a\n__getattribute__ and changes the __setattr__ assignment method to avoid\nlooping by using direct object superclass method calls instead of __dict__\nkeys.\nExample 38-12. getattribute-computed.py\nclass AttrSquare:\n   def __init__(self, start):\n       self.value = start                  # Triggers __setattr__!\n   def __getattribute__(self, attr):       # On all attr fetches\n       if attr == 'X':\n           return self.value ** 2          # Triggers __getattribute__ again!\n       else:\n           return object.__getattribute__(self, attr)\n   def __setattr__(self, attr, value):     # On all attr assignments\n       if attr == 'X':\n           attr = 'value'\n       object.__setattr__(self, attr, value)\n…self-test code same as Example 38-11…\nWhen this version is run, the results are the same again so we won’t relist them\nhere. Notice, though, the implicit and subtle routing going on inside this class’s\nmethods:\nself.value=start inside the constructor triggers __setattr__.\nself.value inside __getattribute__ triggers __getattribute__\nagain.\nIn fact, __getattribute__ is run twice each time we fetch attribute X. This\ndoesn’t happen in the __getattr__ version because the value attribute is not\nundefined (and hence skips the method). If you care about speed and want to",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1532,
      "chapter": null,
      "content": "avoid this, change __getattribute__ to use the superclass to fetch value as\nwell:\n    def __getattribute__(self, attr):\n        if attr == 'X':\n            return object.__getattribute__(self, 'value') ** 2\nOf course, this still incurs a call to the superclass method but not an additional\nrecursive call before we get there. If that’s confusing, add print calls to these\nmethods to trace how and when they run.\n__getattr__ and __getattribute__ Compared\nTo summarize the coding differences between __getattr__ and\n__getattribute__, Example 38-13 uses both to implement three attributes\n—attr1 is a class attribute, attr2 is an instance attribute, and attr3 is a virtual\nmanaged attribute computed when fetched.\nExample 38-13. getattr-v-getattribute.py\nclass GetAttr:\n   attr1 = 1\n   def __init__(self):\n       self.attr2 = 2\n   def __getattr__(self, attr):            # On undefined attrs only\n       print('get:', attr)                 # Not on attr1: inherited from class\n       if attr == 'attr3':                 # Not on attr2: stored on instance\n           return 3\n       else:\n           raise AttributeError(attr)\nX = GetAttr()\nprint(X.attr1)\nprint(X.attr2)\nprint(X.attr3)\nprint('-'*20)\nclass GetAttribute:\n   attr1 = 1\n   def __init__(self):\n       self.attr2 = 2\n   def __getattribute__(self, attr):       # On all attr fetches\n       print('get:',  attr)                # Use superclass to avoid looping here\n       if attr == 'attr3':\n           return 3",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1533,
      "chapter": null,
      "content": "else:\n           return object.__getattribute__(self, attr)\nX = GetAttribute()\nprint(X.attr1)\nprint(X.attr2)\nprint(X.attr3)\nWhen run, the __getattr__ version intercepts only attr3 accesses because it is\nundefined. The __getattribute__ version, on the other hand, intercepts all\nattribute fetches and must route those it does not manage to the superclass\nfetcher to avoid loops:\n$ python3 getattr-v-getattribute.py\n1\n2\nget: attr3\n3\n--------------------\nget: attr1\n1\nget: attr2\n2\nget: attr3\n3\nAlthough __getattribute__ can catch more attribute fetches than\n__getattr__, in practice they are often just variations on a theme—if attributes\nare not physically stored, the two have the same effect.\nManagement Techniques Compared\nTo summarize the coding differences in all four attribute-management schemes\nwe’ve just explored, let’s quickly step through a somewhat more comprehensive\ncomputed-attribute example using each technique. The first version,\nExample 38-14, uses properties to intercept and calculate attributes named\nsquare and cube. Notice how their base values are stored in names that begin\nwith an underscore so they don’t clash with the names of the properties\nthemselves.\nExample 38-14. all_four_props.py\n\"Two dynamically computed attributes with properties\"",
      "content_length": 1269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1534,
      "chapter": null,
      "content": "class Powers:\n   def __init__(self, square, cube):\n       self._square = square                      # _square is the base value\n       self._cube   = cube                        # square is the property name\n   def getSquare(self):\n       return self._square ** 2\n   def setSquare(self, value):\n       self._square = value\n   square = property(getSquare, setSquare)        # Or @property decorator\n   def getCube(self):\n       return self._cube ** 3 \n   cube = property(getCube)                       # Likewise\nTo do the same with descriptors, Example 38-15 defines the attributes with\ncomplete classes. Note that these descriptors store base values as instance state,\nso they must use leading underscores again so as not to clash with the names of\ndescriptors; as called out by the final example of this chapter, we could avoid\nthis renaming requirement by storing base values as descriptor state instead, but\nthat doesn’t as directly address data that must vary per client-class instance.\nExample 38-15. all_four_desc.py\n\"Same, but with descriptors (per-instance state)\"\nclass DescSquare:\n   def __get__(self, instance, owner):\n       return instance._square ** 2\n   def __set__(self, instance, value):\n       instance._square = value\nclass DescCube:\n   def __get__(self, instance, owner):\n       return instance._cube ** 3\nclass Powers:\n   square = DescSquare()\n   cube   = DescCube()\n   def __init__(self, square, cube):\n       self._square = square                  # \"self.square = square\" works too,\n       self._cube   = cube                    # because it triggers desc __set__!\nTo achieve the same result with __getattr__ fetch interception, Example 38-16\nagain stores base values with underscore-prefixed names so that accesses to\nmanaged names are undefined and thus invoke its method; it also needs to code a",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1535,
      "chapter": null,
      "content": "__setattr__ to intercept assignments and take care to avoid its potential for\nlooping.\nExample 38-16. all_four_getattr.py\n\"Same, but with generic __getattr__ undefined-attribute interception\"\nclass Powers:\n   def __init__(self, square, cube):\n       self._square = square\n       self._cube   = cube\n   def __getattr__(self, name):\n       if name == 'square':\n           return self._square ** 2\n       elif name == 'cube':\n           return self._cube ** 3\n       else:\n           raise TypeError('unknown attr:' + name)\n   def __setattr__(self, name, value):\n       if name == 'square':\n           self.__dict__['_square'] = value             # Or use object\n       else:\n           self.__dict__[name] = value\nThe final option in Example 38-17, coding with __getattribute__, is similar\nto the prior version. Because it catches every attribute now, though, it must also\nroute base value fetches to a superclass to avoid looping or extra calls—fetching\nself._square directly works too, but runs a second __getattribute__ call.\nExample 38-17. all_four_getattribute.py\n\"Same, but with generic __getattribute__ all-attribute interception\"\nclass Powers:\n   def __init__(self, square, cube):\n       self._square = square\n       self._cube   = cube\n   def __getattribute__(self, name):\n       if name == 'square':\n           return object.__getattribute__(self, '_square') ** 2\n       elif name == 'cube':\n           return object.__getattribute__(self, '_cube') ** 3\n       else:\n           return object.__getattribute__(self, name)\n   def __setattr__(self, name, value):",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1536,
      "chapter": null,
      "content": "if name == 'square':\n           object.__setattr__(self, '_square', value)   # Or use __dict__\n       else:\n           object.__setattr__(self, name , value)\nTo test, the following REPL session loops through a list of all four modules’\nname strings and imports and fetches classes along the way. Each technique\ntakes a different form in code, but all four produce the same result when run:\n>>> from importlib import import_module\n>>> mods = [f'all_four_{M}' for M in ('props', 'desc', 'getattr', 'getattribute')]\n>>> for modname in mods:\n        module = import_module(modname)    # Import by name string\n        X = module.Powers(3, 4)            # This module's class (print to see)\n        print(X.square)                    # 3 ** 2 = 9\n        print(X.cube)                      # 4 ** 3 = 64\n        X.square = 5\n        print(X.square)                    # 5 ** 2 = 25\n         \n9\n64\n25\n…repeated four times…\nFor more on how these alternatives compare, and other coding options, stay\ntuned for a more realistic application of them in the attribute-validation example\nahead. First, though, we need to take a short side trip to study a pitfall associated\nwith two of these tools—the generic attribute interceptors.\nIntercepting Built-in Operation Attributes\nIf you’ve been reading this book linearly, some of this section is elaboration on\nearlier notes, especially the sidebar “Delegating Built-ins—or Not”. When\n__getattr__ and __getattribute__ were introduced here, it was stated that\nthey intercept undefined- and all-attribute fetches, respectively, which makes\nthem ideal for delegation-based coding patterns.\nWhile this is true for both normally named and explicitly fetched attributes, their\nbehavior needs some additional clarification. Specifically, the implicit method-\nname fetches of built-in operations will never automatically be routed to either\nof these two attribute-interceptor methods. This means that operator-overloading\nmethod calls cannot be delegated to wrapped objects unless wrapper classes",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1537,
      "chapter": null,
      "content": "somehow redefine these methods themselves.\nFor example, attribute fetches for the __str__, __add__, and __getitem__\nmethods run implicitly by printing, + expressions, and indexing, respectively, are\nnot routed to either __getattr__ or __getattribute__. Instead, such methods\nare looked up in classes, and skip the instance and its attribute interceptors.\nHence, there is no direct way to generically catch and delegate built-in\noperations like these.\nThis was a Python 3.X bifurcation, whose purported rationale involved\nmetaclasses and optimization of built-in operations. Whatever its basis, all\nattributes—both __X__ and other—are still dispatched through the instance’s\ninterceptor methods when accessed explicitly by name, so this qualifies as a\nglaring inconsistency: X.__add__ runs __getattr__, but X+Y, which uses\nX.__add__, does not. The net effect complicates delegation-based code.\nThe good news is that wrapper classes can work around this constraint by\nredefining operator-overloading methods in the wrapper itself, in order to catch\nand delegate calls. These extra methods can be added either manually, with tools,\nor by definition in, and inheritance from, common superclasses. It’s more work\nfor delegation classes when operator-overloading methods are part of a wrapped\nobject’s interface, but it’s not a showstopper.\nAs a demo of the issue, consider the code in Example 38-18, which tests various\nattribute types and built-in operations on instances of classes containing\n__getattr__ and __getattribute__ methods.\nExample 38-18. getattr-builtins.py\nclass GetAttr:\n   cattr = 88                       # Attrs stored on class and instance\n   def __init__(self):              # These skip getattr, but not getattribute\n      self.iattr = 77\n   def __len__(self):               # Redefine for len(): doesn't run getattr\n       print('__len__: 66')\n       return 66\n   def __getattr__(self, attr):     # Provide __str__ if asked, else dummy func\n       print('getattr:', attr)      # Never run for __str__: inherited from object\n       if attr == '__str__':\n           return lambda *args: '[Getattr str]'\n       else:",
      "content_length": 2131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1538,
      "chapter": null,
      "content": "return lambda *args: None\nclass GetAttribute:\n   cattr = 88                       # Similar, but catch all attributes\n   def __init__(self):              # Except implicit fetches for built-in ops\n       self.iattr = 77               \n   def __len__(self):               # Redefine for len(): doesn't run getattribute\n       print('__len__: 66')         # But explicit fetches of inherited __str__ do\n       return 66\n   def __getattribute__(self, attr):\n       print('getattribute:', attr)\n       if attr == '__str__':\n           return lambda *args: '[GetAttribute str]'\n       else:\n           return lambda *args: None\nfor Class in GetAttr, GetAttribute:\n   print('\\n' + Class.__name__.ljust(50, '='))\n   X = Class()\n   # Defined attributes trigger getattribute but not getattr\n   X.cattr                   # Class attr    (defined – skips getattr)\n   X.iattr                   # Instance attr (defined – skips getattr)\n   X.other                   # Missing attr\n   len(X)                    # __len__ defined explicitly: moot\n   # Built-in ops do not invoke either getattr or getattribute\n   # No defaults are inherited for these from object superclass\n   try:    X[0]              # Tries to invoke __getitem__\n   except: print('fail []')\n   try:    X + 99            # Ditto, __add__\n   except: print('fail +')\n   try:    X()               # Ditto, __call__\n   except: print('fail ()')\n   # But explicit calls invoke both catchers\n   X.__getitem__(0)\n   X.__add__(99)\n   X.__call__()\n   # The implied object superclass defines a __str__ that precludes getattr\n   # But the absolute getattribute is not called for implicit fetches either\n   print(X.__str__())        # __str__: explicit call => only __getattr__ skipped",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1539,
      "chapter": null,
      "content": "print(X)                  # __str__: implicit via built-in => both skipped\nThis file runs the same set of tests on each of its classes in turn. Match its\nfollowing output with its tests and comments to see how it works. In short,\nneither __getattr__ nor __getattribute__ are run for any of the operator-\noverloading names invoked by built-in operations because such names are\nlooked up in classes only:\n$ python3 getattr-builtins.py\nGetAttr===========================================\ngetattr: other\n__len__: 66\nfail []\nfail +\nfail ()\ngetattr: __getitem__\ngetattr: __add__\ngetattr: __call__\n<__main__.GetAttr object at 0x10f76f020>\n<__main__.GetAttr object at 0x10f76f020>\nGetAttribute======================================\ngetattribute: cattr\ngetattribute: iattr\ngetattribute: other\n__len__: 66\nfail []\nfail +\nfail ()\ngetattribute: __getitem__\ngetattribute: __add__\ngetattribute: __call__\ngetattribute: __str__\n[GetAttribute str]\n<__main__.GetAttribute object at 0x10f74c440>\nMore generally, all explicit method-name attribute fetches are always routed to\nboth attribute-interception methods, but none of the implicit operator-\noverloading methods trigger either attribute-interception method when their\nattributes are fetched by built-in operations. Salient points in this demo worth\ncalling out:\n__str__ access fails to be caught twice by __getattr__: once for the",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1540,
      "chapter": null,
      "content": "built-in print, and once for explicit fetches because a default is\ninherited from the built-in object implied above every topmost class.\n__str__ fails to be caught only once by the __getattribute__\ncatchall—during the built-in print operation. Explicit fetches bypass\nthe inherited __str__ and run __getattribute__.\n__call__ fails to be caught in both schemes for built-in call\nexpressions, but it is intercepted by both when fetched explicitly; unlike\n__str__, there is no inherited __call__ default in object to defeat\n__getattr__ in explicit fetches. The same goes for the __add__ of +\noperations.\n__len__ is handled by both classes because it is an explicitly defined\nmethod in the classes themselves—though its name is not routed to\neither __getattr__ or __getattribute__ if we delete the classes’\n__len__ methods because the len built-in skips them as usual.\nAgain, the net effect is that operator-overloading methods implicitly run by built-\nin operations are never routed through either attribute interception method.\nPython begins the search for such attributes in classes and skips instance lookup\nmechanisms entirely. Normally, named attributes and explicit fetches start with\nthe instance instead.\nFor a more realistic example of this phenomenon’s impact on delegation classes,\nstay tuned for Chapter 39’s Private decorator—along with its coverage of\nmultiple reusable workarounds.\nRevisiting Chapter 28’s delegation example\nAs a coda, you should also now be able to work out why the Manager class\ncoded in Example 28-11 of Chapter 28 had to code a __repr__ to route printing\nrequests to its wrapped object. Just like __str__ in our demo, object provides a\ndefault __repr__, which would prevent print operations from invoking a\n__getattr__. Technically speaking, object defines both __str__ and\n__repr__, but its __str__ simply calls __repr__.\nThat said, object’s defaults are largely a moot point: like all built-in operations,",
      "content_length": 1940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1541,
      "chapter": null,
      "content": "print bypasses both __getattr__ and __getattribute__, as it did for\n__getattribute__ in our demo. Hence, a __repr__ is required by both the\nobject default and the built-in’s behavior.\nAgain, fixes for delegating built-ins are in Chapter 39 (unless we run out of\nunderscores before that!).\nExample: Attribute Validations\nTo close out this chapter, let’s turn to a more realistic example, coded in all four\nof our attribute management schemes. The example we will use defines a\nCardHolder object with four attributes, three of which are managed. The\nmanaged attributes validate or transform values when fetched or stored. All four\nversions produce the same results for the same test code, but they implement\ntheir attributes in very different ways. The examples are largely for self-study;\nalthough we won’t go through their code in detail, they all use concepts we’ve\nalready explored in this chapter.\nUsing Properties to Validate\nOur first coding in Example 38-19 uses properties to manage three attributes. As\nusual, we could use simple methods instead of managed attributes, but properties\nhelp if we have already been using attributes in existing code. Properties run\ncode automatically on attribute access but are focused on a specific set of\nattributes; they cannot be used to intercept all attributes generically.\nTo understand this code, it’s crucial to notice that the attribute assignments inside\nthe __init__ constructor method trigger property setter methods too. When this\nmethod assigns to self.name, for example, it automatically invokes the\nsetName method, which transforms the value and assigns it to an instance\nattribute called __name so it won’t clash with the property’s name.\nThis renaming, sometimes called name mangling, is important because\nproperties use common instance state and have none of their own. Data is stored\nin an attribute called __name, and the attribute called name is always a property,\nnot data. As we saw in Chapter 31, names like __name are known as\npseudoprivate attributes and are changed by Python to include the enclosing",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1542,
      "chapter": null,
      "content": "class’s name when stored in the instance’s namespace; here, this helps keep the\nimplementation-specific attributes distinct from others, including that of the\nproperty that manages them.\nIn the end, this class manages attributes called name, age, and acct; allows the\nattribute addr to be accessed directly; and provides a read-only attribute called\nremain that is entirely virtual and computed on demand. For comparison\npurposes, this property-based coding weighs in at 39 lines of code (including\nblank lines).\nExample 38-19. validate_properties.py\nclass CardHolder:\n   acctlen = 8                                # Class data\n   retireage = 62.5\n   def __init__(self, acct, name, age, addr):\n       self.acct = acct                       # Instance data\n       self.name = name                       # These trigger prop setters too!\n       self.age  = age                        # __X mangled to have class name\n       self.addr = addr                       # addr is not managed\n                                              # remain has no data\n   def getName(self):\n       return self.__name\n   def setName(self, value):\n       value = value.lower().replace(' ', '_')\n       self.__name = value\n   name = property(getName, setName)          # Or @ decorators for both\n   def getAge(self):\n       return self.__age\n   def setAge(self, value):\n       if value < 0 or value > 150:\n           raise ValueError('invalid age')\n       else:\n           self.__age = value\n   age = property(getAge, setAge)\n   def getAcct(self):\n       return self.__acct[:-3] + '***'\n   def setAcct(self, value):\n       value = value.replace('-', '')\n       if len(value) != self.acctlen:\n           raise TypeError('invalid acct number')\n       else:\n           self.__acct = value\n   acct = property(getAcct, setAcct)",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1543,
      "chapter": null,
      "content": "def remainGet(self):                       # Could be a method, not attr\n       return self.retireage - self.age       # Unless already using as attr\n   remain = property(remainGet)\nTesting code\nTo test our class, run the script in Example 38-20 in a console with the name of\nthe class’s module (sans “.py”) as a single command-line argument (you could\nalso import the class in a REPL, but we’re trying to avoid repeating code here).\nWe’ll use this same test script for all four versions of this example so their output\nwill be the same. When it runs, it makes two instances of our managed-attribute\nclass and fetches and changes their various attributes. Operations expected to fail\nare wrapped in try statements.\nExample 38-20. validate_tester.py\ndef loadclass():\n   import sys, importlib\n   modulename = sys.argv[1]                          # Module name in command line\n   module = importlib.import_module(modulename)      # Import module by name string\n   print(f'[Using: {module.CardHolder}]')            # No need for getattr() here\n   return module.CardHolder\ndef printholder(who):\n   print(who.acct, who.name, who.age, who.remain, who.addr, sep=' / ')\nif __name__ == '__main__':\n   CardHolder = loadclass()\n   bob = CardHolder('1234-5678', 'Bob Smith', 40, '123 main st')\n   printholder(bob)\n   bob.name = 'Bob Q. Smith'\n   bob.age  = 50\n   bob.acct = '23-45-67-89'\n   printholder(bob)\n   sue = CardHolder('5678-12-34', 'Sue Jones', 35, '124 main st')\n   printholder(sue)\n   try:\n       sue.age = 200\n   except: print('Bad age for Sue')\n   try:\n       sue.remain = 5\n   except: print(\"Can't set sue.remain\")\n   try:",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1544,
      "chapter": null,
      "content": "sue.acct = '1234567'\n   except: print('Bad acct for Sue')\nFollowing is the output of our test script’s code; again, this is the same for the\nother versions of this example ahead, except for the tested class’s name. Trace\nthrough this code to see how the class’s methods are invoked. Accounts are\ndisplayed with some digits hidden, names are converted to a standard format,\nand time remaining until retirement (hypothetically speaking) is computed when\nfetched using a class-attribute cutoff:\n$ python3 validate_tester.py validate_properties\n[Using: <class 'validate_properties.CardHolder'>]\n12345*** / bob_smith / 40 / 22.5 / 123 main st\n23456*** / bob_q._smith / 50 / 12.5 / 123 main st\n56781*** / sue_jones / 35 / 27.5 / 124 main st\nBad age for Sue\nCan't set sue.remain\nBad acct for Sue\nUsing Descriptors to Validate\nNow, let’s recode our example using descriptors instead of properties. As we’ve\nseen, descriptors are very similar to properties in terms of functionality and\nroles; in fact, properties are basically a focused form of descriptor. Like\nproperties, descriptors are designed to handle specific attributes, not generic\nattribute access. Unlike properties, descriptors can also have their own state, and\nso are perhaps a more general scheme.\nOption 1: Validating with shared descriptor-instance state (badly!)\nTo understand the code in Example 38-21, it’s again important to notice that the\nattribute assignments inside the __init__ constructor method trigger descriptor\n__set__ methods. When the constructor method assigns to self.name, for\nexample, it automatically invokes the Name.__set__() method, which\ntransforms the value and assigns it to a descriptor attribute called name.\nIn the end, this class implements the same attributes as the prior version: it\nmanages attributes called name, age, and acct; allows the attribute addr to be\naccessed directly; and provides a read-only attribute called remain that is\nentirely virtual and computed on demand. Notice how we must catch",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1545,
      "chapter": null,
      "content": "assignments to the remain name in its descriptor and raise an exception; as we\nlearned earlier, if we did not do this, assigning to this attribute of an instance\nwould silently create an instance attribute that hides the class-attribute\ndescriptor.\nFor comparison purposes, this descriptor-based coding takes 45 lines of code.\nExample 38-21. validate_descriptors1.py\nclass CardHolder:                                # Using shared descriptor state\n   acctlen = 8                                  # Class data\n   retireage = 62.5\n   def __init__(self, acct, name, age, addr):\n       self.acct = acct                         # Instance data\n       self.name = name                         # These trigger __set__ calls too!\n       self.age  = age                          # __X not needed: in descriptor\n       self.addr = addr                         # addr is not managed\n                                                # remain has no data\n   class Name:\n       def __get__(self, instance, owner):      # Class names: CardHolder locals\n           return self.name\n       def __set__(self, instance, value):\n           value = value.lower().replace(' ', '_')\n           self.name = value\n   name = Name()\n   class Age:\n       def __get__(self, instance, owner):\n           return self.age                             # Use descriptor data\n       def __set__(self, instance, value):\n           if value < 0 or value > 150:\n               raise ValueError('invalid age')\n           else:\n               self.age = value\n   age = Age()\n   class Acct:\n       def __get__(self, instance, owner):\n           return self.acct[:-3] + '***'\n       def __set__(self, instance, value):\n           value = value.replace('-', '')\n           if len(value) != instance.acctlen:          # Use instance class data\n               raise TypeError('invalid acct number')\n           else:\n               self.acct = value\n   acct = Acct()\n   class Remain:",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1546,
      "chapter": null,
      "content": "def __get__(self, instance, owner):\n           return instance.retireage - instance.age    # Triggers Age.__get__\n       def __set__(self, instance, value):\n           raise TypeError('cannot set remain')        # Else set allowed here\n   remain = Remain()\nWhen run with the prior testing script, all examples in this section produce the\nsame output as shown for properties earlier, except that the name of the class in\nthe first line varies:\n$ python3 validate_tester.py validate_descriptors1\n[Using: <class 'validate_descriptors1.CardHolder'>]\n…rest is same output as properties…\nOption 2: Validating with per-client-instance state (correctly)\nUnlike in the prior property-based variant, though, in Example 38-21, the actual\nname value is attached to the descriptor object, not the client class instance.\nAlthough we could store this value in either instance or descriptor state, the latter\navoids the need to mangle names with underscores to avoid collisions. In the\nCardHolder client class, the attribute called name is always a descriptor object,\nnot data.\nImportantly, the downside of this scheme is that state stored inside a descriptor\nitself is class-level data that is effectively shared by all client-class instances and\nso cannot vary between them. That is, storing state in the descriptor instance\ninstead of the owner (client) class instance means that the state will be the same\nin all owner-class instances. Descriptor state can vary only per attribute\nappearance.\nTo see this at work, try printing attributes of the bob instance after creating the\nsecond instance, sue, with the new test script in Example 38-22. The values of\nsue’s managed attributes (name, age, and acct) overwrite those of the earlier\nobject bob, because both share the same, single descriptor instance attached to\ntheir class.\nExample 38-22. validate_tester_plus.py\nfrom validate_tester import loadclass\nCardHolder = loadclass()\nbob = CardHolder('1234-5678',  'Bob Smith', 40, '123 main st')",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1547,
      "chapter": null,
      "content": "print('bob:', bob.name, bob.acct, bob.age, bob.addr)\nsue = CardHolder('5678-12-34', 'Sue Jones', 35, '124 main st')\nprint('sue:', sue.name, sue.acct, sue.age, sue.addr)    # addr differs: client data\nprint('bob:', bob.name, bob.acct, bob.age, bob.addr)    # name,acct,age overwritten?\nWhen this script is run with the descriptor-state CardHolder of Example 38-21,\nthe results confirm the suspicion—in terms of managed attributes, bob has\nmorphed into sue!\n$ python3 validate_tester_plus.py validate_descriptors1\n[Using: <class 'validate_descriptors1.CardHolder'>]\nbob: bob_smith 12345*** 40 123 main st\nsue: sue_jones 56781*** 35 124 main st\nbob: sue_jones 56781*** 35 123 main st\nThis isn’t an issue for properties because they have no state of their own, and\nthere are valid uses for descriptor state. Such state might be used, for example, to\nmanage descriptor implementation and data that spans all instances, and this\nexample was coded this way on purpose to illustrate the technique. Moreover,\nthe state scope implications of class versus instance attributes should be more or\nless a given at this point in the book.\nHowever, in this particular use case, attributes of CardHolder objects are\nprobably better stored as per-instance data instead of descriptor-instance data,\nperhaps using the same __X naming convention as the property-based equivalent\nto avoid name clashes in the instance—a more important factor this time, as the\nclient is a different class with its own state attributes. Example 38-23 has the\nrequired changes; it doesn’t change line counts (we’re still at 45).\nExample 38-23. validate_descriptors2.py\nclass CardHolder:                                # Using per-client-instance state\n   acctlen = 8                                  # Class data\n   retireage = 62.5\n   def __init__(self, acct, name, age, addr):\n       self.acct = acct                         # Client instance data\n       self.name = name                         # These trigger __set__ calls too!\n       self.age  = age                          # __X needed: in client instance\n       self.addr = addr                         # addr is not managed\n                                                # remain managed but has no data\n   class Name:\n       def __get__(self, instance, owner):      # Class names: CardHolder locals",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1548,
      "chapter": null,
      "content": "return instance.__name\n       def __set__(self, instance, value):\n           value = value.lower().replace(' ', '_')\n           instance.__name = value\n   name = Name()                                       # class.name vs mangled attr\n   class Age:\n       def __get__(self, instance, owner):\n           return instance.__age                       # Use *instance* data\n       def __set__(self, instance, value):\n           if value < 0 or value > 150:\n               raise ValueError('invalid age')\n           else:\n               instance.__age = value\n   age = Age()                                         # class.age vs mangled attr\n   class Acct:\n       def __get__(self, instance, owner):\n           return instance.__acct[:-3] + '***'\n       def __set__(self, instance, value):\n           value = value.replace('-', '')\n           if len(value) != instance.acctlen:          # Use instance class data\n               raise TypeError('invalid acct number')\n           else:\n               instance.__acct = value\n   acct = Acct()                                       # class.acct vs mangled name\n   class Remain:\n       def __get__(self, instance, owner):\n           return instance.retireage - instance.age    # Triggers Age.__get__\n       def __set__(self, instance, value):\n           raise TypeError('cannot set remain')        # Else set allowed here\n   remain = Remain()\nThis supports per-instance data for the name, age, and acct managed fields as\nexpected (bob remains bob), and other tests work as before:\n$ python3 validate_tester_plus.py validate_descriptors2\n[Using: <class 'validate_descriptors2.CardHolder'>]\nbob: bob_smith 12345*** 40 123 main st\nsue: sue_jones 56781*** 35 124 main st\nbob: bob_smith 12345*** 40 123 main st \n$ python3 validate_tester.py validate_descriptors2\n…same output as properties, except class name…\nOne small caveat here: as coded, this version doesn’t support through-class",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1549,
      "chapter": null,
      "content": "descriptor access because such access passes a None to the instance argument\n(also notice the attribute __X name mangling to _Name__name in the error\nmessage when the fetch attempt is made):\n>>> from validate_descriptors1 import CardHolder\n>>> pat = CardHolder('1234-5678', 'Pat Smith', 40, '123 main st')\n>>> pat.name\n'pat_smith'\n>>> CardHolder.name\n'pat_smith'\n>>> from validate_descriptors2 import CardHolder\n>>> pat = CardHolder('1234-5678', 'Pat Smith', 40, '123 main st')\n>>> pat.name\n'pat_smith'\n>>> CardHolder.name\nAttributeError: 'NoneType' object has no attribute '_Name__name'\nWe could detect this with a minor amount of additional code to trigger the error\nmore explicitly, but there’s probably no point—because this version stores data\nin the client instance, there’s no meaning to its descriptors unless they’re\naccompanied by a client instance (much like a normal nonbound instance\nmethod). In fact, that’s really the entire point of this version’s change!\nBecause they are classes, descriptors are a useful and powerful tool, but they\npresent choices that can deeply impact a program’s behavior. As always in OOP,\nchoose your state retention policies carefully.\nUsing __getattr__ to Validate\nAs we’ve seen, the __getattr__ method intercepts all undefined attributes, so it\ncan be more generic than using properties or descriptors. For our example, we\nsimply test the attribute name to know when a managed attribute is being\nfetched; others are stored physically on the instance and so never reach\n__getattr__. Although this approach is more general than using properties or\ndescriptors, extra work may be required to imitate the specific attribute focus of\nother tools. We need to check names at runtime—a multiple-choice that’s a\nprime role for the match statement—and we must code a __setattr__ in order\nto intercept and validate attribute assignments.",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1550,
      "chapter": null,
      "content": "Example 38-24 hosts the __getattr__ version of our validations code. In the\nend, this class, like the prior two, manages attributes called name, age, and acct;\nallows the attribute addr to be accessed directly; and provides a read-only\nattribute called remain that is entirely virtual and is computed on demand.\nAs for the property and descriptor versions of this example, it’s critical to notice\nthat the attribute assignments inside the __init__ constructor method trigger the\nclass’s __setattr__ method too. When this method assigns to self.name, for\nexample, it automatically invokes the __setattr__ method, which transforms\nthe value and assigns it to an instance attribute called name. By storing name on\nthe instance, it ensures that future accesses will not trigger __getattr__. In\ncontrast, acct is stored as _acct so that later accesses to acct do invoke\n__getattr__.\nFor comparison purposes, this alternative comes in at 34 lines of code—5 fewer\nthan the property-based version and 11 fewer than the version using descriptors\n(though replacing if with match here added two lines, along with extra\nindentation). Clarity matters more than code size, of course, but extra code can\nimply extra development and maintenance work. Probably more important here\nare roles: generic tools like __getattr__ are better suited to generic delegation,\nwhile properties and descriptors are designed to manage specific attributes.\nAlso note again that the code here incurs extra calls when setting unmanaged\nattributes (e.g., addr), although no extra calls are incurred for fetching\nunmanaged attributes since they are defined. Though this will likely result in\nnegligible overhead for most programs, the more narrowly focused properties\nand descriptors incur an extra call only when managed attributes are accessed,\nand also appear in dir results automatically when needed by generic tools.\nExample 38-24. validate_getattr.py\nclass CardHolder:\n   acctlen = 8                                  # Class data\n   retireage = 62.5\n   def __init__(self, acct, name, age, addr):\n       self.acct = acct                         # Instance data\n       self.name = name                         # These trigger __setattr__ too\n       self.age  = age                          # _acct not mangled: name tested\n       self.addr = addr                         # addr is not managed",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1551,
      "chapter": null,
      "content": "# remain has no data\n   def __getattr__(self, name):\n       match name:\n           case 'acct':                               # On undefined attr fetches\n               return self._acct[:-3] + '***'         # name, age, addr are defined\n           case 'remain':\n               return self.retireage - self.age       # Doesn't trigger __getattr__\n           case _:\n               raise AttributeError(name)\n   def __setattr__(self, name, value):\n       match name:\n           case 'name':                                 # On all attr assignments\n               value = value.lower().replace(' ', '_')  # addr stored directly\n           case 'age':                                  # acct mangled to _acct\n               if value < 0 or value > 150:\n                   raise ValueError('invalid age')\n           case 'acct':\n               name  = '_acct'\n               value = value.replace('-', '')\n               if len(value) != self.acctlen:\n                   raise TypeError('invalid acct number')\n           case 'remain':\n               raise TypeError('cannot set remain')\n       self.__dict__[name] = value                      # Avoid looping (or object)\nWhen this code is run with either test script, it produces the same output (with a\ndifferent class name):\n$ python3 validate_tester.py validate_getattr\n…same output as properties, except class name…\n$ python3 validate_tester_plus.py validate_getattr\n…same output as instance-state descriptors, except class name…\nUsing __getattribute__ to Validate\nOur final variant uses the __getattribute__ catchall to intercept attribute\nfetches and manage them as needed. Every attribute fetch is caught here, so we\ntest the attribute names to detect managed attributes and route all others to the\nsuperclass for normal fetch processing. This version uses the same __setattr__\nto catch assignments as the prior version (there is no corresponding\n“__setattribute__” in Python—so far?).\nExample 38-25 codes this last mod. It works very much like the __getattr__",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1552,
      "chapter": null,
      "content": "version, so we won’t repeat the full description here. Note, though, that because\nevery attribute fetch is routed to __getattribute__, we don’t need to mangle\nnames to intercept them here (acct is stored as acct). On the other hand, this\ncode must take care to route nonmanaged attribute fetches to a superclass to\navoid looping or extra calls.\nAlso, notice that this version incurs extra calls for both setting and fetching\nunmanaged attributes (e.g., addr); if speed is paramount, this alternative may be\nthe slowest of the bunch. For comparison purposes, this version amounts to 34\nlines of code, just like the prior version (and again including 2 lines added by\nmatch).\nExample 38-25. validate_getattribute.py\nclass CardHolder:\n   acctlen = 8                                  # Class data\n   retireage = 62.5\n   def __init__(self, acct, name, age, addr):\n       self.acct = acct                         # Instance data\n       self.name = name                         # These trigger __setattr__ too\n       self.age  = age                          # acct not mangled: name tested\n       self.addr = addr                         # addr is not managed\n                                                # remain has no data\n   def __getattribute__(self, name):\n       superget = object.__getattribute__                 # Don't loop: level up\n       match name:\n           case 'acct':                                   # On all attr fetches\n               return superget(self, 'acct')[:-3] + '***'\n           case 'remain':\n               return superget(self, 'retireage') - superget(self, 'age')\n           case _:\n               return superget(self, name)                # name, age, addr: stored\n   def __setattr__(self, name, value):\n       match name:\n           case 'name':                                   # On all attr assignments\n               value = value.lower().replace(' ', '_')    # addr stored directly\n           case 'age':\n               if value < 0 or value > 150:\n                   raise ValueError('invalid age')\n           case 'acct':\n               value = value.replace('-', '')\n               if len(value) != self.acctlen:\n                   raise TypeError('invalid acct number')\n           case 'remain':",
      "content_length": 2240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1553,
      "chapter": null,
      "content": "raise TypeError('cannot set remain')\n       self.__dict__[name] = value                         # Avoid loop, orig names\nBoth the __getattr__ and __getattribute__ scripts work the same as the\nproperty and per-client-instance descriptor versions when run by both tester\nscripts—four ways to achieve the same goal in Python, though they vary in\nstructure and are perhaps less redundant in some other roles:\n$ python3 validate_tester.py validate_getattribute\n…same output as properties, except class name…\n$ python3 validate_tester_plus.py validate_getattribute\n…same output as instance-state descriptors, except class name…\nBe sure to study and run this section’s code on your own for more pointers on\nmanaged-attribute coding techniques.",
      "content_length": 736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1554,
      "chapter": null,
      "content": "Chapter Summary\nThis chapter covered the various techniques for managing access to attributes in\nPython, including the __getattr__ and __getattribute__ operator-\noverloading methods, and class properties and descriptors. Along the way, it\ncompared and contrasted these tools and presented a handful of use cases to\ndemonstrate their behavior.\nChapter 39 continues our tool-building focus with a survey of decorators—code\nrun automatically at function and class creation time rather than on attribute\naccess. Before we continue, though, let’s work through a set of questions to\nreview what we’ve covered here.\nTest Your Knowledge: Quiz\n1. How do __getattr__ and __getattribute__ differ?\n2. How do properties and descriptors differ?\n3. How are properties and decorators related?\n4. What are the main functional differences between __getattr__ and\n__getattribute__ and properties and descriptors?\nTest Your Knowledge: Answers\n1. The __getattr__ method is run for explicit fetches of undefined\nattributes only (i.e., those not present on an instance and not inherited\nfrom any of its classes). By contrast, the __getattribute__ method is\ncalled for every explicit attribute fetch, whether the attribute is defined\nor not. Because of this, code inside a __getattr__ can freely fetch\nother attributes if they are defined, whereas __getattribute__ must\nuse special code for all such attribute fetches to avoid looping or extra\ncalls (it must route fetches to a superclass to skip itself). Neither method\nis run for implicit fetches of built-in operations (sans the next chapter’s",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1555,
      "chapter": null,
      "content": "heroics).\n2. Properties serve a specific role, whereas descriptors are more general.\nProperties define get, set, and delete functions for a specific attribute;\ndescriptors provide a class with methods for these actions, too, but they\nprovide extra flexibility to support more arbitrary actions. In fact,\nproperties are really a simple way to create a specific kind of descriptor\n—one that runs functions on attribute accesses. Coding differs too: a\nproperty is created with a built-in function, and a descriptor is coded\nwith a class; thus, descriptors can leverage all the usual OOP features of\nclasses, such as inheritance. Moreover, in addition to the instance’s state\ninformation, descriptors have local state of their own, which can\nsometimes avoid name collisions in the instance.\n3. Properties can be coded with decorator syntax. Because the property\nbuilt-in accepts a single function argument and returns a function, it can\nbe used directly as a function decorator to define a fetch-access\nproperty. Due to the name rebinding behavior of decorators, the name of\nthe decorated function is assigned to a property whose get accessor is\nset to the original function decorated (name=property(name)).\nProperty setter and deleter attributes allow us to further add set and\ndelete accessors with decoration syntax—they set the accessor to the\ndecorated function and return the augmented property. Some may find\nthis a bit clumsy, but this is subjective.\n4. The __getattr__ and __getattribute__ methods are more generic:\nthey can be used to catch arbitrarily many attributes. In contrast, each\nproperty or descriptor provides access interception for only one specific\nattribute—we can’t catch every attribute fetch with a single property or\ndescriptor. On the other hand, properties and descriptors handle both\nattribute fetch and assignment by design: __getattr__ and\n__getattribute__ handle fetches only; to intercept assignments as\nwell, __setattr__ must also be coded. The implementation is also\ndifferent: __getattr__ and __getattribute__ are operator-\noverloading methods, whereas properties and descriptors are objects\nmanually assigned to class attributes. Unlike the others, properties and",
      "content_length": 2198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1556,
      "chapter": null,
      "content": "descriptors can also sometimes avoid extra calls on assignment to\nunmanaged names and show up in dir results automatically, but are\nalso narrower in scope—they can’t address generic delegation goals. In\nPython evolution, new features tend to offer alternatives but often do\nnot fully subsume what came before.",
      "content_length": 309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1557,
      "chapter": null,
      "content": "Chapter 39. Decorators\nIn Chapter 32’s survey of class odds and ends, we met properties and static and\nclass methods, took a quick look at the @ decorator syntax Python offers for\ndeclaring them, and previewed decorator coding techniques. We also met\nfunction decorators briefly while exploring the property built-in in Chapter 38,\nin the context of abstract superclasses in Chapter 29, and in capsule form in\nChapter 19.\nThis chapter picks up where all this previous decorator coverage left off. Here,\nwe’ll dig deeper into the mechanics of decorators and study more ways to code\nnew decorators ourselves with tools like arguments and nesting. As we’ll find,\nother concepts we studied earlier—especially state retention—show up regularly\nin decorators.\nThis is a somewhat advanced topic, and decorator construction tends to be of\nmore interest to tool builders than to application programmers. Still, given that\ndecorators are becoming increasingly common in popular Python frameworks, a\nbasic understanding can help demystify their role, even if you’re just a decorator\nuser.\nBesides covering decorator construction details, this chapter serves as a more\nrealistic case study of Python in action. Because its examples grow larger than\nmany of the others we’ve seen in this book, they better illustrate how code\ncomes together into more complete systems and tools. As an extra perk, some of\nthe code we’ll write here may be used as general-purpose tools in your day-to-\nday programs.\nWhat’s a Decorator?\nSimply put, decoration is a way to specify management or augmentation code\nfor functions and classes. Decorators themselves take the form of callable\nobjects (e.g., functions) that process other callable objects. As suggested earlier\nin this book, Python decorators come in two related flavors:",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1558,
      "chapter": null,
      "content": "Function decorators, added first, do name rebinding at function\ndefinition time, providing a layer of logic that can manage functions and\nmethods or later calls to them.\nClass decorators, added later, do name rebinding at class definition\ntime, providing a layer of logic that can manage classes or the instances\ncreated by later calls to them.\nIn short, decorators provide a way to insert automatically run code at the close\nof function and class definition statements—at the end of a def for function\ndecorators and at the end of a class for class decorators. Such code can play a\nvariety of roles, as described in the following sections.\nManaging Calls and Instances\nIn typical use, this automatically run code may be used to augment calls to\nfunctions and classes. It arranges this by installing wrapper (a.k.a. proxy) objects\nto be invoked later:\nCall proxies\nFunction decorators install wrapper objects to intercept later function calls\nand process them as needed, usually passing the call on to the original\nfunction to run the managed action.\nInterface proxies\nClass decorators install wrapper objects to intercept later instance-creation\ncalls and process them as required, usually passing the call on to the original\nclass to create a managed instance.\nDecorators achieve these effects by automatically rebinding function and class\nnames to other callables at the end of def and class statements. When later\ninvoked, these callables can perform tasks such as tracing and timing function\ncalls, managing access to class instance attributes, and so on.",
      "content_length": 1560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1559,
      "chapter": null,
      "content": "Managing Functions and Classes\nAlthough most examples in this chapter deal with using wrappers to intercept\nlater calls to functions and classes, this is not the only way decorators can be\nused:\nFunction managers\nFunction decorators can also be used to manage function objects instead of or\nin addition to later calls to them—to register a function to an API, for\ninstance. Our primary focus here, though, will be on their more commonly\nused call-wrapper application.\nClass managers\nClass decorators can also be used to manage class objects directly, instead of\nor in addition to instance-creation calls—to augment a class with new\nmethods or data, for example. Because this role intersects strongly with that\nof metaclasses, we’ll explore additional decorator use cases in the next\nchapter. As detailed there, both tools run at the end of the class creation\nprocess, but class decorators often offer a lighter-weight solution.\nIn other words, function decorators can be used to manage both function calls\nand function objects, and class decorators can be used to manage both class\ninstances and classes themselves. By returning the decorated object itself instead\nof a wrapper, decorators become a simple post-creation step for functions and\nclasses.\nRegardless of the role they play, decorators provide a convenient and explicit\nway to code tools useful both during program development and in live\nproduction systems.\nUsing and Defining Decorators\nDepending on your job description, you might encounter decorators as a user or\na provider. As we’ve seen, Python itself comes with built-in decorators that have",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1560,
      "chapter": null,
      "content": "specialized roles—static and class method declaration, property creation, and\nmore. In addition, many popular Python toolkits include decorators to perform\ntasks such as managing database or user-interface logic. In such cases, we can\nget by without knowing how the decorators are coded.\nFor more general tasks, programmers can code arbitrary decorators of their own.\nFor example, function decorators may be used to augment functions with code\nthat adds call tracing or logging, caches call results, performs argument validity\ntesting during debugging, times calls made to functions for optimization, and so\non. Any behavior you can imagine adding to—really, wrapping around—a\nfunction call is a candidate for custom function decorators.\nOn the other hand, function decorators are designed to augment only a specific\nfunction or method call, not an entire object interface. Class decorators fill the\nlatter role better—because they can intercept instance-creation calls, they can be\nused to implement arbitrary object interface augmentation or management tasks.\nFor example, custom class decorators can trace, validate, or otherwise augment\nevery attribute reference made for an object. They can also be used to implement\nproxy objects, singleton classes, and other common coding patterns. In fact,\nyou’ll find that many class decorators are a prime application of the delegation\ncoding pattern we met in Chapter 31.\nWhy Decorators?\nLike many advanced Python tools, decorators are never required from a purely\ntechnical perspective: we can often implement their functionality instead using\nsimple helper function calls or other techniques. And at a base level, we can\nalways manually code the name rebinding that decorators perform automatically.\nThat said, decorators provide an explicit syntax for such tasks, which makes\nintent clearer, can minimize augmentation code redundancy, and may help\nensure correct API usage:\nDecorators have a very explicit syntax, which makes them easier to spot\nthan helper function calls that may be arbitrarily far removed from the\nsubject functions or classes.\nDecorators are applied once when the subject function or class is\ndefined; it’s not necessary to add extra code at every call to the class or",
      "content_length": 2237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1561,
      "chapter": null,
      "content": "function, which may have to be changed in the future.\nBecause of both of the prior points, decorators make it less likely that a\nuser of an API will forget to augment a function or class according to\nAPI requirements.\nIn other words, beyond their technical model, decorators offer some advantages\nin terms of both code maintenance and consistency. Moreover, as structuring\ntools, decorators naturally foster encapsulation of code, which reduces\nredundancy and makes future changes easier.\nLike most tools, decorators have some potential drawbacks, too—when they\ninsert wrapper logic, they can alter the types of the decorated objects, and they\nmay incur extra calls when used as call or interface proxies. On the other hand,\nthe same considerations apply to any technique that adds wrapping logic to\nobjects.\nWe’ll explore these trade-offs in the context of real code later in this chapter.\nAlthough the choice to use decorators is ultimately subjective, their advantages\nare compelling enough to have escalated them to common practice in the Python\nworld. To help you decide for yourself, let’s turn to the details.\nDECORATORS VERSUS MACROS\nPython’s decorators bear similarities to what some call aspect-oriented\nprogramming in other languages—code inserted to run automatically before\nor after a function call runs. Their syntax also very closely resembles (and is\nlikely borrowed from) Java’s annotations, though Python’s model may be\nconsidered more flexible and general.\nSome liken decorators to macros too, but this isn’t entirely apt and can be\nmisleading. Macros, like C’s #define preprocessor directive, are associated\nwith textual replacement and expansion and designed for generating code.\nBy contrast, Python’s decorators are a runtime operation based upon name\nrebinding, callable objects, and often, proxies. While the two may have use\ncases that sometimes overlap, decorators and macros are fundamentally\ndifferent in scope, implementation, and coding patterns. Comparing the two\nseems akin to comparing Python’s import with a C #include, which",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1562,
      "chapter": null,
      "content": "similarly confuses a runtime object-based operation with text insertion.\nOf course, the term macro has also been diluted over time—to some, it now\ncan also refer to any canned series of steps or procedure—and users of other\nlanguages might find the analogy to decorators useful anyhow. But they\nshould also keep in mind that decorators are about callable objects managing\ncallable objects, not text expansion. Python tends to be best understood and\nused in terms of Python idioms.\nThe Basics\nLet’s get started with a first-pass look at decoration behavior from an abstract\nperspective. We’ll write real and more substantial code soon, but since most of\nthe magic of decorators boils down to an automatic rebinding operation, it’s\nimportant to understand this mapping first—for both functions and classes.\nFunction Decorator Basics\nAs previewed earlier in this book, function decorators are largely just syntactic\n“sugar” that runs one function through another at the end of a def statement and\nrebinds the original function name to the result.\nUsage\nA function decorator is a sort of runtime declaration about the function whose\ndefinition follows. The decorator is coded on a line just before the def statement\nthat defines a function or method, and it consists of the @ symbol followed by a\nreference to a metafunction—a function (or other callable object) that manages\nanother function. As of Python 3.9, the code after the @ can be any expression\nreturning a metafunction, but it’s usually a simple name.\nIn terms of code, function decorators automatically map the following syntax:\n@decorator              # Decorate function\ndef F(arg):\n    …\nF(99)                   # Call function",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1563,
      "chapter": null,
      "content": "into this equivalent form, where decorator is a one-argument callable object\nthat returns a callable object with the same number of arguments as F, if not F\nitself:\ndef F(arg):\n    …\nF = decorator(F)        # Rebind function name to decorator result\nF(99)                   # Essentially calls decorator(F)(99)\nThis automatic name rebinding works on any def statement, whether it’s for a\nsimple function or a method within a class. When the function F is later called,\nit’s actually calling the object returned by the decorator, which may be either\nanother object that implements required wrapping logic or the original function\nitself.\nIn other words, decoration essentially maps the first of the following into the\nsecond—though the decorator is really run only once, at decoration time:\nfunc(6, 7)\ndecorator(func)(6, 7)\nThis automatic name rebinding accounts for the static-method and property\ndecoration syntax we met earlier in the book:\nclass C:\n    @staticmethod\n    def meth(…): …            # meth = staticmethod(meth)\nclass C:\n    @property\n    def name(self): …         # name = property(name)\nIn both cases, the method name is rebound to the result of a built-in function\ndecorator at the end of the def statement. Calling the original name later invokes\nwhatever object the decorator returns. In these specific cases, the original names\nare rebound to a static-method router and property descriptor, but the process is\nmuch more general than this—as the next section explains.\nImplementation",
      "content_length": 1504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1564,
      "chapter": null,
      "content": "A decorator itself is a callable that returns a callable. That is, it returns the\nobject to be called later when the decorated function is invoked through its\noriginal name—either a wrapper object to intercept later calls or the original\nfunction augmented in some way. In fact, decorators can be any type of callable\nand return any type of callable: any combination of functions and classes may be\nused, though some are better suited to certain contexts.\nFor example, to tap into the decoration protocol in order to manage a function\njust after it is created, we might code a decorator of this form:\ndef decorator(F):\n    # Process function F here\n    return F\n@decorator\ndef func(): …                 # func = decorator(func)\nBecause the original decorated function is assigned back to its name, this simply\nadds a post-creation step to function definition. Such a structure might be used to\nregister a function to an API, initialize function attributes, and so on.\nIn more typical use, to insert logic that intercepts later calls to a function, we\nmight code a decorator to return a different object than the original function—a\nproxy for later calls:\ndef decorator(F):\n    # Save or use function F\n    # Return a different callable: nested def, class instance with __call__, etc.\n@decorator\ndef func(): …                 # func = decorator(func)\nThis decorator is invoked at decoration time, and the callable it returns is\ninvoked when the original function name is later called. The decorator itself\nreceives the decorated function; the callable returned receives whatever\narguments are later passed to the decorated function’s name. When coded\nproperly, this works the same for class-level methods: the implied instance object\nsimply shows up in the first argument of the returned callable.\nIn skeleton terms, here’s one common coding pattern that captures this idea—the\ndecorator returns a wrapper that retains the original function in an enclosing",
      "content_length": 1955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1565,
      "chapter": null,
      "content": "scope:\ndef decorator(F):                     # On @ decoration\n    def wrapper(*args):               # On wrapped function call\n        # Use F and args\n        # F(*args) calls original function\n    return wrapper\n@decorator                            # func = decorator(func)\ndef func(x, y):                       # func is passed to decorator's F\n    …\nfunc(6, 7)                            # 6, 7 are passed to wrapper's *args\nWhen the name func is later called, it really invokes the wrapper function\nreturned by decorator; the wrapper function can then run the original func\nbecause it is still available in an enclosing scope. When coded this way, each\ndecorated function produces a new scope to retain state.\nTo do the same with classes, we can overload the call operation and use instance\nattributes instead of enclosing scopes:\nclass decorator:\n    def __init__(self, func):         # On @ decoration\n        self.func = func\n    def __call__(self, *args):        # On wrapped function call\n        # Use self.func and args\n        # self.func(*args) calls original function\n@decorator\ndef func(x, y):                       # func = decorator(func)\n    …                                 # func is passed to __init__\nfunc(6, 7)                            # 6, 7 are passed to __call__'s *args\nWhen the name func is later called now, it really invokes the __call__\noperator-overloading method of the instance created by decorator; the\n__call__ method can then run the original func because it is still available in an\ninstance attribute. When coded this way, each decorated function produces a\nnew instance to retain state.\nSupporting method decoration",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1566,
      "chapter": null,
      "content": "One subtle point about the prior class-based coding is that while it works to\nintercept simple function calls, it does not quite work when applied to class-level\nmethod functions:\nclass decorator:\n    def __init__(self, func):           # func is method without instance\n        self.func = func\n    def __call__(self, *args):          # self is decorator instance\n        # self.func(*args) fails!       # C instance not in args!\nclass C:\n    @decorator\n    def method(self, x, y):             # method = decorator(method)\n        …                               # Rebound to decorator instance\nWhen coded this way, the decorated method is rebound to an instance of the\ndecorator class instead of a simple function.\nThe problem with this is that the self in the decorator’s __call__ receives the\ndecorator class instance when the method is later run, and the instance of class\nC is never included in *args. This makes it impossible to dispatch the call to the\noriginal method—the decorator object retains the original method function, but it\nhas no instance to pass to it.\nTo support both functions and methods, the nested function alternative works\nbetter:\ndef decorator(F):                       # F is func or method without instance\n    def wrapper(*args):                 # class instance in args[0] for method\n        # F(*args) runs func or method\n    return wrapper\n@decorator\ndef func(x, y):                         # func = decorator(func)\n    …\nfunc(6, 7)                              # Really calls wrapper(6, 7)\nclass C:\n    @decorator\n    def method(self, x, y):             # method = decorator(method)\n        …                               # Rebound to simple function\nX = C()\nX.method(6, 7)                          # Really calls wrapper(X, 6, 7)",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1567,
      "chapter": null,
      "content": "When coded this way, wrapper receives the C class instance in its first argument,\nso it can dispatch to the original method and access state information.\nTechnically, this nested-function version works because Python creates a bound\nmethod object and thus passes the subject class instance to the self argument\nonly when a method attribute references a simple function; when it references an\ninstance of a callable class instead, the callable class’s instance is passed to self\nto give the callable class access to its own state information. You’ll see how this\nsubtle difference can matter in more realistic examples later in this chapter.\nAlso note that nested functions are perhaps the most straightforward way to\nsupport decoration of both functions and methods, but not necessarily the only\nway. The prior chapter’s descriptors, for example, receive both the descriptor-\nclass and subject-class instance when called. Though more complex, later in this\nchapter you’ll see how this tool can be leveraged in this context as well.\nClass Decorator Basics\nFunction decorators proved so useful that the model was extended to allow class\ndecoration. They were initially resisted because of role overlap with the next\nchapter’s metaclasses; in the end, though, they were adopted because they\nprovide a simpler way to achieve many of the same goals.\nClass decorators are strongly related to function decorators; in fact, they use the\nsame syntax and very similar coding patterns. Rather than wrapping individual\nfunctions or methods, though, class decorators are a way to manage classes or\nwrap up instance-creation calls with extra logic that manages or augments\ninstances created from a class. In the latter role, they may manage full object\ninterfaces instead of a single callable object.\nUsage\nSyntactically, class decorators appear just before class statements, in the same\nway that function decorators appear just before def statements. In symbolic\nterms, for a decorator that must be a one-argument callable that returns a\ncallable, the class decorator syntax:\n@decorator                 # Decorate class\nclass C:",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1568,
      "chapter": null,
      "content": "…\nx = C(99)                  # Make an instance\nis equivalent to the following—the class is automatically passed to the decorator\nfunction, and the decorator’s result is assigned back to the class name:\nclass C:\n    …\nC = decorator(C)           # Rebind class name to decorator result\nx = C(99)                  # Essentially calls decorator(C)(99)\nThe net effect is that calling the class name later to create an instance winds up\ntriggering the callable returned by the decorator, which may or may not call the\noriginal class itself.\nImplementation\nNew class decorators are coded with many of the same techniques used for\nfunction decorators, though some may involve two levels of augmentation—to\nmanage both instance-construction calls as well as instance-interface access.\nBecause a class decorator is also a callable that returns a callable, most\ncombinations of functions and classes suffice.\nHowever it’s coded, the decorator’s result is what runs when an instance is later\ncreated. For example, to simply manage a class just after it is created, return the\noriginal class itself:\ndef decorator(C):\n    # Process class C here\n    return C\n@decorator\nclass C: …                                      # C = decorator(C)\nTo instead insert a wrapper layer that intercepts later instance-creation calls,\nreturn a different callable object:\ndef decorator(C):\n    # Save or use class C\n    # Return a different callable: nested def, class instance with __call__, etc.",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1569,
      "chapter": null,
      "content": "@decorator\nclass C: …                                      # C = decorator(C)\nThe callable returned by such a class decorator typically creates and returns a\nnew instance of the original class, augmented in some way to manage its\ninterface. For example, the following inserts an object that intercepts undefined\nattributes of a class instance:\ndef decorator(cls):                             # On @ decoration\n    class Wrapper:\n        def __init__(self, *args):              # On instance creation\n            self.wrapped = cls(*args)\n        def __getattr__(self, name):            # On attribute fetch\n            return getattr(self.wrapped, name)\n    return Wrapper\n@decorator\nclass C:                             # C = decorator(C)\n    def __init__(self, x, y):        # Run by Wrapper.__init__\n        self.attr = 'hack'\nx = C(6, 7)                          # Really calls Wrapper(6, 7)\nprint(x.attr)                        # Runs Wrapper.__getattr__, prints \"hack\"\nIn this example, the decorator rebinds the class name to another class, which\nretains the original class in an enclosing scope and creates and embeds an\ninstance of the original class when it’s called. When an attribute is later fetched\nfrom the instance, it is intercepted by the wrapper’s __getattr__ and delegated\nto the embedded instance of the original class. Moreover, each decorated class\ncreates a new scope, which remembers the original class. We’ll flesh out this\nexample into some more useful code later in this chapter.\nLike function decorators, class decorators are commonly coded as either closure\n(a.k.a. “factory”) functions that create and return callables, classes that use\n__init__ or __call__ methods to intercept call operations, or some\ncombination thereof. Closure functions typically retain state in enclosing-scope\nreferences, and classes retain state in attributes.\nSupporting multiple instances\nAs for function decorators, some callable-type combinations work better for",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1570,
      "chapter": null,
      "content": "class decorators than others. Consider the following invalid alternative to the\nclass decorator of the prior example:\nclass Decorator:\n    def __init__(self, C):                    # On @ decoration\n        self.C = C\n    def __call__(self, *args):                # On instance creation\n        self.wrapped = self.C(*args)\n        return self\n    def __getattr__(self, attrname):          # On attribute fetch\n        return getattr(self.wrapped, attrname)\n@Decorator\nclass C: …                                    # C = Decorator(C)\nx = C()\ny = C()                                       # Overwrites x!\nThis code handles multiple decorated classes (each makes a new Decorator\ninstance) and will intercept instance-creation calls (each runs __call__). Unlike\nthe prior version, however, this version fails to handle multiple instances of a\ngiven class—each instance-creation call overwrites the prior saved instance. The\nprior version does support multiple instances because each instance-creation call\nmakes a new independent wrapper object. More generally, either of the\nfollowing patterns supports multiple wrapped instances:\ndef decorator(C):                             # On @ decoration\n    class Wrapper:\n        def __init__(self, *args):            # On instance creation: new Wrapper\n            self.wrapped = C(*args)           # Embed instance in instance\n    return Wrapper\nclass Wrapper: …\ndef decorator(C):                             # On @ decoration\n    def onCall(*args):                        # On instance creation: new Wrapper\n        return Wrapper(C(*args))              # Embed instance in instance\n    return onCall\nWe’ll study this phenomenon in a more realistic context later in the chapter too;\nin practice, though, we must be careful to combine callable types properly to\nsupport our intent and choose state policies wisely.",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1571,
      "chapter": null,
      "content": "Decorator Nesting\nSometimes, one decorator isn’t enough. For instance, suppose you’ve coded two\nfunction decorators to be used during development—one to test argument types\nbefore function calls and another to test return value types after function calls.\nYou can use either independently, but what to do if you want to employ both on\na single function? What you really need is a way to nest the two, such that the\nresult of one decorator is the function decorated by the other. It’s irrelevant\nwhich is nested, as long as both steps run on later calls.\nTo support multiple nested steps of augmentation this way, decorator syntax\nallows you to add multiple layers of wrapper logic to a decorated function or\nmethod. When this feature is used, each decorator must appear on a line of its\nown. Decorator syntax of this form:\n@A\n@B\n@C\ndef f(…):\n    …\nruns the same as the following:\ndef f(…):\n    …\nf = A(B(C(f)))\nHere, the original function is passed through three different decorators, and the\nresulting callable object is assigned back to the original name. Each decorator\nprocesses the result of the prior, which may be the original function or an\ninserted wrapper.\nIf all the decorators insert wrappers, the net effect stacks them: when the original\nfunction name is called, three different layers of wrapping object logic will be\ninvoked to augment the original function in three different ways. The last\ndecorator listed is the first applied and, thus, the most deeply nested when the\noriginal function name is later called.\nJust as for functions, multiple class decorators result in multiple nested function\ncalls and possibly multiple levels and steps of wrapper logic around instance-",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1572,
      "chapter": null,
      "content": "creation calls. For example, the following code:\n@hack\n@code\nclass C:\n    …\nX = C()\nis equivalent to the following:\nclass C:\n    …\nC = hack(code(C))\nX = C()\nAgain, each decorator is free to return either the original class or an inserted\nwrapper object. With wrappers, when an instance of the original C class is finally\nrequested, the call is redirected to the wrapping layer objects provided by both\nthe hack and code decorators, which may have arbitrarily different roles—they\nmight trace and validate attribute access for example, and both steps would be\nrun in turn on later requests.\nFor instance, the following do-nothing decorators simply return the decorated\nfunction:\ndef d1(F): return F\ndef d2(F): return F\ndef d3(F): return F\n@d1\n@d2\n@d3\ndef func():               # func = d1(d2(d3(func)))\n    print('hack')\nfunc()                    # Prints \"hack\"\nThe same syntax works on classes, as do these same do-nothing decorators.\nWhen decorators insert wrapper function objects, though, they may augment the\noriginal function when called—the following concatenates to its result in the",
      "content_length": 1091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1573,
      "chapter": null,
      "content": "decorator layers, as it runs the layers from inner to outer:\ndef d1(F): return lambda: 'X' + F()\ndef d2(F): return lambda: 'Y' + F()\ndef d3(F): return lambda: 'Z' + F()\n@d1\n@d2\n@d3\ndef func():               # func = d1(d2(d3(func)))\n    return 'hack'\nprint(func())             # Prints \"XYZhack\"\nWe use lambda functions to implement wrapper layers here (each retains the\nwrapped function F in an enclosing scope); in practice, wrappers can take the\nform of functions, callable classes, and more. When designed well, decorator\nnesting allows us to combine augmentation steps in a wide variety of ways.\nDecorator Arguments\nBoth function and class decorators can also seem to take arguments. Really,\nthough, the role of these arguments is simpler than it may seem: decorator\narguments are passed to a callable that returns the decorator—which in turn\nreturns a callable. By nature, this usually sets up multiple levels of state\nretention. The following, for instance:\n@decorator(A, B)\ndef F(arg):\n    …\nF(99)\nis automatically mapped into this equivalent form, where decorator is a\ncallable that returns the actual decorator. The returned decorator in turn returns\nthe callable run later for calls to the original function name:\ndef F(arg):\n    …\nF = decorator(A, B)(F)    # Rebind F to result of decorator's return value",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1574,
      "chapter": null,
      "content": "F(99)                     # Essentially calls decorator(A, B)(F)(99)\nDecorator arguments are resolved before decoration ever occurs, and they are\nusually used to retain state information for use in later calls. The decorator\nfunction in this example, for instance, might take a form like the following:\ndef decorator(A, B):\n    # Save or use A, B\n    def actualDecorator(F):\n        # Save or use function F\n        # Return a callable: nested def, class instance with __call__, etc.\n        return callable\n    return actualDecorator\nThe outer function in this structure generally saves the decorator arguments\naway as state information for use in the actual decorator, the callable it returns,\nor both. This code snippet retains the state information argument in enclosing\nfunction scope references, but class attributes would work as well.\nIn other words, decorator arguments often imply three levels of callables: a\ncallable to accept decorator arguments, which returns a callable to serve as\ndecorator, which returns a callable to handle calls to the original function or\nclass. Each of the three levels may be a function or class and may retain state in\nthe form of scopes or class attributes.\nDecorator arguments can be used to provide attribute initialization values, call-\ntrace message labels, attribute names to be validated, and much more—any sort\nof configuration parameter for objects or their proxies is a candidate. We’ll code\nconcrete examples of decorator arguments later in this chapter.\nDecorators Manage Functions and Classes, Too\nTo wrap up, although much of the rest of this chapter focuses on wrapping later\ncalls to functions and classes, it’s important to remember that the decorator\nmechanism is more general than this—it is simply a protocol for passing\nfunctions and classes through any callable immediately after they are created. As\nsuch, it can also be used to invoke arbitrary post-creation processing:\ndef decorator(O):\n    # Augment function or class O",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1575,
      "chapter": null,
      "content": "return O\n@decorator\ndef F(): …                 # F = decorator(F)\n@decorator\nclass C: …                 # C = decorator(C)\nIf we return the original decorated object this way instead of a proxy, we can\nmanage functions and classes themselves rather than later calls to them. Such\ndecorators might be used to register callable objects to an API, initialize\nattributes in functions or classes when they are created, and so on. Decorator\nroles are limited only by your imagination.\nCoding Function Decorators\nOn to the code. In the rest of this chapter, we are going to study working\nexamples that demonstrate the decorator concepts we just surveyed. This section\npresents a handful of function decorators in complete form, and the next shows\ntangible class decorators in action. Following that, we’ll close out with two\nlarger case studies that showcase typical decorator roles and code full-scale\nimplementations of class privacy and argument range tests.\nTracing Function Calls\nTo get started, let’s revive the call tracer example we met in Chapter 32.\nExample 39-1 defines and applies a function decorator that counts the number of\ncalls made to the decorated function and prints a trace message for each call.\nExample 39-1. decorator1.py\nclass tracer:\n   def __init__(self, func):         # On @ decoration: save original func\n       self.calls = 0\n       self.func = func\n   def __call__(self, *args):        # On later calls: run original func\n       self.calls += 1\n       print(f'call {self.calls} to {self.func.__name__}')\n       self.func(*args)\n@tracer\ndef hack(a, b, c):           # hack = tracer(hack)",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1576,
      "chapter": null,
      "content": "print(a + b + c)         # Wraps hack in a decorator object\nNotice how each function decorated with this class will create a new instance\nwith its own saved function object and calls counter. Also, observe how the\n*args argument syntax is used to pack and unpack arbitrarily many passed-in\narguments. This generality enables this decorator to be used to wrap any function\nwith any number of positional arguments; this version doesn’t yet work on\nkeyword arguments or class-level methods and doesn’t return results, but we’ll\nfix these shortcomings later in this section.\nNow, if we import this module’s function and test it interactively in a REPL, we\nget the following sort of behavior—each call generates a trace message initially\nbecause the decorator class intercepts it:\n$ python3\n>>> from decorator1 import hack\n>>> hack(1, 2, 3)            # Really calls the tracer wrapper object\ncall 1 to hack\n6\n>>> hack('a', 'b', 'c')      # Invokes __call__ in class\ncall 2 to hack\nabc\n>>> hack.calls               # Number calls in wrapper state information\n2\n>>> hack\n<decorator1.tracer object at 0x10cafc680>\nWhen run, the tracer class saves away the decorated function and intercepts\nlater calls to it in order to add a layer of logic that counts and prints each call.\nNotice how the total number of calls shows up as an attribute of the decorated\nfunction—hack is really an instance of the tracer class when decorated, a\nfinding that may have ramifications for programs that do type checking, but is\ngenerally benign.\nFor function calls, the @ decoration syntax can be more convenient than\nmodifying each call to account for the extra logic level, and it avoids\naccidentally calling the original function directly. Consider a nondecorator\nequivalent such as the following:",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1577,
      "chapter": null,
      "content": ">>> calls = 0\n>>> def tracer(func, *args):\n        global calls\n        calls += 1\n        print(f'call {calls} to {func.__name__}')\n        func(*args)\n \n>>> def hack(a, b, c):       # Nondecorated function\n        print(a, b, c)\n \n>>> hack(1, 2, 3)            # Normal nontraced call: accidental?\n1 2 3\n>>> \n>>> tracer(hack, 1, 2, 3)    # Special traced call without decorators\ncall 1 to hack\n1 2 3\nThis alternative can be used on any function without the special @ syntax, but\nunlike the decorator version, it requires extra syntax at every place where the\nfunction is called in your code. Furthermore, its intent may not be as obvious,\nand it does not ensure that the extra layer will be invoked for normal calls.\nAlthough decorators are never required (we can always rebind names manually),\nthey are often the most convenient and uniform augmentation option.\nDecorator State Retention Options\nThe preceding example raises an important point. Decorators have a variety of\noptions for retaining state information provided at decoration time to be used\nduring later calls to decorated objects. They generally need to support multiple\ndecorated objects and multiple later calls, but there are several ways to\nimplement these goals: instance attributes, global variables, nonlocal closure\nvariables, and function attributes can all be used for retaining state.\nThis topic parallels the initial state coverage in Chapter 17 but can be fleshed out\nhere with class details, and it is so endemic to decorators that it qualifies as a\nprerequisite. This topic also applies to both function and class decorators, but\nlet’s explore it in the narrower function-decorator realm.\nState with class-instance attributes\nAs an opening act in the state-retention show, Example 39-2 codes an augmented\nversion of the prior example, which adds support for keyword arguments with **",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1578,
      "chapter": null,
      "content": "syntax and returns the wrapped function’s result to support more use cases (for\nnonlinear readers, we first studied keyword arguments in Chapter 18).\nExample 39-2. decorator_state_classes.py\nclass tracer:                                # State via instance attributes\n   def __init__(self, func):                # On @ decorator\n       self.calls = 0                       # Save func for later call\n       self.func  = func\n   def __call__(self, *args, **kwargs):     # On call to original function\n       self.calls += 1\n       print(f'call {self.calls} to {self.func.__name__}')\n       return self.func(*args, **kwargs)\n@tracer\ndef hack(a, b, c):           # Same as: hack = tracer(hack)\n   print(a + b + c)         # Triggers tracer.__init__\n@tracer\ndef code(x, y):              # Same as: code = tracer(code)\n   print(x ** y)            # Wraps code in a tracer object\nif __name__ == '__main__':\n   hack(1, 2, 3)            # Really calls tracer instance: runs tracer.__call__\n   hack(a=4, b=5, c=6)      # hack is an instance attribute\n   code(4, 2)               # Really calls tracer instance: self.func is code\n   code(2, y=16)            # self.calls is per-decoration here\nLike the original, this uses class instance attributes to save state explicitly. Both\nthe wrapped function and the calls counter are per-instance information—each\ndecoration gets its own copy. When run as a script, the output of this version is\nas follows; notice how the hack and code functions each have their own calls\ncounter because each decoration creates a new class instance:\n$ python3 decorator_state_classes.py\ncall 1 to hack\n6\ncall 2 to hack\n15\ncall 1 to code\n16\ncall 2 to code\n65536\nWhile useful for decorating functions, this coding scheme still has issues when",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1579,
      "chapter": null,
      "content": "applied to methods—a shortcoming we’ll address in a later revision.\nState with global variables\nFor simpler tasks that don’t require per-function data, moving state variables out\nto the global scope, as illustrated by Example 39-3, might suffice. This code still\nuses an enclosing-scope reference for the original decorated function but pushes\nthe call counter out to the enclosing module.\nExample 39-3. decorator_state_globals.py\ncalls = 0\ndef tracer(func):                         # State via enclosing scope and global\n   def wrapper(*args, **kwargs):         # Instead of class attributes\n       global calls                      # calls is global, not per-function\n       calls += 1\n       print(f'call {calls} to {func.__name__}')\n       return func(*args, **kwargs)\n   return wrapper\n@tracer\ndef hack(a, b, c):           # Same as: hack = tracer(hack)\n   print(a + b + c)\n@tracer\ndef code(x, y):              # Same as: code = tracer(code)\n   print(x ** y)\nif __name__ == '__main__':\n   hack(1, 2, 3)            # Really calls wrapper, assigned to hack\n   hack(a=4, b=5, c=6)      # wrapper calls hack\n   code(4, 2)               # Really calls wrapper, assigned to code\n   code(2, y=16)            # Global calls is not per-decoration here!\nUnfortunately, moving the counter out to the common global scope to allow it to\nbe changed like this also means that it will be shared by every wrapped function.\nUnlike class instance attributes, global counters are cross-program, not per-\nfunction—the counter is incremented for any traced function call. You can tell\nthe difference if you compare this version’s output with the prior version’s—the\nsingle, shared global call counter is incorrectly updated by calls to every\ndecorated function:\n$ python3 decorator_state_globals.py\ncall 1 to hack",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1580,
      "chapter": null,
      "content": "6\ncall 2 to hack\n15\ncall 3 to code\n16\ncall 4 to code\n65536\nState with enclosing-scope nonlocals\nShared global state may be what we want in some cases. If we really want a per-\nfunction counter, though, we can either use classes as before or make use of\nclosure functions and the nonlocal statement described in Chapter 17. Because\nthis statement allows enclosing function scope variables to be changed, they can\nserve as per-decoration, changeable data. Example 39-4 demos the basics of this\nscheme.\nExample 39-4. decorator_state_nonlocals.py\ndef tracer(func):                        # State via enclosing scope and nonlocal\n   calls = 0                            # Instead of class attrs or global\n   def wrapper(*args, **kwargs):        # calls is per-function, not global\n       nonlocal calls\n       calls += 1\n       print(f'call {calls} to {func.__name__}')\n       return func(*args, **kwargs)\n   return wrapper\n@tracer\ndef hack(a, b, c):           # Same as: hack = tracer(hack)\n   print(a + b + c)\n@tracer\ndef code(x, y):              # Same as: code = tracer(code)\n   print(x ** y)\nif __name__ == '__main__':\n   hack(1, 2, 3)            # Really calls wrapper, bound to hack\n   hack(a=4, b=5, c=6)      # wrapper calls hack\n   code(4, 2)               # Really calls wrapper, bound to code\n   code(2, y=16)            # Nonlocal calls _is_ per-decoration here\nNow, because enclosing-scope variables are not cross-program globals, each\nwrapped function gets its own counter again, just as for classes and attributes.\nHere’s the new output:",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1581,
      "chapter": null,
      "content": "$ python3 decorator_state_nonlocals.py\ncall 1 to hack\n6\ncall 2 to hack\n15\ncall 1 to code\n16\ncall 2 to code\n65536\nState with function attributes\nFinally, you can also avoid globals and classes by making use of function\nattributes for some changeable state instead of nonlocal. As we saw in\nChapters 17 and 19, we can attach arbitrary attributes to functions by\nassignment, with func.attr=value. Because a factory function makes a new\nfunction on each call, its attributes become per-call state. Moreover, you need to\nuse this technique only for state variables that must change; enclosing-scope\nreferences are still retained and work normally.\nTo demo, Example 39-5 simply uses wrapper.calls for state. It works the same\nas the preceding nonlocal version because the counter is again per-decorated-\nfunction.\nExample 39-5. decorator_state_attributes.py\ndef tracer(func):                        # State via enclosing scope and func attr\n   def wrapper(*args, **kwargs):        # calls is per-function, not global\n       wrapper.calls += 1\n       print(f'call {wrapper.calls} to {func.__name__}')\n       return func(*args, **kwargs)\n   wrapper.calls = 0\n   return wrapper\n@tracer\ndef hack(a, b, c):           # Same as: hack = tracer(hack)\n   print(a + b + c)\n@tracer\ndef code(x, y):              # Same as: code = tracer(code)\n   print(x ** y)\nif __name__ == '__main__':\n   hack(1, 2, 3)            # Really calls wrapper, assigned to hack\n   hack(a=4, b=5, c=6)      # wrapper calls hack",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1582,
      "chapter": null,
      "content": "code(4, 2)               # Really calls wrapper, assigned to code\n   code(2, y=16)            # wrapper.calls _is_ per-decoration here\nAs we learned in Chapter 17, this works only because the name wrapper is\nretained in the enclosing tracer function’s scope. When we later increment\nwrapper.calls, we are not changing the name wrapper itself, so no nonlocal\ndeclaration is required:\n$ python3 decorator_state_attributes.py\n…same output as prior version…\nThis scheme was almost relegated to a footnote because it may be more obscure\nthan nonlocal and might be better saved for cases where other schemes don’t\nhelp. However, function attributes also have a substantial advantage: like class\ninstances, they allow access to the saved state from outside the decorator’s code;\nnonlocals can only be seen inside the nested function itself, but function\nattributes have wider visibility.\nWe will employ function attributes again in an answer to one of the end-of-\nchapter questions, where their visibility outside callables becomes an asset. As\nchangeable state associated with a context of use, though, they are equivalent to\nenclosing-scope nonlocals. As usual, choosing from multiple tools is an inherent\npart of the programming task.\nBecause decorators often imply multiple levels of callables, you can combine\nfunctions with enclosing scopes, classes with attributes, and function attributes\nto achieve a variety of coding structures. As you’ll see later, though, this\nsometimes may be subtler than you expect—each decorated function should\nhave its own state, and each decorated class may require state both for itself and\nfor each generated instance.\nIn fact, as the next section will explain in more detail, if we want to apply\nfunction decorators to class-level methods, too, we also have to be careful about\nthe distinction Python makes between decorators based on callable class instance\nobjects and decorators based on nested functions.\nClass Pitfall: Decorating Methods\nWhen the preceding section’s class-based tracer function decorator,",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1583,
      "chapter": null,
      "content": "Example 39-2, was initially coded, it was assumed that it could also be applied\nto any method—decorated methods should work the same, but the automatic\nself instance argument would simply be included at the front of *args. The\nonly real downside to this assumption is that it is completely wrong, though the\nreasons for the failure are far from obvious.\nIn short, when applied to a class’s method, this version of the tracer fails\nbecause self is the instance of the decorator class and the instance of the\ndecorated subject class is not included in *args at all. Here’s a relisting of the\nclass in question to avoid page flipping:\nclass tracer:                                # State via instance attributes\n    def __init__(self, func):                # On @ decorator\n        self.calls = 0                       # Save func for later call\n        self.func  = func\n    def __call__(self, *args, **kwargs):     # On call to original function\n        self.calls += 1\n        print(f'call {self.calls} to {self.func.__name__}')\n        return self.func(*args, **kwargs)\nThis phenomenon was introduced abstractly earlier in this chapter, but now we\ncan see it in the context of working code. Example 39-2’s class-based decorator\nworks as advertised earlier for plain functions (copy/pasters: don’t copy the\ninitial “...” REPL prompts included in this chapter to preserve indentation after\ndecorator lines):\n>>> from decorator_state_classes import tracer\n>>> @tracer\n... def hack(a, b, c):                       # hack = tracer(hack)\n        print(a + b + c)                     # Triggers tracer.__init__\n \n>>> hack(1, 2, 3)                            # Runs tracer.__call__\ncall 1 to hack\n6\n>>> hack(a=4, b=5, c=6)                      # hack saved in an instance attribute\ncall 2 to hack\n15\nHowever, decoration of class-level methods fails (more lucid sequential readers\nmight recognize this as an adaptation of our Person class resurrected from the\nobject-oriented tutorial in Chapter 28):",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1584,
      "chapter": null,
      "content": ">>> class Person:\n        def __init__(self, name, pay):\n            self.name = name\n            self.pay  = pay\n        @tracer\n        def giveRaise(self, percent):        # giveRaise = tracer(giveRaise)\n            self.pay *= (1.0 + percent)\n \n>>> pat = Person('Pat Jones', 50_000)        # tracer remembers method funcs\n>>> pat.giveRaise(.10)                       # Runs tracer.__call__(???, .10)\ncall 1 to giveRaise\nTypeError: Person.giveRaise() missing 1 required positional argument: 'percent'\nThe root of the problem here is in the self argument of the tracer class’s\n__call__ method—is it a tracer instance or a Person instance? We ultimately\nneed both as it’s coded: the tracer for decorator state, and the Person for\nrouting on to the original method. Really, self must be the tracer object to\nprovide access to tracer’s state information (its calls and func); this is true\nwhether decorating a simple function or a method.\nUnfortunately, when our decorated method name is rebound to a class instance\nobject with a __call__, Python passes only the tracer instance to self; it\ndoesn’t pass along the Person subject in the arguments list at all. Moreover,\nbecause the tracer knows nothing about the Person instance we are trying to\nprocess with method calls, there’s no way to create a bound method with an\ninstance, and thus, no way to correctly dispatch the call. This isn’t a bug, but it’s\nwildly subtle.\nIn the end, the prior listing winds up passing too few arguments to the decorated\nmethod, and results in an error. Add a line to the decorator’s __call__ to print\nall its arguments to verify this—as you can see, self is the tracer instance, and\nthe Person instance is entirely absent:\n>>> pat.giveRaise(.10)\n<__main__.tracer object at 0x108a02c00> (0.10,) {}\nAs mentioned earlier, this happens because Python passes the implied subject\ninstance to self when a method name is bound to a simple function only; when\nit is an instance of a callable class, that class’s instance is passed instead. That is,\nPython makes a bound method object containing the subject instance only when",
      "content_length": 2098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1585,
      "chapter": null,
      "content": "the method is a simple function, not when it is a callable instance of another\nclass.\nUsing nested functions to decorate methods\nIf you want your function decorators to work on both simple functions and class-\nlevel methods, the most straightforward solution lies in using one of the other\nstate retention solutions described earlier—code your function decorator as\nnested def statements so that you don’t depend on a single self instance\nargument to be both the wrapper class instance and the subject class instance.\nIn fact, we already have—both Examples 39-4 and 39-5 work for both functions\nand class methods by using nested functions along with nonlocal variables or\nfunction attributes:\n>>> from decorator_state_nonlocals import tracer       # See Example 39-4\n>>> @tracer\n... def hack(a, b, c):                                 # Works for functions\n        print(a + b + c)\n \n>>> hack(1, 2, 3)\ncall 1 to hack\n6\n>>> class Person:                                      # AND works for methods\n        def __init__(self, name, pay):\n            self.name = name\n            self.pay  = pay\n        @tracer\n        def giveRaise(self, percent):                  # self included in args\n            self.pay *= (1.0 + percent)                # Counter in nonlocals\n \n>>> pat = Person('Pat Jones', 50_000)\n>>> pat.giveRaise(.10)\ncall 1 to giveRaise\n>>> pat.giveRaise(.10)\ncall 2 to giveRaise\n>>> f'{pat.pay:,.2f}'\n'60,500.00'\n>>> from decorator_state_attributes import tracer      # See Example 39-5\n…same correct results…                                 # Counter in attributes\nBecause decorated methods here are rebound to simple functions instead of",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1586,
      "chapter": null,
      "content": "instance objects, Python correctly passes the Person object as the first argument,\nand the decorator propagates it on in the first item of *args to the self\nargument of the real, decorated methods. Trace through these results and\ndecorators to make sure you have a handle on this model; the next section\nprovides an alternative to it that supports classes but is also substantially more\ncomplex.\nUsing descriptors to decorate methods\nAlthough the nested function solution illustrated in the prior section is the most\nstraightforward way to support decorators that apply to both functions and class-\nlevel methods, other schemes are possible. The descriptor feature we explored in\nthe prior chapter, for example, can help here as well.\nRecall from our discussion in the prior chapter that a descriptor is normally a\nclass attribute assigned to an object with a __get__ method run automatically\nwhenever that attribute is referenced and fetched:\nclass Descriptor:\n    def __get__(self, instance, owner): …\nclass Subject:\n    attr = Descriptor()\nX = Subject()\nX.attr           # Roughly runs Descriptor.__get__(Subject.attr, X, Subject)\nDescriptors may also have __set__ and __del__ access methods, but we don’t\nneed them here. More relevant to this chapter’s topic: because the descriptor’s\n__get__ method receives both the descriptor class instance and subject class\ninstance when invoked, it’s well suited to decorating methods when we need\nboth the decorator’s state and the original class instance for dispatching calls.\nConsider the alternative tracing decorator in Example 39-6, which also happens\nto be a descriptor when used for a class-level method (its “…” are the same as in\nthe prior REPL session).\nExample 39-6. calltracer_desc_class.py\nclass tracer(object):                        # A decorator+descriptor\n   def __init__(self, func):                # On @ decorator\n       self.calls = 0                       # Save func for later call",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1587,
      "chapter": null,
      "content": "self.func  = func\n   def __call__(self, *args, **kwargs):     # On call to original func/meth\n       self.calls += 1\n       print(f'call {self.calls} to {self.func.__name__}')\n       return self.func(*args, **kwargs)\n   def __get__(self, instance, owner):      # On method attribute fetch\n       return wrapper(self, instance)\nclass wrapper:\n   def __init__(self, desc, subj):          # Save both instances\n       self.desc = desc                     # Route calls back to deco/desc\n       self.subj = subj\n   def __call__(self, *args, **kwargs):\n       return self.desc(self.subj, *args, **kwargs)  # Runs tracer.__call__\n@tracer\ndef hack(a, b, c):                           # hack = tracer(hack)\n   …                                        # Uses __call__ only\nclass Person:\n   …\n   @tracer\n   def giveRaise(self, percent):            # giveRaise = tracer(giveRaise)\n       …                                    # Makes giveRaise a descriptor\nThis works the same as the preceding nested function coding. Its operation\nvaries by usage context:\nDecorated functions invoke only its __call__, and never invoke its\n__get__.\nDecorated methods invoke its __get__ first to resolve the method name\nfetch (on I.method); the object returned by __get__ retains the subject\nclass instance and is then invoked to complete the call expression,\nthereby triggering the decorator’s __call__ (on ()).\nFor example, given the same testing code, the call to:\npat.giveRaise(.10)                           # Runs __get__ then __call__\nruns tracer.__get__ first because the giveRaise attribute in the Person class\nhas been rebound to a descriptor by the method function decorator. The call\nexpression then triggers the __call__ method of the returned wrapper object,\nwhich in turn invokes tracer.__call__. In other words, decorated method calls",
      "content_length": 1821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1588,
      "chapter": null,
      "content": "trigger a five-step process: tracer.__get__, which invokes\nwrapper.__init__, followed by three call operations—wrapper.__call__,\ntracer.__call__, and finally the original wrapped method.\nThe wrapper object retains both descriptor and subject instances, so it can route\ncontrol back to the original decorator/descriptor class instance. In effect, the\nwrapper object saves the subject class instance available during method attribute\nfetch and adds it to the later call’s arguments list, which is passed to the\ndecorator__call__. Routing the call back to the descriptor class instance this\nway is required in this application so that all calls to a wrapped method use the\nsame calls counter state information in the descriptor instance object.\nAlternatively, we could use a nested function and enclosing-scope references to\nachieve the same effect—Example 39-7 works the same as the preceding one by\nswapping a wrapper class and attributes for a nested function and scope\nreferences. It requires noticeably less code but follows a similar multistep\nprocess on each decorated method call.\nExample 39-7. calltracer_desc_func.py\nclass tracer(object):\n   def __init__(self, func):                # On @ decorator\n       self.calls = 0                       # Save func for later call\n       self.func  = func\n   def __call__(self, *args, **kwargs):     # On call to original func\n       self.calls += 1\n       print(f'call {self.calls} to {self.func.__name__}')\n       return self.func(*args, **kwargs)\n   def __get__(self, instance, owner):                # On method fetch\n       def wrapper(*args, **kwargs):                  # Retain both inst\n           return self(instance, *args, **kwargs)     # Runs __call__\n       return wrapper\n…rest same as Example 39-6…\nThese two descriptor-based tracers work the same as the nested-functions\nversion, so we’ll skip their output here. Add print statements to their methods\nto trace their multistep get/call processes if it helps. In either coding, this\ndescriptor-based scheme is also substantially subtler than the nested-function\noption, and so is probably a second choice here. To be more blunt, if its\ncomplexity doesn’t send you screaming into the night, its performance costs\nprobably should! Still, this may be a useful coding pattern in other contexts.",
      "content_length": 2302,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1589,
      "chapter": null,
      "content": "Before moving on, it’s also worth briefly noting that we might code this\ndescriptor-based decorator more simply as in Example 39-8, but it would then\napply only to methods, not to simple functions—an intrinsic limitation of\nattribute descriptors, and just the inverse of the problem we’re trying to solve\n(application to both functions and methods).\nExample 39-8. calltracer_desc_fail.py\nclass tracer(object):                         # For methods, but not functions!\n   def __init__(self, meth):                 # On @ decorator\n       self.calls = 0                         \n       self.meth  = meth\n   def __get__(self, instance, owner):       # On method fetch\n       def wrapper(*args, **kwargs):         # On method call: proxy with self+inst\n           self.calls += 1\n           print(f'call {self.calls} to {self.meth.__name__}')\n           return self.meth(instance, *args, **kwargs)\n       return wrapper\n@tracer                                # OK for methods but FAILS for functions\ndef hack(a, b, c):                     # hack = tracer(hack)\n   …                                  # No attribute fetch occurs on calls!\n…rest same as Example 39-6…\nIn the rest of this chapter we’re going to be casual about using classes or\nfunctions to code our function decorators, as long as they are applied only to\nfunctions. Some decorators may not require the instance of the original class,\nand will still work on both functions and methods if coded as a class—\nsomething like Python’s own staticmethod decorator, for example, wouldn’t\nrequire an instance of the subject class (indeed, its whole point is to remove the\ninstance from the call).\nThe simpler moral of this story, though, is that if you want your decorators to\nwork on both simple functions and methods, you’re probably better off using the\nnested-function coding pattern instead of a class with call interception.\nTiming Function Calls\nTo better sample what function decorators are capable of, let’s turn to a different\nuse case. Our next decorator times calls made to a decorated function—both the\ntime for one call and the total time among all calls. As coded in Example 39-9,\nthe decorator is applied to two functions to compare the speeds of list",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1590,
      "chapter": null,
      "content": "comprehensions and the map built-in.\nExample 39-9. timerdeco1.py\n\"Caveat: timer won't work on methods as coded (see quiz solution)\"\nimport time, sys\nclass timer:\n   def __init__(self, func):\n       self.func    = func\n       self.alltime = 0\n   def __call__(self, *args, **kargs):\n       start   = time.perf_counter()\n       result  = self.func(*args, **kargs)\n       elapsed = time.perf_counter() - start\n       self.alltime += elapsed\n       print(f'{self.func.__name__}: {elapsed:.5f}, {self.alltime:.5f}')\n       return result\n@timer\ndef listcomp(N):\n   return [x * 2 for x in range(N)]\n@timer\ndef mapcall(N):\n   return list(map((lambda x: x * 2), range(N)))\nif __name__ == '__main__':\n   for func in (listcomp, mapcall):\n       result = func(5)                        # Time for this call, result\n       func(50_000)\n       func(500_000)\n       func(1_000_000)\n       print(result)\n       print(f'allTime = {func.alltime}\\n')    # Total time for all func calls\n   print('**map/comp =', round(mapcall.alltime / listcomp.alltime, 3))\nWhen run on a macOS host by CPython 3.12, the output of this file’s self-test\ncode is as follows—giving for each function call the function name, time for this\ncall, and time for all calls so far, along with the first call’s return value,\ncumulative time for each function, and the map-to-comprehension time ratio at\nthe end:\n$ python3 timerdeco1.py\nlistcomp: 0.00000, 0.00000\nlistcomp: 0.00366, 0.00366\nlistcomp: 0.03134, 0.03500",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1591,
      "chapter": null,
      "content": "listcomp: 0.05213, 0.08713\n[0, 2, 4, 6, 8]\nallTime = 0.08712841104716063\nmapcall: 0.00001, 0.00001\nmapcall: 0.00396, 0.00397\nmapcall: 0.04082, 0.04479\nmapcall: 0.07789, 0.12268\n[0, 2, 4, 6, 8]\nallTime = 0.12268476499593817\n**map/comp = 1.408\nTimes vary per Python version, test machine, and other variables, of course, and\ncumulative time is available as a class instance attribute here. As usual, map calls\nare slower than list comprehensions when the latter can avoid a function call (or\nequivalently, its requirement of function calls may make map slower).\nFor comparison, see Chapter 21 for a nondecorator approach to timing iteration\nalternatives like these. As a review, we saw two per-call timing techniques there,\nhomegrown and library—here deployed to time the 1M list comprehension case\nof the decorator’s test code, though incurring extra admin costs that skew results\nslightly (add Chapter 21’s folder to your PYTHONPATH or sys.path, or go there\nto run this):\n>>> def listcomp(N): [x * 2 for x in range(N)]\n>>> import timer                                    # Chapter 21 techniques\n>>> timer.total(1, listcomp, 1_000_000)\n(0.08150088600814342, None)\n>>> timer.bestoftotal(5, 1, listcomp, 1_000_000)\n(0.059792334999656305, None)\n>>> import timeit\n>>> timeit.timeit(number=1, stmt=lambda: listcomp(1_000_000))\n0.08125517799635418\n>>> min(timeit.repeat(repeat=5, number=1, stmt=lambda: listcomp(1_000_000)))\n0.06156357398140244\nIn this specific case, a nondecorator approach would allow the subject functions\nto be used with or without timing, but it would also complicate the call signature\nwhen timing is desired—we’d need to add code at every call instead of once at\nthe def. Moreover, in the nondecorator scheme, there would be no direct way to",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1592,
      "chapter": null,
      "content": "guarantee that all list builder calls in a program are routed through timer logic,\nshort of finding and potentially changing them all. This may make it difficult to\ncollect cumulative data for all calls.\nIn general, decorators may be preferred when functions are already deployed as\npart of a larger system and may not be easily passed to analysis functions at\ncalls. On the other hand, because decorators charge each call to a function with\naugmentation logic, a nondecorator approach may be better if you wish to\naugment calls more selectively. As usual, different tools serve different roles.\nAdding Decorator Arguments\nThe timer decorator of the prior section works, but it would be nice if it were\nmore configurable—providing an output label and turning trace messages on and\noff, for instance, might be useful in a general-purpose tool like this. Decorator\narguments come in handy here: when they’re coded properly, we can use them to\nspecify configuration options that can vary for each decorated function. A label,\nfor instance, might be added as abstractly follows:\ndef timer(label=''):\n    def decorator(func):\n        def onCall(*args):        # Multilevel state retention:\n            …                     # args passed to function\n            func(*args)           # func retained in enclosing scope\n            print(label, …)       # label retained in enclosing scope\n        return onCall\n    return decorator              # Returns the actual decorator\n@timer('==>')                     # Like listcomp = timer('==>')(listcomp)\ndef listcomp(N): …                # listcomp is rebound to new onCall\nlistcomp(…)                       # Really calls onCall\nThis code adds an enclosing scope to retain a decorator argument for use on a\nlater actual call. When the listcomp function is defined, Python really invokes\ndecorator—the result of timer, run before decoration actually occurs—with\nthe label value available in its enclosing scope. That is, timer returns the\ndecorator, which remembers both the decorator argument and the original\nfunction, and returns the callable onCall, which ultimately invokes the original\nfunction on later calls. Because this structure creates new decorator and",
      "content_length": 2207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1593,
      "chapter": null,
      "content": "onCall functions, their enclosing scopes are per-decoration state retention.\nWe can put this structure to use in our timer to allow a label and a trace control\nflag to be passed in at decoration time. Example 39-10 does just that, coded in a\nmodule file so it can be imported as a general tool; it uses a class for the second\nstate retention level instead of a nested function, but the net result is similar.\nExample 39-10. timerdeco2.py\nimport time\ndef timer(label='', trace=True):                  # On decorator args: retain args\n   class Timer:\n       def __init__(self, func):                 # On @: retain decorated func\n           self.func    = func\n           self.alltime = 0\n       def __call__(self, *args, **kargs):       # On calls: call original\n           start   = time.perf_counter()\n           result  = self.func(*args, **kargs)\n           elapsed = time.perf_counter() - start\n           self.alltime += elapsed\n           if trace:\n               if label: print(label, end=' ')\n               print(f'{self.func.__name__}: {elapsed:.5f}, {self.alltime:.5f}')\n           return result\n   return Timer\nMostly, all we’ve done here is embed the original Timer class in an enclosing\nfunction in order to create a scope that retains the decorator arguments per\ndeployment. The outer timer function is called before decoration occurs, and it\nsimply returns the Timer class to serve as the actual decorator. On decoration, an\ninstance of Timer is made that remembers the decorated function itself, but also\nhas access to the decorator arguments in the enclosing function scope.\nThis time, rather than embedding self-test code in this file, we’ll run the\ndecorator in a different file. Example 39-11 is a client of our timer decorator,\napplying it to sequence iteration alternatives again.\nExample 39-11. testseqs.py\nimport sys\nfrom timerdeco2 import timer\n@timer(label='[CCC]==>')\ndef listcomp(N):                             # Like listcomp = timer(...)(listcomp)\n   return [x * 2 for x in range(N)]         # listcomp(...) triggers Timer.__call__",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1594,
      "chapter": null,
      "content": "@timer(trace=True, label='[MMM]==>')\ndef mapcall(N):\n   return list(map((lambda x: x * 2), range(N)))\nfor func in (listcomp, mapcall):\n   result = func(5)                         # Time for this call, return value\n   func(50_000)\n   func(500_000)\n   func(1_000_000)\n   print(result)\n   print(f'allTime = {func.alltime}\\n')     # Total time for all calls\nprint('**map/comp =', round(mapcall.alltime / listcomp.alltime, 3))\nWhen run, this file prints the following—each decorated function now has a\nlabel of its own defined by decorator arguments, which may be more useful\nwhen we need to find trace displays mixed in with a larger program’s output:\n$ python3 testseqs.py\n[CCC]==> listcomp: 0.00000, 0.00000\n[CCC]==> listcomp: 0.00379, 0.00379\n[CCC]==> listcomp: 0.03142, 0.03521\n[CCC]==> listcomp: 0.05188, 0.08709\n[0, 2, 4, 6, 8]\nallTime = 0.08709081003325991\n[MMM]==> mapcall: 0.00001, 0.00001\n[MMM]==> mapcall: 0.00401, 0.00402\n[MMM]==> mapcall: 0.04025, 0.04427\n[MMM]==> mapcall: 0.07776, 0.12203\n[0, 2, 4, 6, 8]\nallTime = 0.12203056103317067\n**map/comp = 1.401\nRun additional tests on your own to see how the decorator’s configuration\narguments come into play. As is, this timing function decorator can be used for\nany function, both in modules and interactively. In other words, it automatically\nserves as a general-purpose tool for timing code in our scripts. Watch for\nadditional examples of decorator arguments ahead when we code decorators to\nimplement attribute privacy and argument range checking.\nNOTE",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1595,
      "chapter": null,
      "content": "Timing methods: This section’s timer decorator works on any function, but a minor rewrite is\nrequired to apply it to class-level methods too. In short, and per “Class Pitfall: Decorating\nMethods”, it must avoid using a nested class. Because this last mutation is being saved for an\nend-of-chapter quiz question, though, you’ll have to stay tuned for its final code.\nCoding Class Decorators\nSo far, we’ve been coding function decorators to manage function calls, but as\nwe’ve seen, decorators work on classes too. As described earlier, while similar in\nconcept to function decorators, class decorators are applied to classes instead—\nthey may be used either to manage classes themselves or to intercept instance-\ncreation calls in order to manage instances. Also like function decorators, class\ndecorators are really just optional syntactic sugar, though they can make a\nprogrammer’s intent more obvious and minimize erroneous or missed calls.\nSingleton Classes\nLet’s start with something simple. By intercepting instance-creation calls, class\ndecorators can be used to either manage all the instances of a class, or augment\nthe interfaces of those instances. Example 39-12 lists a first class decorator\nexample that does the former—managing all instances of a class. This code\nimplements the classic singleton coding pattern, where at most one instance of a\nclass ever exists. Its singleton function defines and returns a function for\nmanaging instances, and the @ syntax automatically wraps up a subject class in\nthis function.\nExample 39-12. singletons1.py\ninstances = {}\ndef singleton(aClass):                          # On @ decoration\n   def onCall(*args, **kwargs):                # On instance creation\n       if aClass not in instances:             # One dict entry per class\n           instances[aClass] = aClass(*args, **kwargs)\n       return instances[aClass]\n   return onCall\nTo use this, decorate the classes for which you want to enforce a single-instance\nmodel, as in Example 39-13.",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1596,
      "chapter": null,
      "content": "Example 39-13. singletons-test.py\nfrom singletons1 import singleton\n@singleton                                      # Person = singleton(Person)\nclass Person:                                   # Rebinds Person to onCall\n    def __init__(self, name, hours, rate):     # onCall remembers Person\n       self.name = name\n       self.hours = hours\n       self.rate = rate\n    def pay(self):\n       return self.hours * self.rate\n@singleton                                      # Hack = singleton(Hack)\nclass Hack:                                     # Rebinds Hack to onCall\n   def __init__(self, val):                    # onCall remembers Hack\n       self.attr = val\nsue = Person('Sue', 50, 20)                     # Really calls onCall\nprint(sue.name, sue.pay())\nbob = Person('Bob', 40, 10)                     # Same, single object\nprint(bob.name, bob.pay())\nX = Hack(val=42)                                # One Person, one Hack\nY = Hack(99)\nprint(X.attr, Y.attr)\nNow, when the Person or Hack class is later used to create an instance, the\nwrapping logic layer provided by the decorator routes instance-creation calls to\nonCall, which in turn ensures a single instance per class, regardless of how\nmany construction calls are made. Here’s this code’s output when run via\ncommand line:\n$ python3 singletons.py\nSue 1000\nSue 1000\n42 42\nSingleton coding alternatives\nInterestingly, you can code a more self-contained solution here with the\nnonlocal statement to change enclosing-scope names as described earlier. The\nfollowing alternative achieves an identical effect, by using one enclosing scope\nper class, instead of one global table entry per class. It works the same, but it",
      "content_length": 1674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1597,
      "chapter": null,
      "content": "does not depend on names in the global scope outside the decorator (the None\ncheck could use is instead of == here, but it’s a trivial test either way):\ndef singleton(aClass):                                   # On @ decoration\n    instance = None\n    def onCall(*args, **kwargs):                         # On instance creation\n        nonlocal instance \n        if instance == None:\n            instance = aClass(*args, **kwargs)           # One scope per class\n        return instance\n    return onCall\nYou can also code a self-contained solution with either function attributes or a\nclass instead. The first of the following codes the former, leveraging the fact that\nthere will be one onCall function per decoration—the function object’s\nnamespace serves the same role as an enclosing scope. The second uses one\ninstance per decoration, rather than an enclosing scope, function object, or\nglobal table. In fact, the second option relies on the same coding pattern that we\nwill later label a common decorator class pitfall—here we want just one\ninstance, but that’s not often the case:\ndef singleton(aClass):                                   # On @ decoration\n    def onCall(*args, **kwargs):                         # On instance creation\n        if onCall.instance == None:\n            onCall.instance = aClass(*args, **kwargs)    # One function per class\n        return onCall.instance\n    onCall.instance = None\n    return onCall\nclass singleton:\n    def __init__(self, aClass):                          # On @ decoration\n        self.aClass = aClass\n        self.instance = None\n    def __call__(self, *args, **kwargs):                 # On instance creation\n        if self.instance == None:\n            self.instance = self.aClass(*args, **kwargs) # One instance per class\n        return self.instance\nTo make this singleton decorator a fully general-purpose tool, choose one\nversion, store it in an importable module file, and indent the self-test code under\na __name__ check—steps we’ll leave as suggested exercise. The final class-\nbased version offers an explicit option with extra structure that may better",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1598,
      "chapter": null,
      "content": "support later evolution, but OOP might not be warranted in all contexts.\nTracing Object Interfaces\nThe singleton example of the prior section illustrated using class decorators to\nmanage all the instances of a class. Another common use case for class\ndecorators augments the interface of each generated instance. Class decorators\ncan essentially install a wrapper or “proxy” logic layer atop instances that\nmanages access to their interfaces.\nFor example, in Chapter 31, the __getattr__ operator-overloading method was\nshown as a way to wrap up entire object interfaces of embedded instances in\norder to implement the delegation coding pattern. We saw similar examples in\nthe managed attribute coverage of the prior chapter. Recall that __getattr__ is\nrun when an undefined attribute name is fetched; we can use this hook to\nintercept method calls in a controller class and propagate them to an embedded\nobject.\nThe nondecorator approach\nFor reference and review, here’s the original nondecorator delegation example:\nclass Wrapper:\n    def __init__(self, object):\n        self.wrapped = object                    # Save object\n    def __getattr__(self, attrname):\n        print('Trace:', attrname)                # Trace fetch\n        return getattr(self.wrapped, attrname)   # Delegate fetch\nx = Wrapper([1,2,3])       # Wrap a list object\nx.append(4)                # Delegate to list method\nIn this code, the Wrapper class intercepts access to any of the wrapped object’s\nexplicitly named attributes, prints a trace message, and uses the getattr built-in\nto pass off the request to the wrapped object. Specifically, it traces attribute\naccesses made outside the wrapped object’s class; accesses inside the wrapped\nobject’s methods are not caught and run normally by design. This whole-\ninterface model differs from the behavior of function decorators, which wrap up\njust one specific method.",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1599,
      "chapter": null,
      "content": "The class-decorator approach\nClass decorators provide an alternative and convenient way to code this\n__getattr__ technique and wrap an entire interface. The preceding code, for\nexample, can be coded as a class decorator that triggers wrapped instance\ncreation instead of passing a premade instance into the wrapper’s constructor.\nExample 39-14 codes this mod, also supports keyword arguments with **kargs,\nand counts the number of accesses to illustrate changeable state.\nExample 39-14. interfacetracer.py\ndef Tracer(aClass):                                   # On @ decorator\n   class Wrapper:\n       def __init__(self, *args, **kargs):           # On instance creation\n           self.fetches = 0\n           self.wrapped = aClass(*args, **kargs)     # Use enclosing-scope name\n       def __getattr__(self, attrname):\n           print('Trace: ' + attrname)               # Catches all but own attrs\n           self.fetches += 1\n           return getattr(self.wrapped, attrname)    # Delegate to wrapped obj\n   return Wrapper\nif __name__ == '__main__':\n   @Tracer\n   class Hack:                                  # Hack = Tracer(Hack)\n       def display(self):                       # Hack is rebound to Wrapper\n           print('Hack!' * 3)\n   @Tracer\n   class Person:                                # Person = Tracer(Person)\n       def __init__(self, name, hours, rate):   # Wrapper remembers Person\n           self.name = name\n           self.hours = hours\n           self.rate = rate\n       def pay(self):                           # Accesses outside class traced\n           return self.hours * self.rate        # In-method accesses not traced\n   work = Hack()                                # Triggers Wrapper()\n   work.display()                               # Triggers __getattr__\n   print([work.fetches])\n   print()\n   bob = Person('Bob', 40, 50)                  # bob is really a Wrapper\n   print(bob.name)                              # Wrapper embeds a Person\n   print(bob.pay())",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1600,
      "chapter": null,
      "content": "print()\n   sue = Person('Sue', rate=100, hours=60)      # sue is a different Wrapper\n   print(sue.name)                              # With a different Person\n   print(sue.pay())\n   print()\n   print(bob.name)                              # bob's state != sue's state\n   print(bob.pay())\n   print('calls:', [bob.fetches, sue.fetches])  # Wrapper attrs are not traced\nIt’s important to note that this is very different from the tracer decorator we met\nearlier (despite the name!). In “Coding Function Decorators”, we looked at\ndecorators that enabled us to trace and time calls to a given function or method.\nIn contrast, by intercepting instance-creation calls, the class decorator here\nallows us to trace an entire object interface—that is, accesses to any of the\ninstance’s attributes.\nIt’s also important to note that this decorator’s __getattr__ won’t catch the\nimplicit attribute fetches of built-in operations per the prior chapter, but we’ll\ndefer more on this subject until we code attribute privacy ahead.\nThe following is the output produced by this code: attribute fetches on instances\nof both the Hack and Person classes invoke the __getattr__ logic in the\nWrapper class because work, and bob, and sue are really instances of Wrapper,\nthanks to the decorator’s redirection of instance-creation calls:\n$ python3 interfacetracer.py\nTrace: display\nHack!Hack!Hack!\n[1]\nTrace: name\nBob\nTrace: pay\n2000\nTrace: name\nSue\nTrace: pay\n6000\nTrace: name\nBob\nTrace: pay\n2000",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1601,
      "chapter": null,
      "content": "calls: [4, 2]\nNotice how there is one Wrapper class with state retention per decoration,\ngenerated by the nested class statement in the Tracer function, and how each\ninstance gets its own fetches counter by virtue of generating a new Wrapper\ninstance. As you’ll see ahead, orchestrating this is trickier than you may expect.\nApplying class decorators to built-in types\nAlso notice that the preceding decorates a user-defined class. Just like in the\noriginal example in Chapter 31, we can also use the decorator to wrap up a built-\nin object type such as a list, as long as we either subclass to allow decoration\nsyntax or perform the decoration rebinding manually—decorator syntax requires\na class statement for the @ line. In the following, x is really a Wrapper again\ndue to the indirection of decoration:\n>>> from interfacetracer import Tracer\n>>> @Tracer\n... class MyList(list): pass      # MyList = Tracer(MyList)\n>>> x = MyList([1, 2, 3])         # Triggers Wrapper()\n>>> x.append(4)                   # Triggers __getattr__, append\nTrace: append\n>>> x.wrapped\n[1, 2, 3, 4]\n>>> MyList = Tracer(list)         # Or perform decoration manually\n>>> x = MyList([4, 5, 6])         # Else subclass statement required\n>>> x.append(7)\nTrace: append\n>>> x.wrapped\n[4, 5, 6, 7]\nThe decorator approach allows us to move instance creation into the decorator\nitself instead of requiring a premade object to be passed in. Although this seems\nlike a minor difference, it lets us retain normal instance-creation syntax and\nlimits augmentation syntax to class definition. Rather than requiring all instance-\ncreation calls to route objects through a wrapper manually, we need only\naugment class definitions with decorator syntax:",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1602,
      "chapter": null,
      "content": "@Tracer                                          # Decorator approach\nclass Person: …\nbob = Person('Bob', 40, 50)\nsue = Person('Sue', rate=100, hours=60)\nclass Person: …                                  # Nondecorator approach\nbob = Wrapper(Person('Bob', 40, 50))\nsue = Wrapper(Person('Sue', rate=100, hours=60))\nAssuming you will make more than one instance of a class and want to apply the\naugmentation to every instance of a class, decorators will generally be a net win\nin terms of both code size and code maintenance.\nClass Pitfall: Retaining Multiple Instances\nCuriously, the decorator function in the preceding example can almost be coded\nas a class instead of a function with the proper operator-overloading protocol.\nExample 39-15’s alternative coding works similarly because its __init__ is\ntriggered when the @ decorator is applied to the class, and its __call__ is\ntriggered when a subject class instance is created. Our objects are really\ninstances of Tracer this time, and we essentially just trade an enclosing-scope\nreference for an instance attribute here.\nExample 39-15. interfacetracer-fail.py (start)\nclass Tracer:\n   def __init__(self, aClass):               # On @decorator\n       self.aClass = aClass                  # Use instance attribute\n   def __call__(self, *args):                # On instance creation\n       self.wrapped = self.aClass(*args)     # ONE (LAST) INSTANCE PER CLASS!\n       return self\n   def __getattr__(self, attrname):\n       print('Trace:', attrname)\n       return getattr(self.wrapped, attrname)\n@Tracer                                       # Triggers __init__\nclass Hack:                                   # Like: Hack = Tracer(Hack)\n   def display(self):\n       print('Hack!' * 3)\nwork = Hack()                                 # Triggers __call__\nwork.display()                                # Triggers __getattr__\nAs we saw in the abstract earlier, though, this class-only alternative handles",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1603,
      "chapter": null,
      "content": "multiple classes as before, but it won’t quite work for multiple instances of a\ngiven class: each instance-creation call triggers __call__, which overwrites the\nprior instance. The net effect is that Tracer saves just one instance—the last one\ncreated. Example 39-16 extends this file to demo the problem.\nExample 39-16. interfacetracer-fail.py (continued)\n@Tracer\nclass Person:                                 # Person = Tracer(Person)\n   def __init__(self, name):                 # Person rebound to a Tracer\n       self.name = name\nbob = Person('Bob')                           # bob is really a Tracer\nprint(bob.name)                               # Tracer embeds a Person\nsue = Person('Sue')\nprint(sue.name)                               # sue overwrites bob\nprint(bob.name)                               # OOPS: now bob's name is 'Sue'!\nThis code’s output follows—because this tracer only has a single shared\ninstance, the second overwrites the first:\n$ python3 interfacetracer-fail.py \nTrace: display\nHack!Hack!Hack!\nTrace: name\nBob\nTrace: name\nSue\nTrace: name\nSue\nThe problem here is bad state retention—we make one decorator instance per\nclass but not per class instance, such that only the last instance is retained. The\nsolution, as in our prior class pitfall for decorating methods, lies in abandoning\nclass-based decorators.\nThe earlier function-based Tracer version of Example 39-14, however, does\nwork for multiple instances. Because it returns a class instead of an instance of\nthat class, each instance-creation call makes a new Wrapper instance instead of\noverwriting the state of a single shared Tracer instance. The original\nnondecorator version handles multiple instances correctly for the same reason.\nThe moral here: decorators are not only arguably magical, they can also be\nincredibly subtle!",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1604,
      "chapter": null,
      "content": "Example: “Private” and “Public” Attributes\nThe final two sections of this chapter present larger examples of decorator use,\nwhich give us a chance to see how concepts come together in more useful code.\nBoth are presented with minimal description, partly to conserve space but mostly\nbecause you should already understand decorator basics well enough to be able\nto study these on your own.\nImplementing Private Attributes\nFirst up, the class decorator in Example 39-17 implements a Private\ndeclaration and access checks for class instance attributes—that is, for attributes\nstored on an instance, or inherited from one of its classes.\nThis decorator disallows fetch and change access to such attributes from outside\nthe decorated class but still allows the class itself to access those names freely\nwithin its own methods. It’s not quite the same as “private” in C++ or Java—and\nPython is not about control in general—but this decorator demo provides similar\naccess validations as an option in Python for the rare and atypical cases where\nthis might be useful during development.\nWe saw an initial and incomplete implementation of instance attribute privacy\nfor changes only in Chapter 30. The version here extends this concept to validate\nattribute fetches, too, and it uses delegation instead of inheritance to implement\nthe model. In a sense, this is also just an extension to the attribute-tracer class\ndecorator we met earlier.\nAlthough this example utilizes the syntactic sugar of class decorators to code\nattribute privacy, its attribute interception is ultimately still based upon the\n__getattr__ and __setattr__ operator-overloading methods we met in prior\nchapters. When a private attribute access is detected, this version uses the raise\nstatement to raise an exception, along with an error message; the exception may\nbe caught in a try or allowed to terminate the accessing script.\nExample 39-17 lists the decorator’s first-cut code, along with a self-test at the\nbottom of the file. As coded, it catches all explicit attribute fetches, but not the\nimplicit fetches of built-in operations (more on this in a moment).\nExample 39-17. access1.py",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1605,
      "chapter": null,
      "content": "\"\"\"\nClass decorator with Private attribute declarations.\nPrivacy for attributes fetched from class instances.\nSee self-test code at end of file for a usage example.\nRebinding is: Doubler = Private('data', 'size')(Doubler).\nPrivate returns onDecorator, onDecorator returns onInstance,\nand each onInstance instance embeds a new Doubler instance.\n\"\"\"\ntraceMe = False\ndef trace(*args):\n   if traceMe: print(f'[{' '.join(map(str, args))}]')   # Python 3.12+ f-string\ndef Private(*privates):                              # privates in enclosing scope\n   def onDecorator(aClass):                         # aClass in enclosing scope\n       class onInstance:                            # wrapped in instance attribute\n           def __init__(self, *args, **kargs):\n               self.wrapped = aClass(*args, **kargs)\n           def __getattr__(self, attr):             # My attrs don't call getattr\n               trace('get:', attr)                  # Others assumed in wrapped\n               if attr in privates:\n                   raise TypeError('private attribute fetch, ' + attr)\n               else:\n                   return getattr(self.wrapped, attr)\n           def __setattr__(self, attr, value):             # Outside accesses\n               trace('set:', attr, value)                  # Others run normally\n               if attr == 'wrapped':                       # Allow my attrs\n                   self.__dict__[attr] = value             # Avoid looping\n               elif attr in privates:\n                   raise TypeError('private attribute change, ' + attr)\n               else:\n                   setattr(self.wrapped, attr, value)      # Wrapped obj attrs\n       return onInstance\n   return onDecorator\nif __name__ == '__main__':\n   traceMe = True\n   @Private('data', 'size')                   # Doubler = Private(...)(Doubler)\n   class Doubler:\n       def __init__(self, label, start):\n           self.label = label                 # Accesses inside the subject class\n           self.data  = start                 # Not intercepted: run normally\n       def size(self):",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1606,
      "chapter": null,
      "content": "return len(self.data)              # Method bodies run with no checking\n       def double(self):                      # Because privacy not inherited\n           for i in range(self.size()):\n               self.data[i] = self.data[i] * 2\n       def display(self):\n           print(f'{self.label} => {self.data}')\n   print('Making instances...')\n   X = Doubler('X is', [1, 2, 3])\n   Y = Doubler('Y is', [-10, -20, -30])\n   # The following all succeed properly\n   print('\\nExploring X instance...')\n   print(X.label)                             # Accesses outside subject class\n   X.display(); X.double(); X.display()       # Intercepted: validated, delegated\n   print('\\nExploring Y instance...')\n   print(Y.label)\n   Y.display(); Y.double()\n   Y.label = 'Hack'\n   Y.display()\n   # The following all fail properly\n   \"\"\"\n   print(X.size())          # Prints \"TypeError: private attribute fetch, size\"\n   print(X.data)\n   X.data = [1, 1, 1]       # Prints \"TypeError: private attribute change, data\"\n   X.size = lambda S: 0\n   print(Y.data)\n   print(Y.size())\n   \"\"\"\nWhen its traceMe is True, the module file’s self-test code produces the\nfollowing output. Notice how the decorator catches and validates both attribute\nfetches and assignments run outside of the wrapped class but does not catch\nattribute accesses inside the class itself:\n$ python3 access1.py\nMaking instances...\n[set: wrapped <__main__.Doubler object at 0x1059c7d70>]\n[set: wrapped <__main__.Doubler object at 0x1059c7da0>]\nExploring X instance...\n[get: label]\nX is\n[get: display]\nX is => [1, 2, 3]",
      "content_length": 1563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1607,
      "chapter": null,
      "content": "[get: double]\n[get: display]\nX is => [2, 4, 6]\nExploring Y instance...\n[get: label]\nY is\n[get: display]\nY is => [-10, -20, -30]\n[get: double]\n[set: label Hack]\n[get: display]\nHack => [-20, -40, -60]\nImplementation Details I\nThis code is nontrivial, and you’re probably best off tracing through it on your\nown to see how it works. To help you study, though, here are a few highlights\nworth mentioning.\nInheritance versus delegation\nThe initial and limited privacy example shown in Chapter 30 used inheritance to\nmix in a __setattr__ to catch accesses. Inheritance makes this difficult,\nhowever, because differentiating between accesses from inside or outside the\nclass is not straightforward (inside access should be allowed to run normally, and\noutside access should be restricted). To work around this, the Chapter 30\nexample requires inheriting classes to use __dict__ assignments to set attributes\n—an incomplete solution at best.\nThe version here uses delegation (embedding one object inside another) instead\nof inheritance; this pattern is better suited to our task as it makes it much easier\nto distinguish between accesses inside and outside of the subject class. Attribute\naccesses from outside the subject class are intercepted by the wrapper layer’s\noverloading methods and delegated to the class if valid. Accesses inside the class\nitself (i.e., through self within its methods’ code) are not intercepted and are\nallowed to run normally without checks because privacy is not inherited in this\nversion.\nDecorator arguments",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1608,
      "chapter": null,
      "content": "The class decorator used here accepts any number of arguments to name private\nattributes. Again, though, this simply means that the arguments are passed to the\nPrivate function, and Private returns the decorator function to be applied to\nthe subject class. That is, the arguments are used before decoration ever occurs;\nPrivate returns the decorator, which in turn “remembers” the privates list as an\nenclosing-scope reference.\nState retention and enclosing scopes\nSpeaking of enclosing scopes, there are actually three levels of state retention at\nwork in this code:\nThe arguments to Private are used before decoration occurs and are\nretained as an enclosing-scope reference for use in both onDecorator\nand onInstance.\nThe class argument to onDecorator is used at decoration time and is\nretained as an enclosing-scope reference for use at instance-creation\ntime.\nThe wrapped instance object is retained as an instance attribute in the\nonInstance proxy object for use when attributes are later accessed\nfrom outside the class.\nThis all works fairly naturally, given Python’s scope and namespace rules.\nUsing __dict__ and __slots__ (and other virtuals)\nThe __setattr__ method in this code relies on an instance object’s __dict__\nattribute namespace dictionary in order to set onInstance’s own wrapped\nattribute. As we learned in the prior chapter, this method cannot assign an\nattribute directly without looping. However, it uses the setattr built-in instead\nof __dict__ to set attributes in the wrapped object itself. Moreover, getattr is\nused to fetch attributes in the wrapped object since they may be stored in the\nobject itself or inherited by it.\nBecause of that, this code will work for most classes—including those with\n“virtual” class-level attributes based on slots, properties, descriptors, and even\n__getattr__ and its ilk. By assuming a namespace dictionary for itself only and",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1609,
      "chapter": null,
      "content": "using storage-neutral tools for the wrapped object, the wrapper class avoids\nlimitations imposed by other tools.\nFor example, you may recall from Chapter 32 that classes with __slots__ may\nnot store attributes in a __dict__, and in fact, may not even have one of these at\nall. However, because we rely on a __dict__ only at the onInstance level here\nand not in the wrapped instance, this concern does not apply. Class onInstance\nwill have a __dict__ itself because it does not use slots. In addition, because\nsetattr and getattr apply to attributes based on both __dict__ and\n__slots__, our decorator applies to wrapped classes using either storage\nscheme.\nBy the same reasoning, the decorator also applies to properties and similar tools:\ndelegated names will be looked up anew in the wrapped instance, irrespective of\nattributes of the decorator proxy object itself.\nGeneralizing for Public Declarations\nNow that we have a Private attribute implementation, it’s straightforward to\ngeneralize the code to allow for Public declarations too—they are essentially\nthe inverse of Private declarations, so we need only negate the inner test. The\nexample listed in Example 39-18 allows a class to use decorators to define a set\nof either Private or Public instance attributes—attributes of any kind stored on\nan instance or inherited from its classes—with the following semantics:\nPrivate declares attributes of a class’s instances that cannot be fetched\nor assigned except from within the code of the class’s methods. That is,\nany name declared Private cannot be accessed from outside the class,\nwhile any name not declared Private can be freely fetched or assigned\nfrom outside the class.\nPublic declares attributes of a class’s instances that can be fetched or\nassigned from both outside the class and within the class’s methods.\nThat is, any name declared Public can be freely accessed anywhere,\nwhile any name not declared Public cannot be accessed from outside\nthe class.",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1610,
      "chapter": null,
      "content": "Private and Public declarations are mutually exclusive: when using Private,\nall undeclared names are considered Public, and when using Public, all\nundeclared names are considered Private. They are essentially opposites,\nthough undeclared names not created by a class’s methods behave slightly\ndifferently—new names can be assigned and thus created outside the class under\nPrivate (all undeclared names are accessible) but not under Public (all\nundeclared names are inaccessible).\nAgain, study this code on your own to get a feel for how this works. Notice that\nthis scheme adds an additional fourth level of state retention at the top, beyond\nthat described in the preceding section: the validation functions used by the\nlambdas are saved in an extra enclosing scope coded separately. This version\ncomes with the same caveat as its predecessor for attributes of built-in\noperations, noted in the file’s docstring and expanded on after the example.\nExample 39-18. access2.py\n\"\"\"\nClass decorator with Private and Public attribute declarations.\nControls external access to attributes stored on an instance, or\ninherited by it from its classes.  Private declares attribute names\nthat cannot be fetched or assigned outside the decorated class,\nand Public declares all the names that can.  Choose either decorator.\nCaveat: as is, this works for explicitly-named attributes only.  The\n__X__ operator-overloading methods fetched implicitly for built-in \noperations do not trigger either __getattr__ or __getattribute__, and\nhence won't be delegated to any wrapped objects that define them.  If \nneeded, add __X__ methods to catch and delegate built-ins (per ahead).\n\"\"\"\ntraceMe = False\ndef trace(*args):\n   if traceMe: print('[' + ' '.join(map(str, args)) + ']')\ndef accessControl(failIf):\n   def onDecorator(aClass):\n       class onInstance:\n           def __init__(self, *args, **kargs):\n               self.__wrapped = aClass(*args, **kargs)\n           def __getattr__(self, attr):\n               trace('get:', attr)\n               if failIf(attr):",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1611,
      "chapter": null,
      "content": "raise TypeError('private attribute fetch, ' + attr)\n               else:\n                   return getattr(self.__wrapped, attr)\n           def __setattr__(self, attr, value):\n               trace('set:', attr, value)\n               if attr == '_onInstance__wrapped':\n                   self.__dict__[attr] = value\n               elif failIf(attr):\n                   raise TypeError('private attribute change, ' + attr)\n               else:\n                   setattr(self.__wrapped, attr, value)\n       return onInstance\n   return onDecorator\ndef Private(*attributes):\n   return accessControl(failIf=(lambda attr: attr in attributes))\ndef Public(*attributes):\n   return accessControl(failIf=(lambda attr: attr not in attributes))\nSee the prior example’s self-test code for a usage example—the effect is the\nsame for Private. Here’s a quick look at these class decorators in action at the\ninteractive prompt. As advertised, non-Private or Public names can be fetched\nand changed from outside the subject class, but Private or non-Public names\ncannot:\n>>> from access2 import Private, Public\n>>> @Private('age')                             # Person = Private('age')(Person)\n... class Person:                               # Person = onInstance with state\n        def __init__(self, name, age):\n            self.name = name\n            self.age  = age                     # Inside accesses run normally\n \n>>> X = Person('Pat', 40)\n>>> X.name                                      # Outside accesses validated\n'Pat'\n>>> X.name = 'Sue'\n>>> X.name\n'Sue'\n>>> X.age\nTypeError: private attribute fetch, age\n>>> X.age = 'Bob'\nTypeError: private attribute change, age\n>>> @Public('name')",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1612,
      "chapter": null,
      "content": "... class Person:\n        def __init__(self, name, age):\n            self.name = name\n            self.age  = age\n \n>>> X = Person('Pat', 40)                       # X is an onInstance\n>>> X.name                                      # onInstance embeds Person\n'Pat'\n>>> X.name = 'Sue'\n>>> X.name\n'Sue'\n>>> X.age\nTypeError: private attribute fetch, age\n>>> X.age = 'Bob'\nTypeError: private attribute change, age\nImplementation Details II\nTo help you analyze Example 39-18’s code, here are a few final notes on this\nversion. Since this is just a generalization of the preceding section’s version, the\nimplementation notes there apply here as well.\nUsing “__X” pseudoprivate names\nBesides generalizing, this version also makes use of Python’s __X pseudoprivate\nname mangling feature, which we met in Chapter 31, to localize the wrapped\nattribute to the proxy control class by automatically prefixing it with this class’s\nname. This avoids the prior version’s risk for collisions with a wrapped attribute\nthat may be used by the real, wrapped class, and it’s useful in a general tool like\nthis. It’s not quite “privacy,” though, because the mangled version of the name\ncan be used freely outside the class. Notice that we also have to use the fully\nexpanded name string—'_onInstance__wrapped'—as an admin-name test\nvalue in __setattr__ because that’s what Python changes it to.\nBreaking privacy\nAlthough this example does implement access controls for attributes of an\ninstance and its classes, it is possible to subvert these controls trivially—for\ninstance, by fetching through the expanded version of the wrapped attribute\nexplicitly (bob.pay might not work, but the fully mangled\nbob._onInstance__wrapped.pay could!). If you have to try that hard to break",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1613,
      "chapter": null,
      "content": "them, though, these tools probably suffice for intended roles. Of course, privacy\ncan generally be subverted in other languages too (e.g., #define private\npublic may work in some C++ implementations). Although access controls may\nreduce accidental mods, much of this is up to programmers in any language;\nwhenever source code may be changed, airtight access control will always be a\npipe dream. More fundamentally, Python is about enabling, not controlling;\nprivacy is a tool best used sparingly (if at all).\nDecorator trade-offs\nWe could again achieve the same results without decorators by using helper\nfunctions or coding the name rebinding of decorators manually; the decorator\nsyntax, however, makes this consistent and obvious in code. The chief potential\ndownsides of this and any other wrapper-based approach are that attribute access\nincurs an extra call, and instances of decorated classes are not really instances of\nthe original decorated class—if you test their type with X.__class__ or\nisinstance(X, C), for example, you’ll find that they are instances of the\nwrapper class. Unless you plan to do introspection on objects’ types, though, the\ntype issue is irrelevant, and the extra call may apply mostly to development time;\nas you’ll see later, it’s possible to remove decorations automatically (via -O) if\ndesired.\nDelegating Built-In Operations\nAs is, this section’s examples work as planned for methods and other attributes\nfetched explicitly by name. As with most software, though, there is always room\nfor improvement. Most notably, this tool turns in mixed performance on\noperator-overloading methods if they are used by client classes.\nSpecifically, the proxy class fails to validate or delegate operator-overloading\nmethods fetched implicitly by built-in operations unless such methods are\nredefined in the proxy. Clients that do not use operator overloading are fully\nsupported, but others may require additional code. It’s unclear that operator-\noverloading methods should be validated as private or public, but they are a part\nof an object’s interface and should at least be routed to wrapped objects that\ndefine them.",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1614,
      "chapter": null,
      "content": "We’ve encountered this issue a few times already in this book, but let’s take a\nquick look at its impact on the realistic code we’ve written here and explore\nworkarounds for it. The basic issue is easy to demo—as we’ve learned, the\nfollowing is how a class that overloads print calls and + expressions normally\nworks:\n>>> class Tally:\n        def __init__(self):\n            self.sum = 0\n        def __str__(self):\n            return f'Tally: {self.sum}'\n        def __add__(self, add):\n            self.sum += add\n \n>>> X = Tally()\n>>> X.sum             # All attributes accessible\n0\n>>> print(X)          # Same as X.__str__() {sort of}\nTally: 0\n>>> X + 5             # Same as X.__add__(5) {ditto}\n>>> print(X)\nTally: 5\nUnfortunately, objects that implement built-in operations like this fail in our\nproxy classes because built-in operations skip instance-level lookup protocols\nlike __getattr__, and instead search namespaces of classes:\n>>> from access2 import Private\n>>> @Private('sum', '__add__')\n... class Tally:\n        …same as before…\n>>> X = Tally()\n>>> X.sum             \nTypeError: private attribute fetch, sum\n>>> X.__add__(5)\nTypeError: private attribute fetch, __add__\n>>> print(X)    \n<access2.accessControl.<locals>.onDecorator.<locals>.onInstance object at 0x…etc…>\n \n>>> X + 5             \nTypeError: unsupported operand type(s) for +: 'onInstance' and 'int'",
      "content_length": 1380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1615,
      "chapter": null,
      "content": "In this session, the first two explicit fetches of sum and __add__ are kicked out\nas privates as they should be. Because the last two implicit fetches of print and\n+ aren’t caught by the proxy, though, they are never delegated to the wrapped\nTally object. The print here only works at all because it runs an object\ndefault to print the proxy itself. Per the prior chapter, this is an inconsistency in\nPython; per the following sections, it can also be avoided in full.\nWorkaround: Coding operator-overloading methods inline\nThe most straightforward way to support built-ins in delegation proxies is to\nredefine operator-overloading names that may appear in embedded objects. This\ncreates some code redundancy, but it isn’t impossibly onerous; can be automated\nwith tools or superclasses; and can choose to run or skip validations for operator-\noverloading names declared Private or Public, depending on redefinitions’\nrouting.\nFor instance, the partial listing of Example 39-19 sketches an inline redefinition\napproach—it catches and delegates built-ins by adding method definitions to the\nproxy itself for every operator-overloading method a wrapped object may define.\nIt adds just four operation interceptors to illustrate, but others are similar (in this\nsection, new code is in bold font, and all examples are based on the decorator of\naccess2.py in Example 39-18).\nExample 39-19. access_builtins_inline_direct.py\n\"Inline methods, skip validations\"\ndef accessControl(failIf):\n   def onDecorator(aClass):\n       class onInstance:\n           def __init__(self, *args, **kargs):\n               self.__wrapped = aClass(*args, **kargs)\n           # Intercept and delegate built-in implicit access specifically\n           def __add__(self, other):\n               return self.__wrapped + other           # Or getattr(), __getattr__()\n           def __str__(self):\n               return str(self.__wrapped)              # Or self.__wrapped.__str__()\n           def __getitem__(self, index):\n               return self.__wrapped[index]\n           def __call__(self, *args, **kargs):\n               return self.__wrapped(*args, **kargs)",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1616,
      "chapter": null,
      "content": "# Plus any others needed\n           # Intercept and delegate explicit attribute access generically\n           def __getattr__(self, attr): …same…\n           def __setattr__(self, attr, value): …same…\n       return onInstance\n   return onDecorator\nThis works because built-ins will find their requisite methods in the proxy class\nafter skipping the proxy instance. As coded, the new interceptor methods trigger\nthe wrapped object’s operator-overloading methods directly and so bypass the\naccess controls of __getattr__, which may or may not be desirable. For\nalternative codings, let’s move on.\nWorkaround: Coding operator-overloading methods in superclasses\nMore usefully, the prior section’s added methods can be added by a common\nsuperclass. Given that there are dozens of such methods, an external class may\nbe better suited to the task, especially if it is general enough to be used in any\nsuch interface-proxy class.\nTo demo, the superclass of Example 39-20 catches built-ins and reroutes to the\nwrapped object directly again. It’s largely just a repackaging of the prior\nsection’s inline scheme, but as a separate class it requires a proxy attribute\nnamed _wrapped, giving access to the embedded object. The decorator itself\nmust use this name instead of __wrapped in self references, and sans mangling\nin __setattr__. This may be subpar because it precludes the same name in\nwrapped objects and creates a subclass dependency, but it’s better than using the\nmangled and subclass-specific _onInstance__wrapped and is no worse than a\nsimilarly named method.\nExample 39-20. access_builtins_mixin_direct.py\n\"Inherit methods, skip validations\"\nclass BuiltinsMixin:\n   def __add__(self, other):\n       return self._wrapped + other                          # Assume a _wrapped\n   def __str__(self):                                        # Bypass __getattr__\n       return str(self._wrapped)\n   def __getitem__(self, index):\n       return self._wrapped[index]",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1617,
      "chapter": null,
      "content": "def __call__(self, *args, **kargs):\n       return self._wrapped(*args, **kargs)\n   # Plus any others needed\ndef accessControl(failIf):\n   def onDecorator(aClass):\n       class onInstance(BuiltinsMixin):\n           …rest same, but use unmangled _wrapped instead of __wrapped…\nAlternatively, the superclass in Example 39-21 catches built-ins and reroutes\nthem down through the subclass __getattr__ to apply its access controls to the\noperation’s method name. It requires that operator-overloading names be non-\nPrivate or Public per the decorator’s arguments if they are to be run, but it\ntreats the implicit fetches of built-in operations the same as explicit-name\nfetches, and no _wrapped is required in subclasses.\nExample 39-21. access_builtins_mixin_getattr.py\n\"Inherit methods, run validations\"\nclass BuiltinsMixin:\n   def __add__(self, other):\n       return self.__getattr__('__add__')(other)             # Route to validator\n   def __str__(self):                                        # Finish operations\n       return self.__getattr__('__str__')()\n   def __getitem__(self, index):\n       return self.__getattr__('__getitem__')(index)\n   def __call__(self, *args, **kargs):\n       return self.__getattr__('__call__')(*args, **kargs)\n   # Plus any others needed\ndef accessControl(failIf):\n   def onDecorator(aClass):\n       class onInstance(BuiltinsMixin):                      # Inherit methods\n           …rest unchanged…\nLike the inline approach, both of these mix-ins also require one method per\nbuilt-in operation in general tools that proxy arbitrary objects’ interfaces. The\nnext idea does marginally better.\nWorkaround: Generating operator-overloading descriptors\nFinally, all of the inline and mix-in workarounds for built-ins we’ve seen so far\ncode each operator-overloading method explicitly, and intercept the actual call\nissued for the operation, including its arguments. That makes them responsible",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1618,
      "chapter": null,
      "content": "for completing the operation, whether by operation syntax or equivalent calls.\nWith an alternative coding, we could instead intercept only the attribute fetch\npreceding the call by using the class-level descriptors of the prior chapter.\nMoreover, because all such descriptors will run the same, they can be generated\nautomatically from a list of method names. Example 39-22 shows one way to\ncode this scheme. Like Example 39-21, it routes built-in operations through the\ndecorator’s validations logic to apply private or public checks.\nExample 39-22. access_builtins_mixin_desc.py\n\"Inherit descriptors, run validations\"\nclass BuiltinsMixin:\n   class ProxyDesc:                                          # Define descriptor\n       def __init__(self, attrname):\n           self.attrname = attrname\n       def __get__(self, instance, owner):\n           return instance.__getattr__(self.attrname)        # Run validations\n   builtins = ['add', 'str', 'getitem', 'call']              # Plus any others\n   for attr in builtins:\n       exec(f'__{attr}__ = ProxyDesc(\"__{attr}__\")')         # Make descriptors\ndef accessControl(failIf):\n   def onDecorator(aClass):\n       class onInstance(BuiltinsMixin):                      # Inherit descriptors\n           …rest unchanged…\nThis coding may be the most concise but also the most implicit and complex.\nRecall that the exec built-in by default runs a string of code as if the string was\nsomehow pasted where the exec appears. Hence, the loop at the end of this mix-\nin class is equivalent to the following statements, run in the mix-in class’s local\nscope:\n    __add__ = ProxyDesc(\"__add__\")\n    __str__ = ProxyDesc(\"__str__\")\n    …etc…\nThe net effect creates inherited descriptor instances that respond to initial name\nlookups by fetching from the wrapped object in __get__ rather than catching the\nlater operation call itself (which happens after this step). If you still find this\ncode confusing (and you probably should), it’s equivalent to this stripped-down",
      "content_length": 2003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1619,
      "chapter": null,
      "content": "version, though the name fetch occurs implicitly in a built-in operation that skips\nthe instance’s protocols:\n>>> class B:\n        class D:\n            def __get__(s, i, o): return i.meth()\n        name = D()\n \n>>> class A(B):\n        def meth(self): return 'hack'\n \n>>> I = A()\n>>> I.name\n'hack'\nWe could also skip the decorator’s validations for built-in operations in this\nscheme by routing attribute fetches directly to the wrapped object—though this\nrequires an accessible _wrapped in the decorator just like Example 39-20:\n    class ProxyDesc:                                             \n        …\n        def __get__(self, instance, owner):\n            return getattr(instance._wrapped, self.attrname)     # Assume a _wrapped\nIn the end, all of these workarounds make classes that overload built-in\noperations work correctly with our private and public decorators—and other\ndelegation-based decorators like them:\n>>> from access_builtins_mixin_desc import Private\n>>> @Private('sum')\n... class Tally: \n        def __init__(self):\n            self.sum = 0\n        def __str__(self):\n            return f'Tally: {self.sum}'\n        def __add__(self, add):\n            self.sum += add\n>>> X = Tally()\n>>> X.sum                                          # Explicit validated\nTypeError: private attribute fetch, sum\n>>> X + 10                                         # Built-in delegated\n>>> print(X)                                       # Built-in delegated",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1620,
      "chapter": null,
      "content": "Tally: 10\nPublic (nonprivate) built-ins are now delegated and work, but private built-ins\nare validated and canceled:\n>>> @Private('sum', '__add__')\n... class Tally: \n        …same as before…\n>>> X = Tally()\n>>> X.sum                                          # Explicit validated\nTypeError: private attribute fetch, sum\n>>> X + 10                                         # Built-in canceled: private\nTypeError: private attribute fetch, __add__\n>>> print(X)                                       # Built-in allowed: public\nTally: 0\n>>> @Private('__str__')\n... class Tally: \n        …same as before…\n>>> print(Tally())                                 # Built-in canceled: private\nTypeError: private attribute fetch, __str__\nIf you care to experiment further with this section’s examples, see the book\nexamples package for their complete code, as well as its comprehensive\naccess_builtins_TEST.py test script and results. Here, it’s time to move on to this\nchapter’s next and final decorators case study.\nExample: Validating Function Arguments\nAs a final example of the utility of decorators, this section develops a function\ndecorator that automatically tests whether arguments passed to a function or\nmethod are within a valid numeric range. It’s designed to be used during either\ndevelopment or production, and it can be used as a template for similar tasks\n(e.g., argument type testing, if you must). Again, this example is largely self-\nstudy content with a limited narrative; read the code for more details.\nThe Goal\nIn the object-oriented tutorial of Chapter 28, we wrote a class that gave a pay",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1621,
      "chapter": null,
      "content": "raise to objects representing fictitious people, based upon a passed-in\npercentage:\nclass Person:\n     …\n     def giveRaise(self, percent):\n        self.pay = int(self.pay * (1 + percent))\nThere, we noted that if we wanted the code to be robust, it would be a good idea\nto check the percentage to make sure it’s not too large or too small. We could\nimplement such a check with either if or assert statements in the method itself,\nusing inline tests:\nclass Person:\n    def giveRaise(self, percent):                # Validate with inline code\n        if percent < 0.0 or percent > 1.0:\n            raise TypeError, 'percent invalid'\n        self.pay = int(self.pay * (1 + percent))\nclass Person:                                    # Validate with asserts\n    def giveRaise(self, percent):\n        assert percent >= 0.0 and percent <= 1.0, 'percent invalid'\n        self.pay = int(self.pay * (1 + percent))\nHowever, this approach clutters the method with inline tests that will probably\nbe useful only during development. For more complex cases, this can become\ntedious (imagine trying to inline the code needed to implement the attribute\nprivacy provided by the last section’s decorator). Perhaps worse, if the validation\nlogic ever needs to change, there may be arbitrarily many inline copies to find\nand update.\nA more useful and interesting alternative would be to develop a general tool that\ncan perform range tests for us automatically for the arguments of any function or\nmethod we might code now or in the future. A decorator approach makes this\nexplicit and convenient, and easy to disable once development is complete:\nclass Person:\n    @rangetest(percent=(0.0, 1.0))               # Use decorator to validate\n    def giveRaise(self, percent):\n        self.pay = int(self.pay * (1 + percent))",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1622,
      "chapter": null,
      "content": "Isolating validation logic in a decorator simplifies both clients and future\nmaintenance.\nNotice that our goal here is different than the attribute validations coded in the\nprior chapter’s final example. Here, we mean to validate the values of function\narguments when passed rather than attribute values when accessed. Python’s\ndecorator and introspection tools allow us to code this new task just as easily.\nA Basic Range-Testing Decorator for Positional Arguments\nLet’s start with a basic range-test implementation. To keep things simple, we’ll\nbegin by coding a decorator that works only for positional arguments and\nassumes they always appear at the same position in every call; they cannot be\npassed by keyword name because this can invalidate the positions declared in the\ndecorator. Example 39-23 is our first-cut checker.\nExample 39-23. rangetest0.py\ndef rangetest(*argchecks):                  # Validate positional arg ranges\n   def onDecorator(func):\n       if not __debug__:                   # True if \"python -O main.py args...\"\n           return func                     # No-op: call original directly\n       else:                               # Else wrapper while debugging\n           def onCall(*args):\n               for (ix, low, high) in argchecks:\n                   if args[ix] < low or args[ix] > high:\n                       errmsg = f'Argument {ix} not in {low}..{high}'\n                       raise TypeError(errmsg)\n               return func(*args)\n           return onCall\n   return onDecorator\nAs is, this code is mostly a rehash of the coding patterns we explored earlier: we\nuse decorator arguments, nested scopes for state retention, and so on.\nWe also use nested def statements to ensure that this works for both simple\nfunctions and methods, as we learned earlier. When used for a class’s method,\nonCall receives the subject class’s instance in the first item in *args and passes\nthis along to self in the original method function; explicitly passed argument\nnumbers coded in the @ decorator line start at 1 in this case, not 0, to\naccommodate the implicit self.\nNew here, notice this code’s use of the __debug__ built-in variable introduced in",
      "content_length": 2181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1623,
      "chapter": null,
      "content": "Chapter 34. In brief, Python sets this variable to True unless the program is\nbeing run with the –O optimize command-line flag (e.g., python –O main.py).\nAs discussed earlier, using options in the compile built-in function and\ncompileall standard-library module before code is run can have a similar\neffect.\nEither way, when __debug__ is False, the decorator returns the original\nfunction unchanged to avoid extra later calls and their associated performance\npenalty. In other words, the decorator automatically removes its augmentation\nlogic when –O or similar is used without requiring you to physically remove the\ndecoration lines in your code.\nExample 39-24 demos how this first-iteration solution is used.\nExample 39-24. rangetest0_test.py\nfrom rangetest0 import rangetest\nprint(f'{__debug__=}')                     # False if \"python -O main.py\"\n@rangetest((1, 0, 120))                    # persinfo = rangetest(...)(persinfo)\ndef persinfo(name, age):                   # age must be in 0..120\n   print(f'{name} is {age} years old')\n@rangetest([0, 1, 12], [1, 1, 31], [2, 0, 2024])\ndef birthday(M, D, Y):\n   print(f'birthday = {M}/{D}/{Y}')\nclass Person:\n   def __init__(self, name, job, pay):\n       self.job  = job\n       self.pay  = pay\n   @rangetest([1, 0.0, 1.0])              # giveRaise = rangetest(...)(giveRaise)\n   def giveRaise(self, percent):          # Arg 0 is the self instance here\n       self.pay = int(self.pay * (1 + percent))\n# Comment lines raise TypeError unless \"python -O\" used on shell command line\npersinfo('Bob Smith', 45)                  # Really runs onCall(...) with state\n#persinfo('Bob Smith', 200)                # Or persinfo if -O cmd line argument\nbirthday(8, 31, 2024)\n#birthday(8, 32, 2024)\nsue = Person('Sue Jones', 'dev', 100_000)\nsue.giveRaise(.10)                         # Really runs onCall(self, .10)",
      "content_length": 1852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1624,
      "chapter": null,
      "content": "print(sue.pay)                             # Or giveRaise(self, .10) if -O\n#sue.giveRaise(1.10)\nWhen run, valid calls in this code produce the following output:\n$ python3 rangetest0_test.py\n__debug__=True\nBob Smith is 45 years old\nbirthday = 8/31/2024\n110000\nUncommenting any of the invalid calls causes a TypeError to be raised by the\ndecorator. Here’s the result when the last line is allowed to run (as usual, some of\nthe error message text was trimmed here to save space):\n$ python3 rangetest0_test.py\n__debug__=True\nBob Smith is 45 years old\nbirthday = 8/31/2024\n110000\nTypeError: Argument 1 not in 0.0..1.0\nRunning Python with its -O flag at a system command line will disable range\ntesting but also avoid the performance overhead of the wrapping layer—we wind\nup calling the original undecorated function directly. Assuming this is a\ndebugging tool only, you can use this flag to optimize your program for\nproduction use. Here is the effect with the last line still run and a print added to\nshow sue’s fantastical pay raise:\n$ python3 -O rangetest0_test.py\n__debug__=False\nBob Smith is 45 years old\nbirthday = 8/31/2024\n110000\n231000\nGeneralizing for Keywords and Defaults\nThe prior version illustrates the basics we need to employ, but it’s fairly limited\n—it supports validating arguments passed by position only, and it does not\nvalidate keyword arguments (in fact, it assumes that no keywords are passed in a",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1625,
      "chapter": null,
      "content": "way that makes argument position numbers incorrect). Additionally, it does\nnothing about arguments with defaults that may be omitted in a given call. That’s\nfine if all your arguments are passed by position and never defaulted, but it’s less\nthan ideal in a general tool. As we learned in Chapter 18, Python supports much\nmore flexible argument-passing modes, which we’re not yet addressing.\nThe level up of our decorator in Example 39-25 does better. By matching the\nwrapped function’s expected arguments against the actual arguments passed in a\ncall, it supports range validations for arguments passed by either position or\nkeyword name, and it skips testing for default arguments omitted in the call.\nArguments to be validated are specified by keyword arguments to the decorator\nitself, which later steps through both the call’s *pargs positionals tuple and its\n**kargs keywords dictionary to validate.\nExample 39-25. rangetest.py\n\"\"\"\nA function decorator that performs range-test validation for\narguments passed to any function or method.  Usage synopsis:\n   @rangetest(percent=(0.0, 1.0), month=(1, 12))\n   def func-or-method(..., percent, ..., month=5, ...):\n       ...\n   func-or-method(..., value, month=8, ...)\nArguments are specified by keyword to the decorator. In the actual\ncall, arguments may be passed by position or keyword, and defaults\nmay be omitted.  See rangetest_test.py for example use cases.\n\"\"\"\ntrace = True\ndef rangetest(**argchecks):                 # Validate ranges for both+defaults\n   def onDecorator(func):                  # onCall remembers func and argchecks\n       if not __debug__:                   # True if \"python -O main.py args...\"\n           return func                     # Wrap if debugging; else use original\n       else:\n           funcname = func.__name__\n           funccode = func.__code__\n           funcargs = funccode.co_varnames[:funccode.co_argcount]\n           def onCall(*pargs, **kargs):\n               # All pargs match first N expected args by position\n               # The rest must be in kargs or be omitted defaults\n               positionals = funcargs[:len(pargs)]\n               errormsg    = lambda *args: '%s argument \"%s\" not in %s..%s' % args",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1626,
      "chapter": null,
      "content": "for (argname, (low, high)) in argchecks.items():\n                   # For all args to be checked\n                   if argname in kargs:\n                       # Was passed by name\n                       if kargs[argname] < low or kargs[argname] > high:\n                           raise TypeError(errormsg(funcname, argname, low, high))\n                   elif argname in positionals:\n                       # Was passed by position\n                       position = positionals.index(argname)\n                       if pargs[position] < low or pargs[position] > high:\n                           raise TypeError(errormsg(funcname, argname, low, high))\n                   else:\n                       # Assume not passed: default\n                       if trace:\n                           print(f'-Argument \"{argname}\" defaulted')\n               return func(*pargs, **kargs)    # OK: run original call\n           return onCall\n   return onDecorator\nNext, the test script in Example 39-26 shows how the decorator is used—\narguments to be validated are given by keyword decorator arguments, and at\nactual calls, we can pass by name or position and omit arguments with defaults\neven if they are to be validated otherwise.\nExample 39-26. rangetest_test.py\n\"\"\"\nTest the rangetest decorator (usage differs from rangetest0).\nComment lines raise TypeError unless \"python -O\" or similar in compileall.\n\"\"\"\nfrom rangetest import rangetest\ndef announce(what): print(what.center(24, '-'))   # str method\n# Test functions, positional and keyword\nannounce('Functions')\n@rangetest(age=(0, 120))                  # persinfo = rangetest(...)(persinfo)\ndef persinfo(name, age):\n   print(f'{name} is {age} years old')\n@rangetest(M=(1, 12), D=(1, 31), Y=(0, 2024))\ndef birthday(M, D, Y):\n   print(f'birthday = {M}/{D}/{Y}')\npersinfo('Pat', 40)\npersinfo(age=40, name='Pat')",
      "content_length": 1852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1627,
      "chapter": null,
      "content": "birthday(8, D=31, Y=2024)\n#persinfo('Pat', 150)\n#persinfo(age=150, name='Pat')\n#birthday(8, Y=2025, D=40)\n# Test methods, positional and keyword\nannounce('Methods')\nclass Person:\n   def __init__(self, name, job, pay):\n       self.job  = job\n       self.pay  = pay\n                                         # giveRaise = rangetest(...)(giveRaise)\n   @rangetest(percent=(0.0, 1.0))        # percent passed by name or position\n   def giveRaise(self, percent):\n       self.pay = int(self.pay * (1 + percent))\nsue = Person('Sue Jones', 'dev', 100_000)\nbob = Person('Bob Smith', 'dev', 100_000)\nsue.giveRaise(percent=.20)\nbob.giveRaise(.10)\nprint(f'sue=>{sue.pay}, bob=>{bob.pay}')\n#sue.giveRaise(1.20)\n#bob.giveRaise(percent=1.20)\n# Test omitted defaults: skipped\nannounce('Defaults')\n@rangetest(a=(1, 10), b=(1, 10), c=(1, 10), d=(1, 10))\ndef omitargs(a, b=7, c=8, d=9):\n   print(a, b, c, d)\nomitargs(1, 2, 3, 4)           # Positionals\nomitargs(1, 2, 3)              # Default d\nomitargs(1, 2, 3, d=4)         # Keyword d\nomitargs(1, d=4)               # Default b and c\nomitargs(d=4, a=1)             # Ditto\nomitargs(1, b=2, d=4)          # Default c\nomitargs(d=8, c=7, a=1)        # Default b\n#omitargs(1, 2, 3, 11)         # Bad d\n#omitargs(1, 2, 11)            # Bad c\n#omitargs(1, 2, 3, d=11)       # Bad d\n#omitargs(11, d=4)             # Bad a\n#omitargs(d=4, a=11)           # Bad a\n#omitargs(1, b=11, d=4)        # Bad b\n#omitargs(d=8, c=7, a=11)      # Bad a\nWhen this script is run, out-of-range arguments raise an exception as before, but",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1628,
      "chapter": null,
      "content": "arguments may be passed by either name or position, and omitted defaults are\nnot validated. Trace its output and test this further on your own to experiment; it\nworks like its simpler predecessor, but its scope has been greatly broadened:\n$ python3 rangetest_test.py\n-------Functions--------\nPat is 40 years old\nPat is 40 years old\nbirthday = 8/31/2024\n--------Methods---------\nsue=>120000, bob=>110000\n--------Defaults--------\n1 2 3 4\n-Argument \"d\" defaulted\n1 2 3 9\n1 2 3 4\n-Argument \"b\" defaulted\n-Argument \"c\" defaulted\n1 7 8 4\n-Argument \"b\" defaulted\n-Argument \"c\" defaulted\n1 7 8 4\n-Argument \"c\" defaulted\n1 2 8 4\n-Argument \"b\" defaulted\n1 7 7 8\nNotice that argument checks are run in the order they are listed in the decorator\nbecause Python retains insertion order in dictionaries. On validation errors, we\nget an exception as before unless the -O command-line argument is passed to\nPython to disable the decorator’s logic. Here’s the scene when one of the\nmethod-test lines is uncommented:\n$ python3 rangetest_test.py\n-------Functions--------\nPat is 40 years old\nPat is 40 years old\nbirthday = 8/31/2024\n--------Methods---------\nsue=>120000, bob=>110000\nTypeError: giveRaise argument \"percent\" not in 0.0..1.0\n$ python3 -O rangetest_test.py\n…no error messages or default traces…",
      "content_length": 1287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1629,
      "chapter": null,
      "content": "Implementation Details\nThis range-tester decorator’s code relies on both introspection APIs and subtle\nconstraints of argument passing. To be fully general, we could try to mimic\nPython’s argument-matching logic in its entirety to see which names have been\npassed in which modes, but that’s too much complexity for our tool and is prone\nto change over time. It would be better if we could somehow match the names of\ntestable arguments given to the decorator against the names of actual arguments\nexpected by the function to determine how the former map to the latter during a\ngiven call.\nFunction introspection\nIt turns out that the introspection API available on function objects and their\nassociated code objects has exactly the tool we need. This API was briefly\nintroduced in Chapter 19, but we’ve actually put it to use here. The set of\nexpected argument names is simply the first N variable names attached to a\nfunction’s code object:\n>>> def func(a, b, c, e=True, f=None):       # Args: three required, two defaults\n        x = 1                                # Plus two more local variables\n        y = 2\n>>> code = func.__code__                     # Code object of function object\n>>> code.co_nlocals\n7\n>>> code.co_varnames                         # All local variable names\n('a', 'b', 'c', 'e', 'f', 'x', 'y')\n>>> code.co_varnames[:code.co_argcount]      # <== First N locals are expected args\n('a', 'b', 'c', 'e', 'f')\nAnd as usual, starred-argument names in the call proxy allow it to collect\narbitrarily many arguments to be matched against the expected arguments so\nobtained from the function’s introspection API:\n>>> def catcher(*pargs, **kargs): print(f'{pargs}, {kargs}')\n>>> catcher(1, 2, 3, 4, 5)\n(1, 2, 3, 4, 5), {}\n>>> catcher(1, 2, c=3, d=4, e=5)             # Arguments at calls\n(1, 2), {'d': 4, 'e': 5, 'c': 3}",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1630,
      "chapter": null,
      "content": "Run a dir call on function and code objects for more details.\nArgument assumptions\nGiven the decorated function’s set of expected argument names, the solution\nrelies upon two constraints on argument passing order imposed by Python and\ncovered in Chapter 18:\nAt the call, all positional arguments appear before all keyword\narguments.\nIn the def, all nondefault arguments appear before all default\narguments.\nThat is, a nonkeyword argument cannot generally follow a keyword argument at\na call, and a nondefault argument cannot follow a default argument at a\ndefinition. All name=value syntax must appear after any simple name in both\nplaces. As we’ve also learned, Python matches argument values passed by\nposition to argument names in function headers from left to right, such that these\nvalues always match the leftmost names in headers. Keywords match by name\ninstead, and a given argument can receive only one value.\nTo simplify our work, we can also make the assumption that a call is valid in\ngeneral—that is, that all arguments either will receive values (by name or\nposition) or will be omitted intentionally to pick up defaults. This assumption\nwon’t necessarily hold because the function has not yet actually been called\nwhen the wrapper logic tests validity—the call may still fail later when invoked\nby the wrapper layer due to incorrect argument passing. As long as that doesn’t\ncause the wrapper to fail any worse, though, we can ignore the validity of the\ncall. This helps because validating calls before they are actually made would\nrequire us to emulate Python’s argument-matching algorithm in full.\nMatching algorithm\nNow, given these constraints and assumptions, we can allow for both keywords\nand omitted default arguments in the call with this algorithm. When a call is\nintercepted, we can make the following assumptions and deductions:\n1. Let N be the number of passed positional arguments, obtained from the",
      "content_length": 1928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1631,
      "chapter": null,
      "content": "length of the *pargs tuple.\n2. All N positional arguments in *pargs must match the first N expected\narguments obtained from the function’s code object. This is true per\nPython’s call ordering rules, outlined earlier, since all positionals\nprecede all keywords in a call.\n3. To obtain the names of arguments actually passed by position, we can\nslice the list of all expected arguments up to the length N of the *pargs\npassed positionals tuple.\n4. Any arguments after the first N expected arguments either were passed\nby keyword or were defaulted by omission at the call.\n5. For each argument name to be validated by the decorator:\na. If the name is in **kargs, it was passed by name—indexing\n**kargs gives its passed value.\nb. If the name is in the first N expected arguments, it was passed\nby position—its relative position in the expected list gives its\nrelative position in *pargs.\nc. Otherwise, we can assume it was omitted in the call and\ndefaulted and need not be checked.\nIn other words, we can skip tests for arguments that were omitted in a call by\nassuming that the first N actually passed positional arguments in *pargs must\nmatch the first N argument names in the list of all expected arguments, and that\nany others must either have been passed by keyword and thus be in **kargs, or\nhave been defaulted. Under this scheme, the decorator will simply skip any\nargument to be checked that was omitted between the rightmost positional\nargument and the leftmost keyword argument, between keyword arguments, or\nafter the rightmost positional in general. Trace through the decorator and its test\nscript to see how this is realized in code.\nOpen Issues\nAlthough our range-testing tool works as planned, three caveats remain—it",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1632,
      "chapter": null,
      "content": "doesn’t detect invalid calls, doesn’t handle some arbitrary-argument signatures,\nand doesn’t fully support nesting. Improvements may require extension or\naltogether different approaches. Here’s a quick rundown of the issues.\nInvalid calls\nFirst, as mentioned earlier, calls to the original function that are not valid still fail\nin our final decorator. The following, for example, both trigger TypeError\nexceptions for a missing positional argument a:\nomitargs()\nomitargs(d=8, c=7, b=6)\nThese only fail, though, where we try to invoke the original function, at the end\nof the wrapper. While we could try to imitate Python’s argument matching to\navoid this, there’s not much reason to do so—since the call would fail at this\npoint anyhow, we might as well let Python’s own argument-matching logic\ndetect the problem for us.\nArbitrary arguments\nSecond, although our final version handles positional arguments, keyword\narguments, and omitted defaults, it still doesn’t do anything explicit about\n*pargs and **kargs starred-argument names that may be used in a decorated\nfunction def that accepts arbitrarily many arguments itself. This is probably\nmoot for our purposes, though:\nIf an extra keyword argument is passed, its name will show up in\n**kargs and can be tested normally if mentioned to the decorator.\nIf an extra keyword argument is not passed, its name won’t be in either\n**kargs or the sliced expected positionals list, and it will thus not be\nchecked—it is treated as though it were defaulted, even though it is\nreally an optional extra argument.\nIf an extra positional argument is passed, there’s no way to reference it\nin the decorator anyhow—its name won’t be in either **kargs or the\nsliced expected arguments list, so it will simply be skipped. Because\nsuch arguments are not listed in the function’s definition, there’s no way",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1633,
      "chapter": null,
      "content": "to map a name given to the decorator back to an expected relative\nposition.\nIn other words, as it is the code supports testing arbitrary keyword arguments by\nname, but not arbitrary positionals that are unnamed and hence have no set\nposition in the function’s argument signature. In terms of the function object’s\nAPI, here’s the effect of these tools in decorated functions:\n>>> def func(*pargs, **kargs): pass\n>>> code = func.__code__\n>>> code.co_nlocals, code.co_varnames\n(2, ('pargs', 'kargs'))\n>>> code.co_argcount, code.co_varnames[:code.co_argcount]\n(0, ())\n>>> def func(a, b, *pargs, **kargs): pass\n>>> code = func.__code__\n>>> code.co_argcount, code.co_varnames[:code.co_argcount]\n(2, ('a', 'b'))\nBecause starred-argument names show up as locals but not as expected\narguments, they won’t be a factor in our matching algorithm—names preceding\nthem in function headers can be validated as usual, but not any extra positional\narguments passed. In principle, we could extend the decorator’s interface to\nsupport *pargs in the decorated function, too, for the rare cases where this\nmight be useful (e.g., a special argument name with a test to apply to all\narguments in *pargs beyond the length of the expected arguments list), but we’ll\npass on such an extension here.\nAlso, bear in mind that this pertains to values in starred collectors in def headers\nonly; given that starred unpackings in calls are flattened before they ever reach\nour decorator, they are irrelevant to its code. To borrow a pathological example\nfrom Chapter 18:\n>>> def f(a, b, c, d, e, f, g, h, i): pass\n>>> f.__code__.co_varnames[:f.__code__.co_argcount]\n('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i')\n>>> def f(*p, **k): print(p, k)\n>>> f(*[1], 2, *[3], 4, f=6, *[5], **dict(g=7), h=8, **{'i': 9})\n(1, 2, 3, 4, 5) {'f': 6, 'g': 7, 'h': 8, 'i': 9}",
      "content_length": 1825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1634,
      "chapter": null,
      "content": "The call’s stars here are resolved before the function is started. Because our\ndecorator finds values passed to argument names by indexing keywords and\nmapping expected to actual positionals, it can remain blissfully ignorant of stars\nin the call and will work normally in this example (though, to be fair, “normally”\nmay be an exaggeration here).\nDecorator nesting\nFinally, and perhaps most subtly, this code’s approach does not fully support the\nuse of decorator nesting to combine steps. Because it analyzes arguments using\nnames in function definitions, and the names of the call proxy function returned\nby a nested decoration won’t correspond to argument names in either the original\nfunction or decorator arguments, it does not fully support use in nested mode.\nTechnically, when nested, only the most deeply nested appearance’s validations\nare run in full; all other nesting levels run tests on arguments passed by keyword\nonly. Trace the code to see why; because the onCall proxy’s call signature\nexpects no named positional arguments, any to-be-validated arguments passed to\nit by position are treated as if they were omitted and hence defaulted and are thus\nskipped.\nThis may be inherent in this tool’s approach—proxies change the argument name\nsignatures at their levels, making it impossible to directly map names in\ndecorator arguments to positions in passed argument sequences. When proxies\nare present, argument names ultimately apply to keywords only; by contrast, the\nfirst-cut solution’s argument positions may support proxies better but do not\nfully support keywords.\nIn lieu of this nesting capability, we’ll generalize this decorator to support\nmultiple kinds of validations in a single decoration in an end-of-chapter quiz\nsolution, which also gives examples of the nesting limitation in action. Since\nwe’ve already neared the space allocation for this example, though, if you care\nabout these or any other further improvements, you’ve officially crossed over\ninto the realm of suggested exercises.\nDecorator Arguments Versus Function Annotations\nIn closing, Python’s annotation feature introduced in Chapter 19 could also",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1635,
      "chapter": null,
      "content": "provide an alternative to the decorator arguments used by our example to specify\nrange tests. As we learned earlier, annotations allow us to associate expressions\nwith arguments and return values by coding them in the def header line itself;\nPython collects annotations in a dictionary and attaches it to the annotated\nfunction.\nWe could use this in our example to code range limits in the header line instead\nof in decorator arguments. We would still need a function decorator to wrap the\nfunction in order to intercept later calls, but we would essentially trade decorator\nargument syntax:\n@rangetest(a=(1, 5), c=(0.0, 1.0))\ndef func(a, b, c):                         # func = rangetest(...)(func)\n    print(a + b + c)\nfor annotation syntax like this:\n@rangetest\ndef func(a:(1, 5), b, c:(0.0, 1.0)):\n    print(a + b + c)\nThat is, the range constraints would be moved into the function itself instead of\nbeing coded externally in a decorator line. Example 39-27 illustrates the\nstructure of the resulting decorators under both schemes in incomplete skeleton\ncode for brevity. The decorator-arguments code pattern is that of our complete\nsolution shown earlier; the annotations alternative requires one less level of\nnesting because it doesn’t need to retain decorator arguments as state.\nExample 39-27. decoargs-vs-annotation.py\n# Using decorator arguments\ndef rangetest(**argchecks):\n   def onDecorator(func):\n       def onCall(*pargs, **kargs):\n           print(argchecks)\n           for check in argchecks:\n               pass                         # Add validation code here\n           return func(*pargs, **kargs)\n       return onCall\n   return onDecorator\n@rangetest(a=(1, 5), c=(0.0, 1.0))",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1636,
      "chapter": null,
      "content": "def func(a, b, c):                           # func = rangetest(...)(func)\n   print(a + b + c)\nfunc(1, 2, c=3)                              # Runs onCall, argchecks in scope\n# Using function annotations\ndef rangetest(func):\n   def onCall(*pargs, **kargs):\n       argchecks = func.__annotations__\n       print(argchecks)\n       for check in argchecks:\n           pass                             # Add validation code here\n       return func(*pargs, **kargs)\n   return onCall\n@rangetest\ndef func(a:(1, 5), b, c:(0.0, 1.0)):         # func = rangetest(func)\n   print(a + b + c)\nfunc(1, 2, c=3)                              # Runs onCall, annotations on func\nWhen run, both schemes have access to the same validation test information but\nin different forms—the decorator argument version’s information is retained in\nan argument in an enclosing scope, and the annotation version’s information is\nretained in an attribute of the function itself:\n$ python3 decoargs-vs-annotation.py\n{'a': (1, 5), 'c': (0.0, 1.0)}\n6\n{'a': (1, 5), 'c': (0.0, 1.0)}\n6\nFleshing out the rest of the annotation-based version is left as a suggested\nexercise; its code would be almost identical to that of our earlier solution\nbecause range-test information is simply on the function instead of in an\nenclosing scope. Really, all this buys us is a different user interface for our tool\n—it will still need to match argument names against expected argument names to\nobtain relative positions as before.\nIn fact, using annotation instead of decorator arguments in this example actually\nlimits its utility. By moving the validation specifications into the def header, we\nessentially commit the function to a single role—since annotation directly allows\nus to code only one expression per argument, it can have only one purpose. For",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1637,
      "chapter": null,
      "content": "instance, we cannot use range-test annotations for any other role (including the\noptional and unused type hinting of Chapter 6).\nBy contrast, because decorator arguments are coded outside the function itself,\nthey are both easier to remove and more general—the code of the function itself\ndoes not imply a single decoration purpose. Crucially, by nesting decorators with\narguments, we can often apply multiple augmentation steps to the same function;\nannotation directly supports only one. With decorator arguments, the function\nitself also retains a simpler, normal appearance.\nStill, if you have a single purpose in mind, the choice between annotation and\ndecorator arguments is largely stylistic and subjective. As is so often true in life,\none person’s decoration or annotation may well be another’s syntactic clutter.",
      "content_length": 822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1638,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we explored decorators—both the function and class varieties. As\nwe learned, decorators are a way to insert code to be run automatically when a\nfunction or class is defined. When a decorator is used, Python rebinds a function\nor class name to the callable object that the decorator returns. This hook allows\nus to manage functions and classes themselves or later calls to them. By adding a\nlayer of wrapper logic to catch later calls, we can augment both function calls\nand instance interfaces. Decorators provide an explicit and uniform way to\nachieve such goals.\nAs we also learned, class decorators can be used to manage classes themselves\nrather than just their instances. Because this functionality overlaps with\nmetaclasses—the topic of the next and final technical chapter—you’ll have to\nread on for the conclusion to this story and that of this book at large.\nFirst, though, let’s work through the following quiz. Because this chapter was\nmostly focused on its examples, the quiz will ask you to modify some of its\nexamples’ code in order to review their concepts. Both the examples’ code and\nthe quiz’s solutions are located in the book’s examples package (see the Preface\nfor access pointers). Look for the solutions’ code there in this chapter’s\n_QuizAnswers subfolder. If you’re pressed for time, you’re welcome to jump\nright into studying the solutions; programming is often as much about reading\ncode as writing it.\nTest Your Knowledge: Quiz\n1. Method decorators: As mentioned in one of this chapter’s notes, the\ntimerdeco2.py module’s call-timer decorator that we wrote in\nExample 39-10 of “Adding Decorator Arguments” can be applied only\nto simple functions because it uses a nested class with a __call__\noperator-overloading method to catch calls. This structure does not\nwork for a class’s methods because the decorator instance is passed to\nself, not the subject-class instance. Rewrite this decorator so that it can\nbe applied to both simple functions and methods in classes, and test it",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1639,
      "chapter": null,
      "content": "on both functions and methods. (Hint: see “Class Pitfall: Decorating\nMethods” for pointers.) Note that you will probably need to use\nfunction-object attributes to keep track of total time, since you won’t\nhave a nested class for state retention and can’t access nonlocals from\noutside the decorator code.\n2. Class decorators: The Public/Private class decorators we wrote in\nmodule access2.py of Example 39-18 in this chapter’s first case study\nexample will add performance costs to every attribute fetch in a\ndecorated class. Although we could simply delete the @ decoration line\nto gain speed, we could also augment the decorator itself to check the\n__debug__ switch and perform no wrapping at all when the –O Python\nflag is passed on the command line—just as we did for the argument\nrange-test decorators. That way, we can speed our program without\nchanging its source via command-line arguments (python –O\nmain.py). While we’re at it, we could also use one of the mix-in\nsuperclass techniques we studied to catch a few built-in operations too.\nCode and test these two extensions.\n3. Generalized argument validations: The function and method decorator\nwe wrote in rangetest.py of Example 39-25 checks that passed\narguments are in a valid range, but the same code pattern could apply to\nsimilar goals such as argument type testing and possibly more.\nGeneralize the range tester so that its single code base can be used for\nmultiple kinds of argument validations. Passed-in validation functions\nmay be the simplest solution given the coding structure here, though\nsubclasses that provide expected methods can often provide similar\ngeneralization routes as well. This is substantially challenging, so be\nsure to see the solution for tips.\nTest Your Knowledge: Answers\nAs noted, coding solutions for this quiz are in this chapter’s _QuizAnswers\nsubfolder of the book examples package. Each question has its own subfolder\nthere for its files, with a _Notes.txt plain-text file giving background info. This\nedition opted to move these solutions online instead of listing them here because",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1640,
      "chapter": null,
      "content": "it saves about 10 pages and because this internet thing just might take off after\nall.",
      "content_length": 86,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1641,
      "chapter": null,
      "content": "Chapter 40. Metaclasses and\nInheritance\nIn Chapter 39, we explored decorators and studied examples of their use. In this\nfinal technical chapter of the book, we’re going to continue our tool-builders\nfocus with an in-depth review of another advanced topic: metaclasses, a protocol\nfor managing class objects instead of their instances, introduced briefly in\nChapter 32.\nOn a base level, metaclasses extend the code-insertion model of decorators. As\nwe learned in the prior chapter, decorators allow us to augment functions and\nclasses by intercepting their creation. Metaclasses similarly allow us to intercept\nand augment class creation—they provide a hook for inserting extra logic to be\nrun at the conclusion of a class statement, albeit in different ways than\ndecorators.\nMetaclasses can also provide behavior for classes with methods located in a\nseparate inheritance tree skipped for normal, nonclass instances. While this\nallows metaclasses to process their instance classes after creation, it also\ncompounds class semantics and convolutes inheritance—whose full definition\ncan finally be fleshed out here.\nLike all the subjects covered in this part of the book, this is an advanced topic\nthat can be studied on an as-needed basis. Metaclasses are not generally in scope\nfor most application programmers but may be of interest to others seeking to\nwrite flexible tools. Whatever category you fall into, though, metaclasses can\nteach you more about Python’s classes and are a prerequisite to both code that\nemploys them and the complete inheritance story in Python.\nAs the last technical chapter of this book, this also begins to wrap up some\nthreads concerning Python itself that we have met often along the way and will\nfinalize in the conclusion that follows. Where you go after this book is up to you,\nbut in an open source project, it’s important to keep the big picture in mind while\nhacking the small details.",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1642,
      "chapter": null,
      "content": "To Metaclass or Not to Metaclass\nDespite the advanced status awarded to metaclasses in the preceding opener,\nthey have a variety of potential roles. For example, they can be used to enhance\nclasses with features like tracing, object persistence, exception logging, and\nmore. They can also be used to construct portions of a class at runtime based on\nconfiguration files, apply function decorators to every method of a class\ngenerically, verify conformance to expected interfaces, and so on.\nIn their more grandiose incarnations, metaclasses can even be used to implement\nalternative coding patterns such as aspect-oriented programming,\nobject/relational mappers (ORMs) for databases, and more. Although there are\noften alternative ways to achieve such results—as you’ll see, the roles of class\ndecorators and metaclasses often intersect—metaclasses provide a formal model\ntailored to those tasks. We don’t have space to explore all such applications first-\nhand in this chapter, of course, but you can find additional use cases on the web\nafter studying the basics here.\nProbably the reason for studying metaclasses most relevant to this book is that\nthis topic can help demystify Python’s class mechanics in general. For instance,\nyou’ll find that they are an intrinsic part of the language’s inheritance model\nformalized in full here. Although you may or may not code or reuse them in your\nwork, a cursory understanding of metaclasses can impart a deeper understanding\nof Python at large.\nThe Downside of “Helper” Functions\nBefore we get to metaclass code, let’s get a better handle on its rationale. Like\nthe decorators of the prior chapter, metaclasses are optional in principle. We can\nusually achieve the same effect by passing class objects through functions—\nknown interchangeably as helper or manager functions—much as we can\nachieve the goals of decorators by passing functions and classes through\nmanager code. Just like decorators, though, metaclasses:\nProvide a more uniform and explicit structure\nHelp ensure that application programmers won’t forget to augment their\nclasses according to an API’s requirements",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1643,
      "chapter": null,
      "content": "Avoid code redundancy and its associated maintenance costs by\nfactoring class customization logic into a single location\nTo illustrate, suppose we want to automatically insert a method into a set of\nclasses. Of course, we could do this with simple inheritance if the subject\nmethod is known when we code the classes. In that case, we can simply code the\nmethod in a superclass and have all the classes in question inherit from it:\nclass Extras:\n    def extra(self, args):              # Normal inheritance: too static\n        …\nclass Client1(Extras): …                # Clients inherit extra methods\nclass Client2(Extras): …\nX = Client1()                           # Make an instance\nX.extra()                               # Run the extra methods\nSometimes, though, it’s impossible to predict such augmentation when classes\nare coded. Consider the case where classes are augmented in response to choices\nmade in a user interface at runtime or loaded from an editable configuration file.\nAlthough we could code every class in our imaginary set to manually check\nthese, too, it’s a lot to ask of clients (the required function here is abstract—it’s\nsomething to be filled in):\ndef extra(self, arg): …\nclass Client1: …                        # Client augments: too distributed\nif required():\n    Client1.extra = extra\nclass Client2: …\nif required():\n    Client2.extra = extra               # Add the extra method – maybe\nX = Client1()\nX.extra()\nWe can add methods to a class after the class statement like this because, as\nwe’ve learned, a class-level method is just a plain function that is associated with\na class and has a first argument to receive a self instance when called through\none. Although this works, it might become untenable for larger method sets and",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1644,
      "chapter": null,
      "content": "puts all the burden of augmentation on each client class (and assumes they’ll\nremember to do this at all).\nIt would be better from a maintenance perspective to isolate the decision logic in\na single place. We might encapsulate some of this extra work by routing classes\nthrough a helper function—a function that would extend the class as required\nand handle all the work of runtime testing and configuration:\ndef extra(self, arg): …\ndef extras(Class):                      # Helper function: too manual\n    if required():\n        Class.extra = extra\nclass Client1: …\nextras(Client1)\nclass Client2: …\nextras(Client2)\nX = Client1()\nX.extra()\nThis code runs the class through a helper function immediately after it is created.\nAlthough functions like this one can achieve our goal here, they still put a\nburden on class coders, who must understand the requirements and adhere to\nthem in their code. It would be even better if there was a simple way to enforce\nthe augmentation in the subject classes, so that they don’t need to deal with the\naugmentation so explicitly and would be less likely to forget to use it altogether.\nIn other words, we’d like to be able to insert some code to run automatically at\nthe end of a class statement to augment the class.\nThis is exactly what metaclasses do—by declaring a metaclass, we tell Python to\nroute the creation of the class object to another class we provide:\ndef extra(self, arg): …\nclass Extras(type):\n    def __init__(Class, classname, superclasses, attributedict):\n        if required():\n            Class.extra = extra\nclass Client1(metaclass=Extras): …      # Metaclass declaration only",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1645,
      "chapter": null,
      "content": "class Client2(metaclass=Extras): …      # Client class is instance of meta\nX = Client1()                           # X is instance of client class\nX.extra()\nBecause Python invokes the metaclass automatically at the end of the class\nstatement when the new class is created, it can augment, register, wrap, or\notherwise manage the class as needed. Moreover, the only requirement for the\nclient classes is that they declare their metaclass; every class that does so will\nautomatically acquire whatever augmentation the metaclass provides, both now\nand in the future if the metaclass changes.\nMetaclasses can also augment their class instances with inherited methods,\nwhich are akin to normal class methods but not inherited by the instances of their\nclass instances—an inheritance extension and tongue twister whose true nature\nrequires more info ahead (and quite possibly, sedation). Through both changes\nand methods, though, metaclasses can customize class behavior broadly.\nOf course, this is the standard rationale, which you’ll need to judge for yourself\n—in truth, clients might forget to list a metaclass just as easily as they could\nforget to call a helper function! Still, the explicit nature of metaclasses may\nmake this less likely. Although it may be difficult to glean from this small and\nhypothetical example, metaclasses generally handle such tasks better than more\nmanual approaches.\nMetaclasses Versus Class Decorators: Round 1\nHaving said that, it’s also important to note that the class decorators described in\nthe preceding chapter sometimes overlap with metaclasses—in terms of both\nutility and benefit. Like metaclasses, class decorators can be used to manage\nboth classes and their later instances, and their syntax makes their usage\nsimilarly explicit and arguably more obvious than helper-function calls.\nFor example, suppose we recoded the last section’s helper function to return the\naugmented class instead of simply modifying it in place. This would allow a\ngreater degree of flexibility because the manager would be free to return any\ntype of object that implements the class’s expected interface:\ndef extra(self, arg): …",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1646,
      "chapter": null,
      "content": "def extras(Class):\n    if required():\n        Class.extra = extra\n    return Class               # Return the augmented class\nclass Client1: …\nClient1 = extras(Client1)      # Rebind to augmented class\nclass Client2: …\nClient2 = extras(Client2)\nX = Client1()\nX.extra()\nIf you think this is starting to look reminiscent of class decorators, you’re right.\nIn the prior chapter, we emphasized class decorators’ role in augmenting\ninstance creation calls. Because they work by automatically rebinding a class\nname to the result of a function, though, there’s no reason that we can’t use them\nto augment the class by changing it before any instances are ever created. That\nis, class decorators can apply extra logic to classes, not just instances, at class\ncreation time:\ndef extra(self, arg): …\ndef extras(Class):             # From helper to decorator\n    if required():\n        Class.extra = extra\n    return Class\n@extras\nclass Client1: …               # Client1 = extras(Client1)\n@extras\nclass Client2: …               # Rebinds class independent of instances\nX = Client1()                  # Makes instance of augmented class\nX.extra()                      # X is instance of original Client1\nDecorators essentially automate the prior example’s manual name rebinding\nhere. Just as for metaclasses, because this decorator returns the original class,\ninstances are made from that class, not from a wrapper object. In fact, instance\ncreation is not intercepted at all in this example.",
      "content_length": 1482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1647,
      "chapter": null,
      "content": "In this specific case—adding methods to a class when it’s created—the choice\nbetween metaclasses and decorators is arbitrary. Decorators can be used to\nmanage both instances and classes and intersect most strongly with metaclasses\nin the second of these roles, but this discrimination is not absolute. In fact, the\nroles of each are suggested in part by their mechanics.\nAs we’ll detail ahead, decorators technically correspond to metaclass calls used\nto make and initialize new classes. Metaclasses, though, have additional\ncustomization hooks beyond class creation. Their methods inherited by classes,\nfor example, have no direct counterpart in class decorators sans extra code. This\ncan make metaclasses more complex but also better suited for augmenting\nclasses in some contexts.\nConversely, because metaclasses are designed to manage classes, applying them\nto managing instances alone is less optimal. Because they are also responsible\nfor making the class itself, metaclasses incur this as an extra step in instance-\nmanagement roles and are perhaps less appropriate than decorators.\nWe’ll explore some of these differences in working code later in this chapter. To\nbetter understand how metaclasses do their work, though, we first need to get a\nclearer picture of their underlying model.\nThe Metaclass Model\nTo understand metaclasses, you first need to understand a bit more about both\nPython’s object model and what happens at the end of a class statement. As\nyou’ll learn here, the two are intimately related.\nClasses Are Instances of type\nThe first of these prerequisites was covered previously by “The Python Object\nModel”, which in turn assumes knowledge of the MRO (method resolution\norder) covered in Chapter 31. You should review that content now if needed,\nespecially if you’ve jumped into this chapter at random. We’re not going to\nrepeat its coverage in full here, but some of its conclusions are crucial to\nunderstanding metaclasses. Namely:\nInstances are created from classes.",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1648,
      "chapter": null,
      "content": "Classes are instances of a metaclass.\nThe type built-in is the topmost metaclass.\nMetaclasses customize type with normal class statements.\nIn short, classes are types, types are classes, and metaclasses customize types.\nPer Chapter 32, the relationship between metaclasses and classes is subtly\ndifferent from that between classes and their nonclass instances. The latter do\nnot generate more instances, and while attribute inheritance uses the same core\nmechanisms and MRO everywhere, classes search a metaclass tree that nonclass\ninstances do not.\nWe’ll study the nuts and bolts of inheritance ahead, but it’s easy to see this\nmodel’s fundamentals in code. Built-in objects like lists are actually nonclass\ninstances made from a class, which itself is made from the built-in type:\n>>> type([]), type(type([]))         # List instance is created from list class\n(<class 'list'>, <class 'type'>)     # List class is created from type class\n>>> type(list), type(type)           # Same, but with type names\n(<class 'type'>, <class 'type'>)     # Type of type is type: top of hierarchy\nApart from the literal syntax of built-ins, this works the same way for user-\ndefined classes, which are really just user-defined types—their nonclass\ninstances are made from the class, which itself is made from the built-in type:\n>>> class Hack: pass                 # User-defined classes work the same\n>>> I = Hack()                       # Made from class, which is made from type\n>>> type(I), type(type(I))\n(<class '__main__.Hack'>, <class 'type'>)\nAlthough their behavior varies in ways we explored in Chapter 32, both classes\nand nonclass instances are “instances” in some sense. In fact, the __class__\nattribute in both tells us what they were made from:\n>>> [].__class__, list.__class__     # Instances and classes are both \"instances\"\n(<class 'list'>, <class 'type'>)     # type(X) is normally same as X.__class__\n>>> I.__class__, Hack.__class__",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1649,
      "chapter": null,
      "content": "(<class '__main__.Hack'>, <class 'type'>)\nBecause classes are created from the root type class by default, most\nprogrammers don’t need to think about this model. However, it’s key to\nunderstanding the way that metaclasses work—as the next section explains.\nMetaclasses Are Subclasses of type\nWhy would we care that classes are instances of a type class? It turns out that\nthis is the hook that allows us to code metaclasses. Specifically, we can create\nclasses from subclasses of type that customize it with normal object-oriented\ntechniques and class syntax. And these type subclasses are known as\nmetaclasses.\nIn other words, to control the way classes are created and augment their\nbehavior, all we need to do is specify that a user-defined class be created from a\nuser-defined metaclass instead of the normal and default type class.\nBefore we see how, it’s important to bear in mind that this type instance\nrelationship is not quite the same as normal inheritance. User-defined classes\nmay also have superclasses from which they and their instances inherit attributes\nas usual. As we’ve seen, inheritance superclasses are listed in parentheses in the\nclass statement, show up in a class’s __bases__ tuple, and are searched for\nattributes fetched from nonclass instances.\nHowever, the type from which a class is created and of which it is an instance is\na different relationship. Inheritance searches instance and class namespace\ndictionaries, but classes may also acquire behavior from their type that is not\nexposed to the normal inheritance search. In fact, metaclasses define a separate,\nsecondary inheritance tree available only to classes and used as a fallback when\nthe normal superclass search fails.\nTo lay the groundwork for understanding this distinction, the next section\ndescribes the procedure and syntax Python uses to implement this instance-of\nrelationship.\nClass Statements Call a type",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1650,
      "chapter": null,
      "content": "Subclassing the type class to customize it is really only half of the metaclass\nbackstory. We still need to somehow route a class’s creation to the metaclass\ninstead of the default type. To comprehend the way that this is arranged, we also\nneed to know how class statements do their business.\nWe’ve already learned that when Python reaches a class statement, it runs its\nnested block of code to create the class’s attributes—all the names assigned at\nthe top level of the nested code block generate attributes in the resulting class\nobject. These names are usually method functions created by nested defs, but\nthey can also be arbitrary attributes assigned to create class data shared by all\ninstances.\nTechnically speaking, Python follows a standard protocol to make this happen: at\nthe end of a class statement, and after running all its nested code in a\nnamespace dictionary corresponding to the class’s local scope, Python calls the\ntype object to create the new class object like this:\nclass = type(classname, superclasses, attributedict)\nThe type object in turn defines a __call__ operator-overloading method that\nruns two other methods when the type object is called:\ntype.__new__(typeclass, classname, superclasses, attributedict)\ntype.__init__(class, classname, superclasses, attributedict)\nThe __new__ method creates and returns the new class object, after which the\n__init__ method initializes the newly created object. As you’ll see in a\nmoment, these are the hooks that metaclass subclasses of type generally use to\nperform class customizations at creation time.\nFor example, given a class definition like the following for Hack:\nclass Super: …                    # Inherited names here\nclass Hack(Super):                # Inherits from Super\n    data = 1                      # Class data attribute\n    def meth(self, arg):          # Class method attribute\n        return self.data + arg",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1651,
      "chapter": null,
      "content": "Python will internally run the nested code block to create two attributes of the\nclass (data and meth), and then call the type object to generate the class object\nat the end of the class statement’s processing (extra names like __module__ are\nadded automatically from the code’s context):\nHack = type('Hack', (Super,), {'data': 1, 'meth': meth, '__module__': '__main__'})\nIn fact, you can call type this way yourself to create a class dynamically—albeit\nhere with a fabricated method function and empty superclasses tuple (Python\nadds the object superclass automatically to topmost classes as we learned in\nChapter 32, and the enclosing module’s name is again implied):\n>>> c = type('Hack', (), {'data': 1, 'meth': (lambda x, y: x.data + y)})\n>>> i = c()\n>>> c, i\n(<class '__main__.Hack'>, <__main__.Hack object at 0x108077c20>) \n>>> i.data, i.meth(2)\n(1, 3)\nThe class produced by a direct type call is exactly like that you’d get from\nrunning a class statement (again, if you’ve forgotten what some of the\nfollowing are about, flip, click, or tap back to Chapter 32 for a refresher):\n>>> c.__bases__\n(<class 'object'>,)\n>>> i.__class__.__mro__\n(<class '__main__.Hack'>, <class 'object'>)\n>>> [a for a in dir(i) if not a.startswith('__')]\n['data', 'meth']\n>>> [(a, v) for (a, v) in c.__dict__.items() if not a.startswith('__')]\n[('data', 1), ('meth', <function <lambda> at 0x1082179c0>)]\nBecause this type call is made automatically at the end of the class statement,\nthough, it’s an ideal hook for augmenting or otherwise processing a class. The\ntrick lies in replacing the default type with a custom subclass that will intercept\nthis call. The next section shows how.",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1652,
      "chapter": null,
      "content": "Class Statements Can Choose a type\nAs we’ve just seen, classes are created by the type class by default. To tell\nPython to create a class with a custom metaclass instead, you simply need to\ndeclare a metaclass to intercept the normal instance creation call for a user-\ndefined class. To do so, list the desired metaclass as a keyword argument in the\nclass header:\nclass Hack(metaclass=Meta):                   # Use Meta instead of type default\nIf no such declaration is present, the metaclass to be called defaults to the type\nbuilt-in, per the prior section. When used, though, this declaration overrides the\ntype default and routes the class creation call at the close of the class statement\nto Meta instead:\nclass = Meta(classname, superclasses, attributedict)\nImportantly again, the metaclass keyword specifies an instance-of relationship,\nwhich implies inheritance only through the secondary metaclass tree we’ll\nformalize ahead. Normal inheritance superclasses can be listed in the header as\nwell and take precedence by residing in the primary class tree. In the following,\nfor example, the new class Hack inherits from superclass Super normally but is\nalso an instance of—and is created by—metaclass Meta:\nclass Hack(Super, metaclass=Meta):            # Normal supers OK: listed first\nIn this form, superclasses must be listed before the metaclass; in effect, the\nordering rules used for keyword arguments in function calls apply here too. This\norder also has implications for inheritance, which we’ll formalize soon, but first,\nwe need to learn how to code the metaclasses that tap into calls triggered by this\nspecial class syntax.\nMetaclass Method Protocol\nWhen a specific metaclass is declared per the prior sections’ syntax, the call to\ncreate the class object run at the end of the class statement is modified to",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1653,
      "chapter": null,
      "content": "invoke the metaclass instead of the type default, as we just saw:\nclass = Meta(classname, superclasses, attributedict)\nAssuming the metaclass is a subclass of type, though, the type class’s inherited\n__call__ method delegates creation and initialization of the new class object\nto the metaclass if the metaclass defines custom versions of the methods that\nhandle these steps. In other words, the Meta call may wind up triggering these\nmethod calls in turn:\nclass = Meta.__new__(Meta, classname, superclasses, attributedict)\nMeta.__init__(class, classname, superclasses, attributedict)\nTo demonstrate, here’s the preceding class example again, augmented with a\nmetaclass specification:\nclass Hack(Super, metaclass=Meta):     # Inherits from Super, instance of Meta\n    data = 1                           # Class data attribute\n    def meth(self, arg):               # Class method attribute\n        return self.data + arg\nAt the end of this class statement, Python internally runs the following to create\nthe class object—again, a call you could make manually, too, but automatically\nrun by Python’s class machinery:\nHack = Meta('Hack', (Super,), {'data': 1, 'meth': meth, '__module__': '__main__'})\nIf the metaclass defines its own versions of __new__ or __init__, they will be\ninvoked during this call by the inherited type class’s __call__ method. The net\neffect is to automatically run methods the metaclass provides as part of the class-\nconstruction process. The next section shows how we might go about coding this\nfinal piece of the metaclass puzzle.\nCoding Metaclasses\nSo far, we’ve seen how Python routes class creation calls to a metaclass if one is\nspecified and provided. How, though, do we actually code a metaclass that",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1654,
      "chapter": null,
      "content": "customizes type?\nIt turns out that you already know most of the story—metaclasses are coded with\nnormal Python class statements and semantics. By definition, they are simply\nclasses that inherit from type (normally, at least). Their only substantial\ndistinctions are that Python calls them automatically at the end of a class\nstatement and that they must generally adhere to the interface expected by the\ntype superclass if they subclass it.\nA Basic Metaclass\nPerhaps the simplest metaclass you can code is simply a subclass of type with a\n__new__ method that creates the class object by running the default method in\ntype. A metaclass __new__ like this is run by the __call__ method inherited\nfrom type by virtue of normal inheritance overrides; this method typically\nperforms whatever augmentation is required and calls the type superclass’s\n__new__ method to create and return the new class object:\nclass Meta(type):\n    def __new__(meta, classname, supers, classdict):\n        # Run by inherited type.__call__\n        return type.__new__(meta, classname, supers, classdict)\nThis metaclass doesn’t really do anything (we might as well let the default type\ncreate the class), but it demonstrates the way a metaclass taps into the metaclass\nhook to customize—because the metaclass is called at the end of a class\nstatement, and because the type object’s __call__ dispatches to the __new__\nand __init__ methods, code we provide in these methods can manage all the\nclasses created from the metaclass.\nTo demo, Example 40-1 is our inane metaclass again, but in more tangible form,\nwith prints added to the metaclass and the file at large to trace the process.\nExample 40-1. metaclass1.py\nclass MetaOne(type):\n   def __new__(meta, classname, supers, classdict):\n       print('In MetaOne.new:', meta, classname, supers, classdict, sep='\\n...')\n       return type.__new__(meta, classname, supers, classdict)",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1655,
      "chapter": null,
      "content": "class Super:\n   pass\nprint('Making class')\nclass Hack(Super, metaclass=MetaOne):     # Inherits from Super, instance of MetaOne\n   data = 1                              # Class data attribute\n   def meth(self, arg):                  # Class method attribute\n       return self.data + arg\nprint('Making instance')\nX = Hack()\nprint('Attrs:', X.data, X.meth(2))\nHere, class Hack inherits from Super and is an instance of MetaOne, but X is an\ninstance of and inherits from Hack. When run, notice how the metaclass is\ninvoked at the end of the class statement and before we ever make an instance\nof the new class—metaclasses process classes, and classes process nonclass\ninstances:\n$ python3 metaclass1.py\nMaking class\nIn MetaOne.new:\n...<class '__main__.MetaOne'>\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\nMaking instance\nAttrs: 1 3\nPresentation note: this chapter’s examples often truncate hex addresses and omit\nsome irrelevant built-in __X__ names in namespace dictionaries, both for brevity\nand because built-in attributes tend to change over time. Run these examples on\nyour own for full, if transient, fidelity.\nCustomizing Construction and Initialization\nMetaclasses can also tap into the __init__ protocol invoked by the type object’s\n__call__. In general, __new__ creates and returns the class object, and\n__init__ initializes the already created class passed in as an argument. These\nmethods work in non-type classes, too, but __new__ is rare in such classes.\nMetaclasses can use either or both hooks to manage classes at creation time, as",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1656,
      "chapter": null,
      "content": "Example 40-2 illustrates.\nExample 40-2. metaclass2.py\nclass MetaTwo(type):\n   def __new__(meta, classname, supers, classdict):\n       print()\n       print('In MetaTwo.new:', meta, classname, supers, classdict, sep='\\n...')\n       return type.__new__(meta, classname, supers, classdict)\n   def __init__(Class, classname, supers, classdict):\n       print()\n       print('In MetaTwo.init:', Class, classname, supers, classdict, sep='\\n...')\n       print('...init class object:', list(Class.__dict__.keys()))\nclass Super:\n   pass\nprint('Making class')\nclass Hack(Super, metaclass=MetaTwo):     # Inherits from Super, instance of MetaTwo\n   data = 1                              # Class data attribute\n   def meth(self, arg):                  # Class method attribute\n      return self.data + arg\nprint('\\nMaking instance')\nX = Hack()\nprint('Attrs:', X.data, X.meth(2))\nIn this case, the class initialization method is run after the class construction\nmethod, but both methods run at the end of the class statement and before any\nnonclass instances are made. Conversely, an __init__ in Hack would run later\nat nonclass-instance creation time and would not be affected or run by the\nmetaclass’s __init__:\n$ python3 metaclass2.py\nMaking class\nIn MetaTwo.new:\n...<class '__main__.MetaTwo'>\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\nIn MetaTwo.init:\n...<class '__main__.Hack'>\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1657,
      "chapter": null,
      "content": "...init class object: ['__module__', 'data', 'meth', '__doc__']\nMaking instance\nAttrs: 1 3\nOther Metaclass Coding Techniques\nAlthough redefining the type superclass’s __new__ and __init__ methods is\nthe most common way to insert logic into the class object creation process with\nthe metaclass hook, other schemes are possible. They may not dovetail as neatly\ninto the notion of metaclass methods we’ll study ahead, but they do support\ncreation-time tasks.\nUsing simple factory functions\nFor example, metaclasses need not really be classes at all. As we’ve learned, the\nclass statement issues a simple call to create a class at the conclusion of its\nprocessing. Because of this, any callable object can, in principle, be used as a\nmetaclass, provided it accepts the arguments passed and returns an object\ncompatible with the intended class. In fact, a simple object factory function may\nserve just as well as a type subclass, as Example 40-3 demonstrates.\nExample 40-3. metaclass3.py\n# A simple function can serve as a metaclass too\ndef MetaFunc(classname, supers, classdict):\n   print('In MetaFunc:', classname, supers, classdict, sep='\\n...')\n   return type(classname, supers, classdict)\nclass Super:\n   pass\nprint('Making class')\nclass Hack(Super, metaclass=MetaFunc):           # Run simple function at end\n   data = 1                                     # Function returns class\n   def meth(self, arg):\n       return self.data + arg\nprint('Making instance')\nX = Hack()\nprint('Attrs:', X.data, X.meth(2))\nWhen run, the function is called at the end of the declaring class statement, and",
      "content_length": 1589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1658,
      "chapter": null,
      "content": "it returns the expected new class object. The function is simply catching the call\nthat the type object’s __call__ normally intercepts by default:\n$ python3 metaclass3.py\nMaking class\nIn MetaFunc:\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\nMaking instance\nAttrs: 1 3\nTechnically speaking, such a plain function used as a metaclass can return\nanything: whatever it returns is assigned to the new class’s name, whether it’s a\ntype instance or not. Other kinds of results may not support later instance\ncreation, but this blurs the distinction between metaclasses and class decorators\n—both rebind names in the end.\nOverloading class creation calls with normal classes\nBecause normal (a.k.a. nonclass) instances can respond to call operations with\noperator overloading, they can serve in some metaclass roles, too, much like the\npreceding function. The output of Example 40-4 is similar to the prior class-\nbased versions, but it’s based on a simple class—one that doesn’t inherit from\ntype at all and provides a __call__ for its instances that catches the metaclass\ncall using normal operator overloading.\nAll classes are created from a metaclass, even if it’s the default type metaclass,\nbut here we’re using a nonclass instance of a class for the “metaclass.” Note that\n__new__ and __init__ must use different names here, or else they will run\nwhen the MetaObj instance is created, not when that instance is later called in\nthe role of metaclass after Hack’s class statement. The __call__ here mimics\npart of what type’s method does.\nExample 40-4. metaclass4.py\n# A normal class instance can serve as a metaclass too\nclass MetaObj:\n   def __call__(self, classname, supers, classdict):\n       print('In MetaObj.call:', classname, supers, classdict, sep='\\n...')\n       Class = self.__New__(classname, supers, classdict)",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1659,
      "chapter": null,
      "content": "self.__Init__(Class, classname, supers, classdict)\n       return Class\n   def __New__(self, classname, supers, classdict):\n       print('In MetaObj.new: ', classname, supers, classdict, sep='\\n...')\n       return type(classname, supers, classdict)\n   def __Init__(self, Class, classname, supers, classdict):\n       print('In MetaObj.init:', classname, supers, classdict, sep='\\n...')\n       print('...init class object:', list(Class.__dict__.keys()))\nclass Super:\n   pass\nprint('Making class')\nclass Hack(Super, metaclass=MetaObj()):         # MetaObj() is normal class instance\n   data = 1                                    # Called at end of statement\n   def meth(self, arg):\n       return self.data + arg\nprint('Making instance')\nX = Hack()\nprint('Attrs:', X.data, X.meth(2))\nWhen run, the three methods are dispatched via the normal instance’s __call__\ninherited from its normal class, but without any dependence on type dispatch\nmechanics or semantics. This routing is largely about class alone:\n$ python3 metaclass4.py\nMaking class\nIn MetaObj.call:\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\nIn MetaObj.new: \n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\nIn MetaObj.init:\n...Hack\n...(<class '__main__.Super'>,)\n...{'__module__': '__main__', 'data': 1, 'meth': <function Hack.meth at 0x…>}\n...init class object: ['__module__', 'data', 'meth', '__doc__']\nMaking instance\nAttrs: 1 3\nIn fact, we can use normal superclass inheritance to acquire the call interceptor",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1660,
      "chapter": null,
      "content": "in this coding model—the superclass in Example 40-5 serves essentially the\nsame role as type, at least in terms of metaclass dispatch. Such code may be\natypical, but it demos the underlying metaclass dispatch model.\nExample 40-5. metaclass5.py\n# Instances inherit from classes and their supers normally\nclass SuperMetaObj:\n   def __call__(self, classname, supers, classdict):\n       print('In SuperMetaObj.call:', classname, supers, classdict, sep='\\n...')\n       Class = self.__New__(classname, supers, classdict)\n       self.__Init__(Class, classname, supers, classdict)\n       return Class\nclass SubMetaObj(SuperMetaObj):\n   def __New__(self, classname, supers, classdict):\n       print('In SubMetaObj.new: ', classname, supers, classdict, sep='\\n...')\n       return type(classname, supers, classdict)\n   def __Init__(self, Class, classname, supers, classdict):\n       print('In SubMetaObj.init:', classname, supers, classdict, sep='\\n...')\n       print('...init class object:', list(Class.__dict__.keys()))\nclass Super:\n   pass\nprint('Making class')\nclass Hack(Super, metaclass=SubMetaObj()):   # Invoke Sub instance via Super.__call__\n  …rest of file same as Example 40-4…\nThis example’s output is largely the same as that of its predecessor but reflects\nnormal inheritance at work:\n$ python3 metaclass5.py \nMaking class\nIn SuperMetaObj.call:\n…as before…\nIn SubMetaObj.new:\n…as before…\nIn SubMetaObj.init:\n…as before…\nMaking instance\nAttrs: 1 3\nAlthough such alternative forms work, most metaclasses achieve their creation-\ntime goals by redefining the type superclass’s __new__ and __init__; in",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1661,
      "chapter": null,
      "content": "practice, this may be simpler than other schemes. Regardless of its coding,\nthough, this metaclass role broadly intersects with class decorators—as the next\nsection will demonstrate.\nManaging Classes with Metaclasses and Decorators\nNow that we understand the hook metaclasses use to insert code to be run at\nclass construction time, let’s put it to better use. In general, such code can be\nused to augment classes arbitrarily, and in many of the same ways as the class\ndecorators of Chapter 39. This doesn’t make these two tools identical—\nmetaclasses also support the notion of inherited methods coming up later—but\nthey are functionally redundant in some roles.\nAdding methods to classes\nTo demo both this equivalence and more realistic usage, Example 40-6 uses\nmetaclasses to augment the set of methods available in classes by inserting new\nmethods at class-creation time as sketched in the abstract earlier—not the sort of\nthing most programmers do on a day-to-day basis, but potentially useful in tools\nand libraries nonetheless.\nExample 40-6. extend_meta.py\n\"Extend a class with a metaclass\"\ndef triple(obj):\n   return obj.value * 3                                  # Functions to insert\n                                                         # Methods if in a class\ndef concat(obj):                                          # Where \"obj\" is \"self\"\n   return obj.value + 'Code!'\nclass Extender(type):\n   def __new__(meta, classname, supers, classdict):      # On client-class creation\n       classdict['triple'] = triple                      # Add funcs as attributes \n       classdict['concat'] = concat\n       return type.__new__(meta, classname, supers, classdict)\nclass Client1(metaclass=Extender):\n   def __init__(self, value):                            # Created from Extender\n       self.value = value                                # Own + inserted methods\n   def double(self):\n       return self.value * 2\nclass Client2(metaclass=Extender):                        # Created from Extender",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1662,
      "chapter": null,
      "content": "value = 'grok'                                        # Inherited class data\nX = Client1('hack')                                       \nprint(X.double(), X.triple(), X.concat(), sep='\\n')\nY = Client2()                                             \nprint(Y.triple(), Y.concat(), sep='\\n')\nRecall again that class methods are simply functions that normally receive an\ninstance through which they are called. The metaclass in this code leverages this\nto insert two functions into each of its client classes when those classes are\nmade. The net effect provides methods and behavior for clients that do not exist\nin their class statements:\n$ python3 extend_meta.py\nhackhack\nhackhackhack\nhackCode!\ngrokgrokgrok\ngrokCode!\nOf course, the methods inserted here might be coded in a superclass and\ninherited by clients as usual, but the metaclass here is free to select inserted\nmethods based on conditions tested whenever clients are built. As covered\nahead, we can also almost do the same with metaclass methods, but these\nmethods are inherited only by classes and wouldn’t work in this example; the\nmethods inserted here are instead available to later class instances like X and Y.\nAnd while this may seem novel, it’s easy to accomplish the same result with\nclass decorators. As we’ve learned, there are indeed some striking similarities\nbetween these tools:\nClass decorators work by rebinding class names to the result of a\ncallable at the end of a class statement after the new class has been\ncreated.\nMetaclasses work by routing class-object creation through a callable at\nthe end of a class statement in order to create the new class.\nThe sum makes these two tools functionally equivalent, at least in terms of class-\ncreation dispatch. Example 40-7 illustrates this equivalence by recoding the",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1663,
      "chapter": null,
      "content": "same augmentation with a class decorator instead of a metaclass.\nExample 40-7. extend_deco.py\n\"Extend a class with a decorator\"\ndef triple(obj):\n   return obj.value * 3\ndef concat(obj):\n   return obj.value + 'Code!'\ndef extender(aClass):\n   aClass.triple = triple                   # Manages class, not instance\n   aClass.concat = concat                   # Same as metaclass __call__\n   return aClass\n@extender\nclass Client1:                               # Client1 = Extender(Client1)\n   def __init__(self, value):               # Rebound at end of class stmt\n       self.value = value\n   def double(self):\n       return self.value * 2\n@extender\nclass Client2:\n   value = 'grok'\nX = Client1('hack')                                       \nprint(X.double(), X.triple(), X.concat(), sep='\\n')\nY = Client2()                                             \nprint(Y.triple(), Y.concat(), sep='\\n')\nWhen run, this class-decorator version’s output is identical to that of the\nmetaclass variant in Example 40-6. It works the same because both decorator\nand metaclass are called at the end of a class statement and return an object to\nwhich the class’s name is assigned. Decorators may be closest to the metaclass\n__call__ because they are called to return an object, but like metaclass\n__init__, the class has already been built by the time a decorator runs.\nMetaclass __call__ runs __new__ and __init__, and metaclasses can augment\nin any of these three methods.\nAutomatically decorating class methods\nPerhaps more interesting, metaclasses and decorators can both augment",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1664,
      "chapter": null,
      "content": "individual methods of classes. To demo, we’ll use the utility module in\nExample 40-8, which resurrects the tracer and timer function decorators we\ncoded in the prior chapter. This module is entirely review, so we’ll defer to\nChapter 39 for more details (and promise that this is the last mileage we’ll get\nfrom this code).\nExample 40-8. decorators.py\nimport time\ndef tracer(func):                         # Use function, not class with __call__\n   calls = 0                             # Else self is decorator instance only\n   def onCall(*args, **kwargs):\n       nonlocal calls\n       calls += 1\n       print(f'call {calls} to {func.__name__}')\n       return func(*args, **kwargs)\n   return onCall\ndef timer(label='', trace=True):                # On decorator args: retain args\n   def onDecorator(func):                      # On @: retain decorated func\n       def onCall(*args, **kargs):             # On calls: call original\n           start   = time.perf_counter()       # State is scopes + func attribute\n           result  = func(*args, **kargs)\n           elapsed = time.perf_counter() - start\n           onCall.alltime += elapsed\n           if trace:\n               funcname, alltime = func.__name__, onCall.alltime\n               print(f'{label}{funcname}: {elapsed:.5f}, {alltime:.5f}')\n           return result\n       onCall.alltime = 0\n       return onCall\n   return onDecorator\nAs we learned in Chapter 39, to use these decorators manually, we simply import\nthem from the module and decorate each function we wish to augment. While\nthis suffices for one-off augmentations, it requires us to add decoration syntax\nbefore each method we wish to trace or time and to later remove that syntax\nwhen we no longer desire the extensions (or use the -O trick we’ll skip here). If\nwe want to trace or time every method of a class, this can become tedious—and\nmay not be possible at all in more dynamic contexts that depend upon runtime\nparameters.\nTo do better, Example 40-9 uses a metaclass to add a decorator to each of a\nclass’s methods automatically. Because the metaclass controls decoration, it can",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1665,
      "chapter": null,
      "content": "predicate decoration on runtime checks. As a bonus, it can be used for any\ndecoration: the decorator to apply to methods is passed as a top-level argument,\nand hence is allowed to vary per class.\nExample 40-9. decoall_meta.py\n\"Apply any decorator to all methods of a class, with a metaclass\"\nfrom types import FunctionType\nfrom decorators import tracer, timer\ndef decorateAll(decorator):\n   class MetaDecorate(type):\n       def __new__(meta, classname, supers, classdict):\n           for attr, attrval in classdict.items():\n               if type(attrval) is FunctionType:\n                   classdict[attr] = decorator(attrval)\n           return type.__new__(meta, classname, supers, classdict)\n   return MetaDecorate\nclass Person(metaclass=decorateAll(tracer)):       # Use a metaclass\n   def __init__(self, name, pay):                 # Pass any function decorator\n       self.name = name\n       self.pay  = pay\n   def giveRaise(self, percent):\n       self.pay *= (1.0 + percent)\n   def lastName(self):\n       return self.name.split()[-1]\ndef tester(aPerson):\n   sue = aPerson('Sue Jones', 100_000)\n   bob = aPerson('Bob Smith', 50_000)\n   print(f'{sue.name=}, {bob.name=}')\n   sue.giveRaise(.10) \n   print(f'{sue.pay=:,.2f}')\n   print('Last names:', sue.lastName(), bob.lastName())\nif __name__ == '__main__': tester(Person)\nWhen this code is run as is, its output traces calls to every method of the client\nclass because every method has been automatically decorated by the metaclass:\n$ python3 decoall_meta.py\ncall 1 to __init__\ncall 2 to __init__\nsue.name='Sue Jones', bob.name='Bob Smith'\ncall 1 to giveRaise\nsue.pay=110,000.00",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1666,
      "chapter": null,
      "content": "call 1 to lastName\ncall 2 to lastName\nLast names: Jones Smith\nReally, this result reflects a combination of decorator and metaclass—the\nmetaclass automatically applies the function decorator to every method at class\ncreation time, and the function decorator automatically intercepts method calls in\norder to print the trace messages in this output. The combo “just works,” thanks\nto the generality of both tools.\nTo apply a different decorator to the methods, simply replace the decorator name\nin the class header line. To use the timer function decorator shown earlier, for\nexample, use either of the last two header lines in the following for our Person\nclass—the first accepts the timer’s default arguments, and the second specifies\nlabel text (though methods may run too fast to register runtimes as is: add\ntime.sleep(seconds) calls to pause for a better time):\nclass Person(metaclass=decorateAll(tracer)):               # Apply tracer\nclass Person(metaclass=decorateAll(timer())):              # Apply timer, defaults\nclass Person(metaclass=decorateAll(timer(label='**'))):    # Decorator arguments\nAnd as you might expect by now, class decorators intersect with metaclasses\nhere, too. Example 40-10 replaces the preceding example’s metaclass with a\nclass decorator. Really, it uses a class decorator that applies a function decorator\nto each method of a decorated class. Python’s decorators naturally support\narbitrary nesting and combinations.\nExample 40-10. decoall_deco.py\n\"Apply any decorator to all methods of a class, with a decorator\"\nfrom types import FunctionType\nfrom decoall_meta import tester\nfrom decorators import tracer, timer\ndef decorateAll(decorator):\n   def DecoDecorate(aClass):\n       for attr, attrval in aClass.__dict__.items():\n           if type(attrval) is FunctionType:\n               setattr(aClass, attr, decorator(attrval))    # Not __dict__\n       return aClass\n   return DecoDecorate",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1667,
      "chapter": null,
      "content": "@decorateAll(tracer)                          # Use class deco, pass any func deco\nclass Person:                                 # Applies func decorator to methods\n   def __init__(self, name, pay):            # Person = decorateAll(..)(Person)\n       self.name = name                      # Person = DecoDecorate(Person)\n       self.pay  = pay\n   def giveRaise(self, percent):\n       self.pay *= (1.0 + percent)\n   def lastName(self):\n       return self.name.split()[-1]\nif __name__ == '__main__': tester(Person)\nWhen this code is run, the class decorator applies the tracer function decorator to\nevery method, which in turn produces a trace message on calls (the output is the\nsame as that of the preceding metaclass version of this example):\n$ python3 decoall_deco.py\n…same as Example 40-9…\nMuch as before, we simply mod the @ decorator line to apply a different\ndecorator or provide different arguments for it. Test the following on your own\nto see the effect (and again add sleep calls as needed to boost times):\n@decorateAll(tracer)                        # Apply tracer\n@decorateAll(timer())                       # Apply timer, defaults\n@decorateAll(timer(label='**'))             # Decorator arguments\n@decorateAll(timer(label='**', trace=0))    # More decorator arguments\nNotice that this class decorator returns the augmented class, not a proxy wrapper.\nAs for the metaclass version, this retains the type of the original class—an\ninstance of Person is still an instance of Person. This may matter when type\ntesting is used. The class’s methods are not their original functions because they\nare rebound to decorators, but this is likely less important in practice, and it’s\ntrue in the metaclass alternative as well.\nSo far, what we’ve seen of metaclasses makes them seem largely redundant with\ndecorators—but we have not yet seen all there is to see. As teased earlier,\nmetaclasses may also provide behavior to their instance classes by defining\nmethods, which have no direct counterpart in decorators. These methods,\nhowever, come with a twist that limits their scope. To understand both\nmetaclasses’ methods and their limiting twist, though, we first have to factor",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1668,
      "chapter": null,
      "content": "metaclasses into Python attribute resolution, a.k.a. inheritance, at large. The next\nsection takes us down this prerequisite path.\nInheritance: The Finale\nBecause metaclasses are coded in similar ways to inheritance superclasses, their\nscope can be confusing at first glance. In short, there are really two class trees\nsearched by Python inheritance—a primary tree formed by a class and that\nclass’s superclasses, along with a secondary tree formed by a class’s metaclass\nand that metaclass’s superclasses. The secondary tree is also called the “type”\ntree because it stems from type. In more detail, here is how this pans out:\nMetaclasses inherit from the type class (usually)\nAlthough they have a special role, metaclasses are coded with normal class\nstatements and follow the usual OOP model in Python. As subclasses of\ntype, they can redefine the type object’s methods to customizing classes as\nneeded. Per the prior section, metaclasses often redefine the type class’s\n__new__ and __init__ to intercept class creation and initialization.\nMetaclasses can also define methods for their instance classes and may be\nsimple functions or other callables that return arbitrary objects.\nMetaclass attributes are not acquired by class instances\nMetaclass declarations specify an instance relationship, which is not quite\nthe same as superclass inheritance. Behavior defined in a metaclass applies\nto the classes made from it but not to these classes’ own nonclass instances.\nInheritance for a nonclass instance searches only the instance and the\nprimary tree formed by its class and that class’s superclasses; the secondary\nmetaclass tree is never included in this search. Hence, nonclass instances get\nbehavior from classes and superclasses but not from metaclasses.\nMetaclass attributes are acquired by classes as a fallback",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1669,
      "chapter": null,
      "content": "By contrast, classes do acquire methods of their metaclasses by virtue of the\ninstance relationship. Metaclasses define a separate inheritance tree, which is\na source of class behavior that processes classes themselves. For classes only,\ninheritance first searches the primary tree formed by the class and its\nsuperclasses and then falls back on the secondary tree formed by the class’s\nmetaclass and its superclasses as a separate search. When a name is available\nto a class in both a metaclass and a superclass, the superclass version is used.\nMetaclass declarations are also inherited by subclasses\nThe metaclass=M declaration in a user-defined class is also inherited by the\nclass’s normal subclasses in much the same way that superclass links are\ninherited by subclasses. Thus, the metaclass will run for the construction of\neach class that inherits this specification in a superclass inheritance chain.\nThis model’s conflation of classes and metaclasses can make terminology\nchallenging, but it may be easier to understand in code than in prose. To\nillustrate the preceding points, consider the code in Example 40-11.\nExample 40-11. metainstance.py\nclass Meta(type):\n   def __new__(meta, classname, supers, classdict):        # Redefine type method\n       print('In Meta.new:', classname)\n       return type.__new__(meta, classname, supers, classdict)\n   def meth3(self):\n       return 'three!'\nclass Super(metaclass=Meta):           # Metaclass inherited by subs too\n   def meth2(self):                   # Meta run twice for two classes\n       return 'two!'\nclass Sub(Super):                      # Superclass: inheritance versus instance\n   def meth1(self):                   # Classes inherit from superclasses\n       return 'one!'                  # But not from metaclasses for instance access\nWhen this file’s code is run (as a script or module), the metaclass handles\nconstruction of both client classes. When those classes are later used, nonclass",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1670,
      "chapter": null,
      "content": "instances inherit class attributes but not metaclass attributes:\n>>> from metainstance import *         # Runs class statements: metaclass run twice!\nIn Meta.new: Super\nIn Meta.new: Sub\n>>> X = Sub()              # Nonclass instance of user-defined class\n>>> X.meth1()              # Inherited from Sub\n'one!'\n>>> X.meth2()              # Inherited from Super\n'two!'\n>>> X.meth3()              # Not inherited from metaclass!\nAttributeError: 'Sub' object has no attribute 'meth3'. Did you mean: 'meth1'?\nBy contrast, classes both inherit names from their superclasses and acquire\nnames from their metaclass—whose linkage in this example is itself inherited\nfrom a superclass by Sub:\n>>> Sub.meth1(X)           # Own method\n'one!'\n>>> Sub.meth2(X)           # Inherited from Super\n'two!'\n>>> Sub.meth3()            # Acquired from metaclass\n'three!'\n>>> Sub.meth3(X)           # Not a normal instance method!\nTypeError: Meta.meth3() takes 1 positional argument but 2 were given\nNotice how the last of the preceding calls fails when we pass in an instance\nbecause the name resolves to a metaclass method, not a normal instance method.\nIn fact, both the source of a name and the object through which you fetch it\nmatter here. Methods acquired from metaclasses are bound to the subject class,\nwhile methods from normal classes are plain functions when fetched through the\nclass but bound with an instance when fetched through the instance:\n>>> Sub.meth3\n<bound method Meta.meth3 of <class 'metainstance.Sub'>>\n>>> Sub.meth2\n<function Super.meth2 at 0x1085f7d80>\n>>> X.meth2\n<bound method Super.meth2 of <metainstance.Sub object at 0x108456420>>\nWe studied the last two of these cases before in Chapter 31’s bound-method",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1671,
      "chapter": null,
      "content": "coverage. The first case is reminiscent of Chapter 32’s class methods but is\ntechnically new here. We’ll explore class methods in more detail later. First,\nthough, to understand why metaclass methods aren’t available to normal\ninstances, we need to clarify the metaclass/superclass distinction further.\nMetaclass Versus Superclass\nIn even simpler terms, watch what happens in the following: as an instance of\nthe M metaclass type, class C acquires M’s attribute, but this attribute is not made\navailable for inheritance by C’s own instance I—the acquisition of names by\nmetaclass instances is distinct from the normal inheritance used for nonclass\ninstances:\n>>> class M(type): attr = 1\n>>> class C(metaclass=M): pass         # C is meta instance and acquires meta attr\n>>> I = C()                            # I inherits from class but not meta!\n>>> C.attr\n1\n>>> I.attr\nAttributeError: 'C' object has no attribute 'attr' \n>>> 'attr' in C.__dict__, 'attr' in M.__dict__\n(False, True)\nBy contrast, if M morphs from metaclass to superclass, then names in superclass\nS become available to later instances of C by inheritance—that is, by searching\nthe __dict__ attribute dictionaries of objects in the MRO of I’s class as usual,\nmuch like the mapattrs example we coded back in “Example: Mapping\nAttributes to Inheritance Sources” (a nonclass-instance-only tool):\n>>> class S: attr = 1\n>>> class C(S): pass                   # C is type instance and inherits from supers\n>>> I = C()                            # I inherits from class and supers\n>>> C.attr\n1\n>>> I.attr\n1\n>>> 'attr' in C.__dict__, 'attr' in S.__dict__\n(False, True)\nThis is why metaclasses often do their work by manipulating a new class’s",
      "content_length": 1700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1672,
      "chapter": null,
      "content": "namespace dictionary if they wish to influence the behavior of later instance\nobjects—instances will see names in their class but not its metaclass. Watch\nwhat happens, though, if the same name is available in both attribute sources—\nthe inheritance name in the primary superclass tree is used instead of the\ninstance acquisition name in the secondary metaclass tree:\n>>> class M(type): attr = 1\n>>> class S: attr = 2\n>>> class C(S, metaclass=M): pass       # Supers have precedence over metas\n>>> I = C()                             # Classes search supers before metas\n>>> C.attr, I.attr                      # Instances search only supers\n(2, 2)\n>>> 'attr' in C.__dict__, 'attr' in S.__dict__, 'attr' in M.__dict__\n(False, True, True)\nThis is true regardless of the relative height of the inheritance and instance\nsources—superclass inheritance always beats metaclass acquisition because\nprimary trees are searched before secondary trees:\n>>> class M(type): attr = 1\n>>> class S2: attr = 2\n>>> class S1(S2): pass\n>>> class C(S1, metaclass=M): pass      # Super two levels above meta: still wins\n>>> I = C()\n>>> C.attr, I.attr\n(2, 2)\nIn fact, classes acquire metaclass attributes through their __class__ link, in the\nsame way that nonclass instances inherit from classes through their __class__\n—which makes sense, given that classes are also instances of metaclasses. The\nchief distinction is that instance inheritance does not also follow a class’s\n__class__ but instead restricts its scope to the __dict__ of each class in the\nsuperclass tree, following __bases__ along the way:\n>>> I.__class__                # Followed by inheritance: instance's class\n<class '__main__.C'>\n>>> C.__bases__                # Followed by inheritance: class's supers\n(<class '__main__.S1'>,) \n>>> C.__class__                # Followed by instance acquisition: metaclass\n<class '__main__.M'>",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1673,
      "chapter": null,
      "content": "Really, though, Python checks the __dict__ of each class on the class’s MRO\nbefore falling back on doing the same for its metaclass—and runs the second of\nthese steps only for fetches run on classes, not nonclass instances:\n>>> [x.__name__ for x in C.__mro__]     # See Chapter 32 for all things MRO\n['C', 'S1', 'S2', 'object']\n>>> [x.__name__ for x in M.__mro__]     # Primary/secondary trees: class/meta\n['M', 'type', 'object']\nThese two MROs for class and metaclass are simply the flattened versions of\nwhat we called the primary and secondary class trees earlier. Nonclass-instance\ninheritance searches just the first, but class inheritance searches both if needed.\nIn fact, metaclasses work in much the same way, as the next section explains.\nMetaclass Inheritance\nAs it turns out, instance inheritance works in similar ways, whether the\n“instance” is a nonclass instance created from a class or a class created from a\nmetaclass derived from type. While classes straddle the primary and secondary\ntrees, both trees use the same mechanism to look up names. This yields a single\nattribute search procedure spanning two trees, which fosters the parallel notion\nof metaclass inheritance hierarchies.\nThe following demos this conceptual merger. In it, instance I inherits from all its\nclasses; class C inherits from both superclasses and metaclasses; and metaclass\nM1 inherits from higher metaclasses:\n>>> class M2(type): attr4 = 4                 # Metaclass inheritance tree\n>>> class M1(M2):   attr3 = 3                 # Gets __bases__, __class__, __mro__\n>>> class S: attr2 = 2                        # Superclass inheritance tree\n>>> class C(S, metaclass=M1): attr1 = 1       # Gets __bases__, __class__, __mro__\n>>> I = C()                                   # I gets __class__ but not others\n>>> I.attr1, I.attr2                          # Instance inherits from super tree\n(1, 2) \n>>> C.attr1, C.attr2, C.attr3, C.attr4        # Class gets names from both trees!\n(1, 2, 3, 4) \n>>> M1.attr3, M1.attr4                        # Metaclass inherits names too!\n(3, 4)\n>>> I.attr3",
      "content_length": 2081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1674,
      "chapter": null,
      "content": "AttributeError: 'C' object has no attribute 'attr3'. Did you mean: 'attr1'?\nThe failure at the end of this listing is pivotal. Both inheritance paths—class and\nmetaclass—employ the same links to build class MROs scanned by inheritance.\nBut this is not applied transitively—instances do not inherit their class’s\nmetaclass names, though they may request them explicitly:\n>>> I.__class__                # Links used at instance with no __bases__\n<class '__main__.C'>\n>>> C.__bases__\n(<class '__main__.S'>,) \n>>> C.__class__                # Links used at class after its __bases__\n<class '__main__.M1'>\n>>> M1.__bases__               # Link also used in metaclass inheritance\n(<class '__main__.M2'>,) \n>>> I.__class__.attr4          # Route inheritance to the class's meta tree\n4\n>>> I.attr4                    # Though class's __class__ not followed normally\nAttributeError: 'C' object has no attribute 'attr4'. Did you mean: 'attr1'?\nAnd as usual, both trees have flattened MROs and instance links actually used\nby inheritance:\n>>> M1.__class__                        # Metaclasses are classes too\n<class 'type'>\n>>> [x.__name__ for x in C.__mro__]     # __bases__ tree from I.__class__\n['C', 'S', 'object']\n>>> [x.__name__ for x in M1.__mro__]    # __bases__ tree from C.__class__\n['M1', 'M2', 'type', 'object']\nIf you care about metaclasses—or must use code that does—study these\nexamples carefully. In effect, nonclass instances have no __bases__ to search\nbut follow __class__ to __bases__ once, and classes follow __bases__ before\nfollowing a single __class__ to another __bases__. Because this is crucial to\nthe meaning of attribute names in Python, the next section begins making it more\nformal.\nPython Inheritance Algorithm: The Simple Version",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1675,
      "chapter": null,
      "content": "Now that we know about metaclass acquisition, we’re finally able to formalize\nthe inheritance rules that they augment. Inheritance deploys two distinct but\nsimilar lookup routines, both of which are founded on MROs computed from the\nprior section’s __bases__ links per Chapter 32. The combination yields the\nfollowing first-cut definition of Python’s attribute inheritance algorithm run for\nexplicit attribute fetches.\nTo look up an attribute name:\n1. From a nonclass instance I, search the instance, then its class, and then\nall its superclasses, using:\na. The __dict__ of the instance I\nb. The __dict__ of all classes on the __mro__ found at I’s\n__class__, from left to right\n2. From a class C, search the class, then all its superclasses, and then its\nmetaclasses tree, using:\na. The __dict__ of all classes on the __mro__ found at C itself,\nfrom left to right\nb. The __dict__ of all metaclasses on the __mro__ found at C’s\n__class__, from left to right\n3. In rules 1 and 2, also allow for these exceptions covered ahead:\nGive precedence to data descriptors present in step b sources\nSkip step a and begin the search at step b for built-in\noperations\nPerform a custom MRO search for a proxied object in super\nobjects\nRules 1 and 2 are applied for normal, explicit attribute fetch only, and there are\nexceptions for built-ins, descriptors, and super, which we’ll clarify in a moment.\nIn addition, a __getattr__ or __getattribute__ may also be used for missing\nor all names, respectively, per Chapter 38, and attribute assignment follows",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1676,
      "chapter": null,
      "content": "different rules.\nThe first two rules here, however, are the essentials. They specify the inheritance\nsearch of two separate trees, which works the same in each tree but spans trees\nfor class inheritance alone. The MRO pseudocode of Chapter 32 summarizes\nthis even more concisely:\n[I.__dict__] + [x.__dict__ for x in I.__class__.__mro__]\n[x.__dict__ for x in C.__mro__] + [x.__dict__ for x in C.__class__.__mro__]\nIn this, the first line is sources searched by rule 1 for inheritance from a nonclass\ninstance I—both the instance itself and the flattened version of the primary class\ntree. The second line is rule 2’s sources for inheritance from a class C—the\nflattened versions of both the primary class tree and the secondary metaclass\ntree.\nPut another way, nonclass instances and classes both search a class tree’s MRO\nat their __class__ as a second step, but classes first search their own MRO’s\nsuperclass tree instead of a single __dict__ namespace dictionary. In code,\nclass header lines define both of the trees searched by listing superclasses and\nspecify links to secondary trees with metaclass keywords. When omitted,\nsuperclass defaults to object and metaclass to type.\nMost programmers need only be aware of the first of these rules (1) and perhaps\nthe first step of the second (2a). There’s an extra acquisition step added for\nmetaclasses (2b), but it’s essentially the same as others—a subtle equivalence, to\nbe sure, but metaclass acquisition is not as novel as it may seem. It’s just one\nstep of the procedure.\nThe descriptors deviation\nAt least, that’s the normal—and sugarcoated—case. The prior section listed its\nexceptions separately because they don’t matter in most code and complicate the\nalgorithm substantially. First among these, inheritance also has a special-case\ncoupling with Chapter 38’s attribute descriptors. In short, data descriptors—\nthose that define __set__ methods to intercept assignments—are given\nprecedence, such that their names override other inheritance sources.",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1677,
      "chapter": null,
      "content": "This exception ostensibly serves some practical roles. For example, it is used to\nensure that the special __class__ and __dict__ attributes cannot be redefined\nby the same names in an instance’s own __dict__. In the following, these data-\ndescriptor names in the class override same names created in the instance’s\nattribute dictionary:\n>>> class C: pass                          # Inheritance special case...\n>>> I = C()                                # Class data descriptors have precedence\n>>> I.__class__, I.__dict__\n(<class '__main__.C'>, {})\n>>> I.__dict__['book'] = 'lp6e'            # Dynamic data in the instance\n>>> I.__dict__['__class__'] = 'hack'       # Assign keys, not attributes\n>>> I.__dict__['__dict__']  = {}\n>>> I.book                                 # I.name comes from I.__dict__ as usual\n'lp6e'                                     # But I.__class__ and I.__dict__ do not!\n>>> I.__class__, I.__dict__\n(<class '__main__.C'>, {'book': 'lp6e', '__class__': 'hack', '__dict__': {}})\nThis data-descriptor exception is tested before the preceding two inheritance\nrules as a preliminary step, may be more important to Python implementers than\nPython programmers, and can be reasonably ignored by most application code in\nany event—that is, unless you code data descriptors of your own, which follow\nthe same inheritance special case:\n>>> class D:\n        def __get__(self, instance, owner): print('D.__get__')\n        def __set__(self, instance, value): print('D.__set__')\n>>> class C: d = D()            # Data-descriptor attribute\n>>> I = C()\n>>> I.d                         # Inherited data descriptor access\nD.__get__ \n>>> I.d = 1\nD.__set__ \n>>> I.__dict__['d'] = 'hack'    # Define same name in instance namespace dict\n>>> I.d                         # But doesn't hide data descriptor in class!\nD.__get__\nConversely, if this descriptor did not define a __set__, the name in the\ninstance’s dictionary would hide the name in its class instead, per normal\ninheritance:",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1678,
      "chapter": null,
      "content": ">>> class D:\n        def __get__(self, instance, owner): print('D.__get__')\n>>> class C: d = D()\n>>> I = C()\n>>> I.d                         # Inherited nondata descriptor access\nD.__get__ \n>>> I.__dict__['d'] = 'hack'    # Hides class names per normal inheritance rules\n>>> I.d\n'hack'\nIn both cases, Python automatically runs the descriptor’s __get__ when it’s\nfound by inheritance rather than returning the descriptor object itself—part of\nthe implicit attribute magic we met earlier in the book. The special status\nafforded to data descriptors, however, also modifies the meaning of attribute\ninheritance and, thus, the meaning of names in your code. The next section’s\nfinal swing at a formal inheritance algorithm takes this into account.\nPython Inheritance Algorithm: The Less Simple Version\nWith both the data descriptor special case and general descriptor invocation\nfactored in with class and metaclass trees, Python’s formal inheritance algorithm\ncan be stated as follows—a complex procedure which assumes knowledge of\ndescriptors, metaclasses, and MROs but is the final arbiter of attribute names\nnonetheless.\nTo look up an attribute name:\n1. From a nonclass instance I, search the instance, its class, and its\nsuperclasses, as follows:\na. Search the __dict__ of all classes on the __mro__ found at I’s\n__class__\nb. If a data descriptor was found in step a, call its __get__ and\nexit\nc. Else, return a value in the __dict__ of the instance I\nd. Else, call a nondata descriptor or return a value found in step a\n2. From a class C, search the class, its superclasses, and its metaclasses",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1679,
      "chapter": null,
      "content": "tree as follows:\na. Search the __dict__ of all metaclasses on the __mro__ found\nat C’s __class__\nb. If a data descriptor was found in step a, call its __get__ and\nexit\nc. Else, call any descriptor or return a value in the __dict__ of a\nclass on C’s own __mro__\nd. Else, call a nondata descriptor or return a value found in step a\n3. In rules 1 and 2, built-in operations essentially use just step a sources\n(see ahead)\n4. The super built-in performs a custom MRO search for a proxied object\n(see ahead)\nSome fine print here: in this procedure, items are attempted in sequence as\nnumbered, and Python runs at most one (for instances) or two (for classes) MRO\nsearches per name lookup—the first appearance of a name in a given MRO wins,\nregardless of its kind. Because each MRO (a.k.a. __mro__) is a flattened\nrepresentation of a class tree with duplicates removed, you can also think of\nthese as one or two tree searches.\nIn addition, the implied object superclass provides some defaults at the top of\nevery class and metaclass tree (that is, at the end of every MRO). And beyond all\nthis, method __getattr__ may be run if defined when an attribute is not found,\nand method __getattribute__ may be run for every attribute fetch, though\nthey are special-case extensions to the name lookup model (really, the latter\nreplaces inheritance for the defining class’s instances, and can trigger the former\nwith an attribute exception). See Chapter 38 for more on these tools as well as\ndescriptors.\nAlso, note here again that this algorithm’s first two steps apply only to normal\nand explicit attribute fetch. The rules for attribute assignment vary; the implicit\nlookup of method names for built-ins doesn’t follow these rules in full; and the\nproxied lookup of attributes performed by super is entirely custom. The next",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1680,
      "chapter": null,
      "content": "sections cover these exceptions separately.\nThe assignment addendum\nThe prior section defines inheritance in terms of attribute reference (a.k.a. fetch\nor lookup), but parts of it apply to attribute assignment as well. As we’ve\nlearned, assignment normally changes attributes in the subject object itself, but\ninheritance is also invoked on assignment to test first for some of Chapter 38’s\nattribute management tools, including descriptors, properties, and __setattr__.\nWhen present, such tools intercept attribute assignment and may implement it\narbitrarily.\nFor example, attribute assignment always invokes a __setattr__ if present,\nmuch as a __getattribute__ is run for all references. Moreover, a data\ndescriptor with a __set__ method is acquired from a class by inheritance using\nthe MRO and has precedence over the normal storage model. In terms of the\nprior section’s rules:\nWhen applied to an instance, attribute assignments essentially follow\nsteps a through c of rule 1, searching the instance’s superclass tree,\nthough step b calls __set__ instead of __get__, and step c stops and\nstores in the instance instead of attempting a fetch.\nWhen applied to a class, attribute assignments run the same procedure\non the class’s metaclass tree: roughly the same as rule 2, but step c stops\nand stores in the class.\nBecause descriptors are also the basis for other advanced attribute tools such as\nproperties and slots, this inheritance precheck on assignment is utilized in\nmultiple contexts. The net effect is that descriptors are treated as an inheritance\nspecial case for both reference and assignment.\nThe super supplement\nEven for attribute reference, there are two special cases that are exempt from\ninheritance’s normal rules. For one, the super built-in function we studied in\nChapter 32 does not use normal inheritance.\nAs we learned, for the proxy objects returned by super, attributes are resolved",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1681,
      "chapter": null,
      "content": "by a special context-sensitive scan of a limited tail portion of a different object’s\nMRO. This custom scan searches the MRO of an implicit instance’s class,\nchoosing the first descriptor or value found in a class following the class\ncontaining the super call. This scan is used instead of running full inheritance—\nwhich is used on the super object itself only if this special-case scan fails.\nSee Chapter 32 for more coverage of super. While this built-in may be\nconvenient in some simple roles, it comes with substantial complexity in others\nand adds a convoluting footnote to inheritance itself.\nThe built-ins bifurcation\nAs we’ve also learned, other built-ins don’t follow inheritance’s normal rules\neither. Instances and classes may both be skipped for the implicit method-name\nfetches of built-in operations, as a special case that differs from explicit name\nreferences. Because this is a context-specific divergence, it’s easier to\ndemonstrate in code than to weave it into a single algorithm. In the following,\nstr is the built-in, __str__ is its explicit-name equivalent, and the nonclass\ninstance is inconsistently skipped by the built-in only:\n>>> class C:                              # Inheritance special case...\n        attr = 1                          # Built-ins skip a step\n        def __str__(self): return('class')\n>>> I = C()\n>>> I.__str__(), str(I)                   # Both from class if not in instance\n('class', 'class')\n>>> I.__str__ = lambda: 'instance'\n>>> I.__str__(), str(I)                   # Explicit=>instance, built-in=>class!\n('instance', 'class')\n>>> I.attr                                # Asymmetric with normal or explicit names\n1\n>>> I.attr = 2; I.attr\n2\nAs you may expect by now, the same holds true for classes—explicit names start\nat the class, but built-ins start at the class’s class—which is its metaclass, and\ndefaults to type, which provides access to an implicit default:",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1682,
      "chapter": null,
      "content": ">>> class D(type):\n        def __str__(self): return('D class')\n>>> class C(D):\n        pass\n>>> C.__str__(C), str(C)                  # Explicit=>super, built-in=>metaclass!\n('D class', \"<class '__main__.C'>\")\n>>> class C(D):\n        def __str__(self): return('C class')\n>>> C.__str__(C), str(C)                  # Explicit=>class, built-in=>metaclass!\n('C class', \"<class '__main__.C'>\")\n>>> class C(metaclass=D):\n        def __str__(self): return('C class')\n>>> C.__str__(C), str(C)                  # Built-in=>user-defined metaclass\n('C class', 'D class')\nIn fact, it can sometimes be nontrivial to know where a name comes from in this\nmodel since all classes in both trees also inherit from object—including the\ndefault type metaclass. In the following’s explicit call, C gets a default __str__\nfrom object instead of the metaclass, per the first source of class inheritance\n(the class’s own MRO, which is the primary tree); by contrast, the str built-in\nskips ahead to the metaclass as before:\n>>> class C(metaclass=D):\n        pass\n>>> C.__str__(C), str(C)                  # Explicit=>object, built-in=>metaclass\n(\"<class '__main__.C'>\", 'D class')\n>>> C.__str__\n<slot wrapper '__str__' of 'object' objects>\n>>> for k in (C, C.__class__, type): \n        print([x.__name__ for x in k.__mro__])\n['C', 'object']\n['D', 'type', 'object']\n['type', 'object']\nThis is why we’ve gone to such great lengths to root out the full specs of\ninheritance. While some code may never need to care about all its many plot\ntwists, attribute inheritance is clearly a convoluted business in Python, and\nuncertainty is not generally compatible with engineering.",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1683,
      "chapter": null,
      "content": "The Inheritance Wrap-Up\nAnd with all those details in hand, you finally have the complete Python\ninheritance story—or at least as much as we can cover in this text. It’s a tangled\ntale that today spans instances, classes, superclasses, metaclasses, descriptors,\nsuper, built-ins, and MROs, and all for the sake of looking up a simple attribute\nname.\nSome practical needs warrant exceptions, of course, but you should carefully\nconsider the implications of an object-oriented language that applies inheritance\n—its foundational operation—in such a labyrinthian fashion. At a minimum,\nthis should underscore the importance of keeping your own code simple to avoid\nmaking it dependent on the darker corners of such convoluted rules. As always,\nyour code’s users and maintainers will be glad you did.\nFor more fidelity on this story, see Python’s internal implementation of\ninheritance—a low-level but complete saga chronicled today in its files object.c\nand typeobject.c, the former for normal instances and the latter for classes.\nDelving into internals shouldn’t be required to use Python, but it’s the ultimate\nand sometimes sole source of truth in a complex and perpetually changing\nsystem. This is especially true in boundary cases born of accrued exceptions that\nraise the bar for learners and users, a downside we’ll revisit briefly in the next\nand closing chapter.\nFor now, let’s move on to one last bit of metaclass “magic”—its methods, which\nrely on its inheritance offshoot.\nMetaclass Methods\nNow that we have a handle on the way that metaclasses modify the inheritance\nof names, we can finally turn to their methods with full clarity. In short, methods\nin metaclasses are inherited by and process their instance classes—instead of the\nnonclass instances that classes make.\nThis makes metaclass methods similar in form and function to the class methods\nwe studied in Chapter 32, though their class-focused behavior is automatic.\nMoreover, they are available only to classes due to the last section’s inheritance\nrules—the limiting “twist” alluded to earlier. Metaclass methods also have no",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1684,
      "chapter": null,
      "content": "direct analog in class decorators, though decorators’ freedom to return arbitrary\nobjects makes nearly anything possible with imagination.\nTo demo the basics, the following’s metaclass defines a method made available\nto its class’s instances by metaclass acquisition (i.e., by inheritance from the\nsecondary metaclass tree):\n>>> class M(type):\n        def z(cls):  print('M.z', cls)          # A metaclass: instances=classes\n        def y(cls):  print('M.y', cls)          # y is overridden by instance C\n>>> class C(metaclass=M):\n        def y(self): print('C.y', self)         # A simple class: nonclass instances\n        def x(self): print('C.x', self)         # Namespace dict holds x and y\nMethods fetched from the class are plain functions as usual, but those from a\nmetaclass are automatically bound to the class from which they were fetched, as\nwe saw in an earlier example:\n>>> C.x                                       \n<function C.x at 0x10eaf4b80> \n>>> C.y                                         # x and y defined in class itself\n<function C.y at 0x10eaf4ae0> \n>>> C.z                                         # z acquired from metaclass\n<bound method M.z of <class '__main__.C'>> \n>>> C.z()                                       # Metaclass method call: gets class\nM.z <class '__main__.C'>\nAs we’ve also seen, methods fetched through a nonclass instance are bound to\nthe instance unless classmethod or staticmethod are used, though such\ninstances are moot here because they do not inherit metaclass names:\n>>> I = C()\n>>> I.x()                                       # Instance method calls: get inst\nC.x <__main__.C object at 0x10e9d1400> \n>>> I.y()\nC.y <__main__.C object at 0x10e9d1400>\n>>> I.z()                                       # Instance doesn't see meta names\nAttributeError: 'C' object has no attribute 'z'\n>>> I.x\n<bound method C.x of <__main__.C object at 0x10e9d1400>>\nThe only real new things about metaclass methods is that they are inherited only",
      "content_length": 1977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1685,
      "chapter": null,
      "content": "by classes and are bound to a class automatically. The latter half of this is\nexplored in the next section.\nMetaclass Methods Versus Class Methods\nThough they differ in inheritance visibility, metaclass methods are designed to\nmanage class-level data, just like the class methods we studied in Chapter 32. In\nfact, their roles can overlap—much as metaclasses can in general with class\ndecorators—but metaclass methods are not accessible except through the class\nand do not require an explicit classmethod class-level declaration in order to be\nbound with the class.\nIn other words, metaclass methods can be thought of as implicit class methods,\nwith limited visibility:\n>>> class M(type):\n        def a(cls):                        # Metaclass method: gets class\n            cls.x = cls.y + cls.z\n>>> class C(metaclass=M):\n         y, z = 11, 22\n         @classmethod                      # Class method: gets class\n         def b(cls):\n             return cls.x\n>>> C.a()            # Call metaclass method; visible to class only\n>>> C.x              # Creates class data on C, accessible to normal instances\n33\n>>> I = C()\n>>> I.x, I.y, I.z\n(33, 11, 22)\n>>> I.b()            # Class method: sends class, not instance; visible to instance\n33\n>>> I.a()            # Metaclass methods: accessible through class only\nAttributeError: 'C' object has no attribute 'a'\nThis yields two very different ways to create class methods whose asymmetry is\nleft as suggested pondering here.\nNOTE",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1686,
      "chapter": null,
      "content": "Plus one more: Python 3.6 added an operator-overloading method named\n__init_subclass__. This method is called whenever the containing class is subclassed, and it\nreceives a single argument: the new subclass object. It provides yet another way to manage\nclasses, though this method is nowhere near as general as class decorators or metaclasses: it’s\nrun only for a class’s own subclasses and only when those subclasses are created. Similar to\nmetaclass methods, this method is also implicitly converted to a class method. Because we just\ncan’t get enough of those implicit hooks!\nOperator Overloading in Metaclass Methods\nJust in case your head is not yet spinning, metaclasses, just like normal classes,\nmay also employ operator overloading to make built-in operations applicable to\ntheir instance classes. The __getitem__ indexing method in the following\nmetaclass, for example, is a metaclass method designed to process classes\nthemselves—the classes that are instances of the metaclass, not those classes’\nown nonclass instances:\n>>> class M(type):\n        def __getitem__(cls, i):         # Meta method for processing classes:\n            return cls.data[i]           #  Built-ins skip class, use meta\n                                         #  Explicit names search class + meta\n>>> class C(metaclass=M):                #  Data descriptors in meta used first\n        data = 'hack'\n>>> C[0]                  # Metaclass instance names: visible to class only\n'h'\n>>> C.__getitem__\n<bound method M.__getitem__ of <class '__main__.C'>>\n>>> I = C()\n>>> I.data, C.data        # Normal inheritance names: visible to instance and class\n('hack', 'hack') \n>>> I[0]\nTypeError: 'C' object is not subscriptable\nAnd if that’s not abstruse enough, when both class and metaclass overload the\nsame operator, the former applies to classes and the latter to nonclass instances:\n>>> class C(metaclass=M):\n        data = 'hack'\n        def __getitem__(self, i):\n            return self.data[i]",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1687,
      "chapter": null,
      "content": ">>> I = C()\n>>> I.data = 'code'\n>>> C[0], I[0]            # C's [] from metaclass, I's [] from class \n('h', 'c')\nAll of which leads us to the closing compare-and-contrast of the next section.\nMetaclass Methods Versus Instance Methods\nThe inescapable conclusion of this technical novel is that metaclass methods\nprovide a separate—and arguably redundant—way to code object behavior with\nclasses in Python. As we’ve learned, the usual way to implement objects is with\nclasses and their nonclass instances. Here’s the normal case—with class data,\nconstructor, regular method, and operator overloading available to instances and\nsubclasses:\n>>> class Employee:                               # A \"normal\" class\n        rate = 50                                 # With shared class data\n        def __init__(self, name, hours):          # Plus instance data and methods\n            self.name = name\n            self.hours = hours\n        def pay(self):\n            print(f' \n {self.name} \n ${self.rate * self.hours:,.2f}')\n        def __iadd__(self, hours):\n            self.hours += hours\n            return self\nAs usual, we make an instance of a normal class like this by calling it with\nconstructor arguments:\n>>> pat = Employee('Pat', 2_000)                  # A \"normal\" nonclass instance\n>>> pat.pay()                                     # Methods inherited from class\n Pat \n $100,000.00\n>>> pat += 1_000                                  # Updates instance data\n>>> pat.pay()\n Pat \n $150,000.00\nAnd to make more instances, we simply call the class again:\n>>> pat2 = Employee('Pat2', 1_000)                # Another \"normal\" instance\n>>> pat2.pay()\n Pat2 \n $50,000.00\n>>> pat2",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1688,
      "chapter": null,
      "content": "<__main__.Employee object at 0x10cf5c680>\nThe equivalent metaclass can also provide data, regular methods, and operator\noverloading. But per-instance constructors don’t quite apply; methods receive\nand process a class, not instances of it; and this behavior is for subclasses and\ninstance classes in the secondary “type” tree only:\n>>> class MetaEmployee(type):                     # A metaclass class\n        rate = 50                                 # With metaclass data and methods\n        def pay(cls):\n            print(f'\n {cls.name} \n ${cls.rate * cls.hours:,.2f}')\n        def __iadd__(cls, hours):\n            cls.hours += hours\n            return cls\nBecause metaclass instances are classes, we define a new class per instance\ninstead of running a call and code instance data as class attributes, though all the\nbehavior defined in the metaclass applies to its instance classes:\n>>> class pat(metaclass=MetaEmployee):            # A metaclass instance: class\n        name = 'Pat'                              # With class data per instance\n        hours = 2_000\n \n>>> pat.pay()                                     # Methods inherited from metaclass\n Pat \n $100,000.00\n>>> pat += 1_000                                  # Updates class data\n>>> pat.pay()\n Pat \n $150,000.00\nWe haven’t made any normal instances here—just a class that nevertheless\nserves as an information record with both data and behavior methods. Coding\ndifferences aside, the main functional divergence in the metaclass approach is\nthat nonclass instances created from such a class don’t inherit any of the\nbehavior defined in a metaclass:\n>>> pat2 = pat()                                  # A \"normal\" instance of class pat\n>>> pat2.pay()                                    # Metaclass behavior not available\nAttributeError: 'pat' object has no attribute 'pay'\nTo make additional instances in the metaclass world, we must instead code\nadditional classes:",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1689,
      "chapter": null,
      "content": ">>> class pat2(metaclass=MetaEmployee):           # Metaclass instances are classes\n        name = 'Pat2'\n        hours = 1_000\n \n>>> pat2.pay()                                    # With all metaclass data+methods\n Pat2 \n $50,000.00\nCalling the metaclass with no arguments doesn’t work at all because it makes a\nclass, not an instance of a class—though calling the metaclass with full type\narguments does work because it’s equivalent to running a class statement (and\nyes, the fact that metaclasses are classes, too, makes some of this paragraph’s\nlogic circular, but that’s just how Python works: metaclasses are special-cased\nfor metaclass roles):\n>>> pat2 = MetaEmployee('pat2', (), dict(name='Pat2', hours=1_000))\n>>> pat2.pay()\n Pat2 \n $50,000.00\n>>> pat2\n<class '__main__.pat2'>\nIn sum, metaclasses allow us to implement classes that work much like the\nnonclass instances we’ve used throughout this book. As to why you’d want to\nswap one kind of instance for another with identical functionality but different\ncoding, we’ll have to defer to other resources to justify the redundancy. Given\nthe many convolutions that metaclasses bring to the table, it’s difficult not to see\nthis as complexity for all in the name of rare and narrow roles. While that has\nbeen a recurring theme in both Python and this book, it’s time to wrap up and\nmove on to the conclusion.",
      "content_length": 1365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1690,
      "chapter": null,
      "content": "Chapter Summary\nIn this chapter, we studied metaclasses, explored examples of them in action, and\nformalized the rules of inheritance that they extend. Along the way, we also saw\nhow the roles of class decorators and metaclasses often intersect: because both\nrun at the conclusion of a class statement, they can sometimes be used\ninterchangeably. We also learned how metaclass methods define behavior for\nclasses much like the class methods we met earlier, but are limited in scope to\nthe secondary inheritance tree searched for classes.\nSince this chapter covered an advanced topic, we’ll work through just a few quiz\nquestions to review the basics (candidly, if you’ve made it this far in a chapter on\nmetaclasses, you probably already deserve extra credit!). Because this is the last\npart of the book, we’ll also forgo the end-of-part exercises. Be sure to see the\nappendixes that follow for Python platform pointers and the solutions to the prior\nparts’ exercises; the last of these includes a sampling of short application-level\nprograms for self-study.\nOnce you finish the quiz, you’ve officially reached the end of this book’s\ntechnical material. The next and final chapter offers some brief thoughts about\nPython to wrap up the book at large and closes with some fun. We’ll regroup\nthere in the Learning Python benediction after you work through this final quiz.\nTest Your Knowledge: Quiz\n1. What is a metaclass?\n2. How do you declare the metaclass of a class?\n3. How do class decorators overlap with metaclasses for managing\nclasses?\nTest Your Knowledge: Answers\n1. A metaclass is a class used to create a class. Classes are instances of the",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1691,
      "chapter": null,
      "content": "type class by default. Metaclasses are usually subclasses of the type\nclass, which customize classes. They may redefine class-creation\nprotocol methods like __new__ and __init__to customize the class\ncreation call issued at the end of a class statement. They may also\ndefine data and methods that provide behavior for their instance classes,\nbut these are inherited only by classes, not their nonclass instances.\nMetaclasses can also be coded in other ways—as simple functions, for\nexample—but they are responsible for making and returning an object\nfor the new class. Finally, metaclasses constitute a secondary pathway\nfor inheritance search used for classes alone; this allows classes to ape\nnormal instance behavior, though it bifurcates the class story for\nrelatively rare and narrow roles.\n2. Use a keyword argument in the class header line: class\nC(metaclass=M). The class header line can also name normal\nsuperclasses before the metaclass keyword argument; these\nsuperclasses are searched before metaclasses for inheritance run from a\nclass.\n3. Because both are automatically triggered at the end of a class\nstatement, class decorators and metaclasses can both be used to manage\nclasses. Decorators rebind a class name to a callable’s result, and\nmetaclasses route class creation through a callable, but both hooks can\nbe used for similar purposes. To manage classes, decorators simply\naugment and return the original class objects. Metaclasses augment a\nclass after or before they create it. As noted, metaclasses can also\nprovide behavior for their instance classes in the form of methods that\nare not immediately supported by decorators, though the objects\nreturned by decorators can do anything supported by the Python\nlanguage.",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1692,
      "chapter": null,
      "content": "Chapter 41. All Good Things\nWelcome to the end of the book! Now that you’ve made it this far, this chapter\nsays a few words in closing about Python’s evolution before turning you loose\non the software field and then wraps up with a bit of fun.\nYou’ve now had a chance to see the entire Python language yourself—including\nsome advanced features that may seem at odds with a scripting language meant\nto be accessible to nonprofessionals. Though many users will understandably\naccept this as status quo, in an open source project, it’s crucial that some ask the\n“why” questions too. Ultimately, the trajectory of the Python story—and its true\nconclusion—is at least in part up to you.\nToward that end, this chapter begins by calling out what may be one of Python’s\nbiggest downsides: its rate of change. This topic is unavoidably subjective, and\nyou should weigh its coverage here on whatever scale you bring to the table.\nThe Python Tsunami\nTwelve years ago, this book warned that Python was growing too convoluted and\nbloated—and then Python grew a lot more convoluted and bloated. Clearly, this\nmessage has not reached those behind the convoluting and bloating.\nEven so, this stuff still matters. To parrot the Preface, the last dozen years have\nhosted the rise of f-string literals, named-assignment expressions, match\nstatements, type hinting, async coroutines, dictionary union, star-unpacking\nproliferation, underscore digit separators, module attribute hooks, exception\ngroups, dictionary-key insertion order, positional-only function arguments, hash-\nbased bytecode files, the sys.executable snub, and other superfluous\nadditions, opinionated deprecations, and tangled mutations we’ve met along the\nway.\nMoreover, this tsunami of mods simply added to the flood of complexity and\nredundancy that came before it—including the oddly artificial MRO, the\nstunningly implicit super, and the horrifically convoluted inheritance algorithm",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1693,
      "chapter": null,
      "content": "of the preceding chapter, which elevates metaclasses and descriptors to\nprerequisites. The sum of these is an esoteric morass, which obfuscates the\nfundamental meaning of names in Python.\nMost of what should be said about these changes already has been said in this\nbook, but their combined weight qualifies as problematic for a tool that promotes\nitself as simpler than others. To illustrate, Table 41-1 updates the prior edition’s\naccounting of Python’s largely unchecked growth so far—a partial but\nrepresentative tally.\nTable 41-1. A sampling of redundancy and tool explosion in Python\nCategory\nMembers\n3 major paradigms\nProcedural, functional, object-oriented\n4 string-formatting\ntools\n% expression, str.format, string.Template, f-strings\n4 attribute-accessor\ntools\nproperties, descriptors, __getattr__, __getattribute__\n2 finalization\nstatements\ntry/finally, with plus context managers\n4 varieties of\ncomprehension\nList, set, dictionary, generator\n3 class-augmentation\noptions\nManual rebinding, @ decorators, metaclasses\n4 kinds of methods\nInstance, static, class, metaclass\n2 attribute-storage\nsystems\n__dict__, __slots__\n4 flavors of imports\nModule, package, package relative, namespace\npackage",
      "content_length": 1202,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1694,
      "chapter": null,
      "content": "2 superclass-reference\ntools\nExplicit class names, super plus MRO\n6 assignment forms\nBasic, sequence, multitarget, += augmented, *\nunpacking, := named\n3 types of functions\nBasic, yield generator, async coroutine\n6 function-argument\nforms\nBasic, name=X, *X, **X, keyword-only, positional-only\n2 class-behavior\nsources\nSuperclasses, metaclasses\n3 multiple-choice tools\nif/elif, dictionary indexing, match\n4 state-retention\noptions\nClasses, closures, function attributes, mutables\n2 bytecode storage\nschemes\ntimestamp, hashkey\n3 name-string\nimporters\nexec, __import__, importlib\n2 kinds of decorators\nFunction, class\n4 dictionary-merge\noptions\nfor loops, update method, *D unpacking, | union\noperator\n2 exception-handler\nmodels\nexcept singles, except* groups\n4 statement-aping\nexpressions\nif/else, comprehensions, lambda functions, :=\nassignment\n8 starred\ncollectors/unpackers\nAssignment; function header, call; list, tuple, dict,\nset literal; match",
      "content_length": 946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1695,
      "chapter": null,
      "content": "If you care about Python, you should take a moment to browse this table. It\nreflects a virtual explosion of bifurcation, redundancy, and toolbox size—and 81\nconcepts that can all be required reading for both newcomers learning the\nlanguage and experts reusing code written by others. Most of its categories\nbegan with just one original member in Python; many were added in part to\nimitate other languages; and none can be simplified today by pretending that the\nlatest Python is the only Python that matters. Python 3.X now owns the flux in\nfull.\nF-strings are a prime example in this category. This book’s prior edition\nlamented the three redundant string-formatting tools of its time, but this set was\nsubsequently expanded to a colossal four. While f-strings may be deemed a\nrefinement by some, in truth they are a minor variation on a theme that adds yet\nanother topic to the heap. More fundamentally, millions of programmers have\nwritten millions of programs using longer-lived options; while new code has the\nluxury of using new tools, pretending that the past didn’t happen constitutes a\nbreak with reality.\nEven extensions perhaps more unique often come loaded with surplus\ncomplexity. The match statement, for example, couldn’t simply provide a\npotentially useful multiple-choice option. It had to bolt on the conceptually\ntortuous and syntactically ad hoc structural pattern matching, which seems an\nanswer to a question that nobody asked.\nNor are additions the only user-unfriendly theme in Python. Its subjective\nchanges and deprecations are now so common that they must be expected as an\nimplicit cost of using the language. To be clear, your Python code will almost\ncertainly break eventually when you upgrade to a new Python release—and only\nbecause a Python core developer’s whim was made mandatory for everyone else.\nThe Python Sandbox\nAll of this stems from the fact that Python is, and probably always has been, a\nconstantly morphing sandbox of ideas, which prioritizes its developers’ egos\nover its users’ needs. Playing in a sandbox can be fun, of course, but it’s lousy\nfor the millions of people downstream from its churn. These people are simply\ntrying to write software that’s reliable and durable. Like all engineering",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1696,
      "chapter": null,
      "content": "endeavors, that works best with a stable base, not constantly shifting sand.\nAs a pathological example, Python’s sandbox model seems to have hit its zenith\nin type hinting—the optional, unused, out-of-place, and embarrassingly\nacademic subdomain we glanced in Chapter 6 but largely omitted here by\ndesign. This is unchecked convolution on parade, and leaves Python users to\npuzzle over the paradox of pointless type declarations in a dynamically typed\nlanguage. Sadly, it’s also likely to appeal to control freaks.\nAs we’ve also seen regularly in this book, because the sandbox is oriented\ntoward experts, it inevitably produces tools that assume that you have to already\nbe an expert to use them. Classes and OOP, for example, are required skills for\neven simple exception handling. Python is not just for its developers, but the\nforward knowledge assumptions of many of its additions embed this message\nand raise the bar for newcomers unnecessarily and unkindly.\nTo be fair, it’s not just Python: the entire software field is permeated by a culture\nof change in which churn is an expected constant, and prowess often consists of\nflaunting the latest and greatest tools even when they are unwarranted. This\ndoesn’t prove intelligence (and often demos its absence), but annual and\nmandatory mods are now a norm. Whether one breaks your PC, smartphone, or\nPython code, it’s difficult not to see this as divisive and rude.\nThe Python Upside\nAll that being said, it’s also difficult to deny that Python, despite its warts, is still\nmore productive and pleasant to use than other programming languages. If\nyou’ve coded other languages, you know that many come laden with extraneous\nsyntax and rules, which seem to reflect an assumption that programmers cannot\nbe trusted to do their jobs. By sharp contrast, Python’s dynamic typing and\ninnate flexibility make it more ally than obstacle.\nPython’s rise in popularity seems to attest to this value proposition, though it’s\nimpetus may be more practical than academic. Today’s larger Python world may\nnaturally be less concerned with the language’s original and perhaps idealistic\ngoals than with solving concrete problems. In the real world that hosts Python\npopularity, arcane language topics usually take a back seat to libraries, platforms,\nand schedules—calls that Python has always answered in full.",
      "content_length": 2348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1697,
      "chapter": null,
      "content": "Moreover, some change and complexity is warranted in software. Programming\nis a substantially challenging task (despite what you may have heard), and\ncomputer science is a field still young enough to be excused for some youthful\nthrashing. For Python specifically, though, complexity and thrashing should be\nmodulated by broad appeal.\nIn the end, the Python language remains a remarkably expressive tool that still\nfits both programming tasks and your brain as well as it ever did. Especially if\nyou stick to its tried-and-true parts that propelled Python to the top of the\nlanguage charts, you’ll likely find it an enabling technology that makes coding\nas much fun as chore.\nPrudent engineers, though, would do well to exercise caution when upgrading to\nthe leading edge, and give a pass to the sandbox’s annual outflow except when\nclearly beneficial. Given that this is now an industry-wide requirement, it’s\nhardly cause to dismiss an otherwise useful tool. Bad practice, however, does not\njustify bad practice.\nClosing Thoughts\nSo there you have it: some observations from the trenches, born of three decades\nusing, teaching, and advocating Python, and grounded in a desire to improve the\nPython story.\nNone of these concerns are entirely new. Indeed, the growth of this very book\nover the years seems a testament to that of Python itself—if not an ironic eulogy\nto a mission statement that once stressed simplification of programming, and\naccessibility to both experts and nonprofessionals alike. Judging by language\nheft alone, that dream seems to have been either neglected over time or\nabandoned entirely.\nBut we can do better. A well-established tool like Python can afford to focus\nmore on its users’ needs than its changers’ hubris. Per the old adage, we simply\nhave to stop fixing what isn’t broken. If we can, it will go far toward addressing\nthe concerns of those vetting the language for projects that cannot afford to\nbudget for shifting sands.\nMore importantly, in an open source project like Python the answers to such",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1698,
      "chapter": null,
      "content": "questions must be formed anew by each wave of newcomers. Hopefully, the\nwave you ride in will have as much common sense as fun while plotting\nPython’s future.\nWhere to Go from Here\nAnd that’s a wrap, folks. You’ve officially reached the end of this book. Now\nthat you know Python inside and out, your next step, should you choose to take\nit, is to explore the libraries, techniques, and tools available in the application\ndomains in which you will work.\nBecause Python is so widely used, you’ll find ample resources for using it in\nalmost any domain you can think of—from GUIs, the web, and apps, to numeric\nprogramming, databases, and system administration. See Chapter 1 and your\nfavorite web browser for pointers to popular tools and topics.\nThis is where Python starts to become truly fun, but this is also where this book’s\nstory ends, and those of other resources begin. Good luck with your journey.\nAnd as always: code well!\nEncore: Print Your Own Completion Certificate!\nAnd one last thing: in lieu of exercises for this part of the book, Example 41-1\nlists a bonus script for you to study and run on your own. A book can’t directly\nprovide completion certificates for its readers (and the certificates would be\nworthless if it could), but it can include an arguably cheesy Python script that\ndoes. This one creates a simple book completion certificate in both plain-text\nand HTML files, and auto-opens them in a web browser or other viewer where\nsupported.\nExample 41-1. You-made-it.py\n\"\"\"\nGenerate a simple class completion certificate: printed to\nthe console, and saved in auto-opened text and HTML files.\nRun from a console, and print saved output files if desired.\nWorks on all PCs, but may require manual opens on smartphones.\n\"\"\"\nimport time, sys, html, os",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1699,
      "chapter": null,
      "content": "maxline = 60                               # Text separator lines\nbrowser = True                             # Display in a browser?\nsaveto  = 'Certificate'                    # Output filenames prefix\n# Template values\nSEPT = '*' * maxline\nDATE = time.strftime('%A, %b %d, %Y, %I:%M %p')\nNAME = input('Please enter your name: ').strip() or 'An unknown reader'\nBOOK = 'Learning Python, 6th Edition'\nSITE = 'https://learning-python.com'       # For icon, image, link\n# F-string templates work for preset in-code references\ntexttext = f\"\"\"\n{SEPT}\n  Official Certificate \n \nDate: {DATE}\nThis certifies that:\n\\t{NAME}\nHas survived the massive tome:\n\\t{BOOK}\nAnd is now entitled to all privileges thereof, including\nthe right to proceed on to learning how to develop websites,\ndesktop GUIs, scientific models, smartphone apps, and \nanything else that the future of computing may hold. \n--Your humble \n instructor\n(Note: void where obtained by skipping ahead.)\n{SEPT}\n\"\"\"\n# Interact, setup\nfor c in 'Congratulations!'.upper() + '\n' * 3:\n   print(c, end=' ')\n   sys.stdout.flush()    # Else some shells wait for \\n\n   time.sleep(0.25)      # Reveal message slowly for fun\nprint(); time.sleep(3)\n# Make text-file version\ntextto = saveto + '.txt'\nfileto = open(textto, 'w', encoding='utf8')",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1700,
      "chapter": null,
      "content": "print(texttext, file=fileto)\nfileto.close()\n# Start HTML: replace text markers with tags\nhtmltext = texttext.replace(SEPT,   '<div class=cert>', 1)\nhtmltext = htmltext.replace(SEPT,   '</div>')\nhtmltext = htmltext.replace('\n  ', '<h1 align=center>\n &nbsp;', 1)\nhtmltext = htmltext.replace(' \n ', '&nbsp;\n </h1>')\n# Line-by-line mods\nlinemods = []\nfor line in htmltext.split('\\n'):\n   if line == '':\n       line = '<p>'\n   elif line[:1] == '\\t':\n       line = f\"<i>{'&nbsp;' * 4}{html.escape(line[1:])}</i>\"   # 3.6+\n   linemods.append(line)\nhtmltext = '\\n'.join(linemods)\n    \n# Ignorable HTML bits (mind the {{ and }} escapes)\npreamble = f'''<!doctype html>\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<link rel=\"icon\" type=\"image/x-icon\" href=\"{SITE}/favicon.ico\">\n<style> \nbody  {{font-family: Arial, Helvetica, sans-serif;}} \n.cert {{background-color: cornsilk; padding: 16px; border: medium solid black;}}\n</style>\n<title>LP6E Completion Certificate</title>\n</head>\n'''\nimage, page = 'lp6e-large.jpg', 'about-lp.html'\nfooter = f'''\n<table><tr>\n<td><a href=\"{SITE}/{page}\"><img src=\"{SITE}/{image}\" hspace=10 height=50></a>\n<td><a href=\"{SITE}/{page}\" align=center><i>Book support site</i></a>\n</tr></table>\n'''\n# Put it all together\nhtmltext = f'{preamble}<body bgcolor=\"#eee\">{htmltext}{footer}</body></html>'\n# Make HTML-file version\nhtmlto = saveto + '.html'\nfileto = open(htmlto, 'w', encoding='utf8')\nprint(htmltext, file=fileto)",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1701,
      "chapter": null,
      "content": "fileto.close()\n# Display text results in console\nprint(f'[File: {textto}]', end='')\nprint('\\n' * 2, open(textto, encoding='utf8').read())\n# Open docs (may also fail silently)\nif browser:\n   try:\n       import webbrowser\n       for doc in (textto, htmlto):\n           webbrowser.open('file://' + os.path.abspath(doc))\n   except Exception:\n       print('Unable to auto-open docs: open manually.')\ninput('[Press Enter to close]')    # Keep window open if clicked\nRun this script in a console or other interface on your own, and study its code for\na review of some of the ideas we’ve covered in this book. Copy/paste from\nemedia or fetch it from this book’s examples package as described in the\nPreface, but ignore its undocumented and out-of-scope HTML bits if you’re not\na web developer. You won’t find any descriptors, decorators, metaclasses, or\nsuper calls in this code, but it’s typical Python nonetheless:\n$ python3 You-made-it.py \nPlease enter your name: Some Body\nC O N G R A T U L A T I O N S ! \n \n \n \n…etc…\nThis script works in full on PCs (where code might also open files with\nos.startfile, or “open” or “xdg-open” commands in os.system), but on\nsmartphones you’ll probably need to open the output files manually in file-\nexplorer apps. When run, it generates the web page captured in the fully\ngratuitous Figure 41-1. This could be much more grandiose, of course; see the\nweb for pointers to Python support for PDFs and other document tools such as\nSphinx surveyed in Chapter 15. But hey—if you’ve made it to the end of this\nbook, you deserve another joke or two.",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1702,
      "chapter": null,
      "content": "Figure 41-1. HTML doc created and opened by You-made-it.py",
      "content_length": 58,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1703,
      "chapter": null,
      "content": "Part IX. Appendixes",
      "content_length": 19,
      "extraction_method": "OCR"
    },
    {
      "page_number": 1704,
      "chapter": null,
      "content": "Appendix A. Platform Usage Tips\nOne of Python’s main strengths is its portability: most of the code you’ll write in\nyour Python programs will work the same on all computing platforms. This has\nlimits, of course; programs that use Python’s portable libraries weather device\nhops better than others, and platform idiosyncrasies and restrictions can\nsometimes pose interoperability hurdles that require special handling. But by and\nlarge, the Python language is cross-platform by design.\nPython’s portability means you can run its code on just about every computing\ndevice on the planet—from smartphones and tablets to PCs and supercomputers\n—and each of these systems has unique setup and usage details. While this\nappendix cannot be an exhaustive user guide for every one of those devices, it\nprovides just enough info to help you prepare to run this book’s code examples\non your popular platform—or platforms—of choice, including Windows,\nmacOS, Linux, Android, and iOS.\nBefore we get started, here are two quick content notes up front. First, because\nyou’re going to have enough on your plate just learning Python itself, the focus\nthroughout this appendix is on keeping it simple. There are many ways to run\ncode, and you may find advanced options useful once you graduate from Python\nnovice to master. Especially when starting out, though, this book recommends\nwalking the easiest path.\nSecond, a usage appendix like this is unavoidably doomed to grow out of date\nsoon, given the rapid and constant change in the computing world (even the\nname of macOS, after all, has changed repeatedly on this book’s watch!). Hence,\nplease consider this a snapshot of the current state and practice, and plan to\nconsult the latest resources if when this story changes. For the present, let’s jump\nright into today’s usage options—while they last.\nUsing Python on Windows\nAs the current market-share leader for PCs, Windows will undoubtedly play host\nto many readers’ first encounter with Python. Python has been completely usable",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1705,
      "chapter": null,
      "content": "on this platform since its earliest days, and goes out of its way to smooth\nWindows’ proprietary edges so your code doesn’t have to. A well-coded Python\nprogram from Unix, for example, often runs unchanged on Windows despite the\ntwo platforms’ many glaring differences.\nToday, there are at least three different ways to use Python on Windows: in\nWindows itself, in Windows Subsystem for Linux (a.k.a. WSL and WSL2), and in\nthe third-party Cygwin Unix-like environment. We won’t cover Cygwin here\nbecause it’s not as widely used, and those who may care to use it probably\nalready know how to use it.\nWSL—including its newer WSL2 variant—brings Linux to your Windows PC\nwithout the hassles of dual-boot installs or separate devices. You get a standard\nLinux distribution (e.g., Ubuntu) that runs in the Windows UI and avoids some\nof the trade-offs of classic virtual machines. WSL comes with a command line to\nedit and run Python code, and WSL2 even runs Linux GUI apps (though they’re\nstill marginal at this writing). Because WSL is really Linux, we’ll defer to the\nLinux section ahead for Python setup and usage info, as well as Microsoft’s\nonline documentation for details on installing WSL itself.\nWhile WSL may pique some readers’ interest, it requires extra setup steps and is\nnot wholly without seams today. For most readers, the easiest way to use Python\non Windows is to go native—with a Python built to run in Windows directly.\nPython doesn’t come with Windows, but it is easy to install and use there. In\nshort, Python for Windows can be installed by downloading a self-installer or\nvisiting the Microsoft Store, and can be run with a simple Windows command-\nline interface; a graphical IDE—a GUI for editing and launching code; and\nclicks on program-file entries in Windows File Explorer.\nIn more detail, the recommended way to install Python for Windows begins with\na visit to the Downloads page at python.org. There, you’ll fetch a Windows self-\ninstaller that you’ll run to install Python 3.X, along with its IDLE GUI and\nstandard library (including the library’s tkinter GUI toolkit). Figure A-1\ncaptures the Python installer in action; allow it to run if Windows asks for\npermission. You can generally accept all installation defaults, but it’s\nrecommended to opt in to both adding Python to your PATH at the start of the\ninstall and lifting the Windows path-length limits for filenames at the end.",
      "content_length": 2413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1706,
      "chapter": null,
      "content": "NOTE\nInstallation convolution: Though less common, you can also install Python from the Microsoft\nStore. In fact, typing python3 in a Windows command line at this writing automatically routes\nyou to the store to run the install—confusingly! If you opt to accept this offer, python3 will\nrun the store’s version after the install, and the Start menu will sprout separate entries for\nlaunching this Python and its IDLE (up shortly).\nThis guide uses and generally recommends the python.org install (and its py helper) because\nit’s more traditional and may be better suited to general use. That said, the store version\nultimately comes from the same source, and either may be used for this book’s examples on\nWindows.\nBut beware: the store version imposes access restrictions that may matter to you later and\nshould probably relegate it to a secondary option for most readers. You really shouldn’t install\nboth the store and nonstore Windows Pythons, though, unless you need more drama in your\nlife!\nFigure A-1. The python.org installer on Windows\nOnce you’ve installed Python on Windows, running it there can be as simple as\ntyping code at a command line and clicking file icons or as complex as learning\nthe nuances of a full-featured IDE. Of these, command lines generally add the",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1707,
      "chapter": null,
      "content": "least number of moving parts.\nTo run Python from a command line on Windows, open either Command Prompt\nor PowerShell (the non-ISE flavor, normally). Once open, simply type py or\nother options presented in a moment and add a filename to run a file of code\n(i.e., a script).\nFor example, open Command Prompt from your Start menu (search for it there if\nneeded). Then, type py and press Enter to start a Python interactive session\nwhere you can type and run code at the >>> prompt. To launch a script instead,\ntype py script.py, with the name of your script. Figure A-2 demos these\ncommands live on Windows. For space, some demos in this appendix, including\nthis one, use Python’s -q flag to suppress messages on session startup; this is\ncosmetic and optional.",
      "content_length": 757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1708,
      "chapter": null,
      "content": "Figure A-2. Running Python by command lines on Windows\nThe py command is technically part of the Python Windows launcher that’s\ninstalled along with Python itself. By default, the launcher runs the most recent\nPython version installed on your PC, but you can also specify a version to run if\nthere’s more than one (e.g., py -3 runs the latest 3.X, and py -3.8 runs an older\nversion of it). If you have just one Python or want to use the latest, py suffices to\nlaunch an interactive session or script.\nYou can also start Python with command python if you opted to add Python to\nyour system PATH during the install, though it’s pointless extra typing (python3\nworks too, but only if you installed from the Store per the earlier note). And just\nas on Unix, you can easily save a script’s printed output to a file by adding >\nfilename.txt to the end of a command (see Chapter 3 for more on such stream\nredirections).\nHowever, if you opt to go the command-line route, you’ll also need to choose a\ntext editor to create files of code you wish to save (i.e., scripts to run and\nmodules to import). As demoed in Figure A-2, Windows Notepad suffices, but\nany Windows text editor will fit the bill. To use Notepad, launch it from your\nStart menu (search there if needed) or by typing its name in a command line,\nwith or without a filename to edit.\nBesides command lines, you can also start Python’s interactive session by\nclicking the Python 3.12 (64-bit) (or similar) item in its Start-menu entry, shown\nin Figure A-3. This starts the usual Python REPL (Read-Eval-Print Loop)\ninteractive session with its >>> prompt, just like an explicit py command in\nCommand Prompt or PowerShell. You’ll still use py commands or other\ntechniques, though, to run code files. In all console REPLs on Windows, type or\ntap the two-key combo Ctrl+Z (followed by Enter) at the >>> prompt to exit a\nPython interactive session or simply close the hosting window.",
      "content_length": 1931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1709,
      "chapter": null,
      "content": "Figure A-3. Python Start-menu options on Windows\nIf command lines make you break out in hives, you can also run Python from\ngraphical IDEs like PyCharm or Python’s own IDLE. Of these, IDLE is included\nwith Python for Windows and provides a simple but sufficient IDE for running\nthis book’s examples. Its utility partly overlaps with command lines (it’s\nultimately just a place to type and run code), but it also includes a Python-\nfriendly text editor for code files and simplifies some common coding chores.\nNotably, it’s able to launch program files without command lines.\nAs examples, Figures A-4 and A-5 capture IDLE’s interactive “Shell” and editor\nwindows, respectively, with default configurations. You can start IDLE from\nPython’s entry in your Start menu on Windows (try a search for “idle” there to\nlocate and open IDLE quickly). The command py -m idlelib.idle also starts\nIDLE, for reasons covered elsewhere in this book (tl;dr: this is like a module\nimport, but runs instead of importing), and right-clicks on code files in File\nExplorer can open IDLE too, but require registry edits today.\nIDLE’s own Help menu comes with ample usage info that we’ll defer to here,\nbut one tip is worth a callout: a menu Run→Run Module (or its equivalent F5\nshortcut key) in any editor window like that in Figure A-5 lets you launch a\nscript without typing a command line. This runs the code in that window after\nit’s been saved to a file if needed and routes the code’s printed output back to the",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1710,
      "chapter": null,
      "content": "Shell window. Its Run…Customized version also lets you provide command-line\narguments (see sys.argv in Python’s manuals for details).\nUseful tricks to be sure, but even if you don’t use IDLE to run code this way, it\nhas additional tools we’ll skip here for space, and its code editor alone makes for\na compelling alternative to Notepad if you have no other option in mind for\nscripts and modules.\nFigure A-4. The IDLE GUI’s Shell window on Windows",
      "content_length": 447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1711,
      "chapter": null,
      "content": "Figure A-5. An IDLE GUI editor window on Windows\nBeyond py, python, python3, Start, and IDLE (which already qualifies as a\nWindows embarrassment of riches!), a Python program file can also be launched\non Windows by typing just its name in a command line (e.g., hack.py), and by\nlocating and clicking its name or icon in Windows File Explorer. These both\nwork thanks to the magic of filename associations in Windows, which Python\nsets up automatically during its install: any file whose name ends in .py is\nrouted to the py launcher when named or clicked.\nClicking, however, comes with a drawback: printed output of programs you\nlaunch this way is lost on program exit, because the program’s run window is\nclosed. To keep the window (and hence output) open, simply add a call to\nPython’s input() at the bottom of your script to pause for a user Enter-key\npress. As in Figure A-5, this call can be conditional on the platform to pause\nselectively (some code may warrant standard-stream TTY tests too).\nThe input() trick won’t help if the program commits an error (alas, the error\nmessages may perish with the window before the pause is ever reached!), so\nrunning by clicks is usually best used only for graphical programs and others\nthat log their errors to files. Tip for GUIs: a .py opens a console for standard\nstream IO when clicked, but a .pyw does not.\nBefore we move on, here are some advanced but useful tips for Python on",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1712,
      "chapter": null,
      "content": "Windows:\nScript #! lines\nThe py Windows launcher treats the first line in a script’s file as special if it\nbegins with #!. This line can name the version and location of the Python to\nbe used to run the file’s code (e.g., #!python3.12), and all the usual Unix-\nstyle lines work (e.g., #!/usr/bin/python3.12). This line is entirely\noptional and no different than naming a Python in the command line used to\nlaunch it with py (e.g., py -3.12 hack.py) but may be useful when there\nare multiple Pythons installed on your PC, especially when scripts are run\nwith clicks instead of command lines.\nEnvironment variables\nWindows command-line interfaces use the PATH (a.k.a. Path) environment\nvariable to locate named programs like python: every folder on this list is\nsearched. This is normally set up for Python automatically during its install if\nyou opt in, but you can tweak it later yourself in Settings (search for\n“environment variable” there). In the same way, you may also create or mod\nPYTHONPATH, used to locate imported modules (per Chapter 22), as well as\nPYTHONUTF8 and PYTHONIOENCODING, used on Windows to specify the\ndefault Unicode encoding of files and redirected streams (this convoluted\nstory has changed in 3.X often and will again; see Chapter 37).\nOther Windows options\nMost Python code works the same on Windows as on other platforms,\nespecially if it uses Python’s portable system tools in modules like os. In\ncases where Windows-specific tools are required, though, the pywin32 third-\nparty extension allows your Python programs to access many Windows APIs",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1713,
      "chapter": null,
      "content": "directly.\nFor more about using Python on Windows, try python.org’s HOWTO as well as\nthe copious resources on the web (with the usual caution about vetting their\ncopious sources). Here, let’s move on to the next PC platform on our tour.\nUsing Python on macOS\nAs a Unix-based platform, macOS is well suited to Python, open source, and\nsoftware development in general. At this writing, newer macOS PCs no longer\ncome with a Python preinstalled (and if it seems they do, it’s just a stub for\ninstalling unrelated toolsets, per the note ahead). Older macOS systems do have\na Python, but it’s the now-dated 2.X, and may issue a deprecation warning when\nlaunched (in other words, you can’t use it to run this book’s code, and it’s not\nlong for the macOS world). Hence, an install is required.\nAs for Windows, the recommended way to install Python 3.X for macOS begins\nwith a visit to the Downloads page at python.org. There, you’ll fetch a self-\ninstaller for macOS that you’ll run to install Python, along with its IDLE GUI\nand standard library (including the library’s tkinter GUI toolkit). The Python\nyou’ll get today is a Universal 2 binary that runs natively on macOS PCs using\nboth newer Apple Silicon (ARM) and older Intel (x86) chips, and can be run in\nthe Rosetta 2 emulator (see the web for more on all such terms). Figure A-6\ncaptures the install process on macOS.\nNOTE\nInstallation convolution: If you type python3 in macOS’s Terminal before running the\npython.org install, you may get an Apple popup that asks if you want to install Python as part\nof Xcode’s “command line developer tools.” This is similar in spirit to the Microsoft Store\nredirect on Windows of the preceding section—and similarly confusing!\nOn macOS, though, most users should ignore the offer and instead install Python from\npython.org as described here because it makes your Python independent of the version and\nconfiguration choices made by a tools package. This is also true if you already have Xcode and\nits Python: install a new Python from python.org for generally better control.",
      "content_length": 2063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1714,
      "chapter": null,
      "content": "Figure A-6. The python.org installer on macOS\nAfter the install, you can start Python both with its entries in Launchpad, which\nmostly mirrors your PC’s Applications folder in Finder, as well as command\nlines in Terminal, which provides a standard Unix command-line shell. Of these,\nTerminal may be the most basic way to get started with this book’s examples on\nmacOS.\nTo run code by command line on macOS, open Terminal by clicking its entry in\neither Launchpad’s Other folder, or Applications→Utilities in Finder. Then, type\npython3 to start a Python interactive session where you can type and run code,\nor python3 script.py to launch a code file. This works because python3 is\nadded to your system PATH by the install (and as a caution, python may mean\nPython 2.X on older PCs).\nFigure A-7 demos Python commands live on macOS. As usual on Unix-based\nsystems, type keys combo Control+D at the >>> prompt to exit a Python",
      "content_length": 922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1715,
      "chapter": null,
      "content": "interactive session here (yes, this differs from Windows), and alias python3 to\nsomething shorter in your shell’s startup files if seven characters is too much\n(e.g., alias to py, if you want to avoid some disorientation when hopping to and\nfrom Windows).\nFigure A-7. Running Python in macOS Terminal\nWhen using command lines to run code, you’ll also use a text editor to create\nfiles of Python code (scripts and modules) on macOS. Its built-in TextEdit\nsuffices but isn’t very code-friendly out of the box (e.g., you’ll want to set a\nmonospace font right away). Any macOS text editor is up to the task of editing\nPython code, including IDLE (up next), the vi and nano command-line-based\neditors familiar to Unix users, and other options you can explore on the web.\nAfter the install on macOS, you’ll also find two tools for running Python code in\nother modes, available in both Launchpad and your Applications folder in\nFinder. As captured in both Figures A-8 and A-9, the Python Launcher allows\nyou to run a file of Python code with either a click in Finder or a drag to its icon",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1716,
      "chapter": null,
      "content": "or name, and IDLE provides a basic edit-and-run IDE GUI for Python code.\n(Python itself, invoked by a python3 command, shows up in\n/Library/Frameworks, though you don’t normally need to care.)\nFigure A-8. Python’s IDLE and launcher in macOS Launchpad\nFigure A-9. Python’s Applications folder in macOS Finder\nFigure A-10 shows IDLE on macOS. Launch it most easily with its Launchpad",
      "content_length": 381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1717,
      "chapter": null,
      "content": "or Finder entries or by right-clicking (a.k.a. control-clicking) any Python code\nfile in Finder and choosing IDLE in Open With (you can make this association\npermanent there if desired, and global in Finder’s Get Info).\nBecause IDLE is coded in Python as a tkinter GUI, it looks and works the\nsame on all supported platforms (essentially, all PCs sans mobile heroics). As\nelsewhere, it allows you to edit and run files of Python code without having to\nuse command lines or other editors. See IDLE’s earlier Windows coverage for\nmore tips; as noted there, IDLE can serve as a Python-friendly code editor, even\nif you run files by command lines, clicks, or drags.\nFigure A-10. The IDLE GUI’s Shell window on macOS\nTo wrap up, here are a handful of advanced usage tips, with macOS spins that\nalso apply to other Unix platforms like Linux and Android coming up next:\nScript #! lines\nIf the first line of a script begins with #!, it’s treated as special on macOS,",
      "content_length": 958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1718,
      "chapter": null,
      "content": "just as it is on Windows. On macOS, this is a function of the shell (e.g., Bash\nor Zsh) that’s running your command lines, not Python. For instance, a top-\nof-script line #!/usr/bin/python3.12 tells the shell which Python should\nrun the rest of the file’s lines. This isn’t different than naming your Python in\na command line explicitly (e.g., /usr/bin/python3.12 hack.py), but a #!\nline allows a script to be run by just its name (e.g., hack.py) if it’s also made\nexecutable (see chmod).\nRunning by clicks\nPython code files run with a click in Finder, but you may have to choose the\nPython Launcher in Open With and make it permanent with Always (or Get\nInfo). Unlike on Windows, output is retained in the resulting console window\non exit and errors. A #! first line isn’t required but can be used.\nEnvironment variables\nOn macOS, you can also set or change environment variables like PATH—\nused to locate programs like python3 named in command lines, and\nPYTHONPATH—used to locate imported modules—with code in your shell’s\nstartup files (e.g. ~/.bash_profile or ~/.zprofile). PATH is set automatically by\npython.org installs. For pointers on the shell code used to set these variables,\nsee the example in Chapter 22, as well as the web and your PC’s docs (e.g.,\ninfo bash and the unfortunately coded man bash).\nOther macOS options\nFor completeness, it’s worth noting that there are additional platform-specific\ntools for Python on macOS (e.g., PyObjC), and the third-party Homebrew\npackage manager provides an entirely different install scheme for Python on\nmacOS, which works equally well but has extra setup steps that make it",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1719,
      "chapter": null,
      "content": "better suited to advanced readers.\nFor space, we’ll skip further details here; see the HOWTO at python.org and\nyour local web search engine for more Python options on macOS.\nUsing Python on Linux\nPython is a staple on Linux, where it’s used both for user applications and\nsystem tools. Because it’s so ubiquitous on this Unix-based platform, Python\nmay come preinstalled with your Linux distribution; type python3 in a shell\nwindow (e.g., Terminal) to check. If you need to install manually, do the usual\nthing for your Linux flavor: a sudo apt install python3 in Terminal does the\ndeed in Ubuntu distributions (try yum on some others).\nOnce you’ve got a Python 3.X installed, run code interactively and launch code\nfiles with the usual Terminal command lines, like those captured in Figure A-11.\nA python3 (or a version-specific name) starts an interactive session for typing\nand running code, and adding a filename (e.g., python3 script.py) runs a file.\nAs on other platforms, your PATH setting is used by such commands to locate\nPython. As on all Unixes, the key combo Ctrl+D at >>> exits a Python REPL,\nand shorter shell aliases for python3 can avoid some typing.",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1720,
      "chapter": null,
      "content": "Figure A-11. Running Python in Linux Terminal\nWhen using command lines, you’ll also need to use a text editor for scripts and\nmodules. To make such a file of Python code, any Linux text editor will do,\nincluding the graphical Gedit default on Ubuntu and the shell-oriented vi and\nnano.\nYou can also use Python’s IDLE edit-and-run GUI on Linux. Launch it with an\nidle command line after installing it with sudo apt install idle3, or similar\non other distributions. A code-file right-click and Open With in file explorers\nmay start IDLE too.\nIDLE fine print: you may need to force versions with a more specific name (e.g.,\nidle-python3.12); emojis might not work until a font install (e.g., sudo apt\ninstall ttf-ancient-fonts-symbola); and a platform-agnostic command\nline python3 -m idlelib.idle starts IDLE, too, per the Windows flavor noted\nearlier.\nFigure A-12 demos IDLE running on an Ubuntu Linux PC after all the kinks\nhave been ironed out; it works the same on Linux as on Windows and macOS,\nand is covered in more detail in this appendix’s Windows section.",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1721,
      "chapter": null,
      "content": "Figure A-12. The IDLE GUI’s Shell window on Linux\nAlso noteworthy on Linux:\nIf you wish to use Python’s portable tkinter GUI toolkit, you can\ninstall it separately if needed with sudo apt install python3-tk (or\nsimilar, in the richly bifurcated world of Linux package installs).\nAs on macOS, a line starting with #! at the top of your script can denote\nwhich Python runs the file, and environment variables like PATH and\nPYTHONPATH can be set in shell startup files, but the former is not\nusually required. See the macOS section’s coverage of both topics and\nChapter 22’s PYTHONPATH example.\nPython files can be run by clicks on Linux too, but details vary. In\nUbuntu’s Files, “Run as a Program” runs a clicked Python file that has\nboth executable permission (e.g., chmod +x script.py), and a #!/…\nfirst line that gives the path to Python, but the Windows caution about\noutput disappearing on exit or error applies.",
      "content_length": 915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1722,
      "chapter": null,
      "content": "It’s not uncommon on Linux to build Python from its source code\ndistribution, available at either python.org or GitHub. This entails a few\nsimple command lines (configure and make) but is beyond the scope\nof both this chapter and most Python beginners; see the Downloads\npage at python.org for code and details.\nFor more info about Python on Linux, try the web at large or python.org’s\nHOWTO. Here, it’s time to move ahead to Python’s story on mobile.\nUsing Python on Android\nAndroid is a secure derivative of Linux adapted for the unique constraints of\nmobile devices, and it is the most widely used operating system in the world at\nthis writing. Despite this platform’s Java and Kotlin programming-language\nbiases, Python can be used as a first-class programming citizen on Android\ndevices in both learning and development roles. This book’s examples, for\ninstance, will work well on your Android phone or tablet.\nTo run Python locally on your Android, you’ll first install an app that supports it\nfrom an app store like Play or F-Droid. Among these apps, Termux, Pydroid 3,\nand QPython all allow you to run Python code on Android directly in multiple\nmodes. While we can’t do justice to these and other Python apps, here’s a quick\nrundown of two to get you started.\nThe free and open source Termux app for Android provides a full-featured Linux\nshell, toolset, and package manager. To use Python in Termux, first install the\nTermux app from the F-Droid store (its Play version is defunct). Then, open the\napp from your Apps screen, and install Python 3.X inside it with a pkg install\npython command line in its shell.\nTermux opens with a standard Bash command-line shell, where you can tap out\ncommands to launch an interactive Python session with python, and run a file of\ncode by adding a filename (e.g., python script.py). Stream redirection works\nas on all Unix, and command python3 is the same as python if you prefer Unix\nuniformity and don’t mind the extra tap. Both commands are automatically\nusable post install without PATH mods.",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1723,
      "chapter": null,
      "content": "You can use code files (scripts and modules) located in any folder Termux has\naccess to (which generally means shared or app-private storage, per ahead) and\nmake and change them either with separate text editor apps or within Termux\nitself using Linux text editors like vi and nano (install them in Termux with pkg\ninstall as needed). Termux also supports Android’s Storage Access\nFramework to make its app-private storage visible to some file-explorer apps,\nalthough shared storage is more accessible and usable.\nFigure A-13 demos the Termux app running Python code and file on Android.\nAs on all Unixes, the keys combo Ctrl+D at >>> ends a Python interactive\nsession in Termux (via Termux’s CTRL button or keyboards ahead), as does\nkilling the app, and shell aliases can shorten the python (or python3) command.\nWith apps, you’ll generally use the version of Python provided; in Termux, this\nmeans one of the versions in package repos, but you may be able to build a\nnewer one from source code. You can also install a host of extensions to use in\nyour code by command line, with both Termux’s pkg and Python’s pip.",
      "content_length": 1116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1724,
      "chapter": null,
      "content": "Figure A-13. Running Python in the Termux app on Android\nTermux may be the path of least resistance for getting started with Python on\nAndroid. It has more features we’ll largely skip here, including home-screen\nwidgets that run Python scripts on taps, and all Linux concepts covered earlier\napply, including PYTHONPATH and PATH environment-variable settings. Its chief",
      "content_length": 369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1725,
      "chapter": null,
      "content": "downside for some users may be that it is limited to command lines sans its\noptional X Window System support, which is considerably complex to use;\nIDLE, for example, would be difficult at best to run in Termux.\nIf you’re looking for something a bit more graphical, the Pydroid 3 app also\nprovides a command-line shell and interactive Python session, but it adds a GUI\nIDE for editing and launching Python code. The IDE’s edit/run window is\ncaptured in Figure A-14.\nPydroid 3 is today installed from Play. Its shell and interactive session are less\nuser-friendly than the richer command-line support in Termux, but its IDE may\nseem more comfortable for users unaccustomed to command lines. Similar in\nspirit to IDLE on PCs, this app’s IDE allows you to edit Python code, and launch\nit with a simple (and yellow) button press.",
      "content_length": 825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1726,
      "chapter": null,
      "content": "Figure A-14. The Pydroid 3 app’s IDE on Android\nOn top of its IDE, Pydroid 3 adds support for many popular tools, including\nscientific-programming libraries and, remarkably, Python’s tkinter GUI\ntoolkit. As in Termux, Python’s version is preset in Pydroid 3 (though source\nbuilds are elusive), and Python’s pip is available to install extensions (though",
      "content_length": 353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1727,
      "chapter": null,
      "content": "with a dedicated GUI in this app).\nFair warning: as a substantial trade-off, Pydroid 3 is also a freemium app, which\nwill flash rude full-page ads at you unless and until you pay a required fee—an\nunfortunately common paradigm in Android, which you’ll have to weigh for\nyourself. In addition, Pydroid 3 has a history of waffling on support for storage\naccess in response to Android and Play edicts. By contrast, Termux today is\nentirely free and void of ads, supports broad storage access, and chooses\nalternative app stores rather than limiting functionality for Android changes\nmandated by Play.\nSee the web and app stores for info about other Python programming apps on\nAndroid omitted here for space. While you’re at a store, you may also want to\nexplore text editor apps like QuickEdit—which is able to colorize and run\nPython code; and alternative onscreen keyboards like Hacker’s Keyboard—\nwhich adds PC keys not available in stock options but commonly used for\ncoding (e.g., arrows and Ctrl). Some Python apps also include tools to augment\nonscreen keyboards that can be tailored or disabled, and Bluetooth keyboards\nand casting to larger screens can naturally aid usability too.\nIt’s also worth noting in closing that CPython plans to add Android to its list of\nofficially supported platforms soon, which may foster additional options going\nforward. Moreover, although most beginners will use an app to run Python code\non Android as described, it’s also possible to build standalone apps for Android\nthat are coded in Python but used like any other app. We’ll return to this option\nat the end of this appendix after one last platform.\nANDROID’S PROPRIETARY WORLD\nPython programmers should also be aware that Android imposes numerous\nconstraints on apps, some of which may seem onerous to developers with\nbackgrounds in more interoperable platforms. Most of these constraints are\nrationalized on the grounds of security or performance, but all reduce utility.\nAndroid’s storage, for instance, is split into a shared and persistent area with\ncontrolled access, along with areas partly or wholly private to apps that may\nvaporize on app uninstalls. Hence, while POSIX file tools and paths do work\non Android, Python code must take care to either use accessible folders or",
      "content_length": 2277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1728,
      "chapter": null,
      "content": "run proprietary Java API tools that request or use enhanced permissions.\nIn addition, background or long-running processes may run afoul of limits;\nopinionated choices of tools and languages are nearly imposed on\ndevelopers; and throttling for power, heat, memory, or other bias is a norm\non most phones.\nOn the upside, Android users can install apps outside its owner’s store and\ncan access the filesystem with numerous file-explorer apps. That makes\nAndroid more open than iOS today, but this is a large and fluid topic. If you\ncare about using Python on mobiles, be sure to watch other resources for\nnews on this front.\nUsing Python on iOS\niOS—which includes its iPadOS offshoot in this guide—is a macOS derivative\ntargeted at mobile devices. Like Android, it has limiting biases for programming\nlanguages (Swift and Objective-C), and it is even more strict about carving up\nstorage into restricted app sandboxes with proprietary access rules and tools.\nDespite these constraints, though, your iPhone or iPad can be used to run Python\ncode, too, including the code in this book.\nLike Android, you’ll normally use Python on your iOS devices by installing an\napp that runs Python code. Among these, Pythonista 3 provides an interactive\nPython session and a GUI for editing and running files of code, as in other IDEs.\nIn addition, this app comes with access to native iOS features and a toolkit for\nbuilding GUIs in Python for iOS. It also can share code files with the Files app\nto be opened with taps.\nTo vet for yourself, fetch Pythonista 3 from the App Store. Figure A-15 shows\nthis app in action (yes, on a humble and historical iPod). Be sure to also explore\nthe other iOS options on the store; the Pyto app, for example, provides similar\nfunctionality, and comes with the Toga UI library for coding portable GUIs\n(there’s more on Toga at standalone apps ahead).",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1729,
      "chapter": null,
      "content": "iPod >\n\n) Console » 2)\n\n>>> sys\n\n>>> sys.platform @\n‘ios'\n\n>>> sys.version_info[:3]\n\n(3, 10, 4)\n\n>>>\n\n>>> os\n\n>>> os.getcwd()\n'/private/var/mobile/Containers/\nShared/AppGroup/FF401A8E-B675—4660-\nBE8A-661D7A5400CA/Pythonista3/\nDocuments’\n\n>>>\n\n>>> i range(5):\n\n(‘ds' * (i + 1))\n\nbs ots\ntds ods obs",
      "content_length": 296,
      "extraction_method": "OCR"
    },
    {
      "page_number": 1730,
      "chapter": null,
      "content": "Figure A-15. Running Python in the Pythonista 3 app on iOS\nApart from app choices and platform restrictions, using Python on an iOS device\nis largely the same as on Android and PCs, so we’ll skip further details here. For\nmore on using Python for iPhone and iPad, see the Apple App Store and the web\nat large.\nLike Android, iOS is also scheduled to be granted officially supported status in\nCPython soon, which may yield options impossible to predict today; watch the\nweb for new developments. Also like Android, it’s possible to package your\nPython programs as standalone apps for iOS, but we must move on to this\nappendix’s next section to see how.\nStandalone Apps and Executables\nBesides running Python source code with the traditional schemes we’ve just met,\nit’s also possible to bundle Python code into a standalone program that users run\nthe same way they run any other program on their device (e.g., by a click or tap).\nIn fact, users can’t even tell these bundles are written in Python at all: no source\ncode is visible, no other installs or apps are required, and changes in a locally\ninstalled Python have no effect on the bundle.\nThe way you’ll build standalones varies per platform. As a noncomprehensive\nsample of prominent tools today, you can build standalone executables for\nWindows and Linux with PyInstaller; standalone apps for macOS with\nPyInstaller and py2app; and standalone apps for Android and iOS with\nBuildozer and Briefcase (the latter also offers options for PCs).\nOn Android, for instance, it’s possible to develop standalone apps completely in\nPython. Although this takes more effort than running code in another app, its\nproducts are fully functional and idiomatic GUI apps coded in Python, which run\nwith a tap, leverage Android APIs when needed, and can be both side-loaded\nand uploaded to app stores.\nAs a demo, Figure A-16 captures one of many Python-coded standalone apps for\nAndroid, running on a foldable. This app, made by this book’s author, is freely\navailable in the Play store; is built for Android with Buildozer; uses the portable\nKivy toolkit for its GUI; and relies on Kivy’s pyjnius to access Android Java",
      "content_length": 2154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1731,
      "chapter": null,
      "content": "APIs when required in a small fraction of its code (e.g., to request permissions,\nrun services, open docs, and get drive labels).\nFigure A-16. A Python-coded standalone app running on Android\nCrucially, such apps can also run on PC platforms—Windows, macOS, and\nLinux—from the same code base. Figure A-17, for instance, shows the same app",
      "content_length": 338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1732,
      "chapter": null,
      "content": "running on macOS. Its code is bundled for macOS and other PCs with\nPyInstaller; its Kivy GUI is automatically cross-platform; and its POSIX file-\nsync code works everywhere. The net result is a Python-coded app that runs\nacross a range of PC and mobile hosts, with native behavior on each.\nTo see this for yourself, fetch this app’s Android version on Play and its PC\nversions at this book’s website or quixotely.com. Disclaimer: if the Android app\nis unavailable on Play, check for it at the latter two sites or try a web search;\nbook lifespans tend to be substantially longer than those of apps dependent on\nstores and platforms (see Termux’s troubles!).\nFigure A-17. The same app running on macOS\nNor is this toolset the only interoperability game in town. The alternative\nBeeWare, with its portable Toga GUI toolkit and Briefcase app builder, promises\nsimilar platform independence and advertises additional packaging options on\nPCs. Moreover, some apps built with such tools can work on iOS, too, though its\nlack of a user-accessible filesystem renders much cross-platform code unusable",
      "content_length": 1091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1733,
      "chapter": null,
      "content": "(e.g., POSIX file-path syncs are impossible).\nThe takeaway here: with a portable programming tool like Python, you’re not\nlocked into a single platform’s proprietary realm—unless, that is, you develop\nfor platforms that disqualify code that runs anywhere else. As always, choose\nyour coding battles wisely. Security counts, but closed platforms enable\nmonopolies and stifle innovation.\nStandalones may not be very useful when you’re just getting started with Python\n(and make no sense at all for running the examples in this book!), but they may\nbecome more important when you start writing programs for others to use. When\nyou’re ready to explore standalone deliverables in Python, see the web for\ncurrent tools and details in this domain.\nEtcetera\nWhile the platform techniques we’ve explored here are perhaps the simplest and\nmost common ways to use Python, there’s much more to this story. For instance,\nthis appendix hasn’t said anything about using Python in:\nOther IDEs like PyCharm, PyDev, Wing, and VSCode\nWeb-based notebooks like IPython and Jupyter\nAlternative Python implementations like PyPy, Cython, Numba, and\nJython\nAlternative Python distributions like Anaconda and ActiveState\nThe cells and macros of spreadsheets like Excel\nWeb servers using frameworks like Flask and Django\nWeb browsers using the emerging WebAssembly and Pyodide\nAnd lots of other options in no way judged by omission here. This book visits\nsome of these in Chapter 1, summarizes Python implementations in Chapter 2,\nbriefly reviews Jupyter and WebAssembly in Chapter 3, and uses PyPy for\nbenchmarks in Chapter 21. In general, though, advanced usage contexts like\nthese are interesting but out of scope for this Python fundamentals text, and best",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1734,
      "chapter": null,
      "content": "deferred until you’ve mastered the language itself.\nIn the end, Python usage details and options tend to evolve as rapidly as Python\nitself. Indeed, each prior edition of this book has had to revise its usage coverage\nradically, and this one expects to fare no better. As noted at the start of this\nappendix, you should expect to check both Python’s docs and the web at large\nfor new-and-exciting developments almost certain to emerge by the time you\nread these words.",
      "content_length": 468,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1735,
      "chapter": null,
      "content": "Appendix B. Solutions to End-of-\nPart Exercises\nThis appendix provides solutions for the book’s end-of-part exercises. Code files\nnamed by captions or narrative in these solutions are available in the book\nexamples package’s AppendixB folder, which has one subfolder per part (e.g.,\nAppendixB/Part1 is the first part’s files). See the Preface for more info on the\nexamples package.",
      "content_length": 381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1736,
      "chapter": null,
      "content": "Part I, Getting Started\nSee “Test Your Knowledge: Part I Exercises” in Chapter 3 for the exercises.\n1. Interaction: Assuming Python is configured properly, the interaction\nshould look something like the following. You can run this any way you\nlike—in IDLE, a console, an app, a notebook’s page, and so on:\n$ python3\n…information lines…\n>>> 'Hello World!'\n'Hello World!'\n>>>                 # Use ctrl+D/ctrl+Z to exit on Unix/Windows, or close \nwindow\n2. Programs: Your code (i.e., module) file should look something like\nExample B-1:\nExample B-1. Part1/module1.py\nprint('Hello module world!')\nAnd here is the sort of interaction you should have; for console\nlaunches, be sure to use your platform’s version of the “python3”\ncommand (e.g., try “py -3” on Windows):\n$ python3 module1.py\nHello module world!\nAgain, feel free to run this other ways—by clicking or tapping the file’s\nicon, by using IDLE’s Run→Run Module menu option, by UI options in\nweb notebooks or other IDEs, and so on.\n3. Modules: The following interaction listing illustrates running a module\nfile by importing it:\n$ python3",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1737,
      "chapter": null,
      "content": ">>> import module1\nHello module world!\n>>>\nRemember that you will need to reload the module to run it again\nwithout stopping and restarting the interactive interpreter (i.e., REPL).\nMoving the .py file to a different directory and importing it normally\nfails: Python likely generated a module1.*.pyc file in the __pycache__\nsubdirectory of the source code file’s folder, but it won’t use it when\nyou import the module there if the source code (.py) file has been\nmoved elsewhere and to a folder not in Python’s import search path.\nThe .pyc file is written automatically if Python has access to the source\nfile’s directory; it contains the compiled bytecode version of a module.\nSee Chapter 3 for more on modules, Chapter 2 for more on bytecode,\nand Chapter 22 ahead for more on both. To really use the saved .pyc\nsans .py, as of Python 3.2, you must move it up one level and rename it\nwithout the “*” part in the middle, or generate it from and alongside the\nsource code file with the Python compileall module’s “legacy” (-b)\nmode. For example, the following compiles all source code files in the\ncurrent directory into directly usable bytecode files (you can also list\nspecific files or recurse into subfolders, per Python library docs):\n$ python3 -m compileall -b -l .\n4. Scripts: Assuming your platform supports the #! trick, your solution\nwill look like Example B-2, although your #! line may need to list a\ndifferent path to Python on your machine. This line is significant under\nthe Windows launcher shipped and installed with Python, where it is\nparsed to select a version of Python to run the script, despite the Unix\npath syntax, and subject to a default setting; see Appendix A and\nPython’s docs for more details. This launching scheme is optional and\ngenerally less portable than others.\nExample B-2. Part1/script1.py\n#!/usr/local/bin/python3",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1738,
      "chapter": null,
      "content": "print('Hello module world!')\nRunning this as a program by console command line:\n$ chmod +x script1.py             # See also: #!/usr/bin/env python3\n$ ./script1.py                    # \"./\" needed only if \".\" not on PATH\nHello module world!\n$ python3 script1.py              # Or run normally and portably \nHello module world!\n5. Errors and debugging: The following interaction demonstrates the sorts\nof error messages you’ll get when you complete this exercise. Really,\nyou’re triggering Python exceptions; the default exception-handling\nbehavior terminates the running Python program and prints an error\nmessage and stack trace on the screen. The stack trace shows where you\nwere in a program when the exception occurred (if function calls are\nactive when the error happens, the “Traceback” section displays all\nactive call levels).\nIn Chapter 10 and Part VII, you will learn that you can catch exceptions\nusing try statements and process them arbitrarily. You’ll also learn that\nPython includes a full-blown source code debugger (module pdb) for\nspecial error-detection requirements. For now, notice that Python gives\nmeaningful messages when programming errors occur, instead of\ncrashing silently:\n$ python3\n>>> 2 ** 500\n32733906078961418700131896968275991522166420460430647894832913680961337964\n046745\n54883270092325904157150886684127560071009217256545885393053328527589376\n>>> 1 / 0\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1739,
      "chapter": null,
      "content": "ZeroDivisionError: division by zero\n>>> oops\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'oops' is not defined\n6. Breaks and cycles: When you type this code:\n$ python3\n>>> L = [1, 2]\n>>> L.append(L)\n>>> L\n[1, 2, [...]]\nyou create a cyclic data structure in Python. In Python releases before\n1.5.1, the Python printer wasn’t smart enough to detect cycles in\nobjects, and it would print an unending stream of [1, 2, [1, 2, [1,\n2, [1, 2, and so on until you hit the Ctrl+C break-key combination on\nyour machine (which, technically, raises a keyboard-interrupt exception\nthat prints a default message). Beginning with Python 1.5.1, the printer\nis clever enough to detect cycles, prints [[...]] instead to let you\nknow that it has detected a loop in the object’s structure, and avoids\ngetting stuck printing forever.\nThe reason for the cycle is subtle and requires information you will\nglean in Part II, so this is something of a preview. But in short,\nassignments in Python always generate references to objects, not copies\nof them. You can think of objects as chunks of memory and of\nreferences as implicitly followed pointers. When you run the first\nassignment in the preceding code, the name L becomes a named\nreference to a two-item list object—a pointer to a piece of memory.\nPython lists are really arrays of object references, with an append\nmethod that changes the array in place by tacking on another object\nreference at the end. Here, the append call adds a reference to the front\nof L at the end of L, which leads to the cycle illustrated in Figure B-1: a",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1740,
      "chapter": null,
      "content": "pointer at the end of the list that points back to the front of the list.\nFigure B-1. A cyclic object, created by appending a list to itself\nBesides being printed specially, as you’ll learn in Chapter 6, cyclic\nobjects must also be handled specially by Python’s garbage collector, or\ntheir space will remain unreclaimed even when they are no longer in\nuse. Though rare in practice, in some programs that traverse arbitrary\nobjects or structures, you might have to detect such cycles yourself by\nkeeping track of where you’ve been to avoid looping. Believe it or not,\ncyclic data structures can sometimes be useful, despite their special-case\nprinting.",
      "content_length": 651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1741,
      "chapter": null,
      "content": "Part II, Objects and Operations\nSee “Test Your Knowledge: Part II Exercises” in Chapter 9 for the exercises.\n1. The basics: Here are the sorts of results you should get, along with a\nfew comments about their meaning. Again, note that ; is used in a few\nof these to squeeze more than one statement onto a single line (the ; is a\nstatement separator), and commas build up tuples displayed in\nparentheses. See file Part2/basics.txt for copy/paste sans emedia,\nthough typing these manually is a good way to practice syntax:\n$ python3\n# Numbers\n>>> 2 ** 16                           # 2 raised to the power 16\n65536\n>>> 2 / 5, 2 / 5.0                    # Division keep remainders\n(0.4, 0.4) \n# Strings\n>>> 'hack' + 'code'                   # Concatenation\n'hackcode'\n>>> S = 'Python'\n>>> 'grok ' + S\n'grok Python'\n>>> S * 5                             # Repetition\n'PythonPythonPythonPythonPython'\n>>> S[0], S[:0], S[1:]                # An empty slice at the front - \n[0:0]\n('P', '', 'ython')                    # Empty of same type as object \nsliced\n>>> how = 'fun'",
      "content_length": 1063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1742,
      "chapter": null,
      "content": ">>> 'coding %s is %s!' % (S, how)     # Formatting: expression, method, f-\nstring\n'coding Python is fun!'\n>>> 'coding {} is {}!'.format(S, how)\n'coding Python is fun!'\n>>> f'coding {S} is {how}!'\n'coding Python is fun!'\n# Tuples\n>>> ('x',)[0]                         # Indexing a single-item tuple\n'x'\n>>> ('x', 'y')[1]                     # Indexing a two-item tuple\n'y'\n# Lists\n>>> L = [1, 2, 3] + [4, 5, 6]         # List operations\n>>> L, L[:], L[:0], L[-2], L[-2:]\n([1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6], [], 5, [5, 6])\n>>> ([1, 2, 3] + [4, 5, 6])[2:4]\n[3, 4]\n>>> [L[2], L[3]]                      # Fetch from offsets; store in a \nlist\n[3, 4]\n>>> L.reverse(); L                    # Method: reverse list in place\n[6, 5, 4, 3, 2, 1]\n>>> L.sort(); L                       # Method: sort list in place\n[1, 2, 3, 4, 5, 6]\n>>> L.index(4)                        # Method: offset of first 4 (search)\n3\n# Dictionaries\n>>> {'a': 1, 'b': 2}['b']             # Index a dictionary by key\n2",
      "content_length": 984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1743,
      "chapter": null,
      "content": ">>> D = {'x': 1, 'y': 2, 'z': 3}\n>>> D['w'] = 0                        # Create a new entry\n>>> D['x'] + D['w']\n1\n>>> D[(1, 2, 3)] = 4                  # A tuple used as a key (immutable)\n>>> D\n{'x': 1, 'y': 2, 'z': 3, 'w': 0, (1, 2, 3): 4} \n>>> list(D.keys()), list(D.values()), (1, 2, 3) in D         # Methods, \nkey test\n(['x', 'y', 'z', 'w', (1, 2, 3)], [1, 2, 3, 0, 4], True)\n# Empties\n>>> [[]], [\"\", [], (), {}, None]      # Lots of nothings: empty objects\n([[]], ['', [], (), {}, None])\n2. Indexing and slicing: Indexing out of bounds (e.g., L[4]) raises an\nerror; Python always checks to make sure that all offsets are within the\nbounds of a sequence.\nOn the other hand, slicing out of bounds (e.g., L[-1000:100]) works\nbecause Python scales out-of-bounds slices so that they always fit (the\nlimits are set to zero and the sequence length, if required).\nExtracting a sequence in reverse, with the lower bound greater than the\nhigher bound (e.g., L[3:1]), doesn’t really work. You get back an\nempty slice ([]) because Python scales the slice limits to make sure that\nthe lower bound is always less than or equal to the upper bound (e.g.,\nL[3:1] is scaled to L[3:3], the empty insertion point at offset 3).\nPython slices are always extracted from left to right, even if you use\nnegative indexes (they are first converted to positive indexes by adding\nthe sequence length). Note that Python’s three-limit slices modify this\nbehavior somewhat. For instance, L[3:1:-1] does extract from right to\nleft:",
      "content_length": 1504,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1744,
      "chapter": null,
      "content": ">>> L = [1, 2, 3, 4]\n>>> L[4]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n>>> L[-1000:100]\n[1, 2, 3, 4]\n>>> L[3:1]\n[]\n>>> L\n[1, 2, 3, 4]\n>>> L[3:1] = ['?']\n>>> L\n[1, 2, 3, '?', 4]\n3. Indexing, slicing, and del: Your interaction with the interpreter should\nlook something like the following. Note that assigning an empty list to\nan offset stores an empty list object there, but assigning an empty list to\na slice deletes the slice. Slice assignment expects another sequence, or\nyou’ll get a type error; it inserts items inside the sequence assigned, not\nthe sequence itself:\n>>> L = [1, 2, 3, 4]\n>>> L[2] = []\n>>> L\n[1, 2, [], 4]\n>>> L[2:3] = []\n>>> L\n[1, 2, 4]\n>>> del L[0]\n>>> L",
      "content_length": 744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1745,
      "chapter": null,
      "content": "[2, 4]\n>>> del L[1:]\n>>> L\n[2]\n>>> L[1:2] = 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only assign an iterable\n4. Tuple assignment: The values of X and Y are swapped. When tuples\nappear on the left and right of an assignment symbol (=), Python assigns\nobjects on the right to targets on the left according to their positions.\nThis is probably easiest to understand by noting that the targets on the\nleft aren’t a real tuple, even though they look like one; they are simply a\nset of independent assignment targets. The items on the right are a tuple,\nwhich gets unpacked during the assignment (this tuple provides the\ntemporary assignment needed to achieve the swap effect):\n>>> X = 'code'\n>>> Y = 'hack'\n>>> X, Y = Y, X\n>>> X\n'hack'\n>>> Y\n'code'\n5. Dictionary keys: Any immutable (technically, “hashable”) object can be\nused as a dictionary key, including integers, tuples, strings, and so on.\nThis really is a dictionary, even though some of its keys look like\ninteger offsets. Mixed-type keys work fine, too:\n>>> D = {}\n>>> D[1] = 'a'\n>>> D[2] = 'b'",
      "content_length": 1098,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1746,
      "chapter": null,
      "content": ">>> D[(1, 2, 3)] = 'c'\n>>> D\n{1: 'a', 2: 'b', (1, 2, 3): 'c'}\n6. Dictionary indexing: Indexing a nonexistent key (D['d']) raises an\nerror; assigning to a nonexistent key (D['d']='hack') creates a new\ndictionary entry. On the other hand, out-of-bounds indexing for lists\nraises an error, too, but so do out-of-bounds assignments. Variable\nnames work like dictionary keys; they must have already been assigned\nwhen referenced, but they are created when first assigned. In fact,\nvariable names can be processed as dictionary keys if you wish (they’re\nmade visible in the dictionaries of stack frames or module [or other\nobject] namespaces):\n>>> D = {'a': 1, 'b': 2, 'c': 3}\n>>> D['a']\n1\n>>> D['d']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'd'\n>>> D['d'] = 4\n>>> D\n{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n>>> L = [0, 1]\n>>> L[2]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n>>> L[2] = 3\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list assignment index out of range",
      "content_length": 1101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1747,
      "chapter": null,
      "content": "7. Generic operations: Question answers (with some error text omitted in\nlistings):\na. The + operator doesn’t work on different/mixed types (e.g.,\nstring + list, list + tuple).\nb. + doesn’t work for dictionaries, as they aren’t sequences\n(though | does).\nc. The append method works only for lists, not strings, and keys\nworks only on dictionaries. append assumes its target is\nmutable, since it’s an in-place extension; strings are immutable.\nDictionary keys is similarly type specific.\nd. Slicing and concatenation always return a new object of the\nsame type as the objects processed:\n>>> 'x' + 1\nTypeError: illegal argument type for built-in operation\n>>> {} + {}\nTypeError: bad operand type(s) for +\n>>> [].append(9)\n>>> ''.append('s')\nAttributeError: attribute-less object\n>>> list({}.keys())\n[]\n>>> [].keys()\nAttributeError: keys\n>>> [][:]\n[]\n>>> ''[:]\n''",
      "content_length": 860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1748,
      "chapter": null,
      "content": "8. String indexing: This is a bit of a trick question—because strings are\ncollections of one-character strings, every time you index a string, you\nget back a string that can be indexed again. S[0][0][0][0][0] just\nkeeps indexing the first character over and over. This generally doesn’t\nwork for lists (lists can hold arbitrary objects) unless the list contains\nstrings:\n>>> S = 'hack'\n>>> S[0][0][0][0][0]\n'h'\n>>> L = ['h', 'a']\n>>> L[0][0][0]\n'h'\n9. Immutable types: Either of the following solutions works. Index\nassignment doesn’t because strings are immutable:\n>>> S = 'hack'\n>>> S = S[0] + 'e' + S[2:]\n>>> S\n'heck' \n>>> S = S[0] + 'i' + S[2] + S[3]\n>>> S\n'hick'\n(See also the bytearray string type in Chapter 37—it’s a mutable\nsequence of small integers that is essentially processed the same as a\nstring, especially when its bytes are ASCII character code points.)\n10. Nesting: Here is a sample (your specs will vary):\n>>> pat = {'name': ('Pat', 'Q', 'Jones'), 'age': None, 'job': 'engineer'}\n>>> pat['job']\n'engineer'\n>>> pat['name'][2]\n'Jones'",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1749,
      "chapter": null,
      "content": "11. Files: Examples B-3 and B-4 show one way to create and read back a\ntext file in Python using Unicode encoding defaults on the host (which\nare generally moot for simple ASCII text like this):\nExample B-3. Part2/maker.py\nfile = open('myfile.txt', 'w')\nfile.write('Hello file world!\\n')        # Or: open().write()\nfile.close()                             # close not always needed\nExample B-4. Part2/reader.py\nfile = open('myfile.txt')                # 'r' is default open mode\nprint(file.read())                       # Or print(open().read())\nWhen run (here, from a console command line), the file shows up in the\ndirectory you’re working in because its name has no path prefix. The ls\nhere is a Unix command; use dir on Windows:\n$ python3 maker.py\n$ python3 reader.py\nHello file world!\n$ ls -l myfile.txt\n-rw-r--r--  1 me  staff  18 Aug 11 19:34 myfile.txt",
      "content_length": 861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1750,
      "chapter": null,
      "content": "Part III, Statements and Syntax\nSee “Test Your Knowledge: Part III Exercises” in Chapter 15 for the exercises.\n1. Coding basic loops: As you work through this exercise, you’ll wind up\nwith code that looks like the following:\n>>> S = 'hack'\n>>> for c in S:\n...     print(ord(c))\n...\n104\n97\n99\n107\n>>> x = 0\n>>> for c in S: x += ord(c)             # Or: x = x + ord(c)\n...\n>>> x\n407\n>>> chr(x)                              # Extra credit: non-ASCII, see \nChapter 37\n'Ɨ'\n>>> x = []\n>>> for c in S: x.append(ord(c))        # Manual list construction\n...\n>>> x\n[104, 97, 99, 107] \n>>> list(map(ord, S))\n[115, 112, 97, 109]\n>>> [ord(c) for c in S]                 # map and listcomps automate list \nbuilders",
      "content_length": 701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1751,
      "chapter": null,
      "content": "[115, 112, 97, 109]\n2. Coding basic selections: Here is the sort of code expected. To handle\nout-of-range numbers, add an else for if, a case _ for match, a get\nmethod call or in test for the dictionary, and a try handler for the list.\nFor versions of this code that are easier to copy/paste, see file\nPart3/selections.txt in the examples package:\n>>> month = 3\n>>> if month == 1:\n...     print('January')\n... elif month == 2:\n...     print('February')\n... elif month == 3:\n...     print('March')\n... \nMarch\n>>> match month:\n...     case 1:\n...         print('January')\n...     case 2:\n...         print('February')\n...     case 3:\n...         print('March')\n... \nMarch\n>>> {1: 'January', 2: 'February', 3: 'March'}[month]\n'March'\n>>> ['January', 'February', 'March'][month - 1]\n'March'\n3. Backslash characters: The example prints the bell character (\\a) 50\ntimes. Assuming your machine can handle it, and when it’s run outside",
      "content_length": 927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1752,
      "chapter": null,
      "content": "of some interfaces like IDLE, you may get a series of beeps (or one\nsustained tone if your machine is fast enough). Hey—you were warned.\n4. Sorting dictionaries: Here’s one way to work through this exercise (see\nChapter 8 or Chapter 14 if this doesn’t make sense). You really do have\nto split off the keys and sort calls like this because sort returns None.\nYou can iterate through dictionary keys directly without calling keys\n(e.g., for key in D:), but the keys list will not be sorted like it is by\nthis code. The sorted built-in is simpler but creates a new list object:\n>>> D = {'a': 1, 'c': 3, 'e': 5, 'g': 7, 'f': 6, 'd': 4, 'b': 2}\n>>> D\n{'a': 1, 'c': 3, 'e': 5, 'g': 7, 'f': 6, 'd': 4, 'b': 2}\n>>> keys = list(D.keys())                # Keys view has no sort method\n>>> keys.sort()                          # Sort list in place: returns \nNone\n>>> for key in keys:                     # Iterate over sorted list\n...     print(key, '=>', D[key])\n...\na => 1\nb => 2\nc => 3\nd => 4\ne => 5\nf => 6\ng => 7\n>>> D\n{'a': 1, 'c': 3, 'e': 5, 'g': 7, 'f': 6, 'd': 4, 'b': 2}\n>>>\n>>> for key in sorted(D):                # Simpler alternative, but a new \nlist\n...     print(key, '=>', D[key])\n...\n…same output…",
      "content_length": 1203,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1753,
      "chapter": null,
      "content": "5. Program logic alternatives: Here’s some sample code for the solutions,\navailable in the examples package’s Part3/power*.py. For step e, assign\nthe result of 2 ** X to a variable outside the loops of steps a and b and\nuse it inside the loop. Your results may vary; this exercise is mostly\ndesigned to get you playing with code alternatives, so anything\nreasonable gets full credit:\n# a\nL = [1, 2, 4, 8, 16, 32, 64]\nX = 5\ni = 0\nwhile i < len(L):\n    if 2 ** X == L[i]:\n        print('at index', i)\n        break\n    i += 1\nelse:\n    print(X, 'not found')\n# b\nL = [1, 2, 4, 8, 16, 32, 64]\nX = 5\nfor p in L:\n    if (2 ** X) == p:\n        print((2 ** X), 'was found at', L.index(p))\n        break\nelse:\n    print(X, 'not found')\n# c",
      "content_length": 730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1754,
      "chapter": null,
      "content": "L = [1, 2, 4, 8, 16, 32, 64]\nX = 5\nif (2 ** X) in L:\n    print((2 ** X), 'was found at', L.index(2 ** X))\nelse:\n    print(X, 'not found')\n# d\nX = 5\nL = []\nfor i in range(7): L.append(2 ** i)\nprint(L)\nif (2 ** X) in L:\n    print((2 ** X), 'was found at', L.index(2 ** X))\nelse:\n    print(X, 'not found')\n# \"Deeper thoughts\"\nX = 5\nL = list(map(lambda x: 2 ** x, range(7)))      # Or [2 ** x for x in \nrange(7)]\nprint(L)\nif (2 ** X) in L:\n    print((2 ** X), 'was found at', L.index(2 ** X))\nelse:\n    print(X, 'not found')",
      "content_length": 520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1755,
      "chapter": null,
      "content": "Part IV, Functions and Generators\nSee “Test Your Knowledge: Part IV Exercises” in Chapter 21 for the exercises.\n1. The basics: There’s not much to this one, but notice that using print\n(and hence your function) is technically a polymorphic operation, which\ndoes the right thing for each type of object:\n$ python3\n>>> def echo(x): \n        print(x)\n>>> echo('hack')\nhack\n>>> echo(3.12)\n3.12\n>>> echo([1, 2, 3])\n[1, 2, 3]\n>>> echo({'edition': 6})\n{'edition': 6}\n2. Arguments: Example B-5 gives a sample solution. Remember that you\nhave to use print to see results in the test calls because a file isn’t the\nsame as code typed interactively; Python doesn’t normally echo the\nresults of expression statements in files:\nExample B-5. Part4/adder1.py\ndef adder(x, y):\n    return x + y\nprint(adder(5, 1.0))\nprint(adder('hack', 'code'))\nprint(adder(['a', 'b'], ['c', 'd']))\nAnd the output:",
      "content_length": 880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1756,
      "chapter": null,
      "content": "$ python3 adder1.py\n6.0\nhackcode\n['a', 'b', 'c', 'd']\n3. Arbitrary arguments: Two alternative adder functions are shown in\nExample B-6. The hard part here is figuring out how to initialize an\naccumulator to an empty value of whatever type is passed in. The first\nsolution uses manual type testing to look for an integer and an empty\nslice of the first argument (assumed to be a sequence) if the argument is\ndetermined not to be an integer. The second solution uses the first\nargument to initialize and scan items 2 and beyond, much like one of\nthe min function variants shown in Chapter 18.\nThe second solution may be better. Both of these assume all arguments\nare of the same type, and neither works on dictionaries (as we saw in\nPart II, + doesn’t work on mixed types or dictionaries). You could add a\ntype test and special code using for, update, **, or | to support\ndictionaries combos, too, but that’s extra credit; see solutions 5 and 6\nahead for related notes. And yes, there is a sum(iterable) built-in in\nPython that would make this even simpler, but the point here is to write\ncode of your own; you’ll have to eventually:\nExample B-6. Part4/adder2.py\ndef adder1(*args):\n    print('adder1:', end=' ')\n    if type(args[0]) == type(0):              # Integer?\n         sum = 0                              # Init to zero\n    else:                                     # else sequence:\n         sum = args[0][:0]                    # Use empty slice of arg1\n    for arg in args:\n        sum = sum + arg\n    return sum\ndef adder2(*args):\n    print('adder2:', end=' ')",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1757,
      "chapter": null,
      "content": "sum = args[0]                             # Init to arg1\n    for next in args[1:]:\n        sum += next                           # Add items 2..N\n    return sum\nfor func in (adder1, adder2):\n    print(func(2, 3, 4))\n    print(func('hack', 'code', 'well'))\n    print(func(['a', 'b'], ['c', 'd'], ['e', 'f']))\nHere’s the sort of output you should get:\n$ python3 adder2.py\nadder1: 9\nadder1: hackcodewell\nadder1: ['a', 'b', 'c', 'd', 'e', 'f']\nadder2: 9\nadder2: hackcodewell\nadder2: ['a', 'b', 'c', 'd', 'e', 'f']\n4. Keywords: Example B-7 gives a solution to the first part of this\nexercise, along with its output in a console.\nExample B-7. Part4/adder3.py\ndef adder(red=1, green=2, blue=3):\n    return red + green + blue\nprint(adder())\nprint(adder(5))\nprint(adder(5, 6))\nprint(adder(5, 6, 7))\nprint(adder(blue=7, red=6, green=5))\nprint(adder(blue=1, red=2))\n$ python3 adder3.py\n6",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1758,
      "chapter": null,
      "content": "10\n14\n18\n18\n5\nExample B-8 gives the second part’s solution and its output. To iterate\nover keyword arguments, use the **args form in the function header\nand use a loop (e.g., for x in args.keys(): use args[x]), or use\nargs.values() to make this the same as summing *args positionals in\nexercise number 3:\nExample B-8. Part4/adder4.py\ndef adder1(*args):                  # Sum any number of positional args\n    tot = args[0]                   # Same as #3, for comparison and reuse\n    for arg in args[1:]:\n        tot += arg\n    return tot\ndef adder2(**args):                 # Sum any number of keyword args\n    argskeys = list(args.keys())    # list required to index!\n    tot = args[argskeys[0]]\n    for key in argskeys[1:]:\n        tot += args[key]\n    return tot\ndef adder3(**args):                 # Same, but convert to list of values\n    args = list(args.values())      # list needed to index!\n    tot = args[0]\n    for arg in args[1:]:\n        tot += arg\n    return tot\ndef adder4(**args):                 # Same, but reuse positional version\n    return adder1(*args.values())",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1759,
      "chapter": null,
      "content": "print(adder1(1, 2, 3),       adder1('aa', 'bb', 'cc'))\nprint(adder2(a=1, b=2, c=3), adder2(a='aa', b='bb', c='cc'))\nprint(adder3(a=1, b=2, c=3), adder3(a='aa', b='bb', c='cc'))\nprint(adder4(a=1, b=2, c=3), adder4(a='aa', b='bb', c='cc'))\n$ python3 adder4.py\n6 aabbcc\n…repeated 4 times…\n5. (5 and 6) Dictionary tools: Solutions for exercises 5 and 6 are combined\nand listed in Example B-9. These are just coding exercises because\nPython now provides dictionary methods D.copy() and\nD1.update(D2) to handle things like copying and adding (merging)\ndictionaries. In fact, there are four ways to merge dictionaries today, as\nhinted in solution 3: for loops like those here, D1.update(D2),\n{**D1,**D2}, and D1|D2. See Chapter 8 for more info on and examples\nof these tools. X[:] doesn’t work for dictionaries, as they’re not\nsequences (see Chapter 8 for details). Also, remember that if you assign\n(e = d) rather than copying, you generate a reference to a shared\ndictionary object; changing d changes e, too:\nExample B-9. Part4/dicttools.py\ndef copyDict(old):\n    new = {}\n    for key in old.keys():\n        new[key] = old[key]\n    return new\ndef addDict(d1, d2):\n    new = {}\n    for key in d1.keys():\n        new[key] = d1[key]\n    for key in d2.keys():\n        new[key] = d2[key]\n    return new",
      "content_length": 1293,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1760,
      "chapter": null,
      "content": "Here is the expected behavior of this code demoed in a REPL:\n$ python3\n>>> from dicttools import *\n>>> d = {1: 1, 2: 2}\n>>> e = copyDict(d)\n>>> d[2] = '?'\n>>> d\n{1: 1, 2: '?'}\n>>> e\n{1: 1, 2: 2}\n>>> x = {1: 1}\n>>> y = {2: 2}\n>>> z = addDict(x, y)\n>>> z\n{1: 1, 2: 2}\n6. See #5 (where solutions were combined).\n7. More argument-matching examples: Here is the sort of interaction you\nshould get, along with comments that explain the matching that goes on.\nIt may be easiest to paste the functions into a file and import them all\nwith a * for testing in a REPL; they’re repeated in Example B-10 for\nreference (and in the examples package for copying):\nExample B-10. Part4/testfuncs.py\ndef f1(a, b): print(a, b)            # Normal args\ndef f2(a, *b): print(a, b)           # Positional collectors\ndef f3(a, **b): print(a, b)          # Keyword collectors\ndef f4(a, *b, **c): print(a, b, c)   # Mixed modes\ndef f5(a, b=2, c=3): print(a, b, c)  # Defaults",
      "content_length": 949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1761,
      "chapter": null,
      "content": "def f6(a, b=2, *c): print(a, b, c)   # Defaults and positional collector\nThe expected REPL interaction:\n$ python3\n>>> from testfuncs import *\n>>> f1(1, 2)                         # Matched by position (order matters)\n1 2\n>>> f1(b=2, a=1)                     # Matched by name (order doesn't \nmatter)\n1 2\n>>> f2(1, 2, 3)                      # Extra positionals collected in a \ntuple\n1 (2, 3)\n>>> f3(1, x=2, y=3)                  # Extra keywords collected in a \ndictionary\n1 {'x': 2, 'y': 3}\n>>> f4(1, 2, 3, **dict(x=2, y=3))    # Extras of both kinds, star \nunpacking\n1 (2, 3) {'x': 2, 'y': 3}\n>>> f5(1)                            # Both defaults kick in\n1 2 3\n>>> f5(1, 4)                         # Only one default used\n1 4 3\n>>> f5(1, c=4)                       # Middle default applied\n1 2 4\n>>> f6(1)                            # One argument: matches \"a\"\n1 2 ()\n>>> f6(1, *[3, 4])                   # Extra positional collected, star \nunpacking",
      "content_length": 951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1762,
      "chapter": null,
      "content": "1 3 (4,)\n8. Primes revisited: Example B-11 is the primes example, wrapped up in a\nfunction and a module (file primes.py) so it can be run multiple times.\nAn if test was added to trap negatives, 0, and 1. It’s crucial to use //\nfloor division instead of the / true division we studied in Chapter 5 to\navoid fractional remainders (5 / 2 would yield a false factor 2.5, but 5 /\n2 truncates down to 2). Change // to / to see the difference for\nyourself:\nExample B-11. Part4/primes.py\ndef prime(y):\n    if y <= 1:                                       # For some y > 1\n        print(y, 'is nonprime')\n    else:\n        x = y // 2                                   # But / fails\n        while x > 1:\n            if y % x == 0:                           # No remainder?\n                print(y, 'has factor', x)\n                break                                # Skip else\n            x -= 1\n        else:\n            print(y, 'is prime')\ntests = (27, 24, 13, 13.0, 15, 15.0, 3, 2, 1, -3)\nfor test in tests:\n    prime(test)\nHere is the module in action; the // operator also allows it to work for\nfloating-point numbers by truncating to the floor (5.0 // 2 is 2.0, not\n2.5):\n$ python3 primes.py\n27 has factor 9\n24 has factor 12\n13 is prime",
      "content_length": 1236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1763,
      "chapter": null,
      "content": "13.0 is prime\n15 has factor 5\n15.0 has factor 5.0\n3 is prime\n2 is prime\n1 is nonprime\n-3 is nonprime\nThis function still isn’t very reusable—it could return values, instead of\nprinting—but it’s enough to run experiments. It’s also not a strict\nmathematical prime (floating-point numbers work, but shouldn’t), and\nit’s still perhaps inefficient. Improvements are left as exercises for more\nmathematically minded readers. (Hint: a for loop over range(x, 1,\n−1) may be a bit quicker than the while, but the algorithm may be the\nreal bottleneck here.) To time alternatives, use the homegrown timer or\nstandard-library timeit modules and coding patterns like those used in\nChapter 21’s benchmarking sections (and solution 10 ahead).\n9. Iterations and comprehensions: Here is the sort of code you should\nwrite; coding alternatives are notoriously subjective, so there’s no right\nor wrong preference (though see the next solution for an objective\nfactor):\n>>> values = [2, 4, 9, 16, 25]\n>>> import math\n>>> res = []\n>>> for x in values: res.append(math.sqrt(x))       # Manual loop\n...\n>>> res\n[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]\n>>> list(map(math.sqrt, values))                    # map built-in call\n[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]\n>>> [math.sqrt(x) for x in values]                  # List comprehension",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1764,
      "chapter": null,
      "content": "[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]\n>>> list(math.sqrt(x) for x in values)              # Generator expression\n[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]\n10. Timing tools: The code file in Example B-13 times the three square root\noptions. Each test takes the best of 5 runs; each run takes the total time\nrequired to call the test function 1,000 times; and each test function\niterates 10,000 times. The last result of each function is printed to verify\nthat all three do the same work.\nThis code also uses a preview (really, cheat) to remotely access the\ntimer2.py module in Chapter 21’s code folder (Example B-12) with an\nimport run its own folder, assumed to be the examples’\nAppendixB/Part4. Appending sys.path is one way to augment the\nsearch path used to find imported modules, along with PYTHONPATH\nenvironment settings. This avoids a file copy; we’ll explore it in this\nbook’s next part, so take it on faith for now.\nExample B-12. ../../Chapter21/timer2.py\n…Example 21-7 in Chapter 21…\nExample B-13. Part4/timesqrt.py\nimport sys                          # Add timer2.py's folder to search path\nsys.path.append('../../Chapter21')  # Assuming running in AppendixB/Part4\nimport timer2                       # A cheat! - see Part V for path info\nreps = 10_000\nrepslist = list(range(reps))        # Pull out range list time\nfrom math import sqrt               # Not math.sqrt: adds attr fetch time\ndef mathMod():\n    for i in repslist:\n        res = sqrt(i)\n    return res",
      "content_length": 1477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1765,
      "chapter": null,
      "content": "def powCall():\n    for i in repslist:\n        res = pow(i, .5)\n    return res\ndef powExpr():\n    for i in repslist:\n        res = i ** .5\n    return res\nprint(sys.version)\nfor test in (mathMod, powCall, powExpr):\n    elapsed, result = timer2.bestoftotal(test, _reps1=5, _reps=1000)\n    print (f'{test.__name__}: {elapsed:.5f} => {result}')\nFollowing are the test results for CPython 3.12 (the standard) and PyPy\n7.3 (which implements Python 3.10) on macOS. In short, the math\nmodule is quicker than the ** expression on both Pythons, and ** is\nquicker than the pow built-in function in CPython but the same in PyPy:\n$ python3 timesqrt.py \n3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-\n1300.0.29.30)]\nmathMod: 0.40860 => 99.99499987499375\npowCall: 0.68245 => 99.99499987499375\npowExpr: 0.57762 => 99.99499987499375\n$ pypy3 timesqrt.py \n3.10.14 (75b3de9d9035, Apr 21 2024, 10:56:19)\n[PyPy 7.3.16 with GCC Apple LLVM 15.0.0 (clang-1500.1.0.2.5)]\nmathMod: 0.05246 => 99.99499987499375\npowCall: 0.33288 => 99.99499987499375\npowExpr: 0.33244 => 99.99499987499375\nPyPy is also some 8X to 2X faster than CPython on floating-point math\nand iterations here, but CPython may sprout a JIT, which evens the gap",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1766,
      "chapter": null,
      "content": "(see Chapter 2). The results for CPython jive with the prior edition’s\ntests for CPython 3.3 that follow, which used different repeat counts and\nhosts but were relatively similar. As always, you should try this with\nyour code and on your own machine and version of Python for more\ndefinitive results:\nc:\\code> py −3 timesqrt.py\n3.3.0 (v3.3.0:bd8afb90ebf2, Sep 29 2012, 10:57:17) [MSC v.1600 64 bit \n(AMD64)]\nmathMod: 2.04481 => 99.99499987499375\npowCall: 3.40973 => 99.99499987499375\npowExpr: 2.56458 => 99.99499987499375\nTo time the relative speeds of dictionary comprehensions and equivalent\nfor loops interactively, you can run a session like the following. At\nleast on this test in CPython 3.12, the two are roughly the same in\nspeed, with a slight advantage to comprehensions—though the\ndifference isn’t exactly earth-shattering. As verification, these results\nrelatively match those we obtained from a pybench test in Example 21-\n10 (sans the slower dict call). Do similar to vet the speed of\ncomprehensions with if and for. And again, rather than taking any of\nthese results as gospel, you should investigate further on your own with\nyour computer and your Python:\n$ python3\n>>> def dictcomp(I):\n        return {i: i for i in range(I)}\n>>> def dictloop(I):\n        new = {}\n        for i in range(I): new[i] = i\n        return new\n>>> dictcomp(10)\n{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1767,
      "chapter": null,
      "content": ">>> dictloop(10)\n{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n>>> import sys; sys.path.append('../../Chapter21')\n>>> from timer2 import bestoftotal\n>>> bestoftotal(dictcomp, 10_000, _reps1=5, _reps=500)[0]\n0.17137739405734465\n>>> bestoftotal(dictloop, 10_000, _reps1=5, _reps=500)[0]\n0.18112968490459025\n \n>>> len(bestoftotal(dictcomp, 10_000, _reps1=5, _reps=500)[1])\n10000\n>>> len(bestoftotal(dictloop, 10_000, _reps1=5, _reps=500)[1])\n10000\n11. Recursive functions: One way to code this function follows (typed in a\nREPL here, but also coded in file Part4/countdown.py of the examples\npackage). A simple range, comprehension, or map will do the job here\nas well, of course, but recursion is useful enough to warrant the\nexperimentation here:\n>>> def countdown(N):\n        if N == 0:\n            print('stop')\n        else:\n            print(N, end=' ')\n            countdown(N - 1)\n>>> countdown(5)\n5 4 3 2 1 stop\n>>> countdown(20)\n20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 stop\n# Nonrecursive options",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1768,
      "chapter": null,
      "content": ">>> list(range(5, 0, -1))\n[5, 4, 3, 2, 1]\n>>> t = [print(i, end=' ') for i in range(5, 0, -1)]\n5 4 3 2 1\n>>> t = list(map(lambda x: print(x, end=' '), range(5, 0, -1)))\n5 4 3 2 1\nA generator-based solution isn’t required for this exercise, but one is\nlisted next; all the other techniques seem much simpler in this case—a\ngood example of contexts where generators should probably be avoided.\nRemember that generators produce no results until iterated, so we need\na for loop or yield from here (yielding countdown2(N-1) directly\nsimply returns a generator, not its products):\n>>> def countdown2(N):                            # Generator function, \nrecursive\n        if N == 0:\n            yield 'stop'\n        else:\n            yield N\n            for x in countdown2(N - 1): yield x   # Or: yield from \ncountdown2(N - 1)\n>>> list(countdown2(5))\n[5, 4, 3, 2, 1, 'stop']\n# Nonrecursive options\n>>> def countdown3():                       # Generator function, simpler\n        yield from range(5, 0, -1)          # Or: for x in range(): yield \nx\n>>> list(countdown3())\n[5, 4, 3, 2, 1]",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1769,
      "chapter": null,
      "content": ">>> list(x for x in range(5, 0, -1))        # Equivalent generator \nexpression\n[5, 4, 3, 2, 1]\n12. Computing factorials: Example B-14 shows one way to code this\nexercise, using Python’s standard-library timeit module of Chapter 21.\nNaturally, there are many possible variations on its code; its ranges, for\ninstance, could run from 2..N+1 to skip an iteration, and fact2 could\nuse reduce(operator.mul, range(N, 1, −1)) to avoid a lambda.\nImprove freely.\nExample B-14. Part4/factorials.py\nfrom functools import reduce\nfrom timeit import repeat\nimport math\ndef fact0(N):                                              # Recursive\n    if N == 1:                                             # Fails at stack \nlimit\n        return N\n    else:\n        return N * fact0(N - 1)\ndef fact1(N):\n    return N if N == 1 else N * fact1(N - 1)               # Recursive, one-\nliner\ndef fact2(N):                                              # Functional\n    return reduce(lambda x, y: x * y, range(1, N + 1))\ndef fact3(N):\n    res = 1\n    for i in range(1, N + 1): res *= i                     # Iterative\n    return res\ndef fact4(N):",
      "content_length": 1117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1770,
      "chapter": null,
      "content": "return math.factorial(N)                               # Stdlib \n\"batteries\"\n# Tests\nprint(fact0(6), fact1(6), fact2(6), fact3(6), fact4(6))    # 6*5*4*3*2*1: all \n720\nprint(fact0(500) == fact1(500) == fact2(500) == fact3(500) == fact4(500))  # \nTrue\nfor test in (fact0, fact1, fact2, fact3, fact4):\n    print(test.__name__, min(repeat(stmt=lambda: test(500), number=1000, \nrepeat=5)))\nThis code uses Python’s timeit module to benchmark alternatives. Its\nresults for CPython 3.12 on macOS:\n$ python3 factorials.py\n720 720 720 720 720\nTrue\nfact0 0.08720566902775317\nfact1 0.08635473699541762\nfact2 0.06704489700496197\nfact3 0.05152398400241509\nfact4 0.00873392098583281\nConclusions: recursion is slowest on this Python and machine and fails\nonce N reaches the maximum stack-size setting in sys. Per Chapter 19,\nthis limit can be increased, but simple loops or the standard-library tool\nseem the best route here in any event, and the built-in wins soundly.\nThis general finding holds true often. For instance,\n''.join(reversed(S)) may be the preferred way to reverse a string,\neven though recursive solutions are possible. Time the code in\nExample B-15 to see for yourself:\nExample B-15. Part4/reverses.py\ndef rev1(S):",
      "content_length": 1216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1771,
      "chapter": null,
      "content": "if len(S) == 1:\n        return S\n    else:\n        return S[-1] + rev1(S[:-1])        # Recursive\ndef rev2(S):\n    return ''.join(reversed(S))            # Nonrecursive iterable\n        \ndef rev3(S):\n    return S[::-1]                         # Sequence reversal by slice",
      "content_length": 271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1772,
      "chapter": null,
      "content": "Part V, Modules and Packages\nSee “Test Your Knowledge: Part V Exercises” in Chapter 25 for the exercises.\n1. Import basics: When you’re done, your file and REPL interaction with\nit should look similar to Example B-16. Remember that Python can read\na whole file into a list of line strings, and the len built-in returns the\nlengths of strings and lists:\nExample B-16. Part5/mymod.py (initial code, mymod_start.py)\ndef countLines(name):\n    file = open(name)\n    return len(file.readlines())\ndef countChars(name):\n    return len(open(name).read())\ndef test(name):                                  # Or pass file object\n    return countLines(name), countChars(name)    # Or return a dictionary\n$ python3\n>>> import mymod\n>>> mymod.test('mymod.py')\n(10, 281)\nYour counts may vary for comments, an extra line at the end, and so on,\nand you don’t need to set PYTHONPATH if the module is in the\nautomatically searched current working directory. Note that these\nfunctions load the entire file in memory all at once, so they won’t work\nfor pathologically large files that are too big for your device’s memory.\nTo be more robust, you could read line by line with iterators instead and\ncount as you go (see Part5/mymod_lines.py in the examples package):\ndef countLines(name):\n    tot = 0",
      "content_length": 1276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1773,
      "chapter": null,
      "content": "for line in open(name): tot += 1\n    return tot\ndef countChars(name):\n    tot = 0\n    for line in open(name): tot += len(line)\n    return tot\nA generator expression can have the same effect (though the excessive\nmagic may cost you some points):\ndef countLines(name): return sum(+1 for line in open(name))\ndef countChars(name): return sum(len(line) for line in open(name))\nOn Unix, you can verify your output with a wc command; on Windows,\nright-click on your file to view its properties. Note that your script may\nreport fewer characters than Windows does—for portability, Python\nconverts Windows \\r\\n line-end markers to \\n, thereby dropping one\nbyte (character) per line. To match byte counts with Windows exactly,\nyou must open in binary mode ('rb') or add the number of bytes\ncorresponding to the number of lines. See Chapters 9 and 37 for more\non end-of-line translations in text files.\nThe “ambitious” part of this exercise (passing in a file object so you\nonly open the file once) will require you to use the seek method of the\nbuilt-in file object. It works like C’s fseek call (and may call it behind\nthe scenes): seek resets the current position in the file to a passed-in\noffset. After a seek, future input/output operations are relative to the\nnew position. To rewind to the start of a file without closing and\nreopening it, call file.seek(0); the file read methods all pick up at\nthe current position in the file, so you need to rewind to reread.\nExample B-17 shows what this tweak would look like, along with its\noutput in a REPL:\nExample B-17. Part5/mymod2.py\ndef countLines(file):",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1774,
      "chapter": null,
      "content": "file.seek(0)                                 # Rewind to start of file\n    return len(file.readlines())\ndef countChars(file):\n    file.seek(0)                                 # Ditto (rewind if needed)\n    return len(file.read())\ndef test(name):\n    file = open(name)                            # Pass file object\n    return countLines(file), countChars(file)    # Open file only once\n$ python3\n>>> import mymod2\n>>> mymod2.test('mymod2.py')\n(12, 414)\n2. from/from *: Here’s the from * part; replace * with countChars to do\nthe rest:\n$ python3\n>>> from mymod import *\n>>> countChars('mymod.py')\n281\n3. __main__: If you code it properly, this file works in either mode—\nprogram run or module import, as Example B-18 and the REPL session\nfollowing it demo:\nExample B-18. Part5/mymod.py (edited)\ndef countLines(name):\n    file = open(name)\n    return len(file.readlines())\ndef countChars(name):\n    return len(open(name).read())",
      "content_length": 925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1775,
      "chapter": null,
      "content": "def test(name):                                  # Or pass file object\n    return countLines(name), countChars(name)    # Or return a dictionary\nif __name__ == '__main__':                       # Added: self-test code\n    print(test('mymod.py'))                      # When run, not when \nimported\n$ python3 mymod.py\n(13, 434)\nThis is where you would probably begin to consider using command-\nline arguments or user input to provide the filename to be counted\ninstead of hardcoding it in the script. Examples B-19 and B-20 show the\nrequired mods (see Chapters 21 and 25 for more on sys.argv, and\nChapter 10 for more on input):\nExample B-19. Part5/mymod_argv.py (changed parts)\n…\nif __name__ == '__main__':\n    import sys                                   # Command-line argument\n    print(test(sys.argv[1]))\n$ python3 mymod_argv.py mymod.py\n(13, 434)\nExample B-20. Part5/mymod_input.py (changed parts)\n…\nif __name__ == '__main__':\n    print(test(input('Enter file name: ')))      # Console/user input\n$ python3 mymod_input.py\nEnter file name: mymod.py\n(13, 434)\n4. Nested imports: It’s not much, but Example B-21 gives one solution and\nits results (the point here is to experiment with importing one module\nfrom another in a variety of ways):",
      "content_length": 1242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1776,
      "chapter": null,
      "content": "Example B-21. Part5/myclient.py\nfrom mymod import countLines, countChars\nprint(countLines('mymod.py'), countChars('mymod.py'))\n$ python3 myclient.py\n13 434\nAs for the rest of this question, mymod’s functions are accessible (that is,\nimportable) from the top level of myclient, since from simply assigns\nto names in the importer (it works as if mymod’s defs appeared in\nmyclient). For example, another file can say:\nimport myclient\nmyclient.countLines(…)\nfrom myclient import countChars\ncountChars(…)\nIf myclient used import instead of from, you’d need to use a path to\nget to the functions in mymod through myclient:\nimport myclient\nmyclient.mymod.countLines(…)\nfrom myclient import mymod\nmymod.countChars(…)\nIn general, you can define collector modules that import all the names\nfrom other modules so they’re available in a single convenience\nmodule. The following hypothetical code, for example, creates three\ndifferent copies of the name somename—mod1.somename,\ncollector.somename, and __main__.somename; all three share the\nsame integer object initially, and only the name somename exists at the\ninteractive prompt as is:",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1777,
      "chapter": null,
      "content": "# File mod1.py (hypothetical)\nsomename = 99\n# File collector.py (hypothetical)\nfrom mod1 import *                               # Collect lots of names \nhere\nfrom mod2 import *                               # \"from\" assigns to my \nnames\nfrom mod3 import *\n>>> from collector import somename\n5. Package imports: For this, copy the mymod.py solution file listed for\nexercise 3 (Example B-18) into a directory package. The following\ncommands run in a Unix console set up the directory and an optional\n__init__.py file; you’ll need to interpolate for other platforms and tools\n(e.g., use copy and notepad on Windows instead of cp and vi). This\nworks in any directory, and you can do some of this from a file-explorer\nGUI, too.\nWhen finished, you’ll have a mypkg subdirectory that contains the files\n__init__.py and mymod.py. Technically, mypkg is located in the “home”\ndirectory component of the module search path. Notice how a print\nstatement coded in the directory’s initialization file fires only the first\ntime it is imported, not the second. Raw strings (r'…') can also avoid \\\nescape issues in the file paths if you’re working on Windows, but /\nworks there too:\n$ mkdir mypkg                       # Windows: same\n$ cp mymod.py mypkg/mymod.py        # Windows: copy mymod.py \nmypkg\\mymod.py\n$ vi mypkg/__init__.py              # Windows: notepad mypkg\\__init__.py\n…code a print statement…\n$ python3                                         # Windows: py -3 \n(probably)",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1778,
      "chapter": null,
      "content": ">>> import mypkg.mymod\ninitializing mypkg\n>>> mypkg.mymod.countLines('mypkg/mymod.py')      # Windows: same\n13\n>>> from mypkg.mymod import countChars\n>>> countChars('mypkg/mymod.py')                  # Windows: same\n434\nIf you copy the module to __main__.py, the copy will run if you run the\ndirectory as a whole (though there may be no reason to do so in\npractice, as the original module can be run directly too):\n$ cp mypkg/mymod.py mypkg/__main__.py             # Windows: copy\n$ python3 mypkg\n(13, 434)\n$ python3 mypkg/mymod.py\n(13, 434)\n6. Reloads: This exercise just asks you to experiment with changing the\nchanger.py example in the book’s Example 23-10, so there’s nothing to\nshow here.\n7. Circular imports: The short story is that importing recur2 first works\nbecause the recursive import then happens at the import in recur1, not\nat a from in recur2.\nThe long story goes like this: importing recur2 first works because the\nrecursive import from recur1 to recur2 fetches recur2 as a whole\ninstead of getting specific names. recur2 is incomplete when it’s\nimported from recur1, but because it uses import instead of from,\nyou’re safe: Python finds and returns the already created recur2\nmodule object and continues to run the rest of recur1 without a glitch.\nWhen the recur2 import resumes, the second from finds the name Y in\nrecur1 (it’s been run completely), so no error is reported.\nRunning a file as a script is not the same as importing it as a module;",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1779,
      "chapter": null,
      "content": "these cases are the same as running the first import or from in the script\ninteractively. For instance, running recur1 as a script works because it\nis the same as importing recur2 interactively, as recur2 is the first\nmodule imported in recur1. Running recur2 as a script fails for the\nsame reason—it’s the same as running its first import interactively.",
      "content_length": 354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1780,
      "chapter": null,
      "content": "Part VI, Classes and OOP\nSee “Test Your Knowledge: Part VI Exercises” in Chapter 32 for the exercises.\n1. Inheritance: Example B-22 lists a solution for this exercise, along with\nsome interactive tests. The __add__ overload has to appear only once,\nin the superclass, as it invokes type-specific add methods in subclasses:\nExample B-22. Part6/adder.py\nclass Adder:\n    def add(self, x, y):\n        print('not implemented!')\n    def __init__(self, start=[]):\n        self.data = start\n    def __add__(self, other):                    # Or in subclasses?\n        return self.add(self.data, other)        # Or return type?\nclass ListAdder(Adder):\n    def add(self, x, y):\n        return x + y\nclass DictAdder(Adder):\n    def add(self, x, y):\n        new = {}\n        for k in x.keys(): new[k] = x[k]\n        for k in y.keys(): new[k] = y[k]\n        return new\n$ python3\n>>> from adder import *\n>>> x = Adder()\n>>> x.add(1, 2)\nnot implemented!\n>>> x = ListAdder()\n>>> x.add([1], [2])",
      "content_length": 979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1781,
      "chapter": null,
      "content": "[1, 2]\n>>> x = DictAdder()\n>>> x.add({1: 1}, {2: 2})\n{1: 1, 2: 2}\n>>> x = Adder([1])\n>>> x + [2]\nnot implemented!\n>>>\n>>> x = ListAdder([1])\n>>> x + [2]\n[1, 2]\n>>> [2] + x\nTypeError: can only concatenate list (not \"ListAdder\") to list\nNotice in the last test that you get an error for expressions where a class\ninstance appears on the right of a +; if you want to fix this, use\n__radd__ methods, as described in Chapter 30.\nIf you are saving a value in the instance anyhow, you might as well\nrewrite the add method to take just one argument, in the spirit of other\nexamples in this part of the book. Example B-23 sketches this mutation:\nExample B-23. Part6/adder2.py\nclass Adder:\n    def __init__(self, start=[]):\n        self.data = start\n    def __add__(self, other):              # Pass a single argument\n        return self.add(other)             # The left side is in self\n    def add(self, y):\n        print('not implemented!')\nclass ListAdder(Adder):\n    def add(self, y):\n        return self.data + y\nclass DictAdder(Adder):",
      "content_length": 1032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1782,
      "chapter": null,
      "content": "def add(self, y):\n        d = self.data.copy()               # Change to use self.data instead \nof x\n        d.update(y)                        # Or \"cheat\" by using quicker \nbuilt-ins\n        return d\nx = ListAdder([1, 2, 3])\ny = x + [4, 5, 6]\nprint(y)                                   # Prints [1, 2, 3, 4, 5, 6]\nz = DictAdder(dict(name='x')) + {'a': 1}\nprint(z)                                   # Prints {'name': 'x', 'a': 1}\nBecause values are attached to objects rather than passed around, this\nversion is arguably more object-oriented. And, once you’ve gotten to\nthis point, you’ll probably find that you can get rid of add altogether\nand simply define type-specific __add__ methods in the two subclasses.\n2. Operator overloading: The solution code and its REPL results in\nExample B-24 demo a handful of operator-overloading methods we\nexplored in Chapter 30. Copying the initial value in the constructor is\nimportant because it may be mutable; you don’t want to change or have\na reference to an object that’s possibly shared somewhere outside the\nclass. The __getattr__ method routes calls to the wrapped list. For\ntips on a possibly easier way to code this, See “Extending Types by\nSubclassing” in Chapter 32:\nExample B-24. Part6/mylist.py\nclass MyList:\n    def __init__(self, start):\n        #self.wrapped = start[:]                  # Copy start: no side \neffects\n        self.wrapped = list(start)                # Make sure it's a list \nhere\n    def __add__(self, other):\n        return MyList(self.wrapped + other)",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1783,
      "chapter": null,
      "content": "def __mul__(self, time):\n        return MyList(self.wrapped * time)\n    def __getitem__(self, offset):                # Also passed a slice on \n[:]\n        return self.wrapped[offset]               # For iteration if no \n__iter__\n    def __len__(self):\n        return len(self.wrapped)                  # Also fallback for truth \ntests \n    def append(self, node):\n        self.wrapped.append(node)\n    def __getattr__(self, name):                  # Other methods: \nsort/reverse/etc.\n        return getattr(self.wrapped, name)\n    def __repr__(self):                           # Catchall display method\n        return repr(self.wrapped)\nif __name__ == '__main__':\n    x = MyList('hack')\n    print(x)\n    print(x[2])\n    print(x[1:])\n    print(x + ['code'])\n    print(x * 3)\n    x.append('1'); x.extend(['z'])\n    x.sort()\n    print(' '.join(c for c in x))\n$ python3 mylist.py\n['h', 'a', 'c', 'k']\nc\n['a', 'c', 'k']\n['h', 'a', 'c', 'k', 'code']\n['h', 'a', 'c', 'k', 'h', 'a', 'c', 'k', 'h', 'a', 'c', 'k']\n1 a c h k z",
      "content_length": 1017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1784,
      "chapter": null,
      "content": "Note that it’s also important to copy the start value by calling list\ninstead of slicing here, because otherwise the result may not be a true\nlist, and so will not respond to expected list methods, such as append\n(e.g., slicing a string returns another string, not a list). You would be\nable to copy a MyList start value by slicing because its class overloads\nthe slicing operation and provides the expected list interface; however,\nyou need to avoid slice-based copying for objects such as strings.\n3. Subclassing: One solution appears in Example B-25; your solution will\nbe similar. You can also use super here instead of explicit superclass\nnames for methods and attributes, as partly noted in the code’s\ncomments:\nExample B-25. Part6/mysub.py\nfrom mylist import MyList\nclass MyListSub(MyList):\n    calls = 0                                      # Shared by instances\n    def __init__(self, start):\n        self.adds = 0                              # Varies in each instance\n        MyList.__init__(self, start)               # Or: \nsuper().__init__(start)\n    def __add__(self, other):\n        print('add: ' + str(other))\n        MyListSub.calls += 1                       # Class-wide counter\n        self.adds += 1                             # Per-instance counts\n        return MyList.__add__(self, other)         # Or: \nsuper().__add__(other)\n    def stats(self):\n        return self.calls, self.adds               # All adds, my adds\nif __name__ == '__main__':\n    x = MyListSub('read')\n    y = MyListSub('code')",
      "content_length": 1523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1785,
      "chapter": null,
      "content": "print(x[2])\n    print(x[1:])\n    print(x + ['lp6e'])\n    print(x + ['book'])\n    print(y + ['py312'])\n    print(x.stats())\n$ python3 mysub.py\na\n['e', 'a', 'd']\nadd: ['lp6e']\n['r', 'e', 'a', 'd', 'lp6e']\nadd: ['book']\n['r', 'e', 'a', 'd', 'book']\nadd: ['py312']\n['c', 'o', 'd', 'e', 'py312']\n(3, 2)\n4. Attribute methods: The following works through this exercise. As noted\nin Chapter 28 and elsewhere, __getattr__ is not called for built-in\noperations in Python 3.X, so the expressions aren’t intercepted at all\nhere; a class like this must somehow redefine __X__ operator-\noverloading methods explicitly. You can find more on this limitation in\nChapters 28, 31, 32, and 38, as well as workarounds for it in Chapter 39\nand its inheritance special case in Chapter 40. Its impacts are potentially\nbroad but can be addressed with code:\n$ python3\n>>> class Attrs:\n        def __getattr__(self, name):\n            print('get:', name)\n        def __setattr__(self, name, value):\n            print('set:', name, value)\n>>> x = Attrs()\n>>> x.append",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1786,
      "chapter": null,
      "content": "get append\n>>> x.lang = 'py312'\nset: lang py312\n>>> x + 2\nTypeError: unsupported operand type(s) for +: 'Attrs' and 'int'\n>>> x[1]\nTypeError: 'Attrs' object is not subscriptable\n>>> x[1:5]\nTypeError: 'Attrs' object is not subscriptable\n5. Set objects: Here’s the sort of interaction you should get. To make the\nimport of Chapter32/setwrapper.py work, either run this in the folder\nwhere this file resides, copy this file to your working directory, or add\nthis file’s folder to your import search path per Part V. Comments\nexplain which methods are called. Also, bear in mind that sets are a\nbuilt-in type in Python, so this is mostly just a coding exercise (see\nChapter 5 for more on sets):\n$ python3\n>>> from setwrapper import Set     # Run there, copy here, or mod path\n>>> x = Set([1, 2, 3, 4])          # Runs __init__\n>>> y = Set([3, 4, 5])\n>>> x & y                          # __and__, intersect, then __repr__\nSet:[3, 4]\n>>> x | y                          # __or__, union, then __repr__\nSet:[1, 2, 3, 4, 5]\n>>> z = Set('hello')               # __init__ removes duplicates\n>>> z[0], z[-1], z[2:]             # __getitem__\n('h', 'o', ['l', 'o'])\n>>> for c in z: print(c, end=' ')  # __iter__ (else __getitem__)\n...\nh e l o\n>>> ''.join(c.upper() for c in z)  # __iter__ (else __getitem__)",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1787,
      "chapter": null,
      "content": "'HELO'\n>>> len(z), z                      # __len__, __repr__\n(4, Set:['h', 'e', 'l', 'o'])\n>>> z & 'mello', z | 'mello'\n(Set:['e', 'l', 'o'], Set:['h', 'e', 'l', 'o', 'm'])\nA solution to the multiple-operand extension subclass looks like the\nclass in Example B-26. It needs to replace only two methods in the\noriginal set. The class’s documentation string explains how it works:\nExample B-26. Part6/multiset.py\nfrom setwrapper import Set\nclass MultiSet(Set):\n    \"\"\"\n    Inherits all Set names, but extends intersect and union to support\n    multiple operands.  Note that \"self\" is still the first argument\n    (stored in the *args argument now).  Also note that the inherited\n    & and | operators call the new methods here with 2 arguments, but\n    processing more than 2 requires a method call, not an expression.\n    intersect doesn't remove duplicates here: the Set constructor does.\n    \"\"\"\n    def intersect(self, *others):\n        res = []\n        for x in self:                         # Scan first sequence\n            for other in others:               # For all other args\n                if x not in other: break       # Item in each one?\n            else:                              # No: break out of loop\n                res.append(x)                  # Yes: add item to end\n        return Set(res)\n    def union(*args):                          # self is args[0]\n        res = []\n        for seq in args:                       # For all args\n            for x in seq:                      # For all nodes",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1788,
      "chapter": null,
      "content": "if not x in res:\n                    res.append(x)              # Add new items to result\n        return Set(res)\nYour interaction with this extension will look something like the\nfollowing. Note that you can intersect by using & or calling intersect,\nbut you must call intersect for three or more operands; & is a binary\n(two-sided) operator. Also, note that we could have called MultiSet\nsimply Set to make this change more transparent if we used\nsetwrapper.Set to refer to the original within multiset (the as clause\nin an import could rename the class too if desired):\n>>> from multiset import *\n>>> x = MultiSet([1, 2, 3, 4])\n>>> y = MultiSet([3, 4, 5])\n>>> z = MultiSet([0, 1, 2])\n>>> x & y, x | y                               # Two operands\n(Set:[3, 4], Set:[1, 2, 3, 4, 5])\n>>> x.intersect(y, z)                          # Three operands\nSet:[]\n>>> x.union(y, z)\nSet:[1, 2, 3, 4, 5, 0]\n>>> x.intersect([1,2,3], [2,3,4], [1,2,3])     # Four operands\nSet:[2, 3]\n>>> x.union(range(10))                         # Non-MultiSets work, too\nSet:[1, 2, 3, 4, 0, 5, 6, 7, 8, 9]\n>>> w = MultiSet('soap')                       # String sets\n>>> w\nSet(['s', 'o', 'a', 'p'])\n>>> ''.join(w | 'super')\n'soapuer''\n>>> (w | 'super') & MultiSet('slots')\nSet(['s', 'o'])",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1789,
      "chapter": null,
      "content": "6. Class tree links: Example B-27 lists one way to change the lister class in\nExample 31-10, along with a rerun of the associated tester to show its\naugmented format. For full credit, do the same for the dir-based\nversion, and also do this when formatting class objects in the tree-\nclimber variant.\nTo import testmixin.py as a test, either copy it over from the Chapter 31\nexamples folder or add that folder to sys.path as we did earlier in\nPart IV’s solutions. It was copied here for variety:\nExample B-27. Part6/listinstance-mod.py\nclass ListInstance:\n    def __attrnames(self):\n        …unchanged…\n    def __str__(self):\n        return (f'<Instance of {self.__class__.__name__}'       # My class's \nname\n                f'({self.__supers()}), '                        # My class's \nsupers\n                f'address {id(self):#x}:'                       # My address \n(hex)\n                f'{self.__attrnames()}>')                       # name=value \nlist\n    def __supers(self):\n        names = []\n        for super in self.__class__.__bases__:            # One level up from \nclass\n            names.append(super.__name__)                  # name, not \nstr(super)\n        return ', '.join(names)\n        # Or: ', '.join(super.__name__ for super in self.__class__.__bases__)",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1790,
      "chapter": null,
      "content": "if __name__ == '__main__':\n    import testmixin                    # Assume testmixin.py copied to \".\"\n    testmixin.tester(ListInstance)      # Test class in this module\n$ python3 listinstance-mod.py\n<Instance of Sub(Super, ListInstance), address 0x10edc66c0:\n    data1='code'\n    data2='Python'\n    data3=3.12\n>\n7. Composition: A full-points solution is coded in Example B-28, with\ncomments from the description mixed in with the code. This is one case\nwhere it’s probably easier to express a problem in code than it is in\nnarrative:\nExample B-28. Part6/lunch.py\nclass Lunch:\n    def __init__(self):                          # Make/embed Customer, \nEmployee\n        self.cust = Customer()\n        self.empl = Employee()\n    def order(self, foodName):                   # Start Customer order \nsimulation\n        self.cust.placeOrder(foodName, self.empl)\n    def result(self):                            # Ask the Customer about its \nFood\n        self.cust.printFood()\nclass Customer:\n    def __init__(self):                          # Initialize my food to None\n        self.food = None\n    def placeOrder(self, foodName, employee):    # Place order with Employee\n        self.food = employee.takeOrder(foodName)\n    def printFood(self):                         # Print the name of my food\n        print(self.food.name)",
      "content_length": 1321,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1791,
      "chapter": null,
      "content": "class Employee:\n    def takeOrder(self, foodName):               # Return Food, with desired \nname\n        return Food(foodName)\nclass Food:\n    def __init__(self, name):                    # Store food name\n        self.name = name\nif __name__ == '__main__':\n    x = Lunch()                                  # Self-test code\n    x.order('burritos')                          # If run, not imported\n    x.result()\n    x.order('pizza')\n    x.result()\nWhen run, customers place orders and get food from employees. This\ncould be much more involved, but it suffices to demo the routing of\nmessages between objects that’s typical in OOP code:\n$ python3 lunch.py\nburritos\npizza\n8. Zoo animal hierarchy: Example B-29 shows one way to code the\ntaxonomy in Python; it’s artificial, but the general coding pattern\napplies to many real structures, from GUIs to employee databases to\nspacecraft. Notice that the self.speak call in Animal triggers an\nindependent inheritance search, which generally finds speak in a\nsubclass. Test this interactively by calling the reply method for\ninstances per the exercise description. Try extending this hierarchy with\nnew classes and making instances of various classes in the tree:\nExample B-29. Part6/zoo.py\nclass Animal:",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1792,
      "chapter": null,
      "content": "def reply(self):   self.speak()              # Back to subclass\n    def speak(self):   print('blah')             # Custom message\nclass Mammal(Animal):\n    def speak(self):   print('huh?')\nclass Cat(Mammal):\n    def speak(self):   print('meow')\nclass Dog(Mammal):\n    def speak(self):   print('bark')\nclass Primate(Mammal):\n    def speak(self):   print('Hello world!')\nclass Hacker(Primate): pass                      # Inherit from Primate",
      "content_length": 440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1793,
      "chapter": null,
      "content": "Part VII, Exceptions\nSee “Test Your Knowledge: Part VII Exercises” in Chapter 36 for the exercises.\n1. try/except: One possible coding of the oops function is listed in\nExample B-30. As for the noncoding questions, changing oops to raise\na KeyError instead of an IndexError means that the try handler won’t\ncatch the exception—it “percolates” to the top level and triggers\nPython’s default error message. The names KeyError and IndexError\ncome from the outermost built-in names scope (the B in “LEGB”).\nImport builtins and pass it as an argument to the dir function to see\nthis for yourself, per Chapter 17.\nExample B-30. Part7/oops.py\ndef oops():\n    raise IndexError()\ndef doomed():\n    try:\n        oops()\n    except IndexError:\n        print('caught an index error!')\n    else:\n        print('no error caught...')\nif __name__ == '__main__': doomed()\n$ python3 oops.py\ncaught an index error!\n2. Exception objects and lists: Example B-31 is one way to extend this\nmodule for an exception of its own:\nExample B-31. Part7/oops2.py\nclass MyError(Exception): pass",
      "content_length": 1061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1794,
      "chapter": null,
      "content": "def oops():\n    raise MyError('Hack!')\ndef doomed():\n    try:\n        oops()\n    except IndexError:\n        print('caught an index error!')\n    except MyError as exc:\n        print('caught error:', MyError, exc)\n    else:\n        print('no error caught...')\nif __name__ == '__main__':\n    doomed()\n$ python3 oops2.py\ncaught error: <class '__main__.MyError'> Hack!\nLike all class exceptions, the raised instance is accessible via the as\nvariable data; the error message shows both the class’s (<...>) and its\ninstance’s (Hack!) displays. The instance must be inheriting both an\n__init__ and a __repr__ or __str__ from Python’s Exception class,\nor it would print much as the class does. See Chapter 35 for details on\nhow these defaults work in built-in exception classes.\n3. Error handling: Example B-32 is one way to solve this exercise. It\ncodes tests in a file rather than interactively, but the results are similar\nenough for full credit. Notice that the empty except and sys.exc_info\napproach used here will catch exit-related exceptions that listing\nException with an as variable won’t; that’s probably not ideal in most\napplications code but might be useful in a tool like this designed to\nwork as a sort of exceptions firewall.\nExample B-32. Part7/exctools.py",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1795,
      "chapter": null,
      "content": "import sys, traceback\ndef safe(callee, *pargs, **kargs):\n    try:\n        callee(*pargs, **kargs)            # Catch everything else\n    except:                                # Or \"except Exception as E:\"\n        traceback.print_exc()\n        print(f'Got {sys.exc_info()[0]} {sys.exc_info()[1]}')\nif __name__ == '__main__':\n    import oops2\n    safe(oops2.oops)\n$ python3 exctools.py\nTraceback (most recent call last):\n  File \"/…/LP6E/AppendixB/Part7/exctools.py\", line 5, in safe\n    callee(*pargs, **kargs)            # Catch everything else\n    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/…/LP6E/AppendixB/Part7/oops2.py\", line 4, in oops\n    raise MyError('Hack!')\noops2.MyError: Hack!\nGot <class 'oops2.MyError'> Hack!\nBonus points: the sort of code in Example B-33 could turn this into a\nfunction decorator that could wrap and catch exceptions raised by any\nfunction, using techniques introduced briefly in Chapter 32, but covered\nmore fully in Chapter 39—it augments a function, rather than expecting\nit to be passed in explicitly, and produces similar output when run\n(there’s an extra call level, and filenames differ):\nExample B-33. Part7/exctools_deco.py\nimport sys, traceback\ndef safe(callee):\n    def callproxy(*pargs, **kargs):\n        try:",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1796,
      "chapter": null,
      "content": "return callee(*pargs, **kargs)\n        except Exception as E:\n            traceback.print_exc()\n            print(f'Got {E.__class__} {E}')\n    return callproxy\nif __name__ == '__main__':\n    import oops2\n    @safe\n    def test():                # test = safe(test)\n        oops2.oops()\n    test()\n4. Self-study examples: In closing, Examples B-34 through B-43 are 10\nexamples for you to study on your own. Their code and supporting files\nare in the Self-Study-Demos subfolder of the examples package’s\nAppendixB/Part7 folder. These require no extra installs as they use\nstandard-library tools, though tkinter is sketchy on phones (see\nAppendix A). For more examples, see follow-up books and resources\nfor the application domains you’ll be exploring next:\nExample B-34. Part7/Self-Study-Demos/largest-dir.py\n# Find the largest Python source file in a single directory\nimport os, glob\ndirname = '/Users/me/Downloads'    # Edit me to use (or use input() or \nsys.argv)\nallsizes = []\nallpy = glob.glob(dirname + os.sep + '*.py')\nfor filename in allpy:\n    filesize = os.path.getsize(filename)\n    allsizes.append((filesize, filename))",
      "content_length": 1130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1797,
      "chapter": null,
      "content": "allsizes.sort()\nprint(allsizes[:2])\nprint(allsizes[-2:])\nExample B-35. Part7/Self-Study-Demos/largest-tree.py\n# Find the largest Python source file in an entire directory tree\nimport sys, os, pprint\nif sys.platform[:3] == 'win':\n    dirname = r'C:\\Users\\me\\Downloads'    # Edit me to use\nelse:\n    dirname = '/Users/me/Downloads'\nallsizes = []\nfor (thisDir, subsHere, filesHere) in os.walk(dirname):\n    for filename in filesHere:\n        if filename.endswith('.py'):\n            fullname = os.path.join(thisDir, filename)\n            fullsize = os.path.getsize(fullname)\n            allsizes.append((fullsize, fullname))\nallsizes.sort()\npprint.pprint(allsizes[:2])\npprint.pprint(allsizes[-2:])\nExample B-36. Part7/Self-Study-Demos/largest-import.py\n# Find the largest Python source file on the module import search path\nimport sys, os, pprint\nvisited  = {}\nallsizes = []\nfor srcdir in sys.path:\n    for (thisDir, subsHere, filesHere) in os.walk(srcdir):\n        thisDir = os.path.normpath(thisDir)\n        if thisDir.upper() in visited:\n            continue",
      "content_length": 1058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1798,
      "chapter": null,
      "content": "else:\n            visited[thisDir.upper()] = True\n        for filename in filesHere:\n            if filename.endswith('.py'):\n                pypath  = os.path.join(thisDir, filename)\n                try:\n                    pysize = os.path.getsize(pypath)\n                except:\n                    print('skipping', pypath)\n                allsizes.append((pysize, pypath))\nallsizes.sort()\npprint.pprint(allsizes[:3])\npprint.pprint(allsizes[-3:])\nExample B-37. Part7/Self-Study-Demos/summer1.py\n# Sum columns in a text file separated by commas\nfilename = 'data.txt'    # Edit me for others\nsums = {}\nfor line in open(filename):\n    cols = line.split(',')\n    nums = [int(col) for col in cols]\n    for (ix, num) in enumerate(nums):\n        sums[ix] = sums.get(ix, 0) + num\nfor key in sorted(sums):\n    print(key, '=', sums[key])\nExample B-38. Part7/Self-Study-Demos/summer2.py\n# Similar to summer1, but using lists instead of dictionaries for sums\nimport sys\nfilename = sys.argv[1]            # \"python3 summer2.py data.txt 3\"\nnumcols  = int(sys.argv[2])",
      "content_length": 1057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1799,
      "chapter": null,
      "content": "totals   = [0] * numcols\nfor line in open(filename):\n    cols = line.split(',')\n    nums = [int(x) for x in cols]\n    totals = [(x + y) for (x, y) in zip(totals, nums)]\nprint(totals)\nExample B-39. Part7/Self-Study-Demos/regrtest.py\n# Simple test for regressions in the output of a set of scripts\nimport os\ntestscripts = [dict(script='test1.py', args=''),       # Edit me to use (or \nglob)\n               dict(script='test2.py', args='-opt')]   # Add encodings if \nneeded\nfor testcase in testscripts:\n    commandline = '%(script)s %(args)s' % testcase\n    output = os.popen(commandline).read()\n    result = testcase['script'] + '.result'\n    if not os.path.exists(result):\n        open(result, 'w').write(output)\n        print('Created:', result)\n    else:\n        priorresult = open(result).read()\n        if output != priorresult:\n            print('FAILED:', testcase['script'])\n            print(output)\n        else:\n            print('Passed:', testcase['script'])\nExample B-40. Part7/Self-Study-Demos/gui1.py\n\"\"\"\nBuild a GUI with tkinter having buttons that change color and grow.",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1800,
      "chapter": null,
      "content": "Caution: this GUI may grow until you close its window manually!\n\"\"\"\nfrom tkinter import *\nimport random\nfontsize = 25\ncolors = ['red', 'green', 'blue', 'yellow', 'orange', 'white', 'cyan', \n'purple']\ndef reply(text):\n    print(text)\n    popup = Toplevel()\n    color = random.choice(colors)\n    Label(popup, text='Popup', bg='black', fg=color).pack()\n    L.config(fg=color)\ndef cycle():\n    L.config(fg=random.choice(colors))\n    win.after(250, cycle)\ndef grow():\n    global fontsize\n    fontsize += 5\n    L.config(font=('arial', fontsize, 'italic'))\n    win.after(100, grow)\nwin = Tk()\nL = Label(win, text='Hack',\n          font=('arial', fontsize, 'italic'), fg='yellow', bg='navy',\n          relief=RAISED)\nL.pack(side=TOP, expand=YES, fill=BOTH)\nButton(win, text='popup', command=(lambda: reply('new'))).pack(side=BOTTOM, \nfill=X)\nButton(win, text='cycle', command=cycle).pack(side=BOTTOM, fill=X)\nButton(win, text='grow', command=grow).pack(side=BOTTOM, fill=X)\nwin.mainloop()",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1801,
      "chapter": null,
      "content": "Example B-41. Part7/Self-Study-Demos/gui2.py\n\"\"\"\nSimilar to gui1, but use classes so each window has own state info.\nCaution: this GUI may grow until you press Stop or kill its window!\n\"\"\"\nfrom tkinter import *\nimport random\nclass MyGui:\n    \"\"\"\n    A GUI with buttons that change color and make the label grow\n    \"\"\"\n    colors = ['blue', 'green', 'orange', 'red', 'brown', 'yellow']\n    def __init__(self, parent, title='popup'):\n        parent.title(title)\n        self.growing = False\n        self.fontsize = 10\n        self.lab = Label(parent, text='Hack2', fg='white', bg='navy')\n        self.lab.pack(expand=YES, fill=BOTH)\n        Button(parent, text='Hack', command=self.reply).pack(side=LEFT)\n        Button(parent, text='Grow', command=self.grow).pack(side=LEFT)\n        Button(parent, text='Stop', command=self.stop).pack(side=LEFT)\n    def reply(self):\n        \"change the button's color at random on Hack presses\"\n        self.fontsize += 5\n        color = random.choice(self.colors)\n        self.lab.config(bg=color,\n                font=('courier', self.fontsize, 'bold italic'))\n    def grow(self):\n        \"start making the label grow on Grow presses\"\n        self.growing = True",
      "content_length": 1198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1802,
      "chapter": null,
      "content": "self.grower()\n    def grower(self):\n        \"multiple presses schedule multiple growers\"\n        if self.growing:\n            self.fontsize += 5\n            self.lab.config(font=('courier', self.fontsize, 'bold'))\n            self.lab.after(500, self.grower)\n    def stop(self):\n        \"stop all button grower loops on Stop presses\"\n        self.growing = False\nclass MySubGui(MyGui):\n    colors = ['black', 'purple']           # Customize to change color \nchoices\nMyGui(Tk(), 'main')\nMyGui(Toplevel())\nMySubGui(Toplevel())\nmainloop()\nExample B-42. Part7/Self-Study-Demos/popmail.py\n\"\"\"\nPOP email inbox scanning and deletion utility.\nScan pop email box, fetching just headers, allowing\ndeletions without downloading the complete message.\n\"\"\"\nimport poplib, getpass, sys\nmailserver = 'your pop email server name here'   # Edit me: your \npop.server.net\nmailuser   = 'your pop email user name here'     # Edit me: your userid\nmailpasswd = getpass.getpass(f'Password for {mailserver}? ')",
      "content_length": 984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1803,
      "chapter": null,
      "content": "print('Connecting...')\nserver = poplib.POP3(mailserver)\nserver.user(mailuser)\nserver.pass_(mailpasswd)\ntry:\n    print(server.getwelcome())\n    msgCount, mboxSize = server.stat()\n    print('There are', msgCount, 'mail messages, size ', mboxSize)\n    msginfo = server.list()\n    print(msginfo)\n    for i in range(msgCount):\n        msgnum  = i+1\n        msgsize = msginfo[1][i].split()[1]\n        resp, hdrlines, octets = server.top(msgnum, 0)         # Get hdrs \nonly\n        print('-'*80)\n        print('[%d: octets=%d, size=%s]' % (msgnum, octets, msgsize))\n        for line in hdrlines: print(line)\n        if input('Print?') in ['y', 'Y']:\n            for line in server.retr(msgnum)[1]: print(line)    # Get whole \nmsg\n        if input('Delete?') in ['y', 'Y']:\n            print('deleting')\n            server.dele(msgnum)                                # Delete on \nsrvr\n        else:\n            print('skipping')\nfinally:\n    server.quit()                                  # Make sure we unlock mbox\ninput('Bye.')                                      # Keep window up on \nWindows\nExample B-43. Part7/Self-Study-Demos/sqldbase.py\n# Database script to populate and query an SQLite database, stored in",
      "content_length": 1206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1804,
      "chapter": null,
      "content": "people.db\nimport sqlite3, time\nconn = sqlite3.connect('people.db')    # Filename for database storage\ncurs = conn.cursor()                   # Submit SQL through cursor\n# Make+fill table if doesn't yet exist\ntbl = curs.execute('select name from sqlite_master where name = \\'people\\'')\nif tbl.fetchone() is None:\n    print('Making table anew')\n    curs.execute('create table people (name, job, pay)')\n    recs = [('Pat', 'mgr', 40000), ('Sue', 'dev', 60000), ('Bob', 'dev', \n50000)]\n    for rec in recs:\n        curs.execute('insert into people values (?, ?, ?)', rec)\n    conn.commit()\n# Show all rows\nprint('Rows:')\ncurs.execute('select * from people')\nfor row in curs.fetchall():\n    print(row)\n# Show just devs\nprint('Devs:')\ncurs.execute(\"select name, pay from people where job = 'dev'\")\ncolnames = [desc[0] for desc in curs.description]\nwhile row := curs.fetchone():\n    print('-' * 30)\n    for (name, value) in zip(colnames, row):\n        print(f'{name:<4} => {value}')\n# Update devs' pay: shown on next run\nsecs = int(time.time())  # UTC!\ncurs.execute('update people set pay = ? where job = ?', [secs, 'dev'])",
      "content_length": 1116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1805,
      "chapter": null,
      "content": "conn.commit()",
      "content_length": 13,
      "extraction_method": "OCR"
    },
    {
      "page_number": 1806,
      "chapter": null,
      "content": "Index\nSymbols\n# comments, # Comments\n$ character in interactive coding, What Not to Type: Prompts and Comments\n* (asterisk) use in code, Application to for loops\nA\nabsolute imports, Relative and Absolute Imports, Relative and Absolute Imports\nabstract superclasses, Abstract Superclasses-Preview: Abstract superclasses with\nlibrary tools, Stream Processors Revisited\nlibrary tools, Preview: Abstract superclasses with library tools\naccess-by-key files, Other File Tools\naggregation\ncomposition, OOP and Composition: “Has-a” Relationships\nobject, Other Ways to Combine Classes: Composites\nAI (artificial intelligence), And More: AI, Games, Images, QA, Excel, Apps…\nAjax (Asynchronous JavaScript and XML), Internet and Web Scripting\naliasing, Arguments and Shared References\n__all__ variable, Minimizing from * Damage: _X and __all__-Minimizing from\n* Damage: _X and __all__\nAndroid\ninteractive coding and, Starting an Interactive REPL",
      "content_length": 933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1807,
      "chapter": null,
      "content": "Python installation, Installing Python\nPython on, Using Python on Android-Using Python on Android\nannotations, functions, General Function Concepts, Function Annotations and\nDecorations-Function decorators alternative: Preview\nanonymous functions, lambda Makes Anonymous Functions-lambda Makes\nAnonymous Functions\nAOT (ahead-of-time) compilers, Ahead-of-Time Compilers for Speed\nPyThran, Python Implementation Alternatives\nShed Skin, Python Implementation Alternatives\narbitrary arguments, argument matching, Arbitrary Arguments Examples-Why\narbitrary arguments?\narbitrary expressions, Sequence Operations\narbitrary scope nesting, Arbitrary Scope Nesting\narchitecture, Python Program Architecture\n(see also program architecture)\nargument matching, Special Argument-Matching Modes-Argument Matching\nOverview\narbitrary arguments, Arbitrary Arguments Examples-Why arbitrary\narguments?\ndefaults, Argument Matching Syntax\nfunction calls, Argument Matching Syntax\nunpacking arguments, Calls: Unpacking arguments-Calls: Unpacking\narguments\nfunction definitions, Argument Matching Syntax\ncollecting arguments, Definitions: Collecting arguments-Definitions:",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1808,
      "chapter": null,
      "content": "Collecting arguments\ngeneralized set functions example, Example: Generalized Set Functions-\nTesting the Code\nkeyword-only arguments, Keyword-Only Arguments-Why keyword-only\narguments?\nkeywords, Argument Matching Syntax, Keyword and Default Examples-\nCombining keywords and defaults, Test Your Knowledge: Answers\nminimum value calculation, Example: The min Wakeup Call-The Punch\nLine\nmins.py example, Example: The min Wakeup Call-The Punch Line\npassing arguments, Argument Passing Details-Argument Passing Details\npositional-only arguments, Positional-Only Arguments-Positional-Only\nArguments\nprint function example, Example: Rolling Your Own Print-Using Keyword-\nOnly Arguments\nsyntax, Argument Matching Syntax-Argument Matching Syntax, lambda\nBasics\nargument ordering\nfunction calls, Calls Ordering\nboundary cases, Boundary cases\nformal definitions, Formal definition\nfunction definitions, Definition Ordering\nboundary cases, Boundary cases\nformal definitions, Formal definition\nargument passing, Argument-Passing Basics",
      "content_length": 1021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1809,
      "chapter": null,
      "content": "defaults, Keyword and Default Examples-Combining keywords and\ndefaults\nfunction calls, Argument Passing Details\nfunction definitions, Argument Passing Details\nimmutable arguments, Argument-Passing Basics\nkeywords, Keyword and Default Examples-Combining keywords and\ndefaults\nmatching arguments, Argument Passing Details-Argument Passing Details\nmultiple results simulation, Simulating Output Parameters and Multiple\nResults\nmutable arguments, Argument-Passing Basics\nchanges, Avoiding Mutable Argument Changes-Avoiding Mutable\nArgument Changes\noutput parameter simulation, Simulating Output Parameters and Multiple\nResults\nreferences, shared, Arguments and Shared References-Arguments and\nShared References\narguments, def Statements, Part IV, Functions and Generators\narbitrary, Part IV, Functions and Generators\ndecorators, Decorator Arguments, Adding Decorator Arguments-Adding\nDecorator Arguments\ncallables, Decorator Arguments\nclass decorators, Decorator arguments\ndescriptors, Descriptor method arguments-Descriptor method arguments\nimmutable, Argument-Passing Basics",
      "content_length": 1072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1810,
      "chapter": null,
      "content": "keyword-only, Using Keyword-Only Arguments\nmutable, Argument-Passing Basics\ncoupling, Function Design Concepts\npassing, General Function Concepts\npassing functions as, The First-Class Object Model\npositional, A Basic Range-Testing Decorator for Positional Arguments-A\nBasic Range-Testing Decorator for Positional Arguments\nscopes, Scopes and Argument Defaults-Loops Require Defaults, Not\nScopes\nsuper function, Same argument lists-Same argument lists\nvalidation, Example: Validating Function Arguments-The Goal\nArithmeticError exception class, Built-in Exception Classes\narrays, associative, Dictionaries\nartificial intelligence (AI), And More: AI, Games, Images, QA, Excel, Apps…\nas clause, The as Extension for import and from-The as Extension for import and\nfrom\nASCII, Character Representations-Character Representations\nLatin-1 and, Character Encodings\nUTF-8 and, Character Encodings\naspect-oriented programming, Why Decorators?\nassert statement\nas conditional raise statement, The assert Statement\nconstraint trapping, Example: Trapping Constraints (but Not Errors!)\nassignments",
      "content_length": 1084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1811,
      "chapter": null,
      "content": "argument passing, Argument-Passing Basics-Simulating Output Parameters\nand Multiple Results\naugmented, Assignment Syntax Forms, Augmented Assignments-\nAugmented assignment and shared references\nbasic assignment, Basic Assignments-Basic Assignments\nclass attributes, General Syntax and Usage\nextended-unpacking, Assignment Syntax Forms, Extended-Unpacking\nAssignments-Extended unpacking in action\nboundary cases, Boundary cases-Boundary cases\nfor loops, Application to for loops\nfunction calls, Application to for loops\nfunction headers, Application to for loops\nimplicit, Assignments\nfrom, Changing mutables in modules\nimport, Changing mutables in modules\nlists, Assignment Syntax Forms\nmodule attribute creation, How Files Generate Namespaces\nmultiple-targets, Assignment Syntax Forms, Multiple-Target Assignments\nshared references, Multiple-target assignment and shared references-\nMultiple-target assignment and shared references\nmutables, Common Coding Gotchas\nname references, nested scopes, Nested Scopes Overview\nnamed assignment expression, Assignment Syntax Forms, Named\nAssignment Expressions-When to use named assignment\nnames, Assignments, Scopes Overview, Name Resolution: The LEGB Rule",
      "content_length": 1199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1812,
      "chapter": null,
      "content": "namespaces, The “Zen” of Namespaces: Assignments Classify Names-The\n“Zen” of Namespaces: Assignments Classify Names\nobject references, Assignments\nscopes, Preview: Other Python scopes\nsequence assignments, Sequence Assignments-Advanced sequence-\nassignment patterns\nsequences, Assignment Syntax Forms\n__setitem__ method, Intercepting Item Assignments\nstatements, Python’s Statements, Assignment Syntax Forms\nsyntax, Application to for loops\ntuples, Assignment Syntax Forms\nassociative arrays, Dictionaries\nasync function, Asynchronous Functions: The Short Story\nconcurrent tasks\nasync def, Running concurrent tasks with “await” and “async def”-\nRunning concurrent tasks with “await” and “async def”\nasync for, Running concurrent tasks with “async with” and “async\nfor”-Running concurrent tasks with “async with” and “async for”\nasync with, Running concurrent tasks with “async with” and “async\nfor”-Running concurrent tasks with “async with” and “async for”\nas_completed, Running concurrent tasks with “as_completed” and\n“gather” -Running concurrent tasks with “as_completed” and “gather”\nawait, Running concurrent tasks with “await” and “async def”-\nRunning concurrent tasks with “await” and “async def”\ngather, Running concurrent tasks with “as_completed” and “gather” -\nRunning concurrent tasks with “as_completed” and “gather”",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1813,
      "chapter": null,
      "content": "I/O (input/output) operations, Asynchronous Functions: The Short Story\nmultitasking, Async Basics\nserial tasks, Running serial tasks with normal blocking calls\ntasks to avoid, How not to use async functions-How not to use async\nfunctions\nasync statement, Advanced Function Tools\nattribute accessors\n__get__ method, Inserting Code to Run on Attribute Access\n__getattr__ method, Inserting Code to Run on Attribute Access\n__getattribute__ method, Inserting Code to Run on Attribute Access\n__set__ method, Inserting Code to Run on Attribute Access\n__setattr__ method, Inserting Code to Run on Attribute Access\nattribute fetches, Method Call Syntax, Attribute-fetch algorithm\ncomputing value, Computed Attributes\n__getattr__ method, __getattr__ and __getattribute__-Using\n__getattribute__\n__getattribute__ method, Inserting Code to Run on Attribute Access,\n__getattr__ and __getattribute__-Using __getattribute__\ninheritance, Classes: Under the Hood\nloop avoidance, Avoiding loops in attribute interception methods\nproperties, The Basics\nattribute interception, Managed Attributes\ndescriptors, __getattribute__ and Descriptors: Attribute Implementations\nattribute management, Why Manage Attributes?",
      "content_length": 1193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1814,
      "chapter": null,
      "content": "access, Inserting Code to Run on Attribute Access-Inserting Code to Run\non Attribute Access\nbuilt-in attributes, intercepting, Intercepting Built-in Operation Attributes-\nRevisiting Chapter 28’s delegation example\ncomputed attributes, Computed Attributes\ndescriptors, Descriptors, Management Techniques Compared\ncomputed attributes, Computed Attributes\nmethod arguments, Descriptor method arguments-Descriptor method\narguments\nproperties and, How Properties and Descriptors Relate-Descriptors and\nslots and more\nread-only, Read-only descriptors-Read-only descriptors\nstate information, Using State Information in Descriptors-Using State\nInformation in Descriptors\nvalidation and, Using Descriptors to Validate-Option 2: Validating\nwith per-client-instance state (correctly)\n__getattr__ method, __getattr__ and __getattribute__-Management\nTechniques Compared\nvalidation, Using __getattr__ to Validate-Using __getattr__ to Validate\n__getattribute__ method, __getattr__ and __getattribute__-Management\nTechniques Compared\nvalidation, Using __getattribute__ to Validate-Using __getattribute__\nto Validate\nproperties, Properties, Management Techniques Compared\ndecorators, Coding Properties with Decorators-Setter and deleter\ndecorators",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1815,
      "chapter": null,
      "content": "validation and, Using Properties to Validate-Testing code\nvalidation\ndescriptors and, Using Descriptors to Validate-Option 2: Validating\nwith per-client-instance state (correctly)\nproperties and, Using Properties to Validate-Testing code\nattribute trees, Attribute Tree Construction\nattributes, Imports and Attributes-Imports and Attributes\nassignment, Attribute Assignment and Deletion-Attribute Assignment and\nDeletion, Python Inheritance Algorithm: The Less Simple Version\nassignments, Inserting Code to Run on Attribute Access\nchanging, Changing Class Attributes Can Have Side Effects-Changing\nClass Attributes Can Have Side Effects\nclass statement, Example: Class Attributes\nassignment-rule exception, Example: Class Attributes\ndefs, Example: Class Attributes\nvisibility, Example: Class Attributes\nclasses, Instance Versus Class Attributes-Instance Versus Class Attributes\ncomputing, Computed Attributes-Using __getattribute__\nconflict resolution, Attribute Conflict Resolution-Attribute Conflict\nResolution\ndeleting, Attribute Assignment and Deletion-Attribute Assignment and\nDeletion\ndescriptors, __getattribute__ and Descriptors: Attribute Implementations,\nComputed Attributes\nfunctions, General Function Concepts",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1816,
      "chapter": null,
      "content": "__getattr__ method, Attribute Reference\n__getattribute__ method, __getattribute__ and Descriptors: Attribute\nImplementations\ninstance objects, Coding Constructors\ninstances, Instance Versus Class Attributes-Instance Versus Class Attributes\nintrospection tools, Special Class Attributes-Special Class Attributes\nlisting, The dir Function\nname strings, The dir Function\ntype names, The dir Function\nlisting, mix-in classes, Example: “Mix-in” Attribute Listers\nclass trees, Listing attributes per object in class trees-Listing attributes\nper object in class trees\n__dict__, Listing instance attributes with __dict__-Listing instance\nattributes with __dict__\n__dir__, Listing inherited attributes with dir-Listing inherited\nattributes with dir\n__main__, Dual-Usage Modes: __name__ and __main__-Example: Unit\nTests with __name__\nmetaclasses, Inheritance: The Finale\nmodules, Creating Modules\nclasses, Classes Are Attributes in Modules-Classes Are Attributes in\nModules\ncreating, How Files Generate Namespaces\nnames, qualification, Attribute Name Qualification-Attribute Name\nQualification",
      "content_length": 1083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1817,
      "chapter": null,
      "content": "mutable, Changing Mutable Class Attributes Can Have Side Effects, Too-\nChanging Mutable Class Attributes Can Have Side Effects, Too\n__name__, Dual-Usage Modes: __name__ and __main__-Example: Unit\nTests with __name__\nnames, namespaces, Attribute Names: Object Namespaces-Attribute\nNames: Object Namespaces\nnested modules, Using the basic package\nprivate, class decorators, Implementing Private Attributes-Using __dict__\nand __slots__ (and other virtuals)\nproperties, Properties: Attribute Accessors\nassignment support, Property basics\npseudoprivate, Pseudoprivate Class Attributes-Why Use Pseudoprivate\nAttributes?\npublic, class decorators, Generalizing for Public Declarations-Workaround:\nGenerating operator-overloading descriptors\nreference interception, Attribute Reference\n__self__, Why Use Pseudoprivate Attributes?\n__setattr__ method, Attribute Assignment and Deletion-Attribute\nAssignment and Deletion\nslots, Classes: Under the Hood, Slots: Attribute Declarations\n__slots__, Multiple __slot__ lists in superclasses\nuser-defined, Function Attributes\naugmented assignments, Assignment Syntax Forms, Augmented Assignments-\nAugmented assignment and shared references\nawait statement, Advanced Function Tools",
      "content_length": 1210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1818,
      "chapter": null,
      "content": "B\nbackslash characters, Test Your Knowledge: Part III Exercises, Part III,\nStatements and Syntax\nstrings, Escape Sequences Are Special Characters-Escape Sequences Are\nSpecial Characters\nbackslash escape sequences, Other Ways to Code Strings\nf-string literal, F-string formatting basics\nBaseException exception class, Built-in Exception Classes\n__bases__ attribute, Classes: Under the Hood, Special Class Attributes,\nNamespace Dictionaries: Review, Multiple Inheritance and the MRO, The\nInheritance Bifurcation, Metaclass Inheritance\nbasic assignment, Basic Assignments-Basic Assignments\nBeautiful Soup, Internet and Web Scripting\nBeeWare Toga, GUIs and UIs\nbenchmarking, Benchmarking with Homegrown Tools\niteration, Iteration Results-For more good times: Function calls and map\nfunction calls, For more good times: Function calls and map-For more\ngood times: Function calls and map\nmap function, For more good times: Function calls and map-For more\ngood times: Function calls and map\ntimeit module, Benchmarking with Python’s timeit-Conclusion: Comparing\ntools\ntiming module\nexample, Timer Module: Take 1-Timer Module: Take 2\ntest functions, Timing Runner and Script-Timing Runner and Script",
      "content_length": 1191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1819,
      "chapter": null,
      "content": "binary data, Unicode and Byte Strings, Introducing Python String Tools\nstruct module, The struct Binary-Data Module\nbinary files, Text and Binary Files: The Short Story-Text and Binary Files: The\nShort Story, Text and Binary Files-Text and Binary Files\nbinary-mode files, Using Text and Binary Files, Text and Binary Modes\nbytes object, Text and Binary Modes\nblank lines, Common Coding Gotchas\nblock strings, Triple Quotes and Multiline Strings-Triple Quotes and Multiline\nStrings\nblocks, statements, Block Delimiters: Indentation Rules-Avoid mixing tabs and\nspaces\nBOM (byte order marker), Character Encodings\nadding, Making BOMs in Python-Making BOMs in Python\ntext editors, Making BOMs in Text Editors-Making BOMs in Text Editors\ntext-mode files, Using Text and Binary Files\n__bool__ method, Boolean Tests: __bool__ and __len__\nBoolean type, The bool type\nBoolean values, Boolean Tests: __bool__ and __len__\nBooleans, Booleans and None, Test Your Knowledge: Answers-Test Your\nKnowledge: Answers\nBoost.Python, Component Integration\nbound methods, Method Objects: Bound or Not, Why the Special Methods?\ncallback functions, Function Interfaces and Callback-Based Code\nbounds, Bounds Checking",
      "content_length": 1191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1820,
      "chapter": null,
      "content": "break statements, break, continue, pass, and the Loop else\ninteractive loops, A Simple Interactive Loop\nbreaks, Test Your Knowledge: Part I Exercises, Part I, Getting Started\nbuffering\nfiles, Using Files\noutput, Opening Files\nbuilt-in docstrings, Built-in docstrings-Built-in docstrings\nbuilt-in exception classes, Built-in Exception Classes\nArithmeticError, Built-in Exception Classes\nBaseException, Built-in Exception Classes\nException, Built-in Exception Classes\nLookupError, Built-in Exception Classes\nOSError, Built-in Exception Classes\nbuilt-in functions\nhandling by assignment, Hiding built-ins by assignment\nsuper, Augmenting Methods: The Good Way\nbuilt-in modules, module search path and, The Module Search Path\nbuilt-in objects, Why Use Built-in Objects?-Why Use Built-in Objects?\ncollections, Why Use Built-in Objects?\ndictionaries, Why Use Built-in Objects?\nlists, Why Use Built-in Objects?\nsearch tables, Why Use Built-in Objects?\nbuilt-in operations",
      "content_length": 963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1821,
      "chapter": null,
      "content": "delegating, Other Ways to Combine Classes: Composites, Delegating Built-\nIn Operations\noperator overloading methods, Workaround: Coding operator-\noverloading methods inline-Workaround: Generating operator-\noverloading descriptors\nbuilt-in tools, It’s Powerful, The Python Toolset\nbuilt-in types, Extending Built-in Object Types\nclass creation, Some Instances Are More Equal Than Others\nclasses, The Python Object Model\ncyclic data structures, Beware of Cyclic Data Structures\nextending\nembedding, Extending Types by Embedding-Extending Types by\nEmbedding\nsubclassing, Extending Types by Subclassing-Extending Types by\nSubclassing\nimmutable types, Immutable Types Can’t Be Changed in Place\nreferences, Assignment Creates References, Not Copies\nrepetition, Repetition Adds One Level Deep\nbuilt-ins, The Built-in Scope\ngenerators, Generation in built-ins and classes-Generation in built-ins and\nclasses\nLEGB rule, Redefining built-in names: For better or worse-Redefining\nbuilt-in names: For better or worse\nbyte files, Unicode and Byte Files-Unicode and Byte Files\nbyte strings, Using Byte Strings",
      "content_length": 1095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1822,
      "chapter": null,
      "content": "bytearray object, The bytearray Object-The bytearray Object\nbytes constructor, Other Ways to Make Bytes\nformatting, Formatting\nmethods, Methods\nmixing types, Mixing String Types\nsequence operations, Sequence Operations-Formatting\nbytearray object, The bytearray Object, The bytearray Object-The bytearray\nObject\nbytecode, OK, but What’s the Downside?, Bytecode compilation, Development\nimplications, Test Your Knowledge: Answers, Step 2: Compile It (Maybe)\nexecuting, Step 3: Run It\nfiles, Bytecode compilation\nhash-based files, Step 2: Compile It (Maybe)\nimport optimization, Bytecode compilation\ninteractive prompt code, Bytecode compilation\nmagic number, Step 2: Compile It (Maybe)\nPyPy, OK, but What’s the Downside?\nPython versions, Bytecode compilation\nstartup-speed optimization, Bytecode compilation\ntimestamp-based files, Step 2: Compile It (Maybe)\nbytes object, Unicode and Byte Strings, The bytes Object\nbinary-mode files, Text and Binary Modes\nC",
      "content_length": 956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1823,
      "chapter": null,
      "content": "call expressions, methods, Method Call Syntax\noperator overloading, Call Expressions: __call__-Function Interfaces and\nCallback-Based Code\n__call__ method, Call Expressions: __call__-Function Interfaces and Callback-\nBased Code\ncall proxies, Managing Calls and Instances\ncallback functions\nbound methods, Function Interfaces and Callback-Based Code\nlambda functions, Function Interfaces and Callback-Based Code\ncallbacks, Function Interfaces and Callback-Based Code-Function Interfaces and\nCallback-Based Code\nbound methods, Bound Methods in Action\ncalls\ndef statements, Calls\nfunctions, Calls\nCFFI, Component Integration\nchaining exceptions, Exception Chaining: raise from-Exception Chaining: raise\nfrom\nChaquopy, Component Integration\ncharacter code conversion, Character-code conversions\nCinder, Python Implementation Alternatives\ncircular imports (see recursion)\ncircular references, garbage collection, Objects Are Garbage-Collected\n__class__ attribute, Classes: Under the Hood, Special Class Attributes, How the\nMRO Works, Classes Are Types Are Classes, A “magic” proxy, Metaclass",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1824,
      "chapter": null,
      "content": "Versus Superclass\nclass decorators, Decorators and Metaclasses, A First Look at Class Decorators\nand Metaclasses-A First Look at Class Decorators and Metaclasses, What’s a\nDecorator?, Class Decorator Basics, Adding methods to classes\narguments, Decorator arguments\nbuilt-in types, Applying class decorators to built-in types\nclass managers, Managing Functions and Classes\nimplementation, Implementation-Implementation\ninstances, multiple, Supporting multiple instances-Supporting multiple\ninstances, Class Pitfall: Retaining Multiple Instances-Class Pitfall:\nRetaining Multiple Instances\ninterface proxies, Managing Calls and Instances\nmetaclasses comparison, Metaclasses Versus Class Decorators: Round 1-\nMetaclasses Versus Class Decorators: Round 1\nobject interfaces, Tracing Object Interfaces\nprivate attributes, Implementing Private Attributes-Using __dict__ and\n__slots__ (and other virtuals)\npublic attributes, Generalizing for Public Declarations-Workaround:\nGenerating operator-overloading descriptors\nsingleton classes, Singleton Classes-Applying class decorators to built-in\ntypes\nstate, enclosing scopes and, State retention and enclosing scopes\nclass managers, Managing Functions and Classes\nclass statement, User-Defined Objects, OOP: The Big Picture, The class\nStatement\nattributes, Example: Class Attributes",
      "content_length": 1322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1825,
      "chapter": null,
      "content": "assignment-rule exception, Example: Class Attributes\ndefs, Example: Class Attributes\nvisibility, Example: Class Attributes\nsyntax, General Syntax and Usage-General Syntax and Usage\nclass trees\nattribute listings, Listing attributes per object in class trees-Listing attributes\nper object in class trees\ninheritance, Python Inheritance Algorithm: The Simple Version\nOOP, Coding Class Trees-Coding Class Trees, Part VI, Classes and OOP\nclass-based exceptions, Catching Too Little: Use Class-Based Categories\nclasses, OOP: The Big Picture\nattributes, Instance Versus Class Attributes-Instance Versus Class Attributes\nchanging, Changing Class Attributes Can Have Side Effects-Changing\nClass Attributes Can Have Side Effects\ndescriptors, A First Example\nintrospection tools, Special Class Attributes-Special Class Attributes\nmutable, Changing Mutable Class Attributes Can Have Side Effects,\nToo-Changing Mutable Class Attributes Can Have Side Effects, Too\npseudoprivate, Pseudoprivate Class Attributes-Why Use Pseudoprivate\nAttributes?\nslots, Classes: Under the Hood\n__bases__ attribute, Classes: Under the Hood, Special Class Attributes,\nNamespace Dictionaries: Review, Multiple Inheritance and the MRO, The\nInheritance Bifurcation, Metaclass Inheritance\nbuilt-in operation override, Classes Can Intercept Python Operators",
      "content_length": 1318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1826,
      "chapter": null,
      "content": "built-in types, The Python Object Model\n__class__ attribute, Classes: Under the Hood, Special Class Attributes,\nHow the MRO Works, Classes Are Types Are Classes, A “magic” proxy,\nMetaclass Versus Superclass\ncomposites, Other Ways to Combine Classes: Composites-Other Ways to\nCombine Classes: Composites\ncomposition, Why Use Classes?\nconflation, The Roles of __init__.py Files\ndelegation-based, Miscellaneous Class Gotchas\ndescriptors, Descriptors\nversus dictionaries, Records Revisited: Classes Versus Dictionaries-Records\nRevisited: Classes Versus Dictionaries\ndocstrings, User-defined docstrings, Documentation Strings Revisited-\nDocumentation Strings Revisited\nexception classes, Exception Objects\nextending, Why Use Classes?\ngenerators, Generation in built-ins and classes-Generation in built-ins and\nclasses\ninheritance, OOP: The Big Picture, Why Use Classes?, Inheritance, Some\nInstances Are More Equal Than Others\nattribute trees, Attribute Tree Construction\ninherited methods, Specializing Inherited Methods-Specializing\nInherited Methods\nmetaclass subclasses and, The Inheritance Bifurcation\nMRO (method resolution order), Universal deployment\nmultiple inheritance, Multiple Inheritance and the MRO-Example:",
      "content_length": 1216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1827,
      "chapter": null,
      "content": "Mapping Attributes to Inheritance Sources\nprivacy, Inheritance Fine Print\nsubverting, Augmenting Methods: The Good Way\ninstances, Why Use Classes?\nattributes, State with class-instance attributes\ncreating, Some Instances Are More Equal Than Others\nmetaclasses (see metaclasses)\nmethod objects, Method Objects: Bound or Not-Bound Methods in Action\nmethods, Step 2: Adding Behavior Methods, Static and Class Methods,\nUsing Static and Class Methods\nadding, Adding methods to classes-Adding methods to classes\ndouble underscores, Classes Can Intercept Python Operators\nencapsulation, Coding Methods\ninstance counting, Counting Instances with Class Methods-Counting\ninstances per class with class methods\nmetaclass comparison, Metaclass Methods Versus Class Methods\nmethod calls, Methods-Other Method-Call Possibilities\nmodule attributes, Classes Are Attributes in Modules-Classes Are\nAttributes in Modules\n__mro__ attribute, How the MRO Works, Example: Mapping Attributes to\nInheritance Sources, The Inheritance Bifurcation, Attribute-fetch algorithm,\nMetaclass Versus Superclass\nnamespaces\nhierarchies, Why Use Classes?\ninheritance, Example: Class Attributes",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1828,
      "chapter": null,
      "content": "nested\ndescriptors, A First Example\nscope, Nested Classes: The LEGB Scopes Rule Revisited-Nested\nClasses: The LEGB Scopes Rule Revisited\nobject factories, Classes Are Objects: Generic Object Factories-Why\nFactories?\nobjects, Classes Generate Multiple Instance Objects\ndefault behavior, Class Objects Provide Default Behavior\nOOP, Attribute Inheritance Search, Step 1: Making Instances\nconstructors, Coding Constructors-Coding Constructors\ninheritance, Classes Are Customized by Inheritance-A Second\nExample\nintercepting operators, Classes Can Intercept Python Operators-Other\noperator-overloading methods\nmethods, Step 2: Adding Behavior Methods-Coding Methods\nmethods, augmenting, Augmenting Methods: The Bad Way-\nAugmenting Methods: The Good Way\nnamespaces, Classes and Instances\noperator overloading, Step 3: Operator Overloading-Providing Print\nDisplays\npolymorphism, Polymorphism in Action-Polymorphism in Action\nsubclassing, Coding Subclasses-Augmenting Methods: The Good Way\nsuperclasses, Attribute Inheritance Search, Classes Are Customized by\nInheritance\ntesting, Testing as You Go-Testing as You Go",
      "content_length": 1108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1829,
      "chapter": null,
      "content": "persistence, Stream Processors Revisited\npolymorphism, Polymorphism and classes-Polymorphism and classes\nas program units, Why Use Classes?\nredundancy, OOP: The Big Picture, Coding Methods\nscopes, Scopes in Methods and Classes-Scopes in Methods and Classes\nstate retention, Classes: Changeable, Per-Call, OOP\nstorage, Miscellaneous Class Gotchas\nsubclasses, Classes Are Customized by Inheritance\nsuperclass constructors, Miscellaneous Class Gotchas\ntypes and, The Python Object Model\nuser-defined, Classes Are Types Are Classes-Classes Are Types Are\nClasses\nversus modules, Classes Versus Modules\nclassmethod function, Using Static and Class Methods\nclosure functions, Function Interfaces and Callback-Based Code\nclosures, Closures and Factory Functions-Closures and Factory Functions\ncode folders\ndedicated, Where to Run: Code Folders\ndirectory structure, Where to Run: Code Folders\nper-chapter subfolders, Where to Run: Code Folders\ncode obfuscation, list comprehensions and, When not to use list comprehensions:\nCode obfuscation\ncode points, Escape Sequences Are Special Characters\nUnicode, Character Representations",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1830,
      "chapter": null,
      "content": "escapes, Coding Unicode Strings in Python\nvalue, Coding Unicode Strings in Python\ncode reuse, Test Your Knowledge: Part VI Exercises\nexceptions, Exception Objects\nfunctions, Function Basics\nmodules, Why Use Modules?\nOOP, OOP Is About Code Reuse-Programming by customization\ncode strings, module imports, Running Code Strings\ncode wrapping, “Overwrapping-itis”\ncoding gotchas, Common Coding Gotchas-Common Coding Gotchas\ncollections, Why Use Built-in Objects?\nargument matching, Argument Matching Overview\ncollector modules, Part V, Modules and Packages\ncolons\ncompound statement headers, Common Coding Gotchas\nstatements, What Python Adds\ncolumns in code, Why Indentation Syntax?\ncommand line, Test Your Knowledge: Answers\nlaunchers, Command-line launchers\nprogram file running, Running Files with Command Lines-Running Files\nwith Command Lines\nprogram files, Program Files\nshell programs, Command-Line Usage Variations",
      "content_length": 919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1831,
      "chapter": null,
      "content": "tools, Systems Programming\ncommas\nexpressions, Test Your Knowledge: Part II Exercises\ntuples, Tuple syntax peculiarities: Commas and parentheses-Tuple syntax\npeculiarities: Commas and parentheses\ncomments, Running Code Interactively, A First Script\ninteractive coding, What Not to Type: Prompts and Comments\ncomparison operators, Comparisons: __lt__, __gt__, and Others\ncomparisons, Comparisons, Equality, and Truth-Dictionary comparisons\ncore object types, Comparisons, Equality, and Truth\ndictionaries, Dictionary comparisons\nlists, Basic List Operations\nmixed-type, Comparisons, Equality, and Truth\nrecursive, Comparisons, Equality, and Truth\nrelative magnitude, Comparisons, Equality, and Truth\ncompiled extensions, OK, but What’s the Downside?\ncompilers\nAOT (ahead-of-time), Ahead-of-Time Compilers for Speed\nPython-to-Wasm, WebAssembly for Browsers\ncompletion certificate, Encore: Print Your Own Completion Certificate!-Encore:\nPrint Your Own Completion Certificate!\ncomplex numbers, Numbers, Test Your Knowledge: Answers, Core Types\nReview and Summary\ncomponent integration, Why Do People Use Python?, Component Integration",
      "content_length": 1130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1832,
      "chapter": null,
      "content": "composites\ndelegation, Other Ways to Combine Classes: Composites\nnesting, Other Ways to Combine Classes: Composites\ncomposition\nabstract superclasses, Stream Processors Revisited\naggregation, OOP and Composition: “Has-a” Relationships\nhas-a relationships, OOP and Composition: “Has-a” Relationships-Stream\nProcessors Revisited\nOOP, OOP and Composition: “Has-a” Relationships-Stream Processors\nRevisited\nstream processors, Stream Processors Revisited\ncompound objects, Comparisons, Equality, and Truth\ncompound statements, Missing Keys: if Tests, What Python Adds, if and match\nSelections\nif statements, if Statements\nmultiple-choice selection, Multiple-Choice Selections-Handling larger\nactions\nmatch statements, match Statements\nattribute patterns, Advanced match Usage\nif statements, Match versus if live\ninstance patterns, Advanced match Usage\nliteral patterns, Advanced match Usage\nmapping patterns, Advanced match Usage\nnested patterns, Advanced match Usage",
      "content_length": 962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1833,
      "chapter": null,
      "content": "nesting, Basic match Usage\nparentheses, Advanced match Usage\nsequence patterns, Advanced match Usage\nsyntax, Python Syntax Revisited\nblocks, Block Delimiters: Indentation Rules-Avoid mixing tabs and\nspaces\ndelimiters, Statement Delimiters: Lines and Continuations\nindentation, Block Delimiters: Indentation Rules-Avoid mixing tabs\nand spaces\nspecial syntax, Special Syntax Cases in Action-Special Syntax Cases\nin Action\ncomprehensions, Part IV, Functions and Generators\nconverting tuples, Conversions, methods, and immutability\ndictionaries, Dictionary Comprehensions-Dictionary Comprehensions\ndictionary, Comprehensions versus type calls and generators, Part IV,\nFunctions and Generators\nfor loops, Changing Lists: range and Comprehensions-Changing Lists:\nrange and Comprehensions\ngenerators and, Comprehensions versus type calls and generators\nlist comprehensions, Comprehensions, List Comprehension Basics,\nComprehensions: The Final Act\nfiles, List Comprehensions and Files-List Comprehensions and Files\nif clause, Filter clauses: if\nnested for loops, Nested loops: for\nsequence operations, Conversions, methods, and immutability",
      "content_length": 1132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1834,
      "chapter": null,
      "content": "syntax, Comprehensions\nlists, List comprehensions and maps-List comprehensions and maps\nloop variables, Preview: Other Python scopes\nset, Comprehensions versus type calls and generators\nsyntax, Comprehensions and Generations\ntype calls, Comprehensions versus type calls and generators\nvariables, scopes and, Scopes and comprehension variables-Scopes and\ncomprehension variables\nconcatenation, String Object Basics\nlist literals, List-literal unpacking\nstrings, Single and Double Quotes Are the Same, Basic Operations\nconcept hierarchy, The Python Conceptual Hierarchy Revisited\nexpressions, The Python Conceptual Hierarchy Revisited\nstatements, The Python Conceptual Hierarchy Revisited\nconstraints\nassert statement, Example: Trapping Constraints (but Not Errors!)\nMicroPython, Python Implementation Alternatives\nconstructor convolution, Constructors and Expressions: __init__ and __sub__\nconstructors\nclasses, Coding Constructors-Coding Constructors\ncustomizing, Step 5: Customizing Constructors, Too-Step 5: Customizing\nConstructors, Too\n__init__ method, Operator Overloading\noperator overload, Constructors and Expressions: __init__ and __sub__",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1835,
      "chapter": null,
      "content": "superclass methods, Specializing Inherited Methods\nsuperclasses, Step 5: Customizing Constructors, Too, Miscellaneous Class\nGotchas\ncontainer object length, Test Your Knowledge: Answers\n__contains__ method, Membership: __contains__, __iter__, and __getitem__-\nMembership: __contains__, __iter__, and __getitem__\ncontext managers, File Context Managers\nwith statement\ncontext-management protocol, The Context-Management Protocol-\nThe Context-Management Protocol\nmultiple context managers, Multiple Context Managers-Multiple\nContext Managers\ncontinuation lines in interactive coding, What Not to Type: Prompts and\nComments\ncontinue statements, break, continue, pass, and the Loop else\ncontrol language, Is Python a “Scripting Language”?\ncontrol-nesting, exception handlers, Example: Control-Flow Nesting-Example:\nControl-Flow Nesting\nconversions\ntext files, Storing Objects with Conversions-Storing Objects with\nConversions\ntuples, Conversions, methods, and immutability-Conversions, methods, and\nimmutability\ncooperative method dispatch, Call-chain anchors-Call-chain anchors\ncopies\nbuild-in types, Assignment Creates References, Not Copies",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1836,
      "chapter": null,
      "content": "of objects, References Versus Copies-References Versus Copies\ncopy method, Missing Keys: if Tests\ncore object types, Python’s Core Object Types\ncoroutines, Advanced Function Tools\ncounter loops, range object, Counter Loops: range-Counter Loops: range\ncoupling, Function Design Concepts\nvariables, Function Design Concepts\ncoupling modules, Module Design Concepts\nCPython, Python Implementation Alternatives-Python Implementation\nAlternatives, Test Your Knowledge: Answers\nWebAssembly/Emscripten platform, WebAssembly for Browsers\ncross-file changes, Program Design: Minimize Cross-File Changes-Program\nDesign: Minimize Cross-File Changes\nCSV module, object storage, Storing Objects with Other Tools\ncurrent working directory (CWD), Opening Files\ncustomization by inheritance, Step 4: Customizing Behavior by Subclassing\nCWD (current working directory), Opening Files\ncycles, Test Your Knowledge: Part I Exercises, Part I, Getting Started\nrecursion, Cycles, paths, and stack limits\ncyclic data, Part I, Getting Started\nstructures, Beware of Cyclic Data Structures\nCython, OK, but What’s the Downside?, Component Integration, Numeric and\nScientific Programming, Python Implementation Alternatives, Test Your\nKnowledge: Answers",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1837,
      "chapter": null,
      "content": "D\ndata descriptors, The Basics\ninheritance, The assignment addendum\ndata hiding, The Roles of __init__.py Files, Data Hiding in Modules,\nPseudoprivate Class Attributes\n__all__ variable, Minimizing from * Damage: _X and __all__-Minimizing\nfrom * Damage: _X and __all__\n__dir__ function, Managing Attribute Access: __getattr__ and __dir__-\nManaging Attribute Access: __getattr__ and __dir__\n__getattr__ function, Managing Attribute Access: __getattr__ and __dir__-\nManaging Attribute Access: __getattr__ and __dir__\n_X prefix, Minimizing from * Damage: _X and __all__-Minimizing from *\nDamage: _X and __all__\ndatabases, Database Access\nAPI, Database Access\nbooks database example, Intermission: Books Database-Mapping values to\nkeys\nobject storage, Step 7 (Final): Storing Objects in a Database\npickle module, Pickles and Shelves\nshelf object updates, Updating Objects on a Shelf\nshelve database, Storing Objects on a shelve Database-Exploring\nShelves Interactively\nshelve module, Pickles and Shelves, The shelve module\ndebugging, Development Tools for Larger Projects, Part I, Getting Started\nexceptions and, Test Your Knowledge: Part I Exercises",
      "content_length": 1145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1838,
      "chapter": null,
      "content": "exercise, Test Your Knowledge: Part I Exercises-Test Your Knowledge: Part\nI Exercises\ntry statement, Debugging with Outer try Statements-Debugging with Outer\ntry Statements\ndecimals, Numbers, Test Your Knowledge: Answers, Types Share Operation\nSets by Categories\ndeclaration statements, The Case of the Missing Declaration Statements\ndecoding, Unicode, Character Encodings\ndecoration, What’s a Decorator?\nfunctions, Function Annotations and Decorations-Function decorators\nalternative: Preview\ndecorator timers, More Module Mods\ndecorators\narguments, Decorator Arguments, Adding Decorator Arguments-Adding\nDecorator Arguments\ncallables, Decorator Arguments\nfunction annotations, Decorator Arguments Versus Function\nAnnotations\ncall tracers, Tracing Function Calls-Tracing Function Calls\ncallables returning callables, Implementation\nclass decorators, Decorators and Metaclasses, A First Look at Class\nDecorators and Metaclasses-A First Look at Class Decorators and\nMetaclasses, What’s a Decorator?, Class Decorator Basics, Adding methods\nto classes\narguments, Decorator arguments\nbuilt-in types, Applying class decorators to built-in types",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1839,
      "chapter": null,
      "content": "class managers, Managing Functions and Classes\nimplementation, Implementation-Implementation\ninstances, multiple, Supporting multiple instances-Supporting multiple\ninstances, Class Pitfall: Retaining Multiple Instances-Class Pitfall:\nRetaining Multiple Instances\ninterface proxies, Managing Calls and Instances\nmetaclasses comparison, Metaclasses Versus Class Decorators: Round\n1-Metaclasses Versus Class Decorators: Round 1\nobject interfaces, Tracing Object Interfaces\nprivate attributes, Implementing Private Attributes-Using __dict__ and\n__slots__ (and other virtuals)\npublic attributes, Generalizing for Public Declarations-Workaround:\nGenerating operator-overloading descriptors\nsingleton classes, Singleton Classes-Applying class decorators to built-\nin types\nstate, enclosing scopes and, State retention and enclosing scopes\nclass management, Decorators Manage Functions and Classes, Too\nclass methods, Automatically decorating class methods-Automatically\ndecorating class methods\nfunction decorators, Function Decorator Basics-Function Decorator Basics,\nWhat’s a Decorator?\nargument validation, Example: Validating Function Arguments-\nDecorator Arguments Versus Function Annotations\ncall proxies, Managing Calls and Instances\nfunction call timing, Timing Function Calls-Timing Function Calls\nfunction managers, Managing Functions and Classes",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1840,
      "chapter": null,
      "content": "metafunctions, Usage\nruntime declarations, Usage\nsyntax, Usage\nusage, Function Decorator Basics\nuser-defined, A First Look at User-Defined Function Decorators-A\nFirst Look at User-Defined Function Decorators\nfunction management, Decorators Manage Functions and Classes, Too\nfunction validation\nkeywords, Generalizing for Keywords and Defaults-Generalizing for\nKeywords and Defaults\npositional arguments, A Basic Range-Testing Decorator for Positional\nArguments-A Basic Range-Testing Decorator for Positional\nArguments\nfunctions, General Function Concepts\nimplementation, Implementation-Implementation\nversus macros, Why Decorators?\nmethods, Supporting method decoration, Class Pitfall: Decorating Methods\ndescriptors, Using descriptors to decorate methods-Using descriptors\nto decorate methods\nnested functions, Using nested functions to decorate methods\nnesting, Decorator Nesting-Decorator Nesting, Decorator nesting\nproperties, Coding Properties with Decorators\ndeleters, Setter and deleter decorators\nsetters, Setter and deleter decorators\nrange-testing, A Basic Range-Testing Decorator for Positional Arguments-",
      "content_length": 1116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1841,
      "chapter": null,
      "content": "Decorator Arguments Versus Function Annotations\nstate retention\nclass-instance attributes, State with class-instance attributes\nenclosing-scope nonlocals, State with enclosing-scope nonlocals-State\nwith enclosing-scope nonlocals\nfunction attributes, State with function attributes-State with function\nattributes\nglobal variables and, State with global variables\nsyntax, Why Decorators?\ntimer decorator, Adding Decorator Arguments\nwrapper function objects, Decorator Nesting\ndef statement, Basic Function Tools, def Statements\nfunction calls, Calls\nname assignments, Name Resolution: The LEGB Rule\nnonlocal statement and, The nonlocal Statement-nonlocal Boundary Cases\nruntime execution, def Executes at Runtime\ndefault argument values, scopes, Scopes and Argument Defaults-Loops Require\nDefaults, Not Scopes\ndefinition, functions, Definition\n__del__ method, Object Destruction: __del__-Destructor Usage Notes\ndelegation, Other Ways to Combine Classes: Composites\nversus inheritance, Inheritance versus delegation\nlike-a relationships, OOP and Delegation: “Like-a” Relationships-OOP and\nDelegation: “Like-a” Relationships",
      "content_length": 1120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1842,
      "chapter": null,
      "content": "OOP, OOP and Delegation: “Like-a” Relationships-OOP and Delegation:\n“Like-a” Relationships\ndelimiter string, “Changing” Strings, Part 2: String Methods\ndeprecation policies, Variable Name Rules\ndescriptors, __getattribute__ and Descriptors: Attribute Implementations,\nDescriptors, Management Techniques Compared\narguments, Descriptor method arguments-Descriptor method arguments\nattributes, computed, Computed Attributes\nclass attributes, A First Example\nclasses, Descriptors\ndata descriptors, The Basics\ninheritance, The descriptors deviation\ndecorating methods, Using descriptors to decorate methods-Using\ndescriptors to decorate methods\nfiles, os module, Other File Tools\nhiding, The Basics\ninheritance, A First Example\ninheritance and, The Basics\nnested classes, A First Example\noperator overloading, Workaround: Generating operator-overloading\ndescriptors-Workaround: Generating operator-overloading descriptors\nproperties and, How Properties and Descriptors Relate-Descriptors and\nslots and more\nread-only, Read-only descriptors-Read-only descriptors\n__set__ method, The Basics",
      "content_length": 1083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1843,
      "chapter": null,
      "content": "slots, Descriptors and slots and more\nstate, Using State Information in Descriptors-Using State Information in\nDescriptors\ndesign\ncross-file changes, Program Design: Minimize Cross-File Changes-\nProgram Design: Minimize Cross-File Changes\nfunctions, Function Basics\nglobal variable minimization, Program Design: Minimize Global Variables-\nProgram Design: Minimize Global Variables\nmodules, Module Design Concepts\npatterns, factory functions, Closures and Factory Functions-Closures and\nFactory Functions\ndestructors, Object Destruction: __del__\ndeveloper productivity, Why Do People Use Python?, Developer Productivity\ndevelopment implications, Development implications\ndevelopment tools, Development Tools for Larger Projects\ndebuggers, Development Tools for Larger Projects\ndocumentation tools, Development Tools for Larger Projects\nerror-checking tools, Development Tools for Larger Projects\nIDEs, Development Tools for Larger Projects\ninstallation management, Development Tools for Larger Projects\noptimization, Development Tools for Larger Projects\nprofilers, Development Tools for Larger Projects\nshipping, Development Tools for Larger Projects",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1844,
      "chapter": null,
      "content": "testing tools, Development Tools for Larger Projects\nDFLR (depth first, then left to right), Multiple Inheritance and the MRO\ndiagonals, Example: List Comprehensions and Matrixes\ndiamonds, user-defined, How the MRO Works\n__dict__ attribute, How Files Generate Namespaces, Namespace Dictionaries:\n__dict__-Namespace Dictionaries: __dict__, Module Introspection, Example:\nListing Modules with __dict__-Example: Listing Modules with __dict__\ndictionaries, Why Use Built-in Objects?, Python’s Core Object Types,\nDictionaries, Test Your Knowledge: Answers, Types Share Operation Sets by\nCategories\nbackward indexing, Mapping values to keys\nbooks database example, Intermission: Books Database-Mapping values to\nkeys\nbuilding, Other Dictionary Makers-Dictionary-literal unpacking\ncharacteristics, Dictionaries-Dictionaries\nclasses versus dictionaries, Records Revisited: Classes Versus Dictionaries-\nRecords Revisited: Classes Versus Dictionaries\ncollections, Dictionaries\ncomparisons, Dictionary comparisons\ncomprehensions, Dictionary Comprehensions-Dictionary Comprehensions,\nComprehensions versus type calls and generators, Part IV, Functions and\nGenerators\nsyntax, Formal Comprehension Syntax\nconverting to tuples, Records Revisited: Named Tuples\ncopying, Missing Keys: if Tests\nempty, Dictionaries",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1845,
      "chapter": null,
      "content": "function annotations, Function Annotations and Decorations\nin operator, Basic Dictionary Operations\nindexing, Mapping Operations, Test Your Knowledge: Part II Exercises,\nPart II, Objects and Operations\ninitializing, Dictionary Comprehensions\niteration, Item Iteration: for Loops-Item Iteration: for Loops, Reprise:\nDictionaries, range, enumerate, and zip\nkey insertion order, Key Insertion Ordering-Key Insertion Ordering\nkey order, Dictionaries\nkeys, Dictionaries\nimmutable objects, Dictionary Usage Tips\ninteger keys, Using dictionaries to simulate flexible lists: Integer keys\nmapping to, Mapping values to keys-Mapping values to keys\nmissing, Avoiding missing-key errors\nsorting, Sorting dictionary keys-Sorting dictionary keys\nstorage, Basic Dictionary Operations\ntuple keys, Using dictionaries for sparse data structures: Tuple keys\nkeys method, Basic Dictionary Operations\nlen function, Basic Dictionary Operations\nlists, sorting, Sorting lists\nliteral expressions, Dictionaries\nliteral unpacking, Dictionary-literal unpacking\nliterals, Dictionaries\nmagnitude comparisons, Dictionary magnitude comparisons",
      "content_length": 1112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1846,
      "chapter": null,
      "content": "methods, Missing Keys: if Tests, More Dictionary Methods-More\nDictionary Methods\nmodule namespaces, How Files Generate Namespaces-Namespace\nDictionaries: __dict__\nmutability, Dictionaries, Dictionaries, Changing Dictionaries in Place-\nChanging Dictionaries in Place\nnamespaces, Classes: Under the Hood, Namespace Dictionaries: Review-\nNamespace Dictionaries: Review\nslots, Slot basics, Slots and namespace dictionaries-Slots and\nnamespace dictionaries\nnesting, Nesting Revisited-Nesting Revisited, Nesting in dictionaries\nnew indexes, Dictionary Usage Tips\nobject references, Dictionaries\noperations, Dictionaries\nmapping, Mapping Operations-Mapping Operations\nsequence operations, Dictionary Usage Tips\nsets, Dictionary views and sets-Dictionary views and sets\nsorting, Test Your Knowledge: Part III Exercises, Part III, Statements and\nSyntax\nunion operator, Dictionary “Union” Operator-Dictionary “Union” Operator\nupdating, Missing Keys: if Tests\nviews, Dictionary views and sets-Dictionary views and sets\nviews objects, Dictionary key/value/item view objects-Dictionary\nkey/value/item view objects\nzip object, More zip roles: dictionaries",
      "content_length": 1141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1847,
      "chapter": null,
      "content": "dictionary keys, Test Your Knowledge: Part II Exercises, Part II, Objects and\nOperations\nif statements, Missing Keys: if Tests-Missing Keys: if Tests\nindexing and, Mapping Operations\ninsertion order, Mapping Operations\ndictionary tools, Part IV, Functions and Generators\ndictionary-based formatting expressions, Dictionary-based formatting\nexpressions-Dictionary-based formatting expressions\ndir function, Getting Help\nobject attributes, The dir Function\nname strings, The dir Function\ntype names, The dir Function\ndirect recursion, Coding Alternatives\ndirectories\ncommands, Running Files with Command Lines\nmodule search path, Search-Path Components\n.pth path-file, Search-Path Components\nPYTHONPATH, Search-Path Components, Configuring the Search\nPath\nsite-packages, Search-Path Components\nstandard-library directories, Search-Path Components\ndirectory paths, Files in Action\nfilename, Opening Files\ndisabling code, triple-quoted strings, Triple Quotes and Multiline Strings",
      "content_length": 976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1848,
      "chapter": null,
      "content": "Django, GUIs and UIs\ndo until loop, Examples\ndocstrings, Triple Quotes and Multiline Strings, Python Documentation Sources,\nDocstrings and __doc__, Documentation Strings Revisited-Documentation\nStrings Revisited\n(see also documentation)\nbuilt-in, Built-in docstrings-Built-in docstrings\ncontent, User-defined docstrings\nindentation, User-defined docstrings\nliterals, User-defined docstrings\nobject classes, User-defined docstrings\nprint, User-defined docstrings\nraw strings, User-defined docstrings\nstandards, Docstring standards\ntriple-quoted block, User-defined docstrings\nuser-defined, User-defined docstrings-User-defined docstrings\ndocumentation, Test Your Knowledge: Part I Exercises, Python Documentation\nSources\n(see also docstrings; Pydoc)\n# comments, # Comments\ndir function, The dir Function-The dir Function\npriorities, Docstring standards\nSphinx, Beyond Docstrings: Sphinx\ndocumentation tools, Development Tools for Larger Projects",
      "content_length": 944,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1849,
      "chapter": null,
      "content": "dotted-path syntax, packages, Package Imports\nmodule import, Using the basic package\nrelative imports, Relative-Import Rationales and Trade-Offs\nDurus, Database Access\ndynamic typing, It’s Powerful\ndeclaration statements, The Case of the Missing Declaration Statements\nobjects, Variables, Objects, and References\npolymorphism and, Dynamic Typing Is Everywhere\nvariables, Variables, Objects, and References-Variables, Objects, and\nReferences\nE\nease of use, It’s Relatively Easy to Use\nelse clause, Loop else-Why the loop else?\nelse statements, break, continue, pass, and the Loop else\nembedding\nbuilt-in object types, Extending Types by Embedding-Extending Types by\nEmbedding\nembedded objects, Other Ways to Combine Classes: Composites\nempty data structures, The Meaning of True and False in Python\nempty dictionaries, Dictionaries\nempty strings, Files in Action\nencapsulation, Coding Methods, Python and OOP, Test Your Knowledge: Part\nVI Exercises\npackaging, Python and OOP",
      "content_length": 973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1850,
      "chapter": null,
      "content": "enclosing scopes, Implementation\nfunctions, Enclosing scopes and loop variables\nstate retention and, State retention and enclosing scopes\nencoding\nsource files, Source-File Encoding Declarations-Source-File Encoding\nDeclarations\nUnicode, Character Encodings\nUTF-8, Character Encodings\nend-of-line character, The print function in action\nenumerate function, loops, Offsets and Items: enumerate-Offsets and Items:\nenumerate\nequality, testing for, Comparisons, Equality, and Truth\nerror handling, Part VII, Exceptions\nexceptions, Exception Roles\ntesting user input, Handling Errors by Testing Inputs-Handling Errors by\nTesting Inputs\ntry statement, Handling Errors with try Statements-Handling Errors with try\nStatements\nerror messages, Displaying Errors and Tracebacks\nerror-checking tools, Development Tools for Larger Projects\nerrors, Part I, Getting Started\nexercise, Test Your Knowledge: Part I Exercises\nescape characters, Escape Sequences Are Special Characters-Escape Sequences\nAre Special Characters\nraw strings, Raw Strings Suppress Escapes-Raw Strings Suppress Escapes",
      "content_length": 1076,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1851,
      "chapter": null,
      "content": "eval function, String Conversion Tools\nevent notification, Exception Roles\nExcel, And More: AI, Games, Images, QA, Excel, Apps…, Other Launch\nOptions\nexception classes, Exception Objects\nbuilt-in, Built-in Exception Classes\nArithmeticError, Built-in Exception Classes\nBaseException, Built-in Exception Classes\nException, Built-in Exception Classes\nLookupError, Built-in Exception Classes\nOSError, Built-in Exception Classes\ncategories, catching, Coding Exceptions Classes\nException, Built-in Exception Classes\nexception details, Coding Exceptions Classes\nhierarchies, Exception Classes, Why Exception Hierarchies?-Why\nException Hierarchies?\nraising instances, Coding Exceptions Classes\nstate and, Exception Objects\nsuperclasses, Coding Exceptions Classes\nexception groups, Exception Groups: Yet Another Star!-Exception Groups: Yet\nAnother Star!\nnesting, Example: Control-Flow Nesting\nexception handlers, Test Your Knowledge: Part I Exercises\nnesting, Nesting Exception Handlers",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1852,
      "chapter": null,
      "content": "control-flow, Example: Control-Flow Nesting-Example: Control-Flow\nNesting\nsyntactic nesting, Example: Syntactic Nesting-Example: Syntactic\nNesting\nsys.exc_info, More on sys.exc_info-The sys.exception alternative—and diss\ntry/except/else, Functions Can Signal Conditions with raise-Functions Can\nSignal Conditions with raise\nexception superclass, Coding Exceptions Classes\nexceptions, Why Use Exceptions?\nas string objects, The raise Statement\nassert statement, The assert Statement-Example: Trapping Constraints (but\nNot Errors!)\n(see also assert statement)\nbuilt-in\ncatching, Example: Catching built-in exceptions\ncustom printing, Custom Print Displays-Custom Print Displays\ndefault printing, Default Printing and State-Default Printing and State\nstate, Default Printing and State-Default Printing and State, Custom\nState and Behavior-Providing Exception Methods\ncatching, Catching Exceptions-Catching Exceptions\ncategories, Built-in Exception Categories\nchaining, Exception Chaining: raise from-Exception Chaining: raise from\nclass instance objects, Exception Objects\nclass-based, Catching Too Little: Use Class-Based Categories\ncode reuse, Exception Objects",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1853,
      "chapter": null,
      "content": "connection closure, Closing Files and Server Connections\ncontrol flow, Exception Roles\ndebugging and, Test Your Knowledge: Part I Exercises\ndefault behaviors, Example: Default behavior\ndetails, Providing Exception Details\nerror checks, Test Your Knowledge: Answers\nerror handling, Exception Roles\nerror messages, Displaying Errors and Tracebacks\nevent notification, Exception Roles\nexcept clause, Catching Too Much: Avoid Empty except and Exception-\nCatching Too Much: Avoid Empty except and Exception\nexception handler, Default Exception Handler-Default Exception Handler\nfile closing, Closing Files and Server Connections\nhierarchies, Exception Classes, Why Exception Hierarchies?-Why\nException Hierarchies?\nmethods, Providing Exception Methods-Providing Exception Methods\nno exception case, Catching the no-exception case with else\npropagating, Propagating Exceptions with raise\nraise statement, The raise Statement-Exception Chaining: raise from,\nBreaking Out of Multiple Nested Loops: “go to”\n(see also raise statement)\nraising, Raising Exceptions-Raising Exceptions, Raising Exceptions\nscopes, Scopes and except as-Scopes and except as\nas signals, Exceptions Aren’t Always Errors-Exceptions Aren’t Always",
      "content_length": 1210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1854,
      "chapter": null,
      "content": "Errors\nspecial-case handling, Exception Roles\nsuperclasses, Exception Classes\ntermination, Exception Roles, Termination Actions-Termination Actions\ntermination handlers, The Termination-Handlers Shoot-Out-The\nTermination-Handlers Shoot-Out\ntests, in-process, Running In-Process Tests\ntraceback object, Displaying Errors and Tracebacks\ntry statement, The try Statement-Combined-clauses example\n(see also try statement)\ndebugging and, Debugging with Outer try Statements-Debugging with\nOuter try Statements\ntuples, Catching many exceptions with a tuple\nunexpected, Catching all exceptions with empties and Exception\nuser-defined, User-Defined Exceptions\nnonerror conditions, Functions Can Signal Conditions with raise\nvariables, scopes, Preview: Other Python scopes\nwith statement, The with Statement and Context Managers-The\nTermination-Handlers Shoot-Out\n(see also with statement)\nwrappers, What Should Be Wrapped\nexec function, The exec built-in-The exec built-in\nexecutable statements\nfrom, Imports Are Runtime Assignments",
      "content_length": 1024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1855,
      "chapter": null,
      "content": "import, Imports Are Runtime Assignments\nexecutables, standalone, Standalone Executables, Standalone Apps and\nExecutables-Etcetera\nexecution\ndeferring, Anonymous Functions: lambda\nspeed, OK, but What’s the Downside?\nexponentiation, Numbers\nexpression statements, Expression Statements-Expression Statements\nin-place changes, Expression Statements and In-Place Changes\nexpressions, Running Code Interactively, Test Your Knowledge: Answers\narbitrary, Sequence Operations, List Comprehensions Review\ncommas, Test Your Knowledge: Part II Exercises\nconcept hierarchy, The Python Conceptual Hierarchy Revisited\ncurly braces, Python’s Core Object Types\ndictionaries, Python’s Core Object Types\nfunction-related, Function Basics\ngenerator expressions, Comprehensions and Generations, Generator\nFunctions and Expressions, Generator Functions Versus Generator\nExpressions, Listing instance attributes with __dict__\nas iterables, Generator Expressions: Iterables Meet Comprehensions\nbest uses, Why generator expressions?\nversus filter function, Generator expressions versus filter\ngenerator objects, Generator Expressions: Iterables Meet\nComprehensions",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1856,
      "chapter": null,
      "content": "versus map function, Generator expressions versus map-Generator\nexpressions versus map\nslicing sequences, Generator expressions\nif/then/else, The if/else Ternary Expression-The if/else Ternary Expression\nindexing expressions, Sequence Operations\nslicing, Sequence Operations\nlambda, Basic Function Tools\nlist comprehensions, Comprehensions: The Final Act\nmapping, List Comprehensions Review\nlists, Python’s Core Object Types\nliteral expressions, Python’s Core Object Types\nmethod call expressions, Method Call Syntax\nnamed assignment expression, Assignment Syntax Forms, Named\nAssignment Expressions-When to use named assignment\nobjects, The Python Conceptual Hierarchy\noperator overload, Constructors and Expressions: __init__ and __sub__\nslicing expression, Extended slicing: The third limit and slice objects\nsquare brackets, Python’s Core Object Types\nstring formatting, String-Formatting Options-Formatting expression basics\nadvanced, Advanced formatting expression examples-Advanced\nformatting expression examples\ndictionary-based, Dictionary-based formatting expressions-Dictionary-\nbased formatting expressions\ntype codes, Formatting expression custom formats",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1857,
      "chapter": null,
      "content": "strings, String Object Basics\nextended slicing, Extended slicing: The third limit and slice objects\nextended-unpacking assignments, Extended-Unpacking Assignments-Extended\nunpacking in action\nboundary cases, Boundary cases-Boundary cases\nfor loops, Application to for loops, Extended-unpacking assignment in for\nloops-Extended-unpacking assignment in for loops\nextension modules, Other Kinds of Modules\nextensions, The Python Toolset\nF\nf-strings, The print function in action\nliterals, The F-String Formatting Literal\nbackslash, F-string formatting basics\ncustom formats, F-string custom formats\nquotes, F-string formatting basics\nREPL, F-string formatting basics\nsubstitutions, F-string formatting basics\nfactorials, Part IV, Functions and Generators\nfactory functions, Closures and Factory Functions-Closures and Factory\nFunctions, Classes Are Objects: Generic Object Factories-Why Factories?\nmetaclasses, Using simple factory functions\nfalse values, The Meaning of True and False in Python-The bool type\nFIFO (first in, first out), queues, Other File Tools\nfile extensions, imports, Common Coding Gotchas",
      "content_length": 1107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1858,
      "chapter": null,
      "content": "file icons, program running, Clicking and Tapping File Icons\nfile objects\ncreating, Files\nprint operations, Print Operations\nfile operations, Files\nfile-processing, Files in Action-Files in Action\nfilenames, Filenames in open and Other Filename Tools-Filenames in open and\nOther Filename Tools\ndirectory path, Opening Files\nmodules, Module Filenames-Module Filenames\nprogram files, The Programmer’s View\nfiles, Files, Test Your Knowledge: Answers\naccess-by-key, Other File Tools\nbinary, Text and Binary Files: The Short Story-Text and Binary Files: The\nShort Story\nbinary-mode, Using Text and Binary Files\nbuffered, Using Files\nbyte files, Unicode and Byte Files-Unicode and Byte Files\nbytecode, Bytecode compilation\nclose method, Using Files\ncontent, Using Files\ncontext managers, File Context Managers\ncross-file changes, minimizing, Program Design: Minimize Cross-File\nChanges-Program Design: Minimize Cross-File Changes",
      "content_length": 923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1859,
      "chapter": null,
      "content": "descriptor files, os module, Other File Tools\ndirectory paths, Files in Action\nFIFOs, Other File Tools\niteration, Files\niterators, Using Files, Files in Action\nlist comprehensions, List Comprehensions and Files-List Comprehensions\nand Files\nmethods export, Core Types Review and Summary\nnewline characters, Files in Action\nnumber types, Core Types Review and Summary\nobject storage, Storing Objects with Conversions-Storing Objects with\nConversions\nopen function, Other File-Like Tools\nopening, Opening Files\npipes, Other File Tools\nprogram architecture, How to Structure a Program\nrunning as standalone script, Dual-Usage Modes: __name__ and __main__-\nExample: Unit Tests with __name__\nsets, Core Types Review and Summary\nshell-command streams, Other File Tools\nshelves, Other File Tools\nsockets, Other File Tools\nstandard streams, Other File Tools\nstring types, Core Types Review and Summary",
      "content_length": 893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1860,
      "chapter": null,
      "content": "strings, Using Files\nempty, Files in Action\ntext files, Using Files\nconversions, Storing Objects with Conversions-Storing Objects with\nConversions\npathname, Text-File Basics\ntext output, Files\ntext-mode, Using Text and Binary Files\nUnicode and, Unicode and Byte Files-Unicode and Byte Files\nfilter function, Selecting Items in Iterables: filter-Selecting Items in Iterables:\nfilter, Generator expressions versus filter\nfinalization (see termination actions)\nfind method, “Changing” Strings, Part 2: String Methods\nfirst-class object model, The First-Class Object Model-The First-Class Object\nModel\nFlask, GUIs and UIs, Internet and Web Scripting\nfloating point numbers, Numbers, Test Your Knowledge: Answers, Test Your\nKnowledge: Answers, Types Share Operation Sets by Categories\nconversion, String Conversion Tools\nfiles, Core Types Review and Summary\ninteractive loops, Supporting Floating-Point Numbers-Supporting Floating-\nPoint Numbers\nfolder hierarchies, package imports, Package Imports\nfolders\npackages, Module Packages",
      "content_length": 1027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1861,
      "chapter": null,
      "content": "names, Basic Package Structure\nnesting, Basic Package Structure\nfor loops, Item Iteration: for Loops-Item Iteration: for Loops, for Loops\ncomprehensions, Changing Lists: range and Comprehensions-Changing\nLists: range and Comprehensions\ndata types, Basic usage\ndictionary comprehensions, Dictionary Comprehensions\nextended-unpacking assignments, Application to for loops, Extended-\nunpacking assignment in for loops-Extended-unpacking assignment in for\nloops\nin functions, Definition-Definition\n__getitem__ method, Index Iteration: __getitem__\niteration protocol, Iteration protocol integration\nlists, Changing Lists: range and Comprehensions-Changing Lists: range and\nComprehensions\nnested, Nested for loops-Nested for loops\nlist comprehensions, Nested loops: for, Formal Comprehension Syntax\nsequence assignment, Tuple (sequence) assignment in for loops-Tuple\n(sequence) assignment in for loops\nsequence scans, Sequence Scans: while, range, and for-Sequence Scans:\nwhile, range, and for\ntargets, General Format\ntips, Common Coding Gotchas\ntuple assignment, Tuple (sequence) assignment in for loops-Tuple\n(sequence) assignment in for loops",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1862,
      "chapter": null,
      "content": "FOSS (free and open source software), Who Uses Python Today?\nfractions, Test Your Knowledge: Answers, Types Share Operation Sets by\nCategories\nfiles, Core Types Review and Summary\nframeworks, Programming by customization\nfree and open source software (FOSS), Who Uses Python Today?\nfrom * statement, The from * Statement-The from * Statement\n__all__ variable, Minimizing from * Damage: _X and __all__-Minimizing\nfrom * Damage: _X and __all__\n__init__.py files, The Roles of __init__.py Files\nvariable meanings, from * Can Obscure the Meaning of Variables\n_X prefix, Minimizing from * Damage: _X and __all__-Minimizing from *\nDamage: _X and __all__\nfrom statement, The from Statement\nas clause, The as Extension for import and from-The as Extension for\nimport and from\ninteractive statement, reload, from, and Interactive Testing\nlink problems, from Copies Names but Doesn’t Link\npotential problems, Potential Pitfalls of the from Statement-When import is\nrequired\nreloads, reload May Not Impact from Imports\nfrozen binaries, Standalone Executables\nfunction calls, Running Code Interactively\nargument matching, Argument Matching Overview, Argument Matching\nSyntax",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1863,
      "chapter": null,
      "content": "unpacking arguments, Calls: Unpacking arguments-Calls: Unpacking\narguments\nargument ordering, Calls Ordering\nboundary cases, Boundary cases\nformal definitions, Formal definition\nargument passing, Argument Passing Details\narguments, mutable, Argument-Passing Basics\nassignments, Application to for loops\nbenchmarks, For more good times: Function calls and map-For more good\ntimes: Function calls and map\nchangeable data, Function Attributes: Changeable, Per-Call, Explicit\ninterfaces, Function Interfaces and Callback-Based Code-Function\nInterfaces and Callback-Based Code\nparentheses, Common Coding Gotchas\ntiming, Timing Function Calls-Timing Function Calls\ntracing, decorators, Tracing Function Calls-Tracing Function Calls\nfunction decorators, Decorators and Metaclasses-Function Decorator Basics,\nWhat’s a Decorator?\nargument validation, Example: Validating Function Arguments-The Goal\ncall proxies, Managing Calls and Instances\nfunction call timing, Timing Function Calls-Timing Function Calls\nfunction managers, Managing Functions and Classes\nmetafunctions, Usage\nruntime declarations, Usage",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1864,
      "chapter": null,
      "content": "syntax, Property basics, Usage\nusage, Function Decorator Basics\nuser-defined, A First Look at User-Defined Function Decorators-A First\nLook at User-Defined Function Decorators\nfunction definitions\nargument matching, Argument Matching Syntax, Definitions: Collecting\narguments-Definitions: Collecting arguments\nargument ordering, Definition Ordering\nboundary cases, Boundary cases\nformal definitions, Formal definition\nargument passing, Argument Passing Details\nfunction headers, Application to for loops\nfunction managers, Managing Functions and Classes\nfunction validation\nkeywords, Generalizing for Keywords and Defaults-Generalizing for\nKeywords and Defaults\npositional arguments, A Basic Range-Testing Decorator for Positional\nArguments-A Basic Range-Testing Decorator for Positional Arguments\nfunctional programming, Why Do People Use Python?, Functional Programming\nTools, Classes Generate Multiple Instance Objects\nclosures, Closures and Factory Functions-Closures and Factory Functions\nfirst-class object model, The First-Class Object Model\nmap function, Mapping Functions over Iterables: map-Mapping Functions\nover Iterables: map\nfunctions",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1865,
      "chapter": null,
      "content": "annotations, Type Hinting: Optional, Unused, and Why?, General Function\nConcepts, Function Annotations and Decorations-Function decorators\nalternative: Preview\nanonymous, lambda Makes Anonymous Functions-lambda Makes\nAnonymous Functions, Anonymous Functions: lambda\n(see also lambda expression)\nargument matching syntax, Argument Matching Syntax\narguments, def Statements\ncoupling, Function Design Concepts\npassing, General Function Concepts\nvalidation, Example: Validating Function Arguments-Decorator\nArguments Versus Function Annotations\nasync, Asynchronous Functions: The Short Story-The Async Wrap-Up\nasync statement, Advanced Function Tools\nattributes, General Function Concepts\nstate, State with function attributes-State with function attributes\nstate retention, Function Attributes: Changeable, Per-Call, Explicit -\nFunction Attributes: Changeable, Per-Call, Explicit\nuser-defined, Function Attributes\nawait statement, Advanced Function Tools\nbuilt-in, handling by assignment, Hiding built-ins by assignment\ncalls, Calls\ndef statement, Calls\nclassmethod, Using Static and Class Methods\nclosure functions, Function Interfaces and Callback-Based Code",
      "content_length": 1157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1866,
      "chapter": null,
      "content": "cohesion, Function Design Concepts\ncoroutines, Advanced Function Tools\ncoupling, Function Design Concepts\ndecorations, Function Annotations and Decorations-Function decorators\nalternative: Preview\ndecorators, General Function Concepts\ndef statements, Basic Function Tools, def Statements, def Executes at\nRuntime\ndefaults, Defaults and Mutable Objects-Defaults and Mutable Objects\ndeferring execution, Anonymous Functions: lambda\ndefinitions, General Function Concepts, Definition\ndir, Getting Help\neval, String Conversion Tools\nexec, The exec built-in-The exec built-in\nexecution environment, Function Design Concepts\nfactory functions, Using simple factory functions\nfilter, Selecting Items in Iterables: filter-Selecting Items in Iterables: filter\nfirst-class object model, The First-Class Object Model-The First-Class\nObject Model\nfor loops in, Definition-Definition\ngenerator functions, Comprehensions and Generations, Generator Functions\nand Expressions, Generator functions in action, Generator Functions Versus\nGenerator Expressions\nbest uses, Why generator functions?\niteration protocol, Generator functions in action",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1867,
      "chapter": null,
      "content": "send method, Extended generator function protocol: send versus next\nslicing sequences, Generator functions-Generator functions\nstate suspension, State suspension-State suspension\nyield statement, The yield from extension-The yield from extension\ngenerators, Advanced Function Tools\nglobal statement, Advanced Function Tools\nglobal variables, Function Design Concepts\nhelper functions, The Downside of “Helper” Functions-The Downside of\n“Helper” Functions\ninlining, Anonymous Functions: lambda\nintersect, Calls\nlambda body expression, Calls\npolymorphism, Polymorphism Revisited\nintrospection, Function Introspection, Function introspection\nlambda expressions, Basic Function Tools, lambda Makes Anonymous\nFunctions-lambda Makes Anonymous Functions\nlen, Escape Sequences Are Special Characters\nloop variables, Enclosing scopes and loop variables\nmanager functions, The Downside of “Helper” Functions\nmetafunctions, Function Decorator Basics\nnames, locals, Local Names Are Detected Statically-Local Names Are\nDetected Statically\nnested, decorating methods, Using nested functions to decorate methods\nnonlocal statement, Advanced Function Tools",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1868,
      "chapter": null,
      "content": "objects, mutable, Defaults and Mutable Objects-Defaults and Mutable\nObjects\nopen, Other File-Like Tools\noverview, Function Basics\nparameters, def Statements\npassing as arguments, The First-Class Object Model\nplain functions, Method Objects: Bound or Not, Why the Special Methods?\nrecursive (see recursion)\nreduce, Combining Items in Iterables: reduce-Combining Items in Iterables:\nreduce\nreload, Reloading Modules-reload Odds and Ends\nreturn statements, Basic Function Tools, return Statements\nreturns, Functions Without returns\nscopes, enclosing, Enclosing scopes and loop variables\nset, Sets\nsize, Function Design Concepts\nslicing sequences, Simple functions\nstaticmethod, Using Static and Class Methods\nstr, Numbers\nsuper function, The super Function\nargument lists, Same argument lists-Same argument lists\nattribute-fetch algorithm, Attribute-fetch algorithm\ncall-chain anchors, Call-chain anchors-Call-chain anchors\ndeployment, Universal deployment",
      "content_length": 953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1869,
      "chapter": null,
      "content": "MRO algorithm, A “magic” proxy-A “magic” proxy\nnoncalls, Noncalls and operator overloading-Noncalls and operator\noverloading\noperator overloading, Noncalls and operator overloading-Noncalls and\noperator overloading\ntestdriver, Running In-Process Tests\ntimer utility functions, Timer Module: Take 2\ntype, Types-Types\ntype hints, Type Hinting: Optional, Unused, and Why?\nuses, Why Use Functions?-Why Use Functions?\nyield statement, Advanced Function Tools\n__future__ import, Enabling Language Changes: __future__\nG\ngame programming, And More: AI, Games, Images, QA, Excel, Apps…\ngarbage collection, Nesting Revisited\ncircular references, Objects Are Garbage-Collected\nobjects, Objects Are Garbage-Collected-Objects Are Garbage-Collected\ngeneralized set functions, argument matching, Example: Generalized Set\nFunctions-Testing the Code\ngenerator expressions, Comprehensions and Generations, Generator Functions\nand Expressions, Generator Functions Versus Generator Expressions, Listing\ninstance attributes with __dict__\nas iterables, Generator Expressions: Iterables Meet Comprehensions\nbest uses, Why generator expressions?",
      "content_length": 1121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1870,
      "chapter": null,
      "content": "versus filter function, Generator expressions versus filter\ngenerator objects, Generator Expressions: Iterables Meet Comprehensions\nversus map function, Generator expressions versus map-Generator\nexpressions versus map\nnesting, Generator expressions versus map\nslicing sequences, Generator expressions\ngenerator functions, Comprehensions and Generations, Generator Functions and\nExpressions, Generator functions in action, Generator Functions Versus\nGenerator Expressions\nbest uses, Why generator functions?\niterable objects, Classes versus generators\niteration protocol, Iteration protocol integration, Generator functions in\naction\nsend method, Extended generator function protocol: send versus next\nslicing sequences, Generator functions-Generator functions\nstate suspension, State suspension-State suspension\nyield statement, The yield from extension-The yield from extension\ngenerator objects, Generator Functions Versus Generator Expressions, Coding\nAlternative: __iter__ Plus yield-Multiple iterators with yield\ngenerator expressions, Generator Expressions: Iterables Meet\nComprehensions\ngenerators, Advanced Function Tools\nbuilt-ins, Generation in built-ins and classes-Generation in built-ins and\nclasses\nclasses, Generation in built-ins and classes-Generation in built-ins and\nclasses",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1871,
      "chapter": null,
      "content": "comprehensions, Comprehensions versus type calls and generators\nmemory footprint, Why generators here: Space, time, and more\npermutation, Why generators here: Space, time, and more-Why generators\nhere: Space, time, and more\nresults generation, Generating “infinite” (well, indefinite) results-\nGenerating “infinite” (well, indefinite) results\nsingle-pass iterables, Generators are single-pass iterables-Generators are\nsingle-pass iterables\n__get__ method, Inserting Code to Run on Attribute Access\n__getattr__ method, Other Ways to Combine Classes: Composites, Attribute\nReference, Miscellaneous Class Gotchas, Inserting Code to Run on Attribute\nAccess, __getattr__ and __getattribute__-Using __getattribute__\nversus __getattribute__ method, __getattr__ and __getattribute__\nCompared-__getattr__ and __getattribute__ Compared\n__getattribute__ method, __getattribute__ and Descriptors: Attribute\nImplementations, Inserting Code to Run on Attribute Access, __getattr__ and\n__getattribute__-Using __getattribute__, Management Techniques Compared\nversus __getattr__ method, __getattr__ and __getattribute__ Compared-\n__getattr__ and __getattribute__ Compared\n__getitem__ method, Indexing and Slicing: __getitem__ and __setitem__\niteration, Index Iteration: __getitem__-Index Iteration: __getitem__\niteration and, Intercepting Item Assignments\nslice expressions, Intercepting Slices-Intercepting Slices\nglobal namespaces, Simple Names: Global Unless Assigned\nglobal scope, state retention and, Globals: Changeable but Shared\nglobal statement, Advanced Function Tools, The global Statement-The global",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1872,
      "chapter": null,
      "content": "Statement\nemulation, Other Ways to Access Globals-Other Ways to Access Globals\nglobal variables\ncoupling, Function Design Concepts\ncross-file changes, Program Design: Minimize Cross-File Changes-\nProgram Design: Minimize Cross-File Changes\nmultithreading, Program Design: Minimize Global Variables\nprogram design, Program Design: Minimize Global Variables-Program\nDesign: Minimize Global Variables\nstate, decorators and, State with global variables\nglyphs, The str Object\ngraphics processing, And More: AI, Games, Images, QA, Excel, Apps…\n__gt__ method, Comparisons: __lt__, __gt__, and Others-Comparisons: __lt__,\n__gt__, and Others\nGUIs (graphical user interfaces), OK, but What’s the Downside?, GUIs and UIs\nIDLE, The IDLE Graphical User Interface-The IDLE Graphical User\nInterface\nH\nhas-a relationships, composition, OOP and Composition: “Has-a” Relationships-\nStream Processors Revisited\nhash-based bytecode files, Step 2: Compile It (Maybe)\nhashes, Dictionaries\nHaskell, Comprehensions: The Final Act\nhelper functions, The Downside of “Helper” Functions-The Downside of\n“Helper” Functions",
      "content_length": 1094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1873,
      "chapter": null,
      "content": "hex escapes, Coding Unicode Strings in Python\nhierarchies, subclassing, OOP: The Big Idea\nHPy, Component Integration\nI\nI/O (input/output) operations, async extension, Asynchronous Functions: The\nShort Story\n__iadd__ method, In-Place Addition\nIDEs (integrated development environments), Other IDEs for Python,\nDevelopment Tools for Larger Projects\nattribute lists, The dir Function\nIDLE, The IDLE Graphical User Interface\nIDLE, Using Python on Windows-Using Python on Windows\nIDLE GUI, The IDLE Graphical User Interface-The IDLE Graphical User\nInterface\nif else expression, Missing Keys: if Tests\nrecursion, Coding Alternatives-Coding Alternatives\nif statements, A Tale of Two ifs, if Statements\ndictionary keys, Missing Keys: if Tests-Missing Keys: if Tests\nlist comprehensions, Filter clauses: if\nmatch statements, Match versus if live\nmultiple-choice selection, Multiple-Choice Selections\nfunction definition, Handling larger actions\nswitch defaults, Handling switch defaults\nsyntax, Missing Keys: if Tests",
      "content_length": 1008,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1874,
      "chapter": null,
      "content": "if/then/else expression, The if/else Ternary Expression-The if/else Ternary\nExpression\nimage processing, And More: AI, Games, Images, QA, Excel, Apps…\nimmutability, Test Your Knowledge: Answers\nstrings, Immutability-Immutability, Types Share Operation Sets by\nCategories\ntuples, Tuples, Conversions, methods, and immutability-Conversions,\nmethods, and immutability\nimmutable arguments, Argument-Passing Basics\nimmutable sequence of 8-bit integers, bytes type, The bytes Object\nimmutable sequence of characters, str type, The str Object\nimmutable sequences, strings, String Object Basics\nimmutable types, Mutable Types Can Be Changed in Place, Test Your\nKnowledge: Part II Exercises, Part II, Objects and Operations\nimplementation alternatives, Python Implementation Alternatives-Python\nImplementation Alternatives\nimplementation-related objects, Python’s Core Object Types\nimport models\nmodules, Python Import Models\nnamespace packages, Namespace Packages-Namespace Packages in Action\nmodule searches, The Module Search Algorithm-The Module Search\nAlgorithm\npackage-relative imports, Package-Relative Imports-Python Import Models\npackages, Module Packages-Package Imports, Python Import Models\nmodule searches, Packages and the Module Search Path-Packages and",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1875,
      "chapter": null,
      "content": "the Module Search Path\npackage creation, Creating Packages-Using the updated package\npackage initialization, The Roles of __init__.py Files-The Roles of\n__init__.py Files\nimport optimization, bytecode, Bytecode compilation\nimport statement, The import Statement-The import Statement\nas clause, The as Extension for import and from-The as Extension for\nimport and from\nmodules, Step 1: Find It\nimports\nas runtime operations, How Imports Work\ncircular, Part V, Modules and Packages\n__future__, Enabling Language Changes: __future__\nmodules, Imports and Attributes-Step 1: Find It\nbytecode, Step 2: Compile It (Maybe)-Step 2: Compile It (Maybe)\nbytecode execution, Step 3: Run It\ninitialization code, Initialization code\nnamespace nesting, Namespace Nesting-Namespace Nesting\none-time, Imports Happen Only Once\nprevious imports, Step 2: Compile It (Maybe)\nruntime assignment, Imports Are Runtime Assignments-Cross-file\nname changes\nstatement order, Statement Order Matters in Top-Level Code-\nStatement Order Matters in Top-Level Code",
      "content_length": 1030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1876,
      "chapter": null,
      "content": "variables, Imports Versus Scopes-Imports Versus Scopes\nnested, Part V, Modules and Packages\nnested modules, attributes, Using the basic package\npackage-relative, Module Packages\nabsolute, Relative and Absolute Imports, Relative and Absolute\nImports\nname clashes, Module Name Clashes: Package and Package-Relative\nImports\nrelative, Relative and Absolute Imports-Python Import Models\npackages, Module Packages, Package Imports, Python Import Models, Part\nV, Modules and Packages\nfolder hierarchies, Package Imports\nfrom statement, Using the basic package\nrecursive, Recursive from Imports May Not Work-Recursive from Imports\nMay Not Work\nin-place changes, Shared References and In-Place Changes-Shared References\nand In-Place Changes\nexpression statements, Expression Statements and In-Place Changes\nnames, Scopes Overview\nincremental prototyping, Testing as You Go\nindentation\nblocks, End of indentation is end of block-Why Indentation Syntax?\nconsistency, Common Coding Gotchas\ntext editors, Why Indentation Syntax?\nindex assignments, Index and slice assignments-Index and slice assignments",
      "content_length": 1090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1877,
      "chapter": null,
      "content": "__index__ method, But __index__ Means As-Integer\nindexing, Test Your Knowledge: Part II Exercises, Part II, Objects and\nOperations\ndictionaries, Test Your Knowledge: Part II Exercises, Part II, Objects and\nOperations\nexpressions, Sequence Operations\nslicing, Sequence Operations\ninstances\niteration, __getitem__, Index Iteration: __getitem__-Index Iteration:\n__getitem__\noperator overloading, Indexing and Slicing: __getitem__ and\n__setitem__-But __index__ Means As-Integer\nlists, Indexing and Slicing-Indexing and Slicing\nout of bounds, Test Your Knowledge: Part II Exercises\nstring indexing, Test Your Knowledge: Part II Exercises, Part II, Objects\nand Operations\nstrings, Indexing and Slicing-Extended slicing: The third limit and slice\nobjects\nindirect recursion, Coding Alternatives\ninfinite loops, Examples\ninheritance, Python and OOP, Part VI, Classes and OOP\nattribute fetches, Classes: Under the Hood\nbuilt-ins, The built-ins bifurcation\nclass trees, Python Inheritance Algorithm: The Simple Version\nclasses, OOP: The Big Picture, Inheritance",
      "content_length": 1051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1878,
      "chapter": null,
      "content": "attribute trees, Attribute Tree Construction\ncustomization, Classes Are Customized by Inheritance-A Second\nExample\ninheritance search, Some Instances Are More Equal Than Others\nmetaclass subclasses and, The Inheritance Bifurcation\nprivacy, Inheritance Fine Print\ncomposites, Other Ways to Combine Classes: Composites-Other Ways to\nCombine Classes: Composites\ncustomization by, Step 4: Customizing Behavior by Subclassing\ndata descriptors, The descriptors deviation, The assignment addendum\nversus delegation, Inheritance versus delegation\ndescriptors, The Basics, A First Example\ninherited methods, Specializing Inherited Methods-Specializing Inherited\nMethods\nis-a relationships, OOP and Inheritance: “Is-a” Relationships-OOP and\nInheritance: “Is-a” Relationships\nmetaclasses, Metaclasses and Inheritance, Inheritance: The Finale,\nInheritance: The Finale, Metaclass Inheritance-Metaclass Inheritance\ninheritance relationships, Inheritance: The Finale\nnames acquisition, Metaclass Versus Superclass\nMRO (method resolution order), Multiple Inheritance and the MRO, How\nthe MRO Works-How the MRO Works, The Inheritance Bifurcation\nmultiple, Multiple Inheritance and the MRO\nattribute conflicts, Attribute Conflict Resolution-Attribute Conflict\nResolution",
      "content_length": 1252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1879,
      "chapter": null,
      "content": "DFLR, Multiple Inheritance and the MRO\ninheritance sources, Example: Mapping Attributes to Inheritance\nSources-Example: Mapping Attributes to Inheritance Sources\nmix-in classes, Example: “Mix-in” Attribute Listers-Listing attributes\nper object in class trees\norder, Multiple Inheritance: Order Matters-Multiple Inheritance: Order\nMatters\noverview, How Multiple Inheritance Works-How Multiple Inheritance\nWorks\nslots, Slot usage rules, Example impacts of slots: ListTree and\nmapattrs\nnamespaces, Example: Class Attributes\nnonclass instances, The Inheritance Bifurcation\nOOP, Why Use Classes?, OOP and Inheritance: “Is-a” Relationships-OOP\nand Inheritance: “Is-a” Relationships\nattributes, Attribute Inheritance Search-Attribute Inheritance Search\nclass customization, Classes Are Customized by Inheritance-A Second\nExample\nmultiple inheritance, Coding Class Trees\noperator-overloading methods, Common Operator-Overloading Methods\nproperties, A First Example\nsecondary trees, The Inheritance Bifurcation\nsingle-inheritance, slots, Example impacts of slots: ListTree and mapattrs\nstring representation methods, String Representation: __repr__ and __str__\nsubclassing, Inherit, Customize, and Extend",
      "content_length": 1195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1880,
      "chapter": null,
      "content": "subverting, Augmenting Methods: The Good Way\nsuper function, The super supplement\ninheritance algorithm, Python Inheritance Algorithm: The Simple Version-The\nInheritance Wrap-Up\ninheritance hierarchy, OOP: The Big Picture\n__init__ method\nconstructors, Operator Overloading\noperator overload and, Constructors and Expressions: __init__ and __sub__\n__init__.py files, Package __init__.py Files-Using the updated package, The\nRoles of __init__.py Files\nfrom * statement, The Roles of __init__.py Files\nmodule namespace initialization, The Roles of __init__.py Files\npackage initialization, The Roles of __init__.py Files\nsearches, The Roles of __init__.py Files\ninlining functions, Anonymous Functions: lambda\ninputs, user (see user inputs)\ninstallation\nAndroid, Installing Python\nLinux, Installing Python\nmacOS, Installing Python\nUnix, Installing Python\nWindows, Installing Python\ninstallation management, Development Tools for Larger Projects\ninstance methods, Why the Special Methods?, Using Static and Class Methods",
      "content_length": 1016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1881,
      "chapter": null,
      "content": "metaclass comparison, Metaclass Methods Versus Instance Methods-\nMetaclass Methods Versus Instance Methods\ninstance objects, Classes Generate Multiple Instance Objects, Instance Objects\nAre Concrete Items\nattributes, Coding Constructors\ninstances\nattribute privacy, Emulating Privacy for Instance Attributes: Part 1-\nEmulating Privacy for Instance Attributes: Part 1\nattributes, Instance Versus Class Attributes-Instance Versus Class Attributes\nclass decorators, Supporting multiple instances-Supporting multiple\ninstances, Class Pitfall: Retaining Multiple Instances-Class Pitfall:\nRetaining Multiple Instances\nclasses, Why Use Classes?\ncounting, Counting Instances with Static Methods-Counting Instances with\nStatic Methods\nclass methods, Counting Instances with Class Methods-Counting\ninstances per class with class methods\ncreation, Some Instances Are More Equal Than Others\nindexing\niteration, Index Iteration: __getitem__-Index Iteration: __getitem__\noperator overloading, Indexing and Slicing: __getitem__ and\n__setitem__-But __index__ Means As-Integer\nliteral syntax, Classes Are Types Are Classes\nmetaclasses, Some Instances Are More Equal Than Others\nnonclass instances, Some Instances Are More Equal Than Others\ninheritance, The Inheritance Bifurcation",
      "content_length": 1263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1882,
      "chapter": null,
      "content": "OOP, Attribute Inheritance Search\nnamespaces, Classes and Instances\nstate, Using State Information in Descriptors\nstate, decorators and, State with class-instance attributes\nsubclasses, namespace dictionaries, Namespace Dictionaries: Review\ninteger keys, Using dictionaries to simulate flexible lists: Integer keys\nintegers, Numbers, Test Your Knowledge: Answers, Types Share Operation Sets\nby Categories\nfiles, Core Types Review and Summary\n__index__ method, But __index__ Means As-Integer\nprecision in large numbers, Numbers\ninteraction, Part I, Getting Started\ninteractive coding\n$ character, What Not to Type: Prompts and Comments\nAndroid, Starting an Interactive REPL\ncommand line, Test Your Knowledge: Answers\ncomments, What Not to Type: Prompts and Comments\nexercise, Test Your Knowledge: Part I Exercises\nmacOS, Starting an Interactive REPL\nprompt, Why the Interactive Prompt?-Testing\nprompts, What Not to Type: Prompts and Comments\nREPLs (real-eval-print loops)\ncode folders, Where to Run: Code Folders-Where to Run: Code\nFolders",
      "content_length": 1038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1883,
      "chapter": null,
      "content": "IPython, Other Python REPLs\nstarting, Starting an Interactive REPL-Starting an Interactive REPL,\nTest Your Knowledge: Answers\nrunning code, Running Code Interactively-Running Code Interactively\nWindows, Starting an Interactive REPL\ninteractive command-line\nREPLs (real-eval-print loops), Starting an Interactive REPL\ninteractive loops, A Simple Interactive Loop-A Simple Interactive Loop\nerror handling\ninput testing, Handling Errors by Testing Inputs-Handling Errors by\nTesting Inputs\ntry statements, Handling Errors with try Statements-Handling Errors\nwith try Statements\nfloating-point numbers, Supporting Floating-Point Numbers-Supporting\nFloating-Point Numbers\nmath on user inputs, Doing Math on User Inputs-Doing Math on User\nInputs\nnesting code, Nesting Code Three Levels Deep\ninteractive tests, Test Your Knowledge: Part II Exercises\nfrom statement, reload, from, and Interactive Testing\nreload statement, reload, from, and Interactive Testing\ninterface proxies, wrappers, Managing Calls and Instances\ninternet modules, Internet and Web Scripting\ninterpreted language, Is Python a “Scripting Language”?\ninterpreter, Introducing the Python Interpreter-The Python Virtual Machine",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1884,
      "chapter": null,
      "content": "(PVM), Test Your Knowledge: Answers\nintersect function, Calls\nlambda body expression, Calls\nlocal variables, Segue: Local Variables-Segue: Local Variables\npolymorphism, Polymorphism Revisited\nintrospection\n__dict__ attribute, Module Introspection\nlisting modules, Example: Listing Modules with __dict__-Example:\nListing Modules with __dict__\nfunctions, Function introspection\nintrospection tools, Step 6: Using Introspection Tools\nclass attributes, Special Class Attributes-Instance Versus Class Attributes\ndisplay tools, A Generic Display Tool-A Generic Display Tool\ninstance attributes, Instance Versus Class Attributes-Instance Versus Class\nAttributes\ntool classes, Name Considerations in Tool Classes\niOS, Python on, Using Python on iOS-Using Python on iOS\nIPython, Other Python REPLs\nIronPython, Component Integration, Python Implementation Alternatives\nis-a relationships, inheritance, OOP and Inheritance: “Is-a” Relationships-OOP\nand Inheritance: “Is-a” Relationships\nitems method, Item Iteration: for Loops\n__iter__ method, Iterable Objects: __iter__ and __next__\nuser-defined iterables, User-Defined Iterables-Classes versus generators",
      "content_length": 1145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1885,
      "chapter": null,
      "content": "yield function and, Coding Alternative: __iter__ Plus yield-Multiple\niterators with yield\niterable objects, Iterations, Iterable Objects: __iter__ and __next__\ncombining items, Combining Items in Iterables: reduce-Combining Items in\nIterables: reduce\ngenerator functions, Classes versus generators\nitem selection, Selecting Items in Iterables: filter-Selecting Items in\nIterables: filter\nmapping functions over, Mapping Functions over Iterables: map-Mapping\nFunctions over Iterables: map\niterable, definition, Iterations\niterables\ngenerator expressions as, Generator Expressions: Iterables Meet\nComprehensions\ngenerators, Generators are single-pass iterables-Generators are single-pass\niterables\nmap function, Coding Your Own map\nsingle-pass, Generators are single-pass iterables-Generators are single-pass\niterables\nuser-defined\n__iter__ method, User-Defined Iterables-Classes versus generators\nyield function, Coding Alternative: __iter__ Plus yield-Multiple\niterators with yield\nzip function, Coding Your Own zip and 2.X map\niteration, Files, Iterations, Part IV, Functions and Generators\nbenchmarking, Iteration Results-For more good times: Function calls and",
      "content_length": 1163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1886,
      "chapter": null,
      "content": "map\nfunction calls, For more good times: Function calls and map-For more\ngood times: Function calls and map\nmap function, For more good times: Function calls and map-For more\ngood times: Function calls and map\ndictionaries, Item Iteration: for Loops-Item Iteration: for Loops, Reprise:\nDictionaries, range, enumerate, and zip\nfilter built-in, Functional iterables: map and filter-Functional iterables: map\nand filter\ngenerators, Comprehensions and Generations\n__getitem__ method, Intercepting Item Assignments\ninstance indexing, __getitem__, Index Iteration: __getitem__-Index\nIteration: __getitem__\nlist comprehensions, List Comprehension Basics, Formal Comprehension\nSyntax\nlists, Iteration, Comprehensions, and Unpacking\nmap built-in, Functional iterables: map and filter-Functional iterables: map\nand filter\nmultiple iterations, Multiple Iterators on One Object-Multiple Iterators on\nOne Object\nmultiple-pass iterables, Multiple-pass versus single-pass iterables-Multiple-\npass versus single-pass iterables\nPython morph and, Test Your Knowledge: Answers\nsingle-pass iterables, Multiple-pass versus single-pass iterables-Multiple-\npass versus single-pass iterables\nsingle-scan, Single versus multiple scans",
      "content_length": 1209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1887,
      "chapter": null,
      "content": "standard-library iterables, Standard-library iterables in Python\nstrings, Basic Operations\ntext files, Using Files\nview objects, Dictionary key/value/item view objects\niteration protocol, Iterations and Comprehensions-The Iteration Protocol,\nIterable Objects: __iter__ and __next__\nfor loops, Iteration protocol integration\nfull iteration protocol, The full iteration protocol-The full iteration protocol\ngenerator functions, Iteration protocol integration, Generator functions in\naction\niter, The iter and next built-ins, More on iter and next-More on iter and\nnext, Generator functions in action\niteration tools, The Iteration Protocol\nlists, Reprise: Dictionaries, range, enumerate, and zip\nmanual iteration, Manual iteration\nnext, The iter and next built-ins, More on iter and next\nrange object, Reprise: Dictionaries, range, enumerate, and zip\nreadlines, The Iteration Protocol\nStopIteration exception, The Iteration Protocol\nzip object, Reprise: Dictionaries, range, enumerate, and zip\niteration tools, Conversions, methods, and immutability, Iteration Tools-Other\nIteration Topics\niterator objects, multiple iterations, Multiple Iterators on One Object-Multiple\nIterators on One Object\niterators",
      "content_length": 1202,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1888,
      "chapter": null,
      "content": "definition, Iterations\nnesting, Iterator nesting\nJ\nJava, Component Integration\nJIT compilers\nNumba, Python Implementation Alternatives\nPyPy, Python Implementation Alternatives\njoin method, “Changing” Strings, Part 2: String Methods\njson module, The pickle and json Serialization Modules-The pickle and json\nSerialization Modules\nJSON, object storage, Storing Objects with JSON\nJupyter notebooks, Numeric and Scientific Programming\nJupyter Notebooks, Jupyter Notebooks for Science\nJVM (Java Virtual Machine)\nJython, Python Implementation Alternatives\nJython, Component Integration, Python Implementation Alternatives\nK\nkeys\ndictionaries, Dictionaries\ninsertion order, Key Insertion Ordering-Key Insertion Ordering\ninteger keys, Using dictionaries to simulate flexible lists: Integer keys\nmapping values to, Mapping values to keys-Mapping values to keys\nmissing, Avoiding missing-key errors",
      "content_length": 888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1889,
      "chapter": null,
      "content": "sorting, Sorting dictionary keys-Sorting dictionary keys\ntuple keys, Using dictionaries for sparse data structures: Tuple keys\nkeys method, Item Iteration: for Loops\nkeyword arguments\nlist order, Sorting lists\nprint operations, The print function in action\nkeyword-only arguments, Keyword-Only Arguments-Why keyword-only\narguments?\nkeywords, Part IV, Functions and Generators-Part IV, Functions and Generators\nargument matching, Argument Matching Overview, Argument Matching\nSyntax, Test Your Knowledge: Answers\nargument passing, Keyword and Default Examples-Combining keywords\nand defaults\nfunction validation, Generalizing for Keywords and Defaults-Generalizing\nfor Keywords and Defaults\nrequired, Keyword-Only Arguments\nKivy, GUIs and UIs, Standalone Apps and Executables\nL\nlambda expression, Basic Function Tools, lambda Makes Anonymous Functions-\nlambda Makes Anonymous Functions, lambda Basics\nargument-matching syntax, lambda Basics\ncallbacks, Test Your Knowledge: Answers\ncode proximity and, Multiway branches: The finale\nloops in, How (Not) to Obfuscate Your Python Code",
      "content_length": 1079,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1890,
      "chapter": null,
      "content": "name assignments, Name Resolution: The LEGB Rule\nnames, Python Scopes Basics\nnesting, Scopes: lambdas Can Be Nested Too\nnesting logic, How (Not) to Obfuscate Your Python Code\nreasons to use, Why Use lambda?-Multiway branches: The finale\nselection logic, How (Not) to Obfuscate Your Python Code\nlambda functions\ncallbacks, Function Interfaces and Callback-Based Code\nwrapper layers, Decorator Nesting\nLatin-1, Character Encodings\nlearning curve, It’s Relatively Easy to Learn\nLEGB rule, Name Resolution: The LEGB Rule-Preview: Other Python scopes,\nNested Classes: The LEGB Scopes Rule Revisited-Nested Classes: The LEGB\nScopes Rule Revisited\nbuilt-ins, Redefining built-in names: For better or worse-Redefining built-in\nnames: For better or worse\nlen function, Escape Sequences Are Special Characters, Test Your Knowledge:\nAnswers\nsequence reordering, Sequence Shufflers: range and len\nlexical scoping, Python Scopes Basics, Imports Versus Scopes\nlibraries, Why Do People Use Python?\n(see also standard library)\nutilities, It’s Powerful\nlike-a relationships, OOP and Delegation: “Like-a” Relationships-OOP and\nDelegation: “Like-a” Relationships",
      "content_length": 1143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1891,
      "chapter": null,
      "content": "lines comparison, text files, The Termination-Handlers Shoot-Out\nLinux\nPython installation, Installing Python\nPython on, Using Python on Linux-Using Python on Linux\nlist comprehension pattern, map function, Coding Your Own map\nlist comprehensions, Comprehensions, List Comprehension Basics,\nComprehensions: The Final Act\nbest use, When to use list comprehensions: Speed, conciseness, etc.-When\nto use list comprehensions: Speed, conciseness, etc.\ncode obfuscation and, When not to use list comprehensions: Code\nobfuscation\ndiagonals, Example: List Comprehensions and Matrixes\nexpression mapping, List Comprehensions Review\nexpressions, Comprehensions: The Final Act\narbitrary, List Comprehensions Review\nfiles, List Comprehensions and Files-List Comprehensions and Files\nfilter function, Generator expressions versus filter\nfor clauses, nested, Formal Comprehension Syntax\nif clause, Filter clauses: if, List Comprehensions Review\niteration, Formal Comprehension Syntax\nmap function, List Comprehensions Review\nmatrixes and, Example: List Comprehensions and Matrixes-When to use\nlist comprehensions: Speed, conciseness, etc.\nnested for loops, Nested loops: for",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1892,
      "chapter": null,
      "content": "sequence operations, Conversions, methods, and immutability\nsyntax, formal, Formal Comprehension Syntax\nlists, Why Use Built-in Objects?, Test Your Knowledge: Answers, Types Share\nOperation Sets by Categories\nas literal expression, Lists\nassignments, Assignment Syntax Forms\nbounds, Bounds Checking\nbreadth-first traversal, Recursion versus queues and stacks\nbuilding, Test Your Knowledge: Answers\ncharacteristics, Lists-Lists\ncompared to tuples, Why Lists and Tuples?\ncomparisons, Basic List Operations, Comparisons, Equality, and Truth\ncomprehensions, List comprehensions and maps-List comprehensions and\nmaps\ndeleting one item, More List Methods\n__dict__ attribute, Example: Listing Modules with __dict__-Example:\nListing Modules with __dict__\ndictionaries, sorting, Sorting lists\nindex assignments, Index and slice assignments-Index and slice\nassignments\nindexing, Indexing and Slicing-Indexing and Slicing\niteration, Iteration, Comprehensions, and Unpacking\niteration protocol, Reprise: Dictionaries, range, enumerate, and zip\nliterals, Lists",
      "content_length": 1047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1893,
      "chapter": null,
      "content": "unpacking, List-literal unpacking\nmaps, List comprehensions and maps-List comprehensions and maps\nmethod calls, List method calls-List method calls\nmutability, Lists\nnesting, Nesting-Nesting\noperations, Lists\noperators, Basic List Operations\nordering, Sorting lists-Sorting lists\nreversing, More List Methods\nsearching, Test Your Knowledge: Answers\nsequence operations, Sequence Operations\nslice assignments, Index and slice assignments-Index and slice assignments,\nOther List Operations\nslicing, Indexing and Slicing-Indexing and Slicing\ntype-specific operations, Type-Specific Operations-Type-Specific\nOperations\nliteral expressions, Python’s Core Object Types\ndictionaries, Dictionaries\nlists as, Lists\nliteral patterns, Advanced match Usage\nliterals, Python’s Core Object Types\ndictionaries, Dictionaries\ndocstrings, User-defined docstrings\nf-string formatting, The F-String Formatting Literal",
      "content_length": 897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1894,
      "chapter": null,
      "content": "backslash, F-string formatting basics\ncustom formats, F-string custom formats\nquotes, F-string formatting basics\nREPL, F-string formatting basics\nsubstitutions, F-string formatting basics\nlists, unpacking, List-literal unpacking\nstring formatting, String-Formatting Options\nlocal scopes, Scopes Overview\nlocal variables, Segue: Local Variables-Segue: Local Variables\nrecursion, Scopes Overview\nstatic locals, Function Attributes\nlogic alternatives, Test Your Knowledge: Part III Exercises\nLookupError exception class, Built-in Exception Classes\nloops, Part III, Statements and Syntax\nattribute interception methods, Avoiding loops in attribute interception\nmethods\ncoding, Test Your Knowledge: Part III Exercises\ncomprehensions, Preview: Other Python scopes\ncounters, range object, Counter Loops: range-Counter Loops: range\ndefaults, Loops Require Defaults, Not Scopes-Loops Require Defaults, Not\nScopes\ndictionary comprehensions, Dictionary Comprehensions\nelse clause, Loop else-Why the loop else?\nenumerate function, Offsets and Items: enumerate-Offsets and Items:",
      "content_length": 1066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1895,
      "chapter": null,
      "content": "enumerate\nfor loops, Item Iteration: for Loops-Item Iteration: for Loops, for Loops,\nCommon Coding Gotchas\ndata types, Basic usage\nextended-unpacking assignments, Extended-unpacking assignment in\nfor loops-Extended-unpacking assignment in for loops\nin functions, Definition-Definition\n__getitem__, Index Iteration: __getitem__\nnested, Nested for loops-Nested for loops, Nested loops: for\nsequence assignment, Tuple (sequence) assignment in for loops-Tuple\n(sequence) assignment in for loops\ntargets, General Format\ntuple assignment, Tuple (sequence) assignment in for loops-Tuple\n(sequence) assignment in for loops\ninfinite, Examples\ninteractive, A Simple Interactive Loop-A Simple Interactive Loop\nfloating-point numbers, Supporting Floating-Point Numbers-\nSupporting Floating-Point Numbers\nmath on user inputs, Doing Math on User Inputs-Doing Math on User\nInputs\nnesting code, Nesting Code Three Levels Deep\ntesting inputs, Handling Errors by Testing Inputs-Handling Errors by\nTesting Inputs\ntry statements, Handling Errors with try Statements-Handling Errors\nwith try Statements\nlambda expression, How (Not) to Obfuscate Your Python Code",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1896,
      "chapter": null,
      "content": "nested, multiple, Breaking Out of Multiple Nested Loops: “go to”-Breaking\nOut of Multiple Nested Loops: “go to”\nobject indexes, Examples\noffsets, Offsets and Items: enumerate-Offsets and Items: enumerate\nversus recursion, Loop Statements Versus Recursion-Loop Statements\nVersus Recursion\nsequence reordering, Sequence Shufflers: range and len\nsequence scans, Sequence Scans: while, range, and for-Sequence Scans:\nwhile, range, and for\nskipping items, Skipping Items: range and Slices\nstrings and, Basic Operations\nvariables, Enclosing scopes and loop variables\nwhile loops, while Loops-Examples\nbreak statement, break, continue, pass, and the Loop else, break-The\nnamed-assignment alternative\ncontinue statement, break, continue, pass, and the Loop else-The\nnested-code alternative\ndo until, Examples\nelse statement, break, continue, pass, and the Loop else\npass statement, break, continue, pass, and the Loop else-The ellipsis-\nliteral alternative\nzip object, Parallel Traversals: zip-More zip roles: dictionaries\n__lt__ method, Comparisons: __lt__, __gt__, and Others-Comparisons: __lt__,\n__gt__, and Others\nM",
      "content_length": 1111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1897,
      "chapter": null,
      "content": "machine code, Test Your Knowledge: Answers\nmacOS\ninteractive coding and, Starting an Interactive REPL\nPython installation, Installing Python\nPython on, Using Python on macOS-Using Python on macOS\nmagnitude comparisons, Truth Values Revisited\n__main__ attribute, Dual-Usage Modes: __name__ and __main__-Example:\nUnit Tests with __name__\n__main__.py files, Package __main__.py Files-Using the updated package\nmanager functions, The Downside of “Helper” Functions\nmangled names, Pseudoprivate Class Attributes\nmanual iteration, Manual iteration\nmanuals, standard, The Standard Manuals-Web Resources\nmap function, Mapping Functions over Iterables: map-Mapping Functions over\nIterables: map\nbenchmarking, For more good times: Function calls and map-For more\ngood times: Function calls and map\nemulating, Example: Emulating zip and map-Coding Your Own zip and 2.X\nmap\niterables, multiple, Coding Your Own map\nlist comprehension pattern, Coding Your Own map\nlist comprehensions, List Comprehensions Review\nnesting, Generator expressions versus map\nversus generator expressions, Generator expressions versus map-Generator\nexpressions versus map",
      "content_length": 1136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1898,
      "chapter": null,
      "content": "mapping, Test Your Knowledge: Answers\nmutability, Dictionaries\noperations, Mapping Operations-Mapping Operations\nmapping patterns, Advanced match Usage\nmappings, Types Share Operation Sets by Categories\nmatch statement, Missing Keys: if Tests\nmatch statements, match Statements\nattribute patterns, Advanced match Usage\nif statements, Match versus if live\ninstance patterns, Advanced match Usage\nliteral patterns, Advanced match Usage\nmapping patterns, Advanced match Usage\nnested patterns, Advanced match Usage\nnesting, Basic match Usage\nparentheses, Advanced match Usage\nsequence patterns, Advanced match Usage\nsyntax, Python Syntax Revisited\nblocks, Block Delimiters: Indentation Rules-Avoid mixing tabs and\nspaces\ndelimiters, Statement Delimiters: Lines and Continuations\nindentation, Block Delimiters: Indentation Rules-Avoid mixing tabs\nand spaces\nspecial syntax, Special Syntax Cases in Action-Special Syntax Cases\nin Action",
      "content_length": 930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1899,
      "chapter": null,
      "content": "math module, Numbers\nmathematical operations, Numbers\nuser inputs, Doing Math on User Inputs-Doing Math on User Inputs\nmatplotlib, Numeric and Scientific Programming\nmatrixes, list comprehensions, Example: List Comprehensions and Matrixes-\nWhen to use list comprehensions: Speed, conciseness, etc.\nmemory management, It’s Powerful\ngenerators, Why generators here: Space, time, and more\nobject memory, Nesting Revisited\nmetaclass/class dichotomy, The Metaclass/Class Dichotomy-The\nMetaclass/Class Dichotomy\nmetaclasses, Some Instances Are More Equal Than Others, Coding Metaclasses\nattributes, Inheritance: The Finale\nclass creation, Metaclasses and Inheritance\nclass decorators comparison, Metaclasses Versus Class Decorators: Round\n1-Metaclasses Versus Class Decorators: Round 1\nclass statements, Adding methods to classes\ntypes, Class Statements Call a type-Class Statements Can Choose a\ntype\nconstruction, Customizing Construction and Initialization\ndecorating class methods, Automatically decorating class methods-\nAutomatically decorating class methods\nfactory functions, Using simple factory functions\ninheritance, The Inheritance Bifurcation, Metaclasses and Inheritance,\nCustomizing Construction and Initialization, Inheritance: The Finale,",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1900,
      "chapter": null,
      "content": "Inheritance: The Finale, Metaclass Inheritance-Metaclass Inheritance\nnames acquisition, Metaclass Versus Superclass\ninheritance relationships, Inheritance: The Finale\nmethod protocol, Metaclass Method Protocol\nmethods, Metaclasses and Inheritance, Metaclass Methods\nadding, Adding methods to classes-Adding methods to classes\nclass methods comparison, Metaclass Methods Versus Class Methods\ninstance methods comparison, Metaclass Methods Versus Instance\nMethods-Metaclass Methods Versus Instance Methods\noperator overloading in, Operator Overloading in Metaclass Methods\nnonclass instances, Overloading class creation calls with normal classes-\nOverloading class creation calls with normal classes\nsuperclasses comparison, Metaclass Versus Superclass-Metaclass Versus\nSuperclass\ntypes, Classes Are Instances of type-Classes Are Instances of type\nsubclasses, Metaclasses Are Subclasses of type\nwhen to use, To Metaclass or Not to Metaclass\nmetafunctions, Function Decorator Basics\nfunction decorators, Usage\nmetaprograms, Module Introspection\nmethod calls, Methods-Other Method-Call Possibilities\nlists, List method calls-List method calls\nsyntax, Method Call Syntax\nmethod calls (OOP), Method Calls-Method Calls",
      "content_length": 1211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1901,
      "chapter": null,
      "content": "method objects, Method Objects: Bound or Not-Bound Methods in Action\nbound methods, Method Objects: Bound or Not\nplain functions, Method Objects: Bound or Not\nmethods\nattribute fetches, Method Call Syntax\n__bool__, Boolean Tests: __bool__ and __len__\nbound methods, Why the Special Methods?\nbyte strings, Methods\ncall expressions, Method Call Syntax\n__call__, Call Expressions: __call__-Function Interfaces and Callback-\nBased Code\nclass methods, Why the Special Methods?, Using Static and Class Methods,\nUsing Static and Class Methods\nclasses, Static and Class Methods\nadding to, Adding methods to classes-Adding methods to classes\nmethod calls, Methods-Other Method-Call Possibilities\n__contains__, Membership: __contains__, __iter__, and __getitem__-\nMembership: __contains__, __iter__, and __getitem__\ndecorators, Supporting method decoration, Class Pitfall: Decorating\nMethods\ndescriptors, Using descriptors to decorate methods-Using descriptors\nto decorate methods\nnested functions, Using nested functions to decorate methods\n__del__, Object Destruction: __del__-Destructor Usage Notes\ndictionaries, More Dictionary Methods-More Dictionary Methods",
      "content_length": 1153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1902,
      "chapter": null,
      "content": "double underscores, Classes Can Intercept Python Operators\nencapsulation, Coding Methods\nfind, “Changing” Strings, Part 2: String Methods\n__get__, Inserting Code to Run on Attribute Access\n__getattr__, Other Ways to Combine Classes: Composites, Attribute\nReference, Inserting Code to Run on Attribute Access\n__getattribute__, Inserting Code to Run on Attribute Access\n__getitem__, Intercepting Slices-Intercepting Slices\n__gt__, Comparisons: __lt__, __gt__, and Others-Comparisons: __lt__,\n__gt__, and Others\n__iadd__, In-Place Addition\n__index__, But __index__ Means As-Integer\ninherited, Specializing Inherited Methods-Specializing Inherited Methods\ninstance, Why the Special Methods?\ninstance methods, Using Static and Class Methods\n__iter__, Iterable Objects: __iter__ and __next__, User-Defined Iterables-\nClasses versus generators\njoin, “Changing” Strings, Part 2: String Methods\n__lt__, Comparisons: __lt__, __gt__, and Others-Comparisons: __lt__,\n__gt__, and Others\nmetaclasses, Metaclasses and Inheritance, Metaclass Method Protocol,\nMetaclass Methods-Metaclass Methods Versus Instance Methods\n__next__, Iterable Objects: __iter__ and __next__\nOOP, Coding Class Trees\noperator overloading, Common Operator-Overloading Methods-Common",
      "content_length": 1241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1903,
      "chapter": null,
      "content": "Operator-Overloading Methods\nplain-function, Plain-Function Methods-Plain-Function Methods\n__radd__, Right-Side Addition\nreplace, “Changing” Strings, Part 2: String Methods, “Changing” Strings,\nPart 2: String Methods\n__repr__, String Representation: __repr__ and __str__\nscopes, Scopes in Methods and Classes-Scopes in Methods and Classes\n__set__, Inserting Code to Run on Attribute Access\n__setattr__, Attribute Assignment and Deletion-Attribute Assignment and\nDeletion, Inserting Code to Run on Attribute Access\nsplit, More String Methods: Parsing Text\nstandard-library manual, All String Methods (Today)\nstatic, Other Method-Call Possibilities, Static and Class Methods-Using\nStatic and Class Methods\n__str__, Why Two Display Methods?-Display Usage Notes\nstring formatting, String-Formatting Options\nformat method, The String-Formatting Method-Advanced formatting\nmethod examples\nstring methods, String Methods\nsyntax, Method Call Syntax\nstrings, Type-Specific Methods-Type-Specific Methods\nsubclasses, Augmenting Methods: The Bad Way-Augmenting Methods:\nThe Good Way\nsuperclass, Specializing Inherited Methods\ntuples, Conversions, methods, and immutability-Conversions, methods, and",
      "content_length": 1186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1904,
      "chapter": null,
      "content": "immutability\n__setitem__, Intercepting Item Assignments\nmethods (OOP), A First Example\nMicroPython, Python Implementation Alternatives\nMicrosoft Store, Using Python on Windows\nminimalism of Python, Software Quality\nmins.py, Example: The min Wakeup Call-The Punch Line\nmix-in classes\nattribute lists, Example: “Mix-in” Attribute Listers\nclass trees, Listing attributes per object in class trees-Listing attributes\nper object in class trees\n__dict__, Listing instance attributes with __dict__-Listing instance\nattributes with __dict__\n__dir__, Listing inherited attributes with dir-Listing inherited\nattributes with dir\nmixed-type comparisons, Comparisons, Equality, and Truth\nmodule packages (see packages)\nmodule search path, The Module Search Path\nbuilt-in modules, The Module Search Path\nconfiguring, Configuring the Search Path\nfile selection, Module File Selection-Selection priorities\nfile sources, Module sources\nhome directory, Search-Path Components\npackages, Path Outliers: Standalones and Packages, Packages and the",
      "content_length": 1025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1905,
      "chapter": null,
      "content": "Module Search Path\n.pth path-file directory, Search-Path Components\nPYTHONPATH directory, Search-Path Components-Configuring the\nSearch Path\nsite-packages directory, Search-Path Components\nstandalones, Path Outliers: Standalones and Packages\nstandard-library directories, Search-Path Components\nsys.path list, The sys.path List-Changing the module search path\nmodules, Program Files, Part I, Getting Started\nattributes, Module attributes: a first look-Module attributes: a first look,\nCreating Modules\nclasses, Classes Are Attributes in Modules-Classes Are Attributes in\nModules\ncreating, How Files Generate Namespaces\n__dict__, Namespace Dictionaries: __dict__-Namespace Dictionaries:\n__dict__\nnames, qualification, Attribute Name Qualification-Attribute Name\nQualification\nclasstools, Name Considerations in Tool Classes\ncode reuse, Why Use Modules?\ncohesion, Module Design Concepts\ncollector, Part V, Modules and Packages\ncoupling, Module Design Concepts\ncreating, Creating Modules-Other Kinds of Modules\ndefining, Creating Modules",
      "content_length": 1034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1906,
      "chapter": null,
      "content": "designing, Module Design Concepts\nexercise, Test Your Knowledge: Part I Exercises\nextension modules, Other Kinds of Modules\nfile names, Module attributes: a first look\nfilenames, Module Filenames-Module Filenames\nfiles, Program Files\nfrom * statement, The from * Statement-The from * Statement\nfrom statement, Module Essentials, The from Statement\npotential problems, Potential Pitfalls of the from Statement-When\nimport is required\nglobal scope, Why Use Modules?\nimport statement, Module Essentials, The import Statement-The import\nStatement\nimporting, Importing modules, Test Your Knowledge: Answers, Imports\nand Attributes-Step 1: Find It\nbytecode, Step 2: Compile It (Maybe)-Step 2: Compile It (Maybe)\nbytecode execution, Step 3: Run It\nprevious imports, Step 2: Compile It (Maybe)\nimportlib.reload, Module Essentials\nimports\ndirect calls, Direct Calls: Two Options\ninitialization code, Initialization code\nname strings, Importing Modules by Name String-Testing reload\nvariants",
      "content_length": 981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1907,
      "chapter": null,
      "content": "namespace nesting, Namespace Nesting-Namespace Nesting\none-time, Imports Happen Only Once\nruntime assignment, Imports Are Runtime Assignments-Cross-file\nname changes\nstatement order, Statement Order Matters in Top-Level Code-\nStatement Order Matters in Top-Level Code\nvariables, Imports Versus Scopes-Imports Versus Scopes\njson, The pickle and json Serialization Modules-The pickle and json\nSerialization Modules\nlisting, __dict__ attribute, Example: Listing Modules with __dict__-\nExample: Listing Modules with __dict__\n__main__, Dual-Usage Modes: __name__ and __main__-Example: Unit\nTests with __name__\nmath module, Numbers\nmutables, changing, Changing mutables in modules\n__name__, Dual-Usage Modes: __name__ and __main__-Example: Unit\nTests with __name__\nnamespaces, Why Use Modules?\ndictionaries, How Files Generate Namespaces-Namespace\nDictionaries: __dict__\ngenerating, How Files Generate Namespaces-How Files Generate\nNamespaces\ninitialization, The Roles of __init__.py Files\nnesting, Namespace Nesting-Namespace Nesting\npartitioning, Why Use Modules?\nnested, attributes, Using the basic package",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1908,
      "chapter": null,
      "content": "pickle, Pickles and Shelves, The pickle and json Serialization Modules-The\npickle and json Serialization Modules\nprogram architecture, Importing modules, How to Structure a Program\nrandom, Numbers\nre, The re Pattern-Matching Module\nredundancy, Why Use Modules?\nreload function, Reloading Modules-reload Odds and Ends\nreloads, Reloading modules, Example: Transitive Module Reloads\nrecursive reloaders, A recursive reloader-Testing reload variants\nscopes\nimports and, How Files Generate Namespaces\nvariables, Imports Versus Scopes-Imports Versus Scopes\nshelve, Pickles and Shelves, The shelve module\nstandard-library modules, Standard-Library Modules\nstatements, The Python Conceptual Hierarchy\nimport, How Files Generate Namespaces\nstrings, String Object Basics\nstruct, The struct Binary-Data Module\nusing, Using Modules-When import is required\nversus classes, Classes Versus Modules\nMRO (method resolution order), Multiple Inheritance and the MRO, How the\nMRO Works-How the MRO Works\nattribute fetch, Attribute-fetch algorithm\nclass inheritance, Universal deployment",
      "content_length": 1066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1909,
      "chapter": null,
      "content": "class trees, Python Inheritance Algorithm: The Simple Version, Python\nInheritance Algorithm: The Less Simple Version\ndeployment, Universal deployment\nas flattened tree, The Inheritance Bifurcation\ninheritance, The Inheritance Bifurcation\nsuper function, A “magic” proxy-A “magic” proxy\n__mro__ attribute, How the MRO Works, Example: Mapping Attributes to\nInheritance Sources, The Inheritance Bifurcation, Attribute-fetch algorithm,\nMetaclass Versus Superclass\nmultiline statements, Running Code Interactively\nmultiline strings, Triple Quotes and Multiline Strings-Triple Quotes and\nMultiline Strings\nmultiline text, triple-quoted strings, Triple Quotes and Multiline Strings\nmultiple inheritance, Multiple Inheritance and the MRO\nattribute conflicts, Attribute Conflict Resolution-Attribute Conflict\nResolution\nDFLR (depth first, left to right), Multiple Inheritance and the MRO\ninheritance sources, Example: Mapping Attributes to Inheritance Sources-\nExample: Mapping Attributes to Inheritance Sources\nmix-in classes, attribute lists, Example: “Mix-in” Attribute Listers-Listing\nattributes per object in class trees\nMRO (method resolution order), Multiple Inheritance and the MRO, How\nthe MRO Works-How the MRO Works\norder, Multiple Inheritance: Order Matters-Multiple Inheritance: Order\nMatters\noverview, How Multiple Inheritance Works-How Multiple Inheritance",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1910,
      "chapter": null,
      "content": "Works\nslots, Slot usage rules\nmultiple-target assignments, Assignment Syntax Forms, Multiple-Target\nAssignments\nshared references, Multiple-target assignment and shared references-\nMultiple-target assignment and shared references\nmultiplication, Numbers\nmultitasking, async function, Async Basics\nmultithreading, global variables, Program Design: Minimize Global Variables\nmultiway branching, dictionaries, Multiway branches: The finale\nmutability\ndictionaries, Dictionaries, Dictionaries, Changing Dictionaries in Place-\nChanging Dictionaries in Place\nin-place changes, Shared References and In-Place Changes\nlists, Lists\nmutable defaults, Combining keywords and defaults\nobjects, changing in place, Core Types Review and Summary\nstrings, Immutability-Immutability\nmutable arguments, Argument-Passing Basics\nchanges, Avoiding Mutable Argument Changes-Avoiding Mutable\nArgument Changes\ncoupling, Function Design Concepts\nmutable objects\nchanging, Changing mutables in modules",
      "content_length": 975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1911,
      "chapter": null,
      "content": "functions, Defaults and Mutable Objects-Defaults and Mutable Objects\nmutable types, Mutable Types Can Be Changed in Place\nmutables, assignments, Common Coding Gotchas\nN\nname annotations, basic assignment, Basic Assignments\nname assignments, nested scopes, Nested Scopes Overview\n__name__ attribute, Dual-Usage Modes: __name__ and __main__-Example:\nUnit Tests with __name__\nname clashes, Potential Pitfalls of the from Statement, The as Extension for\nimport and from, Module Name Clashes: Package and Package-Relative Imports\nname collisions, tool classes, Name Considerations in Tool Classes\nname mangling, Pseudoprivate Class Attributes\nname references, nested scopes, Nested Scopes Overview\nname strings, module imports, Importing Modules by Name String\ncode strings, running, Running Code Strings\ndirect calls, Direct Calls: Two Options\nreloads, Example: Transitive Module Reloads-Testing reload variants\nnamed assignment expression, Assignment Syntax Forms, Named Assignment\nExpressions-When to use named assignment\nnamed tuples, Records Revisited: Named Tuples-Records Revisited: Named\nTuples\nnames\naliasing, Arguments and Shared References\nassignments, Assignments, Scopes Overview, Name Resolution: The LEGB",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1912,
      "chapter": null,
      "content": "Rule\ndef statement, Python Scopes Basics\ndef statements, Name Resolution: The LEGB Rule\nglobal statements, Name Resolution: The LEGB Rule\nin-place changes, Scopes Overview\nlambda expressions, Python Scopes Basics\nlambda statements, Name Resolution: The LEGB Rule\nLEGB rule, Name Resolution: The LEGB Rule-Preview: Other Python\nscopes\nlocal, Scopes Overview\nnonlocal statements, Name Resolution: The LEGB Rule\nreferences, Name Resolution: The LEGB Rule\nvariables, Module Filenames\nnamespace packages, Module Packages, Using the updated package, Namespace\nPackages\nmodule searches, The Module Search Algorithm-The Module Search\nAlgorithm\nnamespaces, Python Scopes Basics, Why Use Modules?, Testing as You Go\nassignments, The “Zen” of Namespaces: Assignments Classify Names-The\n“Zen” of Namespaces: Assignments Classify Names\nclasses\nhierarchies, Why Use Classes?\ninheritance, Example: Class Attributes\ndictionaries, Classes: Under the Hood, Namespace Dictionaries: Review-\nNamespace Dictionaries: Review",
      "content_length": 1001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1913,
      "chapter": null,
      "content": "slots and, Slot basics-Slots and namespace dictionaries\ngenerating, How Files Generate Namespaces-How Files Generate\nNamespaces\nglobal, Simple Names: Global Unless Assigned\nglobal statements, The global Statement-The global Statement\nlinks, Namespace Links: A Tree Climber-Namespace Links: A Tree\nClimber\nmodules\ndictionaries, How Files Generate Namespaces-Namespace\nDictionaries: __dict__\n__init__.py files, The Roles of __init__.py Files\nnesting, Namespace Nesting-Namespace Nesting\npartitioning, Why Use Modules?\nnamespace trees, Attribute Tree Construction\nobject namespaces, Attribute Names: Object Namespaces-Attribute Names:\nObject Namespaces\nOOP, Classes and Instances\nnaming variables, Variable Name Rules-Names have no type, but objects do\nnested calls, Numbers\nnested modules, Using the basic package\nnested patterns, match statements, Advanced match Usage\nnested scopes, Nested Scopes Examples-Nested Scopes Examples\narbitrary nesting, Arbitrary Scope Nesting\nname assignments, Nested Scopes Overview",
      "content_length": 1012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1914,
      "chapter": null,
      "content": "name references, Nested Scopes Overview\nnested statement blocks, What Python Adds\nspecial cases, Block rule special case-Block rule special case\nnesting, Test Your Knowledge: Part II Exercises, Part II, Objects and Operations\nassigning nested sequences, Advanced sequence-assignment patterns\nclasses, scope, Nested Classes: The LEGB Scopes Rule Revisited-Nested\nClasses: The LEGB Scopes Rule Revisited\ncombined clauses, try statement, Combining finally and except by nesting\ncomposites, Other Ways to Combine Classes: Composites\ndecorators, Decorator Nesting-Decorator Nesting, Decorator nesting\ndescriptors, A First Example\ndictionaries, Nesting Revisited-Nesting Revisited, Nesting in dictionaries\nexception groups, Example: Control-Flow Nesting\nexception handlers, Nesting Exception Handlers\ncontrol-flow, Example: Control-Flow Nesting-Example: Control-Flow\nNesting\nsyntactic nesting, Example: Syntactic Nesting-Example: Syntactic\nNesting\nfor loops, Nested for loops-Nested for loops\nfunctions, decorating methods, Using nested functions to decorate methods\ngenerator expressions, Generator expressions versus map\nimports, Part V, Modules and Packages\ninteractive loops, Nesting Code Three Levels Deep\niterators, Iterator nesting",
      "content_length": 1232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1915,
      "chapter": null,
      "content": "lambda expression, Scopes: lambdas Can Be Nested Too\nloops, multiple, Breaking Out of Multiple Nested Loops: “go to”-Breaking\nOut of Multiple Nested Loops: “go to”\nmap function, Generator expressions versus map\nmatch statements, Basic match Usage\nnamespaces, in modules, Namespace Nesting-Namespace Nesting\npackage folders, Basic Package Structure\nwhile loops, The nested-code alternative\nnesting objects, Nesting-Nesting\n.NET Framework for Windows, IronPython, Python Implementation\nAlternatives\nnewline character, Escape Sequences Are Special Characters, Files in Action,\nText and Binary Modes\n__next__ method, Iterable Objects: __iter__ and __next__\nnonclass instances, Some Instances Are More Equal Than Others\ninheritance, The Inheritance Bifurcation\nmetaclasses, Overloading class creation calls with normal classes-\nOverloading class creation calls with normal classes\nNone object, Booleans and None, The None object\nnonempty data structures, The Meaning of True and False in Python\nnonlocal statement, Advanced Function Tools, The nonlocal Statement-nonlocal\nBoundary Cases\nstate retention and, Nonlocals: Changeable, Per-Call, LEGB\nnormalization, Unicode, Unicode Normalization: Whither Standard?-Unicode\nNormalization: Whither Standard?",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1916,
      "chapter": null,
      "content": "Notepad, BOM (byte order marker), Making BOMs in Text Editors-Making\nBOMs in Text Editors\nNuitka, Python Implementation Alternatives\nNULL character, Escape Sequences Are Special Characters\nNumba, Numeric and Scientific Programming, Python Implementation\nAlternatives, Test Your Knowledge: Answers\nnumber types, files, Core Types Review and Summary\nnumbers, Test Your Knowledge: Answers\ncomparisons, Comparisons, Equality, and Truth\ncomplex, Numbers\ndecimals, Numbers, Types Share Operation Sets by Categories\nfloating-point, Numbers, Test Your Knowledge: Answers\nfloating-point numbers, Types Share Operation Sets by Categories\nfractions, Types Share Operation Sets by Categories\nintegers, Numbers, Types Share Operation Sets by Categories\nmathematical operations, Numbers\nrationals, Numbers\nsets, Numbers\ntrue and false, The Meaning of True and False in Python\ntypes, Test Your Knowledge: Answers\nnumeric programming, Numeric and Scientific Programming\nNumPy, Why Do People Use Python?, OK, but What’s the Downside?, Numeric\nand Scientific Programming\nO",
      "content_length": 1054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1917,
      "chapter": null,
      "content": "obfuscation, list comprehensions and, When not to use list comprehensions:\nCode obfuscation\nobject class\nas superclass, Some Instances Are More Equal Than Others\ninheritance type class, And One “object” to Rule Them All\nobject factories, Testing as You Go, Classes Are Objects: Generic Object\nFactories-Why Factories?\nobject indexes, loops, Examples\nobject memory, Nesting Revisited\nobject model\nclasses, user-defined, Classes Are Types Are Classes-Classes Are Types\nAre Classes\ninstances, metaclasses, Some Instances Are More Equal Than Others\nobject namespaces, Attribute Names: Object Namespaces-Attribute Names:\nObject Namespaces\nobject serialization, Storing Objects with pickle\nobject storage, conversions, Storing Objects with Conversions-Storing Objects\nwith Conversions\nobject types, It’s Powerful, Python’s Core Object Types, Test Your Knowledge:\nAnswers\n(see also types)\nbuilt-ins\ncyclic data structures, Beware of Cyclic Data Structures\nextending, Extending Built-in Object Types-Extending Types by\nSubclassing",
      "content_length": 1022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1918,
      "chapter": null,
      "content": "immutable types, Immutable Types Can’t Be Changed in Place\nreferences, Assignment Creates References, Not Copies\nrepetition, Repetition Adds One Level Deep\nhierarchy, Python’s Type Hierarchies\nobject-based code, OOP: The Big Picture\nobject-oriented language, Is Python a “Scripting Language”?, It’s Object-\nOriented and Functional\nobject-oriented, definition, User-Defined Objects\nobject-relational mappers, Internet and Web Scripting\nobjects, Introducing Python Objects, Variables, Objects, and References\naggregation, Other Ways to Combine Classes: Composites\nattributes, The dir Function\nname strings, The dir Function\ntype names, The dir Function\nBooleans, Booleans and None\nbuilt-in, Why Use Built-in Objects?-Why Use Built-in Objects?\nbytearray, The bytearray Object, The bytearray Object-The bytearray\nObject\nbytes, Unicode and Byte Strings, The bytes Object\ncalling, core object types, Python’s Core Object Types\ncategories, Core Types Review and Summary\nclass objects, Attribute Inheritance Search, Classes Generate Multiple\nInstance Objects\ndefault behavior, Class Objects Provide Default Behavior",
      "content_length": 1107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1919,
      "chapter": null,
      "content": "classes, docstrings, User-defined docstrings\ncomparisons, Comparisons, Equality, and Truth-Dictionary comparisons\ncore object types, Comparisons, Equality, and Truth\ncopies, References Versus Copies-References Versus Copies\ncopying, Shared References and In-Place Changes\ndatabases, storage in, Step 7 (Final): Storing Objects in a Database\npickle module, Pickles and Shelves\nshelf object update, Updating Objects on a Shelf\nshelve database, Storing Objects on a shelve Database-Exploring\nShelves Interactively\nshelve module, Pickles and Shelves, The shelve module\ndynamic typing, Variables, Objects, and References\nembedding, Other Ways to Combine Classes: Composites\nexpressions, The Python Conceptual Hierarchy\nfile objects, Files\nfirst-class object model, The First-Class Object Model-The First-Class\nObject Model\nflexibility, Object Flexibility-Object Flexibility\ngarbage collection and, Objects Are Garbage-Collected-Objects Are\nGarbage-Collected\ngenerator objects, Generator Expressions: Iterables Meet Comprehensions,\nGenerator Functions Versus Generator Expressions, Coding Alternative:\n__iter__ Plus yield-Multiple iterators with yield\nimplementation-related, Python’s Core Object Types\nin-place changes, Shared References and In-Place Changes-Shared",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1920,
      "chapter": null,
      "content": "References and In-Place Changes\ninstance objects, Attribute Inheritance Search, Classes Generate Multiple\nInstance Objects, Instance Objects Are Concrete Items\nattributes, Coding Constructors\niterable (see iterable objects)\nmethod objects, Method Objects: Bound or Not-Bound Methods in Action\nbound methods, Method Objects: Bound or Not\nplain functions, Method Objects: Bound or Not\nmutable\nchanging in place, Core Types Review and Summary\nfunctions, Defaults and Mutable Objects-Defaults and Mutable Objects\nnamespace objects, Testing as You Go\nnesting, Nesting-Nesting\nNone, Booleans and None, The None object\noperation sharing, Core Types Review and Summary\nprogram units, Python’s Core Object Types\nproperties, class statement, Property basics\nPydoc, Getting Help\nreference counts, Nesting Revisited\nreferences, References Versus Copies-References Versus Copies\nassignments, Assignments\ndictionaries, Dictionaries\nsets, Sets-Sets, Part VI, Classes and OOP",
      "content_length": 959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1921,
      "chapter": null,
      "content": "shared objects, Shared References\nslice objects, Intercepting Slices\nstate information, Function Attributes\nstorage\nCSV module, Storing Objects with Other Tools\nJSON, Storing Objects with JSON-Storing Objects with JSON\npickle module, Storing Objects with pickle-Storing Objects with pickle\nstruct module, Storing Objects with Other Tools\nstr, The str Object\ntype objects, Types-Types\ntypes, Types Live with Objects, Not Variables\noffsets, Offsets and Items: enumerate-Offsets and Items: enumerate\nOO (object-oriented) code, OOP: The Big Picture\nOO (object-oriented), definition, Why Do People Use Python?\nOOP (object-oriented programming), Why Do People Use Python?, OOP: The\nBig Picture\narguments, functions, Why Use Classes?\nattribute fetches, Attribute Inheritance Search-Attribute Inheritance Search\nclass objects, Classes Generate Multiple Instance Objects\ndefault behavior, Class Objects Provide Default Behavior\nclass statement, User-Defined Objects\nclass trees, Coding Class Trees-Coding Class Trees, Part VI, Classes and\nOOP\nclasses, Attribute Inheritance Search, Step 1: Making Instances",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1922,
      "chapter": null,
      "content": "constructors, Coding Constructors-Coding Constructors\ninheritance, Classes Are Customized by Inheritance-A Second\nExample\nintercepting operators, Classes Can Intercept Python Operators-Other\noperator-overloading methods\nmethods, Step 2: Adding Behavior Methods-Coding Methods\nmethods, augmenting, Augmenting Methods: The Bad Way-\nAugmenting Methods: The Good Way\nsubclasses, Classes Are Customized by Inheritance\nsubclassing, Coding Subclasses-Augmenting Methods: The Good Way\nsuperclasses, Attribute Inheritance Search, Classes Are Customized by\nInheritance\ntesting, Testing as You Go-Testing as You Go\ncode reuse, OOP Is About Code Reuse-Programming by customization,\nTest Your Knowledge: Part VI Exercises\ncomposition, Why Use Classes?, OOP and Composition: “Has-a”\nRelationships-Stream Processors Revisited\nconsistency, Test Your Knowledge: Part VI Exercises\ndelegation, OOP and Delegation: “Like-a” Relationships-OOP and\nDelegation: “Like-a” Relationships\nencapsulation, Python and OOP, Test Your Knowledge: Part VI Exercises\nframeworks, Programming by customization\ninheritance, Why Use Classes?, Python and OOP-OOP and Inheritance: “Is-\na” Relationships\nattributes, Attribute Inheritance Search-Attribute Inheritance Search\nmultiple inheritance, Coding Class Trees",
      "content_length": 1271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1923,
      "chapter": null,
      "content": "search attribute, Why Use Classes?\ninstance objects, Instance Objects Are Concrete Items\ninstances, Why Use Classes?, Attribute Inheritance Search, Classes\nGenerate Multiple Instance Objects\nmaintenance, Test Your Knowledge: Part VI Exercises\nmethod calls, Method Calls-Method Calls\nmethods, Coding Class Trees, A First Example\nnamespaces, Classes and Instances\noperator overload, Why Use Classes?, Operator Overloading\noperator overloading, Step 3: Operator Overloading\nprint displays, Providing Print Displays-Providing Print Displays\npolymorphism, Polymorphism and classes-Polymorphism and classes,\nPolymorphism in Action-Polymorphism in Action, Python and OOP, Test\nYour Knowledge: Part VI Exercises\nreloading, Classes Generate Multiple Instance Objects\nsearch attribute, Why Use Classes?\nstructure, Test Your Knowledge: Part VI Exercises\nopen function, Other File-Like Tools, Redefining built-in names: For better or\nworse\ncustomizing, Test Your Knowledge: Answers\noperations\ndictionaries, Dictionaries\nmapping, Mapping Operations-Mapping Operations\nstrings, Basic Operations-Basic Operations",
      "content_length": 1097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1924,
      "chapter": null,
      "content": "operator overloading, Core Types Review and Summary, Classes Can Intercept\nPython Operators, Step 3: Operator Overloading, The Basics, Part VI, Classes\nand OOP\nattribute privacy, Emulating Privacy for Instance Attributes: Part 1-\nEmulating Privacy for Instance Attributes: Part 1\nattributes\nassignment, Attribute Assignment and Deletion-Attribute Assignment\nand Deletion\ndeleting, Attribute Assignment and Deletion-Attribute Assignment and\nDeletion\nreferences, Attribute Reference\nBoolean tests, Boolean Tests: __bool__ and __len__\ncall expressions, Call Expressions: __call__-Function Interfaces and\nCallback-Based Code\nclass type propagation, Propagating class type-Propagating class type\ncomparison operators, Comparisons: __lt__, __gt__, and Others\nconstructors, Constructors and Expressions: __init__ and __sub__\n__contains__ method, Membership: __contains__, __iter__, and\n__getitem__-Membership: __contains__, __iter__, and __getitem__\ndescriptors, Workaround: Generating operator-overloading descriptors-\nWorkaround: Generating operator-overloading descriptors\ndestructors, Object Destruction: __del__-Destructor Usage Notes\ndisplay formats, String Representation: __repr__ and __str__-Display\nUsage Notes\nexpressions, Constructors and Expressions: __init__ and __sub__\nfallback options, Membership: __contains__, __iter__, and __getitem__-",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1925,
      "chapter": null,
      "content": "Membership: __contains__, __iter__, and __getitem__\n__getattr__ method, __getattr__ and __getattribute__\n__getattribute__ method, __getattr__ and __getattribute__\n__iadd__ method, In-Place Addition\nin-place addition, In-Place Addition\nindex iteration, Index Iteration: __getitem__-Index Iteration: __getitem__\ninstance indexing, Indexing and Slicing: __getitem__ and __setitem__-But\n__index__ Means As-Integer\niterable objects, Iterable Objects: __iter__ and __next__-Iterable Objects:\n__iter__ and __next__\nmultiple, Multiple Iterators on One Object-Classes versus slices\nuser-defined iterables, User-Defined Iterables-Classes versus\ngenerators\nmetaclass methods, Operator Overloading in Metaclass Methods\nmethods, Common Operator-Overloading Methods-Common Operator-\nOverloading Methods\ninline coding, Workaround: Coding operator-overloading methods\ninline-Workaround: Coding operator-overloading methods inline\nsuperclasses, Workaround: Coding operator-overloading methods in\nsuperclasses-Workaround: Coding operator-overloading methods in\nsuperclasses\nobject comparisons, Comparisons: __lt__, __gt__, and Others-\nComparisons: __lt__, __gt__, and Others\nOOP, Why Use Classes?, Operator Overloading\nprint displays, Providing Print Displays-Providing Print Displays\n__radd__ method, Right-Side Addition",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1926,
      "chapter": null,
      "content": "__add__ reuse, Reusing __add__ in __radd__\nresults, Returning results—or not\nright-side addition, Right-Side Addition-Propagating class type\nstrings, Basic Operations\nsuper function, Noncalls and operator overloading-Noncalls and operator\noverloading\noperators\ngeneric operations, Part II, Objects and Operations\nlists, Basic List Operations\noptimization tools, Development Tools for Larger Projects\nordinals, String comparisons\nORMs (object-relational mappers), Database Access\nos module, Command-line launchers\ndescriptor files, Other File Tools\nOSError exception class, Built-in Exception Classes\noutput buffering, Opening Files\noverloading operators, Core Types Review and Summary\nP\npackage-relative imports, Module Packages\nabsolute, Relative and Absolute Imports, Relative and Absolute Imports\nname clashes, Module Name Clashes: Package and Package-Relative\nImports\nrelative, Relative and Absolute Imports-The absolute-import solution,\nPython Import Models",
      "content_length": 962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1927,
      "chapter": null,
      "content": "packages\nbenefits, Why Packages?-A Tale of Two Systems\ndotted-path syntax, Package Imports\nmodule import, Using the basic package\nrelative imports, Relative-Import Rationales and Trade-Offs\nencapsulation, Python and OOP\nfolder bundles, Module Packages\nfolders\nnames, Basic Package Structure\nnesting, Basic Package Structure\nimports, Module Packages, Package Imports, Python Import Models, Part\nV, Modules and Packages\nfolder hierarchies, Package Imports\nfrom statement, Using the basic package\nmodules, Using the basic package-Using the basic package\n__init__.py file, Package __init__.py Files-Using the updated package, The\nRoles of __init__.py Files\n__init__.py files\ninitialization, The Roles of __init__.py Files\n__main__.py file, Package __main__.py Files-Using the updated package\nmodule search path and, Packages and the Module Search Path\nname clashes, Module Name Clashes: Package and Package-Relative\nImports\nnamespace packages, Module Packages, Using the updated package,",
      "content_length": 983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1928,
      "chapter": null,
      "content": "Namespace Packages-Namespace Packages in Action\nmodule searches, The Module Search Algorithm-The Module Search\nAlgorithm\nregular packages, Using the updated package\npandas, Numeric and Scientific Programming\nparallel programming, async functions, Asynchronous Functions: The Short\nStory\nparameters, functions, def Statements\nparentheses, Common Coding Gotchas\nfunction calls, Common Coding Gotchas\nlambda expression, lambda Basics\nstatements, Parentheses are optional\ntuples, Tuple syntax peculiarities: Commas and parentheses-Tuple syntax\npeculiarities: Commas and parentheses\nparsing\nslicing, Indexing and Slicing\ntext, string methods, More String Methods: Parsing Text\npass statements, break, continue, pass, and the Loop else\npathnames, Filenames in open and Other Filename Tools-Filenames in open and\nOther Filename Tools\npatterns\nfactory functions, Closures and Factory Functions-Closures and Factory\nFunctions\nre module, The re Pattern-Matching Module\nsequence assignments, Advanced sequence-assignment patterns",
      "content_length": 1018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1929,
      "chapter": null,
      "content": "structural pattern matching, match Statements\nper-call scopes, Scopes Overview\nperformance, Performance implications\nlist comprehensions, When to use list comprehensions: Speed, conciseness,\netc.\npermutation, sequences, Permutating Sequences-Why generators here: Space,\ntime, and more\npersistence, Stream Processors Revisited\npickle module, Test Your Knowledge: Answers, Pickles and Shelves, The pickle\nand json Serialization Modules-The pickle and json Serialization Modules\nobject storage, Storing Objects with pickle-Storing Objects with pickle\npickle objects, Database Access, Stream Processors Revisited\npipes, Other File Tools\nplain functions, Method Objects: Bound or Not, Why the Special Methods?\nplain-function methods, Plain-Function Methods-Plain-Function Methods\npointers, Variables, Objects, and References\npolymorphism, Sequence Operations, Types, Test Your Knowledge: Answers,\nPolymorphism in Python-Polymorphism in Python, Polymorphism and classes-\nPolymorphism and classes, Polymorphism in Action-Polymorphism in Action,\nPython and OOP, Part IV, Functions and Generators\ndynamic typing, Dynamic Typing Is Everywhere\ninterfaces and, Polymorphism Means Interfaces, Not Call Signatures\nintersect function, Polymorphism Revisited\nstrings and, Basic Operations\npopularity rise of Python, The Python Upside",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1930,
      "chapter": null,
      "content": "portability, Why Do People Use Python?, It’s Portable\nPortable Operating System Interface (POSIX), Systems Programming\npositional arguments, A Basic Range-Testing Decorator for Positional\nArguments-A Basic Range-Testing Decorator for Positional Arguments\npositionals, argument matching, Argument Matching Overview\nPOSIX (Portable Operating System Interface), Systems Programming\nprecision in large numbers, Numbers\nprint emulator, keyword-only arguments, Using Keyword-Only Arguments\nprint function, The print function in action-The print function in action\nargument matching, Example: Rolling Your Own Print-Using Keyword-\nOnly Arguments\ncall format, Call format-Call format\nend-of-line character, The print function in action\nf-strings, The print function in action\nkeyword arguments, The print function in action\nprint operations, Print Operations\nfile object methods, Print Operations\nprint statement, Test Your Knowledge: Answers\nprint stream redirection\nautomatic, Automatic stream redirection-Automatic stream redirection\nhello world, The Python “hello world” program\nmanual, Manual stream redirection-Manual stream redirection\nstandard output streams, Print Operations\nstdout, Test Your Knowledge: Answers",
      "content_length": 1213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1931,
      "chapter": null,
      "content": "print statements, The Programmer’s View\nprint stream\nredirecting\nautomatic, Automatic stream redirection-Automatic stream redirection\nhello world, The Python “hello world” program\nmanual, Manual stream redirection-Manual stream redirection\nprint, viewing docstrings, User-defined docstrings\nprinting\nexceptions, built-in, Default Printing and State-Default Printing and State\ncustom printing, Custom Print Displays-Custom Print Displays\noperator overloading and, Providing Print Displays-Providing Print\nDisplays\nprocedures, Why Use Functions?\nproductivity, developers, Why Do People Use Python?, Developer Productivity\nprofilers, Development Tools for Larger Projects\nprogram architecture\nattributes, Imports and Attributes-Imports and Attributes\nfiles\ntext files, How to Structure a Program\ntop-level, How to Structure a Program\nmodules\nfiles, How to Structure a Program\nimporting, Imports and Attributes-Standard-Library Modules\nstandard-library, Standard-Library Modules",
      "content_length": 974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1932,
      "chapter": null,
      "content": "program design (see design)\nprogram execution, The Programmer’s View\nprogram files\ncommand line, Program Files-Running Files with Command Lines\nfile icons, Clicking and Tapping File Icons\nmodules, Program Files\nnaming convention, The Programmer’s View\nscripts, Program Files\nprogram units, Python’s Core Object Types\nprogramming-in-the-large, It’s Powerful\nprograms, Part I, Getting Started\nexercise, Test Your Knowledge: Part I Exercises\nmodule files, Program Files\nmodules, The Python Conceptual Hierarchy\nprompts, interactive coding, What Not to Type: Prompts and Comments, Why\nthe Interactive Prompt?-Testing\nproperties, Properties: Attribute Accessors, Properties, Management Techniques\nCompared\nattribute fetches, The Basics\nattributes, Property basics\ncomputing value, Computed Attributes\ncreating, The Basics\ndecorators, Coding Properties with Decorators\ndeleters, Setter and deleter decorators",
      "content_length": 902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1933,
      "chapter": null,
      "content": "setters, Setter and deleter decorators\ndescriptors and, How Properties and Descriptors Relate-Descriptors and\nslots and more\ninheritance, A First Example\nobjects, class statement, Property basics\nprototyping, Rapid Prototyping\npseudoprivate attributes, Pseudoprivate Class Attributes-Why Use Pseudoprivate\nAttributes?\npseudoprivate naming, Listing instance attributes with __dict__\nPVM (Python Virtual Machine), The Python Virtual Machine (PVM), Test Your\nKnowledge: Answers\n.py suffix, Module Filenames\npy2wasm, Internet and Web Scripting\n.pyc files, Bytecode compilation\nPyCharm, Other IDEs for Python\nPyDev, Other IDEs for Python\nPydoc, Getting Help, Python Documentation Sources\nbrowser mode, Using Pydoc’s browser interface-Using Pydoc’s browser\ninterface\nbuilt-in tools, Running help on built-in tools-Running help on built-in tools\nimported modules, More Pydoc tips\nmodule-index page, Using Pydoc’s browser interface\nmodules, Running help on your own code-Running help on your own code\nPygments, Beyond Docstrings: Sphinx",
      "content_length": 1028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1934,
      "chapter": null,
      "content": "Pyjamas, Internet and Web Scripting\npyjnius, Component Integration\nPyodide, Internet and Web Scripting\nPyPy, OK, but What’s the Downside?, Numeric and Scientific Programming,\nPython Implementation Alternatives, Python Implementation Alternatives, Test\nYour Knowledge: Answers\nPyQt, GUIs and UIs\nPyScript, Internet and Web Scripting\nPyScripter, Other IDEs for Python\nPyston, Python Implementation Alternatives\nPython\non Android, Using Python on Android-Using Python on Android\napplications, What Can I Do with Python?-And More: AI, Games, Images,\nQA, Excel, Apps…\nbytecode and, OK, but What’s the Downside?\ncode size, Why Do People Use Python?\ncurrent users, Who Uses Python Today?\ndeveloper productivity, Why Do People Use Python?, Developer\nProductivity\nembedding, Other Launch Options\nexecution speed, OK, but What’s the Downside?\non iOS, Using Python on iOS-Using Python on iOS\non Linux, Using Python on Linux-Using Python on Linux\non macOS, Using Python on macOS-Using Python on macOS",
      "content_length": 988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1935,
      "chapter": null,
      "content": "popularity rise, The Python Upside\nportability, Why Do People Use Python?\nas scripting language, Is Python a “Scripting Language”?-Is Python a\n“Scripting Language”?\nsoftware quality, Why Do People Use Python?, Software Quality\non Windows, Using Python on Windows\nCommand Prompt, Using Python on Windows-Using Python on\nWindows\nIDLE, Using Python on Windows-Using Python on Windows\nWSL, Using Python on Windows\nPython Virtual Machine, The Python Virtual Machine (PVM), Test Your\nKnowledge: Answers\nPython/Java bridges, Component Integration\nPYTHONPATH directory, Search-Path Components\nconfiguration search path, Configuring the Search Path\nPyThran, Numeric and Scientific Programming, Python Implementation\nAlternatives\npywin32, Component Integration\nPyYAML, Database Access\nQ\nqualification, attribute names, Attribute Name Qualification-Attribute Name\nQualification\nquality assurance applications, And More: AI, Games, Images, QA, Excel,\nApps…\nqueues",
      "content_length": 951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1936,
      "chapter": null,
      "content": "FIFO (first-in-first-out), Recursion versus queues and stacks\nrecursion, Recursion versus queues and stacks-Recursion versus queues and\nstacks\nquote characters in strings, Other Ways to Code Strings\nR\n__radd__ method, Right-Side Addition\n__add__ reuse, Reusing __add__ in __radd__\nraise statement, The raise Statement, Breaking Out of Multiple Nested Loops:\n“go to”\nexcept block, scopes, Scopes and except as-Scopes and except as\nexcept clause, The except as hook-The except as hook\nexception propagation, Propagating Exceptions with raise\nfrom clause, Exception Chaining: raise from-Exception Chaining: raise\nfrom\nrandom module, Numbers\nrange object\ncounter loops, Counter Loops: range-Counter Loops: range\niteration protocol, Reprise: Dictionaries, range, enumerate, and zip\nlists, Changing Lists: range and Comprehensions-Changing Lists: range and\nComprehensions\nsequence reordering, Sequence Shufflers: range and len\nsequence scans, Sequence Scans: while, range, and for-Sequence Scans:\nwhile, range, and for\nskipping items, Skipping Items: range and Slices",
      "content_length": 1061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1937,
      "chapter": null,
      "content": "range-testing decorator, A Basic Range-Testing Decorator for Positional\nArguments-Decorator Arguments Versus Function Annotations\nrational numbers, Numbers\nraw bytes, Coding Unicode Strings in Python\nraw strings, Example: Rolling Your Own Print\ndocstrings, User-defined docstrings\nescape sequences and, Raw Strings Suppress Escapes-Raw Strings\nSuppress Escapes\nre module, pattern-matching, The re Pattern-Matching Module\nread-only descriptors, Read-only descriptors-Read-only descriptors\nreadability, Why Do People Use Python?\nrecord factories, Step 2: Adding Behavior Methods\nrecursion, Comparisons, Equality, and Truth, Part V, Modules and Packages\narbitrary structures, Handling Arbitrary Structures\ntesting, Testing with a separate script-Testing with a separate script\ncycles, Cycles, paths, and stack limits\ndirect, Coding Alternatives\nfrom imports, Recursive from Imports May Not Work-Recursive from\nImports May Not Work\nfunctions, Part IV, Functions and Generators\nif else expression, Coding Alternatives-Coding Alternatives\nindirect, Coding Alternatives\nlocal variables, Scopes Overview\nversus loop statements, Loop Statements Versus Recursion-Loop",
      "content_length": 1157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1938,
      "chapter": null,
      "content": "Statements Versus Recursion\npaths, Cycles, paths, and stack limits\nqueues, Recursion versus queues and stacks-Recursion versus queues and\nstacks\nstack limits, Cycles, paths, and stack limits\nstacks, Recursion versus queues and stacks-Recursion versus queues and\nstacks\nsum function, Summation with Recursion-Summation with Recursion\nreduce function, Combining Items in Iterables: reduce-Combining Items in\nIterables: reduce\nredundancy\nclasses, OOP: The Big Picture\nincrease in, The Python Tsunami\nmodules, Why Use Modules?\nremoving, Coding Methods\nreference counts, Nesting Revisited\nreferences, References Versus Copies-References Versus Copies\nbuilt-in types, Assignment Creates References, Not Copies\nlists, Lists\nnames, Name Resolution: The LEGB Rule\nnested scopes, Nested Scopes Overview\nshared, Shared References\nargument passing, Arguments and Shared References-Arguments and\nShared References",
      "content_length": 900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1939,
      "chapter": null,
      "content": "equality, Shared References and Equality-Shared References and\nEquality\nin-place changes, Shared References and In-Place Changes-Shared\nReferences and In-Place Changes\nmultiple-target assignments, Multiple-target assignment and shared\nreferences-Multiple-target assignment and shared references\nvariables, Variables, Objects, and References\nrelative imports, Relative and Absolute Imports, Python Import Models\nrelative magnitude comparisons, Comparisons, Equality, and Truth\nreload function, Reloading Modules-reload Odds and Ends\nfrom imports, reload May Not Impact from Imports\ninteractive testing and, reload, from, and Interactive Testing\nreloading modules, Example: Transitive Module Reloads\nOOP, Classes Generate Multiple Instance Objects\nrecursive coding, Alternative codings-Alternative codings\nrecursive reloaders, A recursive reloader-A recursive reloader\ntesting, Testing recursive reloads-Testing recursive reloads\nreload variants, Testing reload variants-Testing reload variants\nrepetition, The Python Conceptual Hierarchy\nreplace method, “Changing” Strings, Part 2: String Methods, “Changing”\nStrings, Part 2: String Methods\nREPLs (real-eval-print loops)\ncode folders, Where to Run: Code Folders-Where to Run: Code Folders\nf-string literal, F-string formatting basics",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1940,
      "chapter": null,
      "content": "IPython, Other Python REPLs\npreloading tools, Example: Listing Modules with __dict__\nstarting, Starting an Interactive REPL-Starting an Interactive REPL\n__repr__ method, String Representation: __repr__ and __str__\nreserved words in variable naming, Variable Name Rules\nREST, Component Integration\nreStructuredText markup language, Beyond Docstrings: Sphinx\nreturn statement, Basic Function Tools, return Statements\nreturns, none, Functions Without returns\nruntime declarations, Usage\nfunction decorators, Usage\nruntime operations\nimports, How Imports Work\nmodule imports, Imports Are Runtime Assignments\nS\nsandbox, The Python Sandbox\nscientific programming, Numeric and Scientific Programming\nSciPy, Numeric and Scientific Programming\nscopes, Python Scopes Basics, Scopes in Methods and Classes-Scopes in\nMethods and Classes\nargument defaults and, Scopes and Argument Defaults-Loops Require\nDefaults, Not Scopes\nbuilt-ins, The Built-in Scope",
      "content_length": 941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1941,
      "chapter": null,
      "content": "LEGB rule, Redefining built-in names: For better or worse-Redefining\nbuilt-in names: For better or worse\ncomprehension variables, Scopes and comprehension variables-Scopes and\ncomprehension variables\ncomprehensions, loop variables, Preview: Other Python scopes\ncross-file changes, Program Design: Minimize Cross-File Changes-\nProgram Design: Minimize Cross-File Changes\nenclosing, Enclosing scopes and loop variables, Implementation\nfunctions, Enclosing scopes and loop variables\nstate retention and, State retention and enclosing scopes\nexamples, Scopes Examples-Scopes Examples\nexception variables, Preview: Other Python scopes\nexceptions, Scopes and except as-Scopes and except as\nglobal scopes, Scopes Overview\nglobal statement emulation, Other Ways to Access Globals-Other Ways to\nAccess Globals\nglobal variables, program design, Program Design: Minimize Global\nVariables-Program Design: Minimize Global Variables\nglobal, state retention and, Globals: Changeable but Shared\nlexical scoping, Python Scopes Basics, Imports Versus Scopes\nlocal scopes, Scopes Overview\nmodules\nimports and, How Files Generate Namespaces\nvariables, Imports Versus Scopes-Imports Versus Scopes\nnamed assignments, Preview: Other Python scopes",
      "content_length": 1223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1942,
      "chapter": null,
      "content": "nested class, Nested Classes: The LEGB Scopes Rule Revisited-Nested\nClasses: The LEGB Scopes Rule Revisited\nnested scopes, Nested Scopes Examples-Nested Scopes Examples\narbitrary nesting, Arbitrary Scope Nesting\nname assignments, Nested Scopes Overview\nname references, Nested Scopes Overview\nper-call scopes, Scopes Overview\nscript1.py, A First Script-A First Script\nscripting\nserver-side scripting, Internet and Web Scripting\nweb scripting, Internet and Web Scripting\nscripting language, Is Python a “Scripting Language”?\ncontrol language and, Is Python a “Scripting Language”?\nease of use and, Is Python a “Scripting Language”?\nshell tools and, Is Python a “Scripting Language”?\nscripts, Program Files, Part I, Getting Started\nexercise, Test Your Knowledge: Part I Exercises\nweb servers, Other Launch Options\nsearch tables, Why Use Built-in Objects?\nsearches\n__init__.py files, The Roles of __init__.py Files\nlists, Test Your Knowledge: Answers\npackages, Packages and the Module Search Path",
      "content_length": 993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1943,
      "chapter": null,
      "content": "selection, The Python Conceptual Hierarchy\n__self__ attribute, Why Use Pseudoprivate Attributes?\nsemicolons in statements, Test Your Knowledge: Part II Exercises, End-of-line is\nend of statement\nsend method, Extended generator function protocol: send versus next\nsequence, The Python Conceptual Hierarchy\nsequence assignments, Assignment Syntax Forms, Sequence Assignments-\nAdvanced sequence-assignment patterns\nsequence operations, “Changing” Strings Part 1: Sequence Operations,\n“Changing” Strings Part 1: Sequence Operations, Test Your Knowledge:\nAnswers\narbitrary expressions, Sequence Operations\nbyte strings, Sequence Operations-Formatting\nindexing expressions, Sequence Operations\nlist comprehensions, Conversions, methods, and immutability\nlists, Sequence Operations\nsequence patterns, Advanced match Usage\nsequence scans\nfor loop, Sequence Scans: while, range, and for-Sequence Scans: while,\nrange, and for\nrange object, Sequence Scans: while, range, and for-Sequence Scans: while,\nrange, and for\nwhile loop, Sequence Scans: while, range, and for-Sequence Scans: while,\nrange, and for\nsequences, Test Your Knowledge: Answers",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1944,
      "chapter": null,
      "content": "dictionaries, Dictionary Usage Tips\nfor loops, Tuple (sequence) assignment in for loops-Tuple (sequence)\nassignment in for loops\nlists, Types Share Operation Sets by Categories\npermutation, Permutating Sequences-Why generators here: Space, time, and\nmore\nreordering, Sequence Shufflers: range and len\nslicing, Scrambling Sequences\nfunctions, Simple functions\ngenerator expressions, Generator expressions\ngenerator functions, Generator functions-Generator functions\ntester, Tester client\nstrings, String Object Basics, Types Share Operation Sets by Categories\ntuples, Types Share Operation Sets by Categories, Tuple syntax\npeculiarities: Commas and parentheses\nvirtual sequences, Iterations\nzip object, Parallel Traversals: zip-More zip roles: dictionaries\nserialization\njson module, The pickle and json Serialization Modules-The pickle and\njson Serialization Modules\nobjects, Storing Objects with pickle\npickle module, The pickle and json Serialization Modules-The pickle and\njson Serialization Modules\nserver-side scripting, Internet and Web Scripting\nset comprehensions, Comprehensions versus type calls and generators",
      "content_length": 1120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1945,
      "chapter": null,
      "content": "syntax, Formal Comprehension Syntax\nset function, Sets\n__set__ method, Inserting Code to Run on Attribute Access\n__setattr__ method, Attribute Assignment and Deletion-Attribute Assignment\nand Deletion, Inserting Code to Run on Attribute Access\n__setitem__ method, Indexing and Slicing: __getitem__ and __setitem__,\nIntercepting Item Assignments\nsets, Numbers, Sets-Sets, Test Your Knowledge: Answers, Core Types Review\nand Summary\ncomparisons, Comparisons, Equality, and Truth\ndictionaries, Dictionary views and sets-Dictionary views and sets\nshared objects, Shared References\nshared references, Shared References\nargument passing, Arguments and Shared References-Arguments and\nShared References\nequality, Shared References and Equality-Shared References and Equality\nin-place changes, Shared References and In-Place Changes-Shared\nReferences and In-Place Changes\nmultiple-target assignments, Multiple-target assignment and shared\nreferences-Multiple-target assignment and shared references\nShed Skin, Python Implementation Alternatives, Test Your Knowledge: Answers\nshell programs\ncommand line, Command-Line Usage Variations\nstream redirection, Command-Line Usage Variations\nshell tools, Is Python a “Scripting Language”?, Systems Programming",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1946,
      "chapter": null,
      "content": "shell-command streams, Other File Tools\nshelve module, Pickles and Shelves, The shelve module\nobject storage, Storing Objects on a shelve Database-Exploring Shelves\nInteractively\nupdates, Updating Objects on a Shelf\nshelves, Other File Tools, Stream Processors Revisited\nshipping, Development Tools for Larger Projects\nsingle-scan iteration, Single versus multiple scans\nsite-packages directory, Search-Path Components\nslice assignments, Test Your Knowledge: Answers, Index and slice assignments-\nIndex and slice assignments, Other List Operations\nslice expressions, Intercepting Slices-Intercepting Slices\nslice objects, Intercepting Slices\nslices, Classes versus slices\ncleanup and, Extended slicing: The third limit and slice objects\nskipping items, Skipping Items: range and Slices\nslicing, Sequence Operations, String Object Basics, Test Your Knowledge: Part\nII Exercises, Part II, Objects and Operations\nlists, Indexing and Slicing-Indexing and Slicing\nout of bounds, Test Your Knowledge: Part II Exercises\nparsing, Indexing and Slicing\nsequences, Scrambling Sequences\nfunctions, Simple functions\ngenerator expressions, Generator expressions",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1947,
      "chapter": null,
      "content": "generator functions, Generator functions-Generator functions\ntester, Tester client\nstrings, Indexing and Slicing-Extended slicing: The third limit and slice\nobjects\nextended slicing, Extended slicing: The third limit and slice objects\nslots, Classes: Under the Hood, Namespace Dictionaries: Review, Slots:\nAttribute Declarations\nclass-level defaults, Slot usage rules\ndeclaring, Slot basics\ndescriptors, Descriptors and slots and more\ngeneric programs, Handling slots and other “virtual” attributes generically\nListTree class, Example impacts of slots: ListTree and mapattrs-Example\nimpacts of slots: ListTree and mapattrs\nmultiple inheritance, Slot usage rules\nmultiple inheritance classes, Example impacts of slots: ListTree and\nmapattrs\nnamespaces dictionaries and, Slot basics-Slots and namespace dictionaries\nsingle-inheritance trees, Example impacts of slots: ListTree and mapattrs\nspeed, What about slots speed?\nsubclasses, Slot usage rules\nsuperclasses, Slot usage rules\nusage rules, You shouldn’t normally use slots\n__slots__ attribute, Multiple __slot__ lists in superclasses\nsmartphone apps, And More: AI, Games, Images, QA, Excel, Apps…,\nSmartphone Apps",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1948,
      "chapter": null,
      "content": "SOAP, Component Integration\nsockets, Other File Tools\nsoftware quality of Python, Why Do People Use Python?, Software Quality\nsoftware reuse, Why Do People Use Python?\nspaces, indentation and, Why Indentation Syntax?\nspecial characters, escape sequences, Escape Sequences Are Special Characters-\nEscape Sequences Are Special Characters\nspecial methods, Static and Class Methods\n(see also static methods)\nspecial-case syntax, Advanced match Usage\nSphinx, Beyond Docstrings: Sphinx\nsplit method, More String Methods: Parsing Text\nSpyder, Other IDEs for Python\nSQLAlchemy, Database Access\nSQLite, Database Access\nSQLObject, Database Access\nStackless, Python Implementation Alternatives\nstandalone executables, Standalone Executables, Other Launch Options,\nStandalone Apps and Executables-Etcetera\nstandard library, Why Do People Use Python?, It’s Portable\ndirectories, module search path, Search-Path Components\nmethods, All String Methods (Today)\nmodules, Standard-Library Modules\nstandard manuals, The Standard Manuals-Web Resources",
      "content_length": 1031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1949,
      "chapter": null,
      "content": "standard output streams, print operations, Print Operations\nstandard streams, Other File Tools\nstartup-speed optimization, bytecode and, Bytecode compilation\nstate\ndescriptors, Using State Information in Descriptors-Using State Information\nin Descriptors\nevent handlers, Function Interfaces and Callback-Based Code\nexception classes, Exception Objects\nexceptions, built-in, Default Printing and State-Default Printing and State\ncustom, Custom State and Behavior-Providing Exception Methods\ninstances, Using State Information in Descriptors\nretaining, State with class-instance attributes-State with function attributes\nclasses, Classes: Changeable, Per-Call, OOP\nfunction attributes, Function Attributes: Changeable, Per-Call, Explicit\n-Function Attributes: Changeable, Per-Call, Explicit\nglobal scope, Globals: Changeable but Shared\nnonlocal statement, Nonlocals: Changeable, Per-Call, LEGB\nscope mutables, Function Attributes: Changeable, Per-Call, Explicit\nsuspension, State suspension-State suspension\nstate information, function objects, Function Attributes\nstatements, Introducing Python Statements-Python’s Statements\nas expression-based equivalents, How (Not) to Obfuscate Your Python\nCode-How (Not) to Obfuscate Your Python Code\nassignment statements, Python’s Statements",
      "content_length": 1280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1950,
      "chapter": null,
      "content": "assignments, Assignment Syntax Forms\nasync, Advanced Function Tools\nawait, Advanced Function Tools\nclass, User-Defined Objects, OOP: The Big Picture, The class Statement\nattributes, Example: Class Attributes-Example: Class Attributes\nsyntax, General Syntax and Usage-General Syntax and Usage\ncompound statements, Missing Keys: if Tests, What Python Adds, if and\nmatch Selections\nconcept hierarchy, The Python Conceptual Hierarchy Revisited\ndef, Basic Function Tools, def Statements, def Executes at Runtime, Calls\nexecutable\nfrom, Imports Are Runtime Assignments\nimport, Imports Are Runtime Assignments\nexpression statements, Expression Statements-Expression Statements\nexpressions in, The Python Conceptual Hierarchy\nfor, Basic Operations\nfrom, The from Statement\npotential problems, Potential Pitfalls of the from Statement-When\nimport is required\nfrom *, The from * Statement-The from * Statement\nfunction-related, Function Basics\nglobal, Advanced Function Tools\nnamespaces, The global Statement-The global Statement\nif statements, A Tale of Two ifs, if Statements",
      "content_length": 1067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1951,
      "chapter": null,
      "content": "multiple-choice selections, Multiple-Choice Selections-Handling\nlarger actions\nif/then/else, The if/else Ternary Expression-The if/else Ternary Expression\nimport, The import Statement-The import Statement\nindentation, Why Indentation Syntax?-Why Indentation Syntax?\ninteractive loops, A Simple Interactive Loop-Nesting Code Three Levels\nDeep\nmatch statements, Missing Keys: if Tests, match Statements\nattribute patterns, Advanced match Usage\nif statements, Match versus if live\ninstance patterns, Advanced match Usage\nliteral patterns, Advanced match Usage\nmapping patterns, Advanced match Usage\nnested patterns, Advanced match Usage\nnesting, Basic match Usage\nparentheses, Advanced match Usage\nsequence patterns, Advanced match Usage\nmodule imports, How Files Generate Namespaces\nnested statements\nblocks, What Python Adds\nspecial cases, Block rule special case-Block rule special case\nnonlocal, Advanced Function Tools, The nonlocal Statement-nonlocal\nBoundary Cases\norder, Statement Order Matters in Top-Level Code-Statement Order Matters",
      "content_length": 1041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1952,
      "chapter": null,
      "content": "in Top-Level Code\nparentheses, Parentheses are optional\nraise, Breaking Out of Multiple Nested Loops: “go to”\nreturn, Basic Function Tools, return Statements\nsemicolon, Test Your Knowledge: Part II Exercises\nsemicolons, End-of-line is end of statement\nspecial cases rules, Statement rule special cases-Statement rule special\ncases\nsyntax, Python’s Statements, Python Syntax Revisited\nblocks, Block Delimiters: Indentation Rules\ndelimiters, Statement Delimiters: Lines and Continuations\nindentation, Block Delimiters: Indentation Rules-Avoid mixing tabs\nand spaces\nspecial syntax, Special Syntax Cases in Action-Special Syntax Cases\nin Action\ntruth values, Truth Values Revisited-Truth Values Revisited\ntry, Handling Errors with try Statements-Handling Errors with try\nStatements\nyield, Advanced Function Tools, Iteration protocol integration\nstatic methods, Other Method-Call Possibilities, Static and Class Methods, Why\nthe Special Methods?, Using Static and Class Methods\nalternatives, Static Method Alternatives\ninstance counting, Counting Instances with Static Methods-Counting\nInstances with Static Methods\nstaticmethod function, Using Static and Class Methods",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1953,
      "chapter": null,
      "content": "statistically nested scopes, Nested Functions and Scopes\n(see also nested scopes)\nstdout, Test Your Knowledge: Answers\nSTEM (science, technology, engineering, and math), Why Do People Use\nPython?\nstorage\nclass, Miscellaneous Class Gotchas\nobjects\nCSV module, Storing Objects with Other Tools\nJSON, Storing Objects with JSON-Storing Objects with JSON\npickle module, Storing Objects with pickle-Storing Objects with pickle\nstruct module, Storing Objects with Other Tools\nper-instance, Miscellaneous Class Gotchas\nstr function, Numbers\n__str__ method, Why Two Display Methods?-Display Usage Notes\nstr object, The str Object\nstream processors, composition, Stream Processors Revisited\nstreams\nredirection, shells, Command-Line Usage Variations\nshell command, Other File Tools\nstring formatting, String Formatting: The Triathlon, Formatting\nexpressions, String-Formatting Options\nadvanced, Advanced formatting expression examples\ncustom formats, Formatting expression custom formats-Formatting",
      "content_length": 988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1954,
      "chapter": null,
      "content": "expression custom formats\ndictionary-based, Dictionary-based formatting expressions-Dictionary-\nbased formatting expressions\nformat method, String-Formatting Options, The String-Formatting Method\nadvanced, Advanced formatting method examples-Advanced\nformatting method examples\nattributes, Adding keys, attributes, and offsets\nbinary formats, Advanced formatting method examples\ncustom formats, Formatting method custom formats-Formatting\nmethod custom formats\nf-string literal, The F-String Formatting Literal-Advanced f-string\nexamples\nfloating-point numbers, Advanced formatting method examples\nformatspec component, Formatting method custom formats-\nFormatting method custom formats\nhex formats, Advanced formatting method examples\nkeys, Adding keys, attributes, and offsets\noctal formats, Advanced formatting method examples\noffsets, Adding keys, attributes, and offsets\nsubstitution targets, Formatting method custom formats\nliterals, String-Formatting Options\nmanual, String-Formatting Options\n__repr__ method, String Representation: __repr__ and __str__-Display\nUsage Notes\n__str__ method, String Representation: __repr__ and __str__-Display",
      "content_length": 1149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1955,
      "chapter": null,
      "content": "Usage Notes\nstring indexing, Test Your Knowledge: Part II Exercises, Part II, Objects and\nOperations\nstring interpolation, The F-String Formatting Literal\nstring literals, Literals and Basic Properties-Literals and Basic Properties\nbackslash escape sequences and, Other Ways to Code Strings\ndocstrings, Docstrings and __doc__\nformat, String Literals\noperations, String Object Basics\nquotes, Single and Double Quotes Are the Same-Single and Double Quotes\nAre the Same\nUTF-8 encoding, Source-File Encoding Declarations\nstring methods, String Methods, “Changing” Strings, Part 2: String Methods\nfind, “Changing” Strings, Part 2: String Methods\njoin, “Changing” Strings, Part 2: String Methods\nmethod calls, syntax, Method Call Syntax\nparsing text, More String Methods: Parsing Text\nreplace, “Changing” Strings, Part 2: String Methods, “Changing” Strings,\nPart 2: String Methods\nsplit, More String Methods: Parsing Text\nsubstring test, Other Common String Methods\nwhitespace, Other Common String Methods\nstring objects, The raise Statement\nstring representation methods, String Representation: __repr__ and __str__-",
      "content_length": 1111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1956,
      "chapter": null,
      "content": "Display Usage Notes\nstrings, Test Your Knowledge: Answers, String Fundamentals, Types Share\nOperation Sets by Categories, Using Files\nas immutable sequences, String Object Basics\nbackslash characters, Escape Sequences Are Special Characters-Escape\nSequences Are Special Characters\nbackslash escape, Other Ways to Code Strings\nblock strings, Triple Quotes and Multiline Strings-Triple Quotes and\nMultiline Strings\ncharacter arrays and, String Object Basics\ncode points, Escape Sequences Are Special Characters\ncombining, String Object Basics\ncomparisons, String comparisons, Comparisons, Equality, and Truth\nconcatenation, String Object Basics, Single and Double Quotes Are the\nSame, Basic Operations\nconversion, String Conversion Tools\ncharacter-code, Character-code conversions\nfloating-point numbers, String Conversion Tools\nstrings to numbers, String Conversion Tools\ndelimiter, “Changing” Strings, Part 2: String Methods\ndocstrings, Triple Quotes and Multiline Strings\nempty, Files in Action\nescape characters, Escape Sequences Are Special Characters-Escape\nSequences Are Special Characters\nexpressions, String Object Basics",
      "content_length": 1128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1957,
      "chapter": null,
      "content": "fetching by offset, String Object Basics\nhelp resources, Getting Help-Getting Help\nimmutability, Immutability-Immutability\nindexing, String Object Basics, Indexing and Slicing-Extended slicing: The\nthird limit and slice objects\niteration, Basic Operations\nloops and, Basic Operations\nmethods, Type-Specific Methods-Type-Specific Methods, String Object\nBasics\nmodules, String Object Basics\nmultiline, Triple Quotes and Multiline Strings-Triple Quotes and Multiline\nStrings\nnewline characters, Escape Sequences Are Special Characters\nNULL character, Escape Sequences Are Special Characters\noperations, Basic Operations-Basic Operations\noperator overload and, Basic Operations\npolymorphism and, Basic Operations\nquote characters, Other Ways to Code Strings\nraw, Raw Strings Suppress Escapes-Raw Strings Suppress Escapes\nsection extraction, String Object Basics\nsequence operations, “Changing” Strings Part 1: Sequence\nOperations-“Changing” Strings Part 1: Sequence Operations\narbitrary expressions, Sequence Operations\nindexing expressions, Sequence Operations",
      "content_length": 1057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1958,
      "chapter": null,
      "content": "sequences, String Object Basics\nslicing, String Object Basics, Indexing and Slicing-Extended slicing: The\nthird limit and slice objects\nextended slicing, Extended slicing: The third limit and slice objects\nsplit method, Storing Objects with Conversions\nstrip method, Storing Objects with Conversions\nsubstrings, testing for, Other Common String Methods\ntext strings, Unicode Strings\ntriple-quotes, Triple Quotes and Multiline Strings-Triple Quotes and\nMultiline Strings\nUnicode, Unicode Strings, String Fundamentals\nstruct module, Test Your Knowledge: Answers\nbinary data, The struct Binary-Data Module\nobject storage, Storing Objects with Other Tools\nstructural pattern matching, match Statements\n__sub__ method, operator overloading, Constructors and Expressions: __init__\nand __sub__\nsubclasses, Classes Are Customized by Inheritance\nmetaclasses, Metaclasses Are Subclasses of type\nslots, Slot usage rules\nsubclassing, Coding Subclasses, Part VI, Classes and OOP\nbuilt-in types extension, Extending Types by Subclassing-Extending Types\nby Subclassing\nextending code, Inherit, Customize, and Extend",
      "content_length": 1100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1959,
      "chapter": null,
      "content": "hierarchies, OOP: The Big Idea\ninheritance, Inherit, Customize, and Extend, The Inheritance Bifurcation\ninstances, namespace dictionaries, Namespace Dictionaries: Review\nmethods, Augmenting Methods: The Bad Way-Augmenting Methods: The\nGood Way\npolymorphism, Polymorphism in Action-Polymorphism in Action\nsubprime code, Loop else\nsubroutines, Why Use Functions?\nsubstrings, testing for, Other Common String Methods\nsum function, recursion and, Summation with Recursion-Summation with\nRecursion\nsuper built-in function, Augmenting Methods: The Good Way\nsuper function, The super Function, The super supplement\nargument lists, Same argument lists-Same argument lists\nattribute-fetch algorithm, Attribute-fetch algorithm\ncall-chain anchors, Call-chain anchors-Call-chain anchors\ncooperator method dispatch, Call-chain anchors-Call-chain anchors\ndeployment, Universal deployment\nMRO algorithm, A “magic” proxy-A “magic” proxy\nnoncalls, Noncalls and operator overloading-Noncalls and operator\noverloading\noperator overloading, Noncalls and operator overloading-Noncalls and\noperator overloading\nsuperclasses, Attribute Inheritance Search, Classes Are Customized by\nInheritance",
      "content_length": 1170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1960,
      "chapter": null,
      "content": "abstract, Abstract Superclasses-Preview: Abstract superclasses with library\ntools, Stream Processors Revisited\nlibrary tools, Preview: Abstract superclasses with library tools\nconstructors, Step 5: Customizing Constructors, Too, Miscellaneous Class\nGotchas\nexception superclass, Coding Exceptions Classes\nexceptions, Exception Classes\ninterfacing with, Class Interface Techniques-Class Interface Techniques\nmetaclasses comparison, Metaclass Versus Superclass-Metaclass Versus\nSuperclass\nmethod calls, Specializing Inherited Methods\noperator overloading method coding, Workaround: Coding operator-\noverloading methods in superclasses-Workaround: Coding operator-\noverloading methods in superclasses\nslots, Slot usage rules\n__slots__ attribute, Multiple __slot__ lists in superclasses\nsupport, Why Do People Use Python?\nSWIG, Component Integration\nsyntactic nesting, exception handlers, Example: Syntactic Nesting-Example:\nSyntactic Nesting\nsyntax\nargument matching, Argument Matching Syntax-Argument Matching\nSyntax\nassignments, Application to for loops\nclass statement, General Syntax and Usage-General Syntax and Usage",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1961,
      "chapter": null,
      "content": "combined clauses, try statement, Combined-clause syntax rules\ncompound statements, What Python Adds\ncomprehension syntax, Comprehensions and Generations\ndecorators, Why Decorators?\ndictionary comprehensions, Formal Comprehension Syntax\ndotted-path, Package Imports, Using the basic package\nrelative imports, Relative-Import Rationales and Trade-Offs\nfunction decorator, Property basics\nfunction decorators, Usage\ninstance generation, Classes Are Types Are Classes\nlambda expression, lambda Basics\nlist comprehensions, formal, Formal Comprehension Syntax\nmethod calls, Method Call Syntax\nset comprehensions, Formal Comprehension Syntax\nspecial-case, Advanced match Usage\nstatements, Python’s Statements, Python Syntax Revisited\nblock rule special cases, Block rule special case-Block rule special\ncase\nblocks, Block Delimiters: Indentation Rules-Avoid mixing tabs and\nspaces\ndelimiters, Statement Delimiters: Lines and Continuations\nindentation, End of indentation is end of block-Why Indentation\nSyntax?, Block Delimiters: Indentation Rules-Avoid mixing tabs and\nspaces",
      "content_length": 1069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1962,
      "chapter": null,
      "content": "nested statement blocks, What Python Adds\nparentheses, Parentheses are optional\nsemicolons, End-of-line is end of statement\nspecial cases rules, Statement rule special cases-Statement rule special\ncases\nspecial syntax, Special Syntax Cases in Action-Special Syntax Cases\nin Action\ntuples, Tuple syntax peculiarities: Commas and parentheses-Tuple syntax\npeculiarities: Commas and parentheses\nvariable names, Variable Name Rules\nsys.exc_info, More on sys.exc_info-The sys.exception alternative—and diss\nsys.platform, A First Script\nsystems programming, Systems Programming\nT\ntabs, indentation and, Why Indentation Syntax?\ntemplates, Internet and Web Scripting\nreplacements, “Changing” Strings, Part 2: String Methods\ntermination actions\ntry/finally and, Example: Coding termination actions with try/finally\nwith statement, The with Statement and Context Managers\ntermination handlers, The Termination-Handlers Shoot-Out-The Termination-\nHandlers Shoot-Out\ntestdriver function, Running In-Process Tests\ntester, slicing sequences, Tester client",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1963,
      "chapter": null,
      "content": "testing, And More: AI, Games, Images, QA, Excel, Apps…\nclass creation, Testing as You Go-Testing as You Go\ncomparisons, Comparisons, Equality, and Truth-Dictionary comparisons\nequality, Comparisons, Equality, and Truth\nin-process tests, exceptions, Running In-Process Tests\ninteractive tests, Test Your Knowledge: Part II Exercises\nuser input, error handling, Handling Errors by Testing Inputs-Handling\nErrors by Testing Inputs\ntesting tools, Development Tools for Larger Projects\ntext editors, Other Launch Options\nBOM (byte order marker), Making BOMs in Text Editors-Making BOMs in\nText Editors\nindentation, Why Indentation Syntax?\ntext files, Text and Binary Files: The Short Story-Text and Binary Files: The\nShort Story, Text and Binary Files-Text and Binary Files\nbinary mode, Text and Binary Modes\nbytes object, Text and Binary Modes\nconversions, Storing Objects with Conversions-Storing Objects with\nConversions\niterators, Using Files\nlines comparison, The Termination-Handlers Shoot-Out\npathname, Text-File Basics\ntext mode, Text and Binary Modes\nnewline character, Text and Binary Modes",
      "content_length": 1095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1964,
      "chapter": null,
      "content": "Unicode text files, Unicode-Text Files-Unicode-Text Files\ntext output file, Files\ntext strings\nliterals, Literals and Basic Properties-Literals and Basic Properties\nnon-ASCII characters, Unicode Strings\ntype conversions, String Type Conversions-String Type Conversions\nUnicode code-points, Unicode Strings\nUnicode text, Coding Unicode Strings in Python-Coding Unicode Strings in\nPython\ntext-mode files\nBOM headers, Using Text and Binary Files\nnewline character, Text and Binary Modes\ntextual data, Introducing Python String Tools\nthird-party domain, Why Do People Use Python?\nthird-party utilities, It’s Powerful\ntimeit module\nAPI-calls mode, API-calls mode\nbenchmarking automation, Automating timeit Benchmarking\nbenchmark module, Benchmark module -Benchmark script\nbenchmark script, Benchmark script-Timing individual Pythons\ndictionaries, Timing set and dictionary iterations-Conclusion:\nComparing tools\nindividual Pythons, Timing individual Pythons-Timing individual\nPythons",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1965,
      "chapter": null,
      "content": "multiple Pythons, Timing multiple Pythons\nsets, Timing set and dictionary iterations-Conclusion: Comparing tools\ncallable objects, Basic timeit Usage\ncommand-line mode, Command-line mode-Command-line mode\nmultiline statements, Handling multiline statements-Handling multiline\nstatements\nsetup code, Other timeit usage modes\nsort speed, Timing sort speed-Timing sort speed\nstatement strings, Basic timeit Usage\ntimer decorator, Adding Decorator Arguments\ntimer utility functions, Timer Module: Take 2\ntimestamp-based bytecode files, Step 2: Compile It (Maybe)\ntiming tools, Part IV, Functions and Generators\nToga, GUIs and UIs, Standalone Apps and Executables\ntools\nbuilt-in, It’s Powerful\nincrease in, The Python Tsunami\ntoolsets\nbuilt-in tools, The Python Toolset\ndevelopment tools, Development Tools for Larger Projects\ndebuggers, Development Tools for Larger Projects\ndocumentation tools, Development Tools for Larger Projects\nerror-checking tools, Development Tools for Larger Projects",
      "content_length": 989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1966,
      "chapter": null,
      "content": "IDEs, Development Tools for Larger Projects\ninstallation management, Development Tools for Larger Projects\noptimization, Development Tools for Larger Projects\nprofilers, Development Tools for Larger Projects\nshipping, Development Tools for Larger Projects\ntesting tools, Development Tools for Larger Projects\nextensions, The Python Toolset\ntraceback object, Displaying Errors and Tracebacks\ntriple-quoted strings, Triple Quotes and Multiline Strings-Triple Quotes and\nMultiline Strings\ntruth values, The Meaning of True and False in Python-The bool type, Truth\nValues Revisited-Truth Values Revisited\ntry statement\nbuilt-in exceptions, Example: Catching built-in exceptions\ncombined clauses, Combined try Clauses\nnesting, Combining finally and except by nesting\nsyntax, Combined-clause syntax rules\ndebugging, Debugging with Outer try Statements-Debugging with Outer try\nStatements\ndefault exception behavior, Example: Default behavior\nelse clause, The except and else Clauses-Example: Catching built-in\nexceptions\nerror handling, Handling Errors with try Statements-Handling Errors with\ntry Statements\nexcept clause, The except and else Clauses-Example: Catching built-in",
      "content_length": 1172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1967,
      "chapter": null,
      "content": "exceptions\nexcept* clause, Exception Groups: Yet Another Star!-Exception Groups:\nYet Another Star!\nfinally block, Combined try Clauses-Combined try Clauses\nfinally clause, The finally Clause-Example: Coding termination actions\nwith try/finally\nforms, try Statement Clauses\nhandlers, The try Statement\nnesting, Nesting Exception Handlers-Nesting Exception Handlers\ntuples, Catching many exceptions with a tuple\ntry/finally, Combined try Clauses-Combined try Clauses\nnesting, Combining finally and except by nesting\nsyntax, Combined-clause syntax rules-Combined-clause syntax rules\ntermination actions and, Example: Coding termination actions with\ntry/finally\ntuples, Tuples-Why Tuples?, Test Your Knowledge: Answers, Types Share\nOperation Sets by Categories, List-literal unpacking, Using dictionaries for\nsparse data structures: Tuple keys, Tuples-Tuples, Part II, Objects and\nOperations\nassignments, Assignment Syntax Forms\ncommas, Tuple syntax peculiarities: Commas and parentheses-Tuple syntax\npeculiarities: Commas and parentheses\ncommas in expressions, Test Your Knowledge: Part II Exercises\ncompared to lists, Why Lists and Tuples?\nconversions, Conversions, methods, and immutability-Conversions,\nmethods, and immutability",
      "content_length": 1228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1968,
      "chapter": null,
      "content": "converting from dictionaries, Records Revisited: Named Tuples\nfor loops, Tuple (sequence) assignment in for loops-Tuple (sequence)\nassignment in for loops\ngenerating, Test Your Knowledge: Answers\nimmutability, Conversions, methods, and immutability-Conversions,\nmethods, and immutability\nmethods, Conversions, methods, and immutability-Conversions, methods,\nand immutability\nnamed, Records Revisited: Named Tuples-Records Revisited: Named\nTuples\nparentheses, Tuple syntax peculiarities: Commas and parentheses-Tuple\nsyntax peculiarities: Commas and parentheses\nsequence assignment, Tuple syntax peculiarities: Commas and parentheses\nsorting, Conversions, methods, and immutability\nsyntax, Tuple syntax peculiarities: Commas and parentheses-Tuple syntax\npeculiarities: Commas and parentheses\ntry statement, Catching many exceptions with a tuple\nTurboGears, Internet and Web Scripting\ntype calls, comprehensions, Comprehensions versus type calls and generators\ntype conversions, text strings, String Type Conversions-String Type Conversions\ntype function, Types-Types\ntype hinting, Type Hinting, Type Hinting: Optional, Unused, and Why?-Type\nHinting: Optional, Unused, and Why?\ntype objects, Types-Types, Type Objects\nbuilt-ins, Other Types in Python",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1969,
      "chapter": null,
      "content": "type-specific operations, lists, Type-Specific Operations-Type-Specific\nOperations\ntypes, It’s Powerful\nbuilt-ins, extending, Extending Types by Embedding\nclasses and, The Python Object Model\nimmutable, Mutable Types Can Be Changed in Place, Test Your\nKnowledge: Part II Exercises\nmappings, Types Share Operation Sets by Categories\nmetaclass/class dichotomy, The Metaclass/Class Dichotomy-The\nMetaclass/Class Dichotomy\nmetaclasses\nclass statements, Class Statements Call a type-Class Statements Can\nChoose a type\ninstances, Classes Are Instances of type-Classes Are Instances of type\nsubclasses, Metaclasses Are Subclasses of type-Metaclasses Are\nSubclasses of type\nmutable, Mutable Types Can Be Changed in Place\nnumbers, Types Share Operation Sets by Categories\nobject types, Types Live with Objects, Not Variables\nhierarchy, Python’s Type Hierarchies\nsequences, Types Share Operation Sets by Categories\ntypes library module, Type Objects\nTypeScript, Type Hinting\ntyping, dynamic, It’s Powerful",
      "content_length": 995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1970,
      "chapter": null,
      "content": "U\nUIs (user interfaces), GUIs and UIs\nUnicode, Unicode and Byte Strings\ncharacter encodings, Character Encodings-Character Encodings\ncharacter representations, Character Representations-Character\nRepresentations\ncode points, Escape Sequences Are Special Characters, Character\nRepresentations\nvalue, Coding Unicode Strings in Python\ncode-points, Unicode Strings\ndefault encoding, Filenames in open and Other Filename Tools\nescapes, Coding Unicode Strings in Python\nfiles, Unicode and Byte Files-Unicode and Byte Files\nnormalization, Unicode Normalization: Whither Standard?-Unicode\nNormalization: Whither Standard?\nraw bytes, Coding Unicode Strings in Python\nstrings, Unicode Strings, String Fundamentals\ncode points, Unicode Strings\nstrings, coding, Coding Unicode Strings in Python-Coding Unicode Strings\nin Python\ntext files, Unicode-Text Files-Unicode-Text Files\nUTF-16, Escape Sequences Are Special Characters\nunion operation, dictionaries, Dictionary “Union” Operator, Dictionary “Union”\nOperator",
      "content_length": 1001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1971,
      "chapter": null,
      "content": "unit testing, __name__ attribute, Example: Unit Tests with __name__-Example:\nUnit Tests with __name__\nUnix, Python installation, Installing Python\nunpacking assignments, Assignment Syntax Forms, Extended-Unpacking\nAssignments-Extended unpacking in action\nboundary cases, Boundary cases-Boundary cases\nfor loops, Application to for loops\nupdate method, Missing Keys: if Tests\nusage mode flags, Dual-Usage Modes: __name__ and __main__\nuser inputs\nerror handling, Handling Errors by Testing Inputs-Handling Errors by\nTesting Inputs\nmath, Doing Math on User Inputs-Doing Math on User Inputs\nuser interfaces (UIs), GUIs and UIs\nuser-defined classes, Classes Are Types Are Classes-Classes Are Types Are\nClasses\nuser-defined docstrings, User-defined docstrings-User-defined docstrings\nuser-defined exceptions, nonerror conditions, Functions Can Signal Conditions\nwith raise\nuser-defined function decorators, A First Look at User-Defined Function\nDecorators-A First Look at User-Defined Function Decorators\nuser-defined iterables, User-Defined Iterables-Multiple iterators with yield\nUTF-16 encoding, Escape Sequences Are Special Characters, Character\nEncodings, Coding Unicode Strings in Python\nUTF-32 encoding, Character Encodings, Coding Unicode Strings in Python",
      "content_length": 1258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1972,
      "chapter": null,
      "content": "UTF-8 encoding, Character Encodings, Filenames in open and Other Filename\nTools\nBOM (byte order marker), Making BOMs in Python-Making BOMs in\nPython\nstring literals, Source-File Encoding Declarations\nutilities\nlibrary utilities, It’s Powerful\nthird-party, It’s Powerful\nV\nvalues method, Item Iteration: for Loops\nvariables, Running Code Interactively\n__all__, Minimizing from * Damage: _X and __all__-Minimizing from *\nDamage: _X and __all__\ncomprehensions, Preview: Other Python scopes\nscopes and, Scopes and comprehension variables-Scopes and\ncomprehension variables\ncreating, Variables, Objects, and References\nfrom * statement, from * Can Obscure the Meaning of Variables\nglobal\ncoupling, Function Design Concepts\nprogram design, Program Design: Minimize Global Variables-Program\nDesign: Minimize Global Variables\nstate, decorators and, State with global variables\nimports, Imports Versus Scopes-Imports Versus Scopes",
      "content_length": 921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1973,
      "chapter": null,
      "content": "local, Segue: Local Variables-Segue: Local Variables\nstatic locals, Function Attributes\nloops, Enclosing scopes and loop variables\nnames, Module Filenames\nnames, case, Variable Name Rules\nnaming rules, Variable Name Rules-Names have no type, but objects do\nreferences, Variables, Objects, and References\nscope, Python Scopes Basics\nscopes, Imports Versus Scopes-Imports Versus Scopes\ntypes, Variables, Objects, and References\nuse, Variables, Objects, and References\nview objects, Dictionary key/value/item view objects-Dictionary key/value/item\nview objects\nvirtual machines (VMs) (see VMs (virtual machines)\nvirtual sequences, Iterations\nvisibility, class attributes, Example: Class Attributes\nVMs (virtual machines), Python’s View\nPVM (Python Virtual Machine), The Python Virtual Machine (PVM)\nVSCode, Other IDEs for Python\nW\nWasm (WebAssembly), WebAssembly for Browsers\nweb scripting, Internet and Web Scripting\nweb server scripts, Other Launch Options",
      "content_length": 955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1974,
      "chapter": null,
      "content": "WebAssembly, GUIs and UIs, Internet and Web Scripting\nwhile loops, while Loops-Examples\nbreak statement, break-The named-assignment alternative\nnamed assignment, The named-assignment alternative\nbreak statements, break, continue, pass, and the Loop else\ncontinue statement, continue-The nested-code alternative\nnested code, The nested-code alternative\ndo until, Examples\nelse, General Format\nelse clause, Loop else-Why the loop else?\ninteractive loops, A Simple Interactive Loop\npass statement, pass-The ellipsis-literal alternative\nellipsis, The ellipsis-literal alternative\nsequence scans, Sequence Scans: while, range, and for-Sequence Scans:\nwhile, range, and for\nwhitespace, Other Common String Methods, End of indentation is end of block\nwide-character strings, Character Representations\nWindows\ndirectory paths, Files in Action\ninteractive coding and, Starting an Interactive REPL\nPython installation, Installing Python, Using Python on Windows\nPython on, Using Python on Windows\nCommand Prompt, Using Python on Windows-Using Python on\nWindows",
      "content_length": 1050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1975,
      "chapter": null,
      "content": "IDLE, Using Python on Windows-Using Python on Windows\nWSL, Using Python on Windows\nWing, Other IDEs for Python\nwith statement, The with Statement and Context Managers-Basic with Usage\ncontext managers, multiple, Multiple Context Managers-Multiple Context\nManagers\ncontext-management protocol, The Context-Management Protocol-The\nContext-Management Protocol\ntermination actions, The with Statement and Context Managers\ntermination handlers, The Termination-Handlers Shoot-Out\nworking directory, commands, Running Files with Command Lines\nwrappers, OOP and Delegation: “Like-a” Relationships, What Should Be\nWrapped\ncall proxies, Managing Calls and Instances\nclass decorators, Managing Calls and Instances\nlayers, Decorator Nesting\nstacking, Decorator Nesting\nwrapping code, “Overwrapping-itis”\nWSL (Windows Subsystem for Linux), Using Python on Windows\nwxPython, GUIs and UIs\nX\n_X prefix, Minimizing from * Damage: _X and __all__-Minimizing from *\nDamage: _X and __all__\n__X pseudoprivate name mangling, Pseudoprivate Class Attributes, Using",
      "content_length": 1040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1976,
      "chapter": null,
      "content": "“__X” pseudoprivate names\nXML-RPC, Component Integration\nY\nyield function, generator objects, Coding Alternative: __iter__ Plus yield-\nMultiple iterators with yield\nyield statement, Advanced Function Tools, Iteration protocol integration\ngenerator functions, The yield from extension-The yield from extension\nZ\nzip function\nemulating, Example: Emulating zip and map-Coding Your Own zip and 2.X\nmap\niterables, Coding Your Own zip and 2.X map\nzip object, Parallel Traversals: zip-More zip roles: dictionaries\niteration protocol, Reprise: Dictionaries, range, enumerate, and zip\nZODB, Database Access\nZope, Internet and Web Scripting",
      "content_length": 630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1977,
      "chapter": null,
      "content": "About the Author\nMark Lutz is the author of Python’s classic and foundational texts, a former\ntrainer with two decades of experience teaching Python to newcomers, and one\nof the people responsible for the prominence that Python enjoys today.\nMark wrote the three O’Reilly books Learning Python, Programming Python,\nand Python Pocket Reference, all currently in fourth, fifth, or sixth editions. He\nhas been using and promoting Python since 1992, started writing Python books\nin 1995, and began teaching Python classes in 1997.\nAll told, Mark has taught thousands of learners live and in person, and his\npublished works span 12k pages among 15 books that cover Pythons 1.X\nthrough 3.X. He also has BS and MS degrees in computer science, work\nexperience in compilers and other domains, and a current interest in Python-\ncoded apps that run on both PCs and phones.\nFor more author background, see Mark’s books-and-software website, learning-\npython.com.",
      "content_length": 950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1978,
      "chapter": null,
      "content": "Colophon\nThe animal on the cover of Learning Python, sixth edition, is a wood rat\n(Neotoma muridae). The wood rat lives in a wide range of conditions (mostly\nrocky, scrub, and desert areas) over much of North and Central America,\ngenerally at some distance from humans. Wood rats are good climbers, nesting\nin trees or bushes up to six meters off the ground; some species burrow\nunderground or in rock crevices or inhabit other species’ abandoned holes.\nThese grayish-beige, medium-size rodents are the original pack rats: they carry\nanything and everything into their homes, whether or not it’s needed, and are\nespecially attracted to shiny objects such as tin cans, glass, and silverware.\nMany of the animals on O’Reilly covers are endangered; all of them are\nimportant to the world.\nThe cover illustration is by Karen Montgomery, based on a 19th-century\nengraving from Cuvier’s Animals. The series design is by Edie Freedman, Ellie\nVolckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe\nMyriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 1151,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}