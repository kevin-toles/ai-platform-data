{
  "metadata": {
    "title": "Microservices Up and Running",
    "source_file": "Microservices Up and Running_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 10,
      "title": "Workspace Guidelines for a Superior Developer Experience",
      "start_page": 181,
      "end_page": 319,
      "summary": "module-aws-kubernetes/main.tf (node group IAM)\nresource \"aws_iam_role\" \"ms-node\" {\nresource \"aws_iam_role_policy_attachment\" \"ms-node-AmazonEKSWorkerNodePolicy\" {\nresource \"aws_iam_role_policy_attachment\" \"ms-node-AmazonEKS_CNI_Policy\" {\nresource \"aws_iam_role_policy_attachment\" \"ms-node-ContainerRegistryReadOnly\" {\npolicies because the nodes in our Kubernetes system will need to be able to provision\nIn EKS, a managed node group needs to\nWe could hardcode all of these parameters in our module, but instead we’ll use input\nto use the same Kubernetes module to create different kinds of environments.\nexample, a development environment can be set up to use minimal resources, while a\nAdd the Terraform code in Example 7-15 to the end of the module’s main.tf file to\nmodule-aws-kubernetes/main.tf (node group)\nresource \"aws_eks_node_group\" \"ms-node-group\" {\ncluster_name    = aws_eks_cluster.ms-up-running.name\nnode_group_name = \"microservices\"\naws_iam_role_policy_attachment.ms-node-AmazonEKSWorkerNodePolicy,\naws_iam_role_policy_attachment.ms-node-AmazonEC2ContainerRegistryReadOnly,\nenough here to be able to call this module from our sandbox environment and\ninstantiate a running Kubernetes cluster on the AWS EKS service.\nputs will return the values that are needed to connect to the node group once it’s run‐\nBut it’s also useful to provide those connection details in a configuration file for\nthe kubectl CLI that most operators use for Kubernetes management.\nOur last step is to generate a kubeconfig file that we’ll be able to use to connect to the\nAppend the code in Example 7-16 to your module’s main.tf file.\nChapter 7: Building a Microservices Infrastructure\nmodule-aws-kubernetes/main.tf (generate kubeconfig)\n# Create a kubeconfig file based on the cluster that has been created\n${aws_eks_cluster.ms-up-running.certificate_authority.0.data}\"\nserver: ${aws_eks_cluster.ms-up-running.endpoint}\nname: ${aws_eks_cluster.ms-up-running.arn}\ncluster: ${aws_eks_cluster.ms-up-running.arn}\nuser: ${aws_eks_cluster.ms-up-running.arn}\nname: ${aws_eks_cluster.ms-up-running.arn}\ncurrent-context: ${aws_eks_cluster.ms-up-running.arn}\n- name: ${aws_eks_cluster.ms-up-running.arn}\n- \"${aws_eks_cluster.ms-up-running.name}\"\nraform resource called local_file to create a file named kubeconfig.\nour Kubernetes cluster.\nthe EKS resources that we created in the module.\nWhen Terraform runs this block of code, it will create a kubeconfig file in a local\nWe’ll be able to use that file to connect to the Kubernetes environment\nthis populated configuration file and use it to connect to the cluster.\ntion file will make it a lot easier for you to connect to the cluster from your machine.\nWe’re almost done writing our Kubernetes service module; all that’s left is to define\nKubernetes module variables\nTo declare the variables for our Kubernetes module, create a file called variables.tf in\nyour module-aws-kubernetes repository and add the code in Example 7-17.\nmodule-aws-kubernetes/variables.tf\ndefault = \"microservices\"\nChapter 7: Building a Microservices Infrastructure\nOur AWS Kubernetes module is now fully written.\ncode by running the following Terraform commands:\nmodule-aws-kubernetes$ terraform fmt\nmodule-aws-kubernetes$ terraform init\nmodule-aws-kubernetes$ terraform validate\nGitHub, so that we can use this module in the sandbox environment:\n$ git commit -m \"kubernetes module complete\"\nWith the EKS module ready to go, we can go back to our sandbox Terraform file and\nCreate a sandbox Kubernetes cluster\nNow that our complex Kubernetes system is wrapped up in a simple module, the\nbox environment is defined in its own code repository and has its own Terraform file\nbut this time we’ll add a call to the Terraform module.\nple, we’ll just use those default values in our sandbox environment.\npass some of the output variables from our network module into this Kubernetes\nmodule so that it installs the cluster on the network we’ve just created.\nthose inputs, you’ll need to define the aws_region value for your installation.\nUpdate the main.tf file of your sandbox environment so that it uses the Kubernetes\nmodule \"aws-eks\" {\nms_namespace       = \"microservices\"\ncluster_subnet_ids = module.aws-network.subnet_ids\nnodegroup_subnet_ids     = module.aws-network.private_subnet_ids\ncreate a working EKS cluster.\nDon’t forget that you’ll need to use a tag to get the build\nFor example, you can run the following commands to create a 1.1 version\nbased infrastructure up and running, ready to run your microservices resiliently.\nChapter 7: Building a Microservices Infrastructure\nYou can test that the cluster has been provisioned by running the following AWS CLI\n$  aws eks list-clusters\ntime to release our services into our environment’s Kubernetes cluster.\nWe’ll continue to follow the module\npattern by creating a Terraform module for Argo CD that we can call to bootstrap the\nUnlike the other modules, we’ll be installing\nTo do that, we’ll need to let Terraform know that we’re using a different host.\nFor our Argo CD installation we’ll use a Kubernetes provider;\nthis enables Terraform to issue Kubernetes commands and install the application to\nWe’ll install this resource into the Kubernetes cluster rather than on the AWS\nCreate a file called main.tf file in the root directory of the module-argo-cd Git reposi‐\nAdd the code in Example 7-19 to set up the providers we\nneed for the installation.\nmodule-argo-cd/main.tf\ncluster_ca_certificate = base64decode(var.kubernetes_cluster_cert_data)\nhost                   = var.kubernetes_cluster_endpoint\nargs        = [\"token\", \"-i\", \"${var.kubernetes_cluster_name}\"]\ncluster_ca_certificate = base64decode(var.kubernetes_cluster_cert_data)\nhost                   = var.kubernetes_cluster_endpoint\nargs        = [\"token\", \"-i\", \"${var.kubernetes_cluster_name}\"]\nTo configure the Kubernetes provider, we’re using the properties of the EKS cluster\nThese properties let Terraform know it needs to use the\nLinux world, and is designed to make installation of Kubernetes-based applications\nAdd the code in Example 7-20 to\nChapter 7: Building a Microservices Infrastructure\nmodule-argo-cd/main.tf (Helm)\nresource \"kubernetes_namespace\" \"example\" {\nThis code creates a namespace for the Argo CD installation and uses the Helm pro‐\nCreate a file called variables.tf in your Argo CD module repository and add the code\nmodule-argo-cd/variables.tf\nvariable \"kubernetes_cluster_id\" {\nvariable \"kubernetes_cluster_cert_data\" {\nvariable \"kubernetes_cluster_endpoint\" {\nvariable \"kubernetes_cluster_name\" {\nWe need to define these variables so that we can configure the Kubernetes and Helm\nSo we’ll need to grab them from the Kubernetes module’s out‐\nthem to the GitHub repository so that we can use the module in our sandbox\nNow, as we’ve done before, we just need to call this module from our sandbox\nWe want the Argo CD installation to happen as part of our sandbox environment\nbootstrapping, so we need to call the module from the Terraform definition in our\nAdd the code in Example 7-22 to the end of your sandbox\nmodule’s main.tf file to install Argo CD.\nDon’t forget to use your module’s GitHub\nmodule \"argo-cd-server\" {\nkubernetes_cluster_id        = module.aws-eks.eks_cluster_id\nkubernetes_cluster_name      = module.aws-eks.eks_cluster_name\nkubernetes_cluster_cert_data = module.aws-eks.eks_cluster_certificate_data\nkubernetes_cluster_endpoint  = module.aws-eks.eks_cluster_endpoint\neks_nodegroup_id = module.aws-eks.eks_cluster_nodegroup_id\nChapter 7: Building a Microservices Infrastructure\nwon’t be a Kubernetes cluster for Argo CD to be deployed to.\nBefore we finish with our infrastructure implementation, it’s a good idea to run a test\nEarlier in this chapter, when we were creating the Terraform code for our Kubernetes\nmodule, we added a local file resource to create a kubeconfig file.\ndownload that file so that we can connect to the EKS cluster using the kubectl\nGitHub will package the artifact as a ZIP file, so after downloading it you’ll need to\nTo use it, you just need to set an environment variable called KUBECONFIG that points\nThis will let the Kubernetes command-line application know where to find it.\nFor example, if the kubeconfig file is in your ~/Downloads directory, use the following\nYou can test that everything runs as expected by issuing the following command:\ncall to the Kubernetes cluster we’ve just created.\nthat our cluster is up and running.\nAs a final test, we’ll check to make sure that Argo\nCD has been installed in the cluster.\nRun the following command to verify that the\nLater in the process, we’ll get a chance to use Argo CD, the Kubernetes cluster, and\nwritten, it will be easy to create our environment again when we need it.\nChapter 7: Building a Microservices Infrastructure\nraform declarative files, we can re-create it in the same way whenever we need it, so\nTo destroy the sandbox environment, use\n1. Navigate to the working directory of your sandbox environment code on your\nInstall the Terraform providers that our environment code uses (we’ll need these\nWhen it’s done all of the AWS resources that we created will be gone.\n6. To verify that the EKS resources have been removed, you can run the following\nAWS CLI command to list EKS clusters:\n$ aws eks list-clusters\nYou can also run the following commands to double-check that the other billable\nIf something has gone wrong, you’ll need to use the AWS console and remove the\nWe created a Terraform module for our software-defined\nmodule that instantiates an AWS EKS cluster for Kubernetes.\nmented a sandbox environment as code that uses all of these modules in a declarative,\nthe Terraform module pattern and some of the design decisions you’ll need to make\nwe may need additional infrastructure modules, but later in the book we’ll use pre‐\nIn Chapter 8, we’ll get back to our example microservices and start the work of devel‐\nChapter 7: Building a Microservices Infrastructure\nbases of other microservices.\nCreating a new microservice should be a quick and\nment workspace is set up and what practices teams use for creating code.\nstart both vanilla Docker as well as a lightweight Kubernetes locally.\nshow you an advanced example of containerization: how to install a local Cassandra\nstructure that’s ready for writing some microservices code.\nour code, when we get into the development phase of our implementation.\nCoding Standards and the Developer’s Setup\nthat a new developer unfamiliar to the code should be able to set up a microser‐\nNew microservices can be created quickly, easily, and predictably\nproper code templates for each supported tech stack, such as Java, Go, Node,\ncode, we should only expect to see the Docker runtime and Docker Compose on\nCoding Standards and the Developer’s Setup \nSetup should work regardless of whether a developer runs code on their own lap‐\nA microservices\nmicroservices environment you should see every team picking whatever language\n4. Running a single microservice and/or a subsystem of several ones should be equally\nLet’s say an airlines reservation system is implemented as three microservices.\ndeveloper should be able to check out any particular microservice individually\nMany AWS service alternatives can be installed via\ncode into a Docker container, but making a containerized coding environment\na. Even though the code runtime is containerized, developers must be able to\nUse a Dockerfile for building a container image, and Docker Compose for\nCoding Standards and the Developer’s Setup \nfacilitate painless data management in a microservices environment:\nc. Running database migrations should be part of the project launch (via Make\nd. Running database migrations must be automated and should be part of any\ne. It should be possible to indicate which migrations run on which environments\nc. Check out this example of using Node’s db-migrate-sql for a MySQL\nplatform/stack in which code is being developed (e.g., JUnit for Java).\ndeveloper of the service should not need to set anything up to get things going\nand should be able to easily run tests with a command like make test-all.\na higher level (e.g., an API that invokes microservices, or a UI), or in some\ndeveloper clones, they should know that by running make run they can bring that\nCoding Standards and the Developer’s Setup \n• start: Run the code.\n• build: Build the code (typically a container image).\n• lint: Run a linter to ensure conformance of coding style with defined\n• migrate: Run database migrations.\n• add-migration: Create a new database migration.\n• exec: Execute a custom command inside the code’s container.\nCheck out example microservices in Go and Node that follow the aforementioned\ndevelopment setups is code containerization with Docker.\nmake for running makefiles) should be the only expectation for a developer environ‐\na complete Docker toolset, or even single-node Kubernetes, if needed, on various\nThe easiest way to get Docker and Kubernetes on your macOS or Windows is still\na problem that Docker4Mac only allows you to install one Docker instance and one\nallows you to very quickly launch Ubuntu-based Docker hosts on your macOS or\ndocker                  Running           192.168.64.3     Ubuntu 20.04 LTS\nJava applications with Java-based database systems such as Cassandra, you’ll need\ndocker                  Running           192.168.64.3     Ubuntu 20.04 LTS\ndubuntu                 Running           192.168.64.4     Ubuntu 20.04 LTS\nmemory in the container or to launch a bash shell, use the following:\nNow that we have a functioning virtualized Linux via Multipass, installing Docker (or\nInstalling Docker\nYou can install Docker inside a container by following the usual Docker installation\nubuntu@dubuntu:~$ sudo apt-get install build-essential -y\nubuntu@dubuntu:~$ sudo apt-get remove docker \\\nubuntu@dubuntu:~$ sudo snap install docker\nAfter completing these steps, you should have a working Docker installation, but it\ngroup access to Docker, as shown in the following code.\nubuntu@dubuntu:~$ sudo groupadd docker\nubuntu@dubuntu:~$ sudo usermod -aG docker $USER\nubuntu@dubuntu:~$ docker ps\nubuntu@dubuntu:~$ docker version\nubuntu@dubuntu:~$ $ docker-compose --version\nTo test our new Docker setup, let’s now use it for bringing up a MySQL database with\nFirst, let’s create a mysql-stack.yml file with instructions for Docker Compose:\nInstalling Docker \nDocker container with the following:\nubuntu@dubuntu:~$ docker-compose -f mysql-stack.yml up -d\nubuntu@dubuntu:~$ docker ps\nponents, such as a local Cassandra database, should you need to do so.\nAdvanced Local Docker Usage: Installing Cassandra\nWe have already discussed how to use Docker Compose for running a containerized\nMySQL database, but let’s look now at a somewhat more complex example of running\ncloud native microservices development journey.\nFirst, create a docker-compose.yml file with the following content anywhere in the\nubuntu@dubuntu:~/cassandra$ docker-compose up -d\nubuntu@dubuntu:~/cassandra$ docker-compose ps\nubuntu@dubuntu:~/cassandra$ docker exec -it cassandra-seed cqlsh\nInstalling Kubernetes\nKubernetes is designed to be deployed on a cluster of servers.\nTo install Kubernetes locally with k3s, use the following:\nInstalling Kubernetes \nubuntu@dubuntu:~$ sudo k3s kubectl get nodes\ncific group, and will need to log in again just like we did with Docker:\nubuntu@dubuntu:~$ sudo snap install microk8s --classic\nAnd that’s pretty much all you need to have a functioning Kubernetes setup on a\ncalled Skaffold was developed to make building container images pluggable into\nWe will not use local Kubernetes in most coding examples in this book, but if you\ncheck out the Skaffold microservices repository that we created for demonstration\nDeveloping Microservices\nThe implementation of the microservices in this sam‐\ndevelopment environment for the microservices is properly set up and configured,\numbrella project—a way to execute multiple microservices together in a developer\n• Flights management\nour first two microservices can be ms-flights and ms-reservations!\nNow that we have the target microservices identified, we need to use the SEED(S)\n• The customer trying to book the flight\n• The flights management microservice: ms-flights\n• The reservations management microservice: ms-reservations\n1. When a customer interacts with the UI, the app needs to render a seating chart\n2. When a customer is finalizing a booking, the web app needs to reserve a seat for\nusually jobs for which a BFF API needs microservices.\nmore technical JTBDs, describes the needs between the BFF APIs and microservices:\n1. When the API is asked to provide a seating chart, the API needs ms-flights to pro‐\nvide a seating setup of the flight, so the API can retrieve availabilities and render\n2. When the API needs to render a seating chart, the API needs ms-reservations to\nprovide a list of already reserved seats so the API can add that data to the seating\n3. When the API is asked to reserve a seat, the API needs ms-reservations to fulfill\nthe reservation, so the API can reserve the seat.\nNote that we don’t let ms-flights call ms-reservations to assemble the seating chart,\nChapter 9: Developing Microservices\ncust -[#blue]-> app ++: \"Flight Seats Page\"\napp -[#blue]-> api ++ : flight.getSeatingSituation()\nmsf --> api: flight_id\napp-[#blue]->api ++: \"book the seat\"\nto get a flight_id from the ms-flights microservice.\nreturned, the API will then get the list of seats from ms-flights.\nshow occupied seats, it will separately query ms-reservations for existing reservations\non the flight.\nThis is entirely why ms-flights is not querying the\nlist of reserved seats from ms-reservations directly.\nTo fulfill this task, API will again need to auth and then call a microservice,\nms-reservations, returning the status, success, or failure to the app, based on the\nChapter 9: Developing Microservices\nWe will do this for both ms-flights and ms-reservations.\nFlights Microservice\nTo compile actions and queries for ms-flights:\n• Input: flight_no, departure_local_date_time (ISO8601 format and in the local\n• Response: A unique flight_id identifying a specific flight on a specific date.\nGet flight seating (the diagram of seats on a flight)\n• Input: flight_id\n• Response: Seat Map object in JSON format1\nReservations Microservice\nQuery already reserved seats on a flight\n• Input: flight_id\nReserve a seat on a flight\n• Input: flight_id, customer_id, seat_num\nwe’ll see what this specification for our two microservices could look like.\ntitle: Flights Management Microservice API\nAPI Spec for Flight Management System\nshould also describe the response JSON’s structure, containing flight_id, the origin\n/flights:\nGET http://api.example.com/v1/flights?\n- name: flight_no\nChapter 9: Developing Microservices\nflight_id:\nWhen you design the specification for the /flights/{flight_no}/seat_map end‐\n/flights/{flight_no}/seat_map:\nsummary: Get a seat map for a flight\nv1/flights/AA2532/datetime/2020-05-17T13:20/seats/12C\n- name: flight_no\nChapter 9: Developing Microservices\nSimilarly to the OAS of the flights microservice, the designs for the endpoints of the\ntitle: Seat Reservation System API\nChapter 9: Developing Microservices\nsummary: Get Reservations for a flight\nGet all reservations for a specific flight\n- name: flight_id\nflight_id:\nceed to the last step in the SEED(S) process: writing the code for the microservices.\nAs we implement the flights and reservations microservices, we will practice the prin‐\nThe reservations microservice will be implemented in Python and Flask,\nwhile the flights microservice will be implemented in Node/Express.js.\nImplementing the Data for a Microservice\nRedis for the reservations and MySQL for flights.\nwith the data for the reservations system microservice.\nChapter 9: Developing Microservices\nIn the reservations system, we need to be able to capture a set of seat reservations for\na flight, and reserve a seat if it is not already booked.\nflight_id (specific flight) where keys of the hash are the seat numbers on the flight\nand the value is the customer_id for the customer that the seat is already reserved for.\nwe need to know all reserved seats), and, very conveniently, a command that allows\nfect for us, since we typically do not want to allow double-booking a seat on a flight.\nKey Decision: Use Redis to Implement the Reservations Database\nUse Redis as the data store for reservations to leverage its unique simplicity and flexi‐\nLet’s see an example of reserving several seats on a flight uniquely identified with the\nuse the Redis CLI from the reservations microservice’s workspace by invoking make\nThe HSETNX command we use here sets the value of the HSET key, we indicate, to the\nImplementing the Data for a Microservice \nLet’s see how we would get all of the occupied seats on a specific flight:\nLet’s now see what happens if we try to double-book an already reserved seat, such as\nTo demonstrate this, in the next section, we will implement the data for the ms-flights\nChapter 9: Developing Microservices\nMySQL Data Model for the Flights Microservice\nThe first data model we need here should contain seat maps.\nfor the flights microservice, the seat map is a complex JSON object.\nAdditionally, in the lookup endpoint we need to query data by two fields: flight_no\nCREATE TABLE `seat_maps`  (\nAnother table we need is the mapping of flight_ids with flight_nos and date\nCREATE TABLE `flights`  (\nREFERENCES seat_maps(flight_no)\nImplementing the Data for a Microservice \nINSERT INTO `seat_maps`(`flight_no`, `seat_map`, `origin_code`, /\n`destination_code`) VALUES ('AA2532', '{\\\"Cabin\\\": [{\\\"Row\\\": [{\\\"Seat\\\": /\nNow that we have a working data model for both of our microservices, we can dive\nImplementing Code for a Microservice\nNode.js-implemented flights microservice we’ll use a popular bootstrapper, Node‐\nFor the Python-based reservation microservice we’re going to use a Git‐\nHub template repository that contains most of the boilerplate code that we’ll need.\nUse code templates to jump-start a microservice development in each programming\nDocker installation and the GNU Make, since we use both of them.\nChapter 9: Developing Microservices\nEdit Code on a Host Run Inside Containers\nAs you work on a containerized project, your favorite code editor would be installed\nThe Code Behind the Flights Microservice\nTo use NodeBootstrap for jump-starting a Node/Express microservice, either install\nThe nodebootstrap-microservice’s main repository page\nOnce you have created a new repository for the ms-flights microservice, at the desti‐\nImplementing Code for a Microservice \ndocker run -d --rm --name ms-nb-docs -p 3939:80 -v \\\nms-flights/docs/api.yml:/usr/share/nginx/html/swagger.yaml \\\nRendered OAS of the ms-flights microservice\nThe Nodebootstrap microservice comes with a sample “users” module, located under\nSince we don’t need a user management module and do need a\nLikewise, change the hookup for the flights module in the same file, so that the line\napp.use('/flights', require('flights')); // Attach to sub-route\nChapter 9: Developing Microservices\nthe microservice that will be invoked for each of your two API endpoint routes:\nconst flightNoValidation = check('flight_no',\nrouter.get('/:flight_no/seat_map', seatmapsValidator, actions.getSeatMap);\nFirst we need to create MySQL tables and some\nWe can create several database migrations with some make commands, as follows:\n→ make migration-create name=seat-maps\nms-flights-db is up-to-date\nStarting ms-flights ...\ndocker-compose -p msupandrunning exec ms-flights\nImplementing Code for a Microservice \n./node_modules/db-migrate/bin/db-migrate create seat-maps --sql-file\n[INFO] Created migration at /opt/app/migrations/20200602055112-seat-maps.js\n[INFO] Created migration up sql file at\n[INFO] Created migration down sql file at\n→ make migration-create name=flights\nms-flights-db is up-to-date\nms-flights is up-to-date\ndocker-compose -p msupandrunning exec ms-flights\n./node_modules/db-migrate/bin/db-migrate create flights --sql-file\n[INFO] Created migration at /opt/app/migrations/20200602055121-flights.js\n[INFO] Created migration up sql file\nat /opt/app/migrations/sqls/20200602055121-flights-up.sql\n[INFO] Created migration down sql file\nat /opt/app/migrations/sqls/20200602055121-flights-down.sql\n→ make migration-create name=sample-data\nms-flights-db is up-to-date\nms-flights is up-to-date\ndocker-compose -p msupandrunning exec ms-flights\n./node_modules/db-migrate/bin/db-migrate create sample-data --sql-file\n[INFO] Created migration up sql file at\n[INFO] Created migration down sql file at\nCREATE TABLE `seat_maps` (\nChapter 9: Developing Microservices\n/migrations/sqls/[date]-flights-up.sql\nCREATE TABLE `flights`  (\nREFERENCES seat_maps(`flight_no`)",
      "keywords": [
        "Kubernetes",
        "microservices",
        "flight",
        "code",
        "Docker",
        "aws",
        "Kubernetes module",
        "Ubuntu",
        "Flights Microservice",
        "seat",
        "AWS EKS cluster",
        "Kubernetes cluster",
        "API",
        "type",
        "EKS"
      ],
      "concepts": [
        "microservices",
        "flight",
        "developer",
        "docker",
        "code",
        "coding",
        "kubernetes",
        "types",
        "module",
        "run"
      ],
      "similar_chapters": [],
      "enriched_keywords": {
        "tfidf": [
          "module",
          "flights",
          "ms",
          "kubernetes",
          "microservices"
        ],
        "semantic": [],
        "merged": [
          "module",
          "flights",
          "ms",
          "kubernetes",
          "microservices"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.9999999902872041,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:07:43.747082+00:00"
      }
    }
  ],
  "total_chapters": 1,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Microservices Up and Running_metadata.json",
    "enrichment_date": "2025-12-17T23:07:43.750061+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 48.04712499935704,
    "total_similar_chapters": 0
  }
}