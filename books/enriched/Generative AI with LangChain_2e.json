{
  "metadata": {
    "title": "Generative AI with LangChain_2e",
    "source_file": "Generative AI with LangChain_2e_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "summary": "Ben Auffarth | Leonid Kuligin <packt\nGenerative AI with LangChain\nGenerative AI with LangChain\nNeither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \nPackt Publishing has endeavored to provide trademark information about all of the companies and products \nDr. Ben Auffarth, PhD, is an AI implementation expert with more than 15 years of work \nenterprises implement enterprise-grade AI solutions that deliver tangible ROI.\nLeonid Kuligin is a staff AI engineer at Google Cloud, working on generative AI and classical \none of the key maintainers of Google Cloud integrations on LangChain and a visiting lecturer at \nMax Tschochohei advises enterprise customers on how to realize their AI and ML ambitions \nAs an engineering manager in Google Cloud Consulting, he leads teams of AI \nWhile his work spans the full range of AI prod-\nucts and solutions in the Google Cloud portfolio, he is particularly interested in agentic systems, \nmachine learning operations, and healthcare applications of AI.\nRany ElHousieny is an AI Solutions Architect and AI Engineering Manager with over two \ndecades of experience in AI, NLP, and ML.\nment and deployment of AI models, authoring multiple articles on AI systems architecture and \nethical AI deployment.\nCurrently, he plays a pivotal role at Clearwater Analytics, driving innovation in generative AI and \nAI-driven financial and investment management solutions.\nNicolas Bievre is a Machine Learning Engineer at Meta with extensive experience in AI, recom-\nmender systems, LLMs, and generative AI, applied to advertising and healthcare.\nUniversity, where he published peer-reviewed research in leading AI and bioinformatics journals.\nan AI consultant to the French government and as a reviewer for top AI organizations.\nHave questions about the book or want to contribute to discussions on Generative AI and LLMs?\nChapter 1: The Rise of Generative AI: From Language Models to Agents \b\nUnderstanding LLM applications • 10\nUnderstanding AI agents • 11\nHow LangChain enables agent development • 16\nExploring the LangChain architecture • 17",
      "keywords": [
        "Packt Publishing",
        "Google Cloud",
        "Build production-ready LLM",
        "Edition Ben Auffarth",
        "Ben Auffarth",
        "Leonid Kuligin",
        "production-ready LLM applications",
        "Google",
        "LangChain",
        "Leonid Kuligin Generative",
        "Generative",
        "book",
        "Google Cloud Consulting",
        "LLM applications",
        "LangChain Build production-ready"
      ],
      "concepts": [
        "langchain",
        "max",
        "systems",
        "ben",
        "manager",
        "agents",
        "lead",
        "production",
        "products",
        "enterprise"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 52,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 50,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 51,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.653,
          "base_score": 0.653,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "google cloud",
          "generative",
          "leonid",
          "kuligin"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "google cloud",
          "generative",
          "leonid",
          "kuligin"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3217703203399858,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302125+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "summary": "Model interfaces • 32\nDevelopment testing • 33\nReasoning models • 36\nReducers • 73\nfew-shot prompting • 87\nChaining prompts together • 88\nLangGraph checkpoints • 101\nWhen to implement RAG • 112\nEmbeddings • 114\nRetrieval • 137\nRe-ranking • 141\nCorrective RAG • 155\nAgentic RAG • 157\nDocument retrieval • 166\nEvaluation and performance considerations • 177\nTools in LangChain • 185\nReACT • 188\nBuilt-in LangChain tools • 192\nCustom tools • 199\nControlled generation • 210\nToolNode • 213\nPlan-and-solve agent • 217\nAgentic RAG • 226\nLangGraph streaming • 241\nHandoffs • 243\nLangGraph platform • 247\nCache • 263\nStore • 264\nThe future of development • 269\nLangChain integrations • 281\nAnthropic • 287\nAgentic approach • 289\nDocumentation RAG • 290\nRepository RAG • 293\nSetting up a Python-capable agent • 297\nAgent execution and results • 299\nAsking questions about the dataset • 303\nBuilding consensus for LLM evaluation • 315\nTool usage evaluation • 317\nRAG evaluation • 317\nPlanning and reasoning evaluation • 318\nAutomated evaluation approaches • 320\nSystem-level evaluation • 322\nEvaluating agent trajectory • 330\nEvaluating CoT reasoning • 334\nEvaluating RAG systems • 336\nBuilding the index • 359\nRunning the application • 363\nLangGraph platform • 370\nLangSmith • 387\nOther strategies • 394",
      "keywords": [
        "Table of Contents",
        "RAG",
        "LLM",
        "RAG Systems",
        "Evaluation",
        "Model",
        "Contents",
        "LLM applications",
        "agent",
        "Evaluating LLM agents",
        "LLM agents",
        "Evaluating RAG systems",
        "LangChain",
        "Evaluating",
        "Building Intelligent RAG"
      ],
      "concepts": [
        "evaluation",
        "evaluate",
        "evaluating",
        "agents",
        "model",
        "rag",
        "tool",
        "approaches",
        "approach",
        "llm"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.764,
          "base_score": 0.764,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 22,
          "title": "",
          "score": 0.666,
          "base_score": 0.666,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.595,
          "base_score": 0.595,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.575,
          "base_score": 0.575,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rag",
          "evaluation",
          "evaluating",
          "contents",
          "317"
        ],
        "semantic": [],
        "merged": [
          "rag",
          "evaluation",
          "evaluating",
          "contents",
          "317"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4370813569413067,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.302220+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "summary": "to sophisticated code generation systems, generative AI has rapidly transformed from a research \ntotypes and production-ready AI applications.\nfor generative AI is high, over 30% of projects fail to move beyond proof of concept due to reli-\nThis book is designed to help you close that gap.\nThrough hands-on examples and tested patterns using LangChain, \nLangGraph, and other tools in the growing generative AI ecosystem, you’ll learn to build systems \nWho this book is for\nThis book is primarily written for software developers with basic Python knowledge who want \nto build production-ready applications using LLMs. You don’t need extensive machine learning \nBy the end of the book, you’ll be confidently implementing advanced LLM architectures \nIf you’re a data scientist transitioning into LLM application development, you’ll find the practi-\nThe book’s structured approach to RAG implementation, \nFor technical decision-makers evaluating LLM technologies within their organizations, this book \nchitectural patterns that differentiate experimental systems from production-ready ones, learn \nThe book provides clear criteria for evaluating implementation \nWhat this book covers\nChapter 1, The Rise of Generative AI, From Language Models to Agents, introduces the modern LLM \nlandscape and positions LangChain as the framework for building production-ready AI applica-\nYou’ll learn about the practical limitations of basic LLMs and how frameworks like LangC-\nChapter 2, First Steps with LangChain, gets you building immediately with practical, hands-on exam-\nYou’ll set up a proper development environment, understand LangChain’s core components \nChapter 3, Building Workflows with LangGraph, dives into creating complex workflows with LangC-\nYou’ll learn to build workflows with nodes and edges, including conditional \nChapter 4, Building Intelligent RAG Systems, addresses the “hallucination problem” by ground-\nChapter 5, Building Intelligent Agents, tackles tool use fragility—identified as a core bottleneck \nYou’ll implement the ReACT pattern to improve agent reasoning and deci-\npractical examples like generating structured outputs and building a research agent, you’ll under-\nChapter 6, Advanced Applications and Multi-Agent Systems, covers architectural patterns for agentic \nAI applications.\nChapter 7, Software Development and Data Analysis Agents, demonstrates how natural language has \nYou’ll implement LLM-based \nshow how to integrate LLM agents into existing development and data workflows, illustrating \nChapter 8, Evaluation and Testing, outlines methodologies for assessing LLM applications before \nThe chapter provides practical examples for implementing \nreliability and help justify the business value of your LLM applications.\nChapter 9, Observability and Production Deployment, provides guidelines for deploying LLM appli-\nstrategies specific to LLMs. You’ll explore the Model Context Protocol (MCP) and learn how to \nimplement observability practices that address the unique challenges of deploying generative AI \nThe practical deployment patterns in this chapter help you avoid common pitfalls that \nChapter 10, The Future of LLM Applications, looks ahead to emerging trends, evolving architectures, \ndeploying production-ready, future-proof AI systems.\nTo get the most out of this book\neach chapter.\nthropic, or other LLM providers will allow you to work with more powerful models.\nchapter builds on concepts introduced earlier, so working through them sequentially will \nproduction-readiness aspects that distinguish this book.\nVarious LLM providers (Anthropic, Google, OpenAI, local models)\nThe code bundle for the book is hosted on GitHub at https://github.com/benman1/generative_\nai_with_langchain.\nWe also have other code bundles from our rich catalog of books and videos available at https://\nFor issues on GitHub, see https://github.com/benman1/generative_ai_with_langchain/\nIf you have questions about the book’s content, or bespoke projects, feel free to contact us at ben@\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of ",
      "keywords": [
        "LLM",
        "book",
        "’ll",
        "LLM applications",
        "applications",
        "LLMs",
        "’ll learn",
        "systems",
        "code",
        "building",
        "LangChain",
        "practical LLM applications",
        "generative",
        "LLM application development",
        "agent"
      ],
      "concepts": [
        "llm",
        "agents",
        "chapters",
        "implementation",
        "implementations",
        "implement",
        "providers",
        "provides",
        "likely",
        "models"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.79,
          "base_score": 0.64,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.672,
          "base_score": 0.672,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.651,
          "base_score": 0.651,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 50,
          "title": "",
          "score": 0.633,
          "base_score": 0.633,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "chapter",
          "llm",
          "ll",
          "learn"
        ],
        "semantic": [],
        "merged": [
          "book",
          "chapter",
          "llm",
          "ll",
          "learn"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4216761868675078,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302294+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 25-33)",
      "start_page": 25,
      "end_page": 33,
      "summary": "Language Models to Agents\nexecuting its philosophy behind agentic AI – it allows a developer to give a large language model \ntems and how frameworks like LangChain and LangGraph transform these models into pro-\nThe Rise of Generative AI: From Language Models to Agents\nFrom models to agentic applications\nture, which enabled models to process text with unprecedented understanding of context and \nAs researchers scaled these models from millions to billions of parameters, they \nThe landscape shifted again with the open-source revolution led by models like Llama and Mistral, \nThis gap between raw model power \nthese models from impressive text generators into functional, production-ready agents capable \nTools: External utilities or functions that AI models can use to interact with the world.\nLarge-scale statistical models\nStatistical models dominate\nLarge Multimodal Models \nAI models process text, images, and audio\nAI models learn from direct human feedback, optimizing their performance to align \nThe Rise of Generative AI: From Language Models to Agents\nOpen-weight, large-scale AI model\nTable 1.1: A timeline of major developments in language models\nThe field of LLMs is rapidly evolving, with multiple models competing in terms of performance, \ngeneral-purpose AI to Mistral’s open-weight, high-efficiency models.\nModel comparison\nclosed-source models: Open-source models like Mistral and LLaMA pro-\nvide transparency and the ability to run locally, while closed-source models like GPT-4 and \nSize and capabilities: Larger models generally offer better performance but require more \nThis makes smaller models great for use on devices with limited \nSpecialized models: Some LLMs are optimized for specific tasks, such as code generation \nThe increase in the scale of language models has been a major driving force behind their impressive \nModel scaling laws\nand fitting of model performance with varied data sizes, model sizes, and training \ntween model performance and factors such as model size, dataset size, and training \nFor example, models such \n1 billion parameters, showed that models can – despite a smaller scale – achieve \nmodels to leverage the capabilities of large foundations without replicating their \nmore accessible models that provide faster and cheaper training, maintenance, and \nThe Rise of Generative AI: From Language Models to Agents\nclosed-source models, \nmodel size and capabilities, and specialized models.\nmerous providers have entered the space, each offering models with unique capabilities and \nthese powerful models into their applications.\nmodels they offer:\nNotable models\nStrong general performance, proprietary models, \nEnterprise-scale AI models, optimized for the AWS cloud\nrunning open models\nTable 1.2: Comparative overview of major LLM providers and their flagship models for \nLlama model series, which has strong reasoning, code-generation capabilities, and is released \nThere is a whole zoo of open-source models that you can access through Hugging Face or through \nYou can even download these open-source models, fine-tune them, or fully train \nWhile accessing a model gives you computa-\ntional capability, it’s the choice of generation parameters that transforms raw model power into \nThe licensing terms of different models significantly \nLLMs are available under different licensing models that impact how they can be used in practice.\nOpen-source models like Mixtral and BERT can be freely used, modified, and integrated into \nThese models allow developers to run them locally, investigate their behavior, and \nIn contrast, proprietary models like GPT-4 and Claude are accessible only through APIs, with their \nThe Rise of Generative AI: From Language Models to Agents\nSome models like Llama 2 take a middle ground, offering permissive licenses for both research \nthe model openness framework: https://isitopen.ai/.\nthe models, benefiting both research and commercial development.\nbasic language models to more complex, and finally, fully agentic applications.\nFrom models to agentic applications\nThe model openness framework (MOF) evaluates language models based on cri-\nteria such as access to model architecture details, training methodology and hy-\ndevelopment decisions, ability to evaluate model workings, biases, and limitations, \nThe information provided about AI model licensing is for educational purposes only ",
      "keywords": [
        "Models",
        "Language Models",
        "LLMs",
        "Agents",
        "LLM",
        "Language",
        "Generative",
        "Open-source models",
        "performance",
        "capabilities",
        "model size",
        "applications",
        "Rise of Generative",
        "size",
        "open-source"
      ],
      "concepts": [
        "models",
        "performance",
        "perform",
        "agents",
        "llms",
        "capabilities",
        "capable",
        "capability",
        "developer",
        "developments"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 51,
          "title": "",
          "score": 0.828,
          "base_score": 0.678,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 50,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.703,
          "base_score": 0.703,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.68,
          "base_score": 0.68,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.651,
          "base_score": 0.651,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "source models",
          "open",
          "model",
          "source"
        ],
        "semantic": [],
        "merged": [
          "models",
          "source models",
          "open",
          "model",
          "source"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38279524694972034,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302347+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 34-43)",
      "start_page": 34,
      "end_page": 43,
      "summary": "toward agentic AI—systems that can plan, reason, and take action to accomplish tasks with \nBefore exploring the potential of agentic AI, it’s important to first \nLack of true understanding: LLMs generate human-like text by predicting the next most \nThe Rise of Generative AI: From Language Models to Agents\nTo overcome these limitations, AI systems must evolve from passive text generators into active \nagents that can plan, reason, and interact with their environment.\nThis is where agentic AI comes \nin—integrating LLMs with tool use, decision-making mechanisms, and autonomous execution \nWhile frameworks like LangChain provide comprehensive solutions to LLM limitations, un-\nThe next section explores how agentic AI extends the capabilities of traditional LLMs and unlocks \nLLM applications represent the bridge between raw model capability and practical business \ncomplex integrated applications and autonomous agents.\nComplex integrated applications enhance human workflows by integrating LLMs into existing \nAutonomous agents operate with minimal human intervention, further augmenting workflows \nTask automation agents that execute defined workflows\nMulti-agent systems for complex task coordination\nLangChain provides frameworks for both integrated applications and autonomous agents, offer-\nAutonomous systems of agents are potentially very powerful, and it’s therefore worthwhile ex-\nUnderstanding AI agents\nAn AI agent represents the bridge between raw cognitive capability and practical action.\nAI agents transform this passive capability into active utility through structured \nAgentic AI enables autonomous systems to make decisions and act independently, with minimal \nUnlike deterministic systems that follow fixed rules, agentic AI relies on \nThe Rise of Generative AI: From Language Models to Agents\nThe distinction between raw AI and agents parallels the differ-\nIn the context of LLMs, agentic AI involves developing systems that act autonomously, understand \nThese AI agents leverage LLMs to process information, generate responses, and execute tasks \nParticularly, AI agents extend the capabilities of LLMs by integrating memory, tool use, and de-\nThese agents can:\nprompting an LLM for every request, an agent can proactively execute tasks, react to new data, \nAI agents are systems designed to act on behalf of users, leveraging LLMs alongside external tools, \nThe hope behind AI agents is that they can automate \nsystems to act autonomously, agents promise to unlock new levels of automation in AI-driven \nDespite their potential, AI agents face significant challenges:\nReliability: Ensuring agents make correct, context-aware decisions without supervision \nCoordination complexity: Multi-agent systems often suffer from inefficiencies and mis-\nProduction-ready agent systems must address not just theoretical challenges but practical im-\nLangChain and LangSmith provide robust solutions for these challenges, which we’ll explore in \nWhen developing agent-based systems, therefore, several key factors require careful consideration:\nValue generation: Agents must provide a clear utility that outweighs their costs in terms \nWhile early AI systems focused on pattern matching and predefined templates, modern AI agents \nToday’s AI agents integrate LLMs with interactive environments, enabling them to function au-\nThe development of agent-based AI is a natural progression from statistical models to deep learn-\nModern AI agents leverage multimodal capabilities, \nLooking ahead, AI agents will continue to refine their ability to reason, plan, and act within struc-\nin agent-based AI, will likely drive the next wave of innovations in AI, expanding its applications \nThe Rise of Generative AI: From Language Models to Agents\nWith frameworks like LangChain, developers can build complex and agentic structured systems \nthat overcome the limitations of raw LLMs. It offers built-in solutions for memory management, \nAI agents.\nsolutions for LLM application development.\nappreciating why frameworks like LangChain have become indispensable tools for AI developers.\nhurdles for developers building real-world applications:\nThese constraints directly impact application architecture, making techniques like RAG \nTask coordination challenges: Managing multi-step workflows with LLMs requires \nLangChain addresses these challenges by providing a structured framework with tested solutions, \nsimplifying AI application development and enabling more sophisticated use cases.\nThe Rise of Generative AI: From Language Models to Agents\nHow LangChain enables agent development\nLangChain provides the foundational infrastructure for building sophisticated AI applications \nComposable workflows: The LangChain Expression Language (LCEL) allows develop-\nThis lets you build applications that can easily switch between providers without \nMemory and state management: For applications requiring persistent context across \nAgent architecture: Though LangChain contains agent implementations, LangGraph has \nbecome the preferred framework for building sophisticated agents.\nprehensive ecosystem that transforms LLMs from simple text generators into systems capable \nallowing developers to integrate LLMs seamlessly into various applications.\nlarity extends beyond LLMs to include numerous building blocks for developing complex \ngenerative AI applications.\nSupport for agentic workflows: LangChain offers best-in-class APIs that allow you to \ndevelop sophisticated agents quickly.\nThese agents can make decisions, use tools, and \ntion, and deployment of generative AI applications, including robust building blocks for \nLangChain manages model integration and workflows, while LangGraph handles stateful \nIn Chapter 3, we’ll explore LangChain and LangGraph’s memory mechanisms.\nTo translate model design principles into practical tools, LangChain has developed a comprehen-\neverything they need to build, deploy, and maintain sophisticated AI applications.\nThe Rise of Generative AI: From Language Models to Agents\nAI application development space, particularly for building reasoning-focused LLM applications.\nThe framework’s modular architecture (with components like LangGraph for agent workflows \nand LangSmith for monitoring) has clearly resonated with developers building production AI \nLangChain (Python): Reusable components for building LLM applications\nLangGraph (Python): Tools for building LLM agents as orchestrated graphs\nLangGraph.js: JavaScript implementation for agent workflows\nThe ecosystem provides a complete solution for building reasoning-focused AI applications: from ",
      "keywords": [
        "Agents",
        "LLMs",
        "LangChain",
        "applications",
        "systems",
        "LLM",
        "LLM applications",
        "Language Models",
        "models",
        "tools",
        "workflows",
        "complex",
        "framework",
        "Rise of Generative",
        "language"
      ],
      "concepts": [
        "agents",
        "llms",
        "applications",
        "application",
        "developing",
        "systems",
        "models",
        "tool",
        "tasks",
        "human"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.79,
          "base_score": 0.64,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "",
          "score": 0.711,
          "base_score": 0.711,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.703,
          "base_score": 0.703,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.661,
          "base_score": 0.661,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.652,
          "base_score": 0.652,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "agents",
          "ai agents",
          "llms",
          "applications"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "agents",
          "ai agents",
          "llms",
          "applications"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4072838221685224,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302402+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 44-52)",
      "start_page": 44,
      "end_page": 52,
      "summary": "Organizations report using LangChain and LangSmith \nLangChain also offers a full stack for AI application development:\nBased on our experience building with LangChain, here are some of its benefits we’ve found \nAccelerated development cycles: LangChain dramatically speeds up time-to-market \nSuperior observability: The combination of LangChain and LangSmith provides unpar-\nProduction-ready patterns: Our implementation experience has proven that LangChain’s \nLangChain’s package architecture evolved as a direct response to scaling challenges.\nlazy loading of dependencies, LangChain elegantly solved these issues while preserving a cohesive \nlangchain-core/: Foundational abstractions and interfaces that define the framework\nlangchain/: The main implementation library with core components:\nlangchain-experimental/: Cutting-edge features still under development\nlangchain-community: Houses third-party integrations maintained by the LangChain \nchain-openai, langchain-anthropic) to enhance independent support.\nreside outside the LangChain repository but within the GitHub “langchain-ai” organiza-\ntion (see github.com/orgs/langchain-ai).\nA full list is available at python.langchain.\nsql-mssql package, are developed and maintained outside the LangChain ecosystem.\nLangChain’s core functionality is extended by the following companion projects:\nwith LLMs. While it integrates smoothly with LangChain, it can also be used independent-\nLangSmith: A platform that complements LangChain by providing robust debugging, \nhensive LangChain API reference: https://api.python.langchain.com/.\npython.langchain.com/v0.1/docs/use_cases/.\nLangChain also has an extensive array of tool integrations, which we’ll discuss in detail in Chapter \nMany third-party applications have been built on top of or around LangChain.\nLangChain and similar tools can be deployed locally using libraries like Chainlit, or \nIn summary, LangChain simplifies the development of LLM applications through its modular \nThis chapter introduced the modern LLM landscape and positioned LangChain as a powerful \nframework for building production-ready AI applications.\nWe also examined the LangChain ecosystem’s \nLangChain, translating the conceptual understanding from this chapter into working code.\nand how does LangChain address each one?\nWhat is the difference between a LangChain chain and a LangGraph agent?\nExplain how LangChain’s modular architecture supports the rapid development of AI ap-\nWhat are the key components of the LangChain ecosystem, and how do they work to-\n8.\t How does LangChain help address common challenges like hallucinations, context lim-\nitations, and tool integration that affect all LLM applications?\nExplain how the LangChain package structure (langchain-core, langchain, langchain-\ncommunity) affects dependency management and integration options in your applications.\nWhat role does LangSmith play in the development lifecycle of production LangChain \nFirst Steps with LangChain\nIn the previous chapter, we explored LLMs and introduced LangChain as a powerful framework \nIn this chapter, we’ll move from theory to practice by building our first LangChain application.\nLangChain’s core components, and creating simple chains.\nyou’ll have a solid foundation in LangChain’s building blocks and be ready to create increasingly \nExploring LangChain’s building blocks (model interfaces, prompts and templates, and \nFirst Steps with LangChain\ngithub.com/benman1/generative_ai_with_langchain.\nconda create -n langchain-book python=3.11\nconda activate langchain-book\npip install langchain langchain-openai jupyter\nGiven the rapid evolution of both LangChain and the broader AI field, we maintain \ncom/benman1/generative_ai_with_langchain.\nThis approach provides a clean, isolated environment for working with LangChain.\ngithub.com/benman1/generative_ai_with_langchain.\nOnce you are finished, please make sure you have LangChain version 0.3.17 installed.\ncheck this with the command pip show langchain.\ncode in this book is tested with LangChain 0.3.17, but newer versions may introduce ",
      "keywords": [
        "LangChain",
        "applications",
        "LLM",
        "LLM applications",
        "LLMs",
        "development",
        "integrations",
        "packages",
        "dependencies",
        "framework",
        "building",
        "ecosystem",
        "GitHub",
        "’ll",
        "LangChain ecosystem"
      ],
      "concepts": [
        "langchain",
        "developer",
        "integration",
        "integrations",
        "integrates",
        "integrated",
        "packages",
        "dependency",
        "depend",
        "provides"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 45,
          "title": "",
          "score": 0.586,
          "base_score": 0.586,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.566,
          "base_score": 0.566,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.511,
          "base_score": 0.511,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "langchain",
          "langchain ecosystem",
          "langchain langchain",
          "ecosystem",
          "langchain core"
        ],
        "semantic": [],
        "merged": [
          "langchain",
          "langchain ecosystem",
          "langchain langchain",
          "ecosystem",
          "langchain core"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3167685958917737,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302453+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 53-60)",
      "start_page": 53,
      "end_page": 60,
      "summary": "API key setup\nLangChain’s provider-agnostic approach supports a wide range of LLM providers, each with \nOPENAI_API_KEY\nANTHROPIC_API_KEY\nGOOGLE_API_KEY\nai.google.dev/gemini-api\nTable 2.1: API keys reference table (overview)\nMost providers require an API key, while cloud providers like AWS and Google Cloud also support \nTo set an API key in an environment, in Python, we can execute the following lines:\nos.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\nHere, OPENAI_API_KEY is the environment key that is appropriate for OpenAI.\nSetting the keys in \nevery time you use a model or service integration.\nexport OPENAI_API_KEY=<your token>\nTo permanently set the environment variable in Linux or macOS, you would need to add the \nexport OPENAI_API_KEY=your_key_here.\nOur choice is to create a config.py file where all API keys are stored.\nfrom this module that loads these keys into the environment variables.\nOPENAI_API_KEY =  \"...\nif \"API\" in key or \"ID\" in key:\nos.environ[key] = value\n**/api_keys.txt\nYou can set all your keys in the config.py file.\nThis function, set_environment(), loads all the \nkeys into the environment as mentioned.\nfrom config import set_environment\nWhile OpenAI’s models remain influential, the LLM ecosystem has rapidly diversified, offering \nthe model gateways that provide access to them.\nGoogle Gemini: Advanced multimodal models with industry-leading 1M token \nAmazon Bedrock: Unified API access to models from Anthropic, AI21, Cohere, Mis-\nAzure OpenAI Service: Enterprise-grade access to OpenAI and other models with \nGoogle Vertex AI: Access to Gemini and other models with seamless Google Cloud \nThroughout this book, we’ll work with various models accessed through different providers, giving \nto the Appendix at the end of the book to learn how to get API keys for OpenAI, Hugging Face, \nGoogle, and other providers.\nlangchain-google-vertexai\nand API key.\nkeys, virtual private cloud integration, and more, requiring a Google Cloud account \nTo build practical applications, we need to know how to work with different model providers.\nModel interfaces\nLangChain provides a unified interface for working with various LLM providers.\nmakes it easy to switch between different models while maintaining a consistent code structure.\nThe LLM interface represents traditional text completion models that take a string input and \nLangChain documentation is now deprecating the LLM interface and recommending the use of \nchat models as they represent the current standard to be up to date with LangChain.\nfrom langchain_openai import OpenAI\nfrom langchain_google_genai import GoogleGenerativeAI\n# Initialize OpenAI model\n# Initialize a Gemini model\nas most model providers have adopted a chat-like interface for interacting with \nWe still provide the LLM interface, because it’s very easy to use \nresponse = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\nPlease note that you must set your environment variables to the provider keys when you run this.\nFor example, when running this I’d start the file by calling set_environment() from config:\nfrom config import set_environment\nFor the Gemini model, we can run:\nit easy to experiment with different models or switch providers in production.\nWorking with chat models\nChat models are LLMs that are fine-tuned for multi-turn interaction between a model and a hu-\ninput to the model, such as:\nModel providers typically don’t store the chat history server-side, they get the full history \nSystemMessage: Sets behavior and context for the model.\nHumanMessage(content=\"Write a Python function to calculate factorial\")\nHere’s a Python function that calculates the factorial of a given number:",
      "keywords": [
        "API",
        "API key",
        "API keys",
        "key",
        "models",
        "OpenAI",
        "Environment",
        "LLM",
        "LangChain",
        "keys",
        "Google",
        "factorial",
        "function",
        "API key setup",
        "providers"
      ],
      "concepts": [
        "model",
        "provider",
        "provide",
        "langchain",
        "google",
        "openai",
        "keys",
        "messages",
        "response",
        "integration"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.568,
          "base_score": 0.568,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 6,
          "title": "",
          "score": 0.509,
          "base_score": 0.359,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.426,
          "base_score": 0.426,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.408,
          "base_score": 0.408,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "keys",
          "api",
          "providers",
          "openai_api_key",
          "api key"
        ],
        "semantic": [],
        "merged": [
          "keys",
          "api",
          "providers",
          "openai_api_key",
          "api key"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2082387963087166,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302495+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 61-70)",
      "start_page": 61,
      "end_page": 70,
      "summary": "from langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nchain = template | chat\nresponse = chat.invoke([HumanMessage(content=problem)])\nThe response will include Claude’s step-by-step reasoning about algorithm selection, complexity \nfrom langchain_core.prompts import ChatPromptTemplate\nchain = template | chat\nresponse = chain.invoke({\"problem\": \"Calculate the optimal strategy \nchain = template | chat\nresponse = chain.invoke({\"problem\": \"Calculate the optimal strategy \nThe reasoning_effort parameter streamlines your workflow by eliminating the need for complex \nDeepSeek models also offer explicit thinking configuration through the LangChain integration.\nUnderstanding how to control an LLM’s behavior is crucial for tailoring its output to specific needs.\nThese parameters work together to shape model output:\nLangChain provides a consistent interface for setting these parameters across different LLM \nfactual_llm = OpenAI(temperature=0.1, max_tokens=256)\ncreative_llm = OpenAI(temperature=0.8, top_p=0.95, max_tokens=512)\nbrainstorming tools, higher temperatures produce more diverse outputs, especially when paired \nadjust based on your specific application needs and observed outputs.\nPrompts and templates\nPrompt engineering is a crucial skill for LLM application development, particularly in production \nLangChain provides a robust system for managing prompts with features that \nTemplate systems for dynamic prompt generation\nLangChain’s prompt templates transform static text into dynamic prompts with variable substi-\ndef generate_prompt(question, context=None):\nprompt_text = generate_prompt(\"What is the capital of \nfrom langchain_core.prompts import PromptTemplate\n# Generate prompts by filling in variables\nprompt_text = question_template.format(question=\"What is the capital \nConsistency: They standardize prompts across your application.\nTestability: It is easier to unit test prompt generation separately from LLM calls.\nIn production applications, you’ll often need to manage dozens or hundreds of prompts.\nChat prompt templates\nFor chat models, we can create more structured prompts that incorporate different roles:\nfrom langchain_core.prompts import ChatPromptTemplate\nLet’s start by looking at LangChain Expression Language (LCEL), which provides a clean, intu-\nitive way to build LLM applications.\nLangChain Expression Language (LCEL)\nLCEL represents a significant evolution in how we build LLM-powered applications with Lang-\nChain.\nIntroduced in August 2023, LCEL is a declarative approach to constructing complex LLM \nOriginally, LangChain implemented this pattern through specific Chain classes like LLMChain \nof the more flexible and powerful LCEL approach, which is built upon the Runnable interface.\nLCEL adheres to this interface, which provides consistent methods including:\nThis standardization means any Runnable component—whether it’s an LLM, a prompt template, \nLLM, a prompt template, a document retriever, or a custom function—can be connected to any \nThe consistency of this interface enables complex applications to be built from \nLCEL truly shines when you need to build complex applications that combine multiple compo-\nThe pipe operator (|) serves as the cornerstone of LCEL, allowing you to chain components se-\nBasic sequential chain: Just prompt to LLM\nbasic_chain = prompt | llm | StrOutputParser()\nHere, StrOutputParser() is a simple output parser that extracts the string response from an LLM.\nIt takes the structured output from an LLM and converts it to a plain string, making it easier to \nLangChain applications:\ntion of LCEL-defined chains.\nchain = RunnableSequence(first= prompt, middle=[llm], last= output_parser)\nwith_transformation = prompt | llm | (lambda x: x.upper()) | \ndecision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {\nchain = prompt | length_func | output_parser\nchain = prompt | RunnableLambda(length_func) | output_parser\nThe flexible, composable nature of LCEL will allow us to tackle real-world LLM application chal-\nAs we’ve seen, LCEL provides a declarative syntax for composing LLM application components \nLet’s build a simple joke generator to see LCEL in action:\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nprompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n# Chain them together using LCEL\nchain = prompt | llm | output_parser\nWithout LCEL, the same workflow is equivalent to separate function calls with manual data \nllm_output = llm.invoke(formatted_prompt)\nresult = output_parser.invoke(llm_output)\nComplex chain example\nWhile the simple joke generator demonstrated basic LCEL usage, real-world applications typ-\nGenerate content with one LLM call\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nllm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")",
      "keywords": [
        "LCEL",
        "LLM",
        "LangChain",
        "Prompt",
        "chain",
        "output",
        "Runnable",
        "template",
        "model",
        "question",
        "applications",
        "Claude",
        "response",
        "thinking",
        "tokens"
      ],
      "concepts": [
        "prompts",
        "models",
        "chain",
        "output",
        "applications",
        "application",
        "llm",
        "langchain",
        "tokens",
        "response"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 14,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.593,
          "base_score": 0.593,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lcel",
          "chain",
          "prompt",
          "prompts",
          "llm"
        ],
        "semantic": [],
        "merged": [
          "lcel",
          "chain",
          "prompt",
          "prompts",
          "llm"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4032515870981458,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302555+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 71-78)",
      "start_page": 71,
      "end_page": 78,
      "summary": "# First chain generates a story\nstory_chain = story_prompt | llm | StrOutputParser()\nanalysis_chain = analysis_prompt | llm | StrOutputParser()\ninto the analysis chain:\nstory_with_analysis = story_chain | analysis_chain\n# Run the combined chain\nstory=story_chain  # Add 'story' key with generated content\nprint(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])  \nprint(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])\nsimple_dict_chain = story_chain | {\"analysis\": analysis_chain}\nresult = simple_dict_chain.invoke({\"topic\": \"a rainy day\"}) print(result.\nWhile our previous examples used cloud-based models like OpenAI and Google’s Gemini, Lang-\nChain’s LCEL and other functionality work seamlessly with local models as well.\nRunning local models\nWhen building LLM applications with LangChain, you need to decide where your models will run.\nAdvantages of local models:\nControl over model parameters and fine-tuning\nAdvantages of cloud models:\nWhen to choose local models:\nLet’s start with one of the most developer-friendly options for running local models.\nOllama provides a developer-friendly way to run powerful open-source models locally.\na simple interface for downloading and running various open-source models.\nThen pull a model.\n# Initialize Ollama with your chosen model\n# Create an LCEL chain using the local model\nlocal_chain = prompt | local_llm | StrOutputParser()\n# Use the chain with your local model\nresult = local_chain.invoke({\"concept\": \"quantum computing\"})\nThis LCEL chain functions identically to our cloud-based examples, demonstrating LangChain’s \nPlease note that since you are running a local model, you don’t need to set up any keys.\nan approachable way to run models locally, with access to a vast ecosystem of pre-trained models.\nWorking with Hugging Face models locally\nWith Hugging Face, you can either run a model locally (HuggingFacePipeline) or on the Hug-\nllm = HuggingFacePipeline.from_model_id(\nchat_model = ChatHuggingFace(llm=llm)\nai_msg = chat_model.invoke(messages)\nLangChain supports running models locally through other integrations as well, for example:\nmodels efficiently on consumer hardware.\nGPT4All: GPT4All offers lightweight models that can run on consumer hardware.\nChain’s integration makes it easy to use these models as drop-in replacements for cloud-\nAs you begin working with local models, you’ll want to optimize their performance and handle \nout of your local deployments with LangChain.\nTips for local models\nWhen working with local models, keep these points in mind:\nResource management: Local models require careful configuration to balance perfor-\nOllama model for efficient operation:\n#  Configure model with optimized memory and processing settings\nError handling: Local models can encounter various errors, from out-of-memory condi-\ndef safe_model_call(llm, prompt, max_retries=2):\n\"\"\"Safely call a local model with retry logic and graceful\n# Common error with local models when running out of VRAM\nprint(f\"Unexpected error calling model: {e}\")\nCommon local model errors you might run into are as follows:\nOut of memory: Occurs when the model requires more VRAM than available\nContext length errors: When input exceeds the model’s maximum token limit\nhain applications that leverage local models effectively while maintaining a good user experience \nFigure 2.1: Decision chart for choosing between local and cloud-based models",
      "keywords": [
        "model",
        "story",
        "chain",
        "local models",
        "local",
        "analysis",
        "LangChain",
        "llm",
        "Write a short",
        "Ollama",
        "topic",
        "chain analyzes",
        "LCEL chain",
        "LCEL",
        "prompt"
      ],
      "concepts": [
        "models",
        "llm",
        "story",
        "error",
        "run",
        "running",
        "runs",
        "prompt",
        "analysis",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 36,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.607,
          "base_score": 0.457,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "local",
          "local models",
          "models",
          "story",
          "chain"
        ],
        "semantic": [],
        "merged": [
          "local",
          "local models",
          "models",
          "story",
          "chain"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3643691446330922,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302611+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 79-86)",
      "start_page": 79,
      "end_page": 86,
      "summary": "of data, LangChain provides interfaces for both generating images from text and understanding \nMultimodal understanding represents the ability of models to process multiple types of inputs \nderstand the relationships between different modalities, accepting inputs like text, images, PDFs, \na chart image along with a text question to provide insights about the data trend, combining \nText-to-image models create visual \ncontent from descriptions, text-to-video systems generate video clips from prompts, text-to-\naudio tools produce music or speech, and image-to-image models transform existing visuals.\nExamples include Midjourney, DALL-E, and Stable Diffusion for images; Sora and Pika for video; \nUnlike true multimodal models, many generation systems \nLet’s start with generating images from text descriptions.\nprovides several approaches to incorporate image generation through external integrations and \nText-to-image\nLangChain integrates with various image generation models and services, allowing you to:\nGenerate images from text descriptions\nEdit existing images based on text prompts\nControl image generation parameters\nLangChain includes wrappers and models for popular image generation services.\nhow to generate images with OpenAI’s DALL-E model series.\nLangChain’s wrapper for DALL-E simplifies the process of generating images from text prompts.\nfrom langchain_community.utilities.dalle_image_generator import \nn=1                     # Number of images to generate (only for \n# Generate an image\n# Display the image in a notebook\nfrom IPython.display import Image, display\nresponse = requests.get(image_url)\nFigure 2.2: An image generated by OpenAI’s DALL-E Image Generator\nYou might notice that text generation within these images is not one of the strong suites of these \nYou can find a lot of models for image generation on Replicate, including the latest Stable \nStable Diffusion 3.5 Large is Stability AI’s latest text-to-image model, released in March 2024.\nIt’s a Multimodal Diffusion Transformer (MMDiT) that generates high-resolution images with \n# Initialize the text-to-image model with Stable Diffusion 3.5 Large\n# Generate an image\nimage_url = text2image.invoke(\nFigure 2.3: An image generated by Stable Diffusion\nNow let’s explore how to analyze and understand images using multimodal models.\nImage understanding\nImage understanding refers to an AI system’s ability to interpret and analyze visual information \nGemini 2.5 Pro and GPT-4 Vision, among other models, can analyze images and provide detailed \nthe image we generated using Stable Diffusion:\n{\"type\": \"text\", \"text\": \"Describe the image: \"},\n[{\"type\": \"image_url\",\n\"image_url\": {\"url\": \"data:image/jpeg;base64,{image_bytes_str}\"},\nprompt.invoke({\"image_bytes_str\": \"test-url\"})\nAfter having explored image generation, let’s examine how LangChain handles image under-\nand GPT-4o-mini) allow us to analyze images alongside text, enabling applications that can “see” \nLet’s implement a flexible image analyzer:\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": image_url,",
      "keywords": [
        "image",
        "Stable Diffusion",
        "LangChain",
        "text",
        "models",
        "url",
        "multimodal",
        "image generation",
        "Diffusion",
        "content",
        "video",
        "prompt",
        "type",
        "Stable",
        "Stable Diffusion models"
      ],
      "concepts": [
        "image",
        "models",
        "text",
        "langchain",
        "generate",
        "generation",
        "generated",
        "generator",
        "prompts",
        "types"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.641,
          "base_score": 0.641,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.604,
          "base_score": 0.604,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.473,
          "base_score": 0.473,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.46,
          "base_score": 0.46,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.446,
          "base_score": 0.446,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "image",
          "images",
          "text",
          "stable",
          "diffusion"
        ],
        "semantic": [],
        "merged": [
          "image",
          "images",
          "text",
          "stable",
          "diffusion"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3001639075211319,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.302659+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 87-95)",
      "start_page": 87,
      "end_page": 95,
      "summary": "print(f\"A: {analyze_image(image_url, question)}\")\nThis capability opens numerous possibilities for LangChain applications.\nIn the next chapter, we’ll build on these \nplored the foundations of LangChain development, from basic chains to multimodal capabilities.\nWe’ve seen how LCEL simplifies complex workflows and how LangChain integrates with both \nticated error handling, and build applications that leverage the full potential of modern LLMs. Review questions\nName three types of memory systems available in LangChain\nCompare and contrast LLMs and chat models in LangChain.\nBuilding Workflows with \nSo far, we’ve learned about LLMs, LangChain as a framework, and how to use LLMs with LangC-\nwe’ll start with a quick introduction to LangGraph as a framework and how to develop more \ncomplex workflows with LangChain and LangGraph by chaining together multiple steps.\nexample, we’ll discuss parsing LLM outputs and look into error handling patterns with LangChain \nwhat building blocks LangChain offers for few-shot prompting and other techniques.\ncode samples, and develop our own complex workflows.\nworkflows are and will continue building on that skill in Chapters 5 and 6.\nBuilding Workflows with LangGraph\nLangGraph is a framework developed by LangChain (as a company) that helps control and or-\nuntil Chapter 5, where we’ll touch on agents and agentic workflows, but for now, let us mention \npre-built loops and components dedicated to generative AI applications (for example, human \nmend a free online course on LangGraph that is available at https://academy.langchain.com/ \nter notebooks: https://github.com/benman1/generative_ai_with_langchain/\nnode and returns to the same node by following the directed edges.\nDAGs are often used as a model of workflows in data engineering, where nodes are \nState management is crucial in real-world AI applications.\nLangGraph’s state management lets you maintain this context across a complex \nworkflow of multiple AI components.\nLangGraph allows you to develop and execute complex workflows called graphs.\nwords graph and workflow interchangeably in this chapter.\nA graph consists of nodes and edges \nNodes are components of your workflow, and a workflow has a state.\nFirstly, a state makes your nodes aware of the current context by keeping track of the user input \nSecondly, a state allows you to persist your workflow execution at \nThirdly, a state makes your workflow truly interactive since a node can change \nthe workflow’s behavior by updating the state.\nof the workflow.\nLangGraph state’s schema shouldn’t necessarily be defined as a TypedDict; you can \nBuilding Workflows with LangGraph\nAfter we have defined a schema for a state, we can define our first simple workflow:\nfrom langgraph.graph import StateGraph, START, END, Graph\nreturn {\"is_suitable\": len(state[\"job_description\"]) > 100}\ndef generate_application(state):\nbuilder.add_node(\"analyze_job_description\", analyze_job_description)\nbuilder.add_node(\"generate_application\", generate_application)\nbuilder.add_edge(START, \"analyze_job_description\")\nbuilder.add_edge(\"analyze_job_description\", \"generate_application\")\nbuilder.add_edge(\"generate_application\", END)\nHere, we defined two Python functions that are components of our workflow.\nour workflow by providing a state’s schema, adding nodes and edges between them.\nSTART and END are reserved built-in nodes that define the beginning and end of the workflow ",
      "keywords": [
        "LangChain",
        "State",
        "image",
        "LangGraph",
        "workflow",
        "node",
        "applications",
        "’ll",
        "Graph",
        "model",
        "LLMs",
        "complex workflows",
        "Building Workflows",
        "building",
        "description"
      ],
      "concepts": [
        "model",
        "langchain",
        "graphs",
        "applications",
        "application",
        "workflows",
        "buildings",
        "llms",
        "chapters",
        "state"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.646,
          "base_score": 0.496,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 10,
          "title": "",
          "score": 0.641,
          "base_score": 0.641,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 36,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "workflow",
          "workflows",
          "state",
          "langgraph",
          "generate_application"
        ],
        "semantic": [],
        "merged": [
          "workflow",
          "workflows",
          "state",
          "langgraph",
          "generate_application"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3729697400126825,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302715+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "summary": "LangGraph isolates state updates.\nThe only way for a node to modify a state is to provide an output dictionary with key-value pairs \nA node should modify at least one key in the state.\nWe should provide a dictionary with the initial state, and we’ll get the final state as an output:\nres = graph.invoke({\"job_description\":\"fake_jd\"})\n>>...Analyzing a provided job description ...\n{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_\nthen merge state updates from these nodes).\ncurrent state as an input and returns a string with the node’s name to be executed.\nbuilder.add_node(\"analyze_job_description\", analyze_job_description)\nbuilder.add_node(\"generate_application\", generate_application)\ndef is_suitable_condition(state: StateGraph) -> Literal[\"generate_\nbuilder.add_edge(START, \"analyze_job_description\")\nbuilder.add_conditional_edges(\"analyze_job_description\", is_suitable_\nbuilder.add_edge(\"generate_application\", END)\nIt handles parallel execution of nodes and updates sent to the central graph’s state.\nWe’ve defined an edge is_suitable_condition that takes a state and returns either an END or \ngenerate_application string by analyzing the current state.\ntion nodes directly to the add_conditional_edges function; otherwise, LangGraph will connect \nthe source node with all other nodes in the graph (since it doesn’t analyze the code of an edge \noutput of the analyze_job_description step, our graph can perform different actions.\nSo far, our nodes have changed the state by updating the value for a corresponding key.\nWith the first option, a node should return a list as a value for the key actions.\nhint, we tell the LangGraph compiler that the type of our variable in the state is a list of strings, \nand it should use the add method to concatenate two lists (if the value already exists in the state \nfrom langgraph.graph.message import add_messages \nSince this is such an important reducer, there’s a built-in state that you can inherit from:\nfrom langgraph.graph import MessagesState \nLangGraph provides a powerful API that allows you to make your graph configurable.\nLet’s allow our node to use different LLMs during application generation:\ndef generate_application(state: JobApplicationState, config: \nprint(f\"...generating application with {model_provider} and {model_\nLet’s now compile and execute our graph with a custom configuration (if you don’t provide any, \nres = graph.invoke({\"job_description\":\"fake_jd\"}, config={\"configurable\": \n>> ...Analyzing a provided job description ...\n{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_\ncommon challenge these workflows face: ensuring LLM outputs follow the exact structure needed \nControlled output generation\nLLM to generate an output that follows a certain structure.\nadd corresponding instructions to the prompt and parse the output.\neither continue with an application or ignore this specific job description.\nresult = llm.invoke(prompt_template.format(job_description=job_\nresult = llm.invoke(prompt_template_enum.format(job_description=job_\noffers plenty of OutputParsers that take the output generated by the LLM and try to parse it into a \nLet’s build a parser that parses an output into an enum:\nfrom langchain.output_parsers import EnumOutputParser\nparser handles any generation-like output (not only strings), and it actually also strips the output.\nresult = chain.invoke(prompt_template_enum.format(job_description=job_\nNow let’s make this chain part of our LangGraph workflow:\ndef analyze_job_description(state):\nprompt = prompt_template_enum.format(job_description=state[\"job_",
      "keywords": [
        "description",
        "job",
        "state",
        "LangGraph",
        "output",
        "job description",
        "node",
        "application",
        "LLM",
        "list",
        "Workflows",
        "graph",
        "str",
        "Building Workflows",
        "model"
      ],
      "concepts": [
        "important",
        "state",
        "output",
        "graph",
        "node",
        "reducers",
        "reduce",
        "llm",
        "keys",
        "key"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 33,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "state",
          "job_description",
          "graph",
          "output",
          "analyze_job_description"
        ],
        "semantic": [],
        "merged": [
          "state",
          "job_description",
          "graph",
          "output",
          "analyze_job_description"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27858442446104065,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302767+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 104-112)",
      "start_page": 104,
      "end_page": 112,
      "summary": "result = analyze_chain.invoke(prompt)\nllm = llms[model_provider]\nanalyze_chain = llm | parser\nprompt = prompt_template_enum.format(job_description=job_description)\nresult = analyze_chain.invoke(prompt)\nlogger.error(f\"Exception {e} occurred while executing analyze_job_\nLet’s create a fake LLM that fails every second time:\nfake_llm = GenericFakeChatModel(messages=MessagesIterator())\nanalyze_chain_fake_retries = fake_llm_retry | parser\nFor example, many chat models on LangChain have client-side retries on specific \nRetrying a parsing step won’t help since typically parsing errors are related to the incomplete \nWhat if we retry the generation step and hope for the best, or actually give LLM \nFigure 3.3: Adding a retry mechanism to a chain that has multiple steps\nThen, if our parsing fails, we run it and provide our initial prompt \nfrom langchain.output_parsers import RetryWithErrorOutputParser\nprompt=retry_prompt, # an optional parameter, you can redefine the \ndefault prompt \nfixed_output = fix_parser.parse_with_prompt(\nprompt = \"\"\"\nretry_chain = prompt | llm | StrOutputParser()\n# if it fails, catch an error and try to recover max_retries attempts\ncompletion = retry_chain.invoke(original_prompt, completion, error)\ncustomize default prompts for your workflows.\none that uses an alternative provider (and probably different prompts).\nOur fake model fails every second time, so let’s add a fallback to it.\nchain = fake_llm | RunnableLambda(lambda _: print(\"running main chain\"))\nlangchain.com/docs/how_to/output_parser_retry/.\nPrompts that you send to an LLM are one of the most important building blocks of your work-\nHence, let’s discuss some basics of prompt engineering next and see how to organize your \nprompts with LangChain.\nPrompt engineering\nLet’s continue by looking into prompt engineering and exploring various LangChain syntaxes \nBut first, let’s discuss how prompt engineering is different from prompt design.\nImproving our prompt (or prompt template, to be specific) \nto increase performance on a specific task is called prompt engineering.\nmore universal prompts that guide LLMs to generate generally better responses on a broad set \nof tasks is called prompt design.\nPrompt templates\nWhat we did in Chapter 2 is called zero-shot prompting.\nWe created a prompt template that con-\nthis prompt template with runtime arguments.\non f-string and add your chain, and LangChain would pass parameters from the input, substitute \nthem in the template, and pass the string to the next step in the chain:\nfrom langchain_core.output_parsers import StrOutputParser\nchain = lc_prompt_template | llm | StrOutputParser()\nor message templates, as in this example:\nfrom langchain_core.prompts import ChatPromptTemplate, \nprompt_template)\nmsg_example = msg_template.format(job_description=\"fake_jd\")\nchat_prompt_template = ChatPromptTemplate.from_messages([\nchain = chat_prompt_template | llm | StrOutputParser()\nYou can also do the same more conveniently without using chat prompt templates but by sub-\nchat_prompt_template = ChatPromptTemplate.from_messages(\n(\"human\", prompt_template)])\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nchat_prompt_template = ChatPromptTemplate.from_messages(\n(\"human\", prompt_template)])\nlen(chat_prompt_template.invoke({\"job_description\": \"fake\", \"history\": \nvided, and one human message from a templated prompt.\nfew-shot prompting\nA description of a task without examples of solutions is called zero-shot prompting, ",
      "keywords": [
        "prompt",
        "llm",
        "job",
        "description",
        "template",
        "chain",
        "LangChain",
        "LLMs",
        "analyze",
        "retry",
        "END",
        "fake",
        "prompt engineering",
        "Error",
        "messages"
      ],
      "concepts": [
        "prompts",
        "llm",
        "langchain",
        "important",
        "errors",
        "retries",
        "retry",
        "chain",
        "message",
        "exception"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.717,
          "base_score": 0.567,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 36,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt",
          "prompt engineering",
          "chat_prompt_template",
          "chatprompttemplate",
          "chain"
        ],
        "semantic": [],
        "merged": [
          "prompt",
          "prompt engineering",
          "chat_prompt_template",
          "chatprompttemplate",
          "chain"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3548684549884057,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302823+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 113-121)",
      "start_page": 113,
      "end_page": 121,
      "summary": "have different recommendations on prompt writing or formatting, hence if you have complex \nprompts, always check the documentation of the model provider, evaluate the performance of \nyour workflows before switching to a new model provider, and adjust prompts accordingly if \ntiple prompt templates and select them dynamically based on the model provider.\nAnother big improvement can be to provide an LLM with a few examples of this specific task as \ninput-output pairs as part of the prompt.\nThis is called few-shot prompting.\nprompting is difficult to use in scenarios that require a long input (such as RAG, which we’ll talk \nabout in the next chapter) but it’s still very useful for tasks with relatively short prompts, such \nOf course, you can always hard-code examples in the prompt template itself, but this makes it \nChaining prompts together\nsubstitution in a prompt template:\nAnother way to make your prompts more manageable is to split them into pieces and chain them \nYou can also build more complex substitutions by using the class langchain_core.prompts.\nsystem_prompt_template = PromptTemplate.from_template(\"a: {a} b: {b}\")\nchat_prompt_template = ChatPromptTemplate.from_messages(\n[(\"system\", system_prompt_template.template),\nmessages = chat_prompt_template.invoke({\"a\": \"a\", \"b\": \"b\", \"c\": \"c\"}).\nDynamic few-shot prompting\nAs the number of examples used in your few-shot prompts continues to grow, you might limit \nthe number of examples to be passed into a specific prompt’s template substitution.\na few-shot prompt\ndemonstrated that a relatively simple modification to a prompt that encouraged a model to gen-\nThere are different modifications of CoT prompting, and because it has long outputs, typically, \nCoT prompts are zero-shot.\nexample of CoT is just to add to your prompt template something like “Let’s think step by step.”\nYou can read the original paper introducing CoT, Chain-of-Thought Prompting Elicits \nThere are various CoT prompts reported in different papers.\nFor our learning purposes, let’s use a CoT prompt with few-shot examples:\nmath_cot_prompt = hub.pull(\"arietem/math_cot\")\ncot_chain = math_cot_prompt | llm | StrOutputParser()\nFor example, let us first run a cot_chain and then pass its output (please note \nthat we pass a dictionary with an initial question and a cot_output to the next step) to an LLM \nthat will use a prompt to create a final answer based on CoT reasoning:\nparse_prompt_template = (\nparse_prompt = PromptTemplate.from_template(\nparse_prompt_template\n{\"full_answer\": itemgetter(\"question\") | cot_chain,\n| parse_prompt\nAlthough a CoT prompt seems to be relatively simple, it’s extremely powerful since, as we’ve \nEven with CoT prompting, \nNow that we have learned how to efficiently organize your prompt and use different prompt \nengineering approaches with LangChain, let’s talk about what can we do if prompts become too \nThis approach is straightforward: use prompt \ntemplates to combine all inputs into a single prompt.\nThen, send this consolidated prompt to an \nbest working prompts.\nprompts.\nReasoning models have different prompting guidelines (for example, typ-\nically, you should not use either CoT or few-shot prompting with such models).\nwindow that an LLM can handle – summarization is a good example of such a task.\nFirst, let’s define the state of the graph that keeps track of the video in question, the intermediate \nsummarize_video_chunk for the Map phase\n_summarize_video_chunk nodes with an edge based on a _map_summaries function:\nhuman_part = {\"type\": \"text\", \"text\": \"Provide a summary of the video.\"}\nasync def _summarize_video_chunk(state:  _ChunkState):\nasync def _generate_final_summary(state: AgentState):\nfinal_summary = await (reduce_prompt | llm | StrOutputParser()).",
      "keywords": [
        "prompt",
        "template",
        "CoT",
        "LLM",
        "state",
        "answer",
        "video",
        "system",
        "context",
        "model",
        "final",
        "Building Workflows",
        "prompt template",
        "context window",
        "Workflows"
      ],
      "concepts": [
        "prompting",
        "examples",
        "answer",
        "answering",
        "state",
        "cot",
        "templates",
        "multiple",
        "models",
        "let"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.57,
          "base_score": 0.42,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "",
          "score": 0.534,
          "base_score": 0.384,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.523,
          "base_score": 0.373,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cot",
          "prompt",
          "prompts",
          "prompting",
          "shot"
        ],
        "semantic": [],
        "merged": [
          "cot",
          "prompt",
          "prompts",
          "prompting",
          "shot"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.281779913301942,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302889+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 122-129)",
      "start_page": 122,
      "end_page": 129,
      "summary": "graph.add_conditional_edges(START, _map_summaries, [\"summarize_video_\ngraph.add_edge(\"summarize_video_chunk\", \"generate_final_summary\")\ngraph.add_edge(\"generate_final_summary\", END)\nTrimming chat history\nEvery chat application should preserve a dialogue history.\nThe chat history is essentially a list of messages, but there are situations where trimming this \nThere are five ways to trim the chat history:\nChain function from langchain_core.messages import trim_messages allows you to \ngithub.io/langgraph/how-tos/memory/add-summary-conversation-history/).\nkeep the short-memory history of the latest summary and the message after that summary \nfor the application itself, and you probably want to keep track of the whole history (all \nFor example, you probably don’t need to load all the raw history and summary \nhistory.\ntokenizer that can be passed to a trim_messages function since you can reuse a lot of logic \nOf course, the question remains on how you can persist the chat history.\nSaving history to a database\nAs mentioned above, an application deployed to production can’t store chat history in a local \nwe create a separate function that returns a history given the session_id:\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\ndef get_session_history(session_id: str):\nentire history and keeps a system message only:\nchain = RunnableWithMessageHistory(raw_chain, get_session_history)\nNow let’s run it and make sure that our history keeps all the interactions with the user but a \ntrimmed history is passed to the LLM:\nprint(f\"History length: {len(sessions['1'].messages)}\")\nprint(f\"History length: {len(sessions['1'].messages)}\")\nHistory length: 2\nHistory length: 4\nafter finishing the chain (to add new messages to the history).\nFor example, if you use a sequential session_id, users might easily access sessions that \nmechanism from storing the chat history since you can store the workflow at any given point \nLet’s build a simple example with a single node that prints the amount of messages in the state \na list of messages, and we initiate a MemorySaver that will keep checkpoints in local memory and \nfrom langgraph.checkpoint.memory import MemorySaver\nYou can find the full list of integrations to store chat history on the documenta-\nprint(f\"History length = {len(state[:-1])}\")\ngraph = builder.compile(checkpointer=memory)\nNow, each time we invoke the graph, we should provide either a specific checkpoint or a thread-\nWe invoke our graph two times with different thread-id \nvalues, make sure they each start with an empty history, and then check that the first thread has \n>> History length = 0\nHistory length = 0\nHistory length = 2\ncheckpoints = list(memory.list(config={\"configurable\": {\"thread_id\": \nprint(check_point.config[\"configurable\"][\"checkpoint_id\"])\nhistory:\nconfig={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": \ncheckpoint_id}})\n>> History length = 0\nconfig={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": \ncheckpoint_id}})\n>> History length = 2\nOne obvious use case for checkpoints is implementing workflows that require additional input \nIn this chapter, we dived into building complex workflows with LangChain and LangGraph, going \nproduction deployments, and discussed methods for managing chat history, including trimming \nWhat is LangGraph, and how does LangGraph workflow differ from LangChain’s vanilla \nHow can memory mechanisms be used to trim the history of a conversational bot?\nWhat is the use case of LangGraph checkpoints?",
      "keywords": [
        "history",
        "messages",
        "chat history",
        "History length",
        "checkpoint",
        "LangGraph",
        "LangChain",
        "workflows",
        "chat",
        "config",
        "length",
        "memory",
        "session",
        "input",
        "node"
      ],
      "concepts": [
        "history",
        "histories",
        "checkpoints",
        "messages",
        "langchain",
        "memory",
        "important",
        "importance",
        "graph",
        "workflows"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.586,
          "base_score": 0.436,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.583,
          "base_score": 0.433,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.549,
          "base_score": 0.399,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "history",
          "history length",
          "length",
          "chat history",
          "chat"
        ],
        "semantic": [],
        "merged": [
          "history",
          "history length",
          "length",
          "chat history",
          "chat"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25838774721490193,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302936+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 130-138)",
      "start_page": 130,
      "end_page": 138,
      "summary": "Retrieval-Augmented Generation (RAG) extends LLMs by dynamically incorporating external \nvector embedding, runs a search extracting relevant documents, and passes these to a model that \nThis chapter explores RAG systems and the core components of RAG, including vector stores, \ndocument processing, retrieval strategies, implementation, and evaluation techniques.\nComponents of a RAG system\nTroubleshooting RAG systems\nBuilding Intelligent RAG Systems\nInformation retrieval has been a fundamental human need since the dawn of recorded knowledge.\nFor the past 70 years, retrieval systems have operated under the same core paradigm:\nFirst, a user frames an information need as a query.\nFinally, the system returns references to documents that may satisfy the information need:\nEarly information retrieval systems relied on manual indexing \nTraditional retrieval systems aimed to reduce these costs through various optimizations:\ndocuments but satisfying information needs.\nA fundamental limitation of traditional retrieval systems lies in their lexical approach to docu-\nof meaning enabled retrieval based on conceptual similarity rather than exact word matching.\nfundamentally changed what was possible in information retrieval, enabling the development \nBuilding Intelligent RAG Systems\nnal retrieval, functioning as implicit knowledge bases.\nretrieving documents containing answers to directly generating answers from internalized \nThe solution emerged in RAG, which bridges traditional retrieval systems \nComponents of a RAG system\nRAG enables language models to ground their outputs in external knowledge, providing an elegant \nBy retrieving only relevant information on demand, RAG systems \nRather than simply retrieving documents for human review (as traditional search engines do) or \ngenerating answers solely from internalized knowledge (as pure LLMs do), RAG systems retrieve \nKnowledge base: The storage layer for external information\nRetriever: The knowledge access layer that finds relevant information\nAn indexing pipeline that processes, chunks, and stores documents in the knowledge base\nA query pipeline that retrieves relevant information and generates responses using that \nThe workflow in a RAG system follows a clear sequence: when a query arrives, it’s processed for \nretrieval; the retriever then searches the knowledge base for relevant information; this retrieved \ngenerates a response grounded in both the query and the retrieved information.\nBuilding Intelligent RAG Systems\nthe fundamental building blocks of modern RAG systems: embeddings and vector stores that \npower the knowledge base and retriever components.\nfirst consider the decision between implementing RAG or using pure LLMs. This choice will fun-\nWhen to implement RAG\nLegal applications benefit from RAG’s ability to process complex \njustify the additional complexity of implementing RAG.\nThe benefits of RAG, however, come with significant implementation considerations.\nrequires efficient indexing and retrieval mechanisms to maintain reasonable response times.\nDevelopment teams should consider RAG when their applications require:\nAs mentioned, a RAG system comprises a retriever that finds relevant information, an augmenta-\nretrieve our vector embeddings.\nof a RAG system: vector embeddings, vector stores, and indexing strategies to optimize retrieval.",
      "keywords": [
        "Intelligent RAG Systems",
        "RAG",
        "RAG Systems",
        "Building Intelligent RAG",
        "Intelligent RAG",
        "Systems",
        "retrieval systems",
        "Information",
        "retrieval",
        "RAG systems Building",
        "knowledge",
        "Troubleshooting RAG systems",
        "relevant information",
        "Traditional retrieval systems",
        "Building Intelligent"
      ],
      "concepts": [
        "rag",
        "retrieval",
        "retrieving",
        "retrieve",
        "documents",
        "document",
        "knowledge",
        "systems",
        "information",
        "generation"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 22,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.554,
          "base_score": 0.554,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rag",
          "retrieval",
          "rag systems",
          "information",
          "systems"
        ],
        "semantic": [],
        "merged": [
          "rag",
          "retrieval",
          "rag systems",
          "information",
          "systems"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2722395230143976,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.302981+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 139-148)",
      "start_page": 139,
      "end_page": 148,
      "summary": "create an embedding, we’re converting words or chunks of text into vectors (lists of numbers) \nIn other words, the embedding model transforms text into numerical vectors.\nOnce we have these OpenAI embeddings (the 1536-dimensional vectors we generated for our \nThis brings us to vector stores – specialized databases optimized for similarity searches in high-di-\nVector stores\nVector stores are specialized databases designed to store, manage, and efficiently search vector \nAs we’ve seen, embeddings convert text (or other data) into numerical vectors that \nVector stores solve the fundamental challenge of how to persistently and efficiently search through \n•\t\n•\t\n•\t\n•\t\n•\t\nScale: Applications often need to store millions of embeddings\n•\t\n•\t\nSearch performance: Finding similar vectors quickly becomes computationally intensive\n•\t\n•\t\n•\t\n# Example of data that needs efficient storage in a vector store\nAt their core, vector stores combine two essential components:\n•\t\n•\t\nVector index: A specialized data structure that enables efficient similarity search\nVector stores enable similarity-based search through distance calculations in high-dimensional \nWhile traditional databases excel at exact matching, vector embeddings allow for semantic \nThe key difference from traditional databases is how vector stores handle searches.\n•\t\n•\t\n•\t\nVector store search:\n•\t\n•\t\nOptimized for high-dimensional vector spaces\n•\t\nVector stores comparison\nVector stores manage high-dimensional embeddings for retrieval.\nTable 4.1: Vector store comparison by deployment options, licensing, and key features\nEach vector store offers different tradeoffs in terms of deployment flexibility, licensing, and spe-\n•\t\n•\t\n•\t\n•\t\nModern vector stores support several search patterns:\n•\t\n•\t\n•\t\nHybrid search: Combines vector similarity with text-based search (like keyword matching \n•\t\nFiltered vector search: Applies traditional database filters (for example, metadata con-\nstraints) alongside vector similarity search\nVector stores also handle different types of embeddings:\n•\t\nDense vector search: Uses continuous embeddings where most dimensions have non-zero \n•\t\nSparse vector search: Uses high-dimensional vectors where most values are zero, resem-\n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\nHardware considerations for vector stores\n•\t\nMemory requirements: Vector databases are memory-intensive, with production systems \n•\t\n•\t\nstores, as index loading and search performance depend heavily on I/O speed.\n•\t\nbut production deployments should consider dedicated infrastructure or cloud-based vector \nVector store interface in LangChain\nfor working with vector stores, allowing you to easily switch between different implementations:\nvector_store = Chroma(embedding_function=embeddings)\nids = vector_store.add_documents(docs)\nresults = vector_store.similarity_search(\"How does LangChain work?\", \nvector_store.delete(ids=[\"doc_1\", \"doc_2\"])\nresults = vector_store.max_marginal_relevance_search(\nIt’s important to also briefly highlight applications of vector stores apart from RAG:\n•\t\n•\t\n•\t\n•\t\n•\t\nStoring vectors isn’t enough, however.\nWe need to find similar vectors quickly when processing \nWithout proper indexing, searching through vectors would be like trying to find a book \nVector indexing strategies\nfind similar vectors without comparing against every single vector in the database (brute force \n•\t\nTree-based structures that hierarchically divide the vector space\n•\t\nWhen using a vector store in LangChain, the indexing strategy is typically handled by the under-\nThe key takeaway is that proper indexing transforms vector search from an O(n) operation (where \nvector with \nevery vector \nvectors\nvectors\nvectors to \nvectors and \nvectors by \nvectors\nTable 4.2: Vector store comparison by deployment options, licensing, and key features\nFor maximum accuracy with small datasets (<100K vectors): Exact Search provides \nFor production systems with millions of vectors: HNSW offers the best balance of search ",
      "keywords": [
        "vector",
        "vector stores",
        "search",
        "Embeddings",
        "Intelligent RAG Systems",
        "Building Intelligent RAG",
        "stores",
        "RAG Systems",
        "Intelligent RAG",
        "RAG",
        "Vector Search",
        "Systems",
        "similar vectors",
        "documents",
        "search vector embeddings"
      ],
      "concepts": [
        "vectors",
        "search",
        "embeddings",
        "store",
        "storing",
        "similar",
        "similarities",
        "index",
        "indexes",
        "database"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "",
          "score": 0.785,
          "base_score": 0.635,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "",
          "score": 0.508,
          "base_score": 0.358,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vector",
          "vectors",
          "stores",
          "vector stores",
          "search"
        ],
        "semantic": [],
        "merged": [
          "vector",
          "vectors",
          "stores",
          "vector stores",
          "search"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.20742380052054088,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303024+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 149-156)",
      "start_page": 149,
      "end_page": 156,
      "summary": "# Exact search index\nexact_index.add(vectors)\nhnsw_index.add(vectors)\nexact_D, exact_I = exact_index.search(query, k=10)  # Search for 10 \nhnsw_D, hnsw_I = hnsw_index.search(query, k=10)\nHNSW search time: 0.000412 seconds\nThis example demonstrates the fundamental tradeoff in vector indexing: exact search guarantees \nFor small datasets like this example (10,000 vectors), the absolute time difference is minimal.\nHowever, as your dataset grows to millions or billions of vectors, exact search becomes prohib-\nFor most production RAG applications, you’ll likely end up with HNSW or a combined approach \nlike IVF+HNSW, which clusters vectors first (IVF) and then builds efficient graph structures \nTo improve retrieval, documents must be processed and structured effectively.\nexplores loading various document types and handling multi-modal content.\nVector libraries, like Facebook (Meta) Faiss or Spotify Annoy, provide functionality for working \nwith vector data.\nclustering or tree-based methods, and allow users to perform vector similarity searches for various \nsearch and clustering of dense vectors.\nFaiss is widely used for large-scale vector search tasks and supports \nhnswlib is a C++ library for approximate nearest-neighbor search using the HNSW al-\nNon-Metric Space Library (nmslib) supports various indexing algorithms like HNSW, \nThe need for hybrid search capabilities combining vector and traditional search\nFor many applications, a hybrid approach combining vector search with traditional database \nThink of the RAG pipeline as an assembly line in a library, where raw materials (documents) get \nDocument processing – the foundation\nDocument processing is like preparing books for a library.\nLoaded using document loaders appropriate for their format (PDF, HTML, text, \nThere are a lot more vector search libraries you can choose from.\nOnce documents are processed, we need a way to make them searchable.\nvector indexing comes in.\nAn embedding model converts each document chunk into a vector (think of it as \nThese vectors are organized in a special data structure (the vector store) that makes \nVector stores are like the organized shelves in our library.\nStore both the document vectors and the original document content\nProvide efficient ways to search through the vectors\nOffer different organization methods (like HNSW or IVF) that balance speed and \nFor example, using FAISS (a popular vector store), we might organize our vectors in a hier-\nThe vector store finds documents whose vectors are most similar to the question \nvector\nfrom langchain_community.document_loaders import JSONLoader\nLoad documents\ndocuments = loader.load()\nembeddings = embedder.embed_documents([doc.page_content for doc in \ndocuments])\nStore in vector database\nvector_db = FAISS.from_documents(documents, embedder)\nresults = vector_db.similarity_search(query)This implementation covers the core RAG \nworkflow: document loading, embedding, storage, and retrieval.\nwhich we should discuss a bit more in detail: document loaders and retrievers.\nDocument processing\nLangChain provides a comprehensive system for loading documents from various sources through \ndocument loaders.\nA document loader is a component in LangChain that transforms various \ndata sources into a standardized document format that can be used throughout the LangChain \nEach document contains the actual content and associated metadata.\nDocument loaders serve as the foundation for RAG systems by:\nPreparing documents for further processing (like chunking or embedding)\nLangChain supports loading documents from a wide range of document types and sources through \nArxivLoader for scientific papers\nThe following table organizes LangChain document loaders into a comprehensive table:\ndocuments, data \nTable 4.3: Document loaders in LangChain\nfrom langchain_community.document_loaders import JSONLoader\ndocuments = loader.load()\nprint(documents)",
      "keywords": [
        "Intelligent RAG Systems",
        "Compare search times",
        "HNSW",
        "RAG Systems",
        "Building Intelligent RAG",
        "RAG",
        "search",
        "HNSW search time",
        "time",
        "vector",
        "Exact",
        "document",
        "Intelligent RAG",
        "Exact search time",
        "documents"
      ],
      "concepts": [
        "document",
        "vector",
        "searches",
        "likely",
        "important",
        "content",
        "data",
        "loaders",
        "indexing",
        "rag"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "",
          "score": 0.785,
          "base_score": 0.635,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "",
          "score": 0.579,
          "base_score": 0.429,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.527,
          "base_score": 0.377,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "document",
          "vector",
          "vectors",
          "search",
          "documents"
        ],
        "semantic": [],
        "merged": [
          "document",
          "vector",
          "vectors",
          "search",
          "documents"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2063017685460284,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303068+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "summary": "often need processing before storage and retrieval, and selecting the right chunking strategy \nChunking strategies\nThe way you chunk documents affects:\nRetrieval accuracy: Well-formed chunks maintain semantic coherence, making them \nThe most basic approach divides text into chunks of a specified length without considering con-\nchunk_size=200,\nchunk_overlap=20\nchunks = text_splitter.split_documents(documents)\nprint(f\"Generated {len(chunks)} chunks from document\")\nFixed-size chunking is good for quick prototyping or when document structure is relatively uni-\nchunk_size=150,\nchunk_overlap=20\ndocument = \"\"\"\nRetrieval-Augmented Generation (RAG) combines retrieval systems with \n1. Document processing\n3. Retrieval\n### Document Processing\nThis step involves loading and chunking documents appropriately.\nchunks = text_splitter.split_text(document)\nHere are the chunks:\nretrieval systems with generative AI models.', 'It helps address \nDocument processing\\\nRetrieval\\n4.\nDocument Processing\\nThis step involves loading and chunking documents \nserves natural text boundaries while maintaining reasonable chunk sizes.\nDocument-specific chunking\nDocument-specific chunking adapts to \nThis can be useful when working with specialized document formats where structure matters – \nmarkdown sections), and improves retrieval relevance for domain-specific queries.\nSemantic chunking\nchunks = text_splitter.split_text(document)\nThese are the chunks:\nretrieval systems with generative AI models.\nDocument processing\\\nRetrieval\\n4.',\nGeneration\\n\\n### Document Processing\\nThis step \ninvolves loading and chunking documents appropriately.\nYou may use semantic chunking for complex technical documents where semantic cohesion \nAnalyze the document’s structure and content\nThis type of chunking can be useful for exceptionally complex documents where standard splitting \nContent requires domain-specific understanding to chunk appropriately\nModern documents often contain a mix of text, tables, images, and code.\nYour chunking strategy should be guided by document characteristics, retrieval needs, and com-\nDocument \nDocument-specific chunking\nSemantic chunking\nRetrieval Needs\nFor most RAG applications, the RecursiveCharacterTextSplitter with appropriate chunk size \nHowever, it is often critical to performance to experiment with different chunk sizes specific to your \nRetrieval\nRetrieval integrates a vector store with other LangChain components for simplified querying \nFigure 4.3: The relationship between query, retriever, and documents\nLangChain offers a rich ecosystem of retrievers, each designed to solve specific information re-\nLangChain retrievers\nThese retrievers, \nLangChain supports several sophisticated approaches to retrieval:\nas_retriever() method:\nretriever = KNNRetriever.from_documents(documents, OpenAIEmbeddings())\nThese are the retrievers most relevant for RAG systems.\nLexical search retrievers: These implement traditional text-matching algorithms:",
      "keywords": [
        "Intelligent RAG Systems",
        "chunking",
        "RAG",
        "Document",
        "Building Intelligent RAG",
        "RAG Systems",
        "documents",
        "text",
        "retrieval",
        "Intelligent RAG",
        "retrievers",
        "Document processing",
        "LangChain",
        "Systems",
        "processing"
      ],
      "concepts": [
        "retrieval",
        "retrieved",
        "document",
        "documents",
        "chunk",
        "semantic",
        "text",
        "based",
        "bases",
        "queries"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 22,
          "title": "",
          "score": 0.538,
          "base_score": 0.388,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "",
          "score": 0.533,
          "base_score": 0.383,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chunking",
          "document",
          "retrieval",
          "chunks",
          "documents"
        ],
        "semantic": [],
        "merged": [
          "chunking",
          "document",
          "retrieval",
          "chunks",
          "documents"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27135387667117145,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303115+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 165-173)",
      "start_page": 165,
      "end_page": 173,
      "summary": "Selecting documents similar to the query\nEnsuring retrieved documents are distinct from each other\nIt might miss contextually relevant documents that use different terminology\nTwo particularly powerful approaches are hybrid retrieval and re-ranking.\nHybrid retrieval: Combining semantic and keyword search\nDense retrieval: Uses vector embeddings for semantic understanding\nFor example, a hybrid retriever might use vector similarity to find semantically related documents \nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\nvector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\nbm25_retriever = BM25Retriever.from_documents(documents)\nresults = hybrid_retriever.get_relevant_documents(\"climate change \nFirst, retrieve a larger set of candidate documents\nfrom langchain.retrievers.document_compressors import CohereRerank\nfrom langchain.retrievers import ContextualCompressionRetriever\n# Original documents\noriginal_docs = base_retriever.get_relevant_documents(\"How do \n# Compressed documents\ncompressed_docs = compression_retriever.get_relevant_documents(\"How \nfrom langchain_community.document_compressors.rankllm_rerank import \nLLM-based custom rerankers: Using any LLM to score document relevance:\n\"Rate relevance of document to query on scale of 1-10: \n{document}\"\nPlease note that while Hybrid retrieval focuses on how documents are retrieved, re-ranking fo-\nQuery transformation: Improving retrieval through better queries\nretrieval results.\nA more advanced approach is Hypothetical Document Embeddings (HyDE).\nHypothetical Document Embeddings (HyDE)\nHyDE uses an LLM to generate a hypothetical answer document based on the query, and then \nuses that document’s embedding for retrieval.\n# Create prompt for generating hypothetical document\n# Generate hypothetical document\n# Use the hypothetical document for retrieval\nqueries and documents are common.\nOnce documents are retrieved, context processing techniques help distill and organize the infor-\nContextual compression extracts only the most relevant parts of retrieved documents, removing \nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.retrievers import ContextualCompressionRetriever\nbase_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\ncompressed_docs = compression_retriever.invoke(\"How do transformers \nHere are our compressed documents:\n[Document(metadata={'source': 'Neural Network Review 2021', 'page': 42}, \nDocument(metadata={'source': 'Large Language Models Survey', 'page': 89}, \nAnother powerful approach is Maximum Marginal Relevance (MMR), which balances document \nrelevance with diversity, ensuring that the retrieved set contains varied perspectives rather than \nfrom langchain_core.documents import Document\n# Example documents\ndocuments = [\nDocument(\nDocument(\nDocument(\nSource attribution explicitly connects generated information to the retrieved sources, helping \nWe’ll initialize a vector store with our documents and create a retriever \nconfigured to fetch the top 3 most relevant documents for each query.\n# Create a vector store and retriever\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n# Create a source-formatted string from documents\n# Retrieve relevant documents\nretrieved_docs = retriever.invoke(question)\nsources_formatted = format_sources_with_citations(retrieved_docs)",
      "keywords": [
        "Intelligent RAG Systems",
        "Building Intelligent RAG",
        "documents",
        "RAG Systems",
        "Intelligent RAG",
        "document",
        "query",
        "retriever",
        "Vector",
        "retrieval",
        "RAG",
        "source",
        "LangChain",
        "Building Intelligent",
        "search"
      ],
      "concepts": [
        "retrieval",
        "retrieved",
        "retrieve",
        "documents",
        "document",
        "sources",
        "query",
        "queries",
        "important",
        "generated"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 23,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "documents",
          "document",
          "hypothetical",
          "hypothetical document",
          "retrieval"
        ],
        "semantic": [],
        "merged": [
          "documents",
          "document",
          "hypothetical",
          "hypothetical document",
          "retrieval"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2506979367914731,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303169+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 174-181)",
      "start_page": 174,
      "end_page": 181,
      "summary": "# Generate the response with citations\nRetrieving relevant documents for a query\nattributed_answer = generate_attributed_response(question)\nTransformer models work by utilizing self-attention mechanisms to weigh \nSelf-consistency checking compares the generated response against the retrieved context to verify \nSelf-consistency checking verifies that generated responses accurately reflect the information in \nretrieved documents, providing a crucial layer of protection against hallucinations.\nfrom langchain_core.documents import Document\nretrieved_docs: List[Document],\nVerify if a generated answer is fully supported by the retrieved \ndocuments.\nretrieved_docs: List of documents used to generate the answer\ngenerated_answer: The answer produced by the RAG system\nllm: Language model to use for verification\n# Create context from retrieved documents\nThe function above begins our verification process by accepting the retrieved documents and \ngenerated answers as inputs.\nand combines all document content into a single context string.\n1. List any factual claims in the answer\nThe verification prompt is structured to perform a comprehensive fact check.\nmodel to break down each claim in the answer and categorize it based on how well it’s supported \nverification_prompt\nDocument(page_content=\"The transformer architecture was introduced in \nDocument(page_content=\"BERT is a transformer-based model developed by \ngenerated_answer = \"The transformer architecture was introduced by OpenAI \nverification_result = verify_response_accuracy(retrieved_docs, generated_\n\"claim\": \"The transformer architecture was introduced by \n\"claim\": \"The transformer architecture uses recurrent neural \ntransformer architecture does not use recurrent neural networks but relies \n\"claim\": \"BERT is a transformer model developed by Google\",\n\"evidence\": \"BERT is a transformer-based model developed by \n\"explanation\": \"This claim is fully supported by the provided \nThis approach systematically analyzes generated responses against source documents, identify-\ncorrect information to users while maintaining the fluency and coherence of generated responses.\nRetrieval-Augmented Generation (CRAG) directly addresses this challenge by introducing ex-\nInitial retrieval: Standard document retrieval from the vector store based on the query.\nRetrieval evaluation: A retrieval evaluator component assesses each document’s rele-\nRelevant documents: Pass high-quality documents directly to the generator.\nGeneration: Produce the final response using the filtered or augmented context.\nretrieved documents and the query, determining which documents are truly relevant.\n\"\"\"Binary relevance score for document evaluation.\"\"\"\ndef evaluate_document(document, query, llm):\n\"\"\"Evaluate if a document is relevant to a query.\"\"\"\nprompt = f\"\"\" You are an expert document evaluator.\ndetermine if the following document contains information relevant to the \nDocument content:\n{document.page_content}\nAnalyze whether this document contains information that helps answer the \nBy evaluating each document independently, CRAG can make fine-grained decisions about which \nit allows for conditional branching based on document evaluation.",
      "keywords": [
        "Intelligent RAG Systems",
        "Building Intelligent RAG",
        "RAG Systems",
        "RAG",
        "answer",
        "document",
        "verification",
        "Intelligent RAG",
        "context",
        "transformer",
        "transformer architecture",
        "documents",
        "claim",
        "retrieved documents",
        "response"
      ],
      "concepts": [
        "document",
        "generate",
        "generating",
        "generation",
        "generator",
        "important",
        "importance",
        "verification",
        "claim",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.547,
          "base_score": 0.547,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.512,
          "base_score": 0.512,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.502,
          "base_score": 0.502,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.472,
          "base_score": 0.472,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transformer",
          "document",
          "documents",
          "transformer architecture",
          "claim"
        ],
        "semantic": [],
        "merged": [
          "transformer",
          "document",
          "documents",
          "transformer architecture",
          "claim"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3484300256403769,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.303221+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 182-189)",
      "start_page": 182,
      "end_page": 189,
      "summary": "While CRAG enhances RAG by adding evaluation and correction mechanisms to the retrieval \nAI agents to orchestrate the entire RAG process.\nAgentic RAG\nRAG or even CRAG, which follow relatively structured workflows, agentic RAG uses agents to:\n•\t\n•\t\n•\t\n•\t\n•\t\ndata quality through evaluation and correction, while agentic RAG focuses on process intelligence \nAgentic RAG is particularly valuable for complex use cases that require:\n•\t\n•\t\n•\t\n•\t\nHowever, agentic RAG introduces significant complexity in implementation, potentially higher \nWhen implementing advanced RAG techniques, consider the specific requirements and constraints \n•\t  Low initial \n•\t Straightfor-\n•\t Limited retrieval \n•\t Vulnerability to \n•\t No handling of \n•\t Prototyping\n•\t Increased system \n•\t Challenge in \n•\t Content with \n•\t Improves result \n•\t Can be applied \n•\t Additional com-\n•\t Requires training \n•\t When retrieval \n•\t For handling \n•\t Improves re-\n•\t Depends on \n•\t Potential for \n•\t Complex or \n•\t Users with \ndocuments \n•\t Maximizes con-\n•\t Reduces \n•\t Processing adds \n•\t When context \n•\t Redundant \n•\t Increases out-\n•\t Provides \n•\t Enhances user \n•\t Additional \n•\t Complex imple-\n•\t Educational \n•\t When attribu-\ndocuments \n•\t Explicitly \n•\t Improves \n•\t Can dynamical-\n•\t More complex \n•\t Systems need-\n•\t Applications \nagentic RAG\n•\t Multi-step \n•\t Significant \n•\t Challenging \n•\t Complex \n•\t Research \n•\t Systems \nmation techniques can help bridge the gap between user language and document terminology.\n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\nenterprise system might use hybrid retrieval with query transformation, apply context processing \nto optimize the retrieved information, enhance responses with source attribution, and implement \nExplore agentic RAG (covered more in Chapter 5) for complex, multi-step information \nIn this section, we will build a corporate documentation chatbot that leverages LangChain for \n•\t\nretrieved documents, intermediate results, etc.).\n•\t\nof retrieved documents or other evaluation criteria—essential for ensuring reliable output.\n•\t\nMulti-step reasoning: For complex documentation tasks, LangGraph allows breaking \n•\t\nHuman-in-the-loop integration: When document quality or compliance cannot be au-\nWith the Corporate Documentation Manager tool we built, you can generate, validate, and \n•\t\n•\t\n•\t\n•\t\n•\t\n•\t\nLet’s first look at document loading.\nDocument loading\nThe Document class in LangChain is a fundamental data structure for storing and \nhain’s document processing pipelines, enabling consistent handling during loading, \nThis module is responsible for loading documents in various formats.\n•\t\ndocument processing.\n•\t\nDocumentLoader class: A central class that manages document loading across different \n•\t\nload_document function: A utility function that accepts a file path, determines its ex-\nand returns the loaded content as a list of Document objects.\nfrom langchain_community.document_loaders.epub import \nfrom langchain_community.document_loaders.pdf import PyPDFLoader\nfrom langchain_community.document_loaders.text import TextLoader\nfrom langchain_community.document_loaders.word_document import (\nfrom langchain_core.documents import Document\nThis class is responsible for loading documents with supported extensions.\nextentions dictionary maps file extensions to their corresponding document loader classes.\nThis gives us interfaces to read PDF, text, EPUB, and Word documents with different extensions.\n\"\"\"Loads in a document with a supported extension.\"\"\"\n\"\"\"Load a file and return it as a list of documents.\"\"\"",
      "keywords": [
        "Agentic RAG",
        "RAG",
        "Intelligent RAG Systems",
        "advanced RAG techniques",
        "RAG techniques",
        "RAG systems",
        "Building Intelligent RAG",
        "advanced RAG",
        "agentic RAG systems",
        "Intelligent RAG",
        "CRAG enhances RAG",
        "retrieval",
        "document",
        "agentic RAG focuses",
        "Agentic"
      ],
      "concepts": [
        "document",
        "documentation",
        "rag",
        "retrieval",
        "retrievers",
        "retrieves",
        "complex",
        "important",
        "imports",
        "queries"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.737,
          "base_score": 0.587,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.666,
          "base_score": 0.666,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "",
          "score": 0.538,
          "base_score": 0.388,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.531,
          "base_score": 0.531,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "rag",
          "agentic rag",
          "document",
          "agentic",
          "documents"
        ],
        "semantic": [],
        "merged": [
          "rag",
          "agentic rag",
          "document",
          "agentic",
          "documents"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31833297732134086,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303272+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 190-197)",
      "start_page": 190,
      "end_page": 197,
      "summary": "The load_document function defined above takes a file path, determines its extension, selects the \nFirst, the imports and \nDocument retrieval\nThe rag.py module implements document retrieval based on semantic similarity.\nfrom langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom chapter4.document_loader import load_document\nWe need to set up a vector store for the retriever to use:\nThe document chunks are stored in an InMemoryVectorStore using the cached embeddings, al-\ndocuments into smaller chunks, which makes them more manageable for retrieval:\ndef split_documents(docs: List[Document]) -> list[Document]:\n\"\"\"Split each document.\"\"\"\nreturn text_splitter.split_documents(docs)\nThis custom retriever inherits from a base retriever and manages an internal list of documents:\n\"\"\"A retriever that contains the top k documents that contain the user \nself.store_documents(self.documents)\ndef store_documents(docs: List[Document]) -> None:\n\"\"\"Add documents to the vector store.\"\"\"\nsplits = split_documents(docs)\nVECTOR_STORE.add_documents(splits)\n\"\"\"Add uploaded documents.\"\"\"\ndocs.extend(load_document(temp_filepath))\nself.documents.extend(docs)\nself.store_documents(docs)\ndef _get_relevant_documents(\n) -> List[Document]:\nstore_documents() splits the documents and adds them to the vector store.\nadd_uploaded_docs() processes files uploaded by the user, stores them temporarily, loads \nthem as documents, and adds them to the vector store.\n_get_relevant_documents() returns the top k documents related to a given query from \nThe rag.py module implements the RAG pipeline that ties together document retrieval with \nSystem prompt: A template prompt instructs the AI on how to use the provided document \ninformation like the user’s question, retrieved context documents, generated answers, \nRetrieve function: Fetches relevant documents based on the user’s query\ngenerate function: Creates a draft answer using the retrieved documents and \ndoc_finalizer function: Either returns the original answer if no issues were found \nfrom langchain_core.documents import Document\nfrom langgraph.graph import START, StateGraph, add_messages\nfrom chapter4.llms import chat_model\nfrom chapter4.retriever import DocumentRetriever\ndocument snippets when generating a response:\n\"If none of the documents is relevant to the question, \"\nof the application (for example, question, context documents, answer, issues report):\ncontext: List[Document]\nretrieve function: Uses the retriever to get relevant documents based on the most recent \ngenerate function: Creates a draft answer by combining the retrieved document content \ndoc_finalizer function: If issues are found, it revises the document based on the provided \nretrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\nreturn {\"context\": retrieved_docs}\n{\"question\": state[\"messages\"][-1].content, \"context\": docs_\nf\"Original Document: {state['answer']}\\n\"\nf\"Always return the full revised document, even if no \n[retrieve, generate, double_check, doc_finalizer]",
      "keywords": [
        "document",
        "documents",
        "state",
        "Intelligent RAG Systems",
        "Building Intelligent RAG",
        "issues",
        "list",
        "graph",
        "RAG Systems",
        "docs",
        "Intelligent RAG",
        "vector store",
        "store",
        "vector",
        "Building Intelligent"
      ],
      "concepts": [
        "imports",
        "document",
        "documents",
        "retrieval",
        "retriever",
        "retrieve",
        "message",
        "doc",
        "returns",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.836,
          "base_score": 0.686,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 19,
          "title": "",
          "score": 0.832,
          "base_score": 0.682,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.625,
          "base_score": 0.475,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 18,
          "title": "",
          "score": 0.579,
          "base_score": 0.429,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.538,
          "base_score": 0.538,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "documents",
          "document",
          "docs",
          "retriever",
          "list document"
        ],
        "semantic": [],
        "merged": [
          "documents",
          "document",
          "docs",
          "retriever",
          "list document"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25267628337816467,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303319+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 198-205)",
      "start_page": 198,
      "end_page": 205,
      "summary": "This is what the sequential flow from document retrieval to generation, validation, and finaliza-\nFigure 4.5:  State graph of the corporate documentation pipeline\nBefore building a user interface, it’s important to test our RAG pipeline to ensure it functions \nprint(response[\"messages\"][-1].content)\nBy accessing response[\"messages\"][-1].content, we’re retrieving the content of the \nlast message, which contains the finalized answer generated by our RAG pipeline.\nWe integrate our pipeline with Streamlit to enable interactive documentation generation.\ninterface lets users submit documentation requests and view the process in real time:\nimport streamlit as st\nfrom chapter4.document_loader import DocumentLoader\nfrom chapter4.rag import graph, config, retriever\nst.set_page_config(page_title=\"Corporate Documentation Manager\", \nWe’ll initialize the session state for chat history and file management:\nif \"chat_history\" not in st.session_state:\nst.session_state.chat_history = []\nif 'uploaded_files' not in st.session_state:\nst.session_state.uploaded_files = []\nfor message in st.session_state.chat_history:\nwith st.chat_message(message[\"role\"]):\nst.markdown(message[\"content\"])\nThe retriever processes all uploaded files and embeds them for semantic search:\ndocs = retriever.add_uploaded_docs(st.session_state.uploaded_files)\nreturn response[\"messages\"][-1].content\nif user_message := st.chat_input(\"Enter your message:\"):\n# Display user message in chat message container\nwith st.chat_message(\"User\"):\nst.markdown(user_message)\n# Add user message to chat history\nst.session_state.chat_history.append({\"role\": \"User\", \"content\": \nuser_message})\nresponse = process_message(user_message)\nwith st.chat_message(\"Assistant\"):\nst.session_state.chat_history.append(\nst.subheader(\"Document Management\")\nif file.name not in st.session_state.uploaded_files:\nst.session_state.uploaded_files.append(file)\nTo run our Corporate Documentation Manager application on Linux or macOS, follow these steps:\nIn Chapter 3, we explored implementing RAG with citations in the Corporate Documentation \nnot be retrieved.\nBuilding an effective RAG system means understanding its common failure points and addressing \nGeneration System (2024), and Li and colleagues in their paper Enhancing Retrieval-Augmented \nIntegrating focused retrieval methods, such as retrieving documents first and then extracting key \nIn this chapter, we explored the key aspects of RAG, including vector storage, document pro-\nand user-friendly LLM applications that not only generate creative outputs but also incorporate \nThis foundation opens the door to more advanced RAG systems, whether you’re retrieving doc-\nquantify the performance of RAG systems to ensure performance is up to requirements.\nHow does MMR improve document retrieval?\nWhy is chunking necessary for effective document retrieval?\nHow do hybrid search techniques enhance the retrieval process?\nWhy is performance evaluation critical in RAG-based systems?\n8.\t What are the different retrieval methods in RAG systems?\nHow does contextual compression refine retrieved information before LLM processing?",
      "keywords": [
        "Intelligent RAG Systems",
        "RAG Systems",
        "RAG",
        "Building Intelligent RAG",
        "Corporate Documentation Manager",
        "message",
        "Intelligent RAG",
        "corporate documentation",
        "user",
        "retrieval",
        "Streamlit",
        "Systems",
        "documentation",
        "Documentation Manager",
        "response"
      ],
      "concepts": [
        "retrieving",
        "documents",
        "rag",
        "message",
        "user",
        "search",
        "searches",
        "response",
        "enhance",
        "enhancement"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "st",
          "st session_state",
          "session_state",
          "rag",
          "documentation"
        ],
        "semantic": [],
        "merged": [
          "st",
          "st session_state",
          "session_state",
          "rag",
          "documentation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3363878235543573,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303378+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 206-213)",
      "start_page": 206,
      "end_page": 213,
      "summary": "To understand how these agentic capabilities are built in practice, we’ll start by discussing tool \npattern, and how LLMs can use tools to interact with the external environment and improve \nThen, we’ll touch on how tools are defined in LangChain, \nat how to generate structured outputs with LLM using tools versus utilizing built-in capabilities \nagent that follows a plan-and-solve design pattern and uses tools such as web search, arXiv, and \nWhat is a tool?\nAdvanced tool-calling capabilities\ntools, and how do they extend what LLMs can do?\nWhat is a tool?\nThese tools enable LLMs to perform specific tasks and receive feedback from the external world.\nWhen using tools, LLMs perform three specific generation tasks:\nwith tools (for this specific run).\nNow it’s time to figure out how LLMs invoke tools and how we can make LLMs tool-aware.\ngive an LLM access to two tools: a search engine and a calculator.\nLater in this chapter, we’ll implement fully functional tools \nFinally, let’s give the model output of a tool by incorporating it into a prompt:\nWith that, we have demonstrated how tool calling works.\ntool-calling format.\nThese days, most LLMs provide a better API for tool calling since modern \ntion yourself in the prompt; you just provide both a prompt and a tool description as separate \nopen-source LLMs expect tool descriptions to be part of the raw prompt, but they would expect \nLangChain makes it easy to develop pipelines where an LLM invokes different tools and provides \nLet’s look at how tool handling works with LangChain.\nTools in LangChain\nWith most modern LLMs, to use tools, you can provide a list of tool descriptions as a separate \nFor tools, this happens through LangChain’s tools argument to the \nFor an LLM, a tool is anything that has an OpenAPI specification—in other \nhow to call a tool.\nFor LangChain, a tool is also something that can be called (and we will see later \non when and how to call a specific tool.\nma like this can become tedious, and we’ll see a simpler way to define tools later in this chapter:\nsearch_tool = {\nresult = llm.invoke(question, tools=[search_tool])\nLangChain maps a specific output format of the model provider into a unified tool-calling format:\nprint(result.tool_calls)\ntool calling (for example, there might be reasoning traces on why the model decided to call a \ntool).\nAs we can see, an LLM returned an array of tool-calling dictionaries—each of them contains a \nunique identifier, the name of the tool to be called, and a dictionary with arguments to be provided \nto this tool.\ntool_result = ToolMessage(content=\"Donald Trump ' Age 78 years June 14, \n1946\\n\", tool_call_id=step1.tool_calls[0][\"id\"])\nHumanMessage(content=question), step1, tool_result], tools=[search_\ntool])\nassert len(step2.tool_calls) == 0\nspecial field tool_call_id maps it to the specific tool calling that was generated by the model.\nNow, we can send the whole sequence (consisting of the initial output, the step with tool calling, \nllm_with_tools = llm.bind(tools=[search_tool])\nllm_with_tools.invoke(question)\nWhen we call llm.bind(tools=[search_tool]), LangChain creates a new object (assigned here \nto llm_with_tools) that automatically includes [search_tool] in every subsequent call to a \nllm.invoke(question, tools=[search_tool)\nNow let’s see how we can utilize tool calling even more, and improve LLM reasoning!\nAs you have probably thought already, LLMs can call multiple tools before generating the final \nthe outcome from the previous tool calls).\nThe idea is simple—we should give the LLM access to tools as a \nFirst, let’s create mocked search and calculator tools: ",
      "keywords": [
        "tool",
        "search",
        "Donald Trump",
        "LLM",
        "LLMs",
        "Building Intelligent Agents",
        "question",
        "query",
        "Intelligent Agents",
        "prompt",
        "Trump",
        "Donald",
        "search engine",
        "LangChain",
        "tool calling"
      ],
      "concepts": [
        "tools",
        "query",
        "search",
        "question",
        "questions",
        "llms",
        "llm",
        "langchain",
        "agents",
        "specific"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "",
          "score": 0.659,
          "base_score": 0.659,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.658,
          "base_score": 0.658,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.647,
          "base_score": 0.647,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.595,
          "base_score": 0.595,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "tools",
          "tool calling",
          "calling",
          "search_tool"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "tools",
          "tool calling",
          "calling",
          "search_tool"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3992222672965598,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.303433+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 214-221)",
      "start_page": 214,
      "end_page": 221,
      "summary": "calculator tool and make the LLM aware of both tools it can use.\ncalculator_tool = {\nllm_with_tools = llm.bind(tools=[search_tool, calculator_tool]).\nNow that we have an LLM that can call tools, let’s create the nodes we need.\nthat calls an LLM, another function that invokes tools and returns tool-calling results (by append-\nthe orchestrator should continue calling tools or whether it can return the result to the user:\nreturn {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\ndef call_tools(state: MessagesState):\ntool_calls = last_message.tool_calls\nfor tool_call in tool_calls:\nif tool_call[\"name\"] == \"google_search\":\ntool_result = mocked_google_search(**tool_call[\"args\"])\nnew_messages.append(ToolMessage(content=tool_result, tool_call_\nelif tool_call[\"name\"] == \"calculator\":\ntool_result = mocked_calculator(**tool_call[\"args\"])\nnew_messages.append(ToolMessage(content=tool_result, tool_call_\ndef should_run_tools(state: MessagesState):\nif last_message.tool_calls:\nreturn \"call_tools\"\nbuilder.add_conditional_edges(\"invoke_llm\", should_run_tools)\nbuilder.add_edge(\"call_tools\", \"invoke_llm\")\ntools=[search_tool, calculator_tool],\nDefining tools\nshould be able to call tools itself during the execution.\nA LangChain tool has three essential components: \nDescription: Text that helps the LLM understand when and how to use the tool \nIt allows an LLM to decide when and how to call a tool.\nChain tool is that it can be executed by an orchestrator, such as LangGraph.\nBuilt-in LangChain tools\nLangChain has many tools already available across various categories.\nSince tools are often pro-\nexamples of using tools.\nTools give an LLM access to search engines, such as Bing, DuckDuckGo, Google, and Tavily.\nfrom langchain_community.tools import DuckDuckGoSearchRun\nprint(f\"Tool's name = {search.name}\")\nprint(f\"Tool's name = {search.description}\")\nprint(f\"Tool's arg schema = f{search.args_schema}\")\n>> Tool's name = fduckduckgo_search\nTool's name = fA wrapper around DuckDuckGo Search.\nTool's arg schema = class 'langchain_community.tools.ddg_search.tool.\nfrom langchain_community.tools.ddg_search.tool import DDGInput\nNow we can invoke this tool and get a string output back (results from the search engine):\nWe can also invoke the LLM with tools, and let’s make sure that the LLM invokes the search tool \nresult = llm.invoke(query, tools=[search])\nprint(result.tool_calls[0])\nOur tool is now a callable that LangGraph can call programmatically.\nagent = create_react_agent(model=llm, tools=[search])\nThat’s exactly what we saw earlier as well—an LLM is calling tools until it decides to stop and \nupdate = event.get(\"agent\", event.get(\"tools\", {}))\nTool Calls:\n================================= Tool Message ===========================\nFor now, let’s briefly mention other types of tools that are already available \nTools that enhance the LLM’s knowledge besides using a search engine:\nGmailGetThread tools that allow you to search, retrieve, create, and send messages with \nTools that give an LLM access to a code interpreter: These tools give LLMs access to \nAPI tools: GraphQL and Requests\nTools that give an LLM access to databases by writing and executing SQL code: For ex-\nOther tools: These comprise tools that integrate third-party systems and allow the LLM to \nAny external system with an API can be wrapped as a tool if it enhances an LLM like this: ",
      "keywords": [
        "tools",
        "LLM",
        "Search",
        "Building Intelligent Agents",
        "call",
        "calculator",
        "messages",
        "CALLED CALCULATOR",
        "Google",
        "agent",
        "Building Intelligent",
        "LLMs",
        "Intelligent Agents",
        "LLM access",
        "expression"
      ],
      "concepts": [
        "tools",
        "message",
        "important",
        "search",
        "query",
        "queries",
        "google",
        "llm",
        "expression",
        "expressions"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.63,
          "base_score": 0.48,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tools",
          "tool",
          "search",
          "llm",
          "tool_call"
        ],
        "semantic": [],
        "merged": [
          "tools",
          "tool",
          "search",
          "llm",
          "tool_call"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29283151606050073,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303482+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 222-231)",
      "start_page": 222,
      "end_page": 231,
      "summary": "print(tool.name)\nexisting API as a tool.\nagent = create_react_agent(llm, toolkit.get_tools(), state_\nTool Calls:\n================================= Tool Message ===========================\nCustom tools\nyou can create your own custom tools, besides the example we looked at when we wrapped the \nWrapping a Python function as a tool\nAny Python function (or callable) can be wrapped as a tool.\nAs we remember, a tool on LangChain \nWe’re going to use a special @tool decorator that will wrap our \nfunction as a tool:\nfrom langchain_core.tools import tool\n@tool\nThere are over 50 tools already available.\npage: https://python.langchain.com/docs/integrations/tools/.\nfrom langchain_core.tools import BaseTool\nprint(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")\n>> Tool schema: {'description': 'Calculates a single mathematical \nLet’s try out our new tool to evaluate an expression with complex numbers, which extend real \nTool Calls:\n================================= Tool Message ===========================\nrun this snippet, you should see that the LLM was able to query tools step by step:\nAs the last step, the LLM called the calculator tool with the expression \"sqrt(78*132)\".\nCreating a tool from a Runnable\nexample, we can use another LangChain chain or LangGraph graph as a tool.\ntool from any Runnable by explicitly specifying all needed descriptions.\ntool from a function in an alternative fashion, and we will tune the retry behavior (in our case, \nfrom langchain_core.tools import tool, convert_runnable_to_tool\ncalculator_tool = convert_runnable_to_tool(\nPlease note that we use the same function as above but we removed the @tool dec-\nRunnable (for example, a chain or a graph) to create a tool, and that allows us to build multi-agent \nour Runnable to a tool:\ncalculator_tool = convert_runnable_to_tool(\nllm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n'type': 'tool_call'}\nassert isinstance(calculator_tool, BaseTool)\nprint(f\"Tool name: {calculator_tool.name}\")\nprint(f\"Tool description: {calculator_tool.description}\")\nprint(f\"Args schema: {calculator_tool.args_schema.model_json_schema()}\")\n>> Tool name: calculator\nTool description: Calculates a single mathematical expression, incl.\ntool_call = llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).\ntool_calls[0]\nprint(tool_call)\n>> {'name': 'calculator', 'args': {'expression': '(2+3*i)**2'}, 'id': \nWe can call our calculator tool and pass it to the LangGraph configuration in runtime:\ncalculator_tool.invoke(tool_call[\"args\"], config=config)\ntional details to LangChain to ensure an LLM can correctly handle this tool.\nAnother method to define a tool is by creating a custom tool by subclassing the BaseTool class.\nAs with other approaches, you must specify the tool’s name, description, and argument schema.\nThis option is particularly useful when your tool needs to be stateful (for example, to \nclass method, which allows you to explicitly specify tools’ meta parameters such as description \nfrom langchain_core.tools import StructuredTool\ncalculator_tool = StructuredTool.from_function(\ntool_call = llm.invoke(\n\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\nIf an underlying function besides your tool is a synchronous function, LangChain will wrap it for \nTo conclude, let’s take another look at three options that we have to create a LangChain tool, and \nMethod to create a tool\n@tool decorator\nconvert_runnable_to_tool\ncontrolled on how arguments or tool descriptions are passed to \nYou need full control over tool description and logic (for \nTable 5.1: Options to create a LangChain tool\nenhance an LLM with tools; you need logging, working with exceptions, and so on even more.\nBaseTool has two special flags: handle_tool_error and handle_validation_error.",
      "keywords": [
        "tool",
        "calculator",
        "API",
        "Building Intelligent Agents",
        "expression",
        "LLM",
        "function",
        "Intelligent Agents",
        "message",
        "description",
        "schema",
        "LangChain",
        "Runnable",
        "Building Intelligent",
        "Args"
      ],
      "concepts": [
        "tool",
        "calculator",
        "calculates",
        "expression",
        "description",
        "descriptions",
        "agents",
        "function",
        "api",
        "schema"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "",
          "score": 0.69,
          "base_score": 0.54,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "",
          "score": 0.597,
          "base_score": 0.447,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 25,
          "title": "",
          "score": 0.553,
          "base_score": 0.553,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 36,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "calculator_tool",
          "tools",
          "description",
          "tool_call"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "calculator_tool",
          "tools",
          "description",
          "tool_call"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.293329667678464,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303532+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 232-240)",
      "start_page": 232,
      "end_page": 240,
      "summary": "tool_error function:\ndef _handle_tool_error(\ncontent = e.args[0] if e.args else \"Tool execution error\"\nf\"Got an unexpected type of `handle_tool_error`.\nfrom langchain_core.tools import StructuredTool\ncalculator_tool = StructuredTool.from_function(\nhandle_tool_error=True\nllm, [calculator_tool])\nTool Calls:\n================================= Tool Message ===========================\nI tried to use the calculator tool, but it \nenough, the LLM decides to respond itself without using the tool.\nthe messages key in the graph’s state) and checks whether it has tool calls.\nLangGraph validates the schema of the tool call, and if it doesn’t follow the expected schema, it \nconditional edge that cycles back to the LLM and then the LLM would regenerate the tool call, \nNow that we’ve learned what a tool is, how to create one, and how to use built-in LangChain tools, \nit’s time to take a look at additional instructions that you can pass to an LLM on how to use tools.\nsupport parallel function calling—specifically, an LLM can call multiple tools at once.\nhain natively supports this since the tool_calls field of an AIMessage is a list.\nToolMessage objects as function call results, you should carefully match the tool_call_id field \nAnother advanced capability is forcing an LLM to call a tool, or even to call a specific tool.\nspeaking, an LLM decides whether it should call a tool, and if it should, which tool to call from \nthe list of provided tools.\n\"auto\": An LLM can respond or call one or many tools.\n\"any\": An LLM is forced to respond by calling one or many tools.\n\"tool\" or \"any\" with a provided list of tools: An LLM is forced to respond by calling a tool \n\"None\": An LLM is forced to respond without calling a tool.\nSometimes, compilations of a provided schema to a schema supported by the model’s pro-\ncompiled to a str type if an underlying LLM doesn’t support Union types with tool calling.\ncustom tools, such as a code interpreter or Google search, that can be invoked by the model itself, \nand the model will use the tool’s output to prepare a final generation.\na tool it calls.\nLangChain wrapper with a custom tool created using the provider’s SDK rather than one built \nIncorporating tools into workflows\nIn Chapter 3, we started to discuss a controlled generation, when you want an LLM to follow a \nCalling a tool requires controlled generation since the generated payload should follow a specific \nschema, but we can take a step back and substitute our expected schema with a forced tool calling \nLLM has the with_structured_output method that takes a schema as a Pydantic model, converts \nit to a tool, invokes the LLM with a given prompt by forcing it to call this tool, and parses the \nit as a Pydantic model (a Plan is a list of Steps):\nresult = (prompt | llm.with_structured_output(Plan)).invoke(\neters that can instruct a model to generate a structured output (typically, a JSON or enum).\nforce the model to use JSON generation the same way as above using with_structured_output, \nmodel provider supports controlled generation as JSON):\nresult = (prompt | llm.with_structured_output(schema=plan_schema, \nresult = (prompt | llm_json | JsonOutputParser()).invoke(query)\nllm_enum = ChatVertexAI(model_name=\"gemini-1.5-pro-002\", response_mime_\nresult = (prompt | llm_enum | StrOutputParser()).invoke(review)\nLangChain abstracts the details of the model provider’s implementation with the method=\"json_\nTo simplify agent development, LangGraph has built-in capabilities such as ToolNode and tool_\nIf this message contains tool calls, it invokes the corresponding tools and updates the state.\nreturn {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\nbuilder.add_node(\"tools\", ToolNode([search, calculator]))\nbuilder.add_conditional_edges(\"invoke_llm\", tools_condition)\nbuilder.add_edge(\"tools\", \"invoke_llm\")\nTool-calling paradigm\nattempts to improve your prompts, think whether you could ask the model to call a tool instead.\nInstead, force a model to call a tool (and maybe even through a ReACT agent!).\n@tool\n\"\"\"Returns a date object given year, month and day.\nreturn date(year, month, day).isoformat()\n@tool\n\"\"\"Returns a date given a difference in days, weeks, months and years ",
      "keywords": [
        "llm",
        "tool",
        "model",
        "schema",
        "Call",
        "plan",
        "Building Intelligent Agents",
        "LangChain",
        "date",
        "JSON",
        "result",
        "step",
        "messages",
        "str",
        "Pydantic model"
      ],
      "concepts": [
        "tool",
        "message",
        "date",
        "model",
        "important",
        "type",
        "step",
        "agents",
        "results",
        "schema"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.745,
          "base_score": 0.595,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.627,
          "base_score": 0.477,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.607,
          "base_score": 0.457,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "tool",
          "schema",
          "calling",
          "llm",
          "respond"
        ],
        "semantic": [],
        "merged": [
          "tool",
          "schema",
          "calling",
          "llm",
          "respond"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29179274954363627,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303580+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 241-252)",
      "start_page": 241,
      "end_page": 252,
      "summary": "Building Intelligent Agents\nWe learned how to use tools, or function calls, to enhance LLMs’ performance on complex tasks.\nwhat an agent is.\nWhat are agents?\nbut there are many different definitions of what an agent is.\nLangChain itself defines an agent \nthat concept in mind, let’s describe some properties of an agent in the context of generative AI:\nAgents help a user solve complex non-deterministic tasks without being given an explicit \nTo solve a task, agents typically perform multiple steps and iterations.\nAgents utilize LLMs for reasoning (and solving tasks).\nLet’s start with an example and develop our agent.\nPlan-and-solve agent\ndemonstrated that plan-and-solve prompting improves LLM reasoning.\nFaced with a complex task, let’s first ask the LLM to come up with a detailed plan to solve this \ntask, and then use the same LLM to execute on every step.\n\"For the given task, come up with a step by step plan.\\n\"\n\"This plan should involve individual tasks, that if executed correctly \nBuilding Intelligent Agents\n(\"user\", \"Prepare a plan how to solve the following task:\\\nFor a step execution, let’s use a ReACT agent with built-in tools—DuckDuckGo search, retrievers \nfrom langchain.agents import load_tools\nerated plan, and let’s add past_steps and final_response to the state:\n\"\"\"Returns formatted plan with step numbers and past results.\"\"\"\nfor i, step in enumerate(state[\"plan\"]):\nPlanned step: {step}\\n\"\nplan = await planner.ainvoke(state[\"task\"])\nstep = await execution_agent.ainvoke({\"plan\": get_full_plan(plan), \n\"step\": plan.steps[current_step], \"task\": state[\"task\"]})\nfinal_response = await (final_prompt | llm).ainvoke({\"task\": \nstate[\"task\"], \"plan\": get_full_plan(state)})\nBuilding Intelligent Agents\nFigure 5.3: Plan-and-solve agentic workflow\nLLM prompt with a given task.\nWe started by building a ReACT agent from scratch and \nan LLM to call any tool or a specific one, and instructing it to return responses in structured \nFinally, we built our first plan-and-solve agent with LangGraph, applying all the concepts we’ve \ntinue discussing how to develop agents and look into more advanced architectural patterns.\nHow would you define a generative AI agent?\nBuilding Intelligent Agents\nIn the previous chapter, we defined what an agent is.\ning effective agents involves several distinct design patterns every developer should be familiar \nIn this chapter, we’re going to discuss key architectural patterns behind agentic AI.\nWe will develop an advanced agent with self-reflection that uses tools to answer complex exam \nimplementing agentic architectures, such as details about LangGraph streaming and ways to \nWe will also look into the Tree-of-Thoughts (ToT) pattern and develop a ToT agent ourselves, \nAdvanced Applications and Multi-Agent Systems\nAgentic architectures\nMulti-agent architectures\nAgent memory\nAgentic architectures\nAs we learned in Chapter 5, agents help humans solve tasks.\nAs we discussed in Chapter 5, agents don’t have a specific algorithm to follow.\nLet’s recall the ReACT agent we learned about in Chapter 5, an example of a tool-calling pattern:\nagents.\nYou will see these patterns in various combinations across different domains and agentic \nwith few-shot examples and split complex tasks into smaller steps.\npartial control over the task decomposition and planning process, managing the flow by \nsolve agent.\nduce cooperation between multiple instances of LLM-enabled agents.\nskill sets by initiating your agents with different system prompts, available toolsets, etc.\nAdvanced Applications and Multi-Agent Systems\ntasks and expected outputs, and simulators, a safe way for LLMs to interact with tools, are key \nto building really complex and effective agents.\nprinciples by discussing various agentic architectures and looking at examples.\nenhancing the RAG architecture we discussed in Chapter 4 with an agentic approach.\nAgentic RAG\nLLMs enable the development of intelligent agents capable of tackling complex, non-repetitive \nask an LLM to generate an answer given a system prompt, combined context, and the question.\nQuery expansion tasks an LLM to generate multiple queries based on initial ones, and \nof context in front of an LLM, you perform many smaller reasoning steps in parallel first.\nReflection steps and iterations task LLMs to dynamically iterate on retrieval and query \nBased on our definition from the previous chapters, RAG becomes agentic RAG when you have \nit becomes agentic RAG.\nMulti-agent architectures\nWe built a plan-and-solve agent that goes a step further than CoT and \nencourages the LLM to generate a plan and follow it.\nmulti-agent one since the research agent (which was responsible for generating and following \nthe plan) invoked another agent that focused on a different type of task – solving very specific ",
      "keywords": [
        "LLM",
        "plan",
        "Agents",
        "task",
        "step",
        "LLMs",
        "Building Intelligent Agents",
        "state",
        "Intelligent Agents",
        "tools",
        "final",
        "Building",
        "contract started",
        "RAG",
        "complex"
      ],
      "concepts": [
        "agents",
        "tasks",
        "step",
        "plan",
        "planned",
        "tools",
        "prompting",
        "llm",
        "reason",
        "state"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 34,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.711,
          "base_score": 0.711,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 25,
          "title": "",
          "score": 0.659,
          "base_score": 0.659,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "plan",
          "agent",
          "task",
          "agents",
          "solve"
        ],
        "semantic": [],
        "merged": [
          "plan",
          "agent",
          "task",
          "agents",
          "solve"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37720823833186884,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303637+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 253-261)",
      "start_page": 253,
      "end_page": 261,
      "summary": "Advanced Applications and Multi-Agent Systems\nWe will look into a few core agentic architectures in the remainder of this chapter, and introduce \ndevelop agents.\nWe’ll begin with discussing the importance of specialization in multi-agentic \nAgent roles and specialization\nIn fact, developing specialized agents offers \nSecond, specialized agents help manage complexity.\nAs a best practice, limit each agent to 5-15 different tools, rather than overload-\ning a single agent with all available tools.\nBesides becoming specialized, keep your agents modular.\nprove such agents.\nup with many different agents available for users and developers within your organization that \nHence, keep in mind that you should make such specialized agents \nbuilder.add_node(\"pay\", payments_agent)\nWrap the child agent’s invocation with a Python function and use it within the definition \nresult = payments_agent.invoke({\"client_id\"; state[\"client_id\"]})\nNote, that your agents might have different schemas (since they perform different tasks).\noption gives you full control over how you construct a state that is passed to the child agent, and \nhow the state of the parent agent should be updated as a result.\nWe can let multiple agents work on the same tasks in parallel as well.\nThese agents might have \nAdvanced Applications and Multi-Agent Systems\nconsensus across multiple agents:\nLet each agent see other solutions and score each of them on a scale of 0 to 1, and then \nDevelop another agent that excels at the task of selecting the best solution for a general \nIf you task N agents with \nThe third architecture option is to let agents communicate and work collaboratively on a task.\nexample, the agents might benefit from various personalities configured through system prompts.\napplication and how your agents communicate.\nAdvanced Applications and Multi-Agent Systems\nAgents can work collaboratively on a task by providing critique and reflection.\na slightly different system prompt); cross-reflection, when you use another agent (for example, \nto decide which agent to send a message or a task), introduce a certain hierarchy, or develop more \npage at https://langchain-ai.github.io/langgraph/concepts/multi_agent/).\nWhat and how many agents should we include in our system?\nWhat tools should each agent have access to?\nHow should agents interact with each other and through which mechanism?\nNow that we’ve examined some core considerations and open questions around multi-agent \ncommunication, let’s explore two practical mechanisms to structure and facilitate agent inter-\ninteraction, detailing the specific formats and structures that agents can use to effectively exchange \nAmong many different ways to organize communication between agents in a true multi-agent \nImagine we have implemented three types of agents – one answering general questions grounded \nquestion and routes it to the corresponding agent based on classification results.\nEach agent (or \nextract both goals and route the execution to two specialized agents with different tasks.\nAdvanced Applications and Multi-Agent Systems\nThere are two ways to organize communication in multi-agent systems:\nAgents communicate via specific structures that force them to put their thoughts and \nWe saw how our planning node communicated with the ReACT agent via \nfrom different agents to the shared list of messages!.\nthe history of communication between multiple agents, go with the first approach and let them \nLet’s develop a research agent that uses tools to \nLet’s start with a ReACT agent, but let’s deviate from a default system prompt and write our own \nLet’s focus this agent on being creative and working on an evidence-based solution (please \nfrom langchain.agents import load_tools\nfrom langgraph.prebuilt import create_react_agent\nAdvanced Applications and Multi-Agent Systems\nNow, let’s create the agent itself.\nSince we have a custom prompt for the agent, we need a prompt \nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nWe used a specialized research agent based on the \nto it, and use another role profile for an agent who will actionably criticize our “student’s” work:",
      "keywords": [
        "Agent",
        "task",
        "system",
        "prompt",
        "Multi-Agent Systems",
        "specialized agents",
        "consensus mechanism",
        "mechanism",
        "tools",
        "LLM",
        "Advanced Applications",
        "messages",
        "consensus",
        "question",
        "questions"
      ],
      "concepts": [
        "agents",
        "prompts",
        "question",
        "questions",
        "important",
        "importance",
        "useful",
        "uses",
        "different",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 34,
          "title": "",
          "score": 0.516,
          "base_score": 0.366,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.344,
          "base_score": 0.344,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "",
          "score": 0.342,
          "base_score": 0.342,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.34,
          "base_score": 0.34,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "agents",
          "specialized agents",
          "multi agent",
          "multi"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "agents",
          "specialized agents",
          "multi agent",
          "multi"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22864380035195772,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303678+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 262-269)",
      "start_page": 262,
      "end_page": 269,
      "summary": "\"Reflect on the answer and provide a feedback whether the answer \"\nOnly provide critique if you think the answer might \nanswer: Optional[str] = Field(\nNow we need another research agent that takes not only question and answer options but also the \nanswer or critique is provided), or handling conflicting or ambiguous feedback (for example, struc-\ntools=research_tools, state_schema=ReflectionState, prompt=prompt)\nWhen defining the state of our graph, we need to keep track of the question and answer options, \nthe current answer, and the critique.\nLet’s define the full state, nodes, \nif state.get(\"response\") and state[\"response\"].answer:\nif state.get(\"steps\", 1) > max_reasoning_steps:\ndef _reflection_step(state):\nresult = reflection_chain.invoke(state)\ndef _research_start(state):\nanswer = research_agent.invoke(state)\ndef _research(state):\nagent_state = {\n\"answer\": state[\"answer\"],\n\"options\": state[\"options\"],\n\"feedback\": state[\"response\"].critique\nanswer = research_agent_with_critique.invoke(agent_state)\nbuilder.add_node(\"research_start\", _research_start)\nbuilder.add_node(\"research\", _research)\nbuilder.add_node(\"reflect\", _reflection_step)\nasync for _, event in graph.astream({\"question\": question, \"options\": \noptions}, stream_mode=[\"updates\"]):\nanswer among the options provided.\nstudent states that none of the answers are entirely true, but multiple-\nproperly evaluate the answer, the search results need to be provided, and \nThe Stream method allows you to stream changes to the graph’s state after each super-step.\nover the graph where parallel nodes belong to a single super-step while sequential nodes belong \nStreams only updates to the graph \nStreams the full state of the graph after \nstate\nStreams events emitted by the node \nStreams full events (for example, \nwith the values mode, we’ll get the full state returned after every super-step (you can see that \nasync for _, event in research_agent.astream({\"question\": question, \n\"options\": options}, stream_mode=[\"values\"]):\nber that parallel nodes can be called within a single super-step) and a corresponding update to \nthe state sent by this node:\nasync for _, event in research_agent.astream({\"question\": question, \n\"options\": options}, stream_mode=[\"updates\"]):\nprint(node, len(event[node].get(\"messages\", [])))\nasync for event in research_agent.astream_events({\"question\": question, \nSo far, we have learned that a node in LangGraph does a chunk of work and sends updates to a \ncommon state, and an edge controls the flow – it decides which node to invoke next (in a deter-\nyour nodes can be not only functions but other agents, or subgraphs (with their own state).\nmight need to combine state updates and flow controls.\nLangGraph allows you to do that with a Command – you can update your graph’s state and at the \nsame time invoke another agent by passing a custom state to it.\nof the current state to be sent to your graph – and goto – a name (or list of names) of the nodes \nA destination agent can be a node from the current or a parent (Command.PARENT) graph.\npattern and the Send class, which allowed us to invoke a node in the graph by passing a specific ",
      "keywords": [
        "answer",
        "state",
        "research",
        "node",
        "options",
        "question",
        "graph",
        "agent",
        "Response",
        "stream",
        "event",
        "start",
        "llm",
        "critique",
        "feedback"
      ],
      "concepts": [
        "state",
        "answer",
        "agents",
        "event",
        "streaming",
        "messages",
        "feedback",
        "nodes",
        "question",
        "questions"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 32,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.716,
          "base_score": 0.566,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.597,
          "base_score": 0.447,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "state",
          "answer",
          "options",
          "graph",
          "node"
        ],
        "semantic": [],
        "merged": [
          "state",
          "answer",
          "options",
          "graph",
          "node"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28707171942888865,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303727+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 270-277)",
      "start_page": 270,
      "end_page": 277,
      "summary": "A few chapters earlier, we discussed how two agents can communicate via controlled output (by \nillustrate how agents can communicate with native LangChain messages.\nagent with a cross-reflection and make it work with a shared list of messages.\nagent itself looks simpler – it has a default state since it gets a user’s question as a HumanMessage:\nresearch_agent = create_react_agent(\nThe nodes themselves also look simpler, but we add Command after the reflection node since we \nAlso, we don’t wrap a ReACT research agent as a \nreturn {\"messages\": [(\"human\", question_template.invoke(state).text)]}\nmessages = event[\"messages\"] + [(\"human\", reflection_prompt)]\nbuilder.add_node(\"ask_question\", _ask_question)\nbuilder.add_node(\"research\", research_agent)\nbuilder.add_node(\"reflect\", _give_feedback)\nbuilder.add_edge(START, \"ask_question\")\nbuilder.add_edge(\"ask_question\", \"research\")\nbuilder.add_edge(\"research\", \"reflect\")\npany offers the LangGraph platform – a commercial solution that helps you develop, manage, and \ndeploy agentic applications.\nIDE that helps you visualize and debug your agents – and another is LangGraph Server.\nThe LangGraph platform offers you a native way to deploy agents, and it \nwraps them with a unified API (which makes it easier for your applications to use these agents).\nWhen you’ve built your agent as a LangGraph graph object, you deploy an assistant – a specific \nLangGraph Server also allows you to schedule stateless runs – they are not assigned to any thread, \nAs we discussed in Chapter 5, generative AI agents are adaptive \nsteps (like our ReACT agent adjusting based on search results).\nWe saw how to add a reflection step to our plan-and-solve agent.\noutput of the steps performed so far, we’ll ask the LLM to reflect on the plan and adjust it.\nAn agent can ask a human to approve or reject \na payment), provide additional context to the agent, or give a specific input by modifying the \nImagine we’re developing an agent that searches for job postings, generates an application, and \nlogic might be more complex – the agent might be collecting data about the user, and for some \ngraph, a client should use the Command class, which we discussed earlier in this chapter.\nnode invoking the interrupt function (if there are multiple interrupts in your node, LangGraph \nYou can also use Command to route to different nodes based on the user’s \nOf course, you can use interrupt only when a checkpointer is provided to the graph since \nLet’s construct a very simple graph with only the node that asks a user for their home address:\nbuilder.add_node(\"human_input\", _human_input)\nbuilder.add_edge(START, \"human_input\")\nfor chunk in graph.stream({\"messages\": [(\"human\", \"What is weather \nThe graph returns us a special __interrupt__ state and stops.\nNote that the graph continued to execute the human_input node, but this time the interrupt \nfunction returned the result, and the graph’s state was updated.\nSo far, we’ve discussed a few architectural patterns on how to develop an agent.\nLet’s return to the plan-and-solve agent we built in the previous chapter.\nthe plan on every step (we might need to increase the temperature of the underlying LLM).\nwould help the agent to be more adaptive since the next plan generated will take into account \nWe’ll take the same components of the plan-and-solve agents \nwe developed in Chapter 5 – a planner that creates an initial plan and execution_agent, which \nis a research agent with access to tools and works on a specific step in the plan.\nexecution agent simpler since we don’t need a custom state:\nexecution_agent = prompt_template | create_react_agent(model=llm, \ngenerates multiple potential next steps, encouraging exploration of different solution paths rather ",
      "keywords": [
        "agent",
        "LangGraph",
        "messages",
        "state",
        "question",
        "graph",
        "Command",
        "research agent",
        "LangGraph Server",
        "node",
        "human",
        "user",
        "research",
        "prompt",
        "Advanced Applications"
      ],
      "concepts": [
        "agents",
        "messages",
        "state",
        "step",
        "builder",
        "graph",
        "chapters",
        "result",
        "llm",
        "provide"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.732,
          "base_score": 0.582,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "",
          "score": 0.597,
          "base_score": 0.447,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.561,
          "base_score": 0.411,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "plan",
          "graph",
          "builder",
          "node"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "plan",
          "graph",
          "builder",
          "node"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2905431472398811,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303776+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 278-285)",
      "start_page": 278,
      "end_page": 285,
      "summary": "node = self\nasync def _run_node(state: PlanState, config: RunnableConfig):\nnode = state.get(\"next_node\")\nif node is None:\nwhile queue and not node:\nnode = state[\"queue\"].popleft()\nnode = None\nif not node:\n\"previous_steps\": node.get_full_plan(),\n\"step\": node.step,\nreturn {\"current_node\": node, \"queue\": queue, \"visited_ids\": visited_ids, \n\"next_node\": None}\nnode = state[\"current_node\"]\nplan\": node.get_full_plan()})\nreturn {\"is_current_node_final\": True}\nchild = TreeNode(node_id=max_id+1, step=step, parent=node)\nreturn {\"is_current_node_final\": False, \"next_node\": child, \"max_id\": \nasync def _get_final_response(state: PlanState) -> PlanState:\nnode = state[\"current_node\"]\nfinal_response = await responder.ainvoke({\"task\": state[\"task\"], \"plan\": \nnode.get_full_plan()})\nnode.final_response = final_response\nreturn {\"paths_explored\": 1, \"candidates\": [final_response]}\nThe _run_node function executes the current step, while _plan_next generates new candidate \nWhen we reach a final node (one where no further \nsteps are needed), _get_final_response generates a final solution by picking the best one from \nstate, we should keep track of the root node, the next node, the queue of nodes to be explored, \nis_current_node_final: bool\ndef my_node(state):\nqueue.append(another_node)\nroot = TreeNode(step=plan.steps[0], node_id=1)\nchild = TreeNode(node_id=i+2, step=step, parent=current_root)\nasync def _get_final_response(state: PlanState) -> PlanState:\nnode = state[\"current_node\"]\nfinal_response = await responder.ainvoke({\"task\": state[\"task\"], \"plan\": \nnode.get_full_plan()})\nnode.final_response = final_response\nreturn {\"paths_explored\": 1, \"candidates\": [final_response]}\ndef _should_create_final_response(state: PlanState) -> Literal[\"run\", \nreturn \"generate_response\" if state[\"is_current_node_final\"] else \"run\"\nif state[\"queue\"] or state.get(\"next_node\"):\nbuilder.add_node(\"initial_plan\", _build_initial_plan)\nbuilder.add_node(\"plan_next\", _plan_next)\nbuilder.add_node(\"generate_response\", _get_final_response)\nbuilder.add_node(\"vote\", _vote_for_the_best_option)\nbuilder.add_conditional_edges(\"plan_next\", _should_create_final_response)",
      "keywords": [
        "node",
        "final",
        "response",
        "state",
        "plan",
        "step",
        "task",
        "candidates",
        "queue",
        "max",
        "current",
        "Optional",
        "PlanState",
        "builder.add",
        "Advanced Applications"
      ],
      "concepts": [
        "step",
        "state",
        "node",
        "candidates",
        "plan",
        "planned",
        "returns",
        "important",
        "queue",
        "task"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 12,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.58,
          "base_score": 0.43,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "",
          "score": 0.502,
          "base_score": 0.352,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.5,
          "base_score": 0.35,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 26,
          "title": "",
          "score": 0.498,
          "base_score": 0.348,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "node",
          "state",
          "queue",
          "final_response",
          "planstate"
        ],
        "semantic": [],
        "merged": [
          "node",
          "state",
          "queue",
          "final_response",
          "planstate"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.16908928329071074,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303821+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 286-294)",
      "start_page": 286,
      "end_page": 294,
      "summary": "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models.\nAdvanced Applications and Multi-Agent Systems\nThe key idea was that instead of exploring the whole tree, they use an LLM to evaluate the quality \nNow, as we’ve discussed some more advanced architectures that allow us to build better agents, \nagents.\nAgent memory\nWe discussed memory mechanisms in Chapter 3.\nAs you can imagine, for complex agents, this memory mechanism might be inefficient for two \nration phase when looking for a solution, an agent might learn something important about the \nof long-term memory, which helps an agent to accumulate knowledge and gain from historical \nHow to design and use long-term memory in practice is still an open question.\nmentations of long-term memory for agentic workflows yet.\nuse two components – a built-in cache (a mechanism to cache LLMs responses), a built-in store \n(a persistent key-value store), and a custom cache or database.\nYou need advanced read or write access patterns when working with this memory.\nCaching allows you to save and retrieve key values.\nLangChain allows you to set a global cache for LLM responses in the following way (after you \nhave initialized the cache, the LLM’s response will be added to the cache, as we’ll see below):\nfrom langchain_core.globals import set_llm_cache\nCaching with LangChain works as follows: Each vendor’s implementation of a ChatModel inherits \nprint(langchain.llm_cache._cache)\nLangChain supports in-memory and SQLite caches out of the box (they form part of langchain_\ncore.caches), and there are also many vendor integrations – available through the langchain_\nAdvanced Applications and Multi-Agent Systems\nfrom langgraph.store.memory import InMemoryStore\nin_memory_store.put(namespace=(\"users\", \"user1\"), key=\"fact1\", \nin_memory_store.put(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\", \nin_memory_store.get(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\")\nin_memory_store.get(namespace=(\"users\", \"user1\"), key=\"conv1\")\nprint(len(in_memory_store.search((\"users\", \"user1\", \"conv1\"), \nprint(len(in_memory_store.search((\"users\", \"user1\"), query=\"name\")))\nAs you can see, we were able to retrieve all relevant facts stored in memory by using a partial search.\nAgentic memory \nWe also discussed different agentic AI design patterns and how to develop agents that leverage \nsomewhat similar to a conversation instance), and we learned how to add memory to our workflow \nIn the next chapter, we’ll take a look at how generative AI transforms the software engineering \nindustry by assisting in code development and data analysis.\nAdvanced Applications and Multi-Agent Systems\nName at least three design patterns to consider when building generative AI agents.\nagentic systems.\nSoftware Development and Data \nnew programming languages or frameworks, developers can now articulate their intent in natural \nlanguage, leaving it to advanced LLMs and frameworks such as LangChain to translate these ideas \nMoreover, while traditional programming languages remain \nWe’ll specifically discuss LLMs’ place in software development and the state of the art of perfor-\nWe’ll see how to use LLM chains and agents to help in code \ncode with LLMs, giving examples with different models be it on Google’s generative AI services, \nAfter this, we’ll move on to more advanced approaches with agents \nWe’ll also be applying LLM agents to data science: we’ll first train a model on a dataset, then we’ll \nSoftware Development and Data Analysis Agents\nLLMs in software development\nApplying LLM agents for data science\nLLMs in software development\nTraditional programming languages remain essential in software development—C++ \nfor rapid development, data analysis, and ML workflows.\nEnglish, now serves as a powerful interface to streamline software development and data science \nemerged to support this development approach, each offering different capabilities for AI-assisted \nThe software development landscape has long sought to make programming more accessible \nto simplify syntax, allowing developers to express logic with fewer lines of code.\nbuilt components to democratize application development beyond traditional coding experts.\nThe latest and perhaps most transformative evolution features natural language programming \nplementations, natural language programming generates standard code without vendor lock-in, \ntraditional programming languages handle precise implementation details.\nHowever, this evolution does not spell the end for traditional programming languages.\nbracing a hybrid workflow where natural language directives, powered by LLMs and frameworks \nreduce time spent on boilerplate code, while traditional programming remains essential for the ",
      "keywords": [
        "natural language",
        "memory",
        "LLM",
        "Software Development",
        "Language",
        "LLMs",
        "cache",
        "development",
        "traditional programming languages",
        "programming languages",
        "agents",
        "LangGraph",
        "data",
        "programming",
        "code"
      ],
      "concepts": [
        "developed",
        "cache",
        "caching",
        "coding",
        "agent",
        "language",
        "langchain",
        "llm",
        "advanced",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.54,
          "base_score": 0.54,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 30,
          "title": "",
          "score": 0.516,
          "base_score": 0.366,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "programming",
          "memory",
          "software development",
          "languages",
          "programming languages"
        ],
        "semantic": [],
        "merged": [
          "programming",
          "memory",
          "software development",
          "languages",
          "programming languages"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34214846620930983,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303871+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 295-302)",
      "start_page": 295,
      "end_page": 302,
      "summary": "The 2025 OpenAI SWE-Lancer benchmark study found that even the top-performing model com-\nresearch identified specific challenges including surface-level problem-solving, limited context \nDespite these limitations, many organizations report productivity gains when using AI coding \nCode maintenance has evolved through AI-assisted approaches where developers use natural \ned specific coding tasks 55% faster in controlled experiments, independent field studies show \nimprovements, including automating aspects of code review and security scanning.\nspeed improvements, research consistently shows AI coding assistants reduce developer cognitive \nimportant limitations: generated code often requires significant human verification and rework, \nwith some independent research reporting higher bug rates in AI-assisted code.\nThe field of code debugging has been enhanced as natural language queries help developers \ninternal policies and code repositories, has significantly reduced routine task turnaround times, \nallowing development teams to focus on more strategic work (AXA, AXA offers secure Generative \nWhen it comes to understanding complex systems, developers can use LLMs to generate explana-\nreported implementing a platform that uses generative AI to produce real-time insights via chat \nerate boilerplate code and automate routine tasks, human oversight remains critical for system \nEvolution of code LLMs\nThe development of code-specialized LLMs has followed a rapid trajectory since their inception, \nThe first Foundation phase (2021 to early 2022) introduced the first viable code generation models \nfor AI-assisted development across different programming languages and tasks.\nFigure 7.1: Evolution of code LLMs (2021–2024)\nFigure 7.1 illustrates the progression of code-specialized language models across commercial (upper \nGemini 2.5 Pro (March 2025) and specialized code models such as Mistral AI’s Codestral series.\ning—commonly known as code LLMs. These models are rapidly evolving, each with its own \nCode generation: Transforming natural language requirements into code snippets or \nFor instance, developers can generate boilerplate code or entire modules \nCode documentation: Automatically generating docstrings, comments, and technical \ndocumentation from existing code or specifications.\nCode editing and refactoring: Automatically suggesting improvements, fixing bugs, and \nThe landscape of code-specialized LLMs has grown increasingly diverse and complex.\nlution raises critical questions for developers implementing these models in production environ-\ncompare in terms of code quality, accuracy, and reasoning capabilities?\nBenchmarks for code LLMs\nObjective benchmarks provide standardized methods to compare model performance across a va-\nriety of coding tasks, languages, and complexity levels.\nFor LangChain developers specifically, understanding benchmark results offers several advantages:\nCode-generating LLMs demonstrate varying capabilities across established benchmarks, with \nOn Large Language Models For Code Generation, 2025), while Claude 3 Opus reaches 84.9% on the \nStandard benchmarks provide useful but limited insights into model capabilities for LangChain \nClassEval: This newer benchmark tests class-level code generation, addressing some lim-\nModels in Class-Level Code Generation, 2024) shows performance degradation of 15–30% \nmodels on bug-fixing tasks from actual GitHub repositories.\nEven top-performing models \nsynthetic benchmarks and authentic coding challenges.\nWhen implementing code-generating LLMs within LangChain frameworks, several key chal-\nRepository-level problems that require understanding multiple files, dependencies, and context \nleagues, Evaluating Large Language Models in Class-Level Code Generation, 2024) demonstrated that \nLLMs find class-level code generation “significantly more challenging than generating standalone \nLLMs can be leveraged to understand repository-level code context despite the inherent chal-\nPython codebases with LangChain, loading repository files as context for the model to consider \nThis implementation uses GPT-4o to generate code while considering the context of entire repos-\nGenerated code often appears superficially correct but contains subtle bugs or security vulner-\ncode produced by developers with access to AI coding assistants compared to those without.\nof professionals needed to substantially rework AI-generated code before it was production-ready, \nSecurity researchers have identified a persistent risk where AI models inadvertently introduce \nhuman review and testing of AI-generated code before production deployment.\nically analyzes generated code for common issues, serving as a first line of defense against subtle \nvalidation_template = \"\"\"Analyze the following Python code for:\nCode to analyze:\n{generated_code}\nvalidation_prompt = PromptTemplate( input_variables=[\"generated_code\"], \nThis validation approach creates a specialized LLM-based code review step in the workflow, fo-\nWhen deploying code-generating LLMs in production LangChain applications, several factors \nstrate superior performance on code benchmarks, open-source alternatives such as Llama \nes to addressing security vulnerabilities, preventing hallucinations, and ensuring code quality ",
      "keywords": [
        "Code",
        "Large Language Models",
        "models",
        "language models",
        "Data Analysis Agents",
        "code generation",
        "Evaluating Large Language",
        "Development",
        "language",
        "natural language",
        "Software Development",
        "LLMs",
        "code LLMs",
        "context",
        "Large Language"
      ],
      "concepts": [
        "code",
        "model",
        "development",
        "generated",
        "generate",
        "security",
        "secure",
        "benchmark",
        "implementation",
        "implementations"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.621,
          "base_score": 0.621,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.618,
          "base_score": 0.618,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.604,
          "base_score": 0.604,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.57,
          "base_score": 0.57,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "benchmarks",
          "llms",
          "code generation",
          "language"
        ],
        "semantic": [],
        "merged": [
          "code",
          "benchmarks",
          "llms",
          "code generation",
          "language"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3627261793943686,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.303925+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 303-311)",
      "start_page": 303,
      "end_page": 311,
      "summary": "Security vulnerabilities in LLM-generated code present significant risks, particularly when dealing \nchain can be integrated into any LangChain workflow that involves code generation, providing \nfrom langchain_core.output_parsers import PydanticOutputParser \n# Define the Pydantic model for structured output\n\"\"\"Security analysis results for generated code.\"\"\"\n# Initialize the output parser with the Pydantic model\ntemplate=\"\"\"Analyze the following code for security vulnerabilities: \n{code}\ninput_variables=[\"code\"], \nLLM-generated code should never be directly \nsafe execution environments for testing generated code.\nTo ensure security when building LangChain applications that handle code, a layered approach \nStructure security findings using Pydantic models and LangChain’s output parsers for consistent, \nAlways isolate the execution of LLM-generated code in sandboxed environ-\nimprove code generation through feedback loops incorporating execution results and validation \nMaintain comprehensive logging of all code generation steps, security findings, and mod-\nAdhere to the principle of least privilege by generating code that follows \nversion control to store generated code and implement human review for critical components.\nValidation framework for LLM-generated code\nOrganizations should implement a structured validation process for LLM-generated code and \nthe generated code with representative test data and carefully verify that outputs align \nbusiness requirements, as LLMs sometimes produce impressive-looking code that misses \nexecution time of LLM-generated code against existing solutions to identify potential \nSecurity screening should never be an afterthought when working with generated code.\nsues that LLMs may introduce despite their training in secure coding practices.\nmodel has been instructed to include API access.\nor unintentional data exposures that could create security vulnerabilities in production.\ncases and unexpected inputs that reveal how the code handles extreme conditions.\nEvaluate the code’s resilience to malformed or missing data, \nable use of the generated code.\nEither require the LLM to provide or separately generate \nCreate validation reports that link code functionality directly to business requirements, \nLangChain integrations\nLangChain-integrated agent can safely execute code using dedicated interpreters, interact with \nIntegrations range from code execution and database querying to financial analysis and repos-\nenvironments to safely execute code.\nor data processing tasks to dedicated code interpreters, thereby increasing accuracy and \nto interact with code repositories, streamlining tasks such as issue management, code \nstrate how to generate functional software code with LLMs and execute it directly from within \nWriting code with LLMs \nIn this section, we demonstrate code generation using various models integrated with LangChain.\nThese examples illustrate LangChain’s flexibility in working with various code generation models, \nThe Google generative AI platform offers a range of models designed for instruction following, \nconversion, and code generation/assistance.\nThese models also have different input/output limits \nLet’s see if the Gemini Pro model can solve FizzBuzz, a \ngithub.com/benman1/generative_ai_with_langchain.\nFor any questions or if you have any trouble running the code, please create an issue \nTo test the model’s code generation capabilities, we’ll use LangChain to interface with Gemini \nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\nThe model produced an efficient, well-structured solution that correctly implements the logic \nHugging Face hosts a lot of open-source models, many of which have been trained on code, some \nmodels) or write code (instruction-tuned models).\nthese models and run them locally, or you can access them through the Hugging Face API.\nfrom langchain.llms import HuggingFacePipeline\ndef calculate_primes(n):\n# Use the LangChain LLM to generate text\nthe docstring, understanding that the function should return all prime numbers up to n rather \nThe generated code demonstrates how specialized \ncode models can produce working implementations from minimal specifications.\nrequires authentication with a personal access token to view or use the model; you need to create \nWhen our code from the previous example executes successfully with CodeGemma, it generates \na complete implementation for the prime number calculator function.\ndef calculate_primes(n):\n\"\"\"Return True if n is prime.\"\"\"\nNotice how the model not only implemented the requested calculate_primes() function but also \nThe model even added a complete main() function \nresources, we can also run models directly on Hugging Face’s infrastructure using their Inference \nfrom langchain.llms import HuggingFaceHub\n# Choose a lightweight model good for code generation",
      "keywords": [
        "code",
        "Data Analysis Agents",
        "Data",
        "Hugging Face",
        "LangChain",
        "model",
        "Data Analysis",
        "code generation",
        "Security",
        "Analysis Agents",
        "generated code",
        "validation",
        "llm",
        "LLM-generated code",
        "Analysis"
      ],
      "concepts": [
        "model",
        "data",
        "imports",
        "importantly",
        "langchain",
        "requirements",
        "requires",
        "generated",
        "generate",
        "integrations"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 11,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.582,
          "base_score": 0.432,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 28,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "generated code",
          "security",
          "generated",
          "code generation"
        ],
        "semantic": [],
        "merged": [
          "code",
          "generated code",
          "security",
          "generated",
          "code generation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30479569996358596,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.303977+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 312-319)",
      "start_page": 312,
      "end_page": 319,
      "summary": "# Use the LangChain LLM to generate text\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.prompts.prompt import PromptTemplate\nprint(llm_chain.invoke(text))\nTo calculate the prime numbers up to a given number N, we can follow the \nnumbers = list(range(2, n + 1))\n# Initialize an empty list to store the prime numbers\nprimes = []\n# Take the first number as prime\nprime = numbers[0]\n# Remove all multiples of the prime number from the list\nreturn primes\nWe initialize an empty list called `primes` to store the prime numbers that \nsider it as a prime number.\nWe append this prime number to the `primes` \nWe remove all the multiples of the prime number from the `numbers` list us-\nprime numbers among them.\nFinally, we return the `primes` list, which contains all the prime numbers \nThe function correctly returns the list of prime numbers up to 20.\nWe can also create an LLM agent that can execute Python code to solve problems:\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import load_tools, initialize_agent, AgentType\nfrom langchain_experimental.tools import PythonREPLTool\ntools = [PythonREPLTool()]   # Gives agent ability to run Python code\nllm,  # Language model to power the agent\nresult = agent(\"What are the prime numbers until 20?\")\nI can write a Python script to find the prime numbers up to 20.\n{'input': 'What are the prime numbers until 20?', 'output': '[2, 3, 5, 7, \nWhat is also quite interesting is the use of documents to help write code or to ask questions \nHere’s an example of loading all documentation pages from LangChain’s \nfrom langchain_community.document_loaders import DocusaurusLoader\nloader = DocusaurusLoader(\"https://python.langchain.com\")\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nNow we’ll create a vector store from the document splits:\n# Store document embeddings for efficient retrieval\nfrom langchain import hub\nquestions using the LangChain documentation.\nfrom langchain_community.document_loaders.generic import GenericLoader\nfrom langchain_community.document_loaders.parsers import LanguageParser\nfrom langchain_text_splitters import Language, RecursiveCharacter-\nAfter cloning the repository, we need to parse the Python files using LangChain’s specialized \ntexts = python_splitter.split_documents(documents)\nThis code performs three key operations: it clones our book’s GitHub repository, loads all Python \nNow we’ll create our RAG system by embedding these code chunks and setting up a retrieval chain:\ndocument_chain = create_stuff_documents_chain(ChatOpenAI(), prompt)\nqa = create_retrieval_chain(retriever, document_chain)\nHere, we’ve built our complete RAG pipeline: we store code embeddings in a Chroma vector \nresults), and create a QA chain that combines retrieved code with our prompt template before \nLet’s test our code-aware RAG system with a question about software development examples:",
      "keywords": [
        "prime numbers",
        "prime",
        "numbers",
        "Data Analysis Agents",
        "LangChain",
        "RAG",
        "list",
        "code",
        "prime numbers primes",
        "python",
        "agent",
        "Software Development",
        "Create",
        "LLM",
        "chain"
      ],
      "concepts": [
        "code",
        "coding",
        "documentation",
        "document",
        "model",
        "python",
        "agents",
        "prompt",
        "retrieval",
        "retriever"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 36,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 54,
          "title": "",
          "score": 0.505,
          "base_score": 0.355,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.472,
          "base_score": 0.322,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prime",
          "prime numbers",
          "numbers",
          "primes",
          "import"
        ],
        "semantic": [],
        "merged": [
          "prime",
          "prime numbers",
          "numbers",
          "primes",
          "import"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2474959431492553,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304026+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 320-327)",
      "start_page": 320,
      "end_page": 327,
      "summary": "In the next section, we’ll explore how generative AI agents can automate and enhance data science \nLangChain agents can write and execute code, analyze datasets, and even build and \ntraining a neural network model and analyzing a structured dataset.\nApplying LLM agents for data science\ning to multiple studies, LLMs demonstrate variable effectiveness across different data science \nthe data analysis task increased,” highlighting the limitations of current models when handling \nSoftware Development and Data Analysis Agents\nThey can generate code for common data science tasks, particularly boilerplate operations \nis particularly stark with synthetic data: LLMs effectively create qualitative text samples but \nLanguage Models for Health-Related Text Classification Tasks with Public Social Media Data, 2024) \ndata for supervised models.”\nTraditional statistical and ML techniques for rigorous analysis of structured data and \nrepetitive coding tasks, allowing data scientists to maintain the flow and focus on higher-level \nWhen training ML models, LLMs can now generate synthetic training data, assist in feature engi-\nAs you know by now, LangChain agents can write and execute Python code for data science tasks, \nneed to perform complex data analysis, create visualizations, or implement custom algorithms \nIn this section, we’ll explore how to create and use Python-capable agents through two main \nsteps: setting up the Python agent environment and configuring the agent with the right model \nand tools; and implementing a neural network from scratch, guiding the agent to create a com-\nSetting up a Python-capable agent\nLet’s start by creating a Python-capable agent using LangChain’s experimental tools:\nfrom langchain_experimental.agents.agent_toolkits.python.base import cre-\nagent_executor = create_python_agent(\nSoftware Development and Data Analysis Agents\nThis code creates a Python agent with the Claude 3 Opus model, which offers strong reasoning \nNow that we’ve set up our Python agent, let’s test its capabilities with a practical ML task.\nchallenge the agent to implement a simple neural network that learns a basic linear relationship.\nThis example demonstrates how agents can handle end-to-end ML development tasks from data \nThe following code instructs our agent to create a single-neuron neural network in PyTorch, train \nit on synthetic data representing the function y=2x, and make a prediction:\nTake synthetic data for y=2x.\nThis concise prompt instructs the agent to implement a full neural network pipeline: generating \nPyTorch code for a single-neuron model, creating synthetic training data that follows y=2x, train-\nAgent execution and results\nWhen we run this code, the agent begins reasoning through the problem and executing Python \ndata for y=2x, prints the loss every 100 epochs, and returns the predic-\n# Create synthetic data\n[...] # Code for creating the model omitted for brevity\nSoftware Development and Data Analysis Agents\n- I generated synthetic data where y=2x for training\nto fit the synthetic y=2x data well and make an accurate prediction for a \nFinal Answer: The trained single neuron PyTorch model predicts a value of \nThe results demonstrate that our agent successfully built and trained a neural network.\nThis example showcases how LangChain agents can successfully implement ML workflows with \nThe agent demonstrated strong capabilities in understanding the \nrequested task, generating correct PyTorch code without reference examples, creating appropri-\nate synthetic data, configuring and training the neural network, and evaluating results against \nexplore how agents can assist with data analysis and visualization tasks that build upon these \nNext, we’ll demonstrate how LangChain agents can analyze structured datasets by examining the \nCreating a pandas DataFrame agent\nData analysis is a perfect application for LLM agents.\nFirst, we’ll load the classic Iris dataset and save it as a CSV file for our agent to work with:\nNow we’ll create a specialized agent for working with pandas DataFrames:\nSoftware Development and Data Analysis Agents\nThe example above works well with small datasets like Iris (150 rows), but real-world data analysis \ndata to your agent, consider extracting key statistical information such as shape, column names, \nYou can process the data in manageable segments, run your agent on each chunk \nWe’ve used allow_dangerous_code=True, which permits the agent to execute ",
      "keywords": [
        "data",
        "Data Analysis Agents",
        "agent",
        "data analysis",
        "code",
        "software development",
        "model",
        "synthetic data",
        "Analysis Agents",
        "data science",
        "data science tasks",
        "analysis",
        "Python agent",
        "tasks",
        "LLMs"
      ],
      "concepts": [
        "data",
        "agents",
        "model",
        "task",
        "code",
        "coding",
        "train",
        "llms",
        "statistical",
        "statistics"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.68,
          "base_score": 0.68,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.672,
          "base_score": 0.672,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.661,
          "base_score": 0.661,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 35,
          "title": "",
          "score": 0.618,
          "base_score": 0.618,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "analysis",
          "agent",
          "synthetic",
          "data analysis"
        ],
        "semantic": [],
        "merged": [
          "data",
          "analysis",
          "agent",
          "synthetic",
          "data analysis"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3817583265286339,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.304079+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 328-335)",
      "start_page": 328,
      "end_page": 335,
      "summary": "Now that we’ve set up our data analysis agent, let’s explore its capabilities by asking progressively \nA well-designed agent should be able to handle different \nfollowing examples demonstrate how our agent can work with the classic Iris dataset, which \nWe’ll test our agent with three types of queries that represent common data analysis workflows: \nagent.run(prompt.format(query=\"What's this dataset about?\"))\nThe agent executes this request by examining the dataset structure:\nSoftware Development and Data Analysis Agents\nThis initial query demonstrates how the agent can perform basic data exploration by checking \nchallenge our agent with a more analytical question that requires computation:\nagent.run(prompt.format(query=\"Which row has the biggest difference be-\nThe agent tackles this by creating a new calculated column and finding its maximum value:\n'Row 122 has the biggest difference between petal length and petal width.'\nThis example shows how our agent can perform more complex analysis by:\nFinally, let’s see how our agent handles a request for data visualization:\nFor this visualization query, the agent generates code to create appropriate plots for each mea-\nThis demonstrates how our agent can generate code for creating informative data visu-\nThese three examples showcase the versatility of our data analysis agent in handling different \nexploration to statistical analysis and visualization—we can see how the agent uses its tools \nWhen designing your own data analysis agents, consider providing them with a \nSoftware Development and Data Analysis Agents\nFigure 7.2: Our LLM agent visualizing the well-known Iris dataset\nIn the repository, you can see a UI that wraps a data science agent.\nData science agents represent a powerful application of LangChain’s capabilities.\nThese agents can:\nGenerate and execute Python code for data analysis and machine learning\nAnswer complex questions about datasets through analysis and visualization\nThis chapter has examined how LLMs are reshaping software development and data analysis \nwith documentation and repository knowledge, and created agents capable of training neural \nChapter 8, we’ll delve into evaluation and testing methodologies that help validate AI-generated \n2.\t What key differences exist between traditional low-code platforms and LLM-based de-\nDescribe the validation framework presented in the chapter for LLM-generated code.\n8.\t How does the agentic approach to data science, as demonstrated in the neural network \nAs we’ve discussed so far in this book, LLM agents and systems have diverse applications across \nEvaluating LLM agents and apps in LangChain comes with new methods and metrics that can \ntricacies of evaluating LLM agents, covering system-level evaluation, evaluation-driven design, \nBy the end of this chapter, you will have a comprehensive understanding of how to evaluate LLM \nWhat we evaluate: core agent capabilities\nEvaluating LLM agents in practice\nIn the realm of developing LLM agents, evaluations play a pivotal role in ensuring these complex \nLLM agents represent a new class of AI systems that combine language models with reasoning, \niors, these agents operate with greater autonomy and complexity, making thorough evaluation \nLLM agents make complex, context-dependent decisions.\nLLM model evaluation:",
      "keywords": [
        "LLM agents",
        "Data Analysis Agents",
        "data analysis",
        "data",
        "agent",
        "Evaluating LLM agents",
        "LLM",
        "analysis",
        "petal",
        "petal length",
        "dataset",
        "petal width",
        "Analysis Agents",
        "evaluation",
        "code"
      ],
      "concepts": [
        "agent",
        "evaluation",
        "evaluating",
        "evaluate",
        "evaluations",
        "code",
        "coding",
        "difference",
        "differ",
        "llm"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.604,
          "base_score": 0.454,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 40,
          "title": "",
          "score": 0.442,
          "base_score": 0.292,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.414,
          "base_score": 0.414,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 35,
          "title": "",
          "score": 0.41,
          "base_score": 0.41,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "analysis",
          "agent",
          "data",
          "data analysis",
          "agents"
        ],
        "semantic": [],
        "merged": [
          "analysis",
          "agent",
          "data",
          "data analysis",
          "agents"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27666517639783605,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304146+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 336-343)",
      "start_page": 336,
      "end_page": 343,
      "summary": "Evaluation must verify that \nagents align with human expectations across multiple dimensions: factual accuracy in sensitive \nAlignment evaluation methods must be tailored to domain-specific concerns.\nalignment evaluation focuses on regulatory compliance with frameworks like GDPR and the EU \nFinancial institutions must evalu-\nLLM system/application evaluation:\nEvaluates how components work together (retrieval, tools, memory, etc.)\nWhile both types of evaluation are important, this chapter focuses on system-level \nevaluation, as practitioners building LLM agents with LangChain are concerned with \nEvaluation and Testing\nManufacturing contexts require alignment evaluation focused on safety parameters and opera-\nAlignment evaluation includes testing whether predictive maintenance systems appropriately \nIn educational settings, alignment evaluation must consider developmental appropriateness \nEducational AI systems require evaluation of their ability to provide \nalignment evaluations are essential for ensuring AI systems not only perform well technically \nInsufficient diversity in evaluation datasets: Failing to test performance across the \nDrawing lessons from software testing and other domains, comprehensive evaluation frame-\nPerformance evaluation determines whether agents can reliably achieve their intended goals, in-\nRigorous evaluations identify potential failure modes and risks in diverse real-world scenarios, as \ncost and task performance, highlighting how evaluations can guide resource-effective solutions.\nEvaluations help quantify the actual impact of LLM agents in practical settings.\ncould achieve meaningful practical outcomes, evaluated through metrics like user adherence \nreview work annually, with evaluations focusing on accuracy rates and cost savings compared to \nilarly, assessing user interaction with LLM agents helps refine interfaces and ensures the agents \nComprehensive agent evaluation requires addressing the distinct perspectives and priorities of \nEvaluation and Testing\nEnd users evaluate agents primarily through the lens of practical task completion and interaction \nTechnical stakeholders require a deeper evaluation of the agent’s internal processes rather than \nTheir evaluation extends to the \nBusiness stakeholders evaluate agents through metrics connecting directly to organizational \nTheir evaluation framework \nservices, evaluate agents through strict compliance and safety lenses.\npasses the agent’s adherence to domain-specific regulations (like HIPAA in healthcare or financial \nevaluation dimension becomes increasingly crucial to ensure ethical operation and minimize \nFor organizational decision-makers, evaluations should include cost-benefit analyses, especially \nSimilarly, evaluating the financial \nBuilding consensus for LLM evaluation\nEvaluating LLM agents presents a significant challenge due to their open-ended nature and the \nThe foundation of effective evaluation lies in prioritizing user outcomes.\napproach ensures evaluation priorities align with real-world impact.\ndomain specialists, and user representatives to define and document formalized evaluation cri-\nMaintaining version control for evaluation standards ensures \nEvaluation frame-\nEvaluation and Testing\nIn conclusion, rigorous and well-governed evaluation is an integral part of the LLM agent devel-\nBuilding on the foundational principles of LLM agent evaluation and the importance of establish-\ndetailed guide on the what and how of evaluating LLM agents, breaking down the core capabilities \nevaluation framework for your applications.\nWhat we evaluate: core agent capabilities\nAt the most fundamental level, an LLM agent’s value is tied directly to its ability to successfully \nTherefore, this task performance evaluation forms the cornerstone of agent assessment.\nTask performance evaluation\nTask performance forms the foundation of agent evaluation, measuring how effectively an agent \nevaluating task performance, organizations typically assess both the correctness of the final \nstandardized multi-stage evaluations of LLM-powered agents.\nFinancial services applications demonstrate task performance evaluation in practice, though we \nlacks sufficient information—a critical safety feature that requires specific evaluation protocols \nTool usage evaluation\nhas emerged as a crucial evaluation dimension that distinguishes advanced agents from simple \nEffective tool usage evaluation encompasses multiple aspects: \nIn production systems, evaluation in-\nRAG evaluation\nRAG system evaluation represents a specialized but crucial area of agent assessment, focusing on \nthe foundation of comprehensive RAG evaluation: retrieval quality, contextual relevance, faithful \nEvaluation and Testing\nThis involves evaluating whether the system \nFinally, information synthesis quality evaluates the agent’s ability to integrate information from \nEvaluation here extends \nEvaluating ",
      "keywords": [
        "Evaluation",
        "LLM agents",
        "agent",
        "LLM",
        "LLM agent evaluation",
        "performance",
        "Alignment evaluation",
        "Evaluating LLM agents",
        "systems",
        "alignment",
        "user",
        "metrics",
        "task performance evaluation",
        "human",
        "LLM evaluation"
      ],
      "concepts": [
        "evaluation",
        "evaluates",
        "evaluations",
        "evaluated",
        "agent",
        "user",
        "tools",
        "systems",
        "metrics",
        "specific"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.55,
          "base_score": 0.55,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.487,
          "base_score": 0.487,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "alignment",
          "task performance",
          "alignment evaluation",
          "agents"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "alignment",
          "task performance",
          "alignment evaluation",
          "agents"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3298754733881472,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304200+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 344-352)",
      "start_page": 344,
      "end_page": 352,
      "summary": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning \nThe Recipe2Plan benchmark specifically evaluates this \nReasoning coherence evaluates the logical structure of the agent’s problem-solving approach—\nsoftware testing where only the final output matters, agent evaluation increasingly examines \nBuilding on the foundational principles of LLM agent evaluation and the importance of establish-\nEvaluation and Testing\nIdentifying the core capabilities to evaluate is the first critical step.\nHow we evaluate: methodologies and approaches\nAutomated evaluation approaches\nAutomated evaluation methods provide scalable, consistent assessment of agent capabilities, \nallows for comprehensive automated evaluation that complements human assessment.\nReference-based evaluation compares each agent output against one or more gold-standard \non-based assessment frameworks that evaluate outputs against specific requirements.\nLLM-as-a-judge approaches represent a rapidly evolving evaluation methodology where powerful \nlanguage models serve as automated evaluators, assessing outputs according to defined rubrics.\nThis approach can help evaluate subjective qualities that traditional metrics struggle \nHuman evaluation remains essential for assessing subjective dimensions of agent performance \nEffective human-in-the-loop evaluation requires \nsubtle errors, evaluate reasoning quality, and assess alignment with domain-specific best prac-\nfor expert evaluation, particularly for assessing agent responses in complex regulatory contexts.\nratings are analyzed to identify patterns in agent performance across different query types, user \nThis experimental approach is particularly valuable for evaluating changes to agent \nEvaluation and Testing\nSystem-level evaluation\nSystem-level evaluation is crucial for complex LLM agents, particularly RAG systems, because test-\nCore approaches to system-level evaluation include using diagnostic frameworks that trace in-\nEffective evaluation of LLM applications requires running multiple assessments.\nsubject matter experts when evaluating responses.\nspecific solutions, and validate the effectiveness of changes through re-evaluation.\nMaintain living documentation: Keep centralized records of evaluation results, improvement \nIt’s time now to put the theory to the test and get into the weeds of evaluating LLM agents.\nEvaluating LLM agents in practice\nLangChain provides several predefined evaluators for different evaluation criteria.\nWe can also compare results from an LLM or agent against reference results using different meth-\nevaluation results can be used to determine the preferred LLM or agent based on the comparison \nthe evaluation results.\nEvaluation and Testing\nEvaluating the correctness of results\nthe output against a reference answer using both an exact match and a string distance evaluator.\n# Initialize an Exact Match evaluator that ignores case differences.\n# Evaluate the correct prediction.\nexact_result_correct = exact_evaluator.evaluate_strings(\n# Evaluate an incorrect prediction.\nexact_result_incorrect = exact_evaluator.evaluate_strings(\nA more generalizable approach is LLM‐as‐a‐judge for evaluating correctness.\nstead of using simple string extraction or an exact match, we call an evaluation LLM (for example, \nfrom langchain.evaluation.scoring import ScoreStringEvalChain\n# Initialize the evaluator LLM\n# Evaluate the prediction using the scoring chain\nresult_finance = chain.evaluate_strings(\nprint(\"Finance Evaluation Result:\")\nThe output demonstrates how the LLM evaluator assesses the response quality with nuanced \nFinance Evaluation Result:\nThis evaluation highlights an important advantage of the LLM-as-a-judge approach: it can iden-\nIn this case, the evaluator correctly identified \nmore nuanced assessment than binary correct/incorrect evaluations, giving developers action-\nEvaluation and Testing\nThe next example shows how to use Mistral AI to evaluate a model’s prediction against a refer-\nfrom langchain.evaluation.scoring import LabeledScoreStringEvalChain\n# Initialize the evaluator LLM with deterministic output (temperature 0.)\n# Create the evaluation chain that can use reference answers\n# Evaluate the prediction against the reference\nlabeled_result = labeled_chain.evaluate_strings(\nprint(\"Finance Evaluation Result (with reference):\")\nThe output shows how providing a reference answer significantly changes the evaluation results:\nWithout a reference, the evaluator focused on the lack of citation and timestamp.\nence confirming the factual accuracy, the evaluator now focuses on assessing completeness and \nBoth of these approaches leverage Mistral’s LLM as an evaluator, which can provide more nuanced \nfrom these evaluations should be consistent when using temperature=0, though outputs may \nWe start by importing the evaluator loader and a chat LLM for evaluation (for example GPT-4o):\nevaluation_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)",
      "keywords": [
        "LLM",
        "evaluation",
        "Federal Reserve",
        "Federal Reserve interest",
        "LLM agents",
        "agent",
        "current Federal Reserve",
        "Finance Evaluation Result",
        "LLM agent evaluation",
        "evaluating LLM agents",
        "Reserve interest rate",
        "reference",
        "prediction",
        "result",
        "interest rate"
      ],
      "concepts": [
        "evaluating",
        "evaluation",
        "evaluate",
        "evaluator",
        "evaluations",
        "agents",
        "approach",
        "approaches",
        "assesses",
        "assessment"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 40,
          "title": "",
          "score": 0.827,
          "base_score": 0.677,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.652,
          "base_score": 0.652,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 35,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 39,
          "title": "",
          "score": 0.604,
          "base_score": 0.454,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 29,
          "title": "",
          "score": 0.603,
          "base_score": 0.603,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "evaluator",
          "reference",
          "llm",
          "evaluate"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "evaluator",
          "reference",
          "llm",
          "evaluate"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37736445691927684,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304259+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 353-360)",
      "start_page": 353,
      "end_page": 360,
      "summary": "Evaluation and Testing\nconciseness_result = conciseness_evaluator.evaluate_strings(\nprint(\"Conciseness evaluation result:\", conciseness_result)\nConciseness evaluation result: {'reasoning': \"The criterion is \nfriendliness_result = friendliness_evaluator.evaluate_strings(\nprint(\"Friendliness evaluation result:\", friendliness_result)\nThe evaluator should return whether the tone is friendly (Y/N) along with reasoning.\nFriendliness evaluation result: {'reasoning': \"The criterion is to assess \nThis evaluation approach is particularly valuable for applications in healthcare, customer service, \nThe explicit reasoning provided by the evaluator helps development teams understand \nsystems, consider combining multiple criteria evaluators to create a comprehensive quality score \nEvaluating the output format\n# Initialize the JSON validity evaluator.\nEvaluation and Testing\n# Evaluate the valid JSON.\nvalid_result = json_validator.evaluate_strings(prediction=valid_json_\n# Evaluate the invalid JSON.\ninvalid_result = json_validator.evaluate_strings(prediction=invalid_json_\nJSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting \nEvaluating agent trajectory\nComplex agents require evaluation across three critical dimensions:\nFinal response evaluation: Assess the ultimate output provided to the user (factual ac-\nTrajectory evaluation: Examine the path the agent took to reach its conclusion\nWhile final response evaluation focuses on outcomes, trajectory evaluation examines the process \nTrajectory evaluation compares the actual sequence of steps an agent took against an expected \nLet’s implement a custom trajectory evaluator for a healthcare agent that responds to medi-\n# Custom trajectory subsequence evaluator\n\"Healthcare Agent Trajectory Evaluation\",\ndescription=\"Evaluates agent trajectory for medication queries\"\nEvaluation and Testing\n\"trajectory\": [\nTo evaluate the agent’s trajectory, we need to capture the actual sequence of steps taken.\ntrajectory = []\ntrajectory = [\"intent_classifier\", \"healthcare_agent\", \nevaluators=[trajectory_subsequence],\nexperiment_prefix=\"healthcare-agent-trajectory\",\nThe following screenshot visually demonstrates what trajectory evaluation results look like in \nFigure 8.1: Trajectory evaluation in LangSmith\nTrajectory evaluation provides unique insights beyond simple pass/fail assessments:\nReasoning quality: Evaluate the agent’s decision-making process independent of final \nTrajectory evaluation \nConsider using trajectory evaluation in conjunction with other evaluation types for a holistic as-\nEvaluating CoT reasoning\nNow suppose we want to evaluate the agent’s reasoning.\nexample, the agent must not only answer “What is the current interest rate?” but also provide \n# Simulated chain-of-thought reasoning provided by the agent:\nagent_reasoning = (\nresult_reasoning = cot_evaluator.evaluate_strings(\nprediction=agent_reasoning,\nprint(\"\\nChain-of-Thought Reasoning Evaluation:\")\nThe returned score and reasoning allow us to judge whether the agent’s thought process is sound \nChain-of-Thought Reasoning Evaluation:\nPlease note that in this evaluation, the agent provides detailed reasoning along with its answer.\nThe evaluator (using chain-of-thought evaluation) compares the agent’s reasoning with an ex-",
      "keywords": [
        "trajectory",
        "Evaluation",
        "Trajectory evaluation",
        "JSON",
        "agent",
        "reasoning",
        "JSON validity result",
        "result",
        "evaluator",
        "Conciseness evaluation result",
        "evaluation result",
        "Friendliness evaluation result",
        "JSON validity",
        "response",
        "conciseness"
      ],
      "concepts": [
        "evaluation",
        "evaluator",
        "evaluating",
        "trajectories",
        "reasoning",
        "output",
        "agents",
        "response",
        "score",
        "correct"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 41,
          "title": "",
          "score": 0.436,
          "base_score": 0.436,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 44,
          "title": "",
          "score": 0.436,
          "base_score": 0.436,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 31,
          "title": "",
          "score": 0.42,
          "base_score": 0.42,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.41,
          "base_score": 0.41,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 21,
          "title": "",
          "score": 0.402,
          "base_score": 0.402,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "trajectory",
          "trajectory evaluation",
          "reasoning",
          "evaluation result"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "trajectory",
          "trajectory evaluation",
          "reasoning",
          "evaluation result"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23959628358080629,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.304304+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 361-368)",
      "start_page": 361,
      "end_page": 368,
      "summary": "Evaluation and Testing\nOffline evaluation\nOffline evaluations provide key metrics, error anal-\nIn the next section, we’ll discuss creating an effective evaluation dataset within the context of \nRAG system evaluation.\nEvaluating RAG systems\nThe dimensions of RAG evaluation discussed earlier (retrieval quality, contextual relevance, faith-\nmore effective evaluation strategies.\nContext window limitations create another failure mode when key information is spread \nTo effectively evaluate and address these specific failure modes, we need a structured and com-\ndesigned evaluation dataset in LangSmith that allows for testing each of these failure patterns in \nEvaluation and Testing\nThis dataset structure serves multiple evaluation purposes.\nthat should be retrieved, allowing evaluation of retrieval accuracy.\nWhen implementing this dataset in practice, organizations typically load these examples into \nevaluation platforms like LangSmith, allowing automated testing of their RAG systems.\nHowever, implementing effective RAG evaluation goes beyond simply creating datasets; it requires \nThe ultimate goal of RAG evaluation is to drive continuous improvement.\nchanges, and then re-evaluating to measure the improvement.\nbenchmark and evaluate our system’s performance on a dataset.\nEvaluating a benchmark in LangSmith\nAs we’ve mentioned, comprehensive benchmarking and evaluation, including testing, are critical \ntesting, debugging, monitoring, and improving LLM applications, offers tools for evaluation and \nWe can run evaluations against benchmark datasets in LangSmith, as we’ll see now.\nos.environ[\"LANGCHAIN_PROJECT\"] = \"LLM Evaluation Example\"\n# Create a simple LLM call that will be traced in LangSmith\nEvaluation and Testing\nWe can create a dataset from existing agent runs with the create_example_from_run() func-\nHere’s how to create a dataset with a set of questions:\n# Create dataset in LangSmith\ndataset_name = \"Financial Advisory RAG Evaluation\"\ndescription=\"Evaluation dataset for financial advisory RAG systems \n# Add examples to the dataset\nprint(f\"Created evaluation dataset with {len(financial_examples)} \nThis code creates a new evaluation dataset in LangSmith containing financial advisory questions.\nstandard against which we can evaluate our LLM application responses.\nWe can make changes to our chain and evaluate changes in the application.\nThen we evaluate the results by comparing \nTo run an evaluation on a dataset, we can either specify an LLM or—for parallelism—use a con-\nNow, to evaluate the perfor-\nmance against our dataset, we need to define an evaluator as we saw in the previous section:\n# Define evaluation criteria specific to RAG systems\nevaluators=[\nEvaluation and Testing\nThis shows how to configure multi-dimensional evaluation for RAG systems, assessing factual \nWe’ll now pass a dataset together with the evaluation configuration with evaluators to run_on_\nfrom langchain.smith import run_on_dataset\nIn the same way, we could pass a dataset and evaluators to run_on_dataset() to generate metrics \ncreating a comprehensive evaluation dataset and assessing your RAG system across multiple \nlogs (appropriately anonymized) to ensure your evaluation dataset reflects actual usage patterns.\nLet’s use the datasets and evaluate libraries by HuggingFace to check a coding LLM approach to \nEvaluating a benchmark with HF datasets and Evaluate\nAs a reminder: the pass@k metric is a way to evaluate the performance of an LLM in solving \nHugging Face’s Evaluate library makes it very easy to calculate pass@k and other metrics.\nfrom evaluate import load\nThis shows how to evaluate code generation models using HuggingFace’s code_eval metric, ",
      "keywords": [
        "RAG systems",
        "dataset",
        "Evaluation",
        "RAG",
        "LLM",
        "RAG evaluation",
        "evaluation dataset",
        "LangSmith",
        "RAG system evaluation",
        "system",
        "documents",
        "information",
        "evaluate",
        "context",
        "LangChain"
      ],
      "concepts": [
        "evaluation",
        "evaluating",
        "evaluate",
        "evaluator",
        "information",
        "llm",
        "dataset",
        "documents",
        "document",
        "failure"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 44,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 22,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 16,
          "title": "",
          "score": 0.554,
          "base_score": 0.554,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 40,
          "title": "",
          "score": 0.55,
          "base_score": 0.55,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "dataset",
          "evaluation dataset",
          "evaluate",
          "rag"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "dataset",
          "evaluation dataset",
          "evaluate",
          "rag"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3062946613925587,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304354+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 369-376)",
      "start_page": 369,
      "end_page": 376,
      "summary": "Evaluation and Testing\nEvaluating email extraction\nLet’s show how we can use it to evaluate an LLM’s ability to extract structured information from \ninsurance claim texts.\nWe’ll first create a synthetic dataset using LangSmith.\nconsists of a raw insurance claim text (input) and its corresponding expected structured output \nWe will use this dataset to run extraction chains and evaluate your model’s performance.\n# Define a list of synthetic insurance claim examples\nSmith, Claim ID INS78910, \"\n\"claim_amount\": \"$3500\",\n\"claim_amount\": \"$1500\",\ndataset_name = \"Insurance Claims\"\ndescription=\"Synthetic dataset for insurance claim extraction tasks\",\nfor input_text, expected_output in example_inputs:\nWe’ll first define a schema for our claims:\nclaimant_name: str = Field(..., description=\"The name of the \nclaim_id: str = Field(..., description=\"The unique insurance claim \nclaim_amount: str = Field(..., description=\"The claimed amount (e.g., \nEvaluation and Testing\nclaim\")\n\"Extract the following structured information from the insurance claim \n\"claimant_name, claim_id, policy_number, claim_amount, accident_date, \"\nextraction_chain = instructions | llm | output_parser | (lambda x: \nFinally, we can run the extraction chain on our sample insurance claim:\nsample_claim_text = (\n\"Claim ID INS78910, Policy Number POL12345, and the damage is \nresult = extraction_chain.invoke({\"input\": sample_claim_text})\nThis showed how to evaluate structured information extraction from insurance claims text, using \na Pydantic schema to standardize extraction and LangSmith to assess performance.\nIn this chapter, we outlined critical strategies for evaluating LLM applications, ensuring robust \nevaluation using exact matches and LLM-as-a-judge approaches.\nsented code examples using LangSmith to create and evaluate datasets, demonstrating how to \nwalked through an example of evaluating insurance claim text extraction using structured sche-\nNow that we’ve evaluated our AI workflows, in the next chapter we’ll look at how we can deploy \nWhat are system-level and application-level evaluations and how do they differ?\nHow can LangSmith be used to compare different versions of an LLM application?\nHow does chain-of-thought evaluation differ from traditional output evaluation?\nWhy is trajectory evaluation important for understanding agent behavior?\nWhat are the key considerations when evaluating LLM agents for production deployment?\n8.\t How can bias be mitigated when using language models as evaluators?\nfor LLM agent evaluation?\nIn the previous chapter, we tested and evaluated our LLM app.\nthis chapter, we’ll first examine the pre-deployment requirements for LLM applications, including \ndeployed applications perform reliably in production.\nDeploying LLM apps\nProduction-Ready LLM Deployment and Observability\nLet’s begin by examining security considerations and strategies for protecting LLM applications \nSecurity considerations for LLM applications\nRefer to OWASP Top 10 for LLM Applications for a comprehensive \ninput and output validation, and monitoring semantic anomalies rather than relying on simple ",
      "keywords": [
        "claim",
        "LLM",
        "insurance claim",
        "insurance claim text",
        "LLM applications",
        "extraction",
        "dataset",
        "Evaluation",
        "accident",
        "Policy Number",
        "synthetic insurance claim",
        "description",
        "insurance claim extraction",
        "claim text",
        "Field"
      ],
      "concepts": [
        "evaluation",
        "evaluating",
        "evaluate",
        "evaluations",
        "evaluators",
        "security",
        "extraction",
        "extracted",
        "llm",
        "claims"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 43,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 42,
          "title": "",
          "score": 0.436,
          "base_score": 0.436,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 21,
          "title": "",
          "score": 0.432,
          "base_score": 0.432,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.396,
          "base_score": 0.396,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 39,
          "title": "",
          "score": 0.361,
          "base_score": 0.361,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "claim",
          "insurance",
          "insurance claim",
          "extraction",
          "llm"
        ],
        "semantic": [],
        "merged": [
          "claim",
          "insurance",
          "insurance claim",
          "extraction",
          "llm"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24837979659010406,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304399+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 377-389)",
      "start_page": 377,
      "end_page": 389,
      "summary": "Production-Ready LLM Deployment and Observability\nWe can now explore the practical aspects of deploying LLM applications to production environ-\nDeploying LLM apps\ntively deploy LangChain and LangGraph applications into production.\nDeploying generative AI applications to production is about making sure everything runs smoothly, \nFirst is application deployment and APIs. This is where you set up API endpoints for your \nLangChain applications, making sure they can communicate efficiently with other sys-\noperational framework for LLM applications.\nrefer to the practices of deploying, monitoring, and maintaining LLM applications \nProduction-Ready LLM Deployment and Observability\ntionalizing LangChain applications.\nLangChain and LangGraph applications to production environments.\nWeb framework deployment with FastAPI\nOne of the most common approaches for deploying LangChain applications is to create API end-\nweb framework that works particularly well with LangChain applications.\ntures when working with LLM applications.\nTo deploy LangChain applications as web services, \nFastAPI offers several advantages that make it well suited for LLM-based applications.\nport cost savings for high-volume applications, others find API services more eco-\nWe’ll implement our web server using RESTful principles to handle interactions with the LLM \nBelow is a basic implementation using FastAPI and LangChain’s Anthropic integration:\nfrom langchain_core.messages import HumanMessage\nreturn {\"response\": \"No message provided\"}\n# Create a human message and get response from LLM\nresponse = llm.invoke(messages)\nProduction-Ready LLM Deployment and Observability\nWhen deploying LLM applications, users often expect real-time responses rather than waiting \nThe following code demonstrates how to implement streaming with WebSocket in a FastAPI \napplication using LangChain’s callback system and Anthropic’s Claude model:\napplication, as users can begin reading responses while the model continues generating the rest \nProduction-Ready LLM Deployment and Observability\nchoice for serving asynchronous Python web applications like our LLM-powered chatbot.\nWhile FastAPI provides an excellent foundation for deploying LangChain applications, more \nScalable deployment with Ray Serve\nthrough Ray Serve, which makes it suitable for our search engine implementation.\ntical application, we’ll leverage Ray alongside LangChain to build a search engine specifically for \nRay’s own documentation.\ndeployment scenarios for large-scale ML infrastructure, but demonstrates how the framework \nbuild_index.py: Creates and saves the FAISS index (run once)\nserve_index.py: Loads the index and serves the search API (runs continuously)\ndex-building process from the serving application.\nBuilding the index\nimport ray\nfrom langchain_community.document_loaders import RecursiveUrlLoader\nRay is initialized to enable distributed processing, and we’re using the all-mpnet-base-v2 model \nprint(f\"Preprocessing batch of {len(docs)} documents\")\nchunks = text_splitter.split_documents(docs)\nprint(f\"Embedding batch of {len(chunks)} chunks\")\nreturn FAISS.from_documents(chunks, embeddings)\nProduction-Ready LLM Deployment and Observability\nembed_chunks converts text chunks into vector embeddings and builds FAISS indices.\nThe @ray.remote decorator makes these functions run in separate Ray workers.\nOur main index-building function looks like this:\ndef build_index(base_url=\"https://docs.ray.io/en/master/\", batch_size=50):\nprint(f\"Loading documentation from {base_url}\")\nprint(f\"Loaded {len(docs)} documents\")\nindex_futures = [embed_chunks.remote(batch) for batch in chunk_\nindices = ray.get(index_futures)\nindex = indices[0]\nprint(\"Saving index...\")\nprint(\"Index saved to 'faiss_index' directory\")\nreturn index\n# index = build_index(\"https://docs.ray.io/en/master/ray-core/\")\n# Test the index\nprint(\"\\nTesting the index:\")\nresults = index.similarity_search(\"How can Ray help with deploying \nServing the index\nLet’s deploy our pre-built FAISS index as a REST API using Ray Serve:\nProduction-Ready LLM Deployment and Observability\nprint(\"Loading pre-built index...\")\nindex\"):\nself.index = FAISS.load_local(\"faiss_index\", self.embeddings)\n# Search the index\nresults = self.index.similarity_search_with_score(query, k=5)\nreturn {\"results\": [], \"status\": \"error\", \"message\": f\"Search \ninitializes Ray, which provides the infrastructure for scaling our application.\nSearchDeployment class that loads our pre-built FAISS index and embedding model during initial-\nization, with robust error handling to provide clear feedback if the index is missing or corrupted.\nrun(deployment) print(\"Service started at: http://localhost:8000/\")\nThe main block binds and runs our deployment using Ray Serve, making it accessible through a \nFirst, build the index:\npython chapter9/ray/build_index.py\npython chapter9/ray/serve_index.py\nProduction-Ready LLM Deployment and Observability\nRay Serve makes it easy to deploy complex ML pipelines to production, allowing you to focus \ntions with Ray and LangChain, with a focus on robust error handling and separation of concerns.",
      "keywords": [
        "Production-Ready LLM Deployment",
        "LLM",
        "LLM Deployment",
        "Ray",
        "index",
        "Deployment",
        "LangChain",
        "LLM applications",
        "Ray Serve",
        "Production-Ready LLM",
        "API",
        "applications",
        "chunks",
        "deploying LLM applications",
        "FastAPI"
      ],
      "concepts": [
        "deployment",
        "ray",
        "models",
        "chunks",
        "llm",
        "imports",
        "messages",
        "applications",
        "application",
        "provide"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 49,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 6,
          "title": "",
          "score": 0.586,
          "base_score": 0.586,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ray",
          "index",
          "deployment",
          "llm",
          "production"
        ],
        "semantic": [],
        "merged": [
          "ray",
          "index",
          "deployment",
          "llm",
          "production"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3347945607156074,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304451+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 390-397)",
      "start_page": 390,
      "end_page": 397,
      "summary": "Deployment considerations for LangChain applications\nWhen deploying LangChain applications to production, following industry best practices ensures \napplications at scale.\nThe first step in deploying a LangChain application is containerizing it.\nerfile that installs dependencies, copies your application code, and specifies how to run your \nFastAPI application:\nThis Dockerfile creates a lightweight container that runs your LangChain application using Uvi-\nwith your application’s dependencies before copying in the application code.\nWith your application containerized, you can deploy it to various environments, including cloud \nproviders, Kubernetes clusters, or container-specific services like AWS ECS or Google Cloud Run.\nProduction-Ready LLM Deployment and Observability\nKubernetes provides orchestration capabilities that are particularly valuable for LLM applications, \nLet’s walk through a complete example of deploying a LangChain application to Kubernetes, \n# secrets.yaml - Store API keys securely\nThis YAML file creates a Kubernetes Secret that securely stores your OpenAI API key in an encrypted \nWhen applied to your cluster, this key can be securely mounted as an environment variable \nin your application without ever being visible in plaintext in your deployment configurations.\nNext, we define the actual deployment of your LangChain application, specifying resource re-\n# deployment.yaml - Main application configuration\nkind: Deployment\nProduction-Ready LLM Deployment and Observability\nThis deployment configuration defines how Kubernetes should run your application.\nto healthy instances of your application, improving reliability.\n# service.yaml - Expose the application\nname: langchain-app-service\nThis Service creates an internal network endpoint for your application, allowing other compo-\nconfigure external access to your application using an Ingress resource:\nname: langchain-app-ingress\nname: langchain-app-service\nThe Ingress resource exposes your application to external traffic, mapping a domain name to \nThis provides a way for users to access your LangChain application from outside \nWith all the configuration files ready, you can now deploy your application using the following \nkubectl apply -f deployment.yaml\nThese commands apply your configurations to the Kubernetes cluster and verify that everything \nYou’ll see the status of your Pods, Services, and Ingress resources, allowing \nyou gain several benefits that are essential for production-ready LLM applications.\nenhanced by storing API keys as Kubernetes Secrets rather than hardcoding them directly in your \napplication code.\nYour deployment \nFinally, the implementation provides accessibility through properly configured Ingress rules, \nProduction-Ready LLM Deployment and Observability\nLangChain applications rely on external LLM providers, so it’s important to implement com-\napplication:\nThis health check endpoint verifies that your application can successfully communicate with \nyour application is ready to receive traffic, automatically rerouting requests away from unhealthy \nFor production deployments:\nConsider resource allocation carefully as LLM applications can be CPU-intensive during \nThese considerations are particularly important for LangChain applications, which may experi-\nThe LangGraph platform is specifically designed for deploying applications built with the Lang-\nIt provides a managed service that simplifies deployment and offers monitoring \nLangGraph applications maintain state across interactions, support complex execution flows \nhow to deploy these specialized applications using tools specifically designed for LangGraph.\nLangGraph applications differ from simple LangChain chains in several important ways that \nit easier to deploy sophisticated multi-agent systems to production.\nBefore deploying to production, the LangGraph CLI provides a streamlined environment for local \nCreate a new application from a template:\nlanggraph new path/to/your/app --template react-agent-python\nProduction-Ready LLM Deployment and Observability\nTest your application using the SDK:\nTo deploy a LangGraph application to production, you need to configure your application properly.\nSet up the langgraph.json configuration file:",
      "keywords": [
        "application",
        "deployment",
        "Production-Ready LLM Deployment",
        "LangGraph",
        "LLM Deployment",
        "API keys",
        "Kubernetes",
        "LLM",
        "LLM applications",
        "API",
        "LangChain application",
        "Production-Ready LLM",
        "store API keys",
        "LangChain",
        "Health"
      ],
      "concepts": [
        "deployment",
        "deployments",
        "application",
        "secret",
        "resource",
        "configurations",
        "configuration",
        "service",
        "requirements",
        "require"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 45,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 49,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 7,
          "title": "",
          "score": 0.568,
          "base_score": 0.568,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "application",
          "deployment",
          "kubernetes",
          "production",
          "langchain application"
        ],
        "semantic": [],
        "merged": [
          "application",
          "deployment",
          "kubernetes",
          "production",
          "langchain application"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23668796317664817,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304499+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 398-406)",
      "start_page": 398,
      "end_page": 406,
      "summary": "from langgraph.graph import StateGraph, END, START\n# Add other dependencies your application needs\nThe LangGraph cloud provides a fast path to production with a fully managed service.\nProduction-Ready LLM Deployment and Observability\nTo streamline the deployment of your LangGraph apps, you can choose between automated CI/\nAdd a workflow that runs your test suite against the LangGraph code.\nIn LangSmith, open LangGraph Platform | New Deployment.\nOnce deployed, grab the auto-generated URL and monitor performance in LangGraph \ndata transforms throughout graph execution, providing insights into the application’s internal \nWhen you deploy to the LangGraph cloud, a LangSmith tracing project is automatically created, \nenabling comprehensive monitoring of your application’s performance in production.\nServerless platforms provide a way to deploy LangChain applications without managing the \nAWS Lambda: For lightweight LangChain applications, though with limitations on ex-\nGoogle Cloud Run: Supports containerized LangChain applications with automatic scaling\npricing model, which can be cost-effective for applications with variable traffic patterns.\nThese tools help build interfaces for your LangChain applications:\nChainlit: Specifically designed for deploying LangChain agents with interactive ChatGPT-\nGradio: An easy-to-use library for creating customizable UIs for ML models and LangChain \napplications, with simple deployment to Hugging Face Spaces.\nThese frameworks provide the user-facing layer that connects to your LangChain backend, making \nThe Model Context Protocol (MCP) is an emerging open standard designed to standardize how \nLLM applications interact with external tools, structured data, and predefined prompts.\nexternal data sources, APIs, and enterprise tools.\nThis is particularly relevant for LangChain deployments, which frequently involve interactions \nThe MCP client is embedded in the AI application (like your LangChain app).\nProduction-Ready LLM Deployment and Observability\nIn this section, we’ll work with the langchain-mcp-adapters library, which provides a lightweight \nwrapper to integrate MCP tools into LangChain and LangGraph environments.\nverts MCP tools into LangChain tools and provides a client implementation for connecting to \nmultiple MCP servers and loading tools dynamically.\nTo get started, you need to install the langchain-mcp-adapters library:\nfrom mcp.server.fastmcp import FastMCP\nOnce the MCP server is running, we can connect to it and use its tools within LangChain:\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nagent = create_react_agent(model, tools)\nThis code loads MCP tools into a LangChain-compatible format, creates an AI agent using Lang-\nDeploying LLM applications in production environments requires careful infrastructure planning \nregarding production-grade infrastructure for LLM applications.\nProduction LLM applications need scalable computing resources to handle inference workloads \nAPIs enable integration with client applications, while comprehensive monitoring systems track \nProduction LLM applications require careful consideration of deployment architecture to en-\nstrategic decision: leverage cloud API services, self-host on-premises, implement a cloud-based \nProduction-Ready LLM Deployment and Observability\nFor deployment, \nCI/CD pipelines automate testing and deployment regardless of your infrastructure management \nHow to choose your deployment model\nThere’s no one-size-fits-all when it comes to deploying LLM applications.\nIf you’re self-hosting, isolate your model servers from \nNever hardcode API keys in your application.\nCut costs wherever possible: Use the smallest model that does the job well.\nmany tokens each part of your application uses so you know where your \nThe upside is complete control over your models and data, \nCloud self-hosting for the middle ground: Running models on cloud GPU instances gives \nTry hybrid approaches for complex needs: Route sensitive data to your self-hosted models \nSending public data to cloud APIs and private data to your own servers\nUsing cloud APIs for general tasks and self-hosted models for specialized domains\nRunning base workloads on your hardware and bursting to cloud APIs during \nworks, you’ll need self-hosted open-source models.\nuse case, cloud APIs will save you significant time and resources.\nsuccessful products start with cloud APIs to test their idea, then move to self-hosting \nProduction-Ready LLM Deployment and Observability\nModel serving infrastructure\nModel serving infrastructure provides the foundation for deploying LLMs as production services.\nThese frameworks expose models via APIs, manage memory allocation, optimize inference per-\norganizations deploying their own model infrastructure, rather than using API-based LLMs. These \nframeworks expose models via APIs, manage memory allocation, optimize inference performance, \nfrom langchain_core.prompts import PromptTemplate\n# Configure multiple model deployments with fallbacks\n# Create LangChain LLM with router\n# Build and use a LangChain\nThis makes it invaluable for mission-critical LangChain applications that need to maintain \nFor more implementation examples of serving a self-hosted model or quantized \nmodel, refer to Chapter 2, where we covered the core development environment ",
      "keywords": [
        "cloud APIs",
        "LLM applications",
        "LLM",
        "model",
        "deployment",
        "Production-Ready LLM Deployment",
        "MCP",
        "cloud",
        "LangChain",
        "LLM Deployment",
        "APIs",
        "applications",
        "API",
        "tools",
        "LangChain applications"
      ],
      "concepts": [
        "model",
        "deployment",
        "deployments",
        "data",
        "langchain",
        "tools",
        "application",
        "applications",
        "needs",
        "infrastructure"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 45,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.666,
          "base_score": 0.516,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 49,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 6,
          "title": "",
          "score": 0.5,
          "base_score": 0.5,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "mcp",
          "cloud",
          "apis",
          "deployment",
          "cloud apis"
        ],
        "semantic": [],
        "merged": [
          "mcp",
          "cloud",
          "apis",
          "deployment",
          "cloud apis"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3216272772221878,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304564+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 407-414)",
      "start_page": 407,
      "end_page": 414,
      "summary": "Production-Ready LLM Deployment and Observability\nThe key to cost-effective LLM deployment is memory optimization.\nHow to observe LLM apps\nEffective observability for LLM applications requires a fundamental shift in monitoring approach \ncharacteristics of LLMs. Traditional systems monitor structured inputs and outputs against clear \nOperational metrics for LLM applications\nLLM applications require tracking specialized metrics that have no clear parallels in traditional \nLatency dimensions: Time to First Token (TTFT) measures how quickly the model begins \nToken economy metrics: Unlike traditional ML models, where input and output sizes are \nThe input/output token ratio helps evaluate prompt engineering efficiency by \ndow utilization tracks how effectively the application uses available context, revealing \nponent (chains, agents, and tools) helps identify which parts of complex LLM applications \nCost visibility: LLM applications introduce unique cost structures based on token usage \nModel cost efficiency evaluates whether the application \nTool usage analytics: For agentic LLM applications, monitoring tool selection accuracy and \nfunction calls, LLM agents dynamically decide which tools to use and when.\ntool usage patterns, error rates, and the appropriateness of tool selection provides unique \nvisibility into agent decision quality that has no parallel in traditional ML applications.\nProduction-Ready LLM Deployment and Observability\napplications that adapt to changing requirements while controlling costs and ensuring quality \npabilities for tracking these unique aspects of LLM applications in production environments.\nfoundational aspect of LLM observability is the comprehensive capture of all interactions, which \nand analyzing LLM responses, beginning with how to monitor the trajectory of an agent.\nreturn_intermediate_steps parameter to True when initializing an agent or an LLM.\nfrom langchain_core.tools import StructuredTool\nNow, we set up an agent that uses this tool with an LLM to make the calls given a prompt:\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, AgentType\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\nresult = agent(\"What's the latency like for https://langchain.com?\")\nThe latency for https://langchain.com is 13.773 ms\nlangchain.com', 'return_error': False}, log=\"\\nInvoking: `ping` with \n`{'url': 'https://langchain.com', 'return_error': False}`\\n\\n\\n\", message_\n\"ping\",\\n \"action\": {\\n \"url\": \"https://langchain.com\",\\n \"return_\nerror\": false\\n }\\n }\\n ]\\n}'}}, example=False)]), 'PING langchain.com \nttl=249 time=13.773 ms\\n\\n--- langchain.com ping statistics ---\\n1 packets \nFor RAG applications, it’s essential to track not just what the model outputs, but what information \nProduction-Ready LLM Deployment and Observability\nVisualization tools like LangSmith provide graphical interfaces for tracing complex agent inter-\nLLM is used to assess the factual correctness of a response.\nand response tracking for a moderately busy LLM application generates 10-50 GB \nFor all requests, track only the request ID, timestamp, token counts, latency, \nFor critical use cases (financial advice or healthcare), track complete data for \nTracking bias in model outputs is critical for maintaining fair and ethical systems.\nLangSmith, previously introduced in Chapter 8, provides essential tools for observability in Lang-\ndatasets, using AI-assisted evaluators for performance grading, and monitoring key metrics \nsuch as latency, token usage, and cost.\nProduction-Ready LLM Deployment and Observability\ncan be useful to optimize latency, hardware efficiency, and cost, as we can see on the monitoring \nTrace latency (s), LLM latency (s), LLM calls per trace, tokens / sec\nTotal tokens, tokens per trace, tokens per LLM call\n% traces w/ streaming, % LLM calls w/ streaming, trace time to first token (ms), \nLLM time to first token (ms)\na large set of features for evaluation and monitoring, and because it integrates with LangChain.\nalso important to align technical monitoring with business impact metrics, such as conversion ",
      "keywords": [
        "LLM",
        "LLM applications",
        "LLM Deployment",
        "Production-Ready LLM Deployment",
        "Token",
        "LLMs",
        "Observability",
        "Production-Ready LLM",
        "applications",
        "Tool",
        "agent",
        "LLM call",
        "LangSmith",
        "monitoring",
        "Latency"
      ],
      "concepts": [
        "llm",
        "token",
        "important",
        "monitor",
        "metrics",
        "agents",
        "tools",
        "application",
        "applications",
        "ping"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 49,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 45,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 25,
          "title": "",
          "score": 0.658,
          "base_score": 0.658,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "llm",
          "latency",
          "token",
          "observability",
          "llm deployment"
        ],
        "semantic": [],
        "merged": [
          "llm",
          "latency",
          "token",
          "observability",
          "llm deployment"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3377033891365961,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304623+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 415-422)",
      "start_page": 415,
      "end_page": 422,
      "summary": "Continuous improvement for LLM applications\nof issues and use A/B testing to compare different prompts, models, or parameters based on key \nCost management for LangChain applications\nAs LLM applications move from experimental prototypes to production systems serving real users, \ncially as usage scales, making effective cost optimization essential for sustainable deployments.\nThis section explores practical strategies for managing LLM costs in LangChain applications while \nit’s important to understand the factors that drive costs in LLM applications:\nrates for input tokens (what you send) and output tokens (what the model generates).\nFor example, with GPT-4o, input tokens cost $0.005 per 1K tokens, while output tokens \nModel selection strategies in LangChain\nWhen deploying LLM applications in production, managing cost without compromising quality \nTwo effective strategies for optimizing model usage are tiered model selection and \nThe first uses a lightweight model to classify the complexity of a \nOne of the most effective ways to manage costs is to intelligently select which model to use for \nLangChain makes it easy to implement systems that route queries to different models based on \nThe example below shows how to use a lightweight model to classify a query and \n# Define models with different capabilities and costs\nclassifier = classifier_prompt | affordable_model | StrOutputParser()\n\"\"\"Route the query to the appropriate model based on complexity.\"\"\"\nprint(f\"Using affordable model for: {query}\")\nprint(f\"Using powerful model for: {query}\")\nreturn powerful_model\nmodel = route_query(query)\nreturn model.invoke(query)\nAs mentioned, this logic uses a lightweight model to classify the query, reserving the more pow-\nerful (and costly) model for complex tasks only.\nCascading model approach\nIn this strategy, the system first attempts a response using a cheaper model and escalates to a \nllm=affordable_model)\n\"\"\"Try affordable model first, fallback to powerful model if quality \ninitial_response = affordable_model.invoke(query)\n# If quality score is too low, use the more powerful model\nprint(\"Response quality insufficient, using more powerful model\")\nreturn powerful_model.invoke(query)\nThis cascading fallback method helps minimize costs while ensuring high-quality responses \nSince output tokens typically cost more than input tokens, optimizing response length can yield \nYou can control response length through prompts and model parameters:\nThis approach ensures that responses never exceed a certain length, providing predictable costs.\nCaching is another powerful strategy for reducing costs, especially for applications that receive \nboth latency and operational costs depending on your application’s query patterns, making it an \nAs a final cost management strategy, effective context management can dramatically improve \nperformance and reduce the costs of LangChain applications in production environments.\nContext management directly impacts token usage, which translates to costs in production.\ncontext windowing is particularly important as it provides predictable cost control.\nllm = ChatOpenAI(model=\"gpt-4o\")\nThis allows us to monitor costs in real time and identify queries or patterns that contribute dis-\nanalytics on token usage, costs, and performance, helping you identify opportunities for opti-\ncost management strategy for LangChain applications.\nLLM application, we have to make sure it’s robust and secure, it scales, we can control costs, and \nHow can prompt injection attacks compromise LLM applications, and what strategies \nmodels, LLM apps, or apps that rely on generative models in general?\nWhat key metrics should be included in a comprehensive monitoring strategy for LLM \n8.\t How do tracking, tracing, and monitoring differ in the context of LLM observability, and \nWhat are the different patterns for cost management of LLM applications?\nWhat role does continuous improvement play in the lifecycle of deployed LLM applications, ",
      "keywords": [
        "LLM",
        "LLM applications",
        "model",
        "LLM Deployment",
        "tokens",
        "query",
        "Production-Ready LLM Deployment",
        "applications",
        "costs",
        "Deployment",
        "LangChain",
        "Observability",
        "Production-Ready LLM",
        "response",
        "production"
      ],
      "concepts": [
        "model",
        "cost",
        "llm",
        "query",
        "queries",
        "token",
        "caching",
        "cache",
        "prompts",
        "deployment"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 48,
          "title": "",
          "score": 0.825,
          "base_score": 0.675,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 45,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 46,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 47,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 51,
          "title": "",
          "score": 0.505,
          "base_score": 0.505,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "costs",
          "model",
          "tokens",
          "cost",
          "llm"
        ],
        "semantic": [],
        "merged": [
          "costs",
          "model",
          "tokens",
          "cost",
          "llm"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31291911912891784,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304673+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 423-430)",
      "start_page": 423,
      "end_page": 430,
      "summary": "Models: Beyond Scaling\nin terms of sustainability, accessibility, and addressing fundamental AI limitations.\ngenerative AI lies beyond simple scaling, in more efficient architectures, specialized approach-\nThroughout this book, we have explored building applications using generative AI models.\ncurrent limitations of generative models—not just technical ones, but the bigger social and eth-\nThe Future of Generative Models: Beyond Scaling\nWe’ll also consider what generative AI might mean for jobs, and how it could reshape entire \nThe current state of generative AI\nThe current state of generative AI\nAs discussed in this book, in recent years, generative AI models have attained new milestones in \nThese models represent a fundamental shift in AI capabilities, \nparticularly in domains requiring sophisticated reasoning.\nseen in previous generations, these models demonstrated extraordinary leaps in performance.\nWhat distinguishes newer models like o1 and o3 is their iterative processing approach that builds \nThese models implement what re-\nmodels to allocate additional computational resources to more challenging problems, though this \nWhile these models \noptimization for intermediate reasoning steps, though the core approach remains grounded in \nThe emergence of models marketed as having reasoning capabilities suggests a potential evolution \nin how these systems process information, though significant limitations persist.\nThese models \ndemonstrate improved performance on certain structured reasoning tasks and can follow more \nThis represents an incremental advancement in how businesses might leverage AI technolo-\ncases, with particular attention to edge cases and scenarios requiring true causal reasoning or \nModels with enhanced reasoning approaches show promise but come with important limitations \nStructured analysis approaches: Recent research suggests these models can follow multi-\nstep reasoning patterns for certain types of problems, though their application to stra-\nSemi-autonomous agent systems: Models incorporating reasoning techniques can exe-\nParticularly notable is the rising proficiency in code generation, where these reasoning models \npoints toward a future where AI systems could potentially create and execute code autonomously, \nThe potential business applications of models with reasoning approaches are significant, though \nThe Future of Generative Models: Beyond Scaling\nwidespread deployment of truly reasoning-based systems for complex business decision-making \nFor enterprises evaluating AI capabilities, reasoning models represent a significant step forward \nin making AI a reliable and capable tool for high-value business applications.\ntransforms generative AI from primarily a content creation technology to a strategic decision \nThese practical applications of reasoning capabilities help explain why the development of models \nthe implications of these reasoning capabilities vary significantly across industries, with some \nWhat distinguishes these reasoning models is not just their performance but how they achieve \nWhile previous models struggled with multi-step reasoning, these systems demonstrate an \nreasoning patterns from earlier models—resembling the deliberate problem-solving approaches \nof expert human reasoners rather than statistical pattern matching.\nThe most significant aspect of these models for our discussion of scaling is that their capabilities \nAdvanced reasoning architectures that support recursive thinking processes\nThey suggest that the future of AI advancement may depend more on how mod-\nThe following tracks the progress of AI systems across various capabilities relative to human \nthe vertical axis), while each AI capability’s initial performance is normalized to -100.\nreveals the varying trajectories and timelines for different AI capabilities reaching and exceeding \nThe comparison between human cognition and generative AI reveals several fundamental differ-\nrizing the key strengths and deficiencies of current generative AI compared to human cognition:\nGenerative AI\nThe Future of Generative Models: Beyond Scaling\nTable 10.1: Comparison between human cognition and generative AI\nWhile current AI systems have made extraordinary advances in producing high-quality content \ninformation, represents a fundamental gap between human and AI cognition.\nlimitations, specialized training approaches show promise.\ntext windows, AI systems struggle with coherently tracking object states and agent knowledge \nWhile AI systems have made impressive strides in multimodal integration (text, images, audio, \nSimilarly, in creative generation, AI remains bounded by its training distribution, producing \nspecialized subsystems enables remarkable energy efficiency (~20 watts) compared to AI’s largely \nAdditionally, AI systems \nThese differences suggest that while certain capabilities may improve through better training \nDespite impressive advances in generative AI, fundamental gaps remain between human and AI \nEuropean Union’s AI Act, implemented in 2024, has created stringent requirements that have \ndelayed or limited the availability of some AI tools in European markets.\nof AI beyond technical scaling, as companies must adapt their offerings to meet varying legal ",
      "keywords": [
        "Models",
        "Generative Models",
        "Generative",
        "reasoning",
        "Scaling",
        "capabilities",
        "systems",
        "human",
        "training",
        "limitations",
        "reasoning models",
        "Future of Generative",
        "performance",
        "fundamental",
        "human cognition"
      ],
      "concepts": [
        "reason",
        "models",
        "capabilities",
        "capability",
        "capable",
        "human",
        "businesses",
        "limitations",
        "limited",
        "training"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.824,
          "base_score": 0.674,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 51,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 1,
          "title": "",
          "score": 0.668,
          "base_score": 0.518,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 52,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.633,
          "base_score": 0.633,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "models",
          "reasoning",
          "generative",
          "generative ai"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "models",
          "reasoning",
          "generative",
          "generative ai"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33387151809349785,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304722+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 431-439)",
      "start_page": 431,
      "end_page": 439,
      "summary": "The Future of Generative Models: Beyond Scaling\nThe current doubling time in training compute of very large models is about 8 months, outpacing \nmaking it (as of this writing) likely the most compute-intensive model ever trained.\nlanguage model training datasets have grown by about 3.0x per year since 2010, creating massive \nidea that simply scaling up model size, data, and compute would inevitably lead to artificial gen-\nDespite enormous increases in model size and \nlead of companies like OpenAI has eroded, with 7-10 GPT-4 level models now available \nwhere specialized, domain-specific models developed by smaller teams can outperform general \nmodels from tech giants in specific applications, further eroding the advantages of scale alone.\nfraction of the size of Google or OpenAI, has developed specialized enterprise-focused models \ncommand performance with Claude models that often outperformed larger competitors in rea-\nmodels can achieve performance competitive with models many times their size.\nMultiple capable models have emerged.\n4, there are now 7-10 comparable models available in the market from companies like Anthropic, \nThe Future of Generative Models: Beyond Scaling\nteDance have achieved comparable model quality with dramatically lower training costs, demon-\ntheir own AI models.\nThis approach allows them to create models that are customized to the spe-\nAs costs decline for computing, data storage, and AI talent, custom fine-tuning of specialized \nmodels is already feasible for small and mid-sized companies.\nproviding industry-specific models, while smaller entities could increasingly fine-tune their own \nIf robust tools emerge to simplify and automate AI development, custom generative models may \nThe traditional approach to AI advancement has centered on scaling up—pursuing greater capa-\nbilities through larger models, more compute, and bigger datasets.\nIncreasing model size and complexity: The predominant approach since 2017 has been \nbillion parameters, while more recent models like GPT-4 and Gemini Ultra are estimated \nExpanding computational resources: Training these massive models requires enormous \nsingle training run for a frontier model can cost upwards of $100 million.\nGathering vast datasets: As models grow, so too does their hunger for training data.\nLeading models are trained on trillions of tokens, essentially consuming much of the \nLimitations becoming apparent: While this approach has dominated AI development \nThe Future of Generative Models: Beyond Scaling\nQuantization converts models to lower precision by reducing bit sizes of weights and \nThis technique can compress large model performance into smaller form \nModel distillation transfers knowledge from large “teacher” models to smaller, more \nefficient “student” models, enabling deployment on more limited hardware.\nDecember 2024 research on memory layers demonstrated how to improve model ca-\nmemory-augmented models matched the performance of dense models trained with 4x \nSpecialized models offer another alternative to general-purpose systems.\npursuing general intelligence through scale, focused models tailored to specific domains \nWhile models like GPT-4 were trained on vast, heterogeneous datasets, the \nPhi series achieved remarkable performance with much smaller models by focusing on \nThis distributed paradigm explores how to leverage networks of models and computational \nTest-time compute shifts focus from training larger models to allocating more computation \nThis allows models to reason through problems more thoroughly.\ntasks without requiring larger models, demonstrating the power of evolutionary search strat-\nThis paradigm shift enables models to mimic complex human reasoning \nsolutions, and revision models trained to solve problems iteratively, refining previous attempts.\nwith Self-Evolved Deep Thinking) demonstrated that a model can achieve reasoning capabilities \ndamentally different approach to improving AI capabilities than traditional scaling methods.\nissues more effectively than simply scaling up model size.\nmodels to access accurate, up-to-date information without having to encode it all in parameters.\ntrainable key-value lookup mechanism to add extra parameters to a model without increasing \nNeural Attention Memory Models (NAMMs) improve the performance and efficiency \nThe Future of Generative Models: Beyond Scaling\nVision-centric enhancements like OLA-VLM optimize multimodal models specifically for \nmodels trained primarily on web data.\nThese improvements have dramatically altered traditional scaling laws—a well-trained \n7-billion-parameter model with exceptional data quality can now outperform earlier 175-bil-\nlion-parameter models on complex reasoning tasks.\nThis data-centric approach represents a fundamental alternative to pure parameter scaling, sug-\ngesting that the future of AI may belong to more efficient, specialized models trained on precisely \nAn emerging challenge for data quality is the growing prevalence of AI-generated content across \nonline, future models trained on this data will increasingly be learning from other AI outputs \neventually lead to plateauing performance, as models begin to amplify patterns, limitations, and \nverified human-created content for training future models.\nThe rapidly decreasing costs of AI model training represent a significant shift in the landscape, \nquality, and the introduction of novel model architectures.\nHere are the key techniques and approaches that make generative AI more accessible and effective:\nModel distillation: Knowledge transfer from large models into smaller, more efficient \nof executing AI models on given hardware\nThe Future of Generative Models: Beyond Scaling\nmodels\ngeneration further democratize access to AI training by providing high-quality and data-efficient \nmodel development and removing some reliance on vast, proprietary datasets.\nFinancial barriers are reduced by compressing large model performance into far smaller \nThe landscape is shifting from a focus on sheer model size and brute-force compute to clever, nu-\nanced approaches that maximize computational efficiency and model efficacy.",
      "keywords": [
        "Models",
        "Scaling",
        "data",
        "training",
        "Generative Models",
        "approach",
        "performance",
        "Generative",
        "scaling laws",
        "model size",
        "specialized",
        "models trained",
        "Future",
        "datasets",
        "training data"
      ],
      "concepts": [
        "models",
        "data",
        "approach",
        "approaches",
        "scaling",
        "scale",
        "train",
        "likely",
        "computational",
        "costs"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.828,
          "base_score": 0.678,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 50,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 1,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 52,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 53,
          "title": "",
          "score": 0.545,
          "base_score": 0.545,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "models",
          "scaling",
          "trained",
          "training",
          "model"
        ],
        "semantic": [],
        "merged": [
          "models",
          "scaling",
          "trained",
          "training",
          "model"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33360123823806187,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304772+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 440-447)",
      "start_page": 440,
      "end_page": 447,
      "summary": "Integrating generative AI promises immense productivity gains through automating tasks across \nof Generative AI reports, AI could contribute up to $15.7 trillion to the global economy by 2030, \nThe evolution of AI adoption can be better understood within the context of previous technological \nAwareness: The Decade Ahead (2024) argues that AI implementation may follow a compressed \nbrenner’s analysis suggests that the traditional S-curve might be dramatically steepened for AI \nRecent analyses indicate that AI implementation will likely follow a more complex, phased tra-\nautomation\nPost-2050: Societal and ethical considerations may delay full automation of roles re-\nGlobal Institute’s research on automation potential across sectors, we can map the relative au-\nSpecific automation levels and projections reveal varying rates of adoption:\nAutomation Potential\nAI diagnostic assistance, \nrobotic surgery, automated \nTable 10.2: State of sector-specific automation levels and projections\nThis data supports a nuanced view of automation timelines across different sectors.\nufacturing and logistics are progressing rapidly toward high levels of automation, service sectors \nEarlier McKinsey estimates from 2023 suggested that LLMs could directly automate 20% of tasks \nThe competitive landscape for AI providers has evolved significantly in 2024-2025.\n50-80% of tasks using AI-powered robotics, achieving ROI within 2-3 years.\ncollaborative-robots-pioneer-automation-revolution-market-to-reach-us7.2-billion-\nand the industry average (currently at 45-50% automation) illustrates both the potential and \nGitHub Copilot are changing how developers work, though specific percentages of task automa-\nbe fully automated by demonstrated technologies, while many more have significant portions \nAs automation adoption progresses across industries, the impact on jobs will vary significantly \nAs automation adoption progresses across industries, the impact on jobs will vary significantly \nthan wholesale replacement—will be the predominant pattern as AI capabilities advance.\nThe automation potential varies substantially across sectors.\ntheir structured environments and repetitive tasks, show higher potential for automation than \nsectors requiring complex human interaction like healthcare and education.\nAs service sectors reach 40-60% automation levels over the next decade, we can expect significant \nEducation: Teachers will utilize AI for course preparation, administrative tasks, and per-\nStudents are already using generative AI to learn new concepts \nsupport, documentation, and routine monitoring will be increasingly automated, allowing \nAs technology approaches more empathy-requiring roles, we can expect the following to be in \nSpecialized expertise: Demand will grow significantly for experts in AI ethics, regulations, \nsecurity oversight, and human-AI collaboration design.\nCreative fields: Musicians and artists will develop new forms of human-AI collaboration, \nholder management will be among the last to see significant automation, potentially \nWithout deliberate policy interventions, the economic benefits of AI may accrue disproportion-\nAI systems will likely see wage growth, while others may face displacement or wage \nCapital concentration: Organizations that successfully implement AI may capture dis-\nThe consistent pattern across all timeframes is that while routine tasks face increasing automation \n(at rates determined by sector-specific factors), human expertise to guide AI systems and ensure \nrather than wholesale replacement, with technical experts remaining key to developing AI tools \nBy automating routine tasks, advanced AI models may ultimately free up human time for high-\ner-value work, potentially boosting overall economic output while creating transition challenges \nThe development of reasoning-capable AI will likely \nAs developers and stakeholders in the AI ecosystem, understanding the broader societal implica-\ndecisions we make today will shape the impacts of AI on information environments, intellectual \nand contribute to shaping a future where generative AI creates broad benefits while minimizing \nethical and regulatory considerations that increasingly affect AI development and deployment.\nAI presents a dual-edged sword for information integrity and security.\nGenerative AI can create \nBeyond pure misinformation, generative AI accelerates social engineering attacks by enabling \nAI systems can \ncapabilities, persistent hallucinations even in the most advanced systems suggest that technical \nAI with human oversight and external knowledge verification.\nThe combination of AI’s generative capabilities with internet-scale distribution mechanisms \nGenerative AI raises important copyright questions for developers.\nwww.reuters.com/world/us/us-appeals-court-rejects-copyrights-ai-generated-art-\nlacking-human-creator-2025-03-18/) have established that AI-generated content without \ncopyright law, confirming works created solely by AI cannot be copyrighted.\nAI-only outputs remain uncopyrightable, \nwhile human-directed AI outputs with creative selection may be copyrightable, and AI-assisted \nliterature, where generative AI can produce works stylistically similar to specific artists or authors.\nmodels distributing royalties to creators whose work informed the AI, technical watermarking to \ndistinguish AI-generated content, and legal frameworks establishing clear attribution standards.\nInternational frameworks vary, with the EU’s AI Act of 2024 establishing specific data mining \nnavigate the complex interplay between rights-holders and AI-generated content.",
      "keywords": [
        "scaling",
        "automation",
        "generative",
        "Generative Models",
        "human",
        "systems",
        "tasks",
        "models",
        "capabilities",
        "roles",
        "sectors",
        "McKinsey Global Institute",
        "economic",
        "Future",
        "Impact"
      ],
      "concepts": [
        "automation",
        "automated",
        "automate",
        "human",
        "potentially",
        "potential",
        "scale",
        "systems",
        "significant",
        "capabilities"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 1,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 50,
          "title": "",
          "score": 0.66,
          "base_score": 0.51,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 51,
          "title": "",
          "score": 0.603,
          "base_score": 0.453,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 35,
          "title": "",
          "score": 0.49,
          "base_score": 0.49,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "automation",
          "ai",
          "sectors",
          "generative",
          "human"
        ],
        "semantic": [],
        "merged": [
          "automation",
          "ai",
          "sectors",
          "generative",
          "human"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.25072084959326013,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304821+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 448-456)",
      "start_page": 448,
      "end_page": 456,
      "summary": "to regulating AI systems.\nMinimal risk: Basic AI applications with limited potential for harm\nHigh-risk AI applications like medical software and recruitment tools face strict requirements \nAI models with high impact potential.\nversity, and transparency in AI development.\ncurricula can help reduce biases in AI code by teaching developers how to build applications \nAs we conclude this exploration of generative AI with LangChain, we hope you’re equipped not \n(https://github.com/benman1/generative_ai_with_langchain) will be maintained and up-\ndated as LangChain and the broader generative AI landscape continue to evolve.\nyou’ll need to connect to various model providers, each with its own authentication mechanisms, \nWe’ll first cover the detailed setup instructions for the major LLM providers, including OpenAI, \nHugging Face, Google, and others.\ncounts, generating API keys, and configuring your development environment to use these services \nOpenAI remains one of the most popular LLM providers, offering models with various levels of \nintegration with OpenAI’s APIs, supporting both their traditional completion models and chat \nTo work with OpenAI models, we need to obtain an OpenAI API key first.\nTo create an API key, \nYou need to create a login at https://platform.openai.com/.\nFigure A.1: OpenAI API platform – Create new secret key\nAfter clicking Create secret key, you should see the message API key generated.\nYou can see a list of models at https://platform.openai.com/docs/models.\nOpenAI provides a comprehensive suite of capabilities that integrate seamlessly with LangChain, \nCore language models via the OpenAI API\nWe’ll cover the basics of model integration in this chapter, while deeper explorations of specialized \nLangChain supports leveraging the Hugging Face Hub, which provides access to a massive num-\nFor local use, LangChain provides integration with Hugging Face models and pipelines.\nChatHuggingFace class allows using Hugging Face models for chat applications, while the \nHuggingFacePipeline class enables running Hugging Face models locally through pipe-\nAdditionally, LangChain supports embedding models from Hugging Face, including \nHuggingFaceDatasetLoader to load datasets from the Hugging Face Hub. To use Hugging Face as a provider for your models, you can create an account and API keys at \nGoogle offers two primary platforms to access its LLMs, including the latest Gemini models:\n1. Google AI platform\nThe Google AI platform provides a straightforward setup for developers and users, and access to \nTo use the Gemini models via Google AI:\nAPI Key: Generate an API key to authenticate your requests.\nVisit this page to create your API key: https://ai.google.dev/gemini-api/docs/\nAfter obtaining the API key, set the GOOGLE_API_KEY environment variable in your \n2. Google Cloud Vertex AI\nFor enterprise-level features and integration, Google’s Gemini models are available through \nGoogle Cloud’s Vertex AI platform.\nTo use models via Vertex AI:\nCreate a Google Cloud account, which requires accepting the terms of service and setting \nEnsure that the Vertex AI API is enabled for your Google Cloud project.\nLangChain offers integrations with Google services such as language model inference, embeddings, \ncopy the API key and make it available in your environment as REPLICATE_API_TOKEN.\nAnthropic: You need to set the ANTHROPIC_API_KEY environment variable.\nkeys, virtual private cloud integration, and more, requiring a Google Cloud account ",
      "keywords": [
        "Hugging Face",
        "API key",
        "Hugging Face models",
        "API",
        "API keys",
        "Google Cloud",
        "Google",
        "Hugging Face Hub",
        "face",
        "models",
        "key",
        "Hugging",
        "Google Cloud account",
        "Hugging Face Endpoints",
        "OpenAI API key"
      ],
      "concepts": [
        "models",
        "api",
        "apis",
        "google",
        "developers",
        "risk",
        "includes",
        "including",
        "langchain",
        "services"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 1,
          "title": "",
          "score": 0.653,
          "base_score": 0.653,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.63,
          "base_score": 0.63,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.617,
          "base_score": 0.617,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 38,
          "title": "",
          "score": 0.614,
          "base_score": 0.614,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 6,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "api",
          "google",
          "hugging",
          "api key",
          "face"
        ],
        "semantic": [],
        "merged": [
          "api",
          "google",
          "hugging",
          "api key",
          "face"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3657092549128568,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.304872+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 457-464)",
      "start_page": 457,
      "end_page": 464,
      "summary": "Summarizing long videos\nvideo (by splitting it into offset intervals), and these messages can be processed in parallel.\nWe still need to run two separate steps if we want to pass video_uri as an \ncreate_inputs_chain = RunnableLambda(lambda x: _create_input_\nmap_step_chain = create_inputs_chain | RunnableLambda(lambda x: map_chain.\nsummaries = map_step_chain.invoke({\"video_uri\": video_uri})\nNow let’s merge all summaries provided into a single prompt and ask an LLM to prepare a final \nsummary:\ndef _merge_summaries(summaries: list[str], interval_secs: int = 600, \nsub_summaries = []\nsub_summary = (\nf\"Summary from sec {i*interval_secs} to sec {(i+1)*interval_\nf\"\\n{summary}\\n\"\n\"You are given a list of summaries that\"\n\"SUMMARIES:\\n{summaries}\"\n\"Based on that, prepare a summary of a whole video.\"\nreduce_chain = RunnableLambda(lambda x: _merge_summaries(**x)) | reduce_\nfinal_summary = reduce_chain.invoke({\"summaries\": summaries})\nRunnablePassthrough.assign(summaries=map_step_chain).assign(final_ \nsummary=reduce_chain)\n| RunnableLambda(lambda x: x[\"final_summary\"])\nWe generated multiple summaries of different parts of the video, and \nthen we passed these summaries to an LLM as texts and tasked it to generate a final summary.\nsummary and perform summarization step by step – each time, providing an LLM with a new \npiece of the video and a previously generated summary as input.\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nBuilding AI Agents with LLMs, RAG, and Knowledge Graphs\nDevelop AI agents that plan, reason, and use tools to complete tasks.\nOther Books You May Enjoy\nBuilding Agentic AI Systems\nImplement systems where AI agents can leverage external tools and plan complex tasks\nExplore real-world implementations of AI agents across industries\nOther Books You May Enjoy",
      "keywords": [
        "summaries",
        "video",
        "summary",
        "chain",
        "int",
        "Agents",
        "Python function",
        "books",
        "LLM",
        "final",
        "Python",
        "long videos",
        "reduce",
        "list",
        "summarize long videos"
      ],
      "concepts": [
        "summaries",
        "summary",
        "apply",
        "tasked",
        "piece",
        "prompt",
        "video",
        "information",
        "steps",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 24,
          "title": "",
          "score": 0.636,
          "base_score": 0.486,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 13,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 9,
          "title": "",
          "score": 0.602,
          "base_score": 0.452,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 8,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "summaries",
          "summary",
          "video",
          "videos",
          "runnablelambda lambda"
        ],
        "semantic": [],
        "merged": [
          "summaries",
          "summary",
          "video",
          "videos",
          "runnablelambda lambda"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.282121339043467,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:04.304920+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 465-472)",
      "start_page": 465,
      "end_page": 472,
      "summary": "agentic AI  9\nagentic architectures  224-226\nAgentic RAG  157\nagent memory  262\nagents  3, 216\nAI agents  11, 12\nsignificant challenges  12\nautonomous agents  10\nbuilding blocks, LangChain\nmodel interfaces  32\nbuilt-in LangChain tools  192-198\nagent-based chunking  135\nagentic approach  289, 290\ndocumentation RAG  290-292\nGoogle generative AI  282, 283\nCohere models  30\nconcept-level modeling  412\ndocument retrieval  166-168\nlanguage model setup  165, 166\nCorporate Documentation Manager tool   161\nCorrective Retrieval-Augmented Generation \nDALL-E model\nDeepSeek models  30\ndocumentation RAG  290-292\ndocument processing, RAG pipeline\nretrieval  137\nFoundational Model Orchestration  \ngenerative AI applications\ngenerative AI models  400-405\nGoogle AI platform  430\ngoogle generative AI  282, 283\nversus generative AI models  403, 404\ndeployment model, selecting  378, 379\nmodel serving infrastructure  380-382\nLangChain  14\nagent development  16\nLangChain agents, with datasets\nlangchain-anthropic  20\nLangChain applications\ncost management  391\nmodel selection strategies  391\nLangChain architecture\nlangchain-core  20\nlangchain-openai  20\nLangChain retrievers\nAdvanced/Specialized Retrievers   138\nIntegration Retrievers   138\nLangGraph  21\nlarge language model (LLM)  1\nLLM agents evaluation\nLLM agents, for data science\nML model, training  297\nLLM applications\nLLM applications deployment\nconsiderations, for LangChain  \nLangGraph platform  370, 371\nModel Context Protocol (MCP)  375-377\nLLM evaluation\nLLM-generated code\nLangChain integrations  281\nlocal models\nHugging Face models  50, 51\nMistral models  30\nML model\nPython-capable agent, setting up  297\nModel Context Protocol (MCP)  375\nmodel interfaces, LangChain  32\nchat models, working with  34, 35\nmodel behavior, controlling  38, 39\nreasoning models  36-38\nmodel licenses\nmodel openness framework (MOF)  8\nmodel scaling laws\nmodel selection strategies, LangChain  391\ncascading model approach  393, 394\ntiered model selection  391-393\nmodel comparison  4-6\nmulti-agent architectures  227\nLangGraph platform  247\nmultimodal AI applications  54\nNeural Attention Memory Models  \nperplexity models  30\nagentic approach  226, 227\nRAG grounds model  411\nRAG system\nretriever  110\nagentic RAG  157\nreasoning models  92",
      "keywords": [
        "model",
        "RAG",
        "LLM",
        "LangChain",
        "Index",
        "applications",
        "agent",
        "retrievers",
        "LangGraph",
        "LLMs",
        "chunking",
        "code LLMs",
        "link",
        "Google",
        "considerations"
      ],
      "concepts": [
        "model",
        "agent",
        "retrieval",
        "retrievers",
        "langchain",
        "llm",
        "application",
        "approach",
        "approaches",
        "rag"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.764,
          "base_score": 0.764,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 5,
          "title": "",
          "score": 0.605,
          "base_score": 0.605,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 4,
          "title": "",
          "score": 0.571,
          "base_score": 0.571,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 6,
          "title": "",
          "score": 0.566,
          "base_score": 0.566,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.56,
          "base_score": 0.56,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model",
          "models",
          "langchain",
          "models 30",
          "391"
        ],
        "semantic": [],
        "merged": [
          "model",
          "models",
          "langchain",
          "models 30",
          "391"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39931810956092456,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.305074+00:00"
      }
    },
    {
      "chapter_number": 56,
      "title": "Segment 56 (pages 473-477)",
      "start_page": 473,
      "end_page": 477,
      "summary": "Index\nReduce approach  94\nre-ranking, implementations\nretrievers\nLangChain retrievers  138\npatterns followed  137\nvector store retrievers  139, 140\nscaling, alternative approach  409\nalternative approach  409\nBig tech, versus small enterprises  407, 408\nscaling limitations, alternative approach\ntraditional approach  409\nchallenges  423\nIndex\ntools  2\nbuilt-in LangChain tools  192-199\ntraditional approach\nvector indexing\nvector store retrievers\nDownload a free PDF copy of this book\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nbooks directly into your application.\nhttps://packt.link/free-ebook/9781837022014\nStay connected with Packt’s Generative AI community\ntilled—the go-to newsletter for AI professionals, researchers, and innovators—at https://packt.\nIf you have questions about the book or want to dive deeper into Generative AI and ",
      "keywords": [
        "approach",
        "retrievers",
        "rerankers",
        "Index",
        "scaling",
        "alternative approach",
        "book",
        "tools",
        "alternative",
        "PDF",
        "Stable Diffusion",
        "LangChain",
        "vector",
        "RAG 293-295 re-ranking",
        "search"
      ],
      "concepts": [
        "approach",
        "tools",
        "index",
        "retrievers",
        "free",
        "link",
        "search",
        "scaling",
        "book",
        "challenges"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 55,
          "title": "",
          "score": 0.556,
          "base_score": 0.556,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 20,
          "title": "",
          "score": 0.483,
          "base_score": 0.483,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 3,
          "title": "",
          "score": 0.464,
          "base_score": 0.464,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 2,
          "title": "",
          "score": 0.454,
          "base_score": 0.454,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 1,
          "title": "",
          "score": 0.453,
          "base_score": 0.453,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "alternative approach",
          "alternative",
          "packt",
          "retrievers",
          "book"
        ],
        "semantic": [],
        "merged": [
          "alternative approach",
          "alternative",
          "packt",
          "retrievers",
          "book"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3081741490595051,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:04.305125+00:00"
      }
    }
  ],
  "total_chapters": 56,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Generative AI with LangChain_2e_metadata.json",
    "enrichment_date": "2025-12-17T23:06:04.315240+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4183.136169000136,
    "total_similar_chapters": 280
  }
}