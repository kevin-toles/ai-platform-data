{
  "metadata": {
    "title": "Fluent Python 2nd",
    "author": "Luciano Ramalho",
    "publisher": "O'Reilly Media",
    "edition": "2nd Edition",
    "isbn": "978-1-492-05632-4",
    "total_pages": 1011,
    "conversion_date": "2025-11-05T18:42:12.964519",
    "conversion_method": "PyMuPDF + OCR fallback"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-19)",
      "start_page": 1,
      "end_page": 19,
      "detection_method": "synthetic",
      "chapter_number": 1,
      "summary": "Luciano Ramalho\n2nd Edition\nCovers Python 3.10\nFluent \nPython\nClear, Concise, and \nEffective Programming 9\n\nO'REILLY*\n\nFluent Python\n\nDon't waste time bending Python to fit patterns you've\nlearned in other languages Key topics include type, typed, and typing.",
      "keywords": [
        "Chapter Summary",
        "Table of Contents",
        "Summary",
        "Python",
        "Reading",
        "Luciano Ramalho",
        "Functions",
        "attribute",
        "Fluent Python",
        "types",
        "type hints",
        "Contents",
        "Pattern Matching",
        "Data Class",
        "Sequences"
      ],
      "concepts": [
        "type",
        "typed",
        "typing",
        "classes",
        "functions",
        "function",
        "functional",
        "python",
        "pythonic",
        "attributes"
      ],
      "similar_chapters": [
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 2,
          "title": "Python Language Basics, IPython,",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 1,
          "title": "Preliminaries",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 20-40)",
      "start_page": 20,
      "end_page": 40,
      "detection_method": "synthetic",
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 20-40). Key topics include python, pythonic, and chapters.",
      "keywords": [
        "Python Data Model",
        "Python",
        "Python Data",
        "Card",
        "special methods",
        "book",
        "Data Model",
        "Overriding Deletion",
        "Docstring and Overriding",
        "special",
        "suit",
        "rank",
        "Python code",
        "methods",
        "Pythonic Card Deck"
      ],
      "concepts": [
        "python",
        "pythonic",
        "chapters",
        "card",
        "language",
        "classes",
        "collections",
        "collection",
        "collected",
        "preface"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.82,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.79,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.77,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 41-61)",
      "start_page": 41,
      "end_page": 61,
      "detection_method": "synthetic",
      "chapter_number": 3,
      "summary": "We will fix that with the\nspecial method __rmul__ in Chapter 16 Key topics include python, pythonic, and list. A simple two-dimensional vector class\n\"\"\"\nvector2d.py: a simplistic class demonstrating some special methods\nIt is simplistic for didactic reasons.",
      "keywords": [
        "Python Data Model",
        "Python",
        "Python Data",
        "special methods",
        "Data Model",
        "Python object model",
        "Python object",
        "list",
        "sequences",
        "methods",
        "Python Language Reference",
        "special",
        "Vector",
        "sequence types",
        "List comprehensions"
      ],
      "concepts": [
        "python",
        "pythonic",
        "list",
        "sequence",
        "examples",
        "type",
        "typing",
        "method",
        "abcs",
        "abc"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 3,
          "title": "Segment 3 (pages 17-26)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 62-83)",
      "start_page": 62,
      "end_page": 83,
      "detection_method": "synthetic",
      "chapter_number": 4,
      "summary": "Chapter 5 presents\ntwo ways of creating tuples with named fields Key topics include python, pythonic, and cases. It’s\njust a strange but valid variable name.",
      "keywords": [
        "Pattern Matching",
        "Sequences",
        "Python",
        "Pattern",
        "list",
        "sequence pattern",
        "case",
        "tuple",
        "items",
        "match",
        "Pattern Matching Sequences",
        "Sequences list tuple",
        "unpacking",
        "Unpacking Sequences"
      ],
      "concepts": [
        "python",
        "pythonic",
        "cases",
        "sequences",
        "list",
        "match",
        "matches",
        "examples",
        "slices",
        "sliced"
      ],
      "similar_chapters": [
        {
          "book": "Learning Python Ed6",
          "chapter": 9,
          "title": "Tuples, Files, and Everything Else",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 5,
          "title": "Segment 5 (pages 35-42)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "makinggames",
          "chapter": 22,
          "title": "Segment 22 (pages 189-197)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 6,
          "title": "Segment 6 (pages 43-52)",
          "relevance_score": 0.48,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 84-106)",
      "start_page": 84,
      "end_page": 106,
      "detection_method": "synthetic",
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 84-106). Key topics include python, pythonic, and array. Because string building with += in loops is so common in real codeba‐\nses, CPython is optimized for this use case.",
      "keywords": [
        "Python",
        "Array",
        "items",
        "list",
        "sequences",
        "sorted",
        "Python standard library",
        "NumPy",
        "deque",
        "Python standard",
        "sequence types",
        "array item",
        "list array",
        "Sequences list deque"
      ],
      "concepts": [
        "python",
        "pythonic",
        "array",
        "sequences",
        "sorting",
        "list",
        "numpy",
        "items",
        "floating",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 6,
          "title": "Segment 6 (pages 43-52)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 107-127)",
      "start_page": 107,
      "end_page": 127,
      "detection_method": "synthetic",
      "chapter_number": 6,
      "summary": "• “collections.OrderedDict” on page 95 now focuses on the small but still relevant\ndifferences between dict and OrderedDict—considering that dict keeps the key\ninsertion order since Python 3.6 Key topics include mappings.",
      "keywords": [
        "key",
        "dict",
        "Python",
        "mapping",
        "mapping types",
        "missing",
        "missing keys",
        "keys",
        "word",
        "type",
        "method",
        "index mapping word",
        "Sets",
        "default",
        "list"
      ],
      "concepts": [
        "key",
        "mappings",
        "map",
        "examples",
        "python",
        "pythonic",
        "types",
        "typing",
        "methods",
        "objects"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 10,
          "title": "Segment 10 (pages 83-98)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 6,
          "title": "Segment 6 (pages 43-52)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 9,
          "title": "Segment 9 (pages 70-82)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 128-146)",
      "start_page": 128,
      "end_page": 146,
      "detection_method": "synthetic",
      "chapter_number": 7,
      "summary": "This chapter covers segment 7 (pages 128-146). Key topics include sets, python, and method. This avoids undesired\nrecursion when coding special methods like __setitem__, and simplifies the coding\nof __contains__, compared to Example 3-8.",
      "keywords": [
        "Python",
        "dict",
        "Sets",
        "Set Operations",
        "keys",
        "methods",
        "sets built",
        "key",
        "set operators",
        "items",
        "Dictionaries",
        "mapping",
        "Python operator",
        "Set Theory",
        "views"
      ],
      "concepts": [
        "sets",
        "python",
        "method",
        "keys",
        "key",
        "examples",
        "mapping",
        "map",
        "uses",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 1,
          "title": "Data Structures and Algorithms",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 10,
          "title": "Segment 10 (pages 83-98)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 147-169)",
      "start_page": 147,
      "end_page": 169,
      "detection_method": "synthetic",
      "chapter_number": 8,
      "summary": "This chapter deals with Unicode strings, binary sequences, and the encod‐\nings used to convert between them Key topics include encoding, encode, and bytes.",
      "keywords": [
        "Unicode Text Versus",
        "Text Versus Bytes",
        "Unicode Text",
        "Bytes",
        "Unicode",
        "Encoding",
        "Python",
        "Text",
        "Versus Bytes",
        "text files",
        "handling text files",
        "Text Versus",
        "byte sequence",
        "Unicode characters",
        "File"
      ],
      "concepts": [
        "encoding",
        "encode",
        "bytes",
        "character",
        "python",
        "codes",
        "coding",
        "text",
        "likely",
        "files"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 6,
          "title": "The Dynamic Typing Interlude",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 9,
          "title": "Segment 9 (pages 93-101)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "Segment 4 (pages 60-78)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 170-191)",
      "start_page": 170,
      "end_page": 191,
      "detection_method": "synthetic",
      "chapter_number": 9,
      "summary": "This chapter covers segment 9 (pages 170-191). Key topics include character, normalizing, and normalizations. In the Unicode\nstandard, sequences like 'é' and 'e\\u0301' are called “canonical equivalents,” and\napplications are supposed to treat them as the same.",
      "keywords": [
        "Unicode Text Versus",
        "Text Versus Bytes",
        "Unicode Text",
        "Unicode",
        "text",
        "Bytes",
        "Python",
        "Versus Bytes",
        "Sorting Unicode Text",
        "Text Versus",
        "Unicode character",
        "characters",
        "Unicode string comparison",
        "Unicode Database",
        "NFC"
      ],
      "concepts": [
        "character",
        "normalizing",
        "normalizations",
        "normalized",
        "python",
        "examples",
        "text",
        "bytes",
        "code",
        "coding"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "Segment 4 (pages 60-78)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 9,
          "title": "Segment 9 (pages 93-101)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 6,
          "title": "Segment 6 (pages 43-50)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 6,
          "title": "Segment 6 (pages 43-52)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 192-209)",
      "start_page": 192,
      "end_page": 209,
      "detection_method": "synthetic",
      "chapter_number": 10,
      "summary": "This chapter covers three\ndifferent class builders that you may use as shortcuts to write data classes:\ncollections.namedtuple\nThe simplest way—available since Python 2.6 Key topics include classes, typing, and type.",
      "keywords": [
        "Data Class Builders",
        "Data Class",
        "Class Builders",
        "coordinate",
        "class attribute",
        "Coordinate class",
        "type hints",
        "Data",
        "type",
        "Armin Ronacher",
        "NamedTuple class Coordinate",
        "Python",
        "attribute",
        "class statement",
        "Builders"
      ],
      "concepts": [
        "classes",
        "typing",
        "type",
        "typed",
        "coordinate",
        "python",
        "pythonic",
        "method",
        "examples",
        "annotations"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 20,
          "title": "Segment 20 (pages 198-208)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 23,
          "title": "Segment 23 (pages 232-240)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 210-230)",
      "start_page": 210,
      "end_page": 230,
      "detection_method": "synthetic",
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 210-230). Key topics include classes, type, and typing. The options\nyou are more likely to change from the defaults are:\nfrozen=True\nProtects against accidental changes to the class instances.",
      "keywords": [
        "Data Class Builders",
        "data class",
        "Class Builders",
        "data",
        "Class patterns",
        "dataclass class",
        "Data Classes",
        "class attribute",
        "dataclass",
        "field",
        "dataclass class Spam",
        "default",
        "dataclass class ClubMember",
        "City",
        "data class fields"
      ],
      "concepts": [
        "classes",
        "type",
        "typing",
        "typed",
        "fields",
        "handle",
        "handled",
        "examples",
        "data",
        "list"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 26,
          "title": "Segment 26 (pages 230-237)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 21,
          "title": "Segment 21 (pages 209-217)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 10,
          "title": "Segment 10 (pages 83-98)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 27,
          "title": "Segment 27 (pages 238-246)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 231-249)",
      "start_page": 231,
      "end_page": 249,
      "detection_method": "synthetic",
      "chapter_number": 12,
      "summary": "The name really IS ‘THE AGED AGED MAN.’”\n—Adapted from Lewis Carroll, Through the Looking-Glass, and What Alice Found\nThere\nAlice and the Knight set the tone of what we will see in this chapter Key topics include object, references, and refer.",
      "keywords": [
        "Object",
        "Object References",
        "list",
        "References",
        "Python",
        "Python Language Reference",
        "copy",
        "passengers",
        "list object",
        "Charles",
        "mutable object",
        "function",
        "variables",
        "Online Python Tutor",
        "Default"
      ],
      "concepts": [
        "object",
        "references",
        "refer",
        "referred",
        "bus",
        "python",
        "pythonic",
        "list",
        "passengers",
        "ids"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 25,
          "title": "Segment 25 (pages 209-210)",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.4,
          "method": "sentence_transformers"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.39,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.39,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.39,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 250-271)",
      "start_page": 250,
      "end_page": 271,
      "detection_method": "synthetic",
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 250-271). Key topics include object, function, and functional. Covers function. Other implementations of Python have more sophisticated\ngarbage collectors that do not rely on reference counting, which means the __del__\nmethod may not be called immediately when there are no more references to the\nobject.",
      "keywords": [
        "Python",
        "object",
        "function",
        "Functions",
        "Python functions",
        "First-Class Objects",
        "Python objects",
        "function object",
        "Object References",
        "Callable Objects",
        "references",
        "functions return objects",
        "Python Language Reference",
        "functions first-class objects",
        "Function parameters"
      ],
      "concepts": [
        "object",
        "function",
        "functional",
        "functions",
        "python",
        "classes",
        "examples",
        "reference",
        "referring",
        "refer"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 5,
          "title": "Table 3.11",
          "relevance_score": 0.74,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 18,
          "title": "Segment 18 (pages 175-187)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 272-289)",
      "start_page": 272,
      "end_page": 289,
      "detection_method": "synthetic",
      "chapter_number": 14,
      "summary": "After diving into Python’s flexible argument declaration features, the remainder of\nthis chapter covers the most useful packages in the standard library for programming\nin a functional style Key topics include types, typed, and typing. Covers function.",
      "keywords": [
        "type hints",
        "Python",
        "type",
        "function",
        "functions",
        "hints",
        "Mypy",
        "type hints function",
        "count",
        "Functional Programming",
        "arguments",
        "add type hints",
        "Python Functional Programming",
        "functional",
        "show"
      ],
      "concepts": [
        "types",
        "typed",
        "typing",
        "function",
        "functions",
        "functional",
        "python",
        "examples",
        "useful",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 43,
          "title": "Segment 43 (pages 452-461)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 290-311)",
      "start_page": 290,
      "end_page": 311,
      "detection_method": "synthetic",
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 290-311). Key topics include type, typed, and function. If you don’t assign a default value to plural, the Python runtime will treat it as a\nrequired parameter.",
      "keywords": [
        "type hints",
        "type",
        "Python",
        "str",
        "generic type hints",
        "Bird",
        "Types Usable",
        "hints",
        "Duck",
        "type checker",
        "parameter type hints",
        "function",
        "collection types",
        "generic type",
        "Duck typing"
      ],
      "concepts": [
        "type",
        "typed",
        "function",
        "functions",
        "examples",
        "python",
        "importing",
        "important",
        "bird",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 312-333)",
      "start_page": 312,
      "end_page": 333,
      "detection_method": "synthetic",
      "chapter_number": 16,
      "summary": "This chapter covers segment 16 (pages 312-333). Key topics include type, typing, and typed. On the other hand, the columnize function from Example 8-13 needs a Sequence\nparameter, and not an Iterable, because it must get the len() of the input to com‐\npute the number of rows up front.",
      "keywords": [
        "Type Hints",
        "type",
        "Python",
        "Hints",
        "type parameter",
        "Protocol type",
        "Callable type hint",
        "typing",
        "Callable type",
        "Callable",
        "return type",
        "Iterable",
        "function",
        "Python object types",
        "Types Usable"
      ],
      "concepts": [
        "type",
        "typing",
        "typed",
        "function",
        "functions",
        "functionality",
        "python",
        "pythonic",
        "object",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 13,
          "title": "Segment 13 (pages 108-115)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 43,
          "title": "Segment 43 (pages 452-461)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 334-353)",
      "start_page": 334,
      "end_page": 353,
      "detection_method": "synthetic",
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 334-353). Key topics include decorator, decorated, and decorate. Covers function, decorator. Class decorators are covered in Chapter 24.",
      "keywords": [
        "function",
        "decorator",
        "Python",
        "decorated function",
        "Fibonacci",
        "variable",
        "Python Executes Decorators",
        "Averager",
        "Functions",
        "closures",
        "local variable",
        "local",
        "global",
        "cache",
        "LOAD"
      ],
      "concepts": [
        "decorator",
        "decorated",
        "decorate",
        "function",
        "functions",
        "functional",
        "examples",
        "python",
        "closures",
        "variable"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 16,
          "title": "Segment 16 (pages 133-141)",
          "relevance_score": 0.77,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 13,
          "title": "Segment 13 (pages 126-133)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 8,
          "title": "Classes and Objects",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 354-371)",
      "start_page": 354,
      "end_page": 371,
      "detection_method": "synthetic",
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 354-371). Key topics include functions, function, and functionality. Covers function, decorator. Here is an example invoking @lru_cache with nondefault parameters:\n@lru_cache(maxsize=2**20, typed=True)\ndef costly_function(a, b):.",
      "keywords": [
        "function",
        "decorator",
        "Python",
        "Python function decorators",
        "Functions",
        "Parameterized Clock Decorator",
        "Parameterized Decorators",
        "pre",
        "function decorators",
        "register",
        "Decorator Design Pattern",
        "Clock Decorator",
        "register decorator",
        "Design Patterns",
        "Python decorator"
      ],
      "concepts": [
        "functions",
        "function",
        "functionality",
        "decorator",
        "decorate",
        "decorated",
        "python",
        "registered",
        "register",
        "typed"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 16,
          "title": "Segment 16 (pages 133-141)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 11,
          "title": "Segment 11 (pages 108-115)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 44,
          "title": "Segment 44 (pages 462-467)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 26,
          "title": "Segment 26 (pages 815-849)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 372-391)",
      "start_page": 372,
      "end_page": 391,
      "detection_method": "synthetic",
      "chapter_number": 19,
      "summary": "The goal of this chapter is to show how—in some cases—functions can do the same\nwork as classes, with code that is more readable and concise Key topics include functions, function, and functionality.",
      "keywords": [
        "Design Patterns",
        "order",
        "decimal",
        "Python Design Patterns",
        "Strategy Pattern",
        "Patterns",
        "Strategy",
        "Design",
        "Strategy design pattern",
        "Order total",
        "functions",
        "Design Patterns book",
        "Promo",
        "strategy import Order",
        "Command design pattern"
      ],
      "concepts": [
        "functions",
        "function",
        "functionality",
        "classes",
        "orders",
        "ordered",
        "promotional",
        "promotion",
        "promotions",
        "strategy"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 18,
          "title": "Segment 18 (pages 175-187)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 14,
          "title": "Segment 14 (pages 116-123)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 4,
          "title": "Segment 4 (pages 36-45)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 392-414)",
      "start_page": 392,
      "end_page": 414,
      "detection_method": "synthetic",
      "chapter_number": 20,
      "summary": "We will now\nbuild user-defined classes that behave as real Python objects Key topics include format, formatted, and methods. Covers method. —Martijn Faassen, creator of Python and JavaScript frameworks.1\nThanks to the Python Data Model, your user-defined types can behave as naturally as\nthe built-in types.",
      "keywords": [
        "format",
        "Pythonic Object",
        "Python",
        "Object",
        "method",
        "Pythonic",
        "attribute",
        "class method",
        "Python objects",
        "Format Specification",
        "Pythonic Object return",
        "bytes",
        "special methods"
      ],
      "concepts": [
        "format",
        "formatted",
        "methods",
        "classes",
        "vector",
        "pythonic",
        "python",
        "useful",
        "uses",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 20,
          "title": "Segment 20 (pages 198-208)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 26,
          "title": "Extending and Embedding Python",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 415-432)",
      "start_page": 415,
      "end_page": 432,
      "detection_method": "synthetic",
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 415-432). Key topics include vector, pythonic, and python. Now let’s create a subclass of Pixel in Example 11-14 to see the\ncounterintuitive side of __slots__.",
      "keywords": [
        "vector",
        "slots",
        "Python",
        "class attribute",
        "attributes",
        "special methods",
        "instance",
        "methods",
        "instance attribute",
        "Python special methods",
        "typecode class attribute",
        "object",
        "Pythonic Object",
        "Java",
        "Vector class"
      ],
      "concepts": [
        "vector",
        "pythonic",
        "python",
        "classes",
        "examples",
        "object",
        "attributes",
        "instance",
        "methods",
        "java"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 3,
          "title": "Segment 3 (pages 17-26)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "More Effective C++",
          "chapter": 24,
          "title": "Segment 24 (pages 234-241)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 10,
          "title": "Segment 10 (pages 295-328)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 433-453)",
      "start_page": 433,
      "end_page": 453,
      "detection_method": "synthetic",
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 433-453). Key topics include vector, examples, and attribute. Covers method. With PEP 544—Protocols: Structural subtyping (static duck typ‐\ning), Python 3.8 supports protocol classes: typing constructs, which\nwe studied in “Static Protocols” on page 286.",
      "keywords": [
        "Vector",
        "Vector class",
        "vector components",
        "Special Methods",
        "components",
        "method",
        "vector components array",
        "slice",
        "attribute",
        "zip",
        "sequence",
        "len",
        "format",
        "Special",
        "Python"
      ],
      "concepts": [
        "vector",
        "examples",
        "attribute",
        "slicing",
        "slice",
        "zip",
        "methods",
        "formatting",
        "format",
        "functionality"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 13,
          "title": "Segment 13 (pages 108-115)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 44,
          "title": "Segment 44 (pages 462-467)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 55,
          "title": "Segment 55 (pages 524-531)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 454-475)",
      "start_page": 454,
      "end_page": 475,
      "detection_method": "synthetic",
      "chapter_number": 23,
      "summary": "This concludes our mission for this chapter Key topics include pythonic, typing, and type. 424 \n| \nChapter 12: Special Methods for Sequences Configure a Cartesian coordinate display with parentheses.",
      "keywords": [
        "Duck Typing",
        "Python",
        "Static duck typing",
        "sequence protocol",
        "protocols",
        "typing",
        "Static Protocols",
        "sequence",
        "type",
        "Duck",
        "Goose typing",
        "static type",
        "ABCs",
        "static",
        "method"
      ],
      "concepts": [
        "pythonic",
        "typing",
        "type",
        "typed",
        "classes",
        "likely",
        "protocols",
        "examples",
        "methods",
        "chapters"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 2,
          "title": "Python Language Basics, IPython,",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 476-496)",
      "start_page": 476,
      "end_page": 496,
      "detection_method": "synthetic",
      "chapter_number": 24,
      "summary": "Details of using register are covered in “A Virtual Subclass of an\nABC” on page 460, later in this chapter Key topics include abcs, types. • Runtime type checking using ABCs instead of concrete classes as the second\nargument for isinstance and issubclass.",
      "keywords": [
        "ABC",
        "Tombola ABC",
        "ABCs",
        "Tombola",
        "abc class Tombola",
        "methods",
        "Python",
        "subclass",
        "abstract methods",
        "tombola import",
        "typing",
        "goose typing",
        "Protocols",
        "abc module",
        "abstract"
      ],
      "concepts": [
        "abcs",
        "abc",
        "types",
        "typed",
        "methods",
        "implementing",
        "implement",
        "implementations",
        "returned",
        "tombola"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 5,
          "title": "Segment 5 (pages 36-43)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 21,
          "title": "Segment 21 (pages 176-183)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 29,
          "title": "Segment 29 (pages 271-279)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 497-516)",
      "start_page": 497,
      "end_page": 516,
      "detection_method": "synthetic",
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 497-516). Key topics include typing, types, and typed. Since it’s impossible to name all types\nthat implement a protocol by supporting the required operations, duck typing could\nnot be described by type hints before Python 3.8.",
      "keywords": [
        "Static Protocols",
        "static duck typing",
        "Protocols",
        "complex",
        "Python",
        "type",
        "typing",
        "duck typing",
        "Static",
        "static typing",
        "Python Static Typing",
        "ABCs",
        "Scalar Multiplication",
        "typing import Protocol",
        "numeric static protocols"
      ],
      "concepts": [
        "typing",
        "types",
        "typed",
        "protocol",
        "python",
        "complex",
        "method",
        "abc",
        "abcs",
        "static"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 517-534)",
      "start_page": 517,
      "end_page": 534,
      "detection_method": "synthetic",
      "chapter_number": 26,
      "summary": "Thanks to my friend Christiano Anderson, who shared this reference as I was writing this chapter Key topics include method, examples, and object. Covers method.",
      "keywords": [
        "Multiple inheritance",
        "method resolution order",
        "Inheritance",
        "method",
        "super",
        "ping",
        "Python",
        "method resolution",
        "classes",
        "Multiple",
        "Leaf",
        "MRO",
        "call super",
        "key",
        "call"
      ],
      "concepts": [
        "method",
        "examples",
        "object",
        "calls",
        "inheritance",
        "inherited",
        "keys",
        "key",
        "ping",
        "multiple"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 19,
          "title": "Segment 19 (pages 188-197)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 18,
          "title": "Segment 18 (pages 175-187)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 23,
          "title": "Segment 23 (pages 192-207)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 3,
          "title": "Segment 3 (pages 17-26)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 21,
          "title": "Segment 21 (pages 176-183)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 535-552)",
      "start_page": 535,
      "end_page": 552,
      "detection_method": "synthetic",
      "chapter_number": 27,
      "summary": "If an HTTP POST request is sent to a Tem\nplateView, the inherited View.dispatch method checks that there is no post han‐\ndler, and produces an HTTP 405 Method Not Allowed response.11\nThe TemplateResponseMixin provides functionality that is of interest only to views\nthat need to use a template Key topics include classes, types, and typing.",
      "keywords": [
        "Multiple Inheritance",
        "Inheritance",
        "Python",
        "classes",
        "widget",
        "methods",
        "mixin",
        "type",
        "Tkinter",
        "mixin classes",
        "Multiple",
        "Type Hints",
        "Tkinter GUI class",
        "view class method",
        "iterable"
      ],
      "concepts": [
        "classes",
        "types",
        "typing",
        "methods",
        "widget",
        "inherited",
        "inheritance",
        "python",
        "pythonic",
        "objects"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 24,
          "title": "Segment 24 (pages 208-218)",
          "relevance_score": 0.46,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 16,
          "title": "Segment 16 (pages 488-517)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 8,
          "title": "Segment 8 (pages 57-64)",
          "relevance_score": 0.42,
          "method": "sentence_transformers"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 8,
          "title": "Segment 8 (pages 60-67)",
          "relevance_score": 0.39,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 8,
          "title": "Segment 8 (pages 231-264)",
          "relevance_score": 0.39,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 553-575)",
      "start_page": 553,
      "end_page": 575,
      "detection_method": "synthetic",
      "chapter_number": 28,
      "summary": "Thanks to duck typing, my code has no isinstance\nchecks, and provides the same error checking as those type hints—but only at run‐\ntime, of course Key topics include typing, type, and classes.",
      "keywords": [
        "type hints",
        "type",
        "hints",
        "return type",
        "Python",
        "Mypy",
        "iterable",
        "Reading Type Hints",
        "Jelle Zijlstra",
        "type checker",
        "max",
        "str",
        "overload def max",
        "key",
        "book"
      ],
      "concepts": [
        "typing",
        "type",
        "classes",
        "examples",
        "book",
        "returns",
        "imports",
        "important",
        "python",
        "annotation"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 29,
          "title": "Segment 29 (pages 258-268)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Data Analysis 3rd",
          "chapter": 2,
          "title": "Python Language Basics, IPython,",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 11,
          "title": "Segment 11 (pages 90-99)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 576-595)",
      "start_page": 576,
      "end_page": 595,
      "detection_method": "synthetic",
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 576-595). Key topics include typing, types, and classes. Python mutable collection types—such as list and set—are invariant.",
      "keywords": [
        "type",
        "Type Hints",
        "Generic Type Hints",
        "type parameter",
        "formal type parameter",
        "generic type",
        "Python",
        "Generic",
        "Contravariant generic types",
        "juice",
        "formal type",
        "Hints",
        "return type",
        "Covariant generic types",
        "Operators"
      ],
      "concepts": [
        "typing",
        "types",
        "classes",
        "examples",
        "python",
        "generic",
        "operator",
        "operations",
        "useful",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 5,
          "title": "Numeric Types",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 4,
          "title": "Types and Objects",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 43,
          "title": "Segment 43 (pages 452-461)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 596-614)",
      "start_page": 596,
      "end_page": 614,
      "detection_method": "synthetic",
      "chapter_number": 30,
      "summary": "However, here we will implement + and * as mathematical vector operations, which\nare a bit harder but more meaningful for a Vector type Key topics include vector, examples, and method.",
      "keywords": [
        "Vector",
        "return Vector",
        "operator",
        "Vector class",
        "Operator Overloading",
        "Vector Addition",
        "Python",
        "Vector class def",
        "infix operator",
        "augmented assignment operators",
        "Vector instances",
        "add",
        "Rich Comparison Operators",
        "assignment operators",
        "Comparison Operators"
      ],
      "concepts": [
        "vector",
        "examples",
        "method",
        "type",
        "typing",
        "operators",
        "operations",
        "operation",
        "returns",
        "python"
      ],
      "similar_chapters": [
        {
          "book": "More Effective C++",
          "chapter": 6,
          "title": "Segment 6 (pages 41-51)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 7,
          "title": "Segment 7 (pages 54-61)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 11,
          "title": "Segment 11 (pages 203-224)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 615-636)",
      "start_page": 615,
      "end_page": 636,
      "detection_method": "synthetic",
      "chapter_number": 31,
      "summary": "Chapter Summary\nWe started this chapter by reviewing some restrictions Python imposes on operator\noverloading: no redefining of operators in the built-in types themselves, overloading\nlimited to existing operators, with a few operators left out (is, and, or, not) Key topics include iterate, iteration, and iterating. Covers iterator.",
      "keywords": [
        "iterator",
        "Python",
        "iter",
        "Sentence",
        "Iterables Versus Iterators",
        "operator overloading",
        "classic Iterator pattern",
        "classic Iterator",
        "Classic Coroutines",
        "Iterator design pattern",
        "operator",
        "iterator object",
        "Iterator pattern",
        "method"
      ],
      "concepts": [
        "iterate",
        "iteration",
        "iterating",
        "python",
        "pythonic",
        "operators",
        "operations",
        "operation",
        "type",
        "typing"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 14,
          "title": "Segment 14 (pages 134-147)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 4,
          "title": "Introducing Python Object Types",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 37,
          "title": "Segment 37 (pages 333-344)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Templates_ The Complete Guide",
          "chapter": 67,
          "title": "Segment 67 (pages 2147-2180)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 637-655)",
      "start_page": 637,
      "end_page": 655,
      "detection_method": "synthetic",
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 637-655). Key topics include generator, generates, and generated. Covers generator, function. And you can’t do that if you’re implementing an iterable, of course: the necessary special method must be\nnamed __iter__.",
      "keywords": [
        "generator function",
        "generator",
        "generator object",
        "generator expression",
        "function",
        "list",
        "generator function returns",
        "functions",
        "gen",
        "yield",
        "Classic Coroutines",
        "items",
        "filtering generator functions",
        "Sentence",
        "gen generator function"
      ],
      "concepts": [
        "generator",
        "generates",
        "generated",
        "iterate",
        "iteration",
        "iterative",
        "examples",
        "yield",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 14,
          "title": "Segment 14 (pages 134-147)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 15,
          "title": "Segment 15 (pages 148-156)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 9,
          "title": "Segment 9 (pages 71-78)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 37,
          "title": "Segment 37 (pages 333-344)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 656-676)",
      "start_page": 656,
      "end_page": 676,
      "detection_method": "synthetic",
      "chapter_number": 33,
      "summary": "A common use of repeat: providing a fixed argument in map; here it provides the\n5 multiplier Key topics include generator, examples, and yields. Covers generator.",
      "keywords": [
        "Classic Coroutines",
        "yield",
        "generator",
        "coroutine",
        "Tree",
        "generator functions",
        "tree generator",
        "type",
        "level",
        "Classic",
        "ABC",
        "functions",
        "Iterators",
        "generator functions yield",
        "items"
      ],
      "concepts": [
        "generator",
        "examples",
        "yields",
        "iteration",
        "iterates",
        "iterated",
        "type",
        "typing",
        "values",
        "list"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 14,
          "title": "Segment 14 (pages 134-147)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 8,
          "title": "Segment 8 (pages 62-70)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 15,
          "title": "Segment 15 (pages 148-156)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 16,
          "title": "Segment 16 (pages 157-164)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 43,
          "title": "Segment 43 (pages 452-461)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 677-699)",
      "start_page": 677,
      "end_page": 699,
      "detection_method": "synthetic",
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 677-699). Key topics include examples, python, and generator. It receives\ndata of the SendType and returns a Result tuple when done.",
      "keywords": [
        "context manager",
        "Python",
        "Classic Coroutines",
        "context",
        "context manager object",
        "Generator",
        "coroutines",
        "manager",
        "generator function",
        "yield",
        "glass context manager",
        "Result",
        "function",
        "return True Python",
        "context manager class"
      ],
      "concepts": [
        "examples",
        "python",
        "generator",
        "manage",
        "managers",
        "functions",
        "function",
        "functionality",
        "file",
        "yield"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 19,
          "title": "Segment 19 (pages 159-166)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 18,
          "title": "Segment 18 (pages 175-187)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 700-717)",
      "start_page": 700,
      "end_page": 717,
      "detection_method": "synthetic",
      "chapter_number": 35,
      "summary": "This chapter covers segment 35 (pages 700-717). Key topics include examples, evaluated, and evaluate. Covers function, lambda. Example 18-11 is the same code in Python (shorter than an English explanation of\nthe recursive Euclidean algorithm).",
      "keywords": [
        "env",
        "Python",
        "Environment",
        "Case",
        "Scheme",
        "Symbol",
        "evaluate",
        "function",
        "Expression",
        "lambda",
        "list",
        "symbols case Symbol",
        "Case Study",
        "exp",
        "body"
      ],
      "concepts": [
        "examples",
        "evaluated",
        "evaluate",
        "evaluator",
        "evaluation",
        "define",
        "defined",
        "lambda",
        "expressions",
        "expression"
      ],
      "similar_chapters": [
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 9,
          "title": "Segment 9 (pages 71-78)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 15,
          "title": "Segment 15 (pages 124-132)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 718-737)",
      "start_page": 718,
      "end_page": 737,
      "detection_method": "synthetic",
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 718-737). Key topics include python, pythonic, and thread. In Python, try/except is commonly used for control flow, and not just for error\nhandling.",
      "keywords": [
        "Python",
        "Python threads",
        "thread",
        "Python objects",
        "call",
        "Python code",
        "Python Language",
        "tail call",
        "PTC",
        "GIL",
        "spinner",
        "Python interpreter",
        "Python Language Reference",
        "function",
        "Python processes"
      ],
      "concepts": [
        "python",
        "pythonic",
        "thread",
        "processing",
        "process",
        "makes",
        "concurrency",
        "concurrent",
        "functions",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 11,
          "title": "Segment 11 (pages 90-99)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 7,
          "title": "Program Structure and Control Flow",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 25,
          "title": "Segment 25 (pages 249-264)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 738-758)",
      "start_page": 738,
      "end_page": 758,
      "detection_method": "synthetic",
      "chapter_number": 37,
      "summary": "It returns a Task instance, an\nobject that wraps the coroutine object and provides methods to control and\nquery its state Key topics include python, processes, and process.",
      "keywords": [
        "Python",
        "spinner",
        "Concurrency Models",
        "coroutine",
        "prime",
        "Time",
        "GIL",
        "Models in Python",
        "processes",
        "number",
        "code",
        "results",
        "slow",
        "await",
        "CPU"
      ],
      "concepts": [
        "python",
        "processes",
        "process",
        "processing",
        "threads",
        "prime",
        "result",
        "examples",
        "returns",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.72,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 28,
          "title": "Segment 28 (pages 290-298)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Essential Reference 4th",
          "chapter": 20,
          "title": "Threads and Concurrency",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 38,
          "title": "Segment 38 (pages 321-334)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 22,
          "title": "Segment 22 (pages 175-182)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 759-777)",
      "start_page": 759,
      "end_page": 777,
      "detection_method": "synthetic",
      "chapter_number": 38,
      "summary": "It’s as old as\nWSGI itself, but is actively maintained, and now provides a command-line launcher\ncalled mod_wsgi-express that makes it easier to configure and more suitable for use\nin Docker containers Key topics include python, uses, and threading.",
      "keywords": [
        "Python",
        "Python server-side applications",
        "WSGI Application Servers",
        "Python standard library",
        "Python web",
        "Python application code",
        "Concurrency Models",
        "Python application",
        "Concurrency",
        "Threads",
        "application server",
        "Python server-side",
        "flags",
        "code",
        "Python processes"
      ],
      "concepts": [
        "python",
        "uses",
        "threading",
        "concurrency",
        "concurrent",
        "processes",
        "process",
        "processing",
        "data",
        "chapters"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 25,
          "title": "Segment 25 (pages 249-264)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 5,
          "title": "Segment 5 (pages 33-40)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 32,
          "title": "Segment 32 (pages 264-271)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 33,
          "title": "Segment 33 (pages 272-284)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 778-797)",
      "start_page": 778,
      "end_page": 797,
      "detection_method": "synthetic",
      "chapter_number": 39,
      "summary": "Crucially, HTTPX pro‐\nvides synchronous and asynchronous APIs, so we can use it in all\nHTTP client examples in this chapter and the next Key topics include examples, future, and concurrent.",
      "keywords": [
        "future",
        "Concurrent Executors",
        "concurrent",
        "concurrent import futures",
        "download",
        "Concurrent Web Downloads",
        "result",
        "flags",
        "workers",
        "loiter",
        "code",
        "function",
        "executor",
        "Display",
        "local HTTP server"
      ],
      "concepts": [
        "examples",
        "future",
        "concurrent",
        "concurrency",
        "flags",
        "downloaded",
        "codes",
        "executors",
        "results",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 58,
          "title": "Segment 58 (pages 575-582)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 16,
          "title": "Segment 16 (pages 123-130)",
          "relevance_score": 0.53,
          "method": "sentence_transformers"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 7,
          "title": "Segment 7 (pages 51-58)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 30,
          "title": "Segment 30 (pages 275-283)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 798-818)",
      "start_page": 798,
      "end_page": 818,
      "detection_method": "synthetic",
      "chapter_number": 40,
      "summary": "This chapter covers segment 40 (pages 798-818). Key topics include examples, concurrent, and concurrency. This code\nis straightforward, but it’s worth studying to contrast with the concurrent versions\ncoming up.",
      "keywords": [
        "coroutine",
        "Asynchronous",
        "Python",
        "await",
        "error",
        "Asynchronous Programming",
        "asynchronous context managers",
        "asyncio",
        "event loop",
        "asynchronous context",
        "download",
        "native coroutine",
        "coroutine object",
        "loop",
        "async"
      ],
      "concepts": [
        "examples",
        "concurrent",
        "concurrency",
        "await",
        "asyncio",
        "downloads",
        "asynchronous",
        "futures",
        "python",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 29,
          "title": "Segment 29 (pages 299-314)",
          "relevance_score": 0.77,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 28,
          "title": "Segment 28 (pages 290-298)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 18,
          "title": "Segment 18 (pages 150-158)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 37,
          "title": "Segment 37 (pages 312-320)",
          "relevance_score": 0.7,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 819-841)",
      "start_page": 819,
      "end_page": 841,
      "detection_method": "synthetic",
      "chapter_number": 41,
      "summary": "If you’re using Network-Attached Stor‐\nage, it may even involve network I/O under the covers Key topics include asyncio, server, and examples. Second and third differences: .get is an AsyncClient method, and it’s a corou‐\ntine, so we need to await it.",
      "keywords": [
        "Asynchronous Programming",
        "server",
        "coroutine",
        "asynchronous",
        "Writing asyncio Servers",
        "await",
        "Python",
        "asyncio",
        "asyncio Servers",
        "semaphore",
        "code",
        "country",
        "async",
        "client",
        "function"
      ],
      "concepts": [
        "asyncio",
        "server",
        "examples",
        "await",
        "important",
        "returns",
        "code",
        "coding",
        "client",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "Python Microservices Development",
          "chapter": 37,
          "title": "Segment 37 (pages 312-320)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 29,
          "title": "Segment 29 (pages 299-314)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 18,
          "title": "Segment 18 (pages 150-158)",
          "relevance_score": 0.56,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 28,
          "title": "Segment 28 (pages 290-298)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 842-862)",
      "start_page": 842,
      "end_page": 862,
      "detection_method": "synthetic",
      "chapter_number": 42,
      "summary": "This chapter covers segment 42 (pages 842-862). Key topics include asyncio, python, and asynchronous. Thanks to Yury Selivanov for yet another excellent\ncontribution to asynchronous Python.",
      "keywords": [
        "asynchronous",
        "Asynchronous Generator",
        "Asynchronous Programming",
        "async",
        "Python",
        "probe asynchronous generator",
        "asynchronous generator expression",
        "Asynchronous Generator Functions",
        "Asynchronous Programming await",
        "asyncio",
        "Generator",
        "await",
        "domain",
        "asynchronous generator object",
        "probe"
      ],
      "concepts": [
        "asyncio",
        "python",
        "asynchronous",
        "functions",
        "function",
        "functionality",
        "await",
        "async",
        "library",
        "libraries"
      ],
      "similar_chapters": [
        {
          "book": "Python Distilled",
          "chapter": 18,
          "title": "Segment 18 (pages 150-158)",
          "relevance_score": 0.81,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 29,
          "title": "Segment 29 (pages 299-314)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 37,
          "title": "Segment 37 (pages 312-320)",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Microservices Development",
          "chapter": 6,
          "title": "Segment 6 (pages 41-48)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 28,
          "title": "Segment 28 (pages 290-298)",
          "relevance_score": 0.59,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 863-885)",
      "start_page": 863,
      "end_page": 885,
      "detection_method": "synthetic",
      "chapter_number": 43,
      "summary": "This chapter cov‐\ners the simplest ways: the @property decorator and the __getattr__ special method Key topics include record, attributes, and data. 2 Bertrand Meyer, Object-Oriented Software Construction, 2nd ed.",
      "keywords": [
        "Record",
        "Dynamic Attributes",
        "attribute",
        "Record class",
        "data",
        "Python",
        "Event",
        "event record",
        "Record instance attributes",
        "key",
        "instance attribute",
        "Properties",
        "instance",
        "Dynamic",
        "property"
      ],
      "concepts": [
        "record",
        "attributes",
        "data",
        "examples",
        "python",
        "pythonic",
        "key",
        "keys",
        "speakers",
        "methods"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 21,
          "title": "Segment 21 (pages 209-217)",
          "relevance_score": 0.57,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 22,
          "title": "Segment 22 (pages 218-231)",
          "relevance_score": 0.54,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 20,
          "title": "Segment 20 (pages 198-208)",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 23,
          "title": "Segment 23 (pages 232-240)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.49,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 886-908)",
      "start_page": 886,
      "end_page": 908,
      "detection_method": "synthetic",
      "chapter_number": 44,
      "summary": "This chapter covers segment 44 (pages 886-908). Key topics include property, attribute, and classes. Hettinger is a major contributor to the official Python docs and standard\nlibrary.",
      "keywords": [
        "attribute",
        "property",
        "instance attribute",
        "Dynamic Attributes",
        "Attribute Handling",
        "Properties",
        "class attribute",
        "instance",
        "class data attribute",
        "property factory",
        "data attribute",
        "weight",
        "Special Methods",
        "Python",
        "instance data attribute"
      ],
      "concepts": [
        "property",
        "attribute",
        "classes",
        "examples",
        "method",
        "value",
        "weight",
        "object",
        "instance",
        "chapters"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.67,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 22,
          "title": "Segment 22 (pages 218-231)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 909-929)",
      "start_page": 909,
      "end_page": 929,
      "detection_method": "synthetic",
      "chapter_number": 45,
      "summary": "CHAPTER 23\nAttribute Descriptors\nLearning about descriptors not only provides access to a larger toolset, it creates a\ndeeper understanding of how Python works and an appreciation for the elegance of its\ndesign Key topics include descriptor, classes, and examples.",
      "keywords": [
        "Descriptor",
        "Descriptor instance",
        "Attribute",
        "Managed class",
        "Attribute Descriptors",
        "Quantity descriptor instance",
        "instance",
        "Quantity descriptor class",
        "managed attribute",
        "Quantity descriptor",
        "Managed",
        "Quantity",
        "instance attribute",
        "class attribute",
        "descriptor class"
      ],
      "concepts": [
        "descriptor",
        "classes",
        "examples",
        "instance",
        "methods",
        "attribute",
        "python",
        "text",
        "sets",
        "setting"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 22,
          "title": "Segment 22 (pages 218-231)",
          "relevance_score": 0.55,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 25,
          "title": "Segment 25 (pages 219-229)",
          "relevance_score": 0.52,
          "method": "sentence_transformers"
        },
        {
          "book": "Learning Python Ed6",
          "chapter": 8,
          "title": "Lists and Dictionaries",
          "relevance_score": 0.51,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 20,
          "title": "Segment 20 (pages 198-208)",
          "relevance_score": 0.5,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 930-951)",
      "start_page": 930,
      "end_page": 951,
      "detection_method": "synthetic",
      "chapter_number": 46,
      "summary": "This chapter covers segment 46 (pages 930-951). Key topics include classes, type, and typing. Covers method. Text.reverse operates as a function, even working with objects that are not\ninstances of Text.",
      "keywords": [
        "Type",
        "Python",
        "descriptor class",
        "attribute",
        "method",
        "Class Metaprogramming",
        "class method",
        "descriptor",
        "Class Factory Function",
        "Class Factory",
        "Checked class",
        "init",
        "Field descriptor class",
        "Typical Class Method"
      ],
      "concepts": [
        "classes",
        "type",
        "typing",
        "descriptor",
        "method",
        "attributes",
        "python",
        "examples",
        "named",
        "names"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 3,
          "title": "Segment 3 (pages 17-26)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.63,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 952-973)",
      "start_page": 952,
      "end_page": 973,
      "detection_method": "synthetic",
      "chapter_number": 47,
      "summary": "This chapter covers segment 47 (pages 952-973). Key topics include classes, python, and instances. Enhancing Classes with a Class Decorator\nA class decorator is a callable that behaves similarly to a function decorator: it gets\nthe decorated class as an argument, and should return a class to replace the decorated\nclass.",
      "keywords": [
        "Klass instance",
        "class decorator",
        "Klass",
        "instance",
        "descriptor instance",
        "Class Metaprogramming",
        "Python",
        "init",
        "metaclass",
        "decorated class",
        "descriptor",
        "module",
        "class Klass",
        "deco class Klass",
        "class method"
      ],
      "concepts": [
        "classes",
        "python",
        "instances",
        "imports",
        "important",
        "examples",
        "type",
        "attribute",
        "printed",
        "methods"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.78,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 9,
          "title": "Metaprogramming",
          "relevance_score": 0.69,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 8,
          "title": "Classes and Objects",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 16,
          "title": "Segment 16 (pages 133-141)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 24,
          "title": "Segment 24 (pages 208-218)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 974-992)",
      "start_page": 974,
      "end_page": 992,
      "detection_method": "synthetic",
      "chapter_number": 48,
      "summary": "This chapter covers segment 48 (pages 974-992). Key topics include python, pythonic, and code. UML class diagram annotated with MGN: the CheckedMeta meta-mill\nbuilds the Movie mill.",
      "keywords": [
        "Python",
        "Class Metaprogramming",
        "Checked base class",
        "metaclass",
        "metaclasses",
        "Python standard library",
        "Class decorators",
        "code",
        "Python standard",
        "Python community",
        "type",
        "base class",
        "Field",
        "checked class decorator",
        "checked class"
      ],
      "concepts": [
        "python",
        "pythonic",
        "code",
        "type",
        "typing",
        "pep",
        "peps",
        "implemented",
        "implement",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 24,
          "title": "Segment 24 (pages 241-248)",
          "relevance_score": 0.71,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.66,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Cookbook 3rd",
          "chapter": 9,
          "title": "Metaprogramming",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 40,
          "title": "Segment 40 (pages 420-428)",
          "relevance_score": 0.62,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective-Python",
          "chapter": 2,
          "title": "Segment 2 (pages 10-18)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 993-1010)",
      "start_page": 993,
      "end_page": 1010,
      "detection_method": "synthetic",
      "chapter_number": 49,
      "summary": "This chapter covers segment 49 (pages 993-1010). Key topics include function, functions, and functional. Covers function.",
      "keywords": [
        "Soapbox discussion",
        "topics covered",
        "function",
        "type",
        "reading",
        "Soapbox",
        "discussion",
        "functions",
        "versus",
        "operator",
        "Unicode text versus",
        "overview",
        "Python",
        "classes",
        "covered"
      ],
      "concepts": [
        "function",
        "functions",
        "functional",
        "typing",
        "type",
        "classes",
        "operator",
        "operations",
        "method",
        "objects"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 11,
          "title": "Segment 11 (pages 99-114)",
          "relevance_score": 0.77,
          "method": "sentence_transformers"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 27,
          "title": "Segment 27 (pages 222-231)",
          "relevance_score": 0.76,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.75,
          "method": "sentence_transformers"
        },
        {
          "book": "The Little Book Of Python-Anti-Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.73,
          "method": "sentence_transformers"
        }
      ]
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 1011-1011)",
      "start_page": 1011,
      "end_page": 1011,
      "detection_method": "synthetic",
      "chapter_number": 50,
      "summary": "This chapter covers segment 50 (pages 1011-1011). Key topics include books, virtual, and videos.",
      "keywords": [
        "Learn from experts",
        "Instant Answers",
        "O’Reilly Media",
        "Media",
        "Virtual events",
        "Learn",
        "experts",
        "Live online",
        "O’Reilly",
        "Interactive learning",
        "Live",
        "Answers",
        "Virtual",
        "online courses Instant",
        "Interactive"
      ],
      "concepts": [
        "books",
        "virtual",
        "videos",
        "instant",
        "learn",
        "live",
        "interactive",
        "experts",
        "online",
        "media"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.68,
          "method": "sentence_transformers"
        },
        {
          "book": "Python Distilled",
          "chapter": 39,
          "title": "Segment 39 (pages 353-355)",
          "relevance_score": 0.64,
          "method": "sentence_transformers"
        },
        {
          "book": "Effective_Modern_C++",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.61,
          "method": "sentence_transformers"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.6,
          "method": "sentence_transformers"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 25-33)",
          "relevance_score": 0.58,
          "method": "sentence_transformers"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Luciano Ramalho\n2nd Edition\nCovers Python 3.10\nFluent \nPython\nClear, Concise, and \nEffective Programming",
      "content_length": 104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "9\n\nO'REILLY*\n\nFluent Python\n\nDon't waste time bending Python to fit patterns you've\nlearned in other languages. Python's simplicity lets you\nbecome productive quickly, but often this means you aren't\nusing everything the language has to offer. With the updated\nedition of this hands-on guide, you'll learn how to write\neffective, modern Python 3 code by leveraging its best ideas.\n\nDiscover and apply idiomatic Python 3 features beyond your\npast experience. Author Luciano Ramalho guides you through\nPython's core language features and libraries and teaches you\nhow to make your code shorter, faster, and more readable.\n\nComplete with major updates throughout, this new edition\nfeatures five parts that work as five short books within the\nbook:\n\n© Data structures: Sequences, dicts, sets, Unicode, and\ndata classes\n\nFunctions as objects: First-class functions, related design\npatterns, and type hints in function declarations\n\nObject-oriented idioms: Composition, inheritance, mixins,\ninterfaces, operator overloading, protocols, and more\nstatic types\n\nControl flow: Context managers, generators, coroutines,\nasync/await, and thread/process pools\n\nMetaprogramming: Properties, attribute descriptors, class\ndecorators, and new class metaprogramming hooks that\nreplace or simplify metaclasses\n\nLuciano Ramalho is a principal consultant at Thoughtworks and a\nPython Software Foundation fellow.\n\n“My ‘go to’ book when\nlooking for detailed\nexplanations and uses\nof a Python feature.\nLuciano’s teaching\nand presentation are\nexcellent. A great book\nfor advanced beginners\nlooking to build their\n\nknowledge.\"\n—Carol Willing\nPython Steering Council member\n(2020-2021)\n\n“This is not the usual dry\ncoding book, but full of\nuseful, tested examples,\nand just enough humor.\nMy colleagues and|\nhave used this amazing,\nwell-written book to\ntake our Python coding\nto the next level.”\n\n—Maria McKinley\nSenior Software Engineer\n\nPROGRAMMING / PYTHON\n\nUS $6999 CAN $8799\nISBN: 978-1-492-05635-5\n\nUNO 0N iv\nwat\n\n78149 2\"056355!\n\nTwitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia",
      "content_length": 2090,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Luciano Ramalho\nFluent Python\nClear, Concise, and \nEffective Programming\nSECOND EDITION\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing",
      "content_length": 167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "978-1-492-05635-5\n[LSI]\nFluent Python\nby Luciano Ramalho\nCopyright © 2022 Luciano Ramalho. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institu‐\ntional sales department: 800-998-9938 or corporate@oreilly.com.\nAcquisitions Editor: Amanda Quinn\nDevelopment Editor: Jeff Bleiel\nProduction Editor: Daniel Elfanbaum\nCopyeditor: Sonia Saruba\nProofreader: Kim Cofer\nIndexer: Judith McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Kate Dullea\nApril 2022:\n Second Edition\nRevision History for the Second Edition\n2022-03-31: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492056355 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Fluent Python, the cover image, and\nrelated trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the author and do not represent the publisher’s views.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Para Marta, com todo 0 meu amor.",
      "content_length": 32,
      "extraction_method": "OCR"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xix\nPart I. \nData Structures\n1. The Python Data Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nWhat’s New in This Chapter                                                                                          4\nA Pythonic Card Deck                                                                                                    5\nHow Special Methods Are Used                                                                                    8\nEmulating Numeric Types                                                                                          9\nString Representation                                                                                                12\nBoolean Value of a Custom Type                                                                            13\nCollection API                                                                                                            14\nOverview of Special Methods                                                                                       15\nWhy len Is Not a Method                                                                                             17\nChapter Summary                                                                                                          18\nFurther Reading                                                                                                             18\n2. An Array of Sequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\nWhat’s New in This Chapter                                                                                        22\nOverview of Built-In Sequences                                                                                  22\nList Comprehensions and Generator Expressions                                                   25\nList Comprehensions and Readability                                                                    25\nListcomps Versus map and filter                                                                             27\nCartesian Products                                                                                                     27\nGenerator Expressions                                                                                              29\nTuples Are Not Just Immutable Lists                                                                         30\nTuples as Records                                                                                                       30\nv",
      "content_length": 2795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "Tuples as Immutable Lists                                                                                        32\nComparing Tuple and List Methods                                                                       34\nUnpacking Sequences and Iterables                                                                            35\nUsing * to Grab Excess Items                                                                                   36\nUnpacking with * in Function Calls and Sequence Literals                                 37\nNested Unpacking                                                                                                      37\nPattern Matching with Sequences                                                                               38\nPattern Matching Sequences in an Interpreter                                                      43\nSlicing                                                                                                                              47\nWhy Slices and Ranges Exclude the Last Item                                                       47\nSlice Objects                                                                                                                48\nMultidimensional Slicing and Ellipsis                                                                     49\nAssigning to Slices                                                                                                      50\nUsing + and * with Sequences                                                                                      50\nBuilding Lists of Lists                                                                                                 51\nAugmented Assignment with Sequences                                                               53\nA += Assignment Puzzler                                                                                         54\nlist.sort Versus the sorted Built-In                                                                              56\nWhen a List Is Not the Answer                                                                                    59\nArrays                                                                                                                           59\nMemory Views                                                                                                           62\nNumPy                                                                                                                         64\nDeques and Other Queues                                                                                        67\nChapter Summary                                                                                                          70\nFurther Reading                                                                                                             71\n3. Dictionaries and Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77\nWhat’s New in This Chapter                                                                                        78\nModern dict Syntax                                                                                                       78\ndict Comprehensions                                                                                                 79\nUnpacking Mappings                                                                                                80\nMerging Mappings with |                                                                                          80\nPattern Matching with Mappings                                                                               81\nStandard API of Mapping Types                                                                                 83\nWhat Is Hashable                                                                                                       84\nOverview of Common Mapping Methods                                                             85\nInserting or Updating Mutable Values                                                                   87\nAutomatic Handling of Missing Keys                                                                        90\ndefaultdict: Another Take on Missing Keys                                                           90\nThe __missing__ Method                                                                                         91\nInconsistent Usage of __missing__ in the Standard Library                               94\nVariations of dict                                                                                                           95\nvi \n| \nTable of Contents",
      "content_length": 4759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "collections.OrderedDict                                                                                            95\ncollections.ChainMap                                                                                                95\ncollections.Counter                                                                                                    96\nshelve.Shelf                                                                                                                  97\nSubclassing UserDict Instead of dict                                                                       97\nImmutable Mappings                                                                                                    99\nDictionary Views                                                                                                         101\nPractical Consequences of How dict Works                                                            102\nSet Theory                                                                                                                     103\nSet Literals                                                                                                                 105\nSet Comprehensions                                                                                                106\nPractical Consequences of How Sets Work                                                             107\nSet Operations                                                                                                          107\nSet Operations on dict Views                                                                                     110\nChapter Summary                                                                                                       112\nFurther Reading                                                                                                           113\n4. Unicode Text Versus Bytes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  117\nWhat’s New in This Chapter                                                                                      118\nCharacter Issues                                                                                                           118\nByte Essentials                                                                                                              120\nBasic Encoders/Decoders                                                                                           123\nUnderstanding Encode/Decode Problems                                                              125\nCoping with UnicodeEncodeError                                                                       125\nCoping with UnicodeDecodeError                                                                       126\nSyntaxError When Loading Modules with Unexpected Encoding                  128\nHow to Discover the Encoding of a Byte Sequence                                            128\nBOM: A Useful Gremlin                                                                                         129\nHandling Text Files                                                                                                     131\nBeware of Encoding Defaults                                                                                 134\nNormalizing Unicode for Reliable Comparisons                                                   140\nCase Folding                                                                                                             142\nUtility Functions for Normalized Text Matching                                               143\nExtreme “Normalization”: Taking Out Diacritics                                               144\nSorting Unicode Text                                                                                                  148\nSorting with the Unicode Collation Algorithm                                                   150\nThe Unicode Database                                                                                                150\nFinding Characters by Name                                                                                 151\nNumeric Meaning of Characters                                                                           153\nDual-Mode str and bytes APIs                                                                                  155\nstr Versus bytes in Regular Expressions                                                               155\nstr Versus bytes in os Functions                                                                            156\nTable of Contents \n| \nvii",
      "content_length": 4711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Chapter Summary                                                                                                       157\nFurther Reading                                                                                                           158\n5. Data Class Builders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  163\nWhat’s New in This Chapter                                                                                      164\nOverview of Data Class Builders                                                                               164\nMain Features                                                                                                           167\nClassic Named Tuples                                                                                                 169\nTyped Named Tuples                                                                                                  172\nType Hints 101                                                                                                             173\nNo Runtime Effect                                                                                                   173\nVariable Annotation Syntax                                                                                   174\nThe Meaning of Variable Annotations                                                                 175\nMore About @dataclass                                                                                              179\nField Options                                                                                                            180\nPost-init Processing                                                                                                 183\nTyped Class Attributes                                                                                            185\nInitialization Variables That Are Not Fields                                                        186\n@dataclass Example: Dublin Core Resource Record                                          187\nData Class as a Code Smell                                                                                         190\nData Class as Scaffolding                                                                                        191\nData Class as Intermediate Representation                                                          191\nPattern Matching Class Instances                                                                             192\nSimple Class Patterns                                                                                              192\nKeyword Class Patterns                                                                                          193\nPositional Class Patterns                                                                                         194\nChapter Summary                                                                                                       195\nFurther Reading                                                                                                           196\n6. Object References, Mutability, and Recycling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  201\nWhat’s New in This Chapter                                                                                      202\nVariables Are Not Boxes                                                                                             202\nIdentity, Equality, and Aliases                                                                                   204\nChoosing Between == and is                                                                                  206\nThe Relative Immutability of Tuples                                                                    207\nCopies Are Shallow by Default                                                                                  208\nDeep and Shallow Copies of Arbitrary Objects                                                   211\nFunction Parameters as References                                                                          213\nMutable Types as Parameter Defaults: Bad Idea                                                 214\nDefensive Programming with Mutable Parameters                                            216\ndel and Garbage Collection                                                                                        219\nTricks Python Plays with Immutables                                                                      221\nviii \n| \nTable of Contents",
      "content_length": 4637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Chapter Summary                                                                                                       223\nFurther Reading                                                                                                           224\nPart II. \nFunctions as Objects\n7. Functions as First-Class Objects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  231\nWhat’s New in This Chapter                                                                                      232\nTreating a Function Like an Object                                                                          232\nHigher-Order Functions                                                                                            234\nModern Replacements for map, filter, and reduce                                             235\nAnonymous Functions                                                                                               236\nThe Nine Flavors of Callable Objects                                                                       237\nUser-Defined Callable Types                                                                                     239\nFrom Positional to Keyword-Only Parameters                                                      240\nPositional-Only Parameters                                                                                   242\nPackages for Functional Programming                                                                    243\nThe operator Module                                                                                              243\nFreezing Arguments with functools.partial                                                         247\nChapter Summary                                                                                                       249\nFurther Reading                                                                                                           250\n8. Type Hints in Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  253\nWhat’s New in This Chapter                                                                                      254\nAbout Gradual Typing                                                                                                254\nGradual Typing in Practice                                                                                        255\nStarting with Mypy                                                                                                  256\nMaking Mypy More Strict                                                                                      257\nA Default Parameter Value                                                                                     258\nUsing None as a Default                                                                                         260\nTypes Are Defined by Supported Operations                                                         260\nTypes Usable in Annotations                                                                                     266\nThe Any Type                                                                                                           266\nSimple Types and Classes                                                                                       269\nOptional and Union Types                                                                                     270\nGeneric Collections                                                                                                 271\nTuple Types                                                                                                               274\nGeneric Mappings                                                                                                    276\nAbstract Base Classes                                                                                               278\nIterable                                                                                                                       280\nParameterized Generics and TypeVar                                                                  282\nStatic Protocols                                                                                                         286\nTable of Contents \n| \nix",
      "content_length": 4372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Callable                                                                                                                      291\nNoReturn                                                                                                                   294\nAnnotating Positional Only and Variadic Parameters                                          295\nImperfect Typing and Strong Testing                                                                      296\nChapter Summary                                                                                                       297\nFurther Reading                                                                                                           298\n9. Decorators and Closures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  303\nWhat’s New in This Chapter                                                                                      304\nDecorators 101                                                                                                             304\nWhen Python Executes Decorators                                                                          306\nRegistration Decorators                                                                                              308\nVariable Scope Rules                                                                                                   308\nClosures                                                                                                                         311\nThe nonlocal Declaration                                                                                           315\nVariable Lookup Logic                                                                                            316\nImplementing a Simple Decorator                                                                            317\nHow It Works                                                                                                           318\nDecorators in the Standard Library                                                                          320\nMemoization with functools.cache                                                                       320\nUsing lru_cache                                                                                                        323\nSingle Dispatch Generic Functions                                                                       324\nParameterized Decorators                                                                                          329\nA Parameterized Registration Decorator                                                             329\nThe Parameterized Clock Decorator                                                                     332\nA Class-Based Clock Decorator                                                                             335\nChapter Summary                                                                                                       336\nFurther Reading                                                                                                           336\n10. Design Patterns with First-Class Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  341\nWhat’s New in This Chapter                                                                                      342\nCase Study: Refactoring Strategy                                                                               342\nClassic Strategy                                                                                                         342\nFunction-Oriented Strategy                                                                                   347\nChoosing the Best Strategy: Simple Approach                                                    350\nFinding Strategies in a Module                                                                              351\nDecorator-Enhanced Strategy Pattern                                                                     353\nThe Command Pattern                                                                                               355\nChapter Summary                                                                                                       357\nFurther Reading                                                                                                           358\nx \n| \nTable of Contents",
      "content_length": 4475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Part III. \nClasses and Protocols\n11. A Pythonic Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  363\nWhat’s New in This Chapter                                                                                      364\nObject Representations                                                                                               364\nVector Class Redux                                                                                                     365\nAn Alternative Constructor                                                                                       368\nclassmethod Versus staticmethod                                                                             369\nFormatted Displays                                                                                                     370\nA Hashable Vector2d                                                                                                  374\nSupporting Positional Pattern Matching                                                                 377\nComplete Listing of Vector2d, Version 3                                                                378\nPrivate and “Protected” Attributes in Python                                                         382\nSaving Memory with __slots__                                                                                 384\nSimple Measure of __slot__ Savings                                                                     387\nSummarizing the Issues with __slots__                                                                388\nOverriding Class Attributes                                                                                       389\nChapter Summary                                                                                                       391\nFurther Reading                                                                                                           392\n12. Special Methods for Sequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  397\nWhat’s New in This Chapter                                                                                      398\nVector: A User-Defined Sequence Type                                                                  398\nVector Take #1: Vector2d Compatible                                                                     399\nProtocols and Duck Typing                                                                                       402\nVector Take #2: A Sliceable Sequence                                                                      403\nHow Slicing Works                                                                                                  404\nA Slice-Aware __getitem__                                                                                    406\nVector Take #3: Dynamic Attribute Access                                                             407\nVector Take #4: Hashing and a Faster ==                                                                411\nVector Take #5: Formatting                                                                                       418\nChapter Summary                                                                                                       425\nFurther Reading                                                                                                           426\n13. Interfaces, Protocols, and ABCs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  431\nThe Typing Map                                                                                                          432\nWhat’s New in This Chapter                                                                                      433\nTwo Kinds of Protocols                                                                                              434\nProgramming Ducks                                                                                                   435\nPython Digs Sequences                                                                                           436\nMonkey Patching: Implementing a Protocol at Runtime                                  438\nDefensive Programming and “Fail Fast”                                                              440\nTable of Contents \n| \nxi",
      "content_length": 4449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "Goose Typing                                                                                                               442\nSubclassing an ABC                                                                                                 447\nABCs in the Standard Library                                                                                449\nDefining and Using an ABC                                                                                   451\nABC Syntax Details                                                                                                  457\nSubclassing an ABC                                                                                                 458\nA Virtual Subclass of an ABC                                                                                460\nUsage of register in Practice                                                                                   463\nStructural Typing with ABCs                                                                                 464\nStatic Protocols                                                                                                            466\nThe Typed double Function                                                                                   466\nRuntime Checkable Static Protocols                                                                     468\nLimitations of Runtime Protocol Checks                                                             471\nSupporting a Static Protocol                                                                                   472\nDesigning a Static Protocol                                                                                     474\nBest Practices for Protocol Design                                                                        476\nExtending a Protocol                                                                                               477\nThe numbers ABCs and Numeric Protocols                                                       478\nChapter Summary                                                                                                       481\nFurther Reading                                                                                                           482\n14. Inheritance: For Better or for Worse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  487\nWhat’s New in This Chapter                                                                                      488\nThe super() Function                                                                                                  488\nSubclassing Built-In Types Is Tricky                                                                        490\nMultiple Inheritance and Method Resolution Order                                             494\nMixin Classes                                                                                                                500\nCase-Insensitive Mappings                                                                                     500\nMultiple Inheritance in the Real World                                                                   502\nABCs Are Mixins Too                                                                                             502\nThreadingMixIn and ForkingMixIn                                                                     503\nDjango Generic Views Mixins                                                                               504\nMultiple Inheritance in Tkinter                                                                             507\nCoping with Inheritance                                                                                             510\nFavor Object Composition over Class Inheritance                                             510\nUnderstand Why Inheritance Is Used in Each Case                                           510\nMake Interfaces Explicit with ABCs                                                                     511\nUse Explicit Mixins for Code Reuse                                                                      511\nProvide Aggregate Classes to Users                                                                      511\nSubclass Only Classes Designed for Subclassing                                                 512\nAvoid Subclassing from Concrete Classes                                                           513\nTkinter: The Good, the Bad, and the Ugly                                                           513\nxii \n| \nTable of Contents",
      "content_length": 4634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Chapter Summary                                                                                                       514\nFurther Reading                                                                                                           515\n15. More About Type Hints. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  519\nWhat’s New in This Chapter                                                                                      519\nOverloaded Signatures                                                                                                520\nMax Overload                                                                                                           521\nTakeaways from Overloading max                                                                        525\nTypedDict                                                                                                                     526\nType Casting                                                                                                                 534\nReading Type Hints at Runtime                                                                                537\nProblems with Annotations at Runtime                                                               538\nDealing with the Problem                                                                                       540\nImplementing a Generic Class                                                                                   541\nBasic Jargon for Generic Types                                                                              544\nVariance                                                                                                                        544\nAn Invariant Dispenser                                                                                           545\nA Covariant Dispenser                                                                                            546\nA Contravariant Trash Can                                                                                    547\nVariance Review                                                                                                       549\nImplementing a Generic Static Protocol                                                                  552\nChapter Summary                                                                                                       554\nFurther Reading                                                                                                           555\n16. Operator Overloading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  561\nWhat’s New in This Chapter                                                                                      562\nOperator Overloading 101                                                                                         562\nUnary Operators                                                                                                          563\nOverloading + for Vector Addition                                                                          566\nOverloading * for Scalar Multiplication                                                                   572\nUsing @ as an Infix Operator                                                                                     574\nWrapping-Up Arithmetic Operators                                                                        576\nRich Comparison Operators                                                                                      577\nAugmented Assignment Operators                                                                          580\nChapter Summary                                                                                                       585\nFurther Reading                                                                                                           587\nPart IV. \nControl Flow\n17. Iterators, Generators, and Classic Coroutines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  593\nWhat’s New in This Chapter                                                                                      594\nTable of Contents \n| \nxiii",
      "content_length": 4317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "A Sequence of Words                                                                                                  594\nWhy Sequences Are Iterable: The iter Function                                                     596\nUsing iter with a Callable                                                                                        598\nIterables Versus Iterators                                                                                            599\nSentence Classes with __iter__                                                                                  603\nSentence Take #2: A Classic Iterator                                                                     603\nDon’t Make the Iterable an Iterator for Itself                                                      605\nSentence Take #3: A Generator Function                                                            606\nHow a Generator Works                                                                                         607\nLazy Sentences                                                                                                              610\nSentence Take #4: Lazy Generator                                                                        610\nSentence Take #5: Lazy Generator Expression                                                    611\nWhen to Use Generator Expressions                                                                        613\nAn Arithmetic Progression Generator                                                                     615\nArithmetic Progression with itertools                                                                  618\nGenerator Functions in the Standard Library                                                         619\nIterable Reducing Functions                                                                                      630\nSubgenerators with yield from                                                                                   632\nReinventing chain                                                                                                    633\nTraversing a Tree                                                                                                     634\nGeneric Iterable Types                                                                                                639\nClassic Coroutines                                                                                                       641\nExample: Coroutine to Compute a Running Average                                        643\nReturning a Value from a Coroutine                                                                    646\nGeneric Type Hints for Classic Coroutines                                                         650\nChapter Summary                                                                                                       652\nFurther Reading                                                                                                           652\n18. with, match, and else Blocks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  657\nWhat’s New in This Chapter                                                                                      658\nContext Managers and with Blocks                                                                          658\nThe contextlib Utilities                                                                                            663\nUsing @contextmanager                                                                                         664\nPattern Matching in lis.py: A Case Study                                                                 669\nScheme Syntax                                                                                                          669\nImports and Types                                                                                                   671\nThe Parser                                                                                                                 671\nThe Environment                                                                                                     673\nThe REPL                                                                                                                  675\nThe Evaluator                                                                                                           676\nProcedure: A Class Implementing a Closure                                                       685\nUsing OR-patterns                                                                                                   686\nxiv \n| \nTable of Contents",
      "content_length": 4726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "Do This, Then That: else Blocks Beyond if                                                              687\nChapter Summary                                                                                                       689\nFurther Reading                                                                                                           690\n19. Concurrency Models in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  695\nWhat’s New in This Chapter                                                                                      696\nThe Big Picture                                                                                                             696\nA Bit of Jargon                                                                                                              697\nProcesses, Threads, and Python’s Infamous GIL                                                699\nA Concurrent Hello World                                                                                        701\nSpinner with Threads                                                                                              701\nSpinner with Processes                                                                                            704\nSpinner with Coroutines                                                                                         706\nSupervisors Side-by-Side                                                                                         711\nThe Real Impact of the GIL                                                                                        713\nQuick Quiz                                                                                                                713\nA Homegrown Process Pool                                                                                      716\nProcess-Based Solution                                                                                           718\nUnderstanding the Elapsed Times                                                                        718\nCode for the Multicore Prime Checker                                                                719\nExperimenting with More or Fewer Processes                                                    723\nThread-Based Nonsolution                                                                                    724\nPython in the Multicore World                                                                                 725\nSystem Administration                                                                                           726\nData Science                                                                                                              727\nServer-Side Web/Mobile Development                                                                728\nWSGI Application Servers                                                                                      730\nDistributed Task Queues                                                                                        732\nChapter Summary                                                                                                       733\nFurther Reading                                                                                                           734\nConcurrency with Threads and Processes                                                           734\nThe GIL                                                                                                                     736\nConcurrency Beyond the Standard Library                                                         736\nConcurrency and Scalability Beyond Python                                                      738\n20. Concurrent Executors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  743\nWhat’s New in This Chapter                                                                                      743\nConcurrent Web Downloads                                                                                     744\nA Sequential Download Script                                                                               746\nDownloading with concurrent.futures                                                                 749\nWhere Are the Futures?                                                                                          751\nLaunching Processes with concurrent.futures                                                        754\nTable of Contents \n| \nxv",
      "content_length": 4618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Multicore Prime Checker Redux                                                                           755\nExperimenting with Executor.map                                                                           758\nDownloads with Progress Display and Error Handling                                        762\nError Handling in the flags2 Examples                                                                 766\nUsing futures.as_completed                                                                                   769\nChapter Summary                                                                                                       772\nFurther Reading                                                                                                           772\n21. Asynchronous Programming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  775\nWhat’s New in This Chapter                                                                                      776\nA Few Definitions                                                                                                        777\nAn asyncio Example: Probing Domains                                                                  778\nGuido’s Trick to Read Asynchronous Code                                                        780\nNew Concept: Awaitable                                                                                            781\nDownloading with asyncio and HTTPX                                                                  782\nThe Secret of Native Coroutines: Humble Generators                                       784\nThe All-or-Nothing Problem                                                                                 785\nAsynchronous Context Managers                                                                             786\nEnhancing the asyncio Downloader                                                                         787\nUsing asyncio.as_completed and a Thread                                                          788\nThrottling Requests with a Semaphore                                                                 790\nMaking Multiple Requests for Each Download                                                   794\nDelegating Tasks to Executors                                                                                   797\nWriting asyncio Servers                                                                                              799\nA FastAPI Web Service                                                                                           800\nAn asyncio TCP Server                                                                                           804\nAsynchronous Iteration and Asynchronous Iterables                                           811\nAsynchronous Generator Functions                                                                     812\nAsync Comprehensions and Async Generator Expressions                             818\nasync Beyond asyncio: Curio                                                                                     821\nType Hinting Asynchronous Objects                                                                       824\nHow Async Works and How It Doesn’t                                                                   825\nRunning Circles Around Blocking Calls                                                              825\nThe Myth of I/O-Bound Systems                                                                          826\nAvoiding CPU-Bound Traps                                                                                  826\nChapter Summary                                                                                                       827\nFurther Reading                                                                                                           828\nxvi \n| \nTable of Contents",
      "content_length": 3976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "Part V. \nMetaprogramming\n22. Dynamic Attributes and Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  835\nWhat’s New in This Chapter                                                                                      836\nData Wrangling with Dynamic Attributes                                                              836\nExploring JSON-Like Data with Dynamic Attributes                                        838\nThe Invalid Attribute Name Problem                                                                   842\nFlexible Object Creation with __new__                                                               843\nComputed Properties                                                                                                  845\nStep 1: Data-Driven Attribute Creation                                                               846\nStep 2: Property to Retrieve a Linked Record                                                      848\nStep 3: Property Overriding an Existing Attribute                                             852\nStep 4: Bespoke Property Cache                                                                            853\nStep 5: Caching Properties with functools                                                           855\nUsing a Property for Attribute Validation                                                               857\nLineItem Take #1: Class for an Item in an Order                                                857\nLineItem Take #2: A Validating Property                                                            858\nA Proper Look at Properties                                                                                      860\nProperties Override Instance Attributes                                                              861\nProperty Documentation                                                                                        864\nCoding a Property Factory                                                                                         865\nHandling Attribute Deletion                                                                                      868\nEssential Attributes and Functions for Attribute Handling                                  869\nSpecial Attributes that Affect Attribute Handling                                              870\nBuilt-In Functions for Attribute Handling                                                          870\nSpecial Methods for Attribute Handling                                                              871\nChapter Summary                                                                                                       873\nFurther Reading                                                                                                           873\n23. Attribute Descriptors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  879\nWhat’s New in This Chapter                                                                                      880\nDescriptor Example: Attribute Validation                                                               880\nLineItem Take #3: A Simple Descriptor                                                               880\nLineItem Take #4: Automatic Naming of Storage Attributes                           887\nLineItem Take #5: A New Descriptor Type                                                         889\nOverriding Versus Nonoverriding Descriptors                                                      892\nOverriding Descriptors                                                                                           894\nOverriding Descriptor Without __get__                                                              895\nNonoverriding Descriptor                                                                                      896\nOverwriting a Descriptor in the Class                                                                   897\nMethods Are Descriptors                                                                                           898\nDescriptor Usage Tips                                                                                                 900\nTable of Contents \n| \nxvii",
      "content_length": 4299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "Descriptor Docstring and Overriding Deletion                                                      902\nChapter Summary                                                                                                       903\nFurther Reading                                                                                                           904\n24. Class Metaprogramming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  907\nWhat’s New in This Chapter                                                                                      908\nClasses as Objects                                                                                                        908\ntype: The Built-In Class Factory                                                                                909\nA Class Factory Function                                                                                           911\nIntroducing __init_subclass__                                                                                  914\nWhy __init_subclass__ Cannot Configure __slots__                                        921\nEnhancing Classes with a Class Decorator                                                              922\nWhat Happens When: Import Time Versus Runtime                                           925\nEvaluation Time Experiments                                                                                926\nMetaclasses 101                                                                                                            931\nHow a Metaclass Customizes a Class                                                                    933\nA Nice Metaclass Example                                                                                      934\nMetaclass Evaluation Time Experiment                                                               937\nA Metaclass Solution for Checked                                                                            942\nMetaclasses in the Real World                                                                                   947\nModern Features Simplify or Replace Metaclasses                                             947\nMetaclasses Are Stable Language Features                                                           948\nA Class Can Only Have One Metaclass                                                                948\nMetaclasses Should Be Implementation Details                                                  949\nA Metaclass Hack with __prepare__                                                                        950\nWrapping Up                                                                                                               952\nChapter Summary                                                                                                       953\nFurther Reading                                                                                                           954\nAfterword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  959\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  963\nxviii \n| \nTable of Contents",
      "content_length": 3361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "1 Message to the comp.lang.python Usenet group, Dec. 23, 2002: “Acrimony in c.l.p”.\nPreface\nHere’s the plan: when someone uses a feature you don’t understand, simply shoot\nthem. This is easier than learning something new, and before too long the only living\ncoders will be writing in an easily understood, tiny subset of Python 0.9.6 <wink>.1\n—Tim Peters, legendary core developer and author of The Zen of Python\n“Python is an easy to learn, powerful programming language.” Those are the first\nwords of the official Python 3.10 tutorial. That is true, but there is a catch: because\nthe language is easy to learn and put to use, many practicing Python programmers\nleverage only a fraction of its powerful features.\nAn experienced programmer may start writing useful Python code in a matter of\nhours. As the first productive hours become weeks and months, a lot of developers go\non writing Python code with a very strong accent carried from languages learned\nbefore. Even if Python is your first language, often in academia and in introductory\nbooks it is presented while carefully avoiding language-specific features.\nAs a teacher introducing Python to programmers experienced in other languages, I\nsee another problem that this book tries to address: we only miss stuff we know\nabout. Coming from another language, anyone may guess that Python supports regu‐\nlar expressions, and look that up in the docs. But if you’ve never seen tuple unpacking\nor descriptors before, you will probably not search for them, and you may end up not\nusing those features just because they are specific to Python.\nThis book is not an A-to-Z exhaustive reference of Python. Its emphasis is on the lan‐\nguage features that are either unique to Python or not found in many other popular\nlanguages. This is also mostly a book about the core language and some of its libra‐\nries. I will rarely talk about packages that are not in the standard library, even though\nthe Python package index now lists more than 60,000 libraries, and many of them are\nincredibly useful.\nxix",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "Who This Book Is For\nThis book was written for practicing Python programmers who want to become pro‐\nficient in Python 3. I tested the examples in Python 3.10—most of them also in\nPython 3.9 and 3.8. When an example requires Python 3.10, it should be clearly\nmarked.\nIf you are not sure whether you know enough Python to follow along, review the top‐\nics of the official Python tutorial. Topics covered in the tutorial will not be explained\nhere, except for some features that are new.\nWho This Book Is Not For\nIf you are just learning Python, this book is going to be hard to follow. Not only that,\nif you read it too early in your Python journey, it may give you the impression that\nevery Python script should leverage special methods and metaprogramming tricks.\nPremature abstraction is as bad as premature optimization.\nFive Books in One\nI recommend that everyone read Chapter 1, “The Python Data Model”. The core\naudience for this book should not have trouble jumping directly to any part in this\nbook after Chapter 1, but often I assume you’ve read preceding chapters in each spe‐\ncific part. Think of Parts I through V as books within the book.\nI tried to emphasize using what is available before discussing how to build your own.\nFor example, in Part I, Chapter 2 covers sequence types that are ready to use, includ‐\ning some that don’t get a lot of attention, like collections.deque. Building user-\ndefined sequences is only addressed in Part III, where we also see how to leverage the\nabstract base classes (ABCs) from collections.abc. Creating your own ABCs is dis‐\ncussed even later in Part III, because I believe it’s important to be comfortable using\nan ABC before writing your own.\nThis approach has a few advantages. First, knowing what is ready to use can save you\nfrom reinventing the wheel. We use existing collection classes more often than we\nimplement our own, and we can give more attention to the advanced usage of avail‐\nable tools by deferring the discussion on how to create new ones. We are also more\nlikely to inherit from existing ABCs than to create a new ABC from scratch. And\nfinally, I believe it is easier to understand the abstractions after you’ve seen them in\naction.\nThe downside of this strategy is the forward references scattered throughout the\nchapters. I hope these will be easier to tolerate now that you know why I chose this\npath.\nxx \n| \nPreface",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "How the Book Is Organized\nHere are the main topics in each part of the book:\nPart I, “Data Structures”\nChapter 1 introduces the Python Data Model and explains why the special meth‐\nods (e.g., __repr__) are the key to the consistent behavior of objects of all types.\nSpecial methods are covered in more detail throughout the book. The remaining\nchapters in this part cover the use of collection types: sequences, mappings, and\nsets, as well as the str versus bytes split—the cause of much celebration among\nPython 3 users and much pain for Python 2 users migrating their codebases. Also\ncovered are the high-level class builders in the standard library: named tuple fac‐\ntories and the @dataclass decorator. Pattern matching—new in Python 3.10—is\ncovered in sections in Chapters 2, 3, and 5, which discuss sequence patterns,\nmapping patterns, and class patterns. The last chapter in Part I is about the life\ncycle of objects: references, mutability, and garbage collection.\nPart II, “Functions as Objects”\nHere we talk about functions as first-class objects in the language: what that\nmeans, how it affects some popular design patterns, and how to implement func‐\ntion decorators by leveraging closures. Also covered here is the general concept\nof callables in Python, function attributes, introspection, parameter annotations,\nand the new nonlocal declaration in Python 3. Chapter 8 introduces the major\nnew topic of type hints in function signatures.\nPart III, “Classes and Protocols”\nNow the focus is on building classes “by hand”—as opposed to using the class\nbuilders covered in Chapter 5. Like any Object-Oriented (OO) language, Python\nhas its particular set of features that may or may not be present in the language in\nwhich you and I learned class-based programming. The chapters explain how\nto build your own collections, abstract base classes (ABCs), and protocols, as well\nas how to cope with multiple inheritance, and how to implement operator\noverloading—when that makes sense. Chapter 15 continues the coverage of\ntype hints.\nPart IV, “Control Flow”\nCovered in this part are the language constructs and libraries that go beyond tra‐\nditional control flow with conditionals, loops, and subroutines. We start with\ngenerators, then visit context managers and coroutines, including the challenging\nbut powerful new yield from syntax. Chapter 18 includes a significant example\nusing pattern matching in a simple but functional language interpreter. Chap‐\nter 19, “Concurrency Models in Python” is a new chapter presenting an overview\nof alternatives for concurrent and parallel processing in Python, their limitations,\nand how software architecture allows Python to operate at web scale. I rewrote\nPreface \n| \nxxi",
      "content_length": 2722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "the chapter about asynchronous programming to emphasize core language fea‐\ntures—e.g., await, async dev, async for, and async with, and show how they\nare used with asyncio and other frameworks.\nPart V, “Metaprogramming”\nThis part starts with a review of techniques for building classes with attributes\ncreated dynamically to handle semi-structured data, such as JSON datasets. Next,\nwe cover the familiar properties mechanism, before diving into how object\nattribute access works at a lower level in Python using descriptors. The relation‐\nship among functions, methods, and descriptors is explained. Throughout\nPart V, the step-by-step implementation of a field validation library uncovers\nsubtle issues that lead to the advanced tools of the final chapter: class decorators\nand metaclasses.\nHands-On Approach\nOften we’ll use the interactive Python console to explore the language and libraries. I\nfeel it is important to emphasize the power of this learning tool, particularly for those\nreaders who’ve had more experience with static, compiled languages that don’t pro‐\nvide a read-eval-print loop (REPL).\nOne of the standard Python testing packages, doctest, works by simulating console\nsessions and verifying that the expressions evaluate to the responses shown. I used\ndoctest to check most of the code in this book, including the console listings. You\ndon’t need to use or even know about doctest to follow along: the key feature of\ndoctests is that they look like transcripts of interactive Python console sessions, so\nyou can easily try out the demonstrations yourself.\nSometimes I will explain what we want to accomplish by showing a doctest before the\ncode that makes it pass. Firmly establishing what is to be done before thinking about\nhow to do it helps focus our coding effort. Writing tests first is the basis of test-driven\ndevelopment (TDD), and I’ve also found it helpful when teaching. If you are unfami‐\nliar with doctest, take a look at its documentation and this book’s example code\nrepository.\nI also wrote unit tests for some of the larger examples using pytest—which I find eas‐\nier to use and more powerful than the unittest module in the standard library. You’ll\nfind that you can verify the correctness of most of the code in the book by typing\npython3 -m doctest example_script.py or pytest in the command shell of your\nOS. The pytest.ini configuration at the root of the example code repository ensures\nthat doctests are collected and executed by the pytest command.\nxxii \n| \nPreface",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "Soapbox: My Personal Perspective\nI have been using, teaching, and debating Python since 1998, and I enjoy studying\nand comparing programming languages, their design, and the theory behind them.\nAt the end of some chapters, I have added “Soapbox” sidebars with my own perspec‐\ntive about Python and other languages. Feel free to skip these if you are not into such\ndiscussions. Their content is completely optional.\nCompanion Website: fluentpython.com\nCovering new features—like type hints, data classes, and pattern matching—made\nthis second edition almost 30% larger than the first. To keep the book luggable,\nI moved some content to fluentpython.com. You will find links to articles I published\nthere in several chapters. Some sample chapters are also in the companion website.\nThe full text is available online at the O’Reilly Learning subscription service. The\nexample code repository is on GitHub.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nNote that when a line break falls within a constant_width term, a hyphen is not\nadded—it could be misunderstood as part of the term.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nPreface \n| \nxxiii",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "This element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nEvery script and most code snippets that appear in the book are available in the Flu‐\nent Python code repository on GitHub at https://fpy.li/code.\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com.\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you’re reproducing a significant portion\nof the code. For example, writing a program that uses several chunks of code from\nthis book does not require permission. Selling or distributing examples from O’Reilly\nbooks does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require\npermission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN, e.g., “Fluent Python, 2nd ed., by\nLuciano Ramalho (O’Reilly). Copyright 2022 Luciano Ramalho, 978-1-492-05635-5.”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nxxiv \n| \nPreface",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "O’Reilly Online Learning\nFor more than 40 years, O’Reilly Media has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at https://fpy.li/p-4.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly.\nFollow us on Twitter: https://twitter.com/oreillymedia.\nWatch us on YouTube: http://www.youtube.com/oreillymedia.\nAcknowledgments\nI did not expect updating a Python book five years later to be such a major undertak‐\ning, but it was. Marta Mello, my beloved wife, was always there when I needed her.\nMy dear friend Leonardo Rochael helped me from the earliest writing to the final\ntechnical review, including consolidating and double-checking the feedback from the\nother tech reviewers, readers, and editors. I honestly don’t know if I’d have made it\nwithout your support, Marta and Leo. Thank you so much!\nPreface \n| \nxxv",
      "content_length": 1834,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Jürgen Gmach, Caleb Hattingh, Jess Males, Leonardo Rochael, and Miroslav Šedivý\nwere the outstanding technical review team for the second edition. They reviewed the\nwhole book. Bill Behrman, Bruce Eckel, Renato Oliveira, and Rodrigo Bernardo\nPimentel reviewed specific chapters. Their many suggestions from different perspec‐\ntives made the book much better.\nMany readers sent corrections or made other contributions during the early release\nphase, including: Guilherme Alves, Christiano Anderson, Konstantin Baikov, K. Alex\nBirch, Michael Boesl, Lucas Brunialti, Sergio Cortez, Gino Crecco, Chukwuerika\nDike, Juan Esteras, Federico Fissore, Will Frey, Tim Gates, Alexander Hagerman,\nChen Hanxiao, Sam Hyeong, Simon Ilincev, Parag Kalra, Tim King, David Kwast,\nTina Lapine, Wanpeng Li, Guto Maia, Scott Martindale, Mark Meyer, Andy McFar‐\nland, Chad McIntire, Diego Rabatone Oliveira, Francesco Piccoli, Meredith Rawls,\nMichael Robinson, Federico Tula Rovaletti, Tushar Sadhwani, Arthur Constantino\nScardua, Randal L. Schwartz, Avichai Sefati, Guannan Shen, William Simpson, Vivek\nVashist, Jerry Zhang, Paul Zuradzki—and others who did not want to be named, sent\ncorrections after I delivered the draft, or are omitted because I failed to record their\nnames—sorry.\nDuring my research, I learned about typing, concurrency, pattern matching, and\nmetaprogramming while interacting with Michael Albert, Pablo Aguilar, Kaleb Bar‐\nrett, David Beazley, J. S. O. Bueno, Bruce Eckel, Martin Fowler, Ivan Levkivskyi, Alex\nMartelli, Peter Norvig, Sebastian Rittau, Guido van Rossum, Carol Willing, and Jelle\nZijlstra.\nO’Reilly editors Jeff Bleiel, Jill Leonard, and Amelia Blevins made suggestions that\nimproved the flow of the book in many places. Jeff Bleiel and production editor\nDanny Elfanbaum supported me throughout this long marathon.\nThe insights and suggestions of every one of them made the book better and more\naccurate. Inevitably, there will still be bugs of my own creation in the final product. I\napologize in advance.\nFinally, I want to extend my heartfelt thanks to my colleagues at Thoughtworks Brazil\n—and especially to my sponsor, Alexey Bôas—who supported this project in many\nways, all the way.\nOf course, everyone who helped me understand Python and write the first edition\nnow deserves double thanks. There would be no second edition without a successful\nfirst.\nxxvi \n| \nPreface",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Acknowledgments for the First Edition\nThe Bauhaus chess set by Josef Hartwig is an example of excellent design: beautiful,\nsimple, and clear. Guido van Rossum, son of an architect and brother of a master font\ndesigner, created a masterpiece of language design. I love teaching Python because it\nis beautiful, simple, and clear.\nAlex Martelli and Anna Ravenscroft were the first people to see the outline of this\nbook and encouraged me to submit it to O’Reilly for publication. Their books taught\nme idiomatic Python and are models of clarity, accuracy, and depth in technical writ‐\ning. Alex’s 6,200+ Stack Overflow posts are a fountain of insights about the language\nand its proper use.\nMartelli and Ravenscroft were also technical reviewers of this book, along with Len‐\nnart Regebro and Leonardo Rochael. Everyone in this outstanding technical review\nteam has at least 15 years of Python experience, with many contributions to high-\nimpact Python projects in close contact with other developers in the community.\nTogether they sent me hundreds of corrections, suggestions, questions, and opinions,\nadding tremendous value to the book. Victor Stinner kindly reviewed Chapter 21,\nbringing his expertise as an asyncio maintainer to the technical review team. It was a\ngreat privilege and a pleasure to collaborate with them over these past several\nmonths.\nEditor Meghan Blanchette was an outstanding mentor, helping me improve the orga‐\nnization and flow of the book, letting me know when it was boring, and keeping me\nfrom delaying even more. Brian MacDonald edited chapters in Part II while Meghan\nwas away. I enjoyed working with them, and with everyone I’ve contacted at O’Reilly,\nincluding the Atlas development and support team (Atlas is the O’Reilly book pub‐\nlishing platform, which I was fortunate to use to write this book).\nMario Domenech Goulart provided numerous, detailed suggestions starting with the\nfirst early release. I also received valuable feedback from Dave Pawson, Elias Dor‐\nneles, Leonardo Alexandre Ferreira Leite, Bruce Eckel, J. S. Bueno, Rafael Gonçalves,\nAlex Chiaranda, Guto Maia, Lucas Vido, and Lucas Brunialti.\nOver the years, a number of people urged me to become an author, but the most per‐\nsuasive were Rubens Prates, Aurelio Jargas, Rudá Moura, and Rubens Altimari.\nMauricio Bussab opened many doors for me, including my first real shot at writing a\nbook. Renzo Nuccitelli supported this writing project all the way, even if that meant a\nslow start for our partnership at python.pro.br.\nThe wonderful Brazilian Python community is knowledgeable, generous, and fun.\nThe Python Brasil group has thousands of people, and our national conferences bring\ntogether hundreds, but the most influential in my journey as a Pythonista were Leo‐\nnardo Rochael, Adriano Petrich, Daniel Vainsencher, Rodrigo RBP Pimentel, Bruno\nGola, Leonardo Santagada, Jean Ferri, Rodrigo Senra, J. S. Bueno, David Kwast, Luiz\nPreface \n| \nxxvii",
      "content_length": 2954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Irber, Osvaldo Santana, Fernando Masanori, Henrique Bastos, Gustavo Niemayer,\nPedro Werneck, Gustavo Barbieri, Lalo Martins, Danilo Bellini, and Pedro Kroger.\nDorneles Tremea was a great friend (incredibly generous with his time and knowl‐\nedge), an amazing hacker, and the most inspiring leader of the Brazilian Python\nAssociation. He left us too early.\nMy students over the years taught me a lot through their questions, insights, feed‐\nback, and creative solutions to problems. Érico Andrei and Simples Consultoria\nmade it possible for me to focus on being a Python teacher for the first time.\nMartijn Faassen was my Grok mentor and shared invaluable insights with me about\nPython and Neanderthals. His work and that of Paul Everitt, Chris McDonough, Tres\nSeaver, Jim Fulton, Shane Hathaway, Lennart Regebro, Alan Runyan, Alexander\nLimi, Martijn Pieters, Godefroid Chapelle, and others from the Zope, Plone, and Pyr‐\namid planets have been decisive in my career. Thanks to Zope and surfing the first\nweb wave, I was able to start making a living with Python in 1998. José Octavio Cas‐\ntro Neves was my partner in the first Python-centric software house in Brazil.\nI have too many gurus in the wider Python community to list them all, but besides\nthose already mentioned, I am indebted to Steve Holden, Raymond Hettinger, A.M.\nKuchling, David Beazley, Fredrik Lundh, Doug Hellmann, Nick Coghlan, Mark Pil‐\ngrim, Martijn Pieters, Bruce Eckel, Michele Simionato, Wesley Chun, Brandon Craig\nRhodes, Philip Guo, Daniel Greenfeld, Audrey Roy, and Brett Slatkin for teaching me\nnew and better ways to teach Python.\nMost of these pages were written in my home office and in two labs: CoffeeLab and\nGaroa Hacker Clube. CoffeeLab is the caffeine-geek headquarters in Vila Madalena,\nSão Paulo, Brazil. Garoa Hacker Clube is a hackerspace open to all: a community lab\nwhere anyone can freely try out new ideas.\nThe Garoa community provided inspiration, infrastructure, and slack. I think Aleph\nwould enjoy this book.\nMy mother, Maria Lucia, and my father, Jairo, always supported me in every way. I\nwish he was here to see the book; I am glad I can share it with her.\nMy wife, Marta Mello, endured 15 months of a husband who was always working,\nbut remained supportive and coached me through some critical moments in the\nproject when I feared I might drop out of the marathon.\nThank you all, for everything.\nxxviii \n| \nPreface",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "PARTI\n\nData Structures",
      "content_length": 22,
      "extraction_method": "OCR"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "1 “Story of Jython”, written as a foreword to Jython Essentials by Samuele Pedroni and Noel Rappin (O’Reilly).\nCHAPTER 1\nThe Python Data Model\nGuido’s sense of the aesthetics of language design is amazing. I’ve met many fine lan‐\nguage designers who could build theoretically beautiful languages that no one would\never use, but Guido is one of those rare people who can build a language that is just\nslightly less theoretically beautiful but thereby is a joy to write programs in.\n—Jim Hugunin, creator of Jython, cocreator of AspectJ, and architect of\nthe .Net DLR1\nOne of the best qualities of Python is its consistency. After working with Python for a\nwhile, you are able to start making informed, correct guesses about features that are\nnew to you.\nHowever, if you learned another object-oriented language before Python, you may\nfind it strange to use len(collection) instead of collection.len(). This apparent\noddity is the tip of an iceberg that, when properly understood, is the key to every‐\nthing we call Pythonic. The iceberg is called the Python Data Model, and it is the API\nthat we use to make our own objects play well with the most idiomatic language\nfeatures.\nYou can think of the data model as a description of Python as a framework. It formal‐\nizes the interfaces of the building blocks of the language itself, such as sequences,\nfunctions, iterators, coroutines, classes, context managers, and so on.\nWhen using a framework, we spend a lot of time coding methods that are called by\nthe framework. The same happens when we leverage the Python Data Model to build\nnew classes. The Python interpreter invokes special methods to perform basic object\noperations, often triggered by special syntax. The special method names are always\nwritten with leading and trailing double underscores. For example, the syntax\n3",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "obj[key] is supported by the __getitem__ special method. In order to evaluate\nmy_collection[key], the interpreter calls my_collection.__getitem__(key).\nWe implement special methods when we want our objects to support and interact\nwith fundamental language constructs such as:\n• Collections\n• Attribute access\n• Iteration (including asynchronous iteration using async for)\n• Operator overloading\n• Function and method invocation\n• String representation and formatting\n• Asynchronous programming using await\n• Object creation and destruction\n• Managed contexts using the with or async with statements\nMagic and Dunder\nThe term magic method is slang for special method, but how do we\ntalk about a specific method like __getitem__? I learned to say\n“dunder-getitem” from author and teacher Steve Holden. “Dun‐\nder” is a shortcut for “double underscore before and after.” That’s\nwhy the special methods are also known as dunder methods. The\n“Lexical Analysis” chapter of The Python Language Reference warns\nthat “Any use of __*__ names, in any context, that does not follow\nexplicitly documented use, is subject to breakage without warning.”\nWhat’s New in This Chapter\nThis chapter had few changes from the first edition because it is an introduction to\nthe Python Data Model, which is quite stable. The most significant changes are:\n• Special methods supporting asynchronous programming and other new features,\nadded to the tables in “Overview of Special Methods” on page 15.\n• Figure 1-2 showing the use of special methods in “Collection API” on page 14,\nincluding the collections.abc.Collection abstract base class introduced in\nPython 3.6.\n4 \n| \nChapter 1: The Python Data Model",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "Also, here and throughout this second edition I adopted the f-string syntax intro‐\nduced in Python 3.6, which is more readable and often more convenient than the\nolder string formatting notations: the str.format() method and the % operator.\nOne reason to still use my_fmt.format() is when the definition of\nmy_fmt must be in a different place in the code than where the for‐\nmatting operation needs to happen. For instance, when my_fmt has\nmultiple lines and is better defined in a constant, or when it must\ncome from a configuration file, or from the database. Those are\nreal needs, but don’t happen very often.\nA Pythonic Card Deck\nExample 1-1 is simple, but it demonstrates the power of implementing just two spe‐\ncial methods, __getitem__ and __len__.\nExample 1-1. A deck as a sequence of playing cards\nimport collections\nCard = collections.namedtuple('Card', ['rank', 'suit'])\nclass FrenchDeck:\n    ranks = [str(n) for n in range(2, 11)] + list('JQKA')\n    suits = 'spades diamonds clubs hearts'.split()\n    def __init__(self):\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\n    def __len__(self):\n        return len(self._cards)\n    def __getitem__(self, position):\n        return self._cards[position]\nThe first thing to note is the use of collections.namedtuple to construct a simple\nclass to represent individual cards. We use namedtuple to build classes of objects that\nare just bundles of attributes with no custom methods, like a database record. In the\nexample, we use it to provide a nice representation for the cards in the deck, as shown\nin the console session:\n>>> beer_card = Card('7', 'diamonds')\n>>> beer_card\nCard(rank='7', suit='diamonds')\nA Pythonic Card Deck \n| \n5",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "But the point of this example is the FrenchDeck class. It’s short, but it packs a punch.\nFirst, like any standard Python collection, a deck responds to the len() function by\nreturning the number of cards in it:\n>>> deck = FrenchDeck()\n>>> len(deck)\n52\nReading specific cards from the deck—say, the first or the last—is easy, thanks to the\n__getitem__ method:\n>>> deck[0]\nCard(rank='2', suit='spades')\n>>> deck[-1]\nCard(rank='A', suit='hearts')\nShould we create a method to pick a random card? No need. Python already has a\nfunction to get a random item from a sequence: random.choice. We can use it on a\ndeck instance:\n>>> from random import choice\n>>> choice(deck)\nCard(rank='3', suit='hearts')\n>>> choice(deck)\nCard(rank='K', suit='spades')\n>>> choice(deck)\nCard(rank='2', suit='clubs')\nWe’ve just seen two advantages of using special methods to leverage the Python Data\nModel:\n• Users of your classes don’t have to memorize arbitrary method names for stan‐\ndard operations. (“How to get the number of items? Is it .size(), .length(), or\nwhat?”)\n• It’s easier to benefit from the rich Python standard library and avoid reinventing\nthe wheel, like the random.choice function.\nBut it gets better.\nBecause our __getitem__ delegates to the [] operator of self._cards, our deck\nautomatically supports slicing. Here’s how we look at the top three cards from a\nbrand-new deck, and then pick just the aces by starting at index 12 and skipping 13\ncards at a time:\n>>> deck[:3]\n[Card(rank='2', suit='spades'), Card(rank='3', suit='spades'),\nCard(rank='4', suit='spades')]\n>>> deck[12::13]\n[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),\nCard(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]\n6 \n| \nChapter 1: The Python Data Model",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "Just by implementing the __getitem__ special method, our deck is also iterable:\n>>> for card in deck:  # doctest: +ELLIPSIS\n...   print(card)\nCard(rank='2', suit='spades')\nCard(rank='3', suit='spades')\nCard(rank='4', suit='spades')\n...\nWe can also iterate over the deck in reverse:\n>>> for card in reversed(deck):  # doctest: +ELLIPSIS\n...   print(card)\nCard(rank='A', suit='hearts')\nCard(rank='K', suit='hearts')\nCard(rank='Q', suit='hearts')\n...\nEllipsis in doctests\nWhenever possible, I extracted the Python console listings in this\nbook from doctest to ensure accuracy. When the output was too\nlong, the elided part is marked by an ellipsis (...), like in the last\nline in the preceding code. In such cases, I used the # doctest:\n+ELLIPSIS directive to make the doctest pass. If you are trying\nthese examples in the interactive console, you may omit the doctest\ncomments altogether.\nIteration is often implicit. If a collection has no __contains__ method, the in opera‐\ntor does a sequential scan. Case in point: in works with our FrenchDeck class because\nit is iterable. Check it out:\n>>> Card('Q', 'hearts') in deck\nTrue\n>>> Card('7', 'beasts') in deck\nFalse\nHow about sorting? A common system of ranking cards is by rank (with aces being\nhighest), then by suit in the order of spades (highest), hearts, diamonds, and clubs\n(lowest). Here is a function that ranks cards by that rule, returning 0 for the 2 of clubs\nand 51 for the ace of spades:\nsuit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0)\ndef spades_high(card):\n    rank_value = FrenchDeck.ranks.index(card.rank)\n    return rank_value * len(suit_values) + suit_values[card.suit]\nGiven spades_high, we can now list our deck in order of increasing rank:\nA Pythonic Card Deck \n| \n7",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "2 A C struct is a record type with named fields.\n>>> for card in sorted(deck, key=spades_high):  # doctest: +ELLIPSIS\n...      print(card)\nCard(rank='2', suit='clubs')\nCard(rank='2', suit='diamonds')\nCard(rank='2', suit='hearts')\n... (46 cards omitted)\nCard(rank='A', suit='diamonds')\nCard(rank='A', suit='hearts')\nCard(rank='A', suit='spades')\nAlthough FrenchDeck implicitly inherits from the object class, most of its function‐\nality is not inherited, but comes from leveraging the data model and composition. By\nimplementing the special methods __len__ and __getitem__, our FrenchDeck\nbehaves like a standard Python sequence, allowing it to benefit from core language\nfeatures (e.g., iteration and slicing) and from the standard library, as shown by the\nexamples using random.choice, reversed, and sorted. Thanks to composition, the\n__len__ and __getitem__ implementations can delegate all the work to a list\nobject, self._cards.\nHow About Shuffling?\nAs implemented so far, a FrenchDeck cannot be shuffled because it\nis immutable: the cards and their positions cannot be changed,\nexcept by violating encapsulation and handling the _cards\nattribute directly. In Chapter 13, we will fix that by adding a one-\nline __setitem__ method.\nHow Special Methods Are Used\nThe first thing to know about special methods is that they are meant to be called by\nthe Python interpreter, and not by you. You don’t write my_object.__len__(). You\nwrite len(my_object) and, if my_object is an instance of a user-defined class, then\nPython calls the __len__ method you implemented.\nBut the interpreter takes a shortcut when dealing for built-in types like list, str,\nbytearray, or extensions like the NumPy arrays. Python variable-sized collections\nwritten in C include a struct2 called PyVarObject, which has an ob_size field holding\nthe number of items in the collection. So, if my_object is an instance of one of those\nbuilt-ins, then len(my_object) retrieves the value of the ob_size field, and this is\nmuch faster than calling a method.\n8 \n| \nChapter 1: The Python Data Model",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "More often than not, the special method call is implicit. For example, the statement\nfor i in x: actually causes the invocation of iter(x), which in turn may call\nx.__iter__() if that is available, or use x.__getitem__(), as in the FrenchDeck\nexample.\nNormally, your code should not have many direct calls to special methods. Unless\nyou are doing a lot of metaprogramming, you should be implementing special meth‐\nods more often than invoking them explicitly. The only special method that is fre‐\nquently called by user code directly is __init__ to invoke the initializer of the\nsuperclass in your own __init__ implementation.\nIf you need to invoke a special method, it is usually better to call the related built-in\nfunction (e.g., len, iter, str, etc.). These built-ins call the corresponding special\nmethod, but often provide other services and—for built-in types—are faster than\nmethod calls. See, for example, “Using iter with a Callable” on page 598 in Chapter 17.\nIn the next sections, we’ll see some of the most important uses of special methods:\n• Emulating numeric types\n• String representation of objects\n• Boolean value of an object\n• Implementing collections\nEmulating Numeric Types\nSeveral special methods allow user objects to respond to operators such as +. We will\ncover that in more detail in Chapter 16, but here our goal is to further illustrate the\nuse of special methods through another simple example.\nWe will implement a class to represent two-dimensional vectors—that is, Euclidean\nvectors like those used in math and physics (see Figure 1-1).\nThe built-in complex type can be used to represent two-\ndimensional vectors, but our class can be extended to represent n-\ndimensional vectors. We will do that in Chapter 17.\nHow Special Methods Are Used \n| \n9",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "Figure 1-1. Example of two-dimensional vector addition; Vector(2, 4) + Vector(2, 1)\nresults in Vector(4, 5).\nWe will start designing the API for such a class by writing a simulated console session\nthat we can use later as a doctest. The following snippet tests the vector addition pic‐\ntured in Figure 1-1:\n>>> v1 = Vector(2, 4)\n>>> v2 = Vector(2, 1)\n>>> v1 + v2\nVector(4, 5)\nNote how the + operator results in a new Vector, displayed in a friendly format at the\nconsole.\nThe abs built-in function returns the absolute value of integers and floats, and the\nmagnitude of complex numbers, so to be consistent, our API also uses abs to calcu‐\nlate the magnitude of a vector:\n>>> v = Vector(3, 4)\n>>> abs(v)\n5.0\nWe can also implement the * operator to perform scalar multiplication (i.e., multi‐\nplying a vector by a number to make a new vector with the same direction and a\nmultiplied magnitude):\n>>> v * 3\nVector(9, 12)\n>>> abs(v * 3)\n15.0\n10 \n| \nChapter 1: The Python Data Model",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Example 1-2 is a Vector class implementing the operations just described, through\nthe use of the special methods __repr__, __abs__, __add__, and __mul__.\nExample 1-2. A simple two-dimensional vector class\n\"\"\"\nvector2d.py: a simplistic class demonstrating some special methods\nIt is simplistic for didactic reasons. It lacks proper error handling,\nespecially in the ``__add__`` and ``__mul__`` methods.\nThis example is greatly expanded later in the book.\nAddition::\n    >>> v1 = Vector(2, 4)\n    >>> v2 = Vector(2, 1)\n    >>> v1 + v2\n    Vector(4, 5)\nAbsolute value::\n    >>> v = Vector(3, 4)\n    >>> abs(v)\n    5.0\nScalar multiplication::\n    >>> v * 3\n    Vector(9, 12)\n    >>> abs(v * 3)\n    15.0\n\"\"\"\nimport math\nclass Vector:\n    def __init__(self, x=0, y=0):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return f'Vector({self.x!r}, {self.y!r})'\n    def __abs__(self):\n        return math.hypot(self.x, self.y)\n    def __bool__(self):\nHow Special Methods Are Used \n| \n11",
      "content_length": 998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "return bool(abs(self))\n    def __add__(self, other):\n        x = self.x + other.x\n        y = self.y + other.y\n        return Vector(x, y)\n    def __mul__(self, scalar):\n        return Vector(self.x * scalar, self.y * scalar)\nWe implemented five special methods in addition to the familiar __init__. Note that\nnone of them is directly called within the class or in the typical usage of the class\nillustrated by the doctests. As mentioned before, the Python interpreter is the only\nfrequent caller of most special methods.\nExample 1-2 implements two operators: + and *, to show basic usage of __add__ and\n__mul__. In both cases, the methods create and return a new instance of Vector, and\ndo not modify either operand—self or other are merely read. This is the expected\nbehavior of infix operators: to create new objects and not touch their operands. I will\nhave a lot more to say about that in Chapter 16.\nAs implemented, Example 1-2 allows multiplying a Vector by a\nnumber, but not a number by a Vector, which violates the commu‐\ntative property of scalar multiplication. We will fix that with the\nspecial method __rmul__ in Chapter 16.\nIn the following sections, we discuss the other special methods in Vector.\nString Representation\nThe __repr__ special method is called by the repr built-in to get the string represen‐\ntation of the object for inspection. Without a custom __repr__, Python’s console\nwould display a Vector instance <Vector object at 0x10e100070>.\nThe interactive console and debugger call repr on the results of the expressions eval‐\nuated, as does the %r placeholder in classic formatting with the % operator, and the !r\nconversion field in the new format string syntax used in f-strings the str.format\nmethod.\nNote that the f-string in our __repr__ uses !r to get the standard representation of\nthe attributes to be displayed. This is good practice, because it shows the crucial dif‐\nference between Vector(1, 2) and Vector('1', '2')—the latter would not work in\nthe context of this example, because the constructor’s arguments should be numbers,\nnot str.\n12 \n| \nChapter 1: The Python Data Model",
      "content_length": 2117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "The string returned by __repr__ should be unambiguous and, if possible, match the\nsource code necessary to re-create the represented object. That is why our Vector\nrepresentation looks like calling the constructor of the class (e.g., Vector(3, 4)).\nIn contrast, __str__ is called by the str() built-in and implicitly used by the print\nfunction. It should return a string suitable for display to end users.\nSometimes same string returned by __repr__ is user-friendly, and you don’t need to\ncode __str__ because the implementation inherited from the object class calls\n__repr__ as a fallback. Example 5-2 is one of several examples in this book with a\ncustom __str__.\nProgrammers with prior experience in languages with a toString\nmethod tend to implement __str__ and not __repr__. If you only\nimplement one of these special methods in Python, choose\n__repr__.\n“What is the difference between __str__ and __repr__ in\nPython?” is a Stack Overflow question with excellent contributions\nfrom Pythonistas Alex Martelli and Martijn Pieters.\nBoolean Value of a Custom Type\nAlthough Python has a bool type, it accepts any object in a Boolean context, such as\nthe expression controlling an if or while statement, or as operands to and, or, and\nnot. To determine whether a value x is truthy or falsy, Python applies bool(x), which\nreturns either True or False.\nBy default, instances of user-defined classes are considered truthy, unless either\n__bool__ or __len__ is implemented. Basically, bool(x) calls x.__bool__() and uses\nthe result. If __bool__ is not implemented, Python tries to invoke x.__len__(), and\nif that returns zero, bool returns False. Otherwise bool returns True.\nOur implementation of __bool__ is conceptually simple: it returns False if the mag‐\nnitude of the vector is zero, True otherwise. We convert the magnitude to a Boolean\nusing bool(abs(self)) because __bool__ is expected to return a Boolean. Outside of\n__bool__ methods, it is rarely necessary to call bool() explicitly, because any object\ncan be used in a Boolean context.\nNote how the special method __bool__ allows your objects to follow the truth value\ntesting rules defined in the “Built-in Types” chapter of The Python Standard Library\ndocumentation.\nHow Special Methods Are Used \n| \n13",
      "content_length": 2261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "A faster implementation of Vector.__bool__ is this:\n    def __bool__(self):\n        return bool(self.x or self.y)\nThis is harder to read, but avoids the trip through abs, __abs__, the\nsquares, and square root. The explicit conversion to bool is needed\nbecause __bool__ must return a Boolean, and or returns either\noperand as is: x or y evaluates to x if that is truthy, otherwise the\nresult is y, whatever that is.\nCollection API\nFigure 1-2 documents the interfaces of the essential collection types in the language.\nAll the classes in the diagram are ABCs—abstract base classes. ABCs and the collec\ntions.abc module are covered in Chapter 13. The goal of this brief section is to give\na panoramic view of Python’s most important collection interfaces, showing how\nthey are built from special methods.\nFigure 1-2. UML class diagram with fundamental collection types. Method names in\nitalic are abstract, so they must be implemented by concrete subclasses such as list\nand dict. The remaining methods have concrete implementations, therefore subclasses\ncan inherit them.\nEach of the top ABCs has a single special method. The Collection ABC (new in\nPython 3.6) unifies the three essential interfaces that every collection should\nimplement:\n14 \n| \nChapter 1: The Python Data Model",
      "content_length": 1277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "• Iterable to support for, unpacking, and other forms of iteration\n• Sized to support the len built-in function\n• Container to support the in operator\nPython does not require concrete classes to actually inherit from any of these ABCs.\nAny class that implements __len__ satisfies the Sized interface.\nThree very important specializations of Collection are:\n• Sequence, formalizing the interface of built-ins like list and str\n• Mapping, implemented by dict, collections.defaultdict, etc.\n• Set, the interface of the set and frozenset built-in types\nOnly Sequence is Reversible, because sequences support arbitrary ordering of their\ncontents, while mappings and sets do not.\nSince Python 3.7, the dict type is officially “ordered,” but that only\nmeans that the key insertion order is preserved. You cannot\nrearrange the keys in a dict however you like.\nAll the special methods in the Set ABC implement infix operators. For example,\na & b computes the intersection of sets a and b, and is implemented in the __and__\nspecial method.\nThe next two chapters will cover standard library sequences, mappings, and sets in\ndetail.\nNow let’s consider the major categories of special methods defined in the Python\nData Model.\nOverview of Special Methods\nThe “Data Model” chapter of The Python Language Reference lists more than 80 spe‐\ncial method names. More than half of them implement arithmetic, bitwise, and com‐\nparison operators. As an overview of what is available, see the following tables.\nTable 1-1 shows special method names, excluding those used to implement infix\noperators or core math functions like abs. Most of these methods will be covered\nthroughout the book, including the most recent additions: asynchronous special\nmethods such as __anext__ (added in Python 3.5), and the class customization hook,\n__init_subclass__ (from Python 3.6).\nOverview of Special Methods \n| \n15",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Table 1-1. Special method names (operators excluded)\nCategory\nMethod names\nString/bytes representation\n__repr__  __str__  __format__  __bytes__  __fspath__\nConversion to number\n__bool__  __complex__  __int__  __float__  __hash__  \n__index__\nEmulating collections\n__len__  __getitem__  __setitem__  __delitem__  \n__contains__\nIteration\n__iter__  __aiter__  __next__  __anext__  __reversed__\nCallable or coroutine execution\n__call__  __await__\nContext management\n__enter__  __exit__  __aexit__  __aenter__\nInstance creation and destruction\n__new__  __init__  __del__\nAttribute management\n__getattr__  __getattribute__  __setattr__  __delattr__  \n__dir__\nAttribute descriptors\n__get__  __set__  __delete__  __set_name__\nAbstract base classes\n__instancecheck__  __subclasscheck__\nClass metaprogramming\n__prepare__  __init_subclass__  __class_getitem__  \n__mro_entries__\nInfix and numerical operators are supported by the special methods listed in\nTable 1-2. Here the most recent names are __matmul__, __rmatmul__, and __imat\nmul__, added in Python 3.5 to support the use of @ as an infix operator for matrix\nmultiplication, as we’ll see in Chapter 16.\nTable 1-2. Special method names and symbols for operators\nOperator category\nSymbols\nMethod names\nUnary numeric\n-  +  abs()\n__neg__  __pos__  __abs__\nRich comparison\n<  <=  ==  !=  >  >=\n__lt__  __le__  __eq__  __ne__  \n__gt__  __ge__\nArithmetic\n+  -  *  /  //  %  @  \ndivmod()  round()  **  \npow()\n__add__  __sub__  __mul__  __truediv__  \n__floordiv__  __mod__  __matmul__  __div\nmod__  __round__  __pow__\nReversed arithmetic\n(arithmetic operators with swapped\noperands)\n__radd__  __rsub__  __rmul__  __rtrue\ndiv__  __rfloordiv__  __rmod__  __rmat\nmul__  __rdivmod__  __rpow__\nAugmented\nassignment\narithmetic\n+=  -=  *=  /=  //=  %=  \n@=  **=\n__iadd__  __isub__  __imul__  __itrue\ndiv__  __ifloordiv__  __imod__  __imat\nmul__  __ipow__\nBitwise\n&  |  ^  <<  >>  ~\n__and__  __or__  __xor__  __lshift__  \n__rshift__  __invert__\nReversed bitwise\n(bitwise operators with swapped\noperands)\n__rand__  __ror__  __rxor__  \n__rlshift__  __rrshift__\n16 \n| \nChapter 1: The Python Data Model",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "Operator category\nSymbols\nMethod names\nAugmented\nassignment bitwise\n&=  |=  ^=  <<=  >>=\n__iand__  __ior__  __ixor__  \n__ilshift__  __irshift__\nPython calls a reversed operator special method on the second\noperand when the corresponding special method on the first\noperand cannot be used. Augmented assignments are shortcuts\ncombining an infix operator with variable assignment, e.g., a += b.\nChapter 16 explains reversed operators and augmented assignment\nin detail.\nWhy len Is Not a Method\nI asked this question to core developer Raymond Hettinger in 2013, and the key to\nhis answer was a quote from “The Zen of Python”: “practicality beats purity.” In\n“How Special Methods Are Used” on page 8, I described how len(x) runs very fast\nwhen x is an instance of a built-in type. No method is called for the built-in objects of\nCPython: the length is simply read from a field in a C struct. Getting the number of\nitems in a collection is a common operation and must work efficiently for such basic\nand diverse types as str, list, memoryview, and so on.\nIn other words, len is not called as a method because it gets special treatment as part\nof the Python Data Model, just like abs. But thanks to the special method __len__,\nyou can also make len work with your own custom objects. This is a fair compromise\nbetween the need for efficient built-in objects and the consistency of the language.\nAlso from “The Zen of Python”: “Special cases aren’t special enough to break the\nrules.”\nIf you think of abs and len as unary operators, you may be more\ninclined to forgive their functional look and feel, as opposed to the\nmethod call syntax one might expect in an object-oriented lan‐\nguage. In fact, the ABC language—a direct ancestor of Python that\npioneered many of its features—had an # operator that was the\nequivalent of len (you’d write #s). When used as an infix operator,\nwritten x#s, it counted the occurrences of x in s, which in Python\nyou get as s.count(x), for any sequence s.\nWhy len Is Not a Method \n| \n17",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "Chapter Summary\nBy implementing special methods, your objects can behave like the built-in types,\nenabling the expressive coding style the community considers Pythonic.\nA basic requirement for a Python object is to provide usable string representations of\nitself, one used for debugging and logging, another for presentation to end users.\nThat is why the special methods __repr__ and __str__ exist in the data model.\nEmulating sequences, as shown with the FrenchDeck example, is one of the most\ncommon uses of the special methods. For example, database libraries often return\nquery results wrapped in sequence-like collections. Making the most of existing\nsequence types is the subject of Chapter 2. Implementing your own sequences will be\ncovered in Chapter 12, when we create a multidimensional extension of the Vector\nclass.\nThanks to operator overloading, Python offers a rich selection of numeric types, from\nthe built-ins to decimal.Decimal and fractions.Fraction, all supporting infix\narithmetic operators. The NumPy data science libraries support infix operators\nwith matrices and tensors. Implementing operators—including reversed operators\nand augmented assignment—will be shown in Chapter 16 via enhancements of the\nVector example.\nThe use and implementation of the majority of the remaining special methods of the\nPython Data Model are covered throughout this book.\nFurther Reading\nThe “Data Model” chapter of The Python Language Reference is the canonical source\nfor the subject of this chapter and much of this book.\nPython in a Nutshell, 3rd ed. by Alex Martelli, Anna Ravenscroft, and Steve Holden\n(O’Reilly) has excellent coverage of the data model. Their description of the mechan‐\nics of attribute access is the most authoritative I’ve seen apart from the actual\nC source code of CPython. Martelli is also a prolific contributor to Stack Overflow,\nwith more than 6,200 answers posted. See his user profile at Stack Overflow.\nDavid Beazley has two books covering the data model in detail in the context of\nPython 3: Python Essential Reference, 4th ed. (Addison-Wesley), and Python Cook‐\nbook, 3rd ed. (O’Reilly), coauthored with Brian K. Jones.\nThe Art of the Metaobject Protocol (MIT Press) by Gregor Kiczales, Jim des Rivieres,\nand Daniel G. Bobrow explains the concept of a metaobject protocol, of which the\nPython Data Model is one example.\n18 \n| \nChapter 1: The Python Data Model",
      "content_length": 2402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "Soapbox\nData Model or Object Model?\nWhat the Python documentation calls the “Python Data Model,” most authors would\nsay is the “Python object model.” Martelli, Ravenscroft, and Holden’s Python in a\nNutshell, 3rd ed., and David Beazley’s Python Essential Reference, 4th ed. are the best\nbooks covering the Python Data Model, but they refer to it as the “object model.” On\nWikipedia, the first definition of “object model” is: “The properties of objects in gen‐\neral in a specific computer programming language.” This is what the Python Data\nModel is about. In this book, I will use “data model” because the documentation\nfavors that term when referring to the Python object model, and because it is the title\nof the chapter of The Python Language Reference most relevant to our discussions.\nMuggle Methods\nThe Original Hacker’s Dictionary defines magic as “yet unexplained, or too compli‐\ncated to explain” or “a feature not generally publicized which allows something other‐\nwise impossible.”\nThe Ruby community calls their equivalent of the special methods magic methods.\nMany in the Python community adopt that term as well. I believe the special methods\nare the opposite of magic. Python and Ruby empower their users with a rich metaob‐\nject protocol that is fully documented, enabling muggles like you and me to emulate\nmany of the features available to core developers who write the interpreters for those\nlanguages.\nIn contrast, consider Go. Some objects in that language have features that are magic,\nin the sense that we cannot emulate them in our own user-defined types. For exam‐\nple, Go arrays, strings, and maps support the use brackets for item access, as in a[i].\nBut there’s no way to make the [] notation work with a new collection type that you\ndefine. Even worse, Go has no user-level concept of an iterable interface or an iterator\nobject, therefore its for/range syntax is limited to supporting five “magic” built-in\ntypes, including arrays, strings, and maps.\nMaybe in the future, the designers of Go will enhance its metaobject protocol. But\ncurrently, it is much more limited than what we have in Python or Ruby.\nMetaobjects\nThe Art of the Metaobject Protocol (AMOP) is my favorite computer book title. But I\nmention it because the term metaobject protocol is useful to think about the Python\nData Model and similar features in other languages. The metaobject part refers to the\nobjects that are the building blocks of the language itself. In this context, protocol is a\nsynonym of interface. So a metaobject protocol is a fancy synonym for object model:\nan API for core language constructs.\nFurther Reading \n| \n19",
      "content_length": 2637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "A rich metaobject protocol enables extending a language to support new program‐\nming paradigms. Gregor Kiczales, the first author of the AMOP book, later became a\npioneer in aspect-oriented programming and the initial author of AspectJ, an exten‐\nsion of Java implementing that paradigm. Aspect-oriented programming is much\neasier to implement in a dynamic language like Python, and some frameworks do it.\nThe most important example is zope.interface, part of the framework on which the\nPlone content management system is built.\n20 \n| \nChapter 1: The Python Data Model",
      "content_length": 568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "1 Leo Geurts, Lambert Meertens, and Steven Pemberton, ABC Programmer’s Handbook, p. 8. (Bosko Books).\nCHAPTER 2\nAn Array of Sequences\nAs you may have noticed, several of the operations mentioned work equally for texts,\nlists and tables. Texts, lists and tables together are called ‘trains’. [...] The FOR com‐\nmand also works generically on trains.\n—Leo Geurts, Lambert Meertens, and Steven Pembertonm, ABC Programmer’s\nHandbook1\nBefore creating Python, Guido was a contributor to the ABC language—a 10-year\nresearch project to design a programming environment for beginners. ABC intro‐\nduced many ideas we now consider “Pythonic”: generic operations on different types\nof sequences, built-in tuple and mapping types, structure by indentation, strong\ntyping without variable declarations, and more. It’s no accident that Python is so\nuser-friendly.\nPython inherited from ABC the uniform handling of sequences. Strings, lists, byte\nsequences, arrays, XML elements, and database results share a rich set of common\noperations, including iteration, slicing, sorting, and concatenation.\nUnderstanding the variety of sequences available in Python saves us from reinventing\nthe wheel, and their common interface inspires us to create APIs that properly sup‐\nport and leverage existing and future sequence types.\nMost of the discussion in this chapter applies to sequences in general, from the famil‐\niar list to the str and bytes types added in Python 3. Specific topics on lists, tuples,\narrays, and queues are also covered here, but the specifics of Unicode strings and byte\nsequences appear in Chapter 4. Also, the idea here is to cover sequence types that are\nready to use. Creating your own sequence types is the subject of Chapter 12.\n21",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "These are the main topics this chapter will cover:\n• List comprehensions and the basics of generator expressions\n• Using tuples as records versus using tuples as immutable lists\n• Sequence unpacking and sequence patterns\n• Reading from slices and writing to slices\n• Specialized sequence types, like arrays and queues\nWhat’s New in This Chapter\nThe most important update in this chapter is “Pattern Matching with Sequences” on\npage 38. That’s the first time the new pattern matching feature of Python 3.10 appears\nin this second edition.\nOther changes are not updates but improvements over the first edition:\n• New diagram and description of the internals of sequences, contrasting contain‐\ners and flat sequences\n• Brief comparison of the performance and storage characteristics of list versus\ntuple\n• Caveats of tuples with mutable elements, and how to detect them if needed\nI moved coverage of named tuples to “Classic Named Tuples” on page 169 in Chapter 5,\nwhere they are compared to typing.NamedTuple and @dataclass.\nTo make room for new content and keep the page count within\nreason, the section “Managing Ordered Sequences with Bisect”\nfrom the first edition is now a post in the fluentpython.com com‐\npanion website.\nOverview of Built-In Sequences\nThe standard library offers a rich selection of sequence types implemented in C:\nContainer sequences\nCan hold items of different types, including nested containers. Some examples:\nlist, tuple, and collections.deque.\nFlat sequences\nHold items of one simple type. Some examples: str, bytes, and array.array.\n22 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "A container sequence holds references to the objects it contains, which may be of any\ntype, while a flat sequence stores the value of its contents in its own memory space,\nnot as distinct Python objects. See Figure 2-1.\nFigure 2-1. Simplified memory diagrams for a tuple and an array, each with three\nitems. Gray cells represent the in-memory header of each Python object—not drawn to\nproportion. The tuple has an array of references to its items. Each item is a separate\nPython object, possibly holding references to other Python objects, like that two-item\nlist. In contrast, the Python array is a single object, holding a C language array of three\ndoubles.\nThus, flat sequences are more compact, but they are limited to holding primitive\nmachine values like bytes, integers, and floats.\nEvery Python object in memory has a header with metadata. The\nsimplest Python object, a float, has a value field and two metadata\nfields:\n• ob_refcnt: the object’s reference count\n• ob_type: a pointer to the object’s type\n• ob_fval: a C double holding the value of the float\nOn a 64-bit Python build, each of those fields takes 8 bytes. That’s\nwhy an array of floats is much more compact than a tuple of floats:\nthe array is a single object holding the raw values of the floats,\nwhile the tuple consists of several objects—the tuple itself and each\nfloat object contained in it.\nOverview of Built-In Sequences \n| \n23",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "Another way of grouping sequence types is by mutability:\nMutable sequences\nFor example, list, bytearray, array.array, and collections.deque.\nImmutable sequences\nFor example, tuple, str, and bytes.\nFigure 2-2 helps visualize how mutable sequences inherit all methods from immuta‐\nble sequences, and implement several additional methods. The built-in concrete\nsequence types do not actually subclass the Sequence and MutableSequence abstract\nbase classes (ABCs), but they are virtual subclasses registered with those ABCs—as\nwe’ll see in Chapter 13. Being virtual subclasses, tuple and list pass these tests:\n>>> from collections import abc\n>>> issubclass(tuple, abc.Sequence)\nTrue\n>>> issubclass(list, abc.MutableSequence)\nTrue\nFigure 2-2. Simplified UML class diagram for some classes from collections.abc (super‐\nclasses are on the left; inheritance arrows point from subclasses to superclasses; names\nin italic are abstract classes and abstract methods).\nKeep in mind these common traits: mutable versus immutable; container versus flat.\nThey are helpful to extrapolate what you know about one sequence type to others.\nThe most fundamental sequence type is the list: a mutable container. I expect you\nare very familiar with lists, so we’ll jump right into list comprehensions, a powerful\nway of building lists that is sometimes underused because the syntax may look\nunusual at first. Mastering list comprehensions opens the door to generator expres‐\nsions, which—among other uses—can produce elements to fill up sequences of any\ntype. Both are the subject of the next section.\n24 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "List Comprehensions and Generator Expressions\nA quick way to build a sequence is using a list comprehension (if the target is a list)\nor a generator expression (for other kinds of sequences). If you are not using these\nsyntactic forms on a daily basis, I bet you are missing opportunities to write code that\nis more readable and often faster at the same time.\nIf you doubt my claim that these constructs are “more readable,” read on. I’ll try to\nconvince you.\nFor brevity, many Python programmers refer to list comprehen‐\nsions as listcomps, and generator expressions as genexps. I will use\nthese words as well.\nList Comprehensions and Readability\nHere is a test: which do you find easier to read, Example 2-1 or Example 2-2?\nExample 2-1. Build a list of Unicode code points from a string\n>>> symbols = '$¢£¥€¤'\n>>> codes = []\n>>> for symbol in symbols:\n...     codes.append(ord(symbol))\n...\n>>> codes\n[36, 162, 163, 165, 8364, 164]\nExample 2-2. Build a list of Unicode code points from a string, using a listcomp\n>>> symbols = '$¢£¥€¤'\n>>> codes = [ord(symbol) for symbol in symbols]\n>>> codes\n[36, 162, 163, 165, 8364, 164]\nAnybody who knows a little bit of Python can read Example 2-1. However, after\nlearning about listcomps, I find Example 2-2 more readable because its intent is\nexplicit.\nA for loop may be used to do lots of different things: scanning a sequence to count or\npick items, computing aggregates (sums, averages), or any number of other tasks.\nThe code in Example 2-1 is building up a list. In contrast, a listcomp is more explicit.\nIts goal is always to build a new list.\nList Comprehensions and Generator Expressions \n| \n25",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "2 Thanks to reader Tina Lapine for pointing this out.\nOf course, it is possible to abuse list comprehensions to write truly incomprehensible\ncode. I’ve seen Python code with listcomps used just to repeat a block of code for its\nside effects. If you are not doing something with the produced list, you should not use\nthat syntax. Also, try to keep it short. If the list comprehension spans more than two\nlines, it is probably best to break it apart or rewrite it as a plain old for loop. Use your\nbest judgment: for Python, as for English, there are no hard-and-fast rules for clear\nwriting.\nSyntax Tip\nIn Python code, line breaks are ignored inside pairs of [], {}, or ().\nSo you can build multiline lists, listcomps, tuples, dictionaries, etc.,\nwithout using the \\ line continuation escape, which doesn’t work if\nyou accidentally type a space after it. Also, when those delimiter\npairs are used to define a literal with a comma-separated series of\nitems, a trailing comma will be ignored. So, for example, when cod‐\ning a multiline list literal, it is thoughtful to put a comma after the\nlast item, making it a little easier for the next coder to add one\nmore item to that list, and reducing noise when reading diffs.\nLocal Scope Within Comprehensions and Generator Expressions\nIn Python 3, list comprehensions, generator expressions, and their siblings set and\ndict comprehensions, have a local scope to hold the variables assigned in the for\nclause.\nHowever, variables assigned with the “Walrus operator” := remain accessible after\nthose comprehensions or expressions return—unlike local variables in a function.\nPEP 572—Assignment Expressions defines the scope of the target of := as the enclos‐\ning function, unless there is a global or nonlocal declaration for that target.2\n>>> x = 'ABC'\n>>> codes = [ord(x) for x in x]\n>>> x  \n'ABC'\n>>> codes\n[65, 66, 67]\n>>> codes = [last := ord(c) for c in x]\n>>> last  \n67\n>>> c  \nTraceback (most recent call last):\n26 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "File \"<stdin>\", line 1, in <module>\nNameError: name 'c' is not defined\nx was not clobbered: it’s still bound to 'ABC'.\nlast remains.\nc is gone; it existed only inside the listcomp.\nList comprehensions build lists from sequences or any other iterable type by filtering\nand transforming items. The filter and map built-ins can be composed to do the\nsame, but readability suffers, as we will see next.\nListcomps Versus map and filter\nListcomps do everything the map and filter functions do, without the contortions of\nthe functionally challenged Python lambda. Consider Example 2-3.\nExample 2-3. The same list built by a listcomp and a map/filter composition\n>>> symbols = '$¢£¥€¤'\n>>> beyond_ascii = [ord(s) for s in symbols if ord(s) > 127]\n>>> beyond_ascii\n[162, 163, 165, 8364, 164]\n>>> beyond_ascii = list(filter(lambda c: c > 127, map(ord, symbols)))\n>>> beyond_ascii\n[162, 163, 165, 8364, 164]\nI used to believe that map and filter were faster than the equivalent listcomps, but\nAlex Martelli pointed out that’s not the case—at least not in the preceding examples.\nThe 02-array-seq/listcomp_speed.py script in the Fluent Python code repository is a\nsimple speed test comparing listcomp with filter/map.\nI’ll have more to say about map and filter in Chapter 7. Now we turn to the use of\nlistcomps to compute Cartesian products: a list containing tuples built from all items\nfrom two or more lists.\nCartesian Products\nListcomps can build lists from the Cartesian product of two or more iterables. The\nitems that make up the Cartesian product are tuples made from items from every\ninput iterable. The resulting list has a length equal to the lengths of the input iterables\nmultiplied. See Figure 2-3.\nList Comprehensions and Generator Expressions \n| \n27",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "Figure 2-3. The Cartesian product of 3 card ranks and 4 suits is a sequence of 12\npairings.\nFor example, imagine you need to produce a list of T-shirts available in two colors\nand three sizes. Example 2-4 shows how to produce that list using a listcomp. The\nresult has six items.\nExample 2-4. Cartesian product using a list comprehension\n>>> colors = ['black', 'white']\n>>> sizes = ['S', 'M', 'L']\n>>> tshirts = [(color, size) for color in colors for size in sizes]  \n>>> tshirts\n[('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'),\n ('white', 'M'), ('white', 'L')]\n>>> for color in colors:  \n...     for size in sizes:\n...         print((color, size))\n...\n('black', 'S')\n('black', 'M')\n('black', 'L')\n('white', 'S')\n('white', 'M')\n('white', 'L')\n>>> tshirts = [(color, size) for size in sizes      \n...                          for color in colors]\n>>> tshirts\n[('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'),\n ('black', 'L'), ('white', 'L')]\n28 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "This generates a list of tuples arranged by color, then size.\nNote how the resulting list is arranged as if the for loops were nested in the same\norder as they appear in the listcomp.\nTo get items arranged by size, then color, just rearrange the for clauses; adding a\nline break to the listcomp makes it easier to see how the result will be ordered.\nIn Example 1-1 (Chapter 1), I used the following expression to initialize a card deck\nwith a list made of 52 cards from all 13 ranks of each of the 4 suits, sorted by suit,\nthen rank:\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\nListcomps are a one-trick pony: they build lists. To generate data for other sequence\ntypes, a genexp is the way to go. The next section is a brief look at genexps in the\ncontext of building sequences that are not lists.\nGenerator Expressions\nTo initialize tuples, arrays, and other types of sequences, you could also start from a\nlistcomp, but a genexp (generator expression) saves memory because it yields items\none by one using the iterator protocol instead of building a whole list just to feed\nanother constructor.\nGenexps use the same syntax as listcomps, but are enclosed in parentheses rather\nthan brackets.\nExample 2-5 shows basic usage of genexps to build a tuple and an array.\nExample 2-5. Initializing a tuple and an array from a generator expression\n>>> symbols = '$¢£¥€¤'\n>>> tuple(ord(symbol) for symbol in symbols)  \n(36, 162, 163, 165, 8364, 164)\n>>> import array\n>>> array.array('I', (ord(symbol) for symbol in symbols))  \narray('I', [36, 162, 163, 165, 8364, 164])\nIf the generator expression is the single argument in a function call, there is no\nneed to duplicate the enclosing parentheses.\nThe array constructor takes two arguments, so the parentheses around the gen‐\nerator expression are mandatory. The first argument of the array constructor\ndefines the storage type used for the numbers in the array, as we’ll see in “Arrays”\non page 59.\nList Comprehensions and Generator Expressions \n| \n29",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "Example 2-6 uses a genexp with a Cartesian product to print out a roster of T-shirts\nof two colors in three sizes. In contrast with Example 2-4, here the six-item list of T-\nshirts is never built in memory: the generator expression feeds the for loop produc‐\ning one item at a time. If the two lists used in the Cartesian product had a thousand\nitems each, using a generator expression would save the cost of building a list with a\nmillion items just to feed the for loop.\nExample 2-6. Cartesian product in a generator expression\n>>> colors = ['black', 'white']\n>>> sizes = ['S', 'M', 'L']\n>>> for tshirt in (f'{c} {s}' for c in colors for s in sizes):  \n...     print(tshirt)\n...\nblack S\nblack M\nblack L\nwhite S\nwhite M\nwhite L\nThe generator expression yields items one by one; a list with all six T-shirt varia‐\ntions is never produced in this example.\nChapter 17 explains how generators work in detail. Here the idea\nwas just to show the use of generator expressions to initialize\nsequences other than lists, or to produce output that you don’t\nneed to keep in memory.\nNow we move on to the other fundamental sequence type in Python: the tuple.\nTuples Are Not Just Immutable Lists\nSome introductory texts about Python present tuples as “immutable lists,” but that is\nshort selling them. Tuples do double duty: they can be used as immutable lists and\nalso as records with no field names. This use is sometimes overlooked, so we will start\nwith that.\nTuples as Records\nTuples hold records: each item in the tuple holds the data for one field, and the posi‐\ntion of the item gives its meaning.\nIf you think of a tuple just as an immutable list, the quantity and the order of the\nitems may or may not be important, depending on the context. But when using a\n30 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "tuple as a collection of fields, the number of items is usually fixed and their order is\nalways important.\nExample 2-7 shows tuples used as records. Note that in every expression, sorting the\ntuple would destroy the information because the meaning of each field is given by its\nposition in the tuple.\nExample 2-7. Tuples used as records\n>>> lax_coordinates = (33.9425, -118.408056)  \n>>> city, year, pop, chg, area = ('Tokyo', 2003, 32_450, 0.66, 8014)  \n>>> traveler_ids = [('USA', '31195855'), ('BRA', 'CE342567'),  \n...     ('ESP', 'XDA205856')]\n>>> for passport in sorted(traveler_ids):  \n...     print('%s/%s' % passport)   \n...\nBRA/CE342567\nESP/XDA205856\nUSA/31195855\n>>> for country, _ in traveler_ids:  \n...     print(country)\n...\nUSA\nBRA\nESP\nLatitude and longitude of the Los Angeles International Airport.\nData about Tokyo: name, year, population (thousands), population change (%),\nand area (km²).\nA list of tuples of the form (country_code, passport_number).\nAs we iterate over the list, passport is bound to each tuple.\nThe % formatting operator understands tuples and treats each item as a separate\nfield.\nThe for loop knows how to retrieve the items of a tuple separately—this is called\n“unpacking.” Here we are not interested in the second item, so we assign it to _, a\ndummy variable.\nTuples Are Not Just Immutable Lists \n| \n31",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "In general, using _ as a dummy variable is just a convention. It’s\njust a strange but valid variable name. However, in a match/case\nstatement, _ is a wildcard that matches any value but is not bound\nto a value. See “Pattern Matching with Sequences” on page 38. And\nin the Python console, the result of the preceding command is\nassigned to _—unless the result is None.\nWe often think of records as data structures with named fields. Chapter 5 presents\ntwo ways of creating tuples with named fields.\nBut often, there’s no need to go through the trouble of creating a class just to name\nthe fields, especially if you leverage unpacking and avoid using indexes to access the\nfields. In Example 2-7, we assigned ('Tokyo', 2003, 32_450, 0.66, 8014) to\ncity, year, pop, chg, area in a single statement. Then, the % operator assigned\neach item in the passport tuple to the corresponding slot in the format string in the\nprint argument. Those are two examples of tuple unpacking.\nThe term tuple unpacking is widely used by Pythonistas, but itera‐\nble unpacking is gaining traction, as in the title of PEP 3132 —\nExtended Iterable Unpacking.\n“Unpacking Sequences and Iterables” on page 35 presents a lot more\nabout unpacking not only tuples, but sequences and iterables in\ngeneral.\nNow let’s consider the tuple class as an immutable variant of the list class.\nTuples as Immutable Lists\nThe Python interpreter and standard library make extensive use of tuples as immuta‐\nble lists, and so should you. This brings two key benefits:\nClarity\nWhen you see a tuple in code, you know its length will never change.\nPerformance\nA tuple uses less memory than a list of the same length, and it allows Python\nto do some optimizations.\nHowever, be aware that the immutability of a tuple only applies to the references\ncontained in it. References in a tuple cannot be deleted or replaced. But if one of\nthose references points to a mutable object, and that object is changed, then the value\nof the tuple changes. The next snippet illustrates this point by creating two tuples—a\nand b—which are initially equal. Figure 2-4 represents the initial layout of the b tuple\nin memory.\n32 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "Figure 2-4. The content of the tuple itself is immutable, but that only means the refer‐\nences held by the tuple will always point to the same objects. However, if one of the ref‐\nerenced objects is mutable—like a list—its content may change.\nWhen the last item in b is changed, b and a become different:\n>>> a = (10, 'alpha', [1, 2])\n>>> b = (10, 'alpha', [1, 2])\n>>> a == b\nTrue\n>>> b[-1].append(99)\n>>> a == b\nFalse\n>>> b\n(10, 'alpha', [1, 2, 99])\nTuples with mutable items can be a source of bugs. As we’ll see in “What Is Hasha‐\nble” on page 84, an object is only hashable if its value cannot ever change. An unhasha‐\nble tuple cannot be inserted as a dict key, or a set element.\nIf you want to determine explicitly if a tuple (or any object) has a fixed value, you can\nuse the hash built-in to create a fixed function like this:\n>>> def fixed(o):\n...     try:\n...         hash(o)\n...     except TypeError:\n...         return False\n...     return True\n...\n>>> tf = (10, 'alpha', (1, 2))\n>>> tm = (10, 'alpha', [1, 2])\n>>> fixed(tf)\nTrue\nTuples Are Not Just Immutable Lists \n| \n33",
      "content_length": 1084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": ">>> fixed(tm)\nFalse\nWe explore this issue further in “The Relative Immutability of Tuples” on page 207.\nDespite this caveat, tuples are widely used as immutable lists. They offer some perfor‐\nmance advantages explained by Python core developer Raymond Hettinger in a\nStackOverflow answer to the question: “Are tuples more efficient than lists in\nPython?”. To summarize, Hettinger wrote:\n• To evaluate a tuple literal, the Python compiler generates bytecode for a tuple\nconstant in one operation; but for a list literal, the generated bytecode pushes\neach element as a separate constant to the data stack, and then builds the list.\n• Given a tuple t, tuple(t) simply returns a reference to the same t. There’s no\nneed to copy. In contrast, given a list l, the list(l) constructor must create a\nnew copy of l.\n• Because of its fixed length, a tuple instance is allocated the exact memory space\nit needs. Instances of list, on the other hand, are allocated with room to spare,\nto amortize the cost of future appends.\n• The references to the items in a tuple are stored in an array in the tuple struct,\nwhile a list holds a pointer to an array of references stored elsewhere. The indi‐\nrection is necessary because when a list grows beyond the space currently alloca‐\nted, Python needs to reallocate the array of references to make room. The extra\nindirection makes CPU caches less effective.\nComparing Tuple and List Methods\nWhen using a tuple as an immutable variation of list, it is good to know how similar\ntheir APIs are. As you can see in Table 2-1, tuple supports all list methods that do\nnot involve adding or removing items, with one exception—tuple lacks the\n__reversed__ method. However, that is just for optimization; reversed(my_tuple)\nworks without it.\nTable 2-1. Methods and attributes found in list or tuple (methods implemented by object\nare omitted for brevity)\nlist\ntuple  \ns.__add__(s2)\n●\n●\ns + s2—concatenation\ns.__iadd__(s2)\n●\ns += s2—in-place concatenation\ns.append(e)\n●\nAppend one element after last\ns.clear()\n●\nDelete all items\ns.__contains__(e)\n●\n●\ne in s\ns.copy()\n●\nShallow copy of the list\n34 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "list\ntuple  \ns.count(e)\n●\n●\nCount occurrences of an element\ns.__delitem__(p)\n●\nRemove item at position p\ns.extend(it)\n●\nAppend items from iterable it\ns.__getitem__(p)\n●\n●\ns[p]—get item at position\ns.__getnewargs__()\n●\nSupport for optimized serialization with pickle\ns.index(e)\n●\n●\nFind position of first occurrence of e\ns.insert(p, e)\n●\nInsert element e before the item at position p\ns.__iter__()\n●\n●\nGet iterator\ns.__len__()\n●\n●\nlen(s)—number of items\ns.__mul__(n)\n●\n●\ns * n—repeated concatenation\ns.__imul__(n)\n●\ns *= n—in-place repeated concatenation\ns.__rmul__(n)\n●\n●\nn * s—reversed repeated concatenationa\ns.pop([p])\n●\nRemove and return last item or item at optional position p\ns.remove(e)\n●\nRemove first occurrence of element e by value\ns.reverse()\n●\nReverse the order of the items in place\ns.__reversed__()\n●\nGet iterator to scan items from last to first\ns.__setitem__(p, e)\n●\ns[p] = e—put e in position p, overwriting existing itemb\ns.sort([key], [reverse]) ●\nSort items in place with optional keyword arguments key and\nreverse\na Reversed operators are explained in Chapter 16.\nb Also used to overwrite a subsequence. See “Assigning to Slices” on page 50.\nNow let’s switch to an important subject for idiomatic Python programming: tuple,\nlist, and iterable unpacking.\nUnpacking Sequences and Iterables\nUnpacking is important because it avoids unnecessary and error-prone use of\nindexes to extract elements from sequences. Also, unpacking works with any iterable\nobject as the data source—including iterators, which don’t support index notation\n([]). The only requirement is that the iterable yields exactly one item per variable in\nthe receiving end, unless you use a star (*) to capture excess items, as explained in\n“Using * to Grab Excess Items” on page 36.\nThe most visible form of unpacking is parallel assignment; that is, assigning items\nfrom an iterable to a tuple of variables, as you can see in this example:\n>>> lax_coordinates = (33.9425, -118.408056)\n>>> latitude, longitude = lax_coordinates  # unpacking\n>>> latitude\nUnpacking Sequences and Iterables \n| \n35",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "33.9425\n>>> longitude\n-118.408056\nAn elegant application of unpacking is swapping the values of variables without using\na temporary variable:\n>>> b, a = a, b\nAnother example of unpacking is prefixing an argument with * when calling a\nfunction:\n>>> divmod(20, 8)\n(2, 4)\n>>> t = (20, 8)\n>>> divmod(*t)\n(2, 4)\n>>> quotient, remainder = divmod(*t)\n>>> quotient, remainder\n(2, 4)\nThe preceding code shows another use of unpacking: allowing functions to return\nmultiple values in a way that is convenient to the caller. As another example, the\nos.path.split() function builds a tuple (path, last_part) from a filesystem path:\n>>> import os\n>>> _, filename = os.path.split('/home/luciano/.ssh/id_rsa.pub')\n>>> filename\n'id_rsa.pub'\nAnother way of using just some of the items when unpacking is to use the * syntax, as\nwe’ll see right away.\nUsing * to Grab Excess Items\nDefining function parameters with *args to grab arbitrary excess arguments is a\nclassic Python feature.\nIn Python 3, this idea was extended to apply to parallel assignment as well:\n>>> a, b, *rest = range(5)\n>>> a, b, rest\n(0, 1, [2, 3, 4])\n>>> a, b, *rest = range(3)\n>>> a, b, rest\n(0, 1, [2])\n>>> a, b, *rest = range(2)\n>>> a, b, rest\n(0, 1, [])\nIn the context of parallel assignment, the * prefix can be applied to exactly one vari‐\nable, but it can appear in any position:\n36 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": ">>> a, *body, c, d = range(5)\n>>> a, body, c, d\n(0, [1, 2], 3, 4)\n>>> *head, b, c, d = range(5)\n>>> head, b, c, d\n([0, 1], 2, 3, 4)\nUnpacking with * in Function Calls and Sequence Literals\nPEP 448—Additional Unpacking Generalizations introduced more flexible syntax for\niterable unpacking, best summarized in “What’s New In Python 3.5”.\nIn function calls, we can use * multiple times:\n>>> def fun(a, b, c, d, *rest):\n...     return a, b, c, d, rest\n...\n>>> fun(*[1, 2], 3, *range(4, 7))\n(1, 2, 3, 4, (5, 6))\nThe * can also be used when defining list, tuple, or set literals, as shown in these\nexamples from “What’s New In Python 3.5”:\n>>> *range(4), 4\n(0, 1, 2, 3, 4)\n>>> [*range(4), 4]\n[0, 1, 2, 3, 4]\n>>> {*range(4), 4, *(5, 6, 7)}\n{0, 1, 2, 3, 4, 5, 6, 7}\nPEP 448 introduced similar new syntax for **, which we’ll see in “Unpacking Map‐\npings” on page 80.\nFinally, a powerful feature of tuple unpacking is that it works with nested structures.\nNested Unpacking\nThe target of an unpacking can use nesting, e.g., (a, b, (c, d)). Python will do the\nright thing if the value has the same nesting structure. Example 2-8 shows nested\nunpacking in action.\nExample 2-8. Unpacking nested tuples to access the longitude\nmetro_areas = [\n    ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),  \n    ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),\n    ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),\n    ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),\n    ('São Paulo', 'BR', 19.649, (-23.547778, -46.635833)),\n]\nUnpacking Sequences and Iterables \n| \n37",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "3 Thanks to tech reviewer Leonardo Rochael for this example.\ndef main():\n    print(f'{\"\":15} | {\"latitude\":>9} | {\"longitude\":>9}')\n    for name, _, _, (lat, lon) in metro_areas:  \n        if lon <= 0:  \n            print(f'{name:15} | {lat:9.4f} | {lon:9.4f}')\nif __name__ == '__main__':\n    main()\nEach tuple holds a record with four fields, the last of which is a coordinate pair.\nBy assigning the last field to a nested tuple, we unpack the coordinates.\nThe lon <= 0: test selects only cities in the Western hemisphere.\nThe output of Example 2-8 is:\n                |  latitude | longitude\nMexico City     |   19.4333 |  -99.1333\nNew York-Newark |   40.8086 |  -74.0204\nSão Paulo       |  -23.5478 |  -46.6358\nThe target of an unpacking assignment can also be a list, but good use cases are rare.\nHere is the only one I know: if you have a database query that returns a single record\n(e.g., the SQL code has a LIMIT 1 clause), then you can unpack and at the same time\nmake sure there’s only one result with this code:\n>>> [record] = query_returning_single_row()\nIf the record has only one field, you can get it directly, like this:\n>>> [[field]] = query_returning_single_row_with_single_field()\nBoth of these could be written with tuples, but don’t forget the syntax quirk that\nsingle-item tuples must be written with a trailing comma. So the first target would be\n(record,) and the second ((field,),). In both cases you get a silent bug if you for‐\nget a comma.3\nNow let’s study pattern matching, which supports even more powerful ways to\nunpack sequences.\nPattern Matching with Sequences\nThe most visible new feature in Python 3.10 is pattern matching with the match/case\nstatement proposed in PEP 634—Structural Pattern Matching: Specification.\n38 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Python core developer Carol Willing wrote the excellent introduc‐\ntion to pattern matching in the “Structural Pattern Matching” sec‐\ntion of “What’s New In Python 3.10”. You may want to read that\nquick overview. In this book, I chose to split the coverage of pat‐\ntern matching over different chapters, depending on the pattern\ntypes: “Pattern Matching with Mappings” on page 81 and “Pattern\nMatching Class Instances” on page 192. An extended example is in\n“Pattern Matching in lis.py: A Case Study” on page 669.\nHere is a first example of match/case handling sequences. Imagine you are designing\na robot that accepts commands sent as sequences of words and numbers, like BEEPER\n440 3. After splitting into parts and parsing the numbers, you’d have a message like\n['BEEPER', 440, 3]. You could use a method like this to handle such messages:\nExample 2-9. Method from an imaginary Robot class\n    def handle_command(self, message):\n        match message:  \n            case ['BEEPER', frequency, times]:  \n                self.beep(times, frequency)\n            case ['NECK', angle]:  \n                self.rotate_neck(angle)\n            case ['LED', ident, intensity]:  \n                self.leds[ident].set_brightness(ident, intensity)\n            case ['LED', ident, red, green, blue]:  \n                self.leds[ident].set_color(ident, red, green, blue)\n            case _:  \n                raise InvalidCommand(message)\nThe expression after the match keyword is the subject. The subject is the data that\nPython will try to match to the patterns in each case clause.\nThis pattern matches any subject that is a sequence with three items. The first\nitem must be the string 'BEEPER'. The second and third item can be anything,\nand they will be bound to the variables frequency and times, in that order.\nThis matches any subject with two items, the first being 'NECK'.\nThis will match a subject with three items starting with 'LED'. If the number of\nitems does not match, Python proceeds to the next case.\nAnother sequence pattern starting with 'LED', now with five items—including\nthe 'LED' constant.\nPattern Matching with Sequences \n| \n39",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "4 In my view, a sequence of if/elif/elif/.../else blocks is a fine replacement for switch/case. It doesn’t\nsuffer from the fallthrough and dangling else problems that some language designers irrationally copied from\nC—decades after they were widely known as the cause of countless bugs.\nThis is the default case. It will match any subject that did not match a previous\npattern. The _ variable is special, as we’ll soon see.\nOn the surface, match/case may look like the switch/case statement from the C lan‐\nguage—but that’s only half the story.4 One key improvement of match over switch is\ndestructuring—a more advanced form of unpacking. Destructuring is a new word in\nthe Python vocabulary, but it is commonly used in the documentation of languages\nthat support pattern matching—like Scala and Elixir.\nAs a first example of destructuring, Example 2-10 shows part of Example 2-8 rewrit‐\nten with match/case.\nExample 2-10. Destructuring nested tuples—requires Python ≥ 3.10\nmetro_areas = [\n    ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),\n    ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),\n    ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),\n    ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),\n    ('São Paulo', 'BR', 19.649, (-23.547778, -46.635833)),\n]\ndef main():\n    print(f'{\"\":15} | {\"latitude\":>9} | {\"longitude\":>9}')\n    for record in metro_areas:\n        match record:  \n            case [name, _, _, (lat, lon)] if lon <= 0:  \n                print(f'{name:15} | {lat:9.4f} | {lon:9.4f}')\nThe subject of this match is record— i.e., each of the tuples in metro_areas.\nA case clause has two parts: a pattern and an optional guard with the if\nkeyword.\nIn general, a sequence pattern matches the subject if:\n1. The subject is a sequence and;\n2. The subject and the pattern have the same number of items and;\n3. Each corresponding item matches, including nested items.\n40 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "For example, the pattern [name, _, _, (lat, lon)] in Example 2-10 matches a\nsequence with four items, and the last item must be a two-item sequence.\nSequence patterns may be written as tuples or lists or any combination of nested\ntuples and lists, but it makes no difference which syntax you use: in a sequence pat‐\ntern, square brackets and parentheses mean the same thing. I wrote the pattern as a\nlist with a nested 2-tuple just to avoid repeating brackets or parentheses in\nExample 2-10.\nA sequence pattern can match instances of most actual or virtual subclasses of collec\ntions.abc.Sequence, with the exception of str, bytes, and bytearray.\nInstances of str, bytes, and bytearray are not handled as sequen‐\nces in the context of match/case. A match subject of one of those\ntypes is treated as an “atomic” value—like the integer 987 is treated\nas one value, not a sequence of digits. Treating those three types as\nsequences could cause bugs due to unintended matches. If you\nwant to treat an object of those types as a sequence subject, convert\nit in the match clause. For example, see tuple(phone) in the\nfollowing:\n    match tuple(phone):\n        case ['1', *rest]:  # North America and Caribbean\n            ...\n        case ['2', *rest]:  # Africa and some territories\n            ...\n        case ['3' | '4', *rest]:  # Europe\n            ...\nIn the standard library, these types are compatible with sequence patterns:\nlist     memoryview    array.array\ntuple    range         collections.deque\nUnlike unpacking, patterns don’t destructure iterables that are not sequences (such as\niterators).\nThe _ symbol is special in patterns: it matches any single item in that position, but it\nis never bound to the value of the matched item. Also, the _ is the only variable that\ncan appear more than once in a pattern.\nYou can bind any part of a pattern with a variable using the as keyword:\n        case [name, _, _, (lat, lon) as coord]:\nGiven the subject ['Shanghai', 'CN', 24.9, (31.1, 121.3)], the preceding pat‐\ntern will match, and set the following variables:\nPattern Matching with Sequences \n| \n41",
      "content_length": 2107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Variable\nSet Value\nname\n'Shanghai'\nlat\n31.1\nlon\n121.3\ncoord\n(31.1, 121.3)\nWe can make patterns more specific by adding type information. For example, the\nfollowing pattern matches the same nested sequence structure as the previous exam‐\nple, but the first item must be an instance of str, and both items in the 2-tuple must\nbe instances of float:\n        case [str(name), _, _, (float(lat), float(lon))]:\nThe expressions str(name) and float(lat) look like constructor\ncalls, which we’d use to convert name and lat to str and float.\nBut in the context of a pattern, that syntax performs a runtime type\ncheck: the preceding pattern will match a four-item sequence in\nwhich item 0 must be a str, and item 3 must be a pair of floats.\nAdditionally, the str in item 0 will be bound to the name variable,\nand the floats in item 3 will be bound to lat and lon, respectively.\nSo, although str(name) borrows the syntax of a constructor call,\nthe semantics are completely different in the context of a pattern.\nUsing arbitrary classes in patterns is covered in “Pattern Matching\nClass Instances” on page 192.\nOn the other hand, if we want to match any subject sequence starting with a str, and\nending with a nested sequence of two floats, we can write:\n        case [str(name), *_, (float(lat), float(lon))]:\nThe *_ matches any number of items, without binding them to a variable. Using\n*extra instead of *_ would bind the items to extra as a list with 0 or more items.\nThe optional guard clause starting with if is evaluated only if the pattern matches,\nand can reference variables bound in the pattern, as in Example 2-10:\n        match record:\n            case [name, _, _, (lat, lon)] if lon <= 0:\n                print(f'{name:15} | {lat:9.4f} | {lon:9.4f}')\nThe nested block with the print statement runs only if the pattern matches and the\nguard expression is truthy.\n42 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1903,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "5 The latter is named eval in Norvig’s code; I renamed it to avoid confusion with Python’s eval built-in.\nDestructuring with patterns is so expressive that sometimes a\nmatch with a single case can make code simpler. Guido van Ros‐\nsum has a collection of case/match examples, including one that\nhe titled “A very deep iterable and type match with extraction”.\nExample 2-10 is not an improvement over Example 2-8. It’s just an example to con‐\ntrast two ways of doing the same thing. The next example shows how pattern match‐\ning contributes to clear, concise, and effective code.\nPattern Matching Sequences in an Interpreter\nPeter Norvig of Stanford University wrote lis.py: an interpreter for a subset of the\nScheme dialect of the Lisp programming language in 132 lines of beautiful and reada‐\nble Python code. I took Norvig’s MIT-licensed source and updated it to Python 3.10\nto showcase pattern matching. In this section, we’ll compare a key part of Norvig’s\ncode—which uses if/elif and unpacking—with a rewrite using match/case.\nThe two main functions of lis.py are parse and evaluate.5 The parser takes Scheme\nparenthesized expressions and returns Python lists. Here are two examples:\n>>> parse('(gcd 18 45)')\n['gcd', 18, 45]\n>>> parse('''\n... (define double\n...     (lambda (n)\n...         (* n 2)))\n... ''')\n['define', 'double', ['lambda', ['n'], ['*', 'n', 2]]]\nThe evaluator takes lists like these and executes them. The first example is calling a\ngcd function with 18 and 45 as arguments. When evaluated, it computes the greatest\ncommon divisor of the arguments: 9. The second example is defining a function\nnamed double with a parameter n. The body of the function is the expression (* n\n2). The result of calling a function in Scheme is the value of the last expression in its\nbody.\nOur focus here is destructuring sequences, so I will not explain the evaluator actions.\nSee “Pattern Matching in lis.py: A Case Study” on page 669 to learn more about how\nlis.py works.\nExample 2-11 shows Norvig’s evaluator with minor changes, abbreviated to show\nonly the sequence patterns.\nPattern Matching with Sequences \n| \n43",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Example 2-11. Matching patterns without match/case\ndef evaluate(exp: Expression, env: Environment) -> Any:\n    \"Evaluate an expression in an environment.\"\n    if isinstance(exp, Symbol):      # variable reference\n        return env[exp]\n    # ... lines omitted\n    elif exp[0] == 'quote':          # (quote exp)\n        (_, x) = exp\n        return x\n    elif exp[0] == 'if':             # (if test conseq alt)\n        (_, test, consequence, alternative) = exp\n        if evaluate(test, env):\n            return evaluate(consequence, env)\n        else:\n            return evaluate(alternative, env)\n    elif exp[0] == 'lambda':         # (lambda (parm…) body…)\n        (_, parms, *body) = exp\n        return Procedure(parms, body, env)\n    elif exp[0] == 'define':\n        (_, name, value_exp) = exp\n        env[name] = evaluate(value_exp, env)\n    # ... more lines omitted\nNote how each elif clause checks the first item of the list, and then unpacks the list,\nignoring the first item. The extensive use of unpacking suggests that Norvig is a fan of\npattern matching, but he wrote that code originally for Python 2 (though it now\nworks with any Python 3).\nUsing match/case in Python ≥ 3.10, we can refactor evaluate as shown in\nExample 2-12.\nExample 2-12. Pattern matching with match/case—requires Python ≥ 3.10\ndef evaluate(exp: Expression, env: Environment) -> Any:\n    \"Evaluate an expression in an environment.\"\n    match exp:\n    # ... lines omitted\n        case ['quote', x]:  \n            return x\n        case ['if', test, consequence, alternative]:  \n            if evaluate(test, env):\n                return evaluate(consequence, env)\n            else:\n                return evaluate(alternative, env)\n        case ['lambda', [*parms], *body] if body:  \n            return Procedure(parms, body, env)\n        case ['define', Symbol() as name, value_exp]:  \n            env[name] = evaluate(value_exp, env)\n        # ... more lines omitted\n44 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "case _:  \n            raise SyntaxError(lispstr(exp))\nMatch if subject is a two-item sequence starting with 'quote'.\nMatch if subject is a four-item sequence starting with 'if'.\nMatch if subject is a sequence of three or more items starting with 'lambda'. The\nguard ensures that body is not empty.\nMatch if subject is a three-item sequence starting with 'define', followed by an\ninstance of Symbol.\nIt is good practice to have a catch-all case. In this example, if exp doesn’t match\nany of the patterns, the expression is malformed, and I raise SyntaxError.\nWithout a catch-all, the whole match statement does nothing when a subject does not\nmatch any case—and this can be a silent failure.\nNorvig deliberately avoided error checking in lis.py to keep the code easy to under‐\nstand. With pattern matching, we can add more checks and still keep it readable. For\nexample, in the 'define' pattern, the original code does not ensure that name is an\ninstance of Symbol—that would require an if block, an isinstance call, and more\ncode. Example 2-12 is shorter and safer than Example 2-11.\nAlternative patterns for lambda\nThis is the syntax of lambda in Scheme, using the syntactic convention that the suffix\n… means the element may appear zero or more times:\n(lambda (parms…) body1 body2…)\nA simple pattern for the lambda case 'lambda' would be this:\n       case ['lambda', parms, *body] if body:\nHowever, that matches any value in the parms position, including the first 'x' in this\ninvalid subject:\n['lambda', 'x', ['*', 'x', 2]]\nThe nested list after the lambda keyword in Scheme holds the names of the formal\nparameters for the function, and it must be a list even if it has only one element. It\nmay also be an empty list, if the function takes no parameters—like Python’s ran\ndom.random().\nIn Example 2-12, I made the 'lambda' pattern safer using a nested sequence pattern:\nPattern Matching with Sequences \n| \n45",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "case ['lambda', [*parms], *body] if body:\n            return Procedure(parms, body, env)\nIn a sequence pattern, * can appear only once per sequence. Here we have two\nsequences: the outer and the inner.\nAdding the characters [*] around parms made the pattern look more like the Scheme\nsyntax it handles, and gave us an additional structural check.\nShortcut syntax for function definition\nScheme has an alternative define syntax to create a named function without using a\nnested lambda. This is the syntax:\n(define (name parm…) body1 body2…)\nThe define keyword is followed by a list with the name of the new function and zero\nor more parameter names. After that list comes the function body with one or more\nexpressions.\nAdding these two lines to the match takes care of the implementation:\n        case ['define', [Symbol() as name, *parms], *body] if body:\n            env[name] = Procedure(parms, body, env)\nI’d place that case after the other define case in Example 2-12. The order between\nthe define cases is irrelevant in this example because no subject can match both of\nthese patterns: the second element must be a Symbol in the original define case, but\nit must be a sequence starting with a Symbol in the define shortcut for function\ndefinition.\nNow consider how much work we’d have adding support for this second define syn‐\ntax without the help of pattern matching in Example 2-11. The match statement does\na lot more than the switch in C-like languages.\nPattern matching is an example of declarative programming: the code describes\n“what” you want to match, instead of “how” to match it. The shape of the code fol‐\nlows the shape of the data, as Table 2-2 illustrates.\nTable 2-2. Some Scheme syntactic forms and case patterns to handle them\nScheme syntax\nSequence pattern\n(quote exp)\n['quote', exp]\n(if test conseq alt)\n['if', test, conseq, alt]\n(lambda (parms…) body1 body2…)\n['lambda', [*parms], *body] if body\n(define name exp)\n['define', Symbol() as name, exp]\n(define (name parms…) body1 \nbody2…)\n['define', [Symbol() as name, *parms], *body] \nif body\n46 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "I hope this refactoring of Norvig’s evaluate with pattern matching convinced you\nthat match/case can make your code more readable and safer.\nWe’ll see more of lis.py in “Pattern Matching in lis.py: A Case\nStudy” on page 669, when we’ll review the complete match/case\nexample in evaluate. If you want to learn more about Norvig’s\nlis.py, read his wonderful post “(How to Write a (Lisp) Interpreter\n(in Python))”.\nThis concludes our first tour of unpacking, destructuring, and pattern matching with\nsequences. We’ll cover other types of patterns in later chapters.\nEvery Python programmer knows that sequences can be sliced using the s[a:b] syn‐\ntax. We now turn to some less well-known facts about slicing.\nSlicing\nA common feature of list, tuple, str, and all sequence types in Python is the sup‐\nport of slicing operations, which are more powerful than most people realize.\nIn this section, we describe the use of these advanced forms of slicing. Their imple‐\nmentation in a user-defined class will be covered in Chapter 12, in keeping with our\nphilosophy of covering ready-to-use classes in this part of the book, and creating new\nclasses in Part III.\nWhy Slices and Ranges Exclude the Last Item\nThe Pythonic convention of excluding the last item in slices and ranges works well\nwith the zero-based indexing used in Python, C, and many other languages. Some\nconvenient features of the convention are:\n• It’s easy to see the length of a slice or range when only the stop position is given:\nrange(3) and my_list[:3] both produce three items.\n• It’s easy to compute the length of a slice or range when start and stop are given:\njust subtract stop - start.\n• It’s easy to split a sequence in two parts at any index x, without overlapping: sim‐\nply get my_list[:x] and my_list[x:]. For example:\n>>> l = [10, 20, 30, 40, 50, 60]\n>>> l[:2]  # split at 2\n[10, 20]\n>>> l[2:]\n[30, 40, 50, 60]\n>>> l[:3]  # split at 3\n[10, 20, 30]\nSlicing \n| \n47",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": ">>> l[3:]\n[40, 50, 60]\nThe best arguments for this convention were written by the Dutch computer scientist\nEdsger W. Dijkstra (see the last reference in “Further Reading” on page 71).\nNow let’s take a close look at how Python interprets slice notation.\nSlice Objects\nThis is no secret, but worth repeating just in case: s[a:b:c] can be used to specify a\nstride or step c, causing the resulting slice to skip items. The stride can also be nega‐\ntive, returning items in reverse. Three examples make this clear:\n>>> s = 'bicycle'\n>>> s[::3]\n'bye'\n>>> s[::-1]\n'elcycib'\n>>> s[::-2]\n'eccb'\nAnother example was shown in Chapter 1 when we used deck[12::13] to get all the\naces in the unshuffled deck:\n>>> deck[12::13]\n[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),\nCard(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]\nThe notation a:b:c is only valid within [] when used as the indexing or subscript\noperator, and it produces a slice object: slice(a, b, c). As we will see in “How Slic‐\ning Works” on page 404, to evaluate the expression seq[start:stop:step], Python\ncalls seq.__getitem__(slice(start, stop, step)). Even if you are not imple‐\nmenting your own sequence types, knowing about slice objects is useful because it\nlets you assign names to slices, just like spreadsheets allow naming of cell ranges.\nSuppose you need to parse flat-file data like the invoice shown in Example 2-13.\nInstead of filling your code with hardcoded slices, you can name them. See how read‐\nable this makes the for loop at the end of the example.\nExample 2-13. Line items from a flat-file invoice\n>>> invoice = \"\"\"\n... 0.....6.................................40........52...55........\n... 1909  Pimoroni PiBrella                     $17.50    3    $52.50\n... 1489  6mm Tactile Switch x20                 $4.95    2     $9.90\n... 1510  Panavise Jr. - PV-201                 $28.00    1    $28.00\n... 1601  PiTFT Mini Kit 320x240                $34.95    1    $34.95\n... \"\"\"\n48 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "6 In “Memory Views” on page 62 we show that specially constructed memory views can have more than one\ndimension.\n7 No, I did not get this backwards: the ellipsis class name is really all lowercase, and the instance is a built-in\nnamed Ellipsis, just like bool is lowercase but its instances are True and False.\n>>> SKU = slice(0, 6)\n>>> DESCRIPTION = slice(6, 40)\n>>> UNIT_PRICE = slice(40, 52)\n>>> QUANTITY =  slice(52, 55)\n>>> ITEM_TOTAL = slice(55, None)\n>>> line_items = invoice.split('\\n')[2:]\n>>> for item in line_items:\n...     print(item[UNIT_PRICE], item[DESCRIPTION])\n...\n    $17.50   Pimoroni PiBrella\n     $4.95   6mm Tactile Switch x20\n    $28.00   Panavise Jr. - PV-201\n    $34.95   PiTFT Mini Kit 320x240\nWe’ll come back to slice objects when we discuss creating your own collections in\n“Vector Take #2: A Sliceable Sequence” on page 403. Meanwhile, from a user perspec‐\ntive, slicing includes additional features such as multidimensional slices and ellipsis\n(...) notation. Read on.\nMultidimensional Slicing and Ellipsis\nThe [] operator can also take multiple indexes or slices separated by commas. The\n__getitem__ and __setitem__ special methods that handle the [] operator simply\nreceive the indices in a[i, j] as a tuple. In other words, to evaluate a[i, j], Python\ncalls a.__getitem__((i, j)).\nThis is used, for instance, in the external NumPy package, where items of a two-\ndimensional numpy.ndarray can be fetched using the syntax a[i, j] and a\ntwo-dimensional slice obtained with an expression like a[m:n, k:l]. Example 2-22\nlater in this chapter shows the use of this notation.\nExcept for memoryview, the built-in sequence types in Python are one-dimensional, so\nthey support only one index or slice, and not a tuple of them.6\nThe ellipsis—written with three full stops (...) and not … (Unicode U+2026)—is rec‐\nognized as a token by the Python parser. It is an alias to the Ellipsis object, the sin‐\ngle instance of the ellipsis class.7 As such, it can be passed as an argument to\nfunctions and as part of a slice specification, as in f(a, ..., z) or a[i:...].\nNumPy uses ... as a shortcut when slicing arrays of many dimensions; for example,\nSlicing \n| \n49",
      "content_length": 2181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "if x is a four-dimensional array, x[i, ...] is a shortcut for x[i, :, :, :,]. See\n“NumPy quickstart” to learn more about this.\nAt the time of this writing, I am unaware of uses of Ellipsis or multidimensional\nindexes and slices in the Python standard library. If you spot one, let me know. These\nsyntactic features exist to support user-defined types and extensions such as NumPy.\nSlices are not just useful to extract information from sequences; they can also be used\nto change mutable sequences in place—that is, without rebuilding them from scratch.\nAssigning to Slices\nMutable sequences can be grafted, excised, and otherwise modified in place using\nslice notation on the lefthand side of an assignment statement or as the target of a del\nstatement. The next few examples give an idea of the power of this notation:\n>>> l = list(range(10))\n>>> l\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> l[2:5] = [20, 30]\n>>> l\n[0, 1, 20, 30, 5, 6, 7, 8, 9]\n>>> del l[5:7]\n>>> l\n[0, 1, 20, 30, 5, 8, 9]\n>>> l[3::2] = [11, 22]\n>>> l\n[0, 1, 20, 11, 5, 22, 9]\n>>> l[2:5] = 100  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only assign an iterable\n>>> l[2:5] = [100]\n>>> l\n[0, 1, 100, 22, 9]\nWhen the target of the assignment is a slice, the righthand side must be an itera‐\nble object, even if it has just one item.\nEvery coder knows that concatenation is a common operation with sequences. Intro‐\nductory Python tutorials explain the use of + and * for that purpose, but there are\nsome subtle details on how they work, which we cover next.\nUsing + and * with Sequences\nPython programmers expect that sequences support + and *. Usually both operands\nof + must be of the same sequence type, and neither of them is modified, but a new\nsequence of that same type is created as result of the concatenation.\n50 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "To concatenate multiple copies of the same sequence, multiply it by an integer.\nAgain, a new sequence is created:\n>>> l = [1, 2, 3]\n>>> l * 5\n[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n>>> 5 * 'abcd'\n'abcdabcdabcdabcdabcd'\nBoth + and * always create a new object, and never change their operands.\nBeware of expressions like a * n when a is a sequence containing\nmutable items, because the result may surprise you. For example,\ntrying to initialize a list of lists as my_list = [[]] * 3 will result\nin a list with three references to the same inner list, which is proba‐\nbly not what you want.\nThe next section covers the pitfalls of trying to use * to initialize a list of lists.\nBuilding Lists of Lists\nSometimes we need to initialize a list with a certain number of nested lists—for\nexample, to distribute students in a list of teams or to represent squares on a game\nboard. The best way of doing so is with a list comprehension, as in Example 2-14.\nExample 2-14. A list with three lists of length 3 can represent a tic-tac-toe board\n>>> board = [['_'] * 3 for i in range(3)]  \n>>> board\n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]\n>>> board[1][2] = 'X'  \n>>> board\n[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]\nCreate a list of three lists of three items each. Inspect the structure.\nPlace a mark in row 1, column 2, and check the result.\nA tempting, but wrong, shortcut is doing it like Example 2-15.\nExample 2-15. A list with three references to the same list is useless\n>>> weird_board = [['_'] * 3] * 3  \n>>> weird_board\n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]\n>>> weird_board[1][2] = 'O' \nUsing + and * with Sequences \n| \n51",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": ">>> weird_board\n[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']]\nThe outer list is made of three references to the same inner list. While it is\nunchanged, all seems right.\nPlacing a mark in row 1, column 2, reveals that all rows are aliases referring to\nthe same object.\nThe problem with Example 2-15 is that, in essence, it behaves like this code:\nrow = ['_'] * 3\nboard = []\nfor i in range(3):\n    board.append(row)  \nThe same row is appended three times to board.\nOn the other hand, the list comprehension from Example 2-14 is equivalent to this\ncode:\n>>> board = []\n>>> for i in range(3):\n...     row = ['_'] * 3  \n...     board.append(row)\n...\n>>> board\n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]\n>>> board[2][0] = 'X'\n>>> board  \n[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']]\nEach iteration builds a new row and appends it to board.\nOnly row 2 is changed, as expected.\nIf either the problem or the solution in this section is not clear to\nyou, relax. Chapter 6 was written to clarify the mechanics and pit‐\nfalls of references and mutable objects.\nSo far we have discussed the use of the plain + and * operators with sequences, but\nthere are also the += and *= operators, which produce very different results, depend‐\ning on the mutability of the target sequence. The following section explains how that\nworks.\n52 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "Augmented Assignment with Sequences\nThe augmented assignment operators += and *= behave quite differently, depending\non the first operand. To simplify the discussion, we will focus on augmented addition\nfirst (+=), but the concepts also apply to *= and to other augmented assignment\noperators.\nThe special method that makes += work is __iadd__ (for “in-place addition”).\nHowever, if __iadd__ is not implemented, Python falls back to calling __add__. Con‐\nsider this simple expression:\n>>> a += b\nIf a implements __iadd__, that will be called. In the case of mutable sequences (e.g.,\nlist, bytearray, array.array), a will be changed in place (i.e., the effect will be sim‐\nilar to a.extend(b)). However, when a does not implement __iadd__, the expression\na += b has the same effect as a = a + b: the expression a + b is evaluated first,\nproducing a new object, which is then bound to a. In other words, the identity of\nthe object bound to a may or may not change, depending on the availability of\n__iadd__.\nIn general, for mutable sequences, it is a good bet that __iadd__ is implemented and\nthat += happens in place. For immutable sequences, clearly there is no way for that to\nhappen.\nWhat I just wrote about += also applies to *=, which is implemented via __imul__.\nThe __iadd__ and __imul__ special methods are discussed in Chapter 16. Here is a\ndemonstration of *= with a mutable sequence and then an immutable one:\n>>> l = [1, 2, 3]\n>>> id(l)\n4311953800  \n>>> l *= 2\n>>> l\n[1, 2, 3, 1, 2, 3]\n>>> id(l)\n4311953800  \n>>> t = (1, 2, 3)\n>>> id(t)\n4312681568  \n>>> t *= 2\n>>> id(t)\n4301348296  \nID of the initial list.\nAfter multiplication, the list is the same object, with new items appended.\nUsing + and * with Sequences \n| \n53",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "8 str is an exception to this description. Because string building with += in loops is so common in real codeba‐\nses, CPython is optimized for this use case. Instances of str are allocated in memory with extra room, so that\nconcatenation does not require copying the whole string every time.\n9 Thanks to Leonardo Rochael and Cesar Kawakami for sharing this riddle at the 2013 PythonBrasil\nConference.\n10 Readers suggested that the operation in the example can be done with t[2].extend([50,60]), without\nerrors. I am aware of that, but my intent is to show the strange behavior of the += operator in this case.\nID of the initial tuple.\nAfter multiplication, a new tuple was created.\nRepeated concatenation of immutable sequences is inefficient, because instead of just\nappending new items, the interpreter has to copy the whole target sequence to create\na new one with the new items concatenated.8\nWe’ve seen common use cases for +=. The next section shows an intriguing corner\ncase that highlights what “immutable” really means in the context of tuples.\nA += Assignment Puzzler\nTry to answer without using the console: what is the result of evaluating the two\nexpressions in Example 2-16?9\nExample 2-16. A riddle\n>>> t = (1, 2, [30, 40])\n>>> t[2] += [50, 60]\nWhat happens next? Choose the best answer:\nA. t becomes (1, 2, [30, 40, 50, 60]).\nB. TypeError is raised with the message 'tuple' object does not support item\nassignment.\nC. Neither.\nD. Both A and B.\nWhen I saw this, I was pretty sure the answer was B, but it’s actually D, “Both A and\nB”! Example 2-17 is the actual output from a Python 3.9 console.10\n54 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "Example 2-17. The unexpected result: item t2 is changed and an exception is raised\n>>> t = (1, 2, [30, 40])\n>>> t[2] += [50, 60]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment\n>>> t\n(1, 2, [30, 40, 50, 60])\nOnline Python Tutor is an awesome online tool to visualize how Python works in\ndetail. Figure 2-5 is a composite of two screenshots showing the initial and final states\nof the tuple t from Example 2-17.\nFigure 2-5. Initial and final state of the tuple assignment puzzler (diagram generated\nby Online Python Tutor).\nIf you look at the bytecode Python generates for the expression s[a] += b\n(Example 2-18), it becomes clear how that happens.\nExample 2-18. Bytecode for the expression s[a] += b\n>>> dis.dis('s[a] += b')\n  1           0 LOAD_NAME                0 (s)\n              3 LOAD_NAME                1 (a)\n              6 DUP_TOP_TWO\n              7 BINARY_SUBSCR                      \n              8 LOAD_NAME                2 (b)\n             11 INPLACE_ADD                        \n             12 ROT_THREE\n             13 STORE_SUBSCR                       \n             14 LOAD_CONST               0 (None)\n             17 RETURN_VALUE\nUsing + and * with Sequences \n| \n55",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "11 Receiver is the target of a method call, the object bound to self in the method body.\nPut the value of s[a] on TOS (Top Of Stack).\nPerform TOS += b. This succeeds if TOS refers to a mutable object (it’s a list, in\nExample 2-17).\nAssign s[a] = TOS. This fails if s is immutable (the t tuple in Example 2-17).\nThis example is quite a corner case—in 20 years using Python, I have never seen this\nstrange behavior actually bite somebody.\nI take three lessons from this:\n• Avoid putting mutable items in tuples.\n• Augmented assignment is not an atomic operation—we just saw it throwing an\nexception after doing part of its job.\n• Inspecting Python bytecode is not too difficult, and can be helpful to see what is\ngoing on under the hood.\nAfter witnessing the subtleties of using + and * for concatenation, we can change the\nsubject to another essential operation with sequences: sorting.\nlist.sort Versus the sorted Built-In\nThe list.sort method sorts a list in place—that is, without making a copy. It returns\nNone to remind us that it changes the receiver11 and does not create a new list. This is\nan important Python API convention: functions or methods that change an object in\nplace should return None to make it clear to the caller that the receiver was changed,\nand no new object was created. Similar behavior can be seen, for example, in the ran\ndom.shuffle(s) function, which shuffles the mutable sequence s in place, and\nreturns None.\nThe convention of returning None to signal in-place changes has a\ndrawback: we cannot cascade calls to those methods. In contrast,\nmethods that return new objects (e.g., all str methods) can be cas‐\ncaded in the fluent interface style. See Wikipedia’s “Fluent inter‐\nface” entry for further description of this topic.\nIn contrast, the built-in function sorted creates a new list and returns it. It accepts\nany iterable object as an argument, including immutable sequences and generators\n56 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "12 Python’s main sorting algorithm is named Timsort after its creator, Tim Peters. For a bit of Timsort trivia, see\nthe “Soapbox” on page 73.\n(see Chapter 17). Regardless of the type of iterable given to sorted, it always returns a\nnewly created list.\nBoth list.sort and sorted take two optional, keyword-only arguments:\nreverse\nIf True, the items are returned in descending order (i.e., by reversing the compar‐\nison of the items). The default is False.\nkey\nA one-argument function that will be applied to each item to produce its sorting\nkey. For example, when sorting a list of strings, key=str.lower can be used\nto perform a case-insensitive sort, and key=len will sort the strings by character\nlength. The default is the identity function (i.e., the items themselves are\ncompared).\nYou can also use the optional keyword parameter key with the\nmin() and max() built-ins and with other functions from the stan‐\ndard library (e.g., itertools.groupby() and heapq.nlargest()).\nHere are a few examples to clarify the use of these functions and keyword arguments.\nThe examples also demonstrate that Python’s sorting algorithm is stable (i.e., it pre‐\nserves the relative ordering of items that compare equally):12\n>>> fruits = ['grape', 'raspberry', 'apple', 'banana']\n>>> sorted(fruits)\n['apple', 'banana', 'grape', 'raspberry']  \n>>> fruits\n['grape', 'raspberry', 'apple', 'banana']  \n>>> sorted(fruits, reverse=True)\n['raspberry', 'grape', 'banana', 'apple']  \n>>> sorted(fruits, key=len)\n['grape', 'apple', 'banana', 'raspberry']  \n>>> sorted(fruits, key=len, reverse=True)\n['raspberry', 'banana', 'grape', 'apple']  \n>>> fruits\n['grape', 'raspberry', 'apple', 'banana']  \n>>> fruits.sort()                          \n>>> fruits\n['apple', 'banana', 'grape', 'raspberry']  \nlist.sort Versus the sorted Built-In \n| \n57",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "13 The words in this example are sorted alphabetically because they are 100% made of lowercase ASCII charac‐\nters. See the warning after the example.\nThis produces a new list of strings sorted alphabetically.13\nInspecting the original list, we see it is unchanged.\nThis is the previous “alphabetical” ordering, reversed.\nA new list of strings, now sorted by length. Because the sorting algorithm is\nstable, “grape” and “apple,” both of length 5, are in the original order.\nThese are the strings sorted by length in descending order. It is not the reverse of\nthe previous result because the sorting is stable, so again “grape” appears before\n“apple.”\nSo far, the ordering of the original fruits list has not changed.\nThis sorts the list in place, and returns None (which the console omits).\nNow fruits is sorted.\nBy default, Python sorts strings lexicographically by character code.\nThat means ASCII uppercase letters will come before lowercase let‐\nters, and non-ASCII characters are unlikely to be sorted in a sensi‐\nble way. “Sorting Unicode Text” on page 148 covers proper ways of\nsorting text as humans would expect.\nOnce your sequences are sorted, they can be very efficiently searched. A binary search\nalgorithm is already provided in the bisect module of the Python standard library.\nThat module also includes the bisect.insort function, which you can use to make\nsure that your sorted sequences stay sorted. You’ll find an illustrated introduction to\nthe bisect module in the “Managing Ordered Sequences with Bisect” post in the flu‐\nentpython.com companion website.\nMuch of what we have seen so far in this chapter applies to sequences in general, not\njust lists or tuples. Python programmers sometimes overuse the list type because it\nis so handy—I know I’ve done it. For example, if you are processing large lists of\nnumbers, you should consider using arrays instead. The remainder of the chapter is\ndevoted to alternatives to lists and tuples.\n58 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "14 First in, first out—the default behavior of queues.\nWhen a List Is Not the Answer\nThe list type is flexible and easy to use, but depending on specific requirements,\nthere are better options. For example, an array saves a lot of memory when you need\nto handle millions of floating-point values. On the other hand, if you are constantly\nadding and removing items from opposite ends of a list, it’s good to know that a\ndeque (double-ended queue) is a more efficient FIFO14 data structure.\nIf your code frequently checks whether an item is present in a col‐\nlection (e.g., item in my_collection), consider using a set for\nmy_collection, especially if it holds a large number of items. Sets\nare optimized for fast membership checking. They are also iterable,\nbut they are not sequences because the ordering of set items is\nunspecified. We cover them in Chapter 3.\nFor the remainder of this chapter, we discuss mutable sequence types that can replace\nlists in many cases, starting with arrays.\nArrays\nIf a list only contains numbers, an array.array is a more efficient replacement.\nArrays support all mutable sequence operations (including .pop, .insert,\nand .extend), as well as additional methods for fast loading and saving, such\nas .frombytes and .tofile.\nA Python array is as lean as a C array. As shown in Figure 2-1, an array of float\nvalues does not hold full-fledged float instances, but only the packed bytes repre‐\nsenting their machine values—similar to an array of double in the C language. When\ncreating an array, you provide a typecode, a letter to determine the underlying C\ntype used to store each item in the array. For example, b is the typecode for what\nC calls a signed char, an integer ranging from –128 to 127. If you create an\narray('b'), then each item will be stored in a single byte and interpreted as an inte‐\nger. For large sequences of numbers, this saves a lot of memory. And Python will not\nlet you put any number that does not match the type for the array.\nExample 2-19 shows creating, saving, and loading an array of 10 million floating-\npoint random numbers.\nWhen a List Is Not the Answer \n| \n59",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "Example 2-19. Creating, saving, and loading a large array of floats\n>>> from array import array  \n>>> from random import random\n>>> floats = array('d', (random() for i in range(10**7)))  \n>>> floats[-1]  \n0.07802343889111107\n>>> fp = open('floats.bin', 'wb')\n>>> floats.tofile(fp)  \n>>> fp.close()\n>>> floats2 = array('d')  \n>>> fp = open('floats.bin', 'rb')\n>>> floats2.fromfile(fp, 10**7)  \n>>> fp.close()\n>>> floats2[-1]  \n0.07802343889111107\n>>> floats2 == floats  \nTrue\nImport the array type.\nCreate an array of double-precision floats (typecode 'd') from any iterable object\n—in this case, a generator expression.\nInspect the last number in the array.\nSave the array to a binary file.\nCreate an empty array of doubles.\nRead 10 million numbers from the binary file.\nInspect the last number in the array.\nVerify that the contents of the arrays match.\nAs you can see, array.tofile and array.fromfile are easy to use. If you try the\nexample, you’ll notice they are also very fast. A quick experiment shows that it takes\nabout 0.1 seconds for array.fromfile to load 10 million double-precision floats\nfrom a binary file created with array.tofile. That is nearly 60 times faster than\nreading the numbers from a text file, which also involves parsing each line with the\nfloat built-in. Saving with array.tofile is about seven times faster than writing one\nfloat per line in a text file. In addition, the size of the binary file with 10 million dou‐\nbles is 80,000,000 bytes (8 bytes per double, zero overhead), while the text file has\n181,515,739 bytes for the same data.\n60 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "For the specific case of numeric arrays representing binary data, such as raster\nimages, Python has the bytes and bytearray types discussed in Chapter 4.\nWe wrap up this section on arrays with Table 2-3, comparing the features of list\nand array.array.\nTable 2-3. Methods and attributes found in list or array (deprecated array methods and\nthose also implemented by object are omitted for brevity)\nlist\narray\n \ns.__add__(s2)\n●\n●\ns + s2—concatenation\ns.__iadd__(s2)\n●\n●\ns += s2—in-place concatenation\ns.append(e)\n●\n●\nAppend one element after last\ns.byteswap()\n●\nSwap bytes of all items in array for endianness conversion\ns.clear()\n●\nDelete all items\ns.__contains__(e)\n●\n●\ne in s\ns.copy()\n●\nShallow copy of the list\ns.__copy__()\n●\nSupport for copy.copy\ns.count(e)\n●\n●\nCount occurrences of an element\ns.__deepcopy__()\n●\nOptimized support for copy.deepcopy\ns.__delitem__(p)\n●\n●\nRemove item at position p\ns.extend(it)\n●\n●\nAppend items from iterable it\ns.frombytes(b)\n●\nAppend items from byte sequence interpreted as packed machine values\ns.fromfile(f, n)\n●\nAppend n items from binary file f interpreted as packed machine values\ns.fromlist(l)\n●\nAppend items from list; if one causes TypeError, none are appended\ns.__getitem__(p)\n●\n●\ns[p]—get item or slice at position\ns.index(e)\n●\n●\nFind position of first occurrence of e\ns.insert(p, e)\n●\n●\nInsert element e before the item at position p\ns.itemsize\n●\nLength in bytes of each array item\ns.__iter__()\n●\n●\nGet iterator\ns.__len__()\n●\n●\nlen(s)—number of items\ns.__mul__(n)\n●\n●\ns * n—repeated concatenation\ns.__imul__(n)\n●\n●\ns *= n—in-place repeated concatenation\ns.__rmul__(n)\n●\n●\nn * s—reversed repeated concatenationa\ns.pop([p])\n●\n●\nRemove and return item at position p (default: last)\ns.remove(e)\n●\n●\nRemove first occurrence of element e by value\ns.reverse()\n●\n●\nReverse the order of the items in place\ns.__reversed__()\n●\nGet iterator to scan items from last to first\ns.__setitem__(p, e)\n●\n●\ns[p] = e—put e in position p, overwriting existing item or slice\nWhen a List Is Not the Answer \n| \n61",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "list\narray\n \ns.sort([key], [reverse]) ●\nSort items in place with optional keyword arguments key and\nreverse\ns.tobytes()\n●\nReturn items as packed machine values in a bytes object\ns.tofile(f)\n●\nSave items as packed machine values to binary file f\ns.tolist()\n●\nReturn items as numeric objects in a list\ns.typecode\n●\nOne-character string identifying the C type of the items\na Reversed operators are explained in Chapter 16.\nAs of Python 3.10, the array type does not have an in-place sort\nmethod like list.sort(). If you need to sort an array, use the\nbuilt-in sorted function to rebuild the array:\na = array.array(a.typecode, sorted(a))\nTo keep a sorted array sorted while adding items to it, use the\nbisect.insort function.\nIf you do a lot of work with arrays and don’t know about memoryview, you’re missing\nout. See the next topic.\nMemory Views\nThe built-in memoryview class is a shared-memory sequence type that lets you handle\nslices of arrays without copying bytes. It was inspired by the NumPy library (which\nwe’ll discuss shortly in “NumPy” on page 64). Travis Oliphant, lead author of NumPy,\nanswers the question, “When should a memoryview be used?” like this:\nA memoryview is essentially a generalized NumPy array structure in Python itself\n(without the math). It allows you to share memory between data-structures (things like\nPIL images, SQLite databases, NumPy arrays, etc.) without first copying. This is very\nimportant for large data sets.\nUsing notation similar to the array module, the memoryview.cast method lets you\nchange the way multiple bytes are read or written as units without moving bits\naround. memoryview.cast returns yet another memoryview object, always sharing the\nsame memory.\nExample 2-20 shows how to create alternate views on the same array of 6 bytes, to\noperate on it as a 2×3 matrix or a 3×2 matrix.\nExample 2-20. Handling 6 bytes of memory as 1×6, 2×3, and 3×2 views\n>>> from array import array\n>>> octets = array('B', range(6))  \n62 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": ">>> m1 = memoryview(octets)  \n>>> m1.tolist()\n[0, 1, 2, 3, 4, 5]\n>>> m2 = m1.cast('B', [2, 3])  \n>>> m2.tolist()\n[[0, 1, 2], [3, 4, 5]]\n>>> m3 = m1.cast('B', [3, 2])  \n>>> m3.tolist()\n[[0, 1], [2, 3], [4, 5]]\n>>> m2[1,1] = 22  \n>>> m3[1,1] = 33  \n>>> octets  \narray('B', [0, 1, 2, 33, 22, 5])\nBuild array of 6 bytes (typecode 'B').\nBuild memoryview from that array, then export it as a list.\nBuild new memoryview from that previous one, but with 2 rows and 3 columns.\nYet another memoryview, now with 3 rows and 2 columns.\nOverwrite byte in m2 at row 1, column 1 with 22.\nOverwrite byte in m3 at row 1, column 1 with 33.\nDisplay original array, proving that the memory was shared among octets, m1,\nm2, and m3.\nThe awesome power of memoryview can also be used to corrupt. Example 2-21 shows\nhow to change a single byte of an item in an array of 16-bit integers.\nExample 2-21. Changing the value of a 16-bit integer array item by poking one of its\nbytes\n>>> numbers = array.array('h', [-2, -1, 0, 1, 2])\n>>> memv = memoryview(numbers)  \n>>> len(memv)\n5\n>>> memv[0]  \n-2\n>>> memv_oct = memv.cast('B')  \n>>> memv_oct.tolist()  \n[254, 255, 255, 255, 0, 0, 1, 0, 2, 0]\n>>> memv_oct[5] = 4  \n>>> numbers\narray('h', [-2, -1, 1024, 1, 2])  \nWhen a List Is Not the Answer \n| \n63",
      "content_length": 1268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "Build memoryview from array of 5 16-bit signed integers (typecode 'h').\nmemv sees the same 5 items in the array.\nCreate memv_oct by casting the elements of memv to bytes (typecode 'B').\nExport elements of memv_oct as a list of 10 bytes, for inspection.\nAssign value 4 to byte offset 5.\nNote the change to numbers: a 4 in the most significant byte of a 2-byte unsigned\ninteger is 1024.\nYou’ll find an example of inspecting memoryview with the struct\npackage at fluentpython.com: “Parsing binary records with struct”.\nMeanwhile, if you are doing advanced numeric processing in arrays, you should be\nusing the NumPy libraries. We’ll take a brief look at them right away.\nNumPy\nThroughout this book, I make a point of highlighting what is already in the Python\nstandard library so you can make the most of it. But NumPy is so awesome that a\ndetour is warranted.\nFor advanced array and matrix operations, NumPy is the reason why Python became\nmainstream in scientific computing applications. NumPy implements multi-\ndimensional, homogeneous arrays and matrix types that hold not only numbers but\nalso user-defined records, and provides efficient element-wise operations.\nSciPy is a library, written on top of NumPy, offering many scientific computing algo‐\nrithms from linear algebra, numerical calculus, and statistics. SciPy is fast and reliable\nbecause it leverages the widely used C and Fortran codebase from the Netlib Reposi‐\ntory. In other words, SciPy gives scientists the best of both worlds: an interactive\nprompt and high-level Python APIs, together with industrial-strength number-\ncrunching functions optimized in C and Fortran.\nAs a very brief NumPy demo, Example 2-22 shows some basic operations with two-\ndimensional arrays.\n64 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Example 2-22. Basic operations with rows and columns in a numpy.ndarray\n>>> import numpy as np \n>>> a = np.arange(12)  \n>>> a\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n>>> type(a)\n<class 'numpy.ndarray'>\n>>> a.shape  \n(12,)\n>>> a.shape = 3, 4  \n>>> a\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n>>> a[2]  \narray([ 8,  9, 10, 11])\n>>> a[2, 1]  \n9\n>>> a[:, 1]  \narray([1, 5, 9])\n>>> a.transpose()  \narray([[ 0,  4,  8],\n       [ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11]])\nImport NumPy, after installing (it’s not in the Python standard library). Conven‐\ntionally, numpy is imported as np.\nBuild and inspect a numpy.ndarray with integers 0 to 11.\nInspect the dimensions of the array: this is a one-dimensional, 12-element array.\nChange the shape of the array, adding one dimension, then inspecting the result.\nGet row at index 2.\nGet element at index 2, 1.\nGet column at index 1.\nCreate a new array by transposing (swapping columns with rows).\nNumPy also supports high-level operations for loading, saving, and operating on all\nelements of a numpy.ndarray:\n>>> import numpy\n>>> floats = numpy.loadtxt('floats-10M-lines.txt')  \nWhen a List Is Not the Answer \n| \n65",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": ">>> floats[-3:]  \narray([ 3016362.69195522,   535281.10514262,  4566560.44373946])\n>>> floats *= .5  \n>>> floats[-3:]\narray([ 1508181.34597761,   267640.55257131,  2283280.22186973])\n>>> from time import perf_counter as pc \n>>> t0 = pc(); floats /= 3; pc() - t0 \n0.03690556302899495\n>>> numpy.save('floats-10M', floats)  \n>>> floats2 = numpy.load('floats-10M.npy', 'r+')  \n>>> floats2 *= 6\n>>> floats2[-3:]  \nmemmap([ 3016362.69195522,   535281.10514262,  4566560.44373946])\nLoad 10 million floating-point numbers from a text file.\nUse sequence slicing notation to inspect the last three numbers.\nMultiply every element in the floats array by .5 and inspect the last three\nelements again.\nImport the high-resolution performance measurement timer (available since\nPython 3.3).\nDivide every element by 3; the elapsed time for 10 million floats is less than 40\nmilliseconds.\nSave the array in a .npy binary file.\nLoad the data as a memory-mapped file into another array; this allows efficient\nprocessing of slices of the array even if it does not fit entirely in memory.\nInspect the last three elements after multiplying every element by 6.\nThis was just an appetizer.\nNumPy and SciPy are formidable libraries, and are the foundation of other awesome\ntools such as the Pandas—which implements efficient array types that can hold non‐\nnumeric data and provides import/export functions for many different formats,\nlike .csv, .xls, SQL dumps, HDF5, etc.—and scikit-learn, currently the most widely\nused Machine Learning toolset. Most NumPy and SciPy functions are implemented\nin C or C++, and can leverage all CPU cores because they release Python’s GIL\n(Global Interpreter Lock). The Dask project supports parallelizing NumPy, Pandas,\nand scikit-learn processing across clusters of machines. These packages deserve entire\nbooks about them. This is not one of those books. But no overview of Python sequen‐\nces would be complete without at least a quick look at NumPy arrays.\n66 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "Having looked at flat sequences—standard arrays and NumPy arrays—we now turn\nto a completely different set of replacements for the plain old list: queues.\nDeques and Other Queues\nThe .append and .pop methods make a list usable as a stack or a queue (if you\nuse .append and .pop(0), you get FIFO behavior). But inserting and removing from\nthe head of a list (the 0-index end) is costly because the entire list must be shifted in\nmemory.\nThe class collections.deque is a thread-safe double-ended queue designed for fast\ninserting and removing from both ends. It is also the way to go if you need to keep a\nlist of “last seen items” or something of that nature, because a deque can be bounded\n—i.e., created with a fixed maximum length. If a bounded deque is full, when you add\na new item, it discards an item from the opposite end. Example 2-23 shows some typ‐\nical operations performed on a deque.\nExample 2-23. Working with a deque\n>>> from collections import deque\n>>> dq = deque(range(10), maxlen=10)  \n>>> dq\ndeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)\n>>> dq.rotate(3)  \n>>> dq\ndeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)\n>>> dq.rotate(-4)\n>>> dq\ndeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)\n>>> dq.appendleft(-1)  \n>>> dq\ndeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)\n>>> dq.extend([11, 22, 33])  \n>>> dq\ndeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)\n>>> dq.extendleft([10, 20, 30, 40])  \n>>> dq\ndeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)\nThe optional maxlen argument sets the maximum number of items allowed in\nthis instance of deque; this sets a read-only maxlen instance attribute.\nRotating with n > 0 takes items from the right end and prepends them to the\nleft; when n < 0 items are taken from left and appended to the right.\nWhen a List Is Not the Answer \n| \n67",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Appending to a deque that is full (len(d) == d.maxlen) discards items from the\nother end; note in the next line that the 0 is dropped.\nAdding three items to the right pushes out the leftmost -1, 1, and 2.\nNote that extendleft(iter) works by appending each successive item of the\niter argument to the left of the deque, therefore the final position of the items is\nreversed.\nTable 2-4 compares the methods that are specific to list and deque (removing those\nthat also appear in object).\nNote that deque implements most of the list methods, and adds a few that are spe‐\ncific to its design, like popleft and rotate. But there is a hidden cost: removing\nitems from the middle of a deque is not as fast. It is really optimized for appending\nand popping from the ends.\nThe append and popleft operations are atomic, so deque is safe to use as a FIFO\nqueue in multithreaded applications without the need for locks.\nTable 2-4. Methods implemented in list or deque (those that are also implemented by\nobject are omitted for brevity)\nlist\ndeque  \ns.__add__(s2)\n●\ns + s2—concatenation\ns.__iadd__(s2)\n●\n●\ns += s2—in-place concatenation\ns.append(e)\n●\n●\nAppend one element to the right (after last)\ns.appendleft(e)\n●\nAppend one element to the left (before first)\ns.clear()\n●\n●\nDelete all items\ns.__contains__(e)\n●\ne in s\ns.copy()\n●\nShallow copy of the list\ns.__copy__()\n●\nSupport for copy.copy (shallow copy)\ns.count(e)\n●\n●\nCount occurrences of an element\ns.__delitem__(p)\n●\n●\nRemove item at position p\ns.extend(i)\n●\n●\nAppend items from iterable i to the right\ns.extendleft(i)\n●\nAppend items from iterable i to the left\ns.__getitem__(p)\n●\n●\ns[p]—get item or slice at position\ns.index(e)\n●\nFind position of first occurrence of e\ns.insert(p, e)\n●\nInsert element e before the item at position p\ns.__iter__()\n●\n●\nGet iterator\ns.__len__()\n●\n●\nlen(s)—number of items\n68 \n| \nChapter 2: An Array of Sequences",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "list\ndeque  \ns.__mul__(n)\n●\ns * n—repeated concatenation\ns.__imul__(n)\n●\ns *= n—in-place repeated concatenation\ns.__rmul__(n)\n●\nn * s—reversed repeated concatenationa\ns.pop()\n●\n●\nRemove and return last itemb\ns.popleft()\n●\nRemove and return first item\ns.remove(e)\n●\n●\nRemove first occurrence of element e by value\ns.reverse()\n●\n●\nReverse the order of the items in place\ns.__reversed__()\n●\n●\nGet iterator to scan items from last to first\ns.rotate(n)\n●\nMove n items from one end to the other\ns.__setitem__(p, e)\n●\n●\ns[p] = e—put e in position p, overwriting existing item or slice\ns.sort([key], [reverse]) ●\nSort items in place with optional keyword arguments key and\nreverse\na Reversed operators are explained in Chapter 16.\nb a_list.pop(p) allows removing from position p, but deque does not support that option.\nBesides deque, other Python standard library packages implement queues:\nqueue\nThis provides the synchronized (i.e., thread-safe) classes SimpleQueue, Queue,\nLifoQueue, and PriorityQueue. These can be used for safe communication\nbetween threads. All except SimpleQueue can be bounded by providing a max\nsize argument greater than 0 to the constructor. However, they don’t discard\nitems to make room as deque does. Instead, when the queue is full, the insertion\nof a new item blocks—i.e., it waits until some other thread makes room by taking\nan item from the queue, which is useful to throttle the number of live threads.\nmultiprocessing\nImplements its own unbounded SimpleQueue and bounded Queue, very similar\nto those in the queue package, but designed for interprocess communication. A\nspecialized multiprocessing.JoinableQueue is provided for task management.\nasyncio\nProvides Queue, LifoQueue, PriorityQueue, and JoinableQueue with APIs\ninspired by the classes in the queue and multiprocessing modules, but adapted\nfor managing tasks in asynchronous programming.\nheapq\nIn contrast to the previous three modules, heapq does not implement a queue\nclass, but provides functions like heappush and heappop that let you use a muta‐\nble sequence as a heap queue or priority queue.\nWhen a List Is Not the Answer \n| \n69",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "This ends our overview of alternatives to the list type, and also our exploration of\nsequence types in general—except for the particulars of str and binary sequences,\nwhich have their own chapter (Chapter 4).\nChapter Summary\nMastering the standard library sequence types is a prerequisite for writing concise,\neffective, and idiomatic Python code.\nPython sequences are often categorized as mutable or immutable, but it is also useful\nto consider a different axis: flat sequences and container sequences. The former are\nmore compact, faster, and easier to use, but are limited to storing atomic data such as\nnumbers, characters, and bytes. Container sequences are more flexible, but may sur‐\nprise you when they hold mutable objects, so you need to be careful to use them cor‐\nrectly with nested data structures.\nUnfortunately, Python has no foolproof immutable container sequence type: even\n“immutable” tuples can have their values changed when they contain mutable items\nlike lists or user-defined objects.\nList comprehensions and generator expressions are powerful notations to build and\ninitialize sequences. If you are not yet comfortable with them, take the time to master\ntheir basic usage. It is not hard, and soon you will be hooked.\nTuples in Python play two roles: as records with unnamed fields and as immutable\nlists. When using a tuple as an immutable list, remember that a tuple value is only\nguaranteed to be fixed if all the items in it are also immutable. Calling hash(t) on a\ntuple is a quick way to assert that its value is fixed. A TypeError will be raised if t\ncontains mutable items.\nWhen a tuple is used as a record, tuple unpacking is the safest, most readable way of\nextracting the fields of the tuple. Beyond tuples, * works with lists and iterables in\nmany contexts, and some of its use cases appeared in Python 3.5 with PEP 448—\nAdditional Unpacking Generalizations. Python 3.10 introduced pattern matching\nwith match/case, supporting more powerful unpacking, known as destructuring.\nSequence slicing is a favorite Python syntax feature, and it is even more powerful\nthan many realize. Multidimensional slicing and ellipsis (...) notation, as used in\nNumPy, may also be supported by user-defined sequences. Assigning to slices is a\nvery expressive way of editing mutable sequences.\nRepeated concatenation as in seq * n is convenient and, with care, can be used to\ninitialize lists of lists containing immutable items. Augmented assignment with +=\nand *= behaves differently for mutable and immutable sequences. In the latter case,\nthese operators necessarily build new sequences. But if the target sequence is\n70 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "mutable, it is usually changed in place—but not always, depending on how the\nsequence is implemented.\nThe sort method and the sorted built-in function are easy to use and flexible, thanks\nto the optional key argument: a function to calculate the ordering criterion. By the\nway, key can also be used with the min and max built-in functions.\nBeyond lists and tuples, the Python standard library provides array.array. Although\nNumPy and SciPy are not part of the standard library, if you do any kind of numeri‐\ncal processing on large sets of data, studying even a small part of these libraries can\ntake you a long way.\nWe closed by visiting the versatile and thread-safe collections.deque, comparing its\nAPI with that of list in Table 2-4 and mentioning other queue implementations in\nthe standard library.\nFurther Reading\nChapter 1, “Data Structures,” of the Python Cookbook, 3rd ed. (O’Reilly) by David\nBeazley and Brian K. Jones, has many recipes focusing on sequences, including\n“Recipe 1.11. Naming a Slice,” from which I learned the trick of assigning slices to\nvariables to improve readability, illustrated in our Example 2-13.\nThe second edition of the Python Cookbook was written for Python 2.4, but much of\nits code works with Python 3, and a lot of the recipes in Chapters 5 and 6 deal with\nsequences. The book was edited by Alex Martelli, Anna Ravenscroft, and David\nAscher, and it includes contributions by dozens of Pythonistas. The third edition was\nrewritten from scratch, and focuses more on the semantics of the language—particu‐\nlarly what has changed in Python 3—while the older volume emphasizes pragmatics\n(i.e., how to apply the language to real-world problems). Even though some of the\nsecond edition solutions are no longer the best approach, I honestly think it is worth‐\nwhile to have both editions of the Python Cookbook on hand.\nThe official Python “Sorting HOW TO” has several examples of advanced tricks for\nusing sorted and list.sort.\nPEP 3132—Extended Iterable Unpacking is the canonical source to read about the\nnew use of *extra syntax on the lefthand side of parallel assignments. If you’d like a\nglimpse of Python evolving, “Missing *-unpacking generalizations” is a bug tracker\nissue proposing enhancements to the iterable unpacking notation. PEP 448—Addi‐\ntional Unpacking Generalizations resulted from the discussions in that issue.\nFurther Reading \n| \n71",
      "content_length": 2393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "As I mentioned in “Pattern Matching with Sequences” on page 38, Carol Willing’s\n“Structural Pattern Matching” section of “What’s New In Python 3.10” is a great\nintroduction to this major new feature in about 1,400 words (that’s less than 5 pages\nwhen Firefox makes a PDF from the HTML). PEP 636—Structural Pattern Matching:\nTutorial is also good, but longer. The same PEP 636 includes “Appendix A—Quick\nIntro”. It is shorter than Willing’s intro because it omits high-level considerations\nabout why pattern matching is good for you. If you need more arguments to con‐\nvince yourself or others that pattern matching is good for Python, read the 22-page\nPEP 635—Structural Pattern Matching: Motivation and Rationale.\nEli Bendersky’s blog post “Less copies in Python with the buffer protocol and memo‐\nryviews” includes a short tutorial on memoryview.\nThere are numerous books covering NumPy in the market, and many don’t mention\n“NumPy” in the title. Two examples are the open access Python Data Science Hand‐\nbook by Jake VanderPlas, and the second edition of Wes McKinney’s Python for Data\nAnalysis.\n“NumPy is all about vectorization.” That is the opening sentence of Nicolas P. Rou‐\ngier’s open access book From Python to NumPy. Vectorized operations apply mathe‐\nmatical functions to all elements of an array without an explicit loop written in\nPython. They can operate in parallel, using special vector instructions in modern\nCPUs, leveraging multiple cores or delegating to the GPU, depending on the library.\nThe first example in Rougier’s book shows a speedup of 500 times after refactoring a\nnice Pythonic class using a generator method, into a lean and mean function calling a\ncouple of NumPy vector functions.\nTo learn how to use deque (and other collections), see the examples and practical rec‐\nipes in “Container datatypes” in the Python documentation.\nThe best defense of the Python convention of excluding the last item in ranges and\nslices was written by Edsger W. Dijkstra himself, in a short memo titled “Why Num‐\nbering Should Start at Zero”. The subject of the memo is mathematical notation, but\nit’s relevant to Python because Dijkstra explains with rigor and humor why a\nsequence like 2, 3, …, 12 should always be expressed as 2 ≤ i < 13. All other reason‐\nable conventions are refuted, as is the idea of letting each user choose a convention.\nThe title refers to zero-based indexing, but the memo is really about why it is desira‐\nble that 'ABCDE'[1:3] means 'BC' and not 'BCD' and why it makes perfect sense to\nwrite range(2, 13) to produce 2, 3, 4, …, 12. By the way, the memo is a handwritten\nnote, but it’s beautiful and totally readable. Dijkstra’s handwriting is so clear that\nsomeone created a font out of his notes.\n72 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Soapbox\nThe Nature of Tuples\nIn 2012, I presented a poster about the ABC language at PyCon US. Before creating\nPython, Guido van Rossum had worked on the ABC interpreter, so he came to see\nmy poster. Among other things, we talked about the ABC compounds, which are\nclearly the predecessors of Python tuples. Compounds also support parallel assign‐\nment and are used as composite keys in dictionaries (or tables, in ABC parlance).\nHowever, compounds are not sequences. They are not iterable and you cannot\nretrieve a field by index, much less slice them. You either handle the compound as\nwhole or extract the individual fields using parallel assignment, that’s all.\nI told Guido that these limitations make the main purpose of compounds very clear:\nthey are just records without field names. His response: “Making tuples behave as\nsequences was a hack.”\nThis illustrates the pragmatic approach that made Python more practical and more\nsuccessful than ABC. From a language implementer perspective, making tuples\nbehave as sequences costs little. As a result, the main use case for tuples as records is\nnot so obvious, but we gained immutable lists—even if their type is not as clearly\nnamed as frozenlist.\nFlat Versus Container Sequences\nTo highlight the different memory models of the sequence types, I used the terms\ncontainer sequence and flat sequence. The “container” word is from the “Data Model”\ndocumentation:\nSome objects contain references to other objects; these are called containers.\nI used the term “container sequence” to be specific, because there are containers in\nPython that are not sequences, like dict and set. Container sequences can be nested\nbecause they may contain objects of any type, including their own type.\nOn the other hand, flat sequences are sequence types that cannot be nested because\nthey only hold simple atomic types like integers, floats, or characters.\nI adopted the term flat sequence because I needed something to contrast with “con‐\ntainer sequence.”\nDespite the previous use of the word “containers” in the official documentation, there\nis an abstract class in collections.abc called Container. That ABC has just one\nmethod, __contains__—the special method behind the in operator. This means that\nstrings and arrays, which are not containers in the traditional sense, are virtual sub‐\nclasses of Container because they implement __contains__. This is just one more\nexample of humans using a word to mean different things. In this book I’ll write\n“container” with lowercase letters to mean “an object that contains references to\nFurther Reading \n| \n73",
      "content_length": 2594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "other objects,” and Container with a capitalized initial in a single-spaced font to refer\nto collections.abc.Container.\nMixed-Bag Lists\nIntroductory Python texts emphasize that lists can contain objects of mixed types, but\nin practice that feature is not very useful: we put items in a list to process them later,\nwhich implies that all items should support at least some operation in common (i.e.,\nthey should all “quack” whether or not they are genetically 100% ducks). For exam‐\nple, you can’t sort a list in Python 3 unless the items in it are comparable:\n>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19]\n>>> sorted(l)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unorderable types: str() < int()\nUnlike lists, tuples often hold items of different types. That’s natural: if each item in a\ntuple is a field, then each field may have a different type.\nkey Is Brilliant\nThe optional key argument of list.sort, sorted, max, and min is a great idea. Other\nlanguages force you to provide a two-argument comparison function like the depre‐\ncated cmp(a, b) function in Python 2. Using key is both simpler and more efficient.\nIt’s simpler because you just define a one-argument function that retrieves or calcu‐\nlates whatever criterion you want to use to sort your objects; this is easier than writing\na two-argument function to return –1, 0, 1. It is also more efficient because the key\nfunction is invoked only once per item, while the two-argument comparison is called\nevery time the sorting algorithm needs to compare two items. Of course, Python also\nhas to compare the keys while sorting, but that comparison is done in optimized C\ncode and not in a Python function that you wrote.\nBy the way, using key we can sort a mixed bag of numbers and number-like strings.\nWe just need to decide whether we want to treat all items as integers or strings:\n>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19]\n>>> sorted(l, key=int)\n[0, '1', 5, 6, '9', 14, 19, '23', 28, '28']\n>>> sorted(l, key=str)\n[0, '1', 14, 19, '23', 28, '28', 5, 6, '9']\nOracle, Google, and the Timbot Conspiracy\nThe sorting algorithm used in sorted and list.sort is Timsort, an adaptive algo‐\nrithm that switches from insertion sort to merge sort strategies, depending on how\nordered the data is. This is efficient because real-world data tends to have runs of sor‐\nted items. There is a Wikipedia article about it.\n74 \n| \nChapter 2: An Array of Sequences",
      "content_length": 2461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Timsort was first used in CPython in 2002. Since 2009, Timsort is also used to sort\narrays in both standard Java and Android, a fact that became widely known when\nOracle used some of the code related to Timsort as evidence of Google infringement\nof Sun’s intellectual property. For example, see this order by Judge William Alsup\nfrom 2012. In 2021, the US Supreme Court ruled Google’s use of Java code as “fair\nuse.”\nTimsort was invented by Tim Peters, a Python core developer so prolific that he is\nbelieved to be an AI, the Timbot. You can read about that conspiracy theory in\n“Python Humor”. Tim also wrote “The Zen of Python”: import this.\nFurther Reading \n| \n75",
      "content_length": 666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "CHAPTER 3\nDictionaries and Sets\nPython is basically dicts wrapped in loads of syntactic sugar.\n—Lalo Martins, early digital nomad and Pythonista\nWe use dictionaries in all our Python programs. If not directly in our code, then indi‐\nrectly because the dict type is a fundamental part of Python’s implementation. Class\nand instance attributes, module namespaces, and function keyword arguments are\nsome of the core Python constructs represented by dictionaries in memory. The\n__builtins__.__dict__ stores all built-in types, objects, and functions.\nBecause of their crucial role, Python dicts are highly optimized—and continue to get\nimprovements. Hash tables are the engines behind Python’s high-performance dicts.\nOther built-in types based on hash tables are set and frozenset. These offer richer\nAPIs and operators than the sets you may have encountered in other popular lan‐\nguages. In particular, Python sets implement all the fundamental operations from set\ntheory, like union, intersection, subset tests, etc. With them, we can express algo‐\nrithms in a more declarative way, avoiding lots of nested loops and conditionals.\nHere is a brief outline of this chapter:\n• Modern syntax to build and handle dicts and mappings, including enhanced\nunpacking and pattern matching\n• Common methods of mapping types\n• Special handling for missing keys\n• Variations of dict in the standard library\n• The set and frozenset types\n• Implications of hash tables in the behavior of sets and dictionaries\n77",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "What’s New in This Chapter\nMost changes in this second edition cover new features related to mapping types:\n• “Modern dict Syntax” on page 78 covers enhanced unpacking syntax and different\nways of merging mappings—including the | and |= operators supported by\ndicts since Python 3.9.\n• “Pattern Matching with Mappings” on page 81 illustrates handling mappings with\nmatch/case, since Python 3.10.\n• “collections.OrderedDict” on page 95 now focuses on the small but still relevant\ndifferences between dict and OrderedDict—considering that dict keeps the key\ninsertion order since Python 3.6.\n• New sections on the view objects returned by dict.keys, dict.items, and\ndict.values: “Dictionary Views” on page 101 and “Set Operations on dict Views”\non page 110.\nThe underlying implementation of dict and set still relies on hash tables, but the\ndict code has two important optimizations that save memory and preserve the inser‐\ntion order of the keys in dict. “Practical Consequences of How dict Works” on page\n102 and “Practical Consequences of How Sets Work” on page 107 summarize what you\nneed to know to use them well.\nAfter adding more than 200 pages in this second edition, I moved\nthe optional section “Internals of sets and dicts” to the fluentpy‐\nthon.com companion website. The updated and expanded 18-page\npost includes explanations and diagrams about:\n• The hash table algorithm and data structures, starting with its\nuse in set, which is simpler to understand.\n• The memory optimization that preserves key insertion order\nin dict instances (since Python 3.6).\n• The key-sharing layout for dictionaries holding instance\nattributes—the __dict__ of user-defined objects (optimiza‐\ntion implemented in Python 3.3).\nModern dict Syntax\nThe next sections describe advanced syntax features to build, unpack, and process\nmappings. Some of these features are not new in the language, but may be new to\nyou. Others require Python 3.9 (like the | operator) or Python 3.10 (like match/\ncase). Let’s start with one of the best and oldest of these features.\n78 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "dict Comprehensions\nSince Python 2.7, the syntax of listcomps and genexps was adapted to dict compre‐\nhensions (and set comprehensions as well, which we’ll soon visit). A dictcomp (dict\ncomprehension) builds a dict instance by taking key:value pairs from any iterable.\nExample 3-1 shows the use of dict comprehensions to build two dictionaries from\nthe same list of tuples.\nExample 3-1. Examples of dict comprehensions\n>>> dial_codes = [                                                  \n...     (880, 'Bangladesh'),\n...     (55,  'Brazil'),\n...     (86,  'China'),\n...     (91,  'India'),\n...     (62,  'Indonesia'),\n...     (81,  'Japan'),\n...     (234, 'Nigeria'),\n...     (92,  'Pakistan'),\n...     (7,   'Russia'),\n...     (1,   'United States'),\n... ]\n>>> country_dial = {country: code for code, country in dial_codes}  \n>>> country_dial\n{'Bangladesh': 880, 'Brazil': 55, 'China': 86, 'India': 91, 'Indonesia': 62,\n'Japan': 81, 'Nigeria': 234, 'Pakistan': 92, 'Russia': 7, 'United States': 1}\n>>> {code: country.upper()                                          \n...     for country, code in sorted(country_dial.items())\n...     if code < 70}\n{55: 'BRAZIL', 62: 'INDONESIA', 7: 'RUSSIA', 1: 'UNITED STATES'}\nAn iterable of key-value pairs like dial_codes can be passed directly to the dict\nconstructor, but…\n…here we swap the pairs: country is the key, and code is the value.\nSorting country_dial by name, reversing the pairs again, uppercasing values,\nand filtering items with code < 70.\nIf you’re used to listcomps, dictcomps are a natural next step. If you aren’t, the spread\nof the comprehension syntax means it’s now more profitable than ever to become\nfluent in it.\nModern dict Syntax \n| \n79",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "Unpacking Mappings\nPEP 448—Additional Unpacking Generalizations enhanced the support of mapping\nunpackings in two ways, since Python 3.5.\nFirst, we can apply ** to more than one argument in a function call. This works when\nkeys are all strings and unique across all arguments (because duplicate keyword argu‐\nments are forbidden):\n>>> def dump(**kwargs):\n...     return kwargs\n...\n>>> dump(**{'x': 1}, y=2, **{'z': 3})\n{'x': 1, 'y': 2, 'z': 3}\nSecond, ** can be used inside a dict literal—also multiple times:\n>>> {'a': 0, **{'x': 1}, 'y': 2, **{'z': 3, 'x': 4}}\n{'a': 0, 'x': 4, 'y': 2, 'z': 3}\nIn this case, duplicate keys are allowed. Later occurrences overwrite previous ones—\nsee the value mapped to x in the example.\nThis syntax can also be used to merge mappings, but there are other ways. Please read\non.\nMerging Mappings with |\nPython 3.9 supports using | and |= to merge mappings. This makes sense, since these\nare also the set union operators.\nThe | operator creates a new mapping:\n>>> d1 = {'a': 1, 'b': 3}\n>>> d2 = {'a': 2, 'b': 4, 'c': 6}\n>>> d1 | d2\n{'a': 2, 'b': 4, 'c': 6}\nUsually the type of the new mapping will be the same as the type of the left operand\n—d1 in the example—but it can be the type of the second operand if user-defined\ntypes are involved, according to the operator overloading rules we explore in\nChapter 16.\nTo update an existing mapping in place, use |=. Continuing from the previous exam‐\nple, d1 was not changed, but now it is:\n>>> d1\n{'a': 1, 'b': 3}\n>>> d1 |= d2\n>>> d1\n{'a': 2, 'b': 4, 'c': 6}\n80 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "1 A virtual subclass is any class registered by calling the .register() method of an ABC, as explained in “A\nVirtual Subclass of an ABC” on page 460. A type implemented via Python/C API is also eligible if a specific\nmarker bit is set. See Py_TPFLAGS_MAPPING.\nIf you need to maintain code to run on Python 3.8 or earlier, the\n“Motivation” section of PEP 584—Add Union Operators To dict\nprovides a good summary of other ways to merge mappings.\nNow let’s see how pattern matching applies to mappings.\nPattern Matching with Mappings\nThe match/case statement supports subjects that are mapping objects. Patterns for\nmappings look like dict literals, but they can match instances of any actual or virtual\nsubclass of collections.abc.Mapping.1\nIn Chapter 2 we focused on sequence patterns only, but different types of patterns\ncan be combined and nested. Thanks to destructuring, pattern matching is a power‐\nful tool to process records structured like nested mappings and sequences, which we\noften need to read from JSON APIs and databases with semi-structured schemas, like\nMongoDB, EdgeDB, or PostgreSQL. Example 3-2 demonstrates that. The simple type\nhints in get_creators make it clear that it takes a dict and returns a list.\nExample 3-2. creator.py: get_creators() extracts names of creators from media\nrecords\ndef get_creators(record: dict) -> list:\n    match record:\n        case {'type': 'book', 'api': 2, 'authors': [*names]}:  \n            return names\n        case {'type': 'book', 'api': 1, 'author': name}:  \n            return [name]\n        case {'type': 'book'}:  \n            raise ValueError(f\"Invalid 'book' record: {record!r}\")\n        case {'type': 'movie', 'director': name}:  \n            return [name]\n        case _:  \n            raise ValueError(f'Invalid record: {record!r}')\nPattern Matching with Mappings \n| \n81",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Match any mapping with 'type': 'book', 'api' :2, and an 'authors' key\nmapped to a sequence. Return the items in the sequence, as a new list.\nMatch any mapping with 'type': 'book', 'api' :1, and an 'author' key\nmapped to any object. Return the object inside a list.\nAny other mapping with 'type': 'book' is invalid, raise ValueError.\nMatch any mapping with 'type': 'movie' and a 'director' key mapped to a\nsingle object. Return the object inside a list.\nAny other subject is invalid, raise ValueError.\nExample 3-2 shows some useful practices for handling semi-structured data such as\nJSON records:\n• Include a field describing the kind of record (e.g., 'type': 'movie')\n• Include a field identifying the schema version (e.g., 'api': 2') to allow for\nfuture evolution of public APIs\n• Have case clauses to handle invalid records of a specific type (e.g., 'book'), as\nwell as a catch-all\nNow let’s see how get_creators handles some concrete doctests:\n>>> b1 = dict(api=1, author='Douglas Hofstadter',\n...         type='book', title='Gödel, Escher, Bach')\n>>> get_creators(b1)\n['Douglas Hofstadter']\n>>> from collections import OrderedDict\n>>> b2 = OrderedDict(api=2, type='book',\n...         title='Python in a Nutshell',\n...         authors='Martelli Ravenscroft Holden'.split())\n>>> get_creators(b2)\n['Martelli', 'Ravenscroft', 'Holden']\n>>> get_creators({'type': 'book', 'pages': 770})\nTraceback (most recent call last):\n    ...\nValueError: Invalid 'book' record: {'type': 'book', 'pages': 770}\n>>> get_creators('Spam, spam, spam')\nTraceback (most recent call last):\n    ...\nValueError: Invalid record: 'Spam, spam, spam'\nNote that the order of the keys in the patterns is irrelevant, even if the subject is an\nOrderedDict as b2.\n82 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "In contrast with sequence patterns, mapping patterns succeed on partial matches. In\nthe doctests, the b1 and b2 subjects include a 'title' key that does not appear in any\n'book' pattern, yet they match.\nThere is no need to use **extra to match extra key-value pairs, but if you want to\ncapture them as a dict, you can prefix one variable with **. It must be the last in the\npattern, and **_ is forbidden because it would be redundant. A simple example:\n>>> food = dict(category='ice cream', flavor='vanilla', cost=199)\n>>> match food:\n...     case {'category': 'ice cream', **details}:\n...         print(f'Ice cream details: {details}')\n...\nIce cream details: {'flavor': 'vanilla', 'cost': 199}\nIn “Automatic Handling of Missing Keys” on page 90 we’ll study defaultdict and\nother mappings where key lookups via __getitem__ (i.e., d[key]) succeed because\nmissing items are created on the fly. In the context of pattern matching, a match suc‐\nceeds only if the subject already has the required keys at the top of the match\nstatement.\nThe automatic handling of missing keys is not triggered because\npattern matching always uses the d.get(key, sentinel) method\n—where the default sentinel is a special marker value that cannot\noccur in user data.\nMoving on from syntax and structure, let’s study the API of mappings.\nStandard API of Mapping Types\nThe collections.abc module provides the Mapping and MutableMapping ABCs\ndescribing the interfaces of dict and similar types. See Figure 3-1.\nThe main value of the ABCs is documenting and formalizing the standard interfaces\nfor mappings, and serving as criteria for isinstance tests in code that needs to sup‐\nport mappings in a broad sense:\n>>> my_dict = {}\n>>> isinstance(my_dict, abc.Mapping)\nTrue\n>>> isinstance(my_dict, abc.MutableMapping)\nTrue\nStandard API of Mapping Types \n| \n83",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "2 The Python Glossary entry for “hashable” uses the term “hash value” instead of hash code. I prefer hash code\nbecause that is a concept often discussed in the context of mappings, where items are made of keys and val‐\nues, so it may be confusing to mention the hash code as a value. In this book, I only use hash code.\nUsing isinstance with an ABC is often better than checking\nwhether a function argument is of the concrete dict type, because\nthen alternative mapping types can be used. We’ll discuss this in\ndetail in Chapter 13.\nFigure 3-1. Simplified UML class diagram for the MutableMapping and its superclasses\nfrom collections.abc (inheritance arrows point from subclasses to superclasses;\nnames in italic are abstract classes and abstract methods).\nTo implement a custom mapping, it’s easier to extend collections.UserDict, or to\nwrap a dict by composition, instead of subclassing these ABCs. The collec\ntions.UserDict class and all concrete mapping classes in the standard library encap‐\nsulate the basic dict in their implementation, which in turn is built on a hash table.\nTherefore, they all share the limitation that the keys must be hashable (the values\nneed not be hashable, only the keys). If you need a refresher, the next section\nexplains.\nWhat Is Hashable\nHere is part of the definition of hashable adapted from the Python Glossary:\nAn object is hashable if it has a hash code which never changes during its lifetime (it\nneeds a __hash__() method), and can be compared to other objects (it needs an\n__eq__() method). Hashable objects which compare equal must have the same hash\ncode.2\nNumeric types and flat immutable types str and bytes are all hashable. Container\ntypes are hashable if they are immutable and all contained objects are also hashable.\nA frozenset is always hashable, because every element it contains must be hashable\n84 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "3 See PEP 456—Secure and interchangeable hash algorithm to learn about the security implications and solu‐\ntions adopted.\nby definition. A tuple is hashable only if all its items are hashable. See tuples tt, tl,\nand tf:\n>>> tt = (1, 2, (30, 40))\n>>> hash(tt)\n8027212646858338501\n>>> tl = (1, 2, [30, 40])\n>>> hash(tl)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unhashable type: 'list'\n>>> tf = (1, 2, frozenset([30, 40]))\n>>> hash(tf)\n-4118419923444501110\nThe hash code of an object may be different depending on the version of Python, the\nmachine architecture, and because of a salt added to the hash computation for secu‐\nrity reasons.3 The hash code of a correctly implemented object is guaranteed to be\nconstant only within one Python process.\nUser-defined types are hashable by default because their hash code is their id(), and\nthe __eq__() method inherited from the object class simply compares the object\nIDs. If an object implements a custom __eq__() that takes into account its internal\nstate, it will be hashable only if its __hash__() always returns the same hash code. In\npractice, this requires that __eq__() and __hash__() only take into account instance\nattributes that never change during the life of the object.\nNow let’s review the API of the most commonly used mapping types in Python: dict,\ndefaultdict, and OrderedDict.\nOverview of Common Mapping Methods\nThe basic API for mappings is quite rich. Table 3-1 shows the methods implemented\nby dict and two popular variations: defaultdict and OrderedDict, both defined in\nthe collections module.\nStandard API of Mapping Types \n| \n85",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Table 3-1. Methods of the mapping types dict, collections.defaultdict, and collec\ntions.OrderedDict (common object methods omitted for brevity); optional arguments are\nenclosed in […]\ndict\ndefaultdict\nOrderedDict\n \nd.clear()\n●\n●\n●\nRemove all items\nd.__contains__(k)\n●\n●\n●\nk in d\nd.copy()\n●\n●\n●\nShallow copy\nd.__copy__()\n●\nSupport for copy.copy(d)\nd.default_factory\n●\nCallable invoked by __missing__\nto set missing valuesa\nd.__delitem__(k)\n●\n●\n●\ndel d[k]—remove item with\nkey k\nd.fromkeys(it, [ini\ntial])\n●\n●\n●\nNew mapping from keys in iterable,\nwith optional initial value (defaults\nto None)\nd.get(k, [default])\n●\n●\n●\nGet item with key k, return\ndefault or None if missing\nd.__getitem__(k)\n●\n●\n●\nd[k]—get item with key k\nd.items()\n●\n●\n●\nGet view over items—(key, \nvalue) pairs\nd.__iter__()\n●\n●\n●\nGet iterator over keys\nd.keys()\n●\n●\n●\nGet view over keys\nd.__len__()\n●\n●\n●\nlen(d)—number of items\nd.__missing__(k)\n●\nCalled when __getitem__ cannot\nfind the key\nd.move_to_end(k, \n[last])\n●\nMove k first or last position (last is\nTrue by default)\nd.__or__(other)\n●\n●\n●\nSupport for d1 | d2 to create new\ndict merging d1 and d2 (Python\n≥ 3.9)\nd.__ior__(other)\n●\n●\n●\nSupport for d1 |= d2 to update\nd1 with d2 (Python ≥ 3.9)\nd.pop(k, [default])\n●\n●\n●\nRemove and return value at k, or\ndefault or None if missing\nd.popitem()\n●\n●\n●\nRemove and return the last inserted\nitem as (key, value) b\nd.__reversed__()\n●\n●\n●\nSupport for reverse(d)—returns\niterator for keys from last to first\ninserted.\nd.__ror__(other)\n●\n●\n●\nSupport for other | dd—\nreversed union operator (Python ≥\n3.9)c\n86 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "dict\ndefaultdict\nOrderedDict\n \nd.setdefault(k, \n[default])\n●\n●\n●\nIf k in d, return d[k]; else set\nd[k] = default and return it\nd.__setitem__(k, v)\n●\n●\n●\nd[k] = v—put v at k\nd.update(m, \n[**kwargs])\n●\n●\n●\nUpdate d with items from mapping\nor iterable of (key, value) pairs\nd.values()\n●\n●\n●\nGet view over values\na default_factory is not a method, but a callable attribute set by the end user when a defaultdict is instantiated.\nb OrderedDict.popitem(last=False) removes the first item inserted (FIFO). The last keyword argument is not\nsupported in dict or defaultdict as recently as Python 3.10b3.\nc Reversed operators are explained in Chapter 16.\nThe way d.update(m) handles its first argument m is a prime example of duck typing:\nit first checks whether m has a keys method and, if it does, assumes it is a mapping.\nOtherwise, update() falls back to iterating over m, assuming its items are (key,\nvalue) pairs. The constructor for most Python mappings uses the logic of update()\ninternally, which means they can be initialized from other mappings or from any\niterable object producing (key, value) pairs.\nA subtle mapping method is setdefault(). It avoids redundant key lookups when\nwe need to update the value of an item in place. The next section shows how to use it.\nInserting or Updating Mutable Values\nIn line with Python’s fail-fast philosophy, dict access with d[k] raises an error when\nk is not an existing key. Pythonistas know that d.get(k, default) is an alternative\nto d[k] whenever a default value is more convenient than handling KeyError. How‐\never, when you retrieve a mutable value and want to update it, there is a better way.\nConsider a script to index text, producing a mapping where each key is a word, and\nthe value is a list of positions where that word occurs, as shown in Example 3-3.\nExample 3-3. Partial output from Example 3-4 processing the “Zen of Python”;\neach line shows a word and a list of occurrences coded as pairs:\n(line_number, column_number)\n$ python3 index0.py zen.txt\na [(19, 48), (20, 53)]\nAlthough [(11, 1), (16, 1), (18, 1)]\nambiguity [(14, 16)]\nand [(15, 23)]\nare [(21, 12)]\naren [(10, 15)]\nat [(16, 38)]\nbad [(19, 50)]\nStandard API of Mapping Types \n| \n87",
      "content_length": 2200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "4 The original script appears in slide 41 of Martelli’s “Re-learning Python” presentation. His script is actually a\ndemonstration of dict.setdefault, as shown in our Example 3-5.\nbe [(15, 14), (16, 27), (20, 50)]\nbeats [(11, 23)]\nBeautiful [(3, 1)]\nbetter [(3, 14), (4, 13), (5, 11), (6, 12), (7, 9), (8, 11), (17, 8), (18, 25)]\n...\nExample 3-4 is a suboptimal script written to show one case where dict.get is not\nthe best way to handle a missing key. I adapted it from an example by Alex Martelli.4\nExample 3-4. index0.py uses dict.get to fetch and update a list of word occurrences\nfrom the index (a better solution is in Example 3-5)\n\"\"\"Build an index mapping word -> list of occurrences\"\"\"\nimport re\nimport sys\nWORD_RE = re.compile(r'\\w+')\nindex = {}\nwith open(sys.argv[1], encoding='utf-8') as fp:\n    for line_no, line in enumerate(fp, 1):\n        for match in WORD_RE.finditer(line):\n            word = match.group()\n            column_no = match.start() + 1\n            location = (line_no, column_no)\n            # this is ugly; coded like this to make a point\n            occurrences = index.get(word, [])  \n            occurrences.append(location)       \n            index[word] = occurrences          \n# display in alphabetical order\nfor word in sorted(index, key=str.upper):      \n    print(word, index[word])\nGet the list of occurrences for word, or [] if not found.\nAppend new location to occurrences.\nPut changed occurrences into index dict; this entails a second search through\nthe index.\n88 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "5 This is an example of using a method as a first-class function, the subject of Chapter 7.\nIn the key= argument of sorted, I am not calling str.upper, just passing a refer‐\nence to that method so the sorted function can use it to normalize the words for\nsorting.5\nThe three lines dealing with occurrences in Example 3-4 can be replaced by a single\nline using dict.setdefault. Example 3-5 is closer to Alex Martelli’s code.\nExample 3-5. index.py uses dict.setdefault to fetch and update a list of word\noccurrences from the index in a single line; contrast with Example 3-4\n\"\"\"Build an index mapping word -> list of occurrences\"\"\"\nimport re\nimport sys\nWORD_RE = re.compile(r'\\w+')\nindex = {}\nwith open(sys.argv[1], encoding='utf-8') as fp:\n    for line_no, line in enumerate(fp, 1):\n        for match in WORD_RE.finditer(line):\n            word = match.group()\n            column_no = match.start() + 1\n            location = (line_no, column_no)\n            index.setdefault(word, []).append(location)  \n# display in alphabetical order\nfor word in sorted(index, key=str.upper):\n    print(word, index[word])\nGet the list of occurrences for word, or set it to [] if not found; setdefault\nreturns the value, so it can be updated without requiring a second search.\nIn other words, the end result of this line…\nmy_dict.setdefault(key, []).append(new_value)\n…is the same as running…\nif key not in my_dict:\n    my_dict[key] = []\nmy_dict[key].append(new_value)\n…except that the latter code performs at least two searches for key—three if it’s not\nfound—while setdefault does it all with a single lookup.\nStandard API of Mapping Types \n| \n89",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "A related issue, handling missing keys on any lookup (and not only when inserting),\nis the subject of the next section.\nAutomatic Handling of Missing Keys\nSometimes it is convenient to have mappings that return some made-up value when a\nmissing key is searched. There are two main approaches to this: one is to use a\ndefaultdict instead of a plain dict. The other is to subclass dict or any other map‐\nping type and add a __missing__ method. Both solutions are covered next.\ndefaultdict: Another Take on Missing Keys\nA collections.defaultdict instance creates items with a default value on demand\nwhenever a missing key is searched using d[k] syntax. Example 3-6 uses default\ndict to provide another elegant solution to the word index task from Example 3-5.\nHere is how it works: when instantiating a defaultdict, you provide a callable to\nproduce a default value whenever __getitem__ is passed a nonexistent key argument.\nFor example, given a defaultdict created as dd = defaultdict(list), if 'new-key'\nis not in dd, the expression dd['new-key'] does the following steps:\n1. Calls list() to create a new list.\n2. Inserts the list into dd using 'new-key' as key.\n3. Returns a reference to that list.\nThe callable that produces the default values is held in an instance attribute named\ndefault_factory.\nExample 3-6. index_default.py: using defaultdict instead of the setdefault method\n\"\"\"Build an index mapping word -> list of occurrences\"\"\"\nimport collections\nimport re\nimport sys\nWORD_RE = re.compile(r'\\w+')\nindex = collections.defaultdict(list)     \nwith open(sys.argv[1], encoding='utf-8') as fp:\n    for line_no, line in enumerate(fp, 1):\n        for match in WORD_RE.finditer(line):\n            word = match.group()\n            column_no = match.start() + 1\n            location = (line_no, column_no)\n90 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "6 One such library is Pingo.io, no longer under active development.\n            index[word].append(location)  \n# display in alphabetical order\nfor word in sorted(index, key=str.upper):\n    print(word, index[word])\nCreate a defaultdict with the list constructor as default_factory.\nIf word is not initially in the index, the default_factory is called to produce the\nmissing value, which in this case is an empty list that is then assigned to\nindex[word] and returned, so the .append(location) operation always suc‐\nceeds.\nIf no default_factory is provided, the usual KeyError is raised for missing keys.\nThe default_factory of a defaultdict is only invoked to provide\ndefault values for __getitem__ calls, and not for the other meth‐\nods. For example, if dd is a defaultdict, and k is a missing key,\ndd[k] will call the default_factory to create a default value, but\ndd.get(k) still returns None, and k in dd is False.\nThe mechanism that makes defaultdict work by calling default_factory is the\n__missing__ special method, a feature that we discuss next.\nThe __missing__ Method\nUnderlying the way mappings deal with missing keys is the aptly named __missing__\nmethod. This method is not defined in the base dict class, but dict is aware of it: if\nyou subclass dict and provide a __missing__ method, the standard dict.__geti\ntem__ will call it whenever a key is not found, instead of raising KeyError.\nSuppose you’d like a mapping where keys are converted to str when looked up. A\nconcrete use case is a device library for IoT,6 where a programmable board with\ngeneral-purpose I/O pins (e.g., a Raspberry Pi or an Arduino) is represented by a\nBoard class with a my_board.pins attribute, which is a mapping of physical pin iden‐\ntifiers to pin software objects. The physical pin identifier may be just a number or a\nstring like \"A0\" or \"P9_12\". For consistency, it is desirable that all keys in board.pins\nare strings, but it is also convenient looking up a pin by number, as in my_ardu\nino.pin[13], so that beginners are not tripped when they want to blink the LED on\npin 13 of their Arduinos. Example 3-7 shows how such a mapping would work.\nAutomatic Handling of Missing Keys \n| \n91",
      "content_length": 2182,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Example 3-7. When searching for a nonstring key, StrKeyDict0 converts it to str\nwhen it is not found\nTests for item retrieval using `d[key]` notation::\n    >>> d = StrKeyDict0([('2', 'two'), ('4', 'four')])\n    >>> d['2']\n    'two'\n    >>> d[4]\n    'four'\n    >>> d[1]\n    Traceback (most recent call last):\n      ...\n    KeyError: '1'\nTests for item retrieval using `d.get(key)` notation::\n    >>> d.get('2')\n    'two'\n    >>> d.get(4)\n    'four'\n    >>> d.get(1, 'N/A')\n    'N/A'\nTests for the `in` operator::\n    >>> 2 in d\n    True\n    >>> 1 in d\n    False\nExample 3-8 implements a class StrKeyDict0 that passes the preceding doctests.\nA better way to create a user-defined mapping type is to subclass\ncollections.UserDict instead of dict (as we will do in\nExample 3-9). Here we subclass dict just to show that __miss\ning__ is supported by the built-in dict.__getitem__ method.\nExample 3-8. StrKeyDict0 converts nonstring keys to str on lookup (see tests in\nExample 3-7)\nclass StrKeyDict0(dict):  \n    def __missing__(self, key):\n        if isinstance(key, str):  \n            raise KeyError(key)\n        return self[str(key)]  \n92 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "def get(self, key, default=None):\n        try:\n            return self[key]  \n        except KeyError:\n            return default  \n    def __contains__(self, key):\n        return key in self.keys() or str(key) in self.keys()  \nStrKeyDict0 inherits from dict.\nCheck whether key is already a str. If it is, and it’s missing, raise KeyError.\nBuild str from key and look it up.\nThe get method delegates to __getitem__ by using the self[key] notation; that\ngives the opportunity for our __missing__ to act.\nIf a KeyError was raised, __missing__ already failed, so we return the default.\nSearch for unmodified key (the instance may contain non-str keys), then for a\nstr built from the key.\nTake a moment to consider why the test isinstance(key, str) is necessary in the\n__missing__ implementation.\nWithout that test, our __missing__ method would work OK for any key k—str or\nnot str—whenever str(k) produced an existing key. But if str(k) is not an existing\nkey, we’d have an infinite recursion. In the last line of __missing__, self[str(key)]\nwould call __getitem__, passing that str key, which in turn would call __missing__\nagain.\nThe __contains__ method is also needed for consistent behavior in this example,\nbecause the operation k in d calls it, but the method inherited from dict does not\nfall back to invoking __missing__. There is a subtle detail in our implementation of\n__contains__: we do not check for the key in the usual Pythonic way—k in my_dict\n—because str(key) in self would recursively call __contains__. We avoid this by\nexplicitly looking up the key in self.keys().\nA search like k in my_dict.keys() is efficient in Python 3 even for very large map‐\npings because dict.keys() returns a view, which is similar to a set, as we’ll see in\n“Set Operations on dict Views” on page 110. However, remember that k in my_dict\ndoes the same job, and is faster because it avoids the attribute lookup to find\nthe .keys method.\nAutomatic Handling of Missing Keys \n| \n93",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "I had a specific reason to use self.keys() in the __contains__ method in\nExample 3-8. The check for the unmodified key—key in self.keys()—is necessary\nfor correctness because StrKeyDict0 does not enforce that all keys in the dictionary\nmust be of type str. Our only goal with this simple example is to make searching\n“friendlier” and not enforce types.\nUser-defined classes derived from standard library mappings may\nor may not use __missing__ as a fallback in their implementations\nof __getitem__, get, or __contains__, as explained in the next\nsection.\nInconsistent Usage of __missing__ in the Standard Library\nConsider the following scenarios, and how the missing key lookups are affected:\ndict subclass\nA subclass of dict implementing only __missing__ and no other method. In this\ncase, __missing__ may be called only on d[k], which will use the __getitem__\ninherited from dict.\ncollections.UserDict subclass\nLikewise, a subclass of UserDict implementing only __missing__ and no other\nmethod. The get method inherited from UserDict calls __getitem__. This\nmeans __missing__ may be called to handle lookups with d[k] and d.get(k).\nabc.Mapping subclass with the simplest possible __getitem__\nA minimal subclass of abc.Mapping implementing __missing__ and the required\nabstract methods, including an implementation of __getitem__ that does not\ncall __missing__. The __missing__ method is never triggered in this class.\nabc.Mapping subclass with __getitem__ calling __missing__\nA minimal subclass of abc.Mapping implementing __missing__ and the required\nabstract methods, including an implementation of __getitem__ that calls __miss\ning__. The __missing__ method is triggered in this class for missing key lookups\nmade with d[k], d.get(k), and k in d.\nSee missing.py in the example code repository for demonstrations of the scenarios\ndescribed here.\nThe four scenarios just described assume minimal implementations. If your subclass\nimplements __getitem__, get, and __contains__, then you can make those methods\nuse __missing__ or not, depending on your needs. The point of this section is to\nshow that you must be careful when subclassing standard library mappings to use\n__missing__, because the base classes support different behaviors by default.\n94 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 2290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "Don’t forget that the behavior of setdefault and update is also affected by key\nlookup. And finally, depending on the logic of your __missing__, you may need to\nimplement special logic in __setitem__, to avoid inconsistent or surprising behavior.\nWe’ll see an example of this in “Subclassing UserDict Instead of dict” on page 97.\nSo far we have covered the dict and defaultdict mapping types, but the standard\nlibrary comes with other mapping implementations, which we discuss next.\nVariations of dict\nIn this section is an overview of mapping types included in the standard library,\nbesides defaultdict, already covered in “defaultdict: Another Take on Missing\nKeys” on page 90.\ncollections.OrderedDict\nNow that the built-in dict also keeps the keys ordered since Python 3.6, the most\ncommon reason to use OrderedDict is writing code that is backward compatible with\nearlier Python versions. Having said that, Python’s documentation lists some remain‐\ning differences between dict and OrderedDict, which I quote here—only reordering\nthe items for relevance in daily use:\n• The equality operation for OrderedDict checks for matching order.\n• The popitem() method of OrderedDict has a different signature. It accepts an\noptional argument to specify which item is popped.\n• OrderedDict has a move_to_end() method to efficiently reposition an element to\nan endpoint.\n• The regular dict was designed to be very good at mapping operations. Tracking\ninsertion order was secondary.\n• OrderedDict was designed to be good at reordering operations. Space efficiency,\niteration speed, and the performance of update operations were secondary.\n• Algorithmically, OrderedDict can handle frequent reordering operations better\nthan dict. This makes it suitable for tracking recent accesses (for example, in an\nLRU cache).\ncollections.ChainMap\nA ChainMap instance holds a list of mappings that can be searched as one. The lookup\nis performed on each input mapping in the order it appears in the constructor call,\nand succeeds as soon as the key is found in one of those mappings. For example:\nVariations of dict \n| \n95",
      "content_length": 2101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": ">>> d1 = dict(a=1, b=3)\n>>> d2 = dict(a=2, b=4, c=6)\n>>> from collections import ChainMap\n>>> chain = ChainMap(d1, d2)\n>>> chain['a']\n1\n>>> chain['c']\n6\nThe ChainMap instance does not copy the input mappings, but holds references to\nthem. Updates or insertions to a ChainMap only affect the first input mapping. Con‐\ntinuing from the previous example:\n>>> chain['c'] = -1\n>>> d1\n{'a': 1, 'b': 3, 'c': -1}\n>>> d2\n{'a': 2, 'b': 4, 'c': 6}\nChainMap is useful to implement interpreters for languages with nested scopes, where\neach mapping represents a scope context, from the innermost enclosing scope to the\noutermost scope. The “ChainMap objects” section of the collections docs has sev‐\neral examples of ChainMap usage, including this snippet inspired by the basic rules of\nvariable lookup in Python:\nimport builtins\npylookup = ChainMap(locals(), globals(), vars(builtins))\nExample 18-14 shows a ChainMap subclass used to implement an interpreter for a\nsubset of the Scheme programming language.\ncollections.Counter\nA mapping that holds an integer count for each key. Updating an existing key adds to\nits count. This can be used to count instances of hashable objects or as a multiset (dis‐\ncussed later in this section). Counter implements the + and - operators to combine\ntallies, and other useful methods such as most_common([n]), which returns an\nordered list of tuples with the n most common items and their counts; see the docu‐\nmentation. Here is Counter used to count letters in words:\n>>> ct = collections.Counter('abracadabra')\n>>> ct\nCounter({'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1})\n>>> ct.update('aaaaazzz')\n>>> ct\nCounter({'a': 10, 'z': 3, 'b': 2, 'r': 2, 'c': 1, 'd': 1})\n>>> ct.most_common(3)\n[('a', 10), ('z', 3), ('b', 2)]\n96 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Note that the 'b' and 'r' keys are tied in third place, but ct.most_common(3) shows\nonly three counts.\nTo use collections.Counter as a multiset, pretend each key is an element in the set,\nand the count is the number of occurrences of that element in the set.\nshelve.Shelf\nThe shelve module in the standard library provides persistent storage for a mapping\nof string keys to Python objects serialized in the pickle binary format. The curious\nname of shelve makes sense when you realize that pickle jars are stored on shelves.\nThe shelve.open module-level function returns a shelve.Shelf instance—a simple\nkey-value DBM database backed by the dbm module, with these characteristics:\n• shelve.Shelf subclasses abc.MutableMapping, so it provides the essential meth‐\nods we expect of a mapping type.\n• In addition, shelve.Shelf provides a few other I/O management methods, like\nsync and close.\n• A Shelf instance is a context manager, so you can use a with block to make sure\nit is closed after use.\n• Keys and values are saved whenever a new value is assigned to a key.\n• The keys must be strings.\n• The values must be objects that the pickle module can serialize.\nThe documentation for the shelve, dbm, and pickle modules provides more details\nand some caveats.\nPython’s pickle is easy to use in the simplest cases, but has several\ndrawbacks. Read Ned Batchelder’s “Pickle’s nine flaws” before\nadopting any solution involving pickle. In his post, Ned mentions\nother serialization formats to consider.\nOrderedDict, ChainMap, Counter, and Shelf are ready to use but can also be custom‐\nized by subclassing. In contrast, UserDict is intended only as a base class to be\nextended.\nSubclassing UserDict Instead of dict\nIt’s better to create a new mapping type by extending collections.UserDict rather\nthan dict. We realize that when we try to extend our StrKeyDict0 from Example 3-8\nto make sure that any keys added to the mapping are stored as str.\nVariations of dict \n| \n97",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "7 The exact problem with subclassing dict and other built-ins is covered in “Subclassing Built-In Types Is\nTricky” on page 490.\nThe main reason why it’s better to subclass UserDict rather than dict is that the\nbuilt-in has some implementation shortcuts that end up forcing us to override meth‐\nods that we can just inherit from UserDict with no problems.7\nNote that UserDict does not inherit from dict, but uses composition: it has an inter‐\nnal dict instance, called data, which holds the actual items. This avoids undesired\nrecursion when coding special methods like __setitem__, and simplifies the coding\nof __contains__, compared to Example 3-8.\nThanks to UserDict, StrKeyDict (Example 3-9) is more concise than StrKeyDict0\n(Example 3-8), but it does more: it stores all keys as str, avoiding unpleasant sur‐\nprises if the instance is built or updated with data containing nonstring keys.\nExample 3-9. StrKeyDict always converts nonstring keys to str on insertion, update,\nand lookup\nimport collections\nclass StrKeyDict(collections.UserDict):  \n    def __missing__(self, key):  \n        if isinstance(key, str):\n            raise KeyError(key)\n        return self[str(key)]\n    def __contains__(self, key):\n        return str(key) in self.data  \n    def __setitem__(self, key, item):\n        self.data[str(key)] = item   \nStrKeyDict extends UserDict.\n__missing__ is exactly as in Example 3-8.\n__contains__ is simpler: we can assume all stored keys are str, and we can\ncheck on self.data instead of invoking self.keys() as we did in StrKeyDict0.\n__setitem__ converts any key to a str. This method is easier to overwrite when\nwe can delegate to the self.data attribute.\n98 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "Because UserDict extends abc.MutableMapping, the remaining methods that make\nStrKeyDict a full-fledged mapping are inherited from UserDict, MutableMapping, or\nMapping. The latter have several useful concrete methods, in spite of being abstract\nbase classes (ABCs). The following methods are worth noting:\nMutableMapping.update\nThis powerful method can be called directly but is also used by __init__ to load\nthe instance from other mappings, from iterables of (key, value) pairs, and\nkeyword arguments. Because it uses self[key] = value to add items, it ends up\ncalling our implementation of __setitem__.\nMapping.get\nIn StrKeyDict0 (Example 3-8), we had to code our own get to return the same\nresults as __getitem__, but in Example 3-9 we inherited Mapping.get, which is\nimplemented exactly like StrKeyDict0.get (see the Python source code).\nAntoine Pitrou authored PEP 455—Adding a key-transforming\ndictionary to collections and a patch to enhance the collections\nmodule with a TransformDict, that is more general than StrKey\nDict and preserves the keys as they are provided, before the trans‐\nformation is applied. PEP 455 was rejected in May 2015—see\nRaymond Hettinger’s rejection message. To experiment with Trans\nformDict, I extracted Pitrou’s patch from issue18986 into a stand‐\nalone module (03-dict-set/transformdict.py in the Fluent Python\nsecond edition code repository).\nWe know there are immutable sequence types, but how about an immutable map‐\nping? Well, there isn’t a real one in the standard library, but a stand-in is available.\nThat’s next.\nImmutable Mappings\nThe mapping types provided by the standard library are all mutable, but you may\nneed to prevent users from changing a mapping by accident. A concrete use case can\nbe found, again, in a hardware programming library like Pingo, mentioned in “The\n__missing__ Method” on page 91: the board.pins mapping represents the physical\nGPIO pins on the device. As such, it’s useful to prevent inadvertent updates to\nboard.pins because the hardware can’t be changed via software, so any change in the\nmapping would make it inconsistent with the physical reality of the device.\nThe types module provides a wrapper class called MappingProxyType, which, given a\nmapping, returns a mappingproxy instance that is a read-only but dynamic proxy for\nthe original mapping. This means that updates to the original mapping can be seen in\nImmutable Mappings \n| \n99",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "the mappingproxy, but changes cannot be made through it. See Example 3-10 for a\nbrief demonstration.\nExample 3-10. MappingProxyType builds a read-only mappingproxy instance from a\ndict\n>>> from types import MappingProxyType\n>>> d = {1: 'A'}\n>>> d_proxy = MappingProxyType(d)\n>>> d_proxy\nmappingproxy({1: 'A'})\n>>> d_proxy[1]  \n'A'\n>>> d_proxy[2] = 'x'  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'mappingproxy' object does not support item assignment\n>>> d[2] = 'B'\n>>> d_proxy  \nmappingproxy({1: 'A', 2: 'B'})\n>>> d_proxy[2]\n'B'\n>>>\nItems in d can be seen through d_proxy.\nChanges cannot be made through d_proxy.\nd_proxy is dynamic: any change in d is reflected.\nHere is how this could be used in practice in the hardware programming scenario:\nthe constructor in a concrete Board subclass would fill a private mapping with the pin\nobjects, and expose it to clients of the API via a public .pins attribute implemented\nas a mappingproxy. That way the clients would not be able to add, remove, or change\npins by accident.\nNext, we’ll cover views—which allow high-performance operations on a dict,\nwithout unnecessary copying of data.\n100 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Dictionary Views\nThe dict instance methods .keys(), .values(), and .items() return instances of\nclasses called dict_keys, dict_values, and dict_items, respectively. These dictio‐\nnary views are read-only projections of the internal data structures used in the dict\nimplementation. They avoid the memory overhead of the equivalent Python 2 meth‐\nods that returned lists duplicating data already in the target dict, and they also\nreplace the old methods that returned iterators.\nExample 3-11 shows some basic operations supported by all dictionary views.\nExample 3-11. The .values() method returns a view of the values in a dict\n>>> d = dict(a=10, b=20, c=30)\n>>> values = d.values()\n>>> values\ndict_values([10, 20, 30])  \n>>> len(values)  \n3\n>>> list(values)  \n[10, 20, 30]\n>>> reversed(values)  \n<dict_reversevalueiterator object at 0x10e9e7310>\n>>> values[0] \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'dict_values' object is not subscriptable\nThe repr of a view object shows its content.\nWe can query the len of a view.\nViews are iterable, so it’s easy to create lists from them.\nViews implement __reversed__, returning a custom iterator.\nWe can’t use [] to get individual items from a view.\nA view object is a dynamic proxy. If the source dict is updated, you can immediately\nsee the changes through an existing view. Continuing from Example 3-11:\n>>> d['z'] = 99\n>>> d\n{'a': 10, 'b': 20, 'c': 30, 'z': 99}\n>>> values\ndict_values([10, 20, 30, 99])\nDictionary Views \n| \n101",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "8 That’s how tuples are stored.\nThe classes dict_keys, dict_values, and dict_items are internal: they are not avail‐\nable via __builtins__ or any standard library module, and even if you get a reference\nto one of them, you can’t use it to create a view from scratch in Python code:\n>>> values_class = type({}.values())\n>>> v = values_class()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: cannot create 'dict_values' instances\nThe dict_values class is the simplest dictionary view—it implements only the\n__len__, __iter__, and __reversed__ special methods. In addition to these meth‐\nods, dict_keys and dict_items implement several set methods, almost as many as\nthe frozenset class. After we cover sets, we’ll have more to say about dict_keys and\ndict_items in “Set Operations on dict Views” on page 110.\nNow let’s see some rules and tips informed by the way dict is implemented under\nthe hood.\nPractical Consequences of How dict Works\nThe hash table implementation of Python’s dict is very efficient, but it’s important to\nunderstand the practical effects of this design:\n• Keys must be hashable objects. They must implement proper __hash__ and\n__eq__ methods as described in “What Is Hashable” on page 84.\n• Item access by key is very fast. A dict may have millions of keys, but Python can\nlocate a key directly by computing the hash code of the key and deriving an index\noffset into the hash table, with the possible overhead of a small number of tries to\nfind a matching entry.\n• Key ordering is preserved as a side effect of a more compact memory layout for\ndict in CPython 3.6, which became an official language feature in 3.7.\n• Despite its new compact layout, dicts inevitably have a significant memory over‐\nhead. The most compact internal data structure for a container would be an\narray of pointers to the items.8 Compared to that, a hash table needs to store\nmore data per entry, and Python needs to keep at least one-third of the hash table\nrows empty to remain efficient.\n• To save memory, avoid creating instance attributes outside of the __init__\nmethod.\n102 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "9 Unless the class has a __slots__ attribute, as explained in “Saving Memory with __slots__” on page 384.\nThat last tip about instance attributes comes from the fact that Python’s default\nbehavior is to store instance attributes in a special __dict__ attribute, which is a dict\nattached to each instance.9 Since PEP 412—Key-Sharing Dictionary was implemented\nin Python 3.3, instances of a class can share a common hash table, stored with the\nclass. That common hash table is shared by the __dict__ of each new instance that\nhas the same attributes names as the first instance of that class when __init__\nreturns. Each instance __dict__ can then hold only its own attribute values as a sim‐\nple array of pointers. Adding an instance attribute after __init__ forces Python to\ncreate a new hash table just for the __dict__ of that one instance (which was the\ndefault behavior for all instances before Python 3.3). According to PEP 412, this opti‐\nmization reduces memory use by 10% to 20% for object-oriented programs.\nThe details of the compact layout and key-sharing optimizations are rather complex.\nFor more, please read “Internals of sets and dicts” at fluentpython.com.\nNow let’s dive into sets.\nSet Theory\nSets are not new in Python, but are still somewhat underused. The set type and its\nimmutable sibling frozenset first appeared as modules in the Python 2.3 standard\nlibrary, and were promoted to built-ins in Python 2.6.\nIn this book, I use the word “set” to refer both to set and frozen\nset. When talking specifically about the set class, I use constant\nwidth font: set.\nA set is a collection of unique objects. A basic use case is removing duplication:\n>>> l = ['spam', 'spam', 'eggs', 'spam', 'bacon', 'eggs']\n>>> set(l)\n{'eggs', 'spam', 'bacon'}\n>>> list(set(l))\n['eggs', 'spam', 'bacon']\nSet Theory \n| \n103",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "If you want to remove duplicates but also preserve the order of the\nfirst occurrence of each item, you can now use a plain dict to do it,\nlike this:\n>>> dict.fromkeys(l).keys()\ndict_keys(['spam', 'eggs', 'bacon'])\n>>> list(dict.fromkeys(l).keys())\n['spam', 'eggs', 'bacon']\nSet elements must be hashable. The set type is not hashable, so you can’t build a set\nwith nested set instances. But frozenset is hashable, so you can have frozenset\nelements inside a set.\nIn addition to enforcing uniqueness, the set types implement many set operations as\ninfix operators, so, given two sets a and b, a | b returns their union, a & b computes\nthe intersection, a - b the difference, and a ^ b the symmetric difference. Smart use\nof set operations can reduce both the line count and the execution time of Python\nprograms, at the same time making code easier to read and reason about—by remov‐\ning loops and conditional logic.\nFor example, imagine you have a large set of email addresses (the haystack) and a\nsmaller set of addresses (the needles) and you need to count how many needles\noccur in the haystack. Thanks to set intersection (the & operator) you can code that\nin a simple line (see Example 3-12).\nExample 3-12. Count occurrences of needles in a haystack, both of type set\nfound = len(needles & haystack)\nWithout the intersection operator, you’d have to write Example 3-13 to accomplish\nthe same task as Example 3-12.\nExample 3-13. Count occurrences of needles in a haystack (same end result as\nExample 3-12)\nfound = 0\nfor n in needles:\n    if n in haystack:\n        found += 1\nExample 3-12 runs slightly faster than Example 3-13. On the other hand,\nExample 3-13 works for any iterable objects needles and haystack, while\nExample 3-12 requires that both be sets. But, if you don’t have sets on hand, you can\nalways build them on the fly, as shown in Example 3-14.\n104 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "Example 3-14. Count occurrences of needles in a haystack; these lines work for any\niterable types\nfound = len(set(needles) & set(haystack))\n# another way:\nfound = len(set(needles).intersection(haystack))\nOf course, there is an extra cost involved in building the sets in Example 3-14, but if\neither the needles or the haystack is already a set, the alternatives in Example 3-14\nmay be cheaper than Example 3-13.\nAny one of the preceding examples are capable of searching 1,000 elements in a hay\nstack of 10,000,000 items in about 0.3 milliseconds—that’s close to 0.3 microseconds\nper element.\nBesides the extremely fast membership test (thanks to the underlying hash table), the\nset and frozenset built-in types provide a rich API to create new sets or, in the case\nof set, to change existing ones. We will discuss the operations shortly, but first a note\nabout syntax.\nSet Literals\nThe syntax of set literals—{1}, {1, 2}, etc.—looks exactly like the math notation,\nwith one important exception: there’s no literal notation for the empty set, so we\nmust remember to write set().\nSyntax Quirk\nDon’t forget that to create an empty set, you should use the con‐\nstructor without an argument: set(). If you write {}, you’re creat‐\ning an empty dict—this hasn’t changed in Python 3.\nIn Python 3, the standard string representation of sets always uses the {…} notation,\nexcept for the empty set:\n>>> s = {1}\n>>> type(s)\n<class 'set'>\n>>> s\n{1}\n>>> s.pop()\n1\n>>> s\nset()\nSet Theory \n| \n105",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "10 This may be interesting, but is not super important. The speed up will happen only when a set literal is evalu‐\nated, and that happens at most once per Python process—when a module is initially compiled. If you’re curi‐\nous, import the dis function from the dis module and use it to disassemble the bytecodes for a set literal—\ne.g., dis('{1}')—and a set call—dis('set([1])')\nLiteral set syntax like {1, 2, 3} is both faster and more readable than calling the\nconstructor (e.g., set([1, 2, 3])). The latter form is slower because, to evaluate it,\nPython has to look up the set name to fetch the constructor, then build a list, and\nfinally pass it to the constructor. In contrast, to process a literal like {1, 2, 3},\nPython runs a specialized BUILD_SET bytecode.10\nThere is no special syntax to represent frozenset literals—they must be created by\ncalling the constructor. The standard string representation in Python 3 looks like a\nfrozenset constructor call. Note the output in the console session:\n>>> frozenset(range(10))\nfrozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\nSpeaking of syntax, the idea of listcomps was adapted to build sets as well.\nSet Comprehensions\nSet comprehensions (setcomps) were added way back in Python 2.7, together with the\ndictcomps that we saw in “dict Comprehensions” on page 79. Example 3-15 shows\nhow.\nExample 3-15. Build a set of Latin-1 characters that have the word “SIGN” in their\nUnicode names\n>>> from unicodedata import name  \n>>> {chr(i) for i in range(32, 256) if 'SIGN' in name(chr(i),'')}  \n{'§', '=', '¢', '#', '¤', '<', '¥', 'µ', '×', '$', '¶', '£', '©',\n'°', '+', '÷', '±', '>', '¬', '®', '%'}\nImport name function from unicodedata to obtain character names.\nBuild set of characters with codes from 32 to 255 that have the word 'SIGN' in\ntheir names.\nThe order of the output changes for each Python process, because of the salted hash\nmentioned in “What Is Hashable” on page 84.\nSyntax matters aside, let’s now consider the behavior of sets.\n106 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "Practical Consequences of How Sets Work\nThe set and frozenset types are both implemented with a hash table. This has these\neffects:\n• Set elements must be hashable objects. They must implement proper __hash__\nand __eq__ methods as described in “What Is Hashable” on page 84.\n• Membership testing is very efficient. A set may have millions of elements, but an\nelement can be located directly by computing its hash code and deriving an index\noffset, with the possible overhead of a small number of tries to find a matching\nelement or exhaust the search.\n• Sets have a significant memory overhead, compared to a low-level array pointers\nto its elements—which would be more compact but also much slower to search\nbeyond a handful of elements.\n• Element ordering depends on insertion order, but not in a useful or reliable way.\nIf two elements are different but have the same hash code, their position depends\non which element is added first.\n• Adding elements to a set may change the order of existing elements. That’s\nbecause the algorithm becomes less efficient if the hash table is more than two-\nthirds full, so Python may need to move and resize the table as it grows. When\nthis happens, elements are reinserted and their relative ordering may change.\nSee “Internals of sets and dicts” at fluentpython.com for details.\nLet’s now review the rich assortment of operations provided by sets.\nSet Operations\nFigure 3-2 gives an overview of the methods you can use on mutable and immutable\nsets. Many of them are special methods that overload operators, such as & and >=.\nTable 3-2 shows the math set operators that have corresponding operators or meth‐\nods in Python. Note that some operators and methods perform in-place changes on\nthe target set (e.g., &=, difference_update, etc.). Such operations make no sense in\nthe ideal world of mathematical sets, and are not implemented in frozenset.\nPractical Consequences of How Sets Work \n| \n107",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "The infix operators in Table 3-2 require that both operands be sets,\nbut all other methods take one or more iterable arguments. For\nexample, to produce the union of four collections, a, b, c, and d,\nyou can call a.union(b, c, d), where a must be a set, but b, c,\nand d can be iterables of any type that produce hashable items. If\nyou need to create a new set with the union of four iterables,\ninstead of updating an existing set, you can write {*a, *b, *c,\n*d} since Python 3.5 thanks to PEP 448—Additional Unpacking\nGeneralizations.\nFigure 3-2. Simplified UML class diagram for MutableSet and its superclasses from\ncollections.abc (names in italic are abstract classes and abstract methods; reverse\noperator methods omitted for brevity).\nTable 3-2. Mathematical set operations: these methods either produce a new set or update\nthe target set in place, if it’s mutable\nMath\nsymbol\nPython\noperator\nMethod\nDescription\nS ∩ Z\ns & z\ns.__and__(z)\nIntersection of s and z\nz & s\ns.__rand__(z)\nReversed & operator\ns.intersection(it, …)\nIntersection of s and all sets built from\niterables it, etc.\ns &= z\ns.__iand__(z)\ns updated with intersection of s and z\ns.intersection_update(it, …)\ns updated with intersection of s and all\nsets built from iterables it, etc.\nS ∪ Z\ns | z\ns.__or__(z)\nUnion of s and z\nz | s\ns.__ror__(z)\nReversed |\ns.union(it, …)\nUnion of s and all sets built from iterables\nit, etc.\n108 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "Math\nsymbol\nPython\noperator\nMethod\nDescription\ns |= z\ns.__ior__(z)\ns updated with union of s and z\ns.update(it, …)\ns updated with union of s and all sets built\nfrom iterables it, etc.\nS \\ Z\ns - z\ns.__sub__(z)\nRelative complement or difference between\ns and z\nz - s\ns.__rsub__(z)\nReversed - operator\ns.difference(it, …)\nDifference between s and all sets built\nfrom iterables it, etc.\ns -= z\ns.__isub__(z)\ns updated with difference between s and\nz\ns.difference_update(it, …)\ns updated with difference between s and\nall sets built from iterables it, etc.\nS ∆ Z\ns ^ z\ns.__xor__(z)\nSymmetric difference (the complement of\nthe intersection s & z)\nz ^ s\ns.__rxor__(z)\nReversed ^ operator\ns.symmetric_difference(it)\nComplement of s & set(it)\ns ^= z\ns.__ixor__(z)\ns updated with symmetric difference of s\nand z\ns.symmetric_differ\nence_update(it, …)\ns updated with symmetric difference of s\nand all sets built from iterables it, etc.\nTable 3-3 lists set predicates: operators and methods that return True or False.\nTable 3-3. Set comparison operators and methods that return a bool\nMath symbol\nPython operator\nMethod\nDescription\nS ∩ Z = ∅\ns.isdisjoint(z)\ns and z are disjoint (no elements in common)\ne ∈ S\ne in s\ns.__contains__(e)\nElement e is a member of s\nS ⊆ Z\ns <= z\ns.__le__(z)\ns is a subset of the z set\ns.issubset(it)\ns is a subset of the set built from the iterable it\nS ⊂ Z\ns < z\ns.__lt__(z)\ns is a proper subset of the z set\nS ⊇ Z\ns >= z\ns.__ge__(z)\ns is a superset of the z set\ns.issuperset(it)\ns is a superset of the set built from the iterable it\nS ⊃ Z\ns > z\ns.__gt__(z)\ns is a proper superset of the z set\nIn addition to the operators and methods derived from math set theory, the set types\nimplement other methods of practical use, summarized in Table 3-4.\nPractical Consequences of How Sets Work \n| \n109",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Table 3-4. Additional set methods\nset\nfrozenset  \ns.add(e)\n●\nAdd element e to s\ns.clear()\n●\nRemove all elements of s\ns.copy()\n●\n●\nShallow copy of s\ns.discard(e) ●\nRemove element e from s if it is present\ns.__iter__()\n●\n●\nGet iterator over s\ns.__len__()\n●\n●\nlen(s)\ns.pop()\n●\nRemove and return an element from s, raising KeyError if s is empty\ns.remove(e)\n●\nRemove element e from s, raising KeyError if e not in s\nThis completes our overview of the features of sets. As promised in “Dictionary\nViews” on page 101, we’ll now see how two of the dictionary view types behave very\nmuch like a frozenset.\nSet Operations on dict Views\nTable 3-5 shows that the view objects returned by the dict methods .keys()\nand .items() are remarkably similar to frozenset.\nTable 3-5. Methods implemented by frozenset, dict_keys, and dict_items\nfrozenset dict_keys\ndict_items\nDescription\ns.__and__(z)\n●\n●\n●\ns & z (intersection of s and z)\ns.__rand__(z)\n●\n●\n●\nReversed & operator\ns.__contains__()\n●\n●\n●\ne in s\ns.copy()\n●\nShallow copy of s\ns.difference(it, …)\n●\nDifference between s and iterables it, etc.\ns.intersection(it, …)\n●\nIntersection of s and iterables it, etc.\ns.isdisjoint(z)\n●\n●\n●\ns and z are disjoint (no elements in common)\ns.issubset(it)\n●\ns is a subset of iterable it\ns.issuperset(it)\n●\ns is a superset of iterable it\ns.__iter__()\n●\n●\n●\nGet iterator over s\ns.__len__()\n●\n●\n●\nlen(s)\ns.__or__(z)\n●\n●\n●\ns | z (union of s and z)\ns.__ror__()\n●\n●\n●\nReversed | operator\ns.__reversed__()\n●\n●\nGet iterator over s in reverse order\ns.__rsub__(z)\n●\n●\n●\nReversed - operator\ns.__sub__(z)\n●\n●\n●\ns - z (difference between s and z)\n110 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "frozenset dict_keys\ndict_items\nDescription\ns.symmetric_differ\nence(it)\n●\nComplement of s & set(it)\ns.union(it, …)\n●\nUnion of s and iterables it, etc.\ns.__xor__()\n●\n●\n●\ns ^ z (symmetric difference of s and z)\ns.__rxor__()\n●\n●\n●\nReversed ^ operator\nIn particular, dict_keys and dict_items implement the special methods to support\nthe powerful set operators & (intersection), | (union), - (difference), and ^ (symmet‐\nric difference).\nFor example, using & is easy to get the keys that appear in two dictionaries:\n>>> d1 = dict(a=1, b=2, c=3, d=4)\n>>> d2 = dict(b=20, d=40, e=50)\n>>> d1.keys() & d2.keys()\n{'b', 'd'}\nNote that the return value of & is a set. Even better: the set operators in dictionary\nviews are compatible with set instances. Check this out:\n>>> s = {'a', 'e', 'i'}\n>>> d1.keys() & s\n{'a'}\n>>> d1.keys() | s\n{'a', 'c', 'b', 'd', 'i', 'e'}\nA dict_items view only works as a set if all values in the dict are\nhashable. Attempting set operations on a dict_items view with an\nunhashable value raises TypeError: unhashable type 'T', with T\nas the type of the offending value.\nOn the other hand, a dict_keys view can always be used as a set,\nbecause every key is hashable—by definition.\nUsing set operators with views will save a lot of loops and ifs when inspecting the\ncontents of dictionaries in your code. Let Python’s efficient implementation in C\nwork for you!\nWith this, we can wrap up this chapter.\nSet Operations on dict Views \n| \n111",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Chapter Summary\nDictionaries are a keystone of Python. Over the years, the familiar {k1: v1, k2: v2}\nliteral syntax was enhanced to support unpacking with **, pattern matching, as well\nas dict comprehensions.\nBeyond the basic dict, the standard library offers handy, ready-to-use specialized\nmappings like defaultdict, ChainMap, and Counter, all defined in the collections\nmodule. With the new dict implementation, OrderedDict is not as useful as before,\nbut should remain in the standard library for backward compatibility—and has spe‐\ncific characteristics that dict doesn’t have, such as taking into account key ordering\nin == comparisons. Also in the collections module is the UserDict, an easy to use\nbase class to create custom mappings.\nTwo powerful methods available in most mappings are setdefault and update. The\nsetdefault method can update items holding mutable values—for example, in a\ndict of list values—avoiding a second search for the same key. The update method\nallows bulk insertion or overwriting of items from any other mapping, from iterables\nproviding (key, value) pairs, and from keyword arguments. Mapping constructors\nalso use update internally, allowing instances to be initialized from mappings, itera‐\nbles, or keyword arguments. Since Python 3.9, we can also use the |= operator to\nupdate a mapping, and the | operator to create a new one from the union of two\nmappings.\nA clever hook in the mapping API is the __missing__ method, which lets you cus‐\ntomize what happens when a key is not found when using the d[k] syntax that\ninvokes __getitem__.\nThe collections.abc module provides the Mapping and MutableMapping abstract\nbase classes as standard interfaces, useful for runtime type checking. The Mapping\nProxyType from the types module creates an immutable façade for a mapping\nyou want to protect from accidental change. There are also ABCs for Set and\nMutableSet.\nDictionary views were a great addition in Python 3, eliminating the memory over‐\nhead of the Python 2 .keys(), .values(), and .items() methods that built lists\nduplicating data in the target dict instance. In addition, the dict_keys and\ndict_items classes support the most useful operators and methods of frozenset.\n112 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Further Reading\nIn The Python Standard Library documentation, “collections—Container datatypes”,\nincludes examples and practical recipes with several mapping types. The Python\nsource code for the module Lib/collections/__init__.py is a great reference for anyone\nwho wants to create a new mapping type or grok the logic of the existing ones. Chap‐\nter 1 of the Python Cookbook, 3rd ed. (O’Reilly) by David Beazley and Brian K. Jones\nhas 20 handy and insightful recipes with data structures—the majority using dict in\nclever ways.\nGreg Gandenberger advocates for the continued use of collections.OrderedDict,\non the grounds that “explicit is better than implicit,” backward compatibility, and the\nfact that some tools and libraries assume the ordering of dict keys is irrelevant—his\npost: “Python Dictionaries Are Now Ordered. Keep Using OrderedDict”.\nPEP 3106—Revamping dict.keys(), .values() and .items() is where Guido van Rossum\npresented the dictionary views feature for Python 3. In the abstract, he wrote that the\nidea came from the Java Collections Framework.\nPyPy was the first Python interpreter to implement Raymond Hettinger’s proposal of\ncompact dicts, and they blogged about it in “Faster, more memory efficient and more\nordered dictionaries on PyPy”, acknowledging that a similar layout was adopted in\nPHP 7, described in PHP’s new hashtable implementation. It’s always great when cre‐\nators cite prior art.\nAt PyCon 2017, Brandon Rhodes presented “The Dictionary Even Mightier”, a sequel\nto his classic animated presentation “The Mighty Dictionary”—including animated\nhash collisions! Another up-to-date, but more in-depth video on the internals of\nPython’s dict is “Modern Dictionaries” by Raymond Hettinger, where he tells that\nafter initially failing to sell compact dicts to the CPython core devs, he lobbied the\nPyPy team, they adopted it, the idea gained traction, and was finally contributed to\nCPython 3.6 by INADA Naoki. For all details, check out the extensive comments in\nthe CPython code for Objects/dictobject.c and the design document Objects/dict‐\nnotes.txt.\nThe rationale for adding sets to Python is documented in PEP 218—Adding a Built-\nIn Set Object Type. When PEP 218 was approved, no special literal syntax was adop‐\nted for sets. The set literals were created for Python 3 and backported to Python 2.7,\nalong with dict and set comprehensions. At PyCon 2019, I presented “Set Practice:\nlearning from Python’s set types” describing use cases of sets in real programs, cover‐\ning their API design, and the implementation of uintset, a set class for integer ele‐\nments using a bit vector instead of a hash table, inspired by an example in Chapter 6\nof the excellent The Go Programming Language, by Alan Donovan and Brian Ker‐\nnighan (Addison-Wesley).\nFurther Reading \n| \n113",
      "content_length": 2807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "IEEE’s Spectrum magazine has a story about Hans Peter Luhn, a prolific inventor\nwho patented a punched card deck to select cocktail recipes depending on ingredi‐\nents available, among other diverse inventions including…hash tables! See “Hans\nPeter Luhn and the Birth of the Hashing Algorithm”.\nSoapbox\nSyntactic Sugar\nMy friend Geraldo Cohen once remarked that Python is “simple and correct.”\nProgramming language purists like to dismiss syntax as unimportant.\nSyntactic sugar causes cancer of the semicolon.\n—Alan Perlis\nSyntax is the user interface of a programming language, so it does matter in practice.\nBefore finding Python, I did some web programming using Perl and PHP. The syntax\nfor mappings in these languages is very useful, and I badly miss it whenever I have to\nuse Java or C.\nA good literal syntax for mappings is very convenient for configuration, table-driven\nimplementations, and to hold data for prototyping and testing. That’s one lesson the\ndesigners of Go learned from dynamic languages. The lack of a good way to express\nstructured data in code pushed the Java community to adopt the verbose and overly\ncomplex XML as a data format.\nJSON was proposed as “The Fat-Free Alternative to XML” and became a huge suc‐\ncess, replacing XML in many contexts. A concise syntax for lists and dictionaries\nmakes an excellent data interchange format.\nPHP and Ruby imitated the hash syntax from Perl, using => to link keys to values.\nJavaScript uses : like Python. Why use two characters when one is readable enough?\nJSON came from JavaScript, but it also happens to be an almost exact subset of\nPython syntax. JSON is compatible with Python except for the spelling of the values\ntrue, false, and null.\nArmin Ronacher tweeted that he likes to hack Python’s global namespace to add\nJSON-compatible aliases for Python’s True, False, and None so he can paste JSON\ndirectly in the console. The basic idea:\n114 \n| \nChapter 3: Dictionaries and Sets",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": ">>> true, false, null = True, False, None\n>>> fruit = {\n...     \"type\": \"banana\",\n...     \"avg_weight\": 123.2,\n...     \"edible_peel\": false,\n...     \"species\": [\"acuminata\", \"balbisiana\", \"paradisiaca\"],\n...     \"issues\": null,\n... }\n>>> fruit\n{'type': 'banana', 'avg_weight': 123.2, 'edible_peel': False,\n'species': ['acuminata', 'balbisiana', 'paradisiaca'], 'issues': None}\nThe syntax everybody now uses for exchanging data is Python’s dict and list syn‐\ntax. Now we have the nice syntax with the convenience of preserved insertion order.\nSimple and correct.\nFurther Reading \n| \n115",
      "content_length": 585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "1 Slide 12 of PyCon 2014 talk “Character Encoding and Unicode in Python” (slides, video).\nCHAPTER 4\nUnicode Text Versus Bytes\nHumans use text. Computers speak bytes.\n—Esther Nam and Travis Fischer, “Character Encoding and Unicode in Python”1\nPython 3 introduced a sharp distinction between strings of human text and sequen‐\nces of raw bytes. Implicit conversion of byte sequences to Unicode text is a thing of\nthe past. This chapter deals with Unicode strings, binary sequences, and the encod‐\nings used to convert between them.\nDepending on the kind of work you do with Python, you may think that understand‐\ning Unicode is not important. That’s unlikely, but anyway there is no escaping the\nstr versus byte divide. As a bonus, you’ll find that the specialized binary sequence\ntypes provide features that the “all-purpose” Python 2 str type did not have.\nIn this chapter, we will visit the following topics:\n• Characters, code points, and byte representations\n• Unique features of binary sequences: bytes, bytearray, and memoryview\n• Encodings for full Unicode and legacy character sets\n• Avoiding and dealing with encoding errors\n• Best practices when handling text files\n• The default encoding trap and standard I/O issues\n• Safe Unicode text comparisons with normalization\n117",
      "content_length": 1280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "• Utility functions for normalization, case folding, and brute-force diacritic\nremoval\n• Proper sorting of Unicode text with locale and the pyuca library\n• Character metadata in the Unicode database\n• Dual-mode APIs that handle str and bytes\nWhat’s New in This Chapter\nSupport for Unicode in Python 3 has been comprehensive and stable, so the most\nnotable addition is “Finding Characters by Name” on page 151, describing a utility for\nsearching the Unicode database—a great way to find circled digits and smiling cats\nfrom the command line.\nOne minor change worth mentioning is the Unicode support on Windows, which is\nbetter and simpler since Python 3.6, as we’ll see in “Beware of Encoding Defaults” on\npage 134.\nLet’s start with the not-so-new, but fundamental concepts of characters, code points,\nand bytes.\nFor the second edition, I expanded the section about the struct\nmodule and published it online at “Parsing binary records with\nstruct”, in the fluentpython.com companion website.\nThere you will also find “Building Multi-character Emojis”,\ndescribing how to make country flags, rainbow flags, people with\ndifferent skin tones, and diverse family icons by combining Uni‐\ncode characters.\nCharacter Issues\nThe concept of “string” is simple enough: a string is a sequence of characters. The\nproblem lies in the definition of “character.”\nIn 2021, the best definition of “character” we have is a Unicode character. Accord‐\ningly, the items we get out of a Python 3 str are Unicode characters, just like the\nitems of a unicode object in Python 2—and not the raw bytes we got from a Python 2\nstr.\nThe Unicode standard explicitly separates the identity of characters from specific\nbyte representations:\n118 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "• The identity of a character—its code point—is a number from 0 to 1,114,111\n(base 10), shown in the Unicode standard as 4 to 6 hex digits with a “U+” prefix,\nfrom U+0000 to U+10FFFF. For example, the code point for the letter A is U\n+0041, the Euro sign is U+20AC, and the musical symbol G clef is assigned to\ncode point U+1D11E. About 13% of the valid code points have characters\nassigned to them in Unicode 13.0.0, the standard used in Python 3.10.0b4.\n• The actual bytes that represent a character depend on the encoding in use. An\nencoding is an algorithm that converts code points to byte sequences and vice\nversa. The code point for the letter A (U+0041) is encoded as the single byte \\x41\nin the UTF-8 encoding, or as the bytes \\x41\\x00 in UTF-16LE encoding. As\nanother example, UTF-8 requires three bytes—\\xe2\\x82\\xac—to encode the\nEuro sign (U+20AC), but in UTF-16LE the same code point is encoded as two\nbytes: \\xac\\x20.\nConverting from code points to bytes is encoding; converting from bytes to code\npoints is decoding. See Example 4-1.\nExample 4-1. Encoding and decoding\n>>> s = 'café'\n>>> len(s)  \n4\n>>> b = s.encode('utf8')  \n>>> b\nb'caf\\xc3\\xa9'  \n>>> len(b)  \n5\n>>> b.decode('utf8')  \n'café'\nThe str 'café' has four Unicode characters.\nEncode str to bytes using UTF-8 encoding.\nbytes literals have a b prefix.\nbytes b has five bytes (the code point for “é” is encoded as two bytes in UTF-8).\nDecode bytes to str using UTF-8 encoding.\nCharacter Issues \n| \n119",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "2 Python 2.6 and 2.7 also had bytes, but it was just an alias to the str type.\nIf you need a memory aid to help distinguish .decode()\nfrom .encode(), convince yourself that byte sequences can be\ncryptic machine core dumps, while Unicode str objects are\n“human” text. Therefore, it makes sense that we decode bytes to\nstr to get human-readable text, and we encode str to bytes for\nstorage or transmission.\nAlthough the Python 3 str is pretty much the Python 2 unicode type with a new\nname, the Python 3 bytes is not simply the old str renamed, and there is also the\nclosely related bytearray type. So it is worthwhile to take a look at the binary\nsequence types before advancing to encoding/decoding issues.\nByte Essentials\nThe new binary sequence types are unlike the Python 2 str in many regards. The first\nthing to know is that there are two basic built-in types for binary sequences: the\nimmutable bytes type introduced in Python 3 and the mutable bytearray, added\nway back in Python 2.6.2 The Python documentation sometimes uses the generic\nterm “byte string” to refer to both bytes and bytearray. I avoid that confusing term.\nEach item in bytes or bytearray is an integer from 0 to 255, and not a one-character\nstring like in the Python 2 str. However, a slice of a binary sequence always produces\na binary sequence of the same type—including slices of length 1. See Example 4-2.\nExample 4-2. A five-byte sequence as bytes and as bytearray\n>>> cafe = bytes('café', encoding='utf_8')  \n>>> cafe\nb'caf\\xc3\\xa9'\n>>> cafe[0]  \n99\n>>> cafe[:1]  \nb'c'\n>>> cafe_arr = bytearray(cafe)\n>>> cafe_arr  \nbytearray(b'caf\\xc3\\xa9')\n>>> cafe_arr[-1:]  \nbytearray(b'\\xa9')\nbytes can be built from a str, given an encoding.\nEach item is an integer in range(256).\n120 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "3 Trivia: the ASCII “single quote” character that Python uses by default as the string delimiter is actually named\nAPOSTROPHE in the Unicode standard. The real single quotes are asymmetric: left is U+2018 and right is\nU+2019.\nSlices of bytes are also bytes—even slices of a single byte.\nThere is no literal syntax for bytearray: they are shown as bytearray() with a\nbytes literal as argument.\nA slice of bytearray is also a bytearray.\nThe fact that my_bytes[0] retrieves an int but my_bytes[:1]\nreturns a bytes sequence of length 1 is only surprising because we\nare used to Python’s str type, where s[0] == s[:1]. For all other\nsequence types in Python, 1 item is not the same as a slice of\nlength 1.\nAlthough binary sequences are really sequences of integers, their literal notation\nreflects the fact that ASCII text is often embedded in them. Therefore, four different\ndisplays are used, depending on each byte value:\n• For bytes with decimal codes 32 to 126—from space to ~ (tilde)—the ASCII char‐\nacter itself is used.\n• For bytes corresponding to tab, newline, carriage return, and \\, the escape\nsequences \\t, \\n, \\r, and \\\\ are used.\n• If both string delimiters ' and \" appear in the byte sequence, the whole sequence\nis delimited by ', and any ' inside are escaped as \\'.3\n• For other byte values, a hexadecimal escape sequence is used (e.g., \\x00 is the\nnull byte).\nThat is why in Example 4-2 you see b'caf\\xc3\\xa9': the first three bytes b'caf' are\nin the printable ASCII range, the last two are not.\nBoth bytes and bytearray support every str method except those that do format‐\nting (format, format_map) and those that depend on Unicode data, including case\nfold, isdecimal, isidentifier, isnumeric, isprintable, and encode. This means\nthat you can use familiar string methods like endswith, replace, strip, translate,\nupper, and dozens of others with binary sequences—only using bytes and not str\narguments. In addition, the regular expression functions in the re module also work\nByte Essentials \n| \n121",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "4 It did not work in Python 3.0 to 3.4, causing much pain to developers dealing with binary data. The reversal is\ndocumented in PEP 461—Adding % formatting to bytes and bytearray.\non binary sequences, if the regex is compiled from a binary sequence instead of a str.\nSince Python 3.5, the % operator works with binary sequences again.4\nBinary sequences have a class method that str doesn’t have, called fromhex, which\nbuilds a binary sequence by parsing pairs of hex digits optionally separated by spaces:\n>>> bytes.fromhex('31 4B CE A9')\nb'1K\\xce\\xa9'\nThe other ways of building bytes or bytearray instances are calling their construc‐\ntors with:\n• A str and an encoding keyword argument\n• An iterable providing items with values from 0 to 255\n• An object that implements the buffer protocol (e.g., bytes, bytearray, memory\nview, array.array) that copies the bytes from the source object to the newly cre‐\nated binary sequence\nUntil Python 3.5, it was also possible to call bytes or bytearray\nwith a single integer to create a binary sequence of that size initial‐\nized with null bytes. This signature was deprecated in Python 3.5\nand removed in Python 3.6. See PEP 467—Minor API improve‐\nments for binary sequences.\nBuilding a binary sequence from a buffer-like object is a low-level operation that may\ninvolve type casting. See a demonstration in Example 4-3.\nExample 4-3. Initializing bytes from the raw data of an array\n>>> import array\n>>> numbers = array.array('h', [-2, -1, 0, 1, 2])  \n>>> octets = bytes(numbers)  \n>>> octets\nb'\\xfe\\xff\\xff\\xff\\x00\\x00\\x01\\x00\\x02\\x00'  \nTypecode 'h' creates an array of short integers (16 bits).\noctets holds a copy of the bytes that make up numbers.\nThese are the 10 bytes that represent the 5 short integers.\n122 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "Creating a bytes or bytearray object from any buffer-like source will always copy\nthe bytes. In contrast, memoryview objects let you share memory between binary data\nstructures, as we saw in “Memory Views” on page 62.\nAfter this basic exploration of binary sequence types in Python, let’s see how they are\nconverted to/from strings.\nBasic Encoders/Decoders\nThe Python distribution bundles more than 100 codecs (encoder/decoders) for text to\nbyte conversion and vice versa. Each codec has a name, like 'utf_8', and often\naliases, such as 'utf8', 'utf-8', and 'U8', which you can use as the encoding argu‐\nment in functions like open(), str.encode(), bytes.decode(), and so on.\nExample 4-4 shows the same text encoded as three different byte sequences.\nExample 4-4. The string “El Niño” encoded with three codecs producing very different\nbyte sequences\n>>> for codec in ['latin_1', 'utf_8', 'utf_16']:\n...     print(codec, 'El Niño'.encode(codec), sep='\\t')\n...\nlatin_1 b'El Ni\\xf1o'\nutf_8   b'El Ni\\xc3\\xb1o'\nutf_16  b'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'\nFigure 4-1 demonstrates a variety of codecs generating bytes from characters like the\nletter “A” through the G-clef musical symbol. Note that the last three encodings are\nvariable-length, multibyte encodings.\nFigure 4-1. Twelve characters, their code points, and their byte representation (in hex)\nin 7 different encodings (asterisks indicate that the character cannot be represented in\nthat encoding).\nBasic Encoders/Decoders \n| \n123",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "All those asterisks in Figure 4-1 make clear that some encodings, like ASCII and even\nthe multibyte GB2312, cannot represent every Unicode character. The UTF encod‐\nings, however, are designed to handle every Unicode code point.\nThe encodings shown in Figure 4-1 were chosen as a representative sample:\nlatin1 a.k.a. iso8859_1\nImportant because it is the basis for other encodings, such as cp1252 and Uni‐\ncode itself (note how the latin1 byte values appear in the cp1252 bytes and even\nin the code points).\ncp1252\nA useful latin1 superset created by Microsoft, adding useful symbols like curly\nquotes and € (euro); some Windows apps call it “ANSI,” but it was never a real\nANSI standard.\ncp437\nThe original character set of the IBM PC, with box drawing characters. Incom‐\npatible with latin1, which appeared later.\ngb2312\nLegacy standard to encode the simplified Chinese ideographs used in mainland\nChina; one of several widely deployed multibyte encodings for Asian languages.\nutf-8\nThe most common 8-bit encoding on the web, by far, as of July 2021, “W3Techs:\nUsage statistics of character encodings for websites” claims that 97% of sites use\nUTF-8, up from 81.4% when I wrote this paragraph in the first edition of this\nbook in September 2014.\nutf-16le\nOne form of the UTF 16-bit encoding scheme; all UTF-16 encodings support\ncode points beyond U+FFFF through escape sequences called “surrogate pairs.”\nUTF-16 superseded the original 16-bit Unicode 1.0 encoding—\nUCS-2—way back in 1996. UCS-2 is still used in many systems\ndespite being deprecated since the last century because it only sup‐\nports code points up to U+FFFF. As of 2021, more than 57% of the\nallocated code points are above U+FFFF, including the all-\nimportant emojis.\nWith this overview of common encodings now complete, we move to handling issues\nin encoding and decoding operations.\n124 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Understanding Encode/Decode Problems\nAlthough there is a generic UnicodeError exception, the error reported by Python is\nusually more specific: either a UnicodeEncodeError (when converting str to binary\nsequences) or a UnicodeDecodeError (when reading binary sequences into str).\nLoading Python modules may also raise SyntaxError when the source encoding is\nunexpected. We’ll show how to handle all of these errors in the next sections.\nThe first thing to note when you get a Unicode error is the exact\ntype of the exception. Is it a UnicodeEncodeError, a UnicodeDeco\ndeError, or some other error (e.g., SyntaxError) that mentions an\nencoding problem? To solve the problem, you have to understand\nit first.\nCoping with UnicodeEncodeError\nMost non-UTF codecs handle only a small subset of the Unicode characters. When\nconverting text to bytes, if a character is not defined in the target encoding, Unico\ndeEncodeError will be raised, unless special handling is provided by passing an\nerrors argument to the encoding method or function. The behavior of the error han‐\ndlers is shown in Example 4-5.\nExample 4-5. Encoding to bytes: success and error handling\n>>> city = 'São Paulo'\n>>> city.encode('utf_8')  \nb'S\\xc3\\xa3o Paulo'\n>>> city.encode('utf_16')\nb'\\xff\\xfeS\\x00\\xe3\\x00o\\x00 \\x00P\\x00a\\x00u\\x00l\\x00o\\x00'\n>>> city.encode('iso8859_1')  \nb'S\\xe3o Paulo'\n>>> city.encode('cp437')  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/.../lib/python3.4/encodings/cp437.py\", line 12, in encode\n    return codecs.charmap_encode(input,errors,encoding_map)\nUnicodeEncodeError: 'charmap' codec can't encode character '\\xe3' in\nposition 1: character maps to <undefined>\n>>> city.encode('cp437', errors='ignore')  \nb'So Paulo'\n>>> city.encode('cp437', errors='replace')  \nb'S?o Paulo'\n>>> city.encode('cp437', errors='xmlcharrefreplace')  \nb'S&#227;o Paulo'\nUnderstanding Encode/Decode Problems \n| \n125",
      "content_length": 1928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "The UTF encodings handle any str.\niso8859_1 also works for the 'São Paulo' string.\ncp437 can’t encode the 'ã' (“a” with tilde). The default error handler\n—'strict'—raises UnicodeEncodeError.\nThe error='ignore' handler skips characters that cannot be encoded; this is\nusually a very bad idea, leading to silent data loss.\nWhen encoding, error='replace' substitutes unencodable characters with '?';\ndata is also lost, but users will get a clue that something is amiss.\n'xmlcharrefreplace' replaces unencodable characters with an XML entity. If\nyou can’t use UTF, and you can’t afford to lose data, this is the only option.\nThe codecs error handling is extensible. You may register extra\nstrings for the errors argument by passing a name and an error\nhandling function to the codecs.register_error function. See the\ncodecs.register_error documentation.\nASCII is a common subset to all the encodings that I know about, therefore encoding\nshould always work if the text is made exclusively of ASCII characters. Python 3.7\nadded a new boolean method str.isascii() to check whether your Unicode text is\n100% pure ASCII. If it is, you should be able to encode it to bytes in any encoding\nwithout raising UnicodeEncodeError.\nCoping with UnicodeDecodeError\nNot every byte holds a valid ASCII character, and not every byte sequence is valid\nUTF-8 or UTF-16; therefore, when you assume one of these encodings while convert‐\ning a binary sequence to text, you will get a UnicodeDecodeError if unexpected bytes\nare found.\nOn the other hand, many legacy 8-bit encodings like 'cp1252', 'iso8859_1', and\n'koi8_r' are able to decode any stream of bytes, including random noise, without\nreporting errors. Therefore, if your program assumes the wrong 8-bit encoding, it\nwill silently decode garbage.\n126 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "Garbled characters are known as gremlins or mojibake (文字化け\n—Japanese for “transformed text”).\nExample 4-6 illustrates how using the wrong codec may produce gremlins or a\nUnicodeDecodeError.\nExample 4-6. Decoding from str to bytes: success and error handling\n>>> octets = b'Montr\\xe9al'  \n>>> octets.decode('cp1252')  \n'Montréal'\n>>> octets.decode('iso8859_7')  \n'Montrιal'\n>>> octets.decode('koi8_r')  \n'MontrИal'\n>>> octets.decode('utf_8')  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 5:\ninvalid continuation byte\n>>> octets.decode('utf_8', errors='replace')  \n'MontrÝal'\nThe word “Montréal” encoded as latin1; '\\xe9' is the byte for “é”.\nDecoding with Windows 1252 works because it is a superset of latin1.\nISO-8859-7 is intended for Greek, so the '\\xe9' byte is misinterpreted, and no\nerror is issued.\nKOI8-R is for Russian. Now '\\xe9' stands for the Cyrillic letter “И”.\nThe 'utf_8' codec detects that octets is not valid UTF-8, and raises UnicodeDe\ncodeError.\nUsing 'replace' error handling, the \\xe9 is replaced by “ແ” (code point\nU+FFFD), the official Unicode REPLACEMENT CHARACTER intended to represent\nunknown characters.\nUnderstanding Encode/Decode Problems \n| \n127",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "SyntaxError When Loading Modules with Unexpected Encoding\nUTF-8 is the default source encoding for Python 3, just as ASCII was the default for\nPython 2. If you load a .py module containing non-UTF-8 data and no encoding dec‐\nlaration, you get a message like this:\nSyntaxError: Non-UTF-8 code starting with '\\xe1' in file ola.py on line\n  1, but no encoding declared; see https://python.org/dev/peps/pep-0263/\n  for details\nBecause UTF-8 is widely deployed in GNU/Linux and macOS systems, a likely sce‐\nnario is opening a .py file created on Windows with cp1252. Note that this error hap‐\npens even in Python for Windows, because the default encoding for Python 3 source\nis UTF-8 across all platforms.\nTo fix this problem, add a magic coding comment at the top of the file, as shown in\nExample 4-7.\nExample 4-7. ola.py: “Hello, World!” in Portuguese\n# coding: cp1252\nprint('Olá, Mundo!')\nNow that Python 3 source code is no longer limited to ASCII and\ndefaults to the excellent UTF-8 encoding, the best “fix” for source\ncode in legacy encodings like 'cp1252' is to convert them to\nUTF-8 already, and not bother with the coding comments. If your\neditor does not support UTF-8, it’s time to switch.\nSuppose you have a text file, be it source code or poetry, but you don’t know its\nencoding. How do you detect the actual encoding? Answers in the next section.\nHow to Discover the Encoding of a Byte Sequence\nHow do you find the encoding of a byte sequence? Short answer: you can’t. You must\nbe told.\nSome communication protocols and file formats, like HTTP and XML, contain head‐\ners that explicitly tell us how the content is encoded. You can be sure that some byte\nstreams are not ASCII because they contain byte values over 127, and the way UTF-8\nand UTF-16 are built also limits the possible byte sequences.\n128 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "Leo’s Hack for Guessing UTF-8 Decoding\n(The next paragraphs come from a note left by tech reviewer Leonardo Rochael in the\ndraft of this book.)\nThe way UTF-8 was designed, it’s almost impossible for a random sequence of bytes,\nor even a nonrandom sequence of bytes coming from a non-UTF-8 encoding, to be\ndecoded accidentally as garbage in UTF-8, instead of raising UnicodeDecodeError.\nThe reasons for this are that UTF-8 escape sequences never use ASCII characters, and\nthese escape sequences have bit patterns that make it very hard for random data to be\nvalid UTF-8 by accident.\nSo if you can decode some bytes containing codes > 127 as UTF-8, it’s probably\nUTF-8.\nIn dealing with Brazilian online services, some of which were attached to legacy back‐\nends, I’ve had, on occasion, to implement a decoding strategy of trying to decode via\nUTF-8 and treat a UnicodeDecodeError by decoding via cp1252. It was ugly but\neffective.\nHowever, considering that human languages also have their rules and restrictions,\nonce you assume that a stream of bytes is human plain text, it may be possible to sniff\nout its encoding using heuristics and statistics. For example, if b'\\x00' bytes are\ncommon, it is probably a 16- or 32-bit encoding, and not an 8-bit scheme, because\nnull characters in plain text are bugs. When the byte sequence b'\\x20\\x00' appears\noften, it is more likely to be the space character (U+0020) in a UTF-16LE encoding,\nrather than the obscure U+2000 EN QUAD character—whatever that is.\nThat is how the package “Chardet—The Universal Character Encoding Detector”\nworks to guess one of more than 30 supported encodings. Chardet is a Python library\nthat you can use in your programs, but also includes a command-line utility, charde\ntect. Here is what it reports on the source file for this chapter:\n$ chardetect 04-text-byte.asciidoc\n04-text-byte.asciidoc: utf-8 with confidence 0.99\nAlthough binary sequences of encoded text usually don’t carry explicit hints of their\nencoding, the UTF formats may prepend a byte order mark to the textual content.\nThat is explained next.\nBOM: A Useful Gremlin\nIn Example 4-4, you may have noticed a couple of extra bytes at the beginning of a\nUTF-16 encoded sequence. Here they are again:\nUnderstanding Encode/Decode Problems \n| \n129",
      "content_length": 2280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": ">>> u16 = 'El Niño'.encode('utf_16')\n>>> u16\nb'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'\nThe bytes are b'\\xff\\xfe'. That is a BOM—byte-order mark—denoting the “little-\nendian” byte ordering of the Intel CPU where the encoding was performed.\nOn a little-endian machine, for each code point the least significant byte comes first:\nthe letter 'E', code point U+0045 (decimal 69), is encoded in byte offsets 2 and 3 as\n69 and 0:\n>>> list(u16)\n[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]\nOn a big-endian CPU, the encoding would be reversed; 'E' would be encoded as 0\nand 69.\nTo avoid confusion, the UTF-16 encoding prepends the text to be encoded with the\nspecial invisible character ZERO WIDTH NO-BREAK SPACE (U+FEFF). On a little-\nendian system, that is encoded as b'\\xff\\xfe' (decimal 255, 254). Because, by\ndesign, there is no U+FFFE character in Unicode, the byte sequence b'\\xff\\xfe'\nmust mean the ZERO WIDTH NO-BREAK SPACE on a little-endian encoding, so the\ncodec knows which byte ordering to use.\nThere is a variant of UTF-16—UTF-16LE—that is explicitly little-endian, and\nanother one explicitly big-endian, UTF-16BE. If you use them, a BOM is not\ngenerated:\n>>> u16le = 'El Niño'.encode('utf_16le')\n>>> list(u16le)\n[69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]\n>>> u16be = 'El Niño'.encode('utf_16be')\n>>> list(u16be)\n[0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]\nIf present, the BOM is supposed to be filtered by the UTF-16 codec, so that you only\nget the actual text contents of the file without the leading ZERO WIDTH NO-BREAK\nSPACE. The Unicode standard says that if a file is UTF-16 and has no BOM, it should\nbe assumed to be UTF-16BE (big-endian). However, the Intel x86 architecture is\nlittle-endian, so there is plenty of little-endian UTF-16 with no BOM in the wild.\nThis whole issue of endianness only affects encodings that use words of more than\none byte, like UTF-16 and UTF-32. One big advantage of UTF-8 is that it produces\nthe same byte sequence regardless of machine endianness, so no BOM is needed.\nNevertheless, some Windows applications (notably Notepad) add the BOM to UTF-8\nfiles anyway—and Excel depends on the BOM to detect a UTF-8 file, otherwise it\nassumes the content is encoded with a Windows code page. This UTF-8 encoding\nwith BOM is called UTF-8-SIG in Python’s codec registry. The character U+FEFF\n130 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 2421,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "5 I first saw the term “Unicode sandwich” in Ned Batchelder’s excellent “Pragmatic Unicode” talk at US PyCon\n2012.\nencoded in UTF-8-SIG is the three-byte sequence b'\\xef\\xbb\\xbf'. So if a file starts\nwith those three bytes, it is likely to be a UTF-8 file with a BOM.\nCaleb’s Tip about UTF-8-SIG\nCaleb Hattingh—one of the tech reviewers—suggests always using\nthe UTF-8-SIG codec when reading UTF-8 files. This is harmless\nbecause UTF-8-SIG reads files with or without a BOM correctly,\nand does not return the BOM itself. When writing, I recommend\nusing UTF-8 for general interoperability. For example, Python\nscripts can be made executable in Unix systems if they start with\nthe comment: #!/usr/bin/env python3. The first two bytes of the\nfile must be b'#!' for that to work, but the BOM breaks that con‐\nvention. If you have a specific requirement to export data to apps\nthat need the BOM, use UTF-8-SIG but be aware that Python’s\ncodecs documentation says: “In UTF-8, the use of the BOM is dis‐\ncouraged and should generally be avoided.”\nWe now move on to handling text files in Python 3.\nHandling Text Files\nThe best practice for handling text I/O is the “Unicode sandwich” (Figure 4-2).5 This\nmeans that bytes should be decoded to str as early as possible on input (e.g., when\nopening a file for reading). The “filling” of the sandwich is the business logic of your\nprogram, where text handling is done exclusively on str objects. You should never be\nencoding or decoding in the middle of other processing. On output, the str are enco‐\nded to bytes as late as possible. Most web frameworks work like that, and we rarely\ntouch bytes when using them. In Django, for example, your views should output\nUnicode str; Django itself takes care of encoding the response to bytes, using UTF-8\nby default.\nPython 3 makes it easier to follow the advice of the Unicode sandwich, because the\nopen() built-in does the necessary decoding when reading and encoding when\nwriting files in text mode, so all you get from my_file.read() and pass to\nmy_file.write(text) are str objects.\nTherefore, using text files is apparently simple. But if you rely on default encodings,\nyou will get bitten.\nHandling Text Files \n| \n131",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Figure 4-2. Unicode sandwich: current best practice for text processing.\nConsider the console session in Example 4-8. Can you spot the bug?\nExample 4-8. A platform encoding issue (if you try this on your machine, you may or\nmay not see the problem)\n>>> open('cafe.txt', 'w', encoding='utf_8').write('café')\n4\n>>> open('cafe.txt').read()\n'cafÃ©'\nThe bug: I specified UTF-8 encoding when writing the file but failed to do so when\nreading it, so Python assumed Windows default file encoding—code page 1252—and\nthe trailing bytes in the file were decoded as characters 'Ã©' instead of 'é'.\nI ran Example 4-8 on Python 3.8.1, 64 bits, on Windows 10 (build 18363). The same\nstatements running on recent GNU/Linux or macOS work perfectly well because\ntheir default encoding is UTF-8, giving the false impression that everything is fine. If\nthe encoding argument was omitted when opening the file to write, the locale default\nencoding would be used, and we’d read the file correctly using the same encoding.\nBut then this script would generate files with different byte contents depending on\nthe platform or even depending on locale settings in the same platform, creating\ncompatibility problems.\nCode that has to run on multiple machines or on multiple occa‐\nsions should never depend on encoding defaults. Always pass an\nexplicit encoding= argument when opening text files, because the\ndefault may change from one machine to the next, or from one day\nto the next.\n132 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "A curious detail in Example 4-8 is that the write function in the first statement\nreports that four characters were written, but in the next line five characters are read.\nExample 4-9 is an extended version of Example 4-8, explaining that and other details.\nExample 4-9. Closer inspection of Example 4-8 running on Windows reveals the bug\nand how to fix it\n>>> fp = open('cafe.txt', 'w', encoding='utf_8')\n>>> fp  \n<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>\n>>> fp.write('café')  \n4\n>>> fp.close()\n>>> import os\n>>> os.stat('cafe.txt').st_size  \n5\n>>> fp2 = open('cafe.txt')\n>>> fp2  \n<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>\n>>> fp2.encoding  \n'cp1252'\n>>> fp2.read() \n'cafÃ©'\n>>> fp3 = open('cafe.txt', encoding='utf_8')  \n>>> fp3\n<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>\n>>> fp3.read() \n'café'\n>>> fp4 = open('cafe.txt', 'rb')  \n>>> fp4                           \n<_io.BufferedReader name='cafe.txt'>\n>>> fp4.read()  \nb'caf\\xc3\\xa9'\nBy default, open uses text mode and returns a TextIOWrapper object with a spe‐\ncific encoding.\nThe write method on a TextIOWrapper returns the number of Unicode charac‐\nters written.\nos.stat says the file has 5 bytes; UTF-8 encodes 'é' as 2 bytes, 0xc3 and 0xa9.\nOpening a text file with no explicit encoding returns a TextIOWrapper with the\nencoding set to a default from the locale.\nA TextIOWrapper object has an encoding attribute that you can inspect: cp1252\nin this case.\nHandling Text Files \n| \n133",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "In the Windows cp1252 encoding, the byte 0xc3 is an “Ã” (A with tilde), and\n0xa9 is the copyright sign.\nOpening the same file with the correct encoding.\nThe expected result: the same four Unicode characters for 'café'.\nThe 'rb' flag opens a file for reading in binary mode.\nThe returned object is a BufferedReader and not a TextIOWrapper.\nReading that returns bytes, as expected.\nDo not open text files in binary mode unless you need to analyze\nthe file contents to determine the encoding—even then, you should\nbe using Chardet instead of reinventing the wheel (see “How to\nDiscover the Encoding of a Byte Sequence” on page 128). Ordinary\ncode should only use binary mode to open binary files, like raster\nimages.\nThe problem in Example 4-9 has to do with relying on a default setting while opening\na text file. There are several sources for such defaults, as the next section shows.\nBeware of Encoding Defaults\nSeveral settings affect the encoding defaults for I/O in Python. See the default_encod‐\nings.py script in Example 4-10.\nExample 4-10. Exploring encoding defaults\nimport locale\nimport sys\nexpressions = \"\"\"\n        locale.getpreferredencoding()\n        type(my_file)\n        my_file.encoding\n        sys.stdout.isatty()\n        sys.stdout.encoding\n        sys.stdin.isatty()\n        sys.stdin.encoding\n        sys.stderr.isatty()\n        sys.stderr.encoding\n        sys.getdefaultencoding()\n        sys.getfilesystemencoding()\n    \"\"\"\n134 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "my_file = open('dummy', 'w')\nfor expression in expressions.split():\n    value = eval(expression)\n    print(f'{expression:>30} -> {value!r}')\nThe output of Example 4-10 on GNU/Linux (Ubuntu 14.04 to 19.10) and macOS\n(10.9 to 10.14) is identical, showing that UTF-8 is used everywhere in these systems:\n$ python3 default_encodings.py\n locale.getpreferredencoding() -> 'UTF-8'\n                 type(my_file) -> <class '_io.TextIOWrapper'>\n              my_file.encoding -> 'UTF-8'\n           sys.stdout.isatty() -> True\n           sys.stdout.encoding -> 'utf-8'\n            sys.stdin.isatty() -> True\n            sys.stdin.encoding -> 'utf-8'\n           sys.stderr.isatty() -> True\n           sys.stderr.encoding -> 'utf-8'\n      sys.getdefaultencoding() -> 'utf-8'\n   sys.getfilesystemencoding() -> 'utf-8'\nOn Windows, however, the output is Example 4-11.\nExample 4-11. Default encodings on Windows 10 PowerShell (output is the same on\ncmd.exe)\n> chcp  \nActive code page: 437\n> python default_encodings.py  \n locale.getpreferredencoding() -> 'cp1252'  \n                 type(my_file) -> <class '_io.TextIOWrapper'>\n              my_file.encoding -> 'cp1252'  \n           sys.stdout.isatty() -> True      \n           sys.stdout.encoding -> 'utf-8'   \n            sys.stdin.isatty() -> True\n            sys.stdin.encoding -> 'utf-8'\n           sys.stderr.isatty() -> True\n           sys.stderr.encoding -> 'utf-8'\n      sys.getdefaultencoding() -> 'utf-8'\n   sys.getfilesystemencoding() -> 'utf-8'\nchcp shows the active code page for the console: 437.\nRunning default_encodings.py with output to console.\nlocale.getpreferredencoding() is the most important setting.\nText files use locale.getpreferredencoding() by default.\nHandling Text Files \n| \n135",
      "content_length": 1746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "6 Source: “Windows Command-Line: Unicode and UTF-8 Output Text Buffer”.\nThe output is going to the console, so sys.stdout.isatty() is True.\nNow, sys.stdout.encoding is not the same as the console code page reported by\nchcp!\nUnicode support in Windows itself, and in Python for Windows, got better since I\nwrote the first edition of this book. Example 4-11 used to report four different encod‐\nings in Python 3.4 on Windows 7. The encodings for stdout, stdin, and stderr used\nto be the same as the active code page reported by the chcp command, but now\nthey’re all utf-8 thanks to PEP 528—Change Windows console encoding to UTF-8\nimplemented in Python 3.6, and Unicode support in PowerShell in cmd.exe (since\nWindows 1809 from October 2018).6 It’s weird that chcp and sys.stdout.encoding\nsay different things when stdout is writing to the console, but it’s great that now we\ncan print Unicode strings without encoding errors on Windows—unless the user\nredirects output to a file, as we’ll soon see. That does not mean all your favorite emo‐\njis will appear in the console: that also depends on the font the console is using.\nAnother change was PEP 529—Change Windows filesystem encoding to UTF-8, also\nimplemented in Python 3.6, which changed the filesystem encoding (used to repre‐\nsent names of directories and files) from Microsoft’s proprietary MBCS to UTF-8.\nHowever, if the output of Example 4-10 is redirected to a file, like this:\nZ:\\>python default_encodings.py > encodings.log\nthen, the value of sys.stdout.isatty() becomes False, and sys.stdout.encoding\nis set by locale.getpreferredencoding(), 'cp1252' in that machine—but\nsys.stdin.encoding and sys.stderr.encoding remain utf-8.\nIn Example 4-12 I use the '\\N{}' escape for Unicode literals,\nwhere we write the official name of the character inside the \\N{}.\nIt’s rather verbose, but explicit and safe: Python raises SyntaxError\nif the name doesn’t exist—much better than writing a hex number\nthat could be wrong, but you’ll only find out much later. You’d\nprobably want to write a comment explaining the character codes\nanyway, so the verbosity of \\N{} is easy to accept.\nThis means that a script like Example 4-12 works when printing to the console, but\nmay break when output is redirected to a file.\n136 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Example 4-12. stdout_check.py\nimport sys\nfrom unicodedata import name\nprint(sys.version)\nprint()\nprint('sys.stdout.isatty():', sys.stdout.isatty())\nprint('sys.stdout.encoding:', sys.stdout.encoding)\nprint()\ntest_chars = [\n    '\\N{HORIZONTAL ELLIPSIS}',       # exists in cp1252, not in cp437\n    '\\N{INFINITY}',                  # exists in cp437, not in cp1252\n    '\\N{CIRCLED NUMBER FORTY TWO}',  # not in cp437 or in cp1252\n]\nfor char in test_chars:\n    print(f'Trying to output {name(char)}:')\n    print(char)\nExample 4-12 displays the result of sys.stdout.isatty(), the value of sys.\nstdout.encoding, and these three characters:\n• '…' HORIZONTAL ELLIPSIS—exists in CP 1252 but not in CP 437.\n• '∞' INFINITY—exists in CP 437 but not in CP 1252.\n• '㊷' CIRCLED NUMBER FORTY TWO—doesn’t exist in CP 1252 or CP 437.\nWhen I run stdout_check.py on PowerShell or cmd.exe, it works as captured in\nFigure 4-3.\nFigure 4-3. Running stdout_check.py on PowerShell.\nHandling Text Files \n| \n137",
      "content_length": 983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Despite chcp reporting the active code as 437, sys.stdout.encoding is UTF-8, so the\nHORIZONTAL ELLIPSIS and INFINITY both output correctly. The CIRCLED NUMBER\nFORTY TWO is replaced by a rectangle, but no error is raised. Presumably it is recog‐\nnized as a valid character, but the console font doesn’t have the glyph to display it.\nHowever, when I redirect the output of stdout_check.py to a file, I get Figure 4-4.\nFigure 4-4. Running stdout_check.py on PowerShell, redirecting output.\nThe first problem demonstrated by Figure 4-4 is the UnicodeEncodeError mention‐\ning character '\\u221e', because sys.stdout.encoding is 'cp1252'—a code page\nthat doesn’t have the INFINITY character.\nReading out.txt with the type command—or a Windows editor like VS Code or Sub‐\nlime Text—shows that instead of HORIZONTAL ELLIPSIS, I got 'à' (LATIN SMALL\nLETTER A WITH GRAVE). As it turns out, the byte value 0x85 in CP 1252 means '…',\nbut in CP 437 the same byte value represents 'à'. So it seems the active code page\ndoes matter, not in a sensible or useful way, but as partial explanation of a bad Uni‐\ncode experience.\nI used a laptop configured for the US market, running Windows 10\nOEM to run these experiments. Windows versions localized for\nother countries may have different encoding configurations. For\nexample, in Brazil the Windows console uses code page 850 by\ndefault—not 437.\nTo wrap up this maddening issue of default encodings, let’s give a final look at the\ndifferent encodings in Example 4-11:\n138 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "• If you omit the encoding argument when opening a file, the default is given by\nlocale.getpreferredencoding() ('cp1252' in Example 4-11).\n• The encoding of sys.stdout|stdin|stderr used to be set by the PYTHONIOENCOD\nING environment variable before Python 3.6—now that variable is ignored,\nunless PYTHONLEGACYWINDOWSSTDIO is set to a nonempty string. Otherwise, the\nencoding for standard I/O is UTF-8 for interactive I/O, or defined by\nlocale.getpreferredencoding() if the output/input is redirected to/from a file.\n• sys.getdefaultencoding() is used internally by Python in implicit conversions\nof binary data to/from str. Changing this setting is not supported.\n• sys.getfilesystemencoding() is used to encode/decode filenames (not file\ncontents). It is used when open() gets a str argument for the filename; if the file‐\nname is given as a bytes argument, it is passed unchanged to the OS API.\nOn GNU/Linux and macOS, all of these encodings are set to\nUTF-8 by default, and have been for several years, so I/O handles\nall Unicode characters. On Windows, not only are different encod‐\nings used in the same system, but they are usually code pages like\n'cp850' or 'cp1252' that support only ASCII, with 127 additional\ncharacters that are not the same from one encoding to the other.\nTherefore, Windows users are far more likely to face encoding\nerrors unless they are extra careful.\nTo summarize, the most important encoding setting is that returned by locale.get\npreferredencoding(): it is the default for opening text files and for sys.stdout/\nstdin/stderr when they are redirected to files. However, the documentation reads\n(in part):\nlocale.getpreferredencoding(do_setlocale=True)\nReturn the encoding used for text data, according to user preferences. User pref‐\nerences are expressed differently on different systems, and might not be available\nprogrammatically on some systems, so this function only returns a guess. […]\nTherefore, the best advice about encoding defaults is: do not rely on them.\nYou will avoid a lot of pain if you follow the advice of the Unicode sandwich and\nalways are explicit about the encodings in your programs. Unfortunately, Unicode is\npainful even if you get your bytes correctly converted to str. The next two sections\ncover subjects that are simple in ASCII-land, but get quite complex on planet Uni‐\ncode: text normalization (i.e., converting text to a uniform representation for\ncomparisons) and sorting.\nHandling Text Files \n| \n139",
      "content_length": 2471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Normalizing Unicode for Reliable Comparisons\nString comparisons are complicated by the fact that Unicode has combining charac‐\nters: diacritics and other marks that attach to the preceding character, appearing as\none when printed.\nFor example, the word “café” may be composed in two ways, using four or five code\npoints, but the result looks exactly the same:\n>>> s1 = 'café'\n>>> s2 = 'cafe\\N{COMBINING ACUTE ACCENT}'\n>>> s1, s2\n('café', 'café')\n>>> len(s1), len(s2)\n(4, 5)\n>>> s1 == s2\nFalse\nPlacing COMBINING ACUTE ACCENT (U+0301) after “e” renders “é”. In the Unicode\nstandard, sequences like 'é' and 'e\\u0301' are called “canonical equivalents,” and\napplications are supposed to treat them as the same. But Python sees two different\nsequences of code points, and considers them not equal.\nThe solution is unicodedata.normalize(). The first argument to that function is one\nof four strings: 'NFC', 'NFD', 'NFKC', and 'NFKD'. Let’s start with the first two.\nNormalization Form C (NFC) composes the code points to produce the shortest\nequivalent string, while NFD decomposes, expanding composed characters into base\ncharacters and separate combining characters. Both of these normalizations make\ncomparisons work as expected, as the next example shows:\n>>> from unicodedata import normalize\n>>> s1 = 'café'\n>>> s2 = 'cafe\\N{COMBINING ACUTE ACCENT}'\n>>> len(s1), len(s2)\n(4, 5)\n>>> len(normalize('NFC', s1)), len(normalize('NFC', s2))\n(4, 4)\n>>> len(normalize('NFD', s1)), len(normalize('NFD', s2))\n(5, 5)\n>>> normalize('NFC', s1) == normalize('NFC', s2)\nTrue\n>>> normalize('NFD', s1) == normalize('NFD', s2)\nTrue\nKeyboard drivers usually generate composed characters, so text typed by users will be\nin NFC by default. However, to be safe, it may be good to normalize strings with\nnormalize('NFC', user_text) before saving. NFC is also the normalization form\nrecommended by the W3C in “Character Model for the World Wide Web: String\nMatching and Searching”.\n140 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 2002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "7 Curiously, the micro sign is considered a “compatibility character,” but the ohm symbol is not. The end result\nis that NFC doesn’t touch the micro sign but changes the ohm symbol to capital omega, while NFKC and\nNFKD change both the ohm and the micro into Greek characters.\nSome single characters are normalized by NFC into another single character. The\nsymbol for the ohm (Ω) unit of electrical resistance is normalized to the Greek upper‐\ncase omega. They are visually identical, but they compare as unequal, so it is essential\nto normalize to avoid surprises:\n>>> from unicodedata import normalize, name\n>>> ohm = '\\u2126'\n>>> name(ohm)\n'OHM SIGN'\n>>> ohm_c = normalize('NFC', ohm)\n>>> name(ohm_c)\n'GREEK CAPITAL LETTER OMEGA'\n>>> ohm == ohm_c\nFalse\n>>> normalize('NFC', ohm) == normalize('NFC', ohm_c)\nTrue\nThe other two normalization forms are NFKC and NFKD, where the letter K stands\nfor “compatibility.” These are stronger forms of normalization, affecting the so-called\n“compatibility characters.” Although one goal of Unicode is to have a single “canoni‐\ncal” code point for each character, some characters appear more than once for\ncompatibility with preexisting standards. For example, the MICRO SIGN, µ (U+00B5),\nwas added to Unicode to support round-trip conversion to latin1, which includes it,\neven though the same character is part of the Greek alphabet with code point U+03BC\n(GREEK SMALL LETTER MU). So, the micro sign is considered a “compatibility\ncharacter.”\nIn the NFKC and NFKD forms, each compatibility character is replaced by a “com‐\npatibility decomposition” of one or more characters that are considered a “preferred”\nrepresentation, even if there is some formatting loss—ideally, the formatting should\nbe the responsibility of external markup, not part of Unicode. To exemplify, the\ncompatibility decomposition of the one-half fraction '½' (U+00BD) is the sequence of\nthree characters '1/2', and the compatibility decomposition of the micro sign 'µ' (U\n+00B5) is the lowercase mu 'μ' (U+03BC).7\nHere is how the NFKC works in practice:\n>>> from unicodedata import normalize, name\n>>> half = '\\N{VULGAR FRACTION ONE HALF}'\n>>> print(half)\n½\n>>> normalize('NFKC', half)\n'1⁄2'\nNormalizing Unicode for Reliable Comparisons \n| \n141",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": ">>> for char in normalize('NFKC', half):\n...     print(char, name(char), sep='\\t')\n...\n1 DIGIT ONE\n⁄ FRACTION SLASH\n2 DIGIT TWO\n>>> four_squared = '4²'\n>>> normalize('NFKC', four_squared)\n'42'\n>>> micro = 'µ'\n>>> micro_kc = normalize('NFKC', micro)\n>>> micro, micro_kc\n('µ', 'μ')\n>>> ord(micro), ord(micro_kc)\n(181, 956)\n>>> name(micro), name(micro_kc)\n('MICRO SIGN', 'GREEK SMALL LETTER MU')\nAlthough '1⁄2' is a reasonable substitute for '½', and the micro sign is really a low‐\nercase Greek mu, converting '4²' to '42' changes the meaning. An application\ncould store '4²' as '4<sup>2</sup>', but the normalize function knows nothing\nabout formatting. Therefore, NFKC or NFKD may lose or distort information,\nbut they can produce convenient intermediate representations for searching and\nindexing.\nUnfortunately, with Unicode everything is always more complicated than it first\nseems. For the VULGAR FRACTION ONE HALF, the NFKC normalization produced 1\nand 2 joined by FRACTION SLASH, instead of SOLIDUS, a.k.a. “slash”—the familiar\ncharacter with ASCII code decimal 47. Therefore, searching for the three-character\nASCII sequence '1/2' would not find the normalized Unicode sequence.\nNFKC and NFKD normalization cause data loss and should be\napplied only in special cases like search and indexing, and not for\npermanent storage of text.\nWhen preparing text for searching or indexing, another operation is useful: case fold‐\ning, our next subject.\nCase Folding\nCase folding is essentially converting all text to lowercase, with some additional\ntransformations. It is supported by the str.casefold() method.\nFor any string s containing only latin1 characters, s.casefold() produces the same\nresult as s.lower(), with only two exceptions—the micro sign 'µ' is changed to the\n142 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "Greek lowercase mu (which looks the same in most fonts) and the German Eszett or\n“sharp s” (ß) becomes “ss”:\n>>> micro = 'µ'\n>>> name(micro)\n'MICRO SIGN'\n>>> micro_cf = micro.casefold()\n>>> name(micro_cf)\n'GREEK SMALL LETTER MU'\n>>> micro, micro_cf\n('µ', 'μ')\n>>> eszett = 'ß'\n>>> name(eszett)\n'LATIN SMALL LETTER SHARP S'\n>>> eszett_cf = eszett.casefold()\n>>> eszett, eszett_cf\n('ß', 'ss')\nThere are nearly 300 code points for which str.casefold() and str.lower() return\ndifferent results.\nAs usual with anything related to Unicode, case folding is a hard issue with plenty of\nlinguistic special cases, but the Python core team made an effort to provide a solution\nthat hopefully works for most users.\nIn the next couple of sections, we’ll put our normalization knowledge to use develop‐\ning utility functions.\nUtility Functions for Normalized Text Matching\nAs we’ve seen, NFC and NFD are safe to use and allow sensible comparisons between\nUnicode strings. NFC is the best normalized form for most applications. str.case\nfold() is the way to go for case-insensitive comparisons.\nIf you work with text in many languages, a pair of functions like nfc_equal and\nfold_equal in Example 4-13 are useful additions to your toolbox.\nExample 4-13. normeq.py: normalized Unicode string comparison\n\"\"\"\nUtility functions for normalized Unicode string comparison.\nUsing Normal Form C, case sensitive:\n    >>> s1 = 'café'\n    >>> s2 = 'cafe\\u0301'\n    >>> s1 == s2\n    False\n    >>> nfc_equal(s1, s2)\n    True\nNormalizing Unicode for Reliable Comparisons \n| \n143",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": ">>> nfc_equal('A', 'a')\n    False\nUsing Normal Form C with case folding:\n    >>> s3 = 'Straße'\n    >>> s4 = 'strasse'\n    >>> s3 == s4\n    False\n    >>> nfc_equal(s3, s4)\n    False\n    >>> fold_equal(s3, s4)\n    True\n    >>> fold_equal(s1, s2)\n    True\n    >>> fold_equal('A', 'a')\n    True\n\"\"\"\nfrom unicodedata import normalize\ndef nfc_equal(str1, str2):\n    return normalize('NFC', str1) == normalize('NFC', str2)\ndef fold_equal(str1, str2):\n    return (normalize('NFC', str1).casefold() ==\n            normalize('NFC', str2).casefold())\nBeyond Unicode normalization and case folding—which are both part of the Uni‐\ncode standard—sometimes it makes sense to apply deeper transformations, like\nchanging 'café' into 'cafe'. We’ll see when and how in the next section.\nExtreme “Normalization”: Taking Out Diacritics\nThe Google Search secret sauce involves many tricks, but one of them apparently is\nignoring diacritics (e.g., accents, cedillas, etc.), at least in some contexts. Removing\ndiacritics is not a proper form of normalization because it often changes the meaning\nof words and may produce false positives when searching. But it helps coping with\nsome facts of life: people sometimes are lazy or ignorant about the correct use of dia‐\ncritics, and spelling rules change over time, meaning that accents come and go in liv‐\ning languages.\nOutside of searching, getting rid of diacritics also makes for more readable URLs, at\nleast in Latin-based languages. Take a look at the URL for the Wikipedia article about\nthe city of São Paulo:\nhttps://en.wikipedia.org/wiki/S%C3%A3o_Paulo\n144 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "The %C3%A3 part is the URL-escaped, UTF-8 rendering of the single letter “ã” (“a”\nwith tilde). The following is much easier to recognize, even if it is not the right\nspelling:\nhttps://en.wikipedia.org/wiki/Sao_Paulo\nTo remove all diacritics from a str, you can use a function like Example 4-14.\nExample 4-14. simplify.py: function to remove all combining marks\nimport unicodedata\nimport string\ndef shave_marks(txt):\n    \"\"\"Remove all diacritic marks\"\"\"\n    norm_txt = unicodedata.normalize('NFD', txt)  \n    shaved = ''.join(c for c in norm_txt\n                     if not unicodedata.combining(c))  \n    return unicodedata.normalize('NFC', shaved)  \nDecompose all characters into base characters and combining marks.\nFilter out all combining marks.\nRecompose all characters.\nExample 4-15 shows a couple of uses of shave_marks.\nExample 4-15. Two examples using shave_marks from Example 4-14\n>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n>>> shave_marks(order)\n'“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”'  \n>>> Greek = 'Ζέφυρος, Zéfiro'\n>>> shave_marks(Greek)\n'Ζεφυρος, Zefiro'  \nOnly the letters “è”, “ç”, and “í” were replaced.\nBoth “έ” and “é” were replaced.\nThe function shave_marks from Example 4-14 works all right, but maybe it goes too\nfar. Often the reason to remove diacritics is to change Latin text to pure ASCII, but\nshave_marks also changes non-Latin characters—like Greek letters—which will never\nbecome ASCII just by losing their accents. So it makes sense to analyze each base\nNormalizing Unicode for Reliable Comparisons \n| \n145",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "character and to remove attached marks only if the base character is a letter from the\nLatin alphabet. This is what Example 4-16 does.\nExample 4-16. Function to remove combining marks from Latin characters (import\nstatements are omitted as this is part of the simplify.py module from Example 4-14)\ndef shave_marks_latin(txt):\n    \"\"\"Remove all diacritic marks from Latin base characters\"\"\"\n    norm_txt = unicodedata.normalize('NFD', txt)  \n    latin_base = False\n    preserve = []\n    for c in norm_txt:\n        if unicodedata.combining(c) and latin_base:   \n            continue  # ignore diacritic on Latin base char\n        preserve.append(c)                            \n        # if it isn't a combining char, it's a new base char\n        if not unicodedata.combining(c):              \n            latin_base = c in string.ascii_letters\n    shaved = ''.join(preserve)\n    return unicodedata.normalize('NFC', shaved)   \nDecompose all characters into base characters and combining marks.\nSkip over combining marks when base character is Latin.\nOtherwise, keep current character.\nDetect new base character and determine if it’s Latin.\nRecompose all characters.\nAn even more radical step would be to replace common symbols in Western texts\n(e.g., curly quotes, em dashes, bullets, etc.) into ASCII equivalents. This is what the\nfunction asciize does in Example 4-17.\nExample 4-17. Transform some Western typographical symbols into ASCII (this\nsnippet is also part of simplify.py from Example 4-14)\nsingle_map = str.maketrans(\"\"\"‚ƒ„ˆ‹‘’“”•–—˜›\"\"\",  \n                           \"\"\"'f\"^<''\"\"---~>\"\"\")\nmulti_map = str.maketrans({  \n    '€': 'EUR',\n    '…': '...',\n    'Æ': 'AE',\n    'æ': 'ae',\n    'Œ': 'OE',\n    'œ': 'oe',\n146 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "'™': '(TM)',\n    '‰': '<per mille>',\n    '†': '**',\n    '‡': '***',\n})\nmulti_map.update(single_map)  \ndef dewinize(txt):\n    \"\"\"Replace Win1252 symbols with ASCII chars or sequences\"\"\"\n    return txt.translate(multi_map)  \ndef asciize(txt):\n    no_marks = shave_marks_latin(dewinize(txt))     \n    no_marks = no_marks.replace('ß', 'ss')          \n    return unicodedata.normalize('NFKC', no_marks)  \nBuild mapping table for char-to-char replacement.\nBuild mapping table for char-to-string replacement.\nMerge mapping tables.\ndewinize does not affect ASCII or latin1 text, only the Microsoft additions to\nlatin1 in cp1252.\nApply dewinize and remove diacritical marks.\nReplace the Eszett with “ss” (we are not using case fold here because we want to\npreserve the case).\nApply NFKC normalization to compose characters with their compatibility code\npoints.\nExample 4-18 shows asciize in use.\nExample 4-18. Two examples using asciize from Example 4-17\n>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'\n>>> dewinize(order)\n'\"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí.\"'  \n>>> asciize(order)\n'\"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai.\"'  \nNormalizing Unicode for Reliable Comparisons \n| \n147",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "8 Diacritics affect sorting only in the rare case when they are the only difference between two words—in that\ncase, the word with a diacritic is sorted after the plain word.\ndewinize replaces curly quotes, bullets, and ™ (trademark symbol).\nasciize applies dewinize, drops diacritics, and replaces the 'ß'.\nDifferent languages have their own rules for removing diacritics.\nFor example, Germans change the 'ü' into 'ue'. Our asciize\nfunction is not as refined, so it may or not be suitable for your lan‐\nguage. It works acceptably for Portuguese, though.\nTo summarize, the functions in simplify.py go way beyond standard normalization\nand perform deep surgery on the text, with a good chance of changing its meaning.\nOnly you can decide whether to go so far, knowing the target language, your users,\nand how the transformed text will be used.\nThis wraps up our discussion of normalizing Unicode text.\nNow let’s sort out Unicode sorting.\nSorting Unicode Text\nPython sorts sequences of any type by comparing the items in each sequence one by\none. For strings, this means comparing the code points. Unfortunately, this produces\nunacceptable results for anyone who uses non-ASCII characters.\nConsider sorting a list of fruits grown in Brazil:\n>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n>>> sorted(fruits)\n['acerola', 'atemoia', 'açaí', 'caju', 'cajá']\nSorting rules vary for different locales, but in Portuguese and many languages that\nuse the Latin alphabet, accents and cedillas rarely make a difference when sorting.8 So\n“cajá” is sorted as “caja,” and must come before “caju.”\nThe sorted fruits list should be:\n['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\nThe standard way to sort non-ASCII text in Python is to use the locale.strxfrm\nfunction which, according to the locale module docs, “transforms a string to one\nthat can be used in locale-aware comparisons.”\n148 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "9 Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of\nthe tech reviewers, had no problem using setlocale and locale.strxfrm on his Macintosh with macOS\n10.9. In summary: your mileage may vary.\nTo enable locale.strxfrm, you must first set a suitable locale for your application,\nand pray that the OS supports it. The sequence of commands in Example 4-19 may\nwork for you.\nExample 4-19. locale_sort.py: using the locale.strxfrm function as the sort key\nimport locale\nmy_locale = locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')\nprint(my_locale)\nfruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\nsorted_fruits = sorted(fruits, key=locale.strxfrm)\nprint(sorted_fruits)\nRunning Example 4-19 on GNU/Linux (Ubuntu 19.10) with the pt_BR.UTF-8 locale\ninstalled, I get the correct result:\n'pt_BR.UTF-8'\n['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\nSo you need to call setlocale(LC_COLLATE, «your_locale») before using\nlocale.strxfrm as the key when sorting.\nThere are some caveats, though:\n• Because locale settings are global, calling setlocale in a library is not recom‐\nmended. Your application or framework should set the locale when the process\nstarts, and should not change it afterward.\n• The locale must be installed on the OS, otherwise setlocale raises a\nlocale.Error: unsupported locale setting exception.\n• You must know how to spell the locale name.\n• The locale must be correctly implemented by the makers of the OS. I was suc‐\ncessful on Ubuntu 19.10, but not on macOS 10.14. On macOS, the call setlo\ncale(LC_COLLATE, 'pt_BR.UTF-8') returns the string 'pt_BR.UTF-8' with no\ncomplaints. But sorted(fruits, key=locale.strxfrm) produced the same\nincorrect result as sorted(fruits) did. I also tried the fr_FR, es_ES, and de_DE\nlocales on macOS, but locale.strxfrm never did its job.9\nSo the standard library solution to internationalized sorting works, but seems to be\nwell supported only on GNU/Linux (perhaps also on Windows, if you are an expert).\nEven then, it depends on locale settings, creating deployment headaches.\nSorting Unicode Text \n| \n149",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "Fortunately, there is a simpler solution: the pyuca library, available on PyPI.\nSorting with the Unicode Collation Algorithm\nJames Tauber, prolific Django contributor, must have felt the pain and created pyuca,\na pure-Python implementation of the Unicode Collation Algorithm (UCA).\nExample 4-20 shows how easy it is to use.\nExample 4-20. Using the pyuca.Collator.sort_key method\n>>> import pyuca\n>>> coll = pyuca.Collator()\n>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n>>> sorted_fruits = sorted(fruits, key=coll.sort_key)\n>>> sorted_fruits\n['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\nThis is simple and works on GNU/Linux, macOS, and Windows, at least with my\nsmall sample.\npyuca does not take the locale into account. If you need to customize the sorting, you\ncan provide the path to a custom collation table to the Collator() constructor. Out\nof the box, it uses allkeys.txt, which is bundled with the project. That’s just a copy of\nthe Default Unicode Collation Element Table from Unicode.org.\nPyICU: Miro’s Recommendation for Unicode Sorting\n(Tech reviewer Miroslav Šedivý is a polyglot and an expert on Uni‐\ncode. This is what he wrote about pyuca.)\npyuca has one sorting algorithm that does not respect the sorting\norder in individual languages. For instance, Ä in German is\nbetween A and B, while in Swedish it comes after Z. Have a look at\nPyICU that works like locale without changing the locale of the\nprocess. It is also needed if you want to change the case of iİ/ıI in\nTurkish. PyICU includes an extension that must be compiled, so it\nmay be harder to install in some systems than pyuca, which is just\nPython.\nBy the way, that collation table is one of the many data files that comprise the Uni‐\ncode database, our next subject.\nThe Unicode Database\nThe Unicode standard provides an entire database—in the form of several structured\ntext files—that includes not only the table mapping code points to character names,\n150 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "10 That’s an image—not a code listing—because emojis are not well supported by O’Reilly’s digital publishing\ntoolchain as I write this.\nbut also metadata about the individual characters and how they are related. For\nexample, the Unicode database records whether a character is printable, is a letter, is\na decimal digit, or is some other numeric symbol. That’s how the str methods isal\npha, isprintable, isdecimal, and isnumeric work. str.casefold also uses infor‐\nmation from a Unicode table.\nThe unicodedata.category(char) function returns the two-letter\ncategory of char from the Unicode database. The higher-level str\nmethods are easier to use. For example, label.isalpha() returns\nTrue if every character in label belongs to one of these categories:\nLm, Lt, Lu, Ll, or Lo. To learn what those codes mean, see “General\nCategory” in the English Wikipedia’s “Unicode character property”\narticle.\nFinding Characters by Name\nThe unicodedata module has functions to retrieve character metadata, including uni\ncodedata.name(), which returns a character’s official name in the standard.\nFigure 4-5 demonstrates that function.10\nFigure 4-5. Exploring unicodedata.name() in the Python console.\nYou can use the name() function to build apps that let users search for characters by\nname. Figure 4-6 demonstrates the cf.py command-line script that takes one or more\nwords as arguments, and lists the characters that have those words in their official\nUnicode names. The full source code for cf.py is in Example 4-21.\nThe Unicode Database \n| \n151",
      "content_length": 1537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Figure 4-6. Using cf.py to find smiling cats.\nEmoji support varies widely across operating systems and apps. In\nrecent years the macOS terminal offers the best support for emojis,\nfollowed by modern GNU/Linux graphic terminals. Windows\ncmd.exe and PowerShell now support Unicode output, but as I\nwrite this section in January 2020, they still don’t display emojis—\nat least not “out of the box.” Tech reviewer Leonardo Rochael told\nme about a new, open source Windows Terminal by Microsoft,\nwhich may have better Unicode support than the older Microsoft\nconsoles. I did not have time to try it.\nIn Example 4-21, note the if statement in the find function using the .issubset()\nmethod to quickly test whether all the words in the query set appear in the list of\nwords built from the character’s name. Thanks to Python’s rich set API, we don’t\nneed a nested for loop and another if to implement this check.\nExample 4-21. cf.py: the character finder utility\n#!/usr/bin/env python3\nimport sys\nimport unicodedata\nSTART, END = ord(' '), sys.maxunicode + 1           \ndef find(*query_words, start=START, end=END):       \n    query = {w.upper() for w in query_words}        \n    for code in range(start, end):\n        char = chr(code)                            \n        name = unicodedata.name(char, None)         \n        if name and query.issubset(name.split()):   \n            print(f'U+{code:04X}\\t{char}\\t{name}')  \ndef main(words):\n    if words:\n        find(*words)\n    else:\n        print('Please provide words to find.')\nif __name__ == '__main__':\n    main(sys.argv[1:])\n152 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "Set defaults for the range of code points to search.\nfind accepts query_words and optional keyword-only arguments to limit the\nrange of the search, to facilitate testing.\nConvert query_words into a set of uppercased strings.\nGet the Unicode character for code.\nGet the name of the character, or None if the code point is unassigned.\nIf there is a name, split it into a list of words, then check that the query set is a\nsubset of that list.\nPrint out line with code point in U+9999 format, the character, and its name.\nThe unicodedata module has other interesting functions. Next, we’ll see a few that\nare related to getting information from characters that have numeric meaning.\nNumeric Meaning of Characters\nThe unicodedata module includes functions to check whether a Unicode character\nrepresents a number and, if so, its numeric value for humans—as opposed to its code\npoint number. Example 4-22 shows the use of unicodedata.name() and unicode\ndata.numeric(), along with the .isdecimal() and .isnumeric() methods of str.\nExample 4-22. Demo of Unicode database numerical character metadata (callouts\ndescribe each column in the output)\nimport unicodedata\nimport re\nre_digit = re.compile(r'\\d')\nsample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\nfor char in sample:\n    print(f'U+{ord(char):04x}',                       \n          char.center(6),                             \n          're_dig' if re_digit.match(char) else '-',  \n          'isdig' if char.isdigit() else '-',         \n          'isnum' if char.isnumeric() else '-',       \n          f'{unicodedata.numeric(char):5.2f}',        \n          unicodedata.name(char),                     \n          sep='\\t')\nThe Unicode Database \n| \n153",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "11 Although it was not better than re at identifying digits in this particular sample.\nCode point in U+0000 format.\nCharacter centralized in a str of length 6.\nShow re_dig if character matches the r'\\d' regex.\nShow isdig if char.isdigit() is True.\nShow isnum if char.isnumeric() is True.\nNumeric value formatted with width 5 and 2 decimal places.\nUnicode character name.\nRunning Example 4-22 gives you Figure 4-7, if your terminal font has all those\nglyphs.\nFigure 4-7. macOS terminal showing numeric characters and metadata about them;\nre_dig means the character matches the regular expression r'\\d'.\nThe sixth column of Figure 4-7 is the result of calling unicodedata.numeric(char)\non the character. It shows that Unicode knows the numeric value of symbols that\nrepresent numbers. So if you want to create a spreadsheet application that supports\nTamil digits or Roman numerals, go for it!\nFigure 4-7 shows that the regular expression r'\\d' matches the digit “1” and the\nDevanagari digit 3, but not some other characters that are considered digits by the\nisdigit function. The re module is not as savvy about Unicode as it could be. The\nnew regex module available on PyPI was designed to eventually replace re and pro‐\nvides better Unicode support.11 We’ll come back to the re module in the next section.\n154 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "Throughout this chapter we’ve used several unicodedata functions, but there are\nmany more we did not cover. See the standard library documentation for the unicode\ndata module.\nNext we’ll take a quick look at dual-mode APIs offering functions that accept str or\nbytes arguments with special handling depending on the type.\nDual-Mode str and bytes APIs\nPython’s standard library has functions that accept str or bytes arguments and\nbehave differently depending on the type. Some examples can be found in the re and\nos modules.\nstr Versus bytes in Regular Expressions\nIf you build a regular expression with bytes, patterns such as \\d and \\w only match\nASCII characters; in contrast, if these patterns are given as str, they match Unicode\ndigits or letters beyond ASCII. Example 4-23 and Figure 4-8 compare how letters,\nASCII digits, superscripts, and Tamil digits are matched by str and bytes patterns.\nExample 4-23. ramanujan.py: compare behavior of simple str and bytes regular\nexpressions\nimport re\nre_numbers_str = re.compile(r'\\d+')     \nre_words_str = re.compile(r'\\w+')\nre_numbers_bytes = re.compile(rb'\\d+')  \nre_words_bytes = re.compile(rb'\\w+')\ntext_str = (\"Ramanujan saw \\u0be7\\u0bed\\u0be8\\u0bef\"  \n            \" as 1729 = 1³ + 12³ = 9³ + 10³.\")        \ntext_bytes = text_str.encode('utf_8')  \nprint(f'Text\\n  {text_str!r}')\nprint('Numbers')\nprint('  str  :', re_numbers_str.findall(text_str))      \nprint('  bytes:', re_numbers_bytes.findall(text_bytes))  \nprint('Words')\nprint('  str  :', re_words_str.findall(text_str))        \nprint('  bytes:', re_words_bytes.findall(text_bytes))    \nThe first two regular expressions are of the str type.\nThe last two are of the bytes type.\nDual-Mode str and bytes APIs \n| \n155",
      "content_length": 1724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "Unicode text to search, containing the Tamil digits for 1729 (the logical line con‐\ntinues until the right parenthesis token).\nThis string is joined to the previous one at compile time (see “2.4.2. String literal\nconcatenation” in The Python Language Reference).\nA bytes string is needed to search with the bytes regular expressions.\nThe str pattern r'\\d+' matches the Tamil and ASCII digits.\nThe bytes pattern rb'\\d+' matches only the ASCII bytes for digits.\nThe str pattern r'\\w+' matches the letters, superscripts, Tamil, and ASCII\ndigits.\nThe bytes pattern rb'\\w+' matches only the ASCII bytes for letters and digits.\nFigure 4-8. Screenshot of running ramanujan.py from Example 4-23.\nExample 4-23 is a trivial example to make one point: you can use regular expressions\non str and bytes, but in the second case, bytes outside the ASCII range are treated as\nnondigits and nonword characters.\nFor str regular expressions, there is a re.ASCII flag that makes \\w, \\W, \\b, \\B, \\d, \\D,\n\\s, and \\S perform ASCII-only matching. See the documentation of the re module\nfor full details.\nAnother important dual-mode module is os.\nstr Versus bytes in os Functions\nThe GNU/Linux kernel is not Unicode savvy, so in the real world you may find file‐\nnames made of byte sequences that are not valid in any sensible encoding scheme,\nand cannot be decoded to str. File servers with clients using a variety of OSes are\nparticularly prone to this problem.\n156 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "In order to work around this issue, all os module functions that accept filenames or\npathnames take arguments as str or bytes. If one such function is called with a str\nargument, the argument will be automatically converted using the codec named by\nsys.getfilesystemencoding(), and the OS response will be decoded with the same\ncodec. This is almost always what you want, in keeping with the Unicode sandwich\nbest practice.\nBut if you must deal with (and perhaps fix) filenames that cannot be handled in that\nway, you can pass bytes arguments to the os functions to get bytes return values.\nThis feature lets you deal with any file or pathname, no matter how many gremlins\nyou may find. See Example 4-24.\nExample 4-24. listdir with str and bytes arguments and results\n>>> os.listdir('.')  \n['abc.txt', 'digits-of-π.txt']\n>>> os.listdir(b'.')  \n[b'abc.txt', b'digits-of-\\xcf\\x80.txt']\nThe second filename is “digits-of-π.txt” (with the Greek letter pi).\nGiven a byte argument, listdir returns filenames as bytes: b'\\xcf\\x80' is the\nUTF-8 encoding of the Greek letter pi.\nTo help with manual handling of str or bytes sequences that are filenames or path‐\nnames, the os module provides special encoding and decoding functions os.fsen\ncode(name_or_path) and os.fsdecode(name_or_path). Both of these functions\naccept an argument of type str, bytes, or an object implementing the os.PathLike\ninterface since Python 3.6.\nUnicode is a deep rabbit hole. Time to wrap up our exploration of str and bytes.\nChapter Summary\nWe started the chapter by dismissing the notion that 1 character == 1 byte. As the\nworld adopts Unicode, we need to keep the concept of text strings separated from the\nbinary sequences that represent them in files, and Python 3 enforces this separation.\nAfter a brief overview of the binary sequence data types—bytes, bytearray, and\nmemoryview—we jumped into encoding and decoding, with a sampling of important\ncodecs, followed by approaches to prevent or deal with the infamous UnicodeEnco\ndeError, UnicodeDecodeError, and the SyntaxError caused by wrong encoding in\nPython source files.\nChapter Summary \n| \n157",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "We then considered the theory and practice of encoding detection in the absence of\nmetadata: in theory, it can’t be done, but in practice the Chardet package pulls it off\npretty well for a number of popular encodings. Byte order marks were then presented\nas the only encoding hint commonly found in UTF-16 and UTF-32 files—sometimes\nin UTF-8 files as well.\nIn the next section, we demonstrated opening text files, an easy task except for one\npitfall: the encoding= keyword argument is not mandatory when you open a text file,\nbut it should be. If you fail to specify the encoding, you end up with a program that\nmanages to generate “plain text” that is incompatible across platforms, due to con‐\nflicting default encodings. We then exposed the different encoding settings that\nPython uses as defaults and how to detect them. A sad realization for Windows users\nis that these settings often have distinct values within the same machine, and the val‐\nues are mutually incompatible; GNU/Linux and macOS users, in contrast, live in a\nhappier place where UTF-8 is the default pretty much everywhere.\nUnicode provides multiple ways of representing some characters, so normalizing is a\nprerequisite for text matching. In addition to explaining normalization and case fold‐\ning, we presented some utility functions that you may adapt to your needs, including\ndrastic transformations like removing all accents. We then saw how to sort Unicode\ntext correctly by leveraging the standard locale module—with some caveats—and an\nalternative that does not depend on tricky locale configurations: the external pyuca\npackage.\nWe leveraged the Unicode database to program a command-line utility to search for\ncharacters by name—in 28 lines of code, thanks to the power of Python. We glanced\nat other Unicode metadata, and had a brief overview of dual-mode APIs where some\nfunctions can be called with str or bytes arguments, producing different results.\nFurther Reading\nNed Batchelder’s 2012 PyCon US talk “Pragmatic Unicode, or, How Do I Stop the\nPain?” was outstanding. Ned is so professional that he provides a full transcript of the\ntalk along with the slides and video.\n“Character encoding and Unicode in Python: How to (╯°□°)╯︵ ┻━┻ with dig‐\nnity” (slides, video) was the excellent PyCon 2014 talk by Esther Nam and Travis\nFischer, where I found this chapter’s pithy epigraph: “Humans use text. Computers\nspeak bytes.”\nLennart Regebro—one of the technical reviewers for the first edition of this book—\nshares his “Useful Mental Model of Unicode (UMMU)” in the short post “Unconfus‐\ning Unicode: What Is Unicode?”. Unicode is a complex standard, so Lennart’s\nUMMU is a really useful starting point.\n158 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 2730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "The official “Unicode HOWTO” in the Python docs approaches the subject from sev‐\neral different angles, from a good historic intro, to syntax details, codecs, regular\nexpressions, filenames, and best practices for Unicode-aware I/O (i.e., the Unicode\nsandwich), with plenty of additional reference links from each section. Chapter 4,\n“Strings”, of Mark Pilgrim’s awesome book Dive into Python 3 (Apress) also provides\na very good intro to Unicode support in Python 3. In the same book, Chapter 15\ndescribes how the Chardet library was ported from Python 2 to Python 3, a valuable\ncase study given that the switch from the old str to the new bytes is the cause of\nmost migration pains, and that is a central concern in a library designed to detect\nencodings.\nIf you know Python 2 but are new to Python 3, Guido van Rossum’s “What’s New in\nPython 3.0” has 15 bullet points that summarize what changed, with lots of links.\nGuido starts with the blunt statement: “Everything you thought you knew about\nbinary data and Unicode has changed.” Armin Ronacher’s blog post “The Updated\nGuide to Unicode on Python” is deep and highlights some of the pitfalls of Unicode\nin Python 3 (Armin is not a big fan of Python 3).\nChapter 2, “Strings and Text,” of the Python Cookbook, 3rd ed. (O’Reilly), by David\nBeazley and Brian K. Jones, has several recipes dealing with Unicode normalization,\nsanitizing text, and performing text-oriented operations on byte sequences. Chapter 5\ncovers files and I/O, and it includes “Recipe 5.17. Writing Bytes to a Text File,” show‐\ning that underlying any text file there is always a binary stream that may be accessed\ndirectly when needed. Later in the cookbook, the struct module is put to use in\n“Recipe 6.11. Reading and Writing Binary Arrays of Structures.”\nNick Coghlan’s “Python Notes” blog has two posts very relevant to this chapter:\n“Python 3 and ASCII Compatible Binary Protocols” and “Processing Text Files in\nPython 3”. Highly recommended.\nA list of encodings supported by Python is available at “Standard Encodings” in the\ncodecs module documentation. If you need to get that list programmatically, see how\nit’s done in the /Tools/unicode/listcodecs.py script that comes with the CPython\nsource code.\nThe books Unicode Explained by Jukka K. Korpela (O’Reilly) and Unicode Demysti‐\nfied by Richard Gillam (Addison-Wesley) are not Python-specific but were very help‐\nful as I studied Unicode concepts. Programming with Unicode by Victor Stinner is a\nfree, self-published book (Creative Commons BY-SA) covering Unicode in general,\nas well as tools and APIs in the context of the main operating systems and a few pro‐\ngramming languages, including Python.\nThe W3C pages “Case Folding: An Introduction” and “Character Model for the\nWorld Wide Web: String Matching” cover normalization concepts, with the former\nbeing a gentle introduction and the latter a working group note written in dry\nFurther Reading \n| \n159",
      "content_length": 2942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "standard-speak—the same tone of the “Unicode Standard Annex #15—Unicode Nor‐\nmalization Forms”. The “Frequently Asked Questions, Normalization” section from\nUnicode.org is more readable, as is the “NFC FAQ” by Mark Davis—author of several\nUnicode algorithms and president of the Unicode Consortium at the time of this\nwriting.\nIn 2016, the Museum of Modern Art (MoMA) in New York added to its collection\nthe original emoji, the 176 emojis designed by Shigetaka Kurita in 1999 for NTT\nDOCOMO—the Japanese mobile carrier. Going further back in history, Emojipedia\npublished “Correcting the Record on the First Emoji Set”, crediting Japan’s SoftBank\nfor the earliest known emoji set, deployed in cell phones in 1997. SoftBank’s set is the\nsource of 90 emojis now in Unicode, including U+1F4A9 (PILE OF POO). Matthew\nRothenberg’s emojitracker.com is a live dashboard showing counts of emoji usage on\nTwitter, updated in real time. As I write this, FACE WITH TEARS OF JOY (U+1F602)\nis the most popular emoji on Twitter, with more than 3,313,667,315 recorded\noccurrences.\nSoapbox\nNon-ASCII Names in Source Code: Should You Use Them?\nPython 3 allows non-ASCII identifiers in source code:\n>>> ação = 'PBR'  # ação = stock\n>>> ε = 10**-6    # ε = epsilon\nSome people dislike the idea. The most common argument to stick with ASCII iden‐\ntifiers is to make it easy for everyone to read and edit code. That argument misses the\npoint: you want your source code to be readable and editable by its intended audi‐\nence, and that may not be “everyone.” If the code belongs to a multinational corpora‐\ntion or is open source and you want contributors from around the world, the\nidentifiers should be in English, and then all you need is ASCII.\nBut if you are a teacher in Brazil, your students will find it easier to read code that\nuses Portuguese variable and function names, correctly spelled. And they will have no\ndifficulty typing the cedillas and accented vowels on their localized keyboards.\nNow that Python can parse Unicode names and UTF-8 is the default source encod‐\ning, I see no point in coding identifiers in Portuguese without accents, as we used to\ndo in Python 2 out of necessity—unless you need the code to run on Python 2 also. If\nthe names are in Portuguese, leaving out the accents won’t make the code more read‐\nable to anyone.\nThis is my point of view as a Portuguese-speaking Brazilian, but I believe it applies\nacross borders and cultures: choose the human language that makes the code easier to\nread by the team, then use the characters needed for correct spelling.\n160 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 2618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "What Is “Plain Text”?\nFor anyone who deals with non-English text on a daily basis, “plain text” does not\nimply “ASCII.” The Unicode Glossary defines plain text like this:\nComputer-encoded text that consists only of a sequence of code points from a given\nstandard, with no other formatting or structural information.\nThat definition starts very well, but I don’t agree with the part after the comma.\nHTML is a great example of a plain-text format that carries formatting and structural\ninformation. But it’s still plain text because every byte in such a file is there to repre‐\nsent a text character, usually using UTF-8. There are no bytes with nontext meaning,\nas you can find in a .png or .xls document where most bytes represent packed binary\nvalues like RGB values and floating-point numbers. In plain text, numbers are repre‐\nsented as sequences of digit characters.\nI am writing this book in a plain-text format called—ironically—AsciiDoc, which is\npart of the toolchain of O’Reilly’s excellent Atlas book publishing platform. AsciiDoc\nsource files are plain text, but they are UTF-8, not ASCII. Otherwise, writing this\nchapter would have been really painful. Despite the name, AsciiDoc is just great.\nThe world of Unicode is constantly expanding and, at the edges, tool support is not\nalways there. Not all characters I wanted to show were available in the fonts used to\nrender the book. That’s why I had to use images instead of listings in several examples\nin this chapter. On the other hand, the Ubuntu and macOS terminals display most\nUnicode text very well—including the Japanese characters for the word “mojibake”:\n文字化け.\nHow Are str Code Points Represented in RAM?\nThe official Python docs avoid the issue of how the code points of a str are stored in\nmemory. It is really an implementation detail. In theory, it doesn’t matter: whatever\nthe internal representation, every str must be encoded to bytes on output.\nIn memory, Python 3 stores each str as a sequence of code points using a fixed num‐\nber of bytes per code point, to allow efficient direct access to any character or slice.\nSince Python 3.3, when creating a new str object, the interpreter checks the charac‐\nters in it and chooses the most economic memory layout that is suitable for that par‐\nticular str: if there are only characters in the latin1 range, that str will use just one\nbyte per code point. Otherwise, two or four bytes per code point may be used,\ndepending on the str. This is a simplification; for the full details, look up PEP 393—\nFlexible String Representation.\nThe flexible string representation is similar to the way the int type works in Python\n3: if the integer fits in a machine word, it is stored in one machine word. Otherwise,\nthe interpreter switches to a variable-length representation like that of the Python 2\nlong type. It is nice to see the spread of good ideas.\nFurther Reading \n| \n161",
      "content_length": 2895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "However, we can always count on Armin Ronacher to find problems in Python 3. He\nexplained to me why that was not such as great idea in practice: it takes a single RAT\n(U+1F400) to inflate an otherwise all-ASCII text into a memory-hogging array using\nfour bytes per character, when one byte would suffice for each character except the\nRAT. In addition, because of all the ways Unicode characters combine, the ability to\nquickly retrieve an arbitrary character by position is overrated—and extracting arbi‐\ntrary slices from Unicode text is naïve at best, and often wrong, producing mojibake.\nAs emojis become more popular, these problems will only get worse.\n162 \n| \nChapter 4: Unicode Text Versus Bytes",
      "content_length": 702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "1 From Refactoring, first edition, Chapter 3, “Bad Smells in Code, Data Class” section, page 87 (Addison-\nWesley).\nCHAPTER 5\nData Class Builders\nData classes are like children. They are okay as a starting point, but to participate as a\ngrownup object, they need to take some responsibility.\n—Martin Fowler and Kent Beck1\nPython offers a few ways to build a simple class that is just a collection of fields, with\nlittle or no extra functionality. That pattern is known as a “data class”—and data\nclasses is one of the packages that supports this pattern. This chapter covers three\ndifferent class builders that you may use as shortcuts to write data classes:\ncollections.namedtuple\nThe simplest way—available since Python 2.6.\ntyping.NamedTuple\nAn alternative that requires type hints on the fields—since Python 3.5, with\nclass syntax added in 3.6.\n@dataclasses.dataclass\nA class decorator that allows more customization than previous alternatives,\nadding lots of options and potential complexity—since Python 3.7.\nAfter covering those class builders, we will discuss why Data Class is also the name of\na code smell: a coding pattern that may be a symptom of poor object-oriented design.\n163",
      "content_length": 1190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "typing.TypedDict may seem like another data class builder. It uses\nsimilar syntax and is described right after typing.NamedTuple\nin the typing module documentation for Python 3.9.\nHowever, TypedDict does not build concrete classes that you can\ninstantiate. It’s just syntax to write type hints for function parame‐\nters and variables that will accept mapping values used as records,\nwith keys as field names. We’ll see them in Chapter 15,\n“TypedDict” on page 526.\nWhat’s New in This Chapter\nThis chapter is new in the second edition of Fluent Python. The section “Classic\nNamed Tuples” on page 169 appeared in Chapter 2 of the first edition, but the rest of\nthe chapter is completely new.\nWe begin with a high-level overview of the three class builders.\nOverview of Data Class Builders\nConsider a simple class to represent a geographic coordinate pair, as shown in\nExample 5-1.\nExample 5-1. class/coordinates.py\nclass Coordinate:\n    def __init__(self, lat, lon):\n        self.lat = lat\n        self.lon = lon\nThat Coordinate class does the job of holding latitude and longitude attributes. Writ‐\ning the __init__ boilerplate becomes old real fast, especially if your class has more\nthan a couple of attributes: each of them is mentioned three times! And that boiler‐\nplate doesn’t buy us basic features we’d expect from a Python object:\n>>> from coordinates import Coordinate\n>>> moscow = Coordinate(55.76, 37.62)\n>>> moscow\n<coordinates.Coordinate object at 0x107142f10>  \n>>> location = Coordinate(55.76, 37.62)\n>>> location == moscow  \nFalse\n>>> (location.lat, location.lon) == (moscow.lat, moscow.lon)  \nTrue\n164 \n| \nChapter 5: Data Class Builders",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "__repr__ inherited from object is not very helpful.\nMeaningless ==; the __eq__ method inherited from object compares object IDs.\nComparing two coordinates requires explicit comparison of each attribute.\nThe data class builders covered in this chapter provide the necessary __init__,\n__repr__, and __eq__ methods automatically, as well as other useful features.\nNone of the class builders discussed here depend on inheritance to\ndo their work. Both collections.namedtuple and typing.Name\ndTuple build classes that are tuple subclasses. @dataclass is a class\ndecorator that does not affect the class hierarchy in any way. Each\nof them uses different metaprogramming techniques to inject\nmethods and data attributes into the class under construction.\nHere is a Coordinate class built with namedtuple—a factory function that builds a\nsubclass of tuple with the name and fields you specify:\n>>> from collections import namedtuple\n>>> Coordinate = namedtuple('Coordinate', 'lat lon')\n>>> issubclass(Coordinate, tuple)\nTrue\n>>> moscow = Coordinate(55.756, 37.617)\n>>> moscow\nCoordinate(lat=55.756, lon=37.617)  \n>>> moscow == Coordinate(lat=55.756, lon=37.617)  \nTrue\nUseful __repr__.\nMeaningful __eq__.\nThe newer typing.NamedTuple provides the same functionality, adding a type anno‐\ntation to each field:\n>>> import typing\n>>> Coordinate = typing.NamedTuple('Coordinate',\n...     [('lat', float), ('lon', float)])\n>>> issubclass(Coordinate, tuple)\nTrue\n>>> typing.get_type_hints(Coordinate)\n{'lat': <class 'float'>, 'lon': <class 'float'>}\nOverview of Data Class Builders \n| \n165",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "2 Metaclasses are one of the subjects covered in Chapter 24, “Class Metaprogramming”.\nA typed named tuple can also be constructed with the fields given\nas keyword arguments, like this:\nCoordinate = typing.NamedTuple('Coordinate', lat=float, lon=float)\nThis is more readable, and also lets you provide the mapping of\nfields and types as **fields_and_types.\nSince Python 3.6, typing.NamedTuple can also be used in a class statement, with\ntype annotations written as described in PEP 526—Syntax for Variable Annotations.\nThis is much more readable, and makes it easy to override methods or add new ones.\nExample 5-2 is the same Coordinate class, with a pair of float attributes and a cus‐\ntom __str__ to display a coordinate formatted like 55.8°N, 37.6°E.\nExample 5-2. typing_namedtuple/coordinates.py\nfrom typing import NamedTuple\nclass Coordinate(NamedTuple):\n    lat: float\n    lon: float\n    def __str__(self):\n        ns = 'N' if self.lat >= 0 else 'S'\n        we = 'E' if self.lon >= 0 else 'W'\n        return f'{abs(self.lat):.1f}°{ns}, {abs(self.lon):.1f}°{we}'\nAlthough NamedTuple appears in the class statement as a super‐\nclass, it’s actually not. typing.NamedTuple uses the advanced func‐\ntionality of a metaclass2 to customize the creation of the user’s\nclass. Check this out:\n>>> issubclass(Coordinate, typing.NamedTuple)\nFalse\n>>> issubclass(Coordinate, tuple)\nTrue\nIn the __init__ method generated by typing.NamedTuple, the fields appear as\nparameters in the same order they appear in the class statement.\nLike typing.NamedTuple, the dataclass decorator supports PEP 526 syntax to\ndeclare instance attributes. The decorator reads the variable annotations and auto‐\nmatically generates methods for your class. For comparison, check out the equivalent\n166 \n| \nChapter 5: Data Class Builders",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "3 Class decorators are covered in Chapter 24, “Class Metaprogramming,” along with metaclasses. Both are ways\nof customizing class behavior beyond what is possible with inheritance.\nCoordinate class written with the help of the dataclass decorator, as shown in\nExample 5-3.\nExample 5-3. dataclass/coordinates.py\nfrom dataclasses import dataclass\n@dataclass(frozen=True)\nclass Coordinate:\n    lat: float\n    lon: float\n    def __str__(self):\n        ns = 'N' if self.lat >= 0 else 'S'\n        we = 'E' if self.lon >= 0 else 'W'\n        return f'{abs(self.lat):.1f}°{ns}, {abs(self.lon):.1f}°{we}'\nNote that the body of the classes in Example 5-2 and Example 5-3 are identical—the\ndifference is in the class statement itself. The @dataclass decorator does not depend\non inheritance or a metaclass, so it should not interfere with your own use of these\nmechanisms.3 The Coordinate class in Example 5-3 is a subclass of object.\nMain Features\nThe different data class builders have a lot in common, as summarized in Table 5-1.\nTable 5-1. Selected features compared across the three data class builders; x stands for an\ninstance of a data class of that kind\nnamedtuple\nNamedTuple\ndataclass\nmutable instances\nNO\nNO\nYES\nclass statement syntax\nNO\nYES\nYES\nconstruct dict\nx._asdict()\nx._asdict()\ndataclasses.asdict(x)\nget field names\nx._fields\nx._fields\n[f.name for f in dataclasses.fields(x)]\nget defaults\nx._field_defaults\nx._field_defaults\n[f.default for f in dataclasses.fields(x)]\nget field types\nN/A\nx.__annotations__\nx.__annotations__\nnew instance with\nchanges\nx._replace(…)\nx._replace(…)\ndataclasses.replace(x, …)\nnew class at runtime\nnamedtuple(…)\nNamedTuple(…)\ndataclasses.make_dataclass(…)\nOverview of Data Class Builders \n| \n167",
      "content_length": 1728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "The classes built by typing.NamedTuple and @dataclass have an\n__annotations__ attribute holding the type hints for the fields.\nHowever, reading from __annotations__ directly is not recom‐\nmended. Instead, the recommended best practice to get that\ninformation is to call inspect.get_annotations(MyClass) (added\nin Python 3.10) or typing.get_type_hints(MyClass) (Python 3.5\nto 3.9). That’s because those functions provide extra services, like\nresolving forward references in type hints. We’ll come back to this\nissue much later in the book, in “Problems with Annotations at\nRuntime” on page 538.\nNow let’s discuss those main features.\nMutable instances\nA key difference between these class builders is that collections.namedtuple and\ntyping.NamedTuple build tuple subclasses, therefore the instances are immutable.\nBy default, @dataclass produces mutable classes. But the decorator accepts a key‐\nword argument frozen—shown in Example 5-3. When frozen=True, the class will\nraise an exception if you try to assign a value to a field after the instance is initialized.\nClass statement syntax\nOnly typing.NamedTuple and dataclass support the regular class statement syn‐\ntax, making it easier to add methods and docstrings to the class you are creating.\nConstruct dict\nBoth named tuple variants provide an instance method (._asdict) to construct a\ndict object from the fields in a data class instance. The dataclasses module pro‐\nvides a function to do it: dataclasses.asdict.\nGet field names and default values\nAll three class builders let you get the field names and default values that may be con‐\nfigured for them. In named tuple classes, that metadata is in the ._fields\nand ._fields_defaults class attributes. You can get the same metadata from a data\nclass decorated class using the fields function from the dataclasses module. It\nreturns a tuple of Field objects that have several attributes, including name and\ndefault.\n168 \n| \nChapter 5: Data Class Builders",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "Get field types\nClasses defined with the help of typing.NamedTuple and @dataclass have a mapping\nof field names to type the __annotations__ class attribute. As mentioned, use the\ntyping.get_type_hints function instead of reading __annotations__ directly.\nNew instance with changes\nGiven a named tuple instance x, the call x._replace(**kwargs) returns a new\ninstance with some attribute values replaced according to the keyword arguments\ngiven. The dataclasses.replace(x, **kwargs) module-level function does the\nsame for an instance of a dataclass decorated class.\nNew class at runtime\nAlthough the class statement syntax is more readable, it is hardcoded. A framework\nmay need to build data classes on the fly, at runtime. For that, you can use the default\nfunction call syntax of collections.namedtuple, which is likewise supported by\ntyping.NamedTuple. The dataclasses module provides a make_dataclass function\nfor the same purpose.\nAfter this overview of the main features of the data class builders, let’s focus on each\nof them in turn, starting with the simplest.\nClassic Named Tuples\nThe collections.namedtuple function is a factory that builds subclasses of tuple\nenhanced with field names, a class name, and an informative __repr__. Classes built\nwith namedtuple can be used anywhere where tuples are needed, and in fact many\nfunctions of the Python standard library that are used to return tuples now return\nnamed tuples for convenience, without affecting the user’s code at all.\nEach instance of a class built by namedtuple takes exactly the same\namount of memory as a tuple because the field names are stored in\nthe class.\nExample 5-4 shows how we could define a named tuple to hold information about a\ncity.\nExample 5-4. Defining and using a named tuple type\n>>> from collections import namedtuple\n>>> City = namedtuple('City', 'name country population coordinates')  \nClassic Named Tuples \n| \n169",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": ">>> tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667))  \n>>> tokyo\nCity(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722,\n139.691667))\n>>> tokyo.population  \n36.933\n>>> tokyo.coordinates\n(35.689722, 139.691667)\n>>> tokyo[1]\n'JP'\nTwo parameters are required to create a named tuple: a class name and a list of\nfield names, which can be given as an iterable of strings or as a single space-\ndelimited string.\nField values must be passed as separate positional arguments to the constructor\n(in contrast, the tuple constructor takes a single iterable).\nYou can access the fields by name or position.\nAs a tuple subclass, City inherits useful methods such as __eq__ and the special\nmethods for comparison operators—including __lt__, which allows sorting lists of\nCity instances.\nA named tuple offers a few attributes and methods in addition to those inherited\nfrom the tuple. Example 5-5 shows the most useful: the _fields class attribute, the\nclass method _make(iterable), and the _asdict() instance method.\nExample 5-5. Named tuple attributes and methods (continued from the previous\nexample)\n>>> City._fields  \n('name', 'country', 'population', 'location')\n>>> Coordinate = namedtuple('Coordinate', 'lat lon')\n>>> delhi_data = ('Delhi NCR', 'IN', 21.935, Coordinate(28.613889, 77.208889))\n>>> delhi = City._make(delhi_data)  \n>>> delhi._asdict()  \n{'name': 'Delhi NCR', 'country': 'IN', 'population': 21.935,\n'location': Coordinate(lat=28.613889, lon=77.208889)}\n>>> import json\n>>> json.dumps(delhi._asdict())  \n'{\"name\": \"Delhi NCR\", \"country\": \"IN\", \"population\": 21.935,\n\"location\": [28.613889, 77.208889]}'\n._fields is a tuple with the field names of the class.\n._make() builds City from an iterable; City(*delhi_data) would do the same.\n170 \n| \nChapter 5: Data Class Builders",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "._asdict() returns a dict built from the named tuple instance.\n._asdict() is useful to serialize the data in JSON format, for example.\nThe _asdict method returned an OrderedDict until Python 3.7.\nSince Python 3.8, it returns a simple dict—which is OK now that\nwe can rely on key insertion order. If you must have an Ordered\nDict, the _asdict documentation recommends building one from\nthe result: OrderedDict(x._asdict()).\nSince Python 3.7, namedtuple accepts the defaults keyword-only argument provid‐\ning an iterable of N default values for each of the N rightmost fields of the class.\nExample 5-6 shows how to define a Coordinate named tuple with a default value for\na reference field.\nExample 5-6. Named tuple attributes and methods, continued from Example 5-5\n>>> Coordinate = namedtuple('Coordinate', 'lat lon reference', defaults=['WGS84'])\n>>> Coordinate(0, 0)\nCoordinate(lat=0, lon=0, reference='WGS84')\n>>> Coordinate._field_defaults\n{'reference': 'WGS84'}\nIn “Class statement syntax” on page 168, I mentioned it’s easier to code methods with\nthe class syntax supported by typing.NamedTuple and @dataclass. You can also add\nmethods to a namedtuple, but it’s a hack. Skip the following box if you’re not interes‐\nted in hacks.\nHacking a namedtuple to Inject a Method\nRecall how we built the Card class in Example 1-1 in Chapter 1:\nCard = collections.namedtuple('Card', ['rank', 'suit'])\nLater in Chapter 1, I wrote a spades_high function for sorting. It would be nice if\nthat logic was encapsulated in a method of Card, but adding spades_high to Card\nwithout the benefit of a class statement requires a quick hack: define the function\nand then assign it to a class attribute. Example 5-7 shows how.\nExample 5-7. frenchdeck.doctest: Adding a class attribute and a method to Card,\nthe namedtuple from “A Pythonic Card Deck” on page 5\n>>> Card.suit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0)  \n>>> def spades_high(card):                                            \n...     rank_value = FrenchDeck.ranks.index(card.rank)\nClassic Named Tuples \n| \n171",
      "content_length": 2066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "4 If you know Ruby, you know that injecting methods is a well-known but controversial technique among\nRubyists. In Python, it’s not as common, because it doesn’t work with any built-in type—str, list, etc. I con‐\nsider this limitation of Python a blessing.\n...     suit_value = card.suit_values[card.suit]\n...     return rank_value * len(card.suit_values) + suit_value\n...\n>>> Card.overall_rank = spades_high                                   \n>>> lowest_card = Card('2', 'clubs')\n>>> highest_card = Card('A', 'spades')\n>>> lowest_card.overall_rank()                                        \n0\n>>> highest_card.overall_rank()\n51\nAttach a class attribute with values for each suit.\nspades_high will become a method; the first argument doesn’t need to be named\nself. Anyway, it will get the receiver when called as a method.\nAttach the function to the Card class as a method named overall_rank.\nIt works!\nFor readability and future maintenance, it’s much better to code methods inside a\nclass statement. But it’s good to know this hack is possible, because it may come in\nhandy.4\nThis was a small detour to showcase the power of a dynamic language.\nNow let’s check out the typing.NamedTuple variation.\nTyped Named Tuples\nThe Coordinate class with a default field from Example 5-6 can be written using\ntyping.NamedTuple, as shown in Example 5-8.\nExample 5-8. typing_namedtuple/coordinates2.py\nfrom typing import NamedTuple\nclass Coordinate(NamedTuple):\n    lat: float                \n    lon: float\n    reference: str = 'WGS84'  \n172 \n| \nChapter 5: Data Class Builders",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Every instance field must be annotated with a type.\nThe reference instance field is annotated with a type and a default value.\nClasses built by typing.NamedTuple don’t have any methods beyond those that col\nlections.namedtuple also generates—and those that are inherited from tuple. The\nonly difference is the presence of the __annotations__ class attribute—which Python\ncompletely ignores at runtime.\nGiven that the main feature of typing.NamedTuple are the type annotations, we’ll\ntake a brief look at them before resuming our exploration of data class builders.\nType Hints 101\nType hints—a.k.a. type annotations—are ways to declare the expected type of func‐\ntion arguments, return values, variables, and attributes.\nThe first thing you need to know about type hints is that they are not enforced at all\nby the Python bytecode compiler and interpreter.\nThis is a very brief introduction to type hints, just enough to make\nsense of the syntax and meaning of the annotations used in typ\ning.NamedTuple and @dataclass declarations. We will cover type\nhints for function signatures in Chapter 8 and more advanced\nannotations in Chapter 15. Here we’ll mostly see hints with simple\nbuilt-in types, such as str, int, and float, which are probably the\nmost common types used to annotate fields of data classes.\nNo Runtime Effect\nThink about Python type hints as “documentation that can be verified by IDEs and\ntype checkers.”\nThat’s because type hints have no impact on the runtime behavior of Python pro‐\ngrams. Check out Example 5-9.\nExample 5-9. Python does not enforce type hints at runtime\n>>> import typing\n>>> class Coordinate(typing.NamedTuple):\n...     lat: float\n...     lon: float\n...\n>>> trash = Coordinate('Ni!', None)\n>>> print(trash)\nCoordinate(lat='Ni!', lon=None)    \nType Hints 101 \n| \n173",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "5 In the context of type hints, None is not the NoneType singleton, but an alias for NoneType itself. This is strange\nwhen we stop to think about it, but appeals to our intuition and makes function return annotations easier to\nread in the common case of functions that return None.\nI told you: no type checking at runtime!\nIf you type the code of Example 5-9 in a Python module, it will run and display a\nmeaningless Coordinate, with no error or warning:\n$ python3 nocheck_demo.py\nCoordinate(lat='Ni!', lon=None)\nThe type hints are intended primarily to support third-party type checkers, like Mypy\nor the PyCharm IDE built-in type checker. These are static analysis tools: they check\nPython source code “at rest,” not running code.\nTo see the effect of type hints, you must run one of those tools on your code—like a\nlinter. For instance, here is what Mypy has to say about the previous example:\n$ mypy nocheck_demo.py\nnocheck_demo.py:8: error: Argument 1 to \"Coordinate\" has\nincompatible type \"str\"; expected \"float\"\nnocheck_demo.py:8: error: Argument 2 to \"Coordinate\" has\nincompatible type \"None\"; expected \"float\"\nAs you can see, given the definition of Coordinate, Mypy knows that both arguments\nto create an instance must be of type float, but the assignment to trash uses a str\nand None.5\nNow let’s talk about the syntax and meaning of type hints.\nVariable Annotation Syntax\nBoth typing.NamedTuple and @dataclass use the syntax of variable annotations\ndefined in PEP 526. This is a quick introduction to that syntax in the context defining\nattributes in class statements.\nThe basic syntax of variable annotation is:\nvar_name: some_type\nThe “Acceptable type hints” section in PEP 484 explains what are acceptable types,\nbut in the context of defining a data class, these types are more likely to be useful:\n• A concrete class, for example, str or FrenchDeck\n• A parameterized collection type, like list[int], tuple[str, float], etc.\n174 \n| \nChapter 5: Data Class Builders",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "• typing.Optional, for example, Optional[str]—to declare a field that can be a\nstr or None\nYou can also initialize the variable with a value. In a typing.NamedTuple or @data\nclass declaration, that value will become the default for that attribute if the corre‐\nsponding argument is omitted in the constructor call:\nvar_name: some_type = a_value\nThe Meaning of Variable Annotations\nWe saw in “No Runtime Effect” on page 173 that type hints have no effect at runtime.\nBut at import time—when a module is loaded—Python does read them to build the\n__annotations__ dictionary that typing.NamedTuple and @dataclass then use to\nenhance the class.\nWe’ll start this exploration with a simple class in Example 5-10, so that we can later\nsee what extra features are added by typing.NamedTuple and @dataclass.\nExample 5-10. meaning/demo_plain.py: a plain class with type hints\nclass DemoPlainClass:\n    a: int           \n    b: float = 1.1   \n    c = 'spam'       \na becomes an entry in __annotations__, but is otherwise discarded: no attribute\nnamed a is created in the class.\nb is saved as an annotation, and also becomes a class attribute with value 1.1.\nc is just a plain old class attribute, not an annotation.\nWe can verify that in the console, first reading the __annotations__ of the Demo\nPlainClass, then trying to get its attributes named a, b, and c:\n>>> from demo_plain import DemoPlainClass\n>>> DemoPlainClass.__annotations__\n{'a': <class 'int'>, 'b': <class 'float'>}\n>>> DemoPlainClass.a\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: type object 'DemoPlainClass' has no attribute 'a'\n>>> DemoPlainClass.b\n1.1\n>>> DemoPlainClass.c\n'spam'\nType Hints 101 \n| \n175",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "6 Python has no concept of undefined, one of the silliest mistakes in the design of JavaScript. Thank Guido!\nNote that the __annotations__ special attribute is created by the interpreter to\nrecord the type hints that appear in the source code—even in a plain class.\nThe a survives only as an annotation. It doesn’t become a class attribute because no\nvalue is bound to it.6 The b and c are stored as class attributes because they are bound\nto values.\nNone of those three attributes will be in a new instance of DemoPlainClass. If you\ncreate an object o = DemoPlainClass(), o.a will raise AttributeError, while o.b\nand o.c will retrieve the class attributes with values 1.1 and 'spam'—that’s just nor‐\nmal Python object behavior.\nInspecting a typing.NamedTuple\nNow let’s examine a class built with typing.NamedTuple (Example 5-11), using the\nsame attributes and annotations as DemoPlainClass from Example 5-10.\nExample 5-11. meaning/demo_nt.py: a class built with typing.NamedTuple\nimport typing\nclass DemoNTClass(typing.NamedTuple):\n    a: int           \n    b: float = 1.1   \n    c = 'spam'       \na becomes an annotation and also an instance attribute.\nb is another annotation, and also becomes an instance attribute with default\nvalue 1.1.\nc is just a plain old class attribute; no annotation will refer to it.\nInspecting the DemoNTClass, we get:\n>>> from demo_nt import DemoNTClass\n>>> DemoNTClass.__annotations__\n{'a': <class 'int'>, 'b': <class 'float'>}\n>>> DemoNTClass.a\n<_collections._tuplegetter object at 0x101f0f940>\n>>> DemoNTClass.b\n<_collections._tuplegetter object at 0x101f0f8b0>\n>>> DemoNTClass.c\n'spam'\n176 \n| \nChapter 5: Data Class Builders",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "Here we have the same annotations for a and b as we saw in Example 5-10. But typ\ning.NamedTuple creates a and b class attributes. The c attribute is just a plain class\nattribute with the value 'spam'.\nThe a and b class attributes are descriptors—an advanced feature covered in Chap‐\nter 23. For now, think of them as similar to property getters: methods that don’t\nrequire the explicit call operator () to retrieve an instance attribute. In practice, this\nmeans a and b will work as read-only instance attributes—which makes sense when\nwe recall that DemoNTClass instances are just fancy tuples, and tuples are immutable.\nDemoNTClass also gets a custom docstring:\n>>> DemoNTClass.__doc__\n'DemoNTClass(a, b)'\nLet’s inspect an instance of DemoNTClass:\n>>> nt = DemoNTClass(8)\n>>> nt.a\n8\n>>> nt.b\n1.1\n>>> nt.c\n'spam'\nTo construct nt, we need to give at least the a argument to DemoNTClass. The con‐\nstructor also takes a b argument, but it has a default value of 1.1, so it’s optional. The\nnt object has the a and b attributes as expected; it doesn’t have a c attribute, but\nPython retrieves it from the class, as usual.\nIf you try to assign values to nt.a, nt.b, nt.c, or even nt.z, you’ll get Attribute\nError exceptions with subtly different error messages. Try that and reflect on the\nmessages.\nInspecting a class decorated with dataclass\nNow, we’ll examine Example 5-12.\nExample 5-12. meaning/demo_dc.py: a class decorated with @dataclass\nfrom dataclasses import dataclass\n@dataclass\nclass DemoDataClass:\n    a: int           \n    b: float = 1.1   \n    c = 'spam'       \nType Hints 101 \n| \n177",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "a becomes an annotation and also an instance attribute controlled by a\ndescriptor.\nb is another annotation, and also becomes an instance attribute with a descriptor\nand a default value 1.1.\nc is just a plain old class attribute; no annotation will refer to it.\nNow let’s check out __annotations__, __doc__, and the a, b, c attributes on Demo\nDataClass:\n>>> from demo_dc import DemoDataClass\n>>> DemoDataClass.__annotations__\n{'a': <class 'int'>, 'b': <class 'float'>}\n>>> DemoDataClass.__doc__\n'DemoDataClass(a: int, b: float = 1.1)'\n>>> DemoDataClass.a\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: type object 'DemoDataClass' has no attribute 'a'\n>>> DemoDataClass.b\n1.1\n>>> DemoDataClass.c\n'spam'\nThe __annotations__ and __doc__ are not surprising. However, there is no attribute\nnamed a in DemoDataClass—in contrast with DemoNTClass from Example 5-11,\nwhich has a descriptor to get a from the instances as read-only attributes (that myste‐\nrious <_collections._tuplegetter>). That’s because the a attribute will only exist\nin instances of DemoDataClass. It will be a public attribute that we can get and set,\nunless the class is frozen. But b and c exist as class attributes, with b holding the\ndefault value for the b instance attribute, while c is just a class attribute that will not\nbe bound to the instances.\nNow let’s see how a DemoDataClass instance looks:\n>>> dc = DemoDataClass(9)\n>>> dc.a\n9\n>>> dc.b\n1.1\n>>> dc.c\n'spam'\nAgain, a and b are instance attributes, and c is a class attribute we get via the instance.\nAs mentioned, DemoDataClass instances are mutable—and no type checking is done\nat runtime:\n178 \n| \nChapter 5: Data Class Builders",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "7 Setting an attribute after __init__ defeats the __dict__ key-sharing memory optimization mentioned in\n“Practical Consequences of How dict Works” on page 102.\n>>> dc.a = 10\n>>> dc.b = 'oops'\nWe can do even sillier assignments:\n>>> dc.c = 'whatever'\n>>> dc.z = 'secret stash'\nNow the dc instance has a c attribute—but that does not change the c class attribute.\nAnd we can add a new z attribute. This is normal Python behavior: regular instances\ncan have their own attributes that don’t appear in the class.7\nMore About @dataclass\nWe’ve only seen simple examples of @dataclass use so far. The decorator accepts\nseveral keyword arguments. This is its signature:\n@dataclass(*, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False)\nThe * in the first position means the remaining parameters are keyword-only.\nTable 5-2 describes them.\nTable 5-2. Keyword parameters accepted by the @dataclass decorator\nOption\nMeaning\nDefault\nNotes\ninit\nGenerate __init__\nTrue\nIgnored if __init__ is implemented by\nuser.\nrepr\nGenerate __repr__\nTrue\nIgnored if __repr__ is implemented by\nuser.\neq\nGenerate __eq__\nTrue\nIgnored if __eq__ is implemented by\nuser.\norder\nGenerate __lt__,\n__le__, __gt__,\n__ge__\nFalse\nIf True, raises exceptions if eq=False,\nor if any of the comparison methods that\nwould be generated are defined or\ninherited.\nunsafe_hash\nGenerate __hash__\nFalse\nComplex semantics and several caveats—\nsee: dataclass documentation.\nfrozen\nMake instances\n“immutable”\nFalse\nInstances will be reasonably safe from\naccidental change, but not really\nimmutable.a\na @dataclass emulates immutability by generating __setattr__ and __delattr__, which raise data\nclass.FrozenInstanceError—a subclass of AttributeError—when the user attempts to set or delete a field.\nMore About @dataclass \n| \n179",
      "content_length": 1811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "The defaults are really the most useful settings for common use cases. The options\nyou are more likely to change from the defaults are:\nfrozen=True\nProtects against accidental changes to the class instances.\norder=True\nAllows sorting of instances of the data class.\nGiven the dynamic nature of Python objects, it’s not too hard for a nosy programmer\nto go around the protection afforded by frozen=True. But the necessary tricks should\nbe easy to spot in a code review.\nIf the eq and frozen arguments are both True, @dataclass produces a suitable\n__hash__ method, so the instances will be hashable. The generated __hash__ will use\ndata from all fields that are not individually excluded using a field option we’ll see in\n“Field Options” on page 180. If frozen=False (the default), @dataclass will set\n__hash__ to None, signalling that the instances are unhashable, therefore overriding\n__hash__ from any superclass.\nPEP 557—Data Classes has this to say about unsafe_hash:\nAlthough not recommended, you can force Data Classes to create a __hash__ method\nwith unsafe_hash=True. This might be the case if your class is logically immutable but\ncan nonetheless be mutated. This is a specialized use case and should be considered\ncarefully.\nI will leave unsafe_hash at that. If you feel you must use that option, check the data\nclasses.dataclass documentation.\nFurther customization of the generated data class can be done at a field level.\nField Options\nWe’ve already seen the most basic field option: providing (or not) a default value\nwith the type hint. The instance fields you declare will become parameters in the gen‐\nerated __init__. Python does not allow parameters without defaults after parameters\nwith defaults, therefore after you declare a field with a default value, all remaining\nfields must also have default values.\nMutable default values are a common source of bugs for beginning Python develop‐\ners. In function definitions, a mutable default value is easily corrupted when one\ninvocation of the function mutates the default, changing the behavior of further invo‐\ncations—an issue we’ll explore in “Mutable Types as Parameter Defaults: Bad Idea”\non page 214 (Chapter 6). Class attributes are often used as default attribute values for\ninstances, including in data classes. And @dataclass uses the default values in the\n180 \n| \nChapter 5: Data Class Builders",
      "content_length": 2371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "type hints to generate parameters with defaults for __init__. To prevent bugs, @data\nclass rejects the class definition in Example 5-13.\nExample 5-13. dataclass/club_wrong.py: this class raises ValueError\n@dataclass\nclass ClubMember:\n    name: str\n    guests: list = []\nIf you load the module with that ClubMember class, this is what you get:\n$ python3 club_wrong.py\nTraceback (most recent call last):\n  File \"club_wrong.py\", line 4, in <module>\n    class ClubMember:\n  ...several lines omitted...\nValueError: mutable default <class 'list'> for field guests is not allowed:\nuse default_factory\nThe ValueError message explains the problem and suggests a solution: use\ndefault_factory. Example 5-14 shows how to correct ClubMember.\nExample 5-14. dataclass/club.py: this ClubMember definition works\nfrom dataclasses import dataclass, field\n@dataclass\nclass ClubMember:\n    name: str\n    guests: list = field(default_factory=list)\nIn the guests field of Example 5-14, instead of a literal list, the default value is set by\ncalling the dataclasses.field function with default_factory=list.\nThe default_factory parameter lets you provide a function, class, or any other call‐\nable, which will be invoked with zero arguments to build a default value each time an\ninstance of the data class is created. This way, each instance of ClubMember will have\nits own list—instead of all instances sharing the same list from the class, which is\nrarely what we want and is often a bug.\nIt’s good that @dataclass rejects class definitions with a list\ndefault value in a field. However, be aware that it is a partial solu‐\ntion that only applies to list, dict, and set. Other mutable values\nused as defaults will not be flagged by @dataclass. It’s up to you to\nunderstand the problem and remember to use a default factory to\nset mutable default values.\nMore About @dataclass \n| \n181",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "If you browse the dataclasses module documentation, you’ll see a list field defined\nwith a novel syntax, as in Example 5-15.\nExample 5-15. dataclass/club_generic.py: this ClubMember definition is more precise\nfrom dataclasses import dataclass, field\n@dataclass\nclass ClubMember:\n    name: str\n    guests: list[str] = field(default_factory=list)  \nlist[str] means “a list of str.”\nThe new syntax list[str] is a parameterized generic type: since Python 3.9, the list\nbuilt-in accepts that bracket notation to specify the type of the list items.\nPrior to Python 3.9, the built-in collections did not support generic\ntype notation. As a temporary workaround, there are correspond‐\ning collection types in the typing module. If you need a parameter‐\nized list type hint in Python 3.8 or earlier, you must import the\nList type from typing and use it: List[str]. For more about this\nissue, see “Legacy Support and Deprecated Collection Types” on\npage 272.\nWe’ll cover generics in Chapter 8. For now, note that Examples 5-14 and 5-15 are\nboth correct, and the Mypy type checker does not complain about either of those\nclass definitions.\nThe difference is that guests: list means that guests can be a list of objects of\nany kind, while guests: list[str] says that guests must be a list in which every\nitem is a str. This will allow the type checker to find (some) bugs in code that puts\ninvalid items in the list, or that read items from it.\nThe default_factory is likely to be the most common option of the field function,\nbut there are several others, listed in Table 5-3.\nTable 5-3. Keyword arguments accepted by the field function\nOption\nMeaning\nDefault\ndefault\nDefault value for field\n_MISSING_TYPEa\ndefault_factory\n0-parameter function used to produce a default\n_MISSING_TYPE\ninit\nInclude field in parameters to __init__\nTrue\nrepr\nInclude field in __repr__\nTrue\n182 \n| \nChapter 5: Data Class Builders",
      "content_length": 1897,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Option\nMeaning\nDefault\ncompare\nUse field in comparison methods __eq__,\n__lt__, etc.\nTrue\nhash\nInclude field in __hash__ calculation\nNoneb\nmetadata\nMapping with user-defined data; ignored by\nthe @dataclass\nNone\na dataclass._MISSING_TYPE is a sentinel value indicating the option was not provided. It exists so we can set None\nas an actual default value, a common use case.\nb The option hash=None means the field will be used in __hash__ only if compare=True.\nThe default option exists because the field call takes the place of the default value\nin the field annotation. If you want to create an athlete field with a default value of\nFalse, and also omit that field from the __repr__ method, you’d write this:\n@dataclass\nclass ClubMember:\n    name: str\n    guests: list = field(default_factory=list)\n    athlete: bool = field(default=False, repr=False)\nPost-init Processing\nThe __init__ method generated by @dataclass only takes the arguments passed and\nassigns them—or their default values, if missing—to the instance attributes that are\ninstance fields. But you may need to do more than that to initialize the instance.\nIf that’s the case, you can provide a __post_init__ method. When that method\nexists, @dataclass will add code to the generated __init__ to call __post_init__ as\nthe last step.\nCommon use cases for __post_init__ are validation and computing field values\nbased on other fields. We’ll study a simple example that uses __post_init__ for both\nof these reasons.\nFirst, let’s look at the expected behavior of a ClubMember subclass named HackerClub\nMember, as described by doctests in Example 5-16.\nExample 5-16. dataclass/hackerclub.py: doctests for HackerClubMember\n\"\"\"\n``HackerClubMember`` objects accept an optional ``handle`` argument::\n    >>> anna = HackerClubMember('Anna Ravenscroft', handle='AnnaRaven')\n    >>> anna\n    HackerClubMember(name='Anna Ravenscroft', guests=[], handle='AnnaRaven')\nIf ``handle`` is omitted, it's set to the first part of the member's name::\nMore About @dataclass \n| \n183",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": ">>> leo = HackerClubMember('Leo Rochael')\n    >>> leo\n    HackerClubMember(name='Leo Rochael', guests=[], handle='Leo')\nMembers must have a unique handle. The following ``leo2`` will not be created,\nbecause its ``handle`` would be 'Leo', which was taken by ``leo``::\n    >>> leo2 = HackerClubMember('Leo DaVinci')\n    Traceback (most recent call last):\n      ...\n    ValueError: handle 'Leo' already exists.\nTo fix, ``leo2`` must be created with an explicit ``handle``::\n    >>> leo2 = HackerClubMember('Leo DaVinci', handle='Neo')\n    >>> leo2\n    HackerClubMember(name='Leo DaVinci', guests=[], handle='Neo')\n\"\"\"\nNote that we must provide handle as a keyword argument, because HackerClubMem\nber inherits name and guests from ClubMember, and adds the handle field. The gen‐\nerated docstring for HackerClubMember shows the order of the fields in the\nconstructor call:\n>>> HackerClubMember.__doc__\n\"HackerClubMember(name: str, guests: list = <factory>, handle: str = '')\"\nHere, <factory> is a short way of saying that some callable will produce the default\nvalue for guests (in our case, the factory is the list class). The point is: to provide a\nhandle but no guests, we must pass handle as a keyword argument.\nThe “Inheritance” section of the dataclasses module documentation explains how\nthe order of the fields is computed when there are several levels of inheritance.\nIn Chapter 14 we’ll talk about misusing inheritance, particularly\nwhen the superclasses are not abstract. Creating a hierarchy of data\nclasses is usually a bad idea, but it served us well here to make\nExample 5-17 shorter, focusing on the handle field declaration and\n__post_init__ validation.\nExample 5-17 shows the implementation.\nExample 5-17. dataclass/hackerclub.py: code for HackerClubMember\nfrom dataclasses import dataclass\nfrom club import ClubMember\n184 \n| \nChapter 5: Data Class Builders",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "@dataclass\nclass HackerClubMember(ClubMember):                         \n    all_handles = set()                                     \n    handle: str = ''                                        \n    def __post_init__(self):\n        cls = self.__class__                                \n        if self.handle == '':                               \n            self.handle = self.name.split()[0]\n        if self.handle in cls.all_handles:                  \n            msg = f'handle {self.handle!r} already exists.'\n            raise ValueError(msg)\n        cls.all_handles.add(self.handle)                    \nHackerClubMember extends ClubMember.\nall_handles is a class attribute.\nhandle is an instance field of type str with an empty string as its default value;\nthis makes it optional.\nGet the class of the instance.\nIf self.handle is the empty string, set it to the first part of name.\nIf self.handle is in cls.all_handles, raise ValueError.\nAdd the new handle to cls.all_handles.\nExample 5-17 works as intended, but is not satisfactory to a static type checker. Next,\nwe’ll see why, and how to fix it.\nTyped Class Attributes\nIf we type check Example 5-17 with Mypy, we are reprimanded:\n$ mypy hackerclub.py\nhackerclub.py:37: error: Need type annotation for \"all_handles\"\n(hint: \"all_handles: Set[<type>] = ...\")\nFound 1 error in 1 file (checked 1 source file)\nUnfortunately, the hint provided by Mypy (version 0.910 as I review this) is not help‐\nful in the context of @dataclass usage. First, it suggests using Set, but I am using\nPython 3.9 so I can use set—and avoid importing Set from typing. More impor‐\ntantly, if we add a type hint like set[…] to all_handles, @dataclass will find that\nannotation and make all_handles an instance field. We saw this happening in\n“Inspecting a class decorated with dataclass” on page 177.\nMore About @dataclass \n| \n185",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "The workaround defined in PEP 526—Syntax for Variable Annotations is ugly. To\ncode a class variable with a type hint, we need to use a pseudotype named typ\ning.ClassVar, which leverages the generics [] notation to set the type of the variable\nand also declare it a class attribute.\nTo make the type checker and @dataclass happy, this is how we are supposed to\ndeclare all_handles in Example 5-17:\n    all_handles: ClassVar[set[str]] = set()\nThat type hint is saying:\nall_handles is a class attribute of type set-of-str, with an empty set as its default\nvalue.\nTo code that annotation, we must import ClassVar from the typing module.\nThe @dataclass decorator doesn’t care about the types in the annotations, except in\ntwo cases, and this is one of them: if the type is ClassVar, an instance field will not be\ngenerated for that attribute.\nThe other case where the type of the field is relevant to @dataclass is when declaring\ninit-only variables, our next topic.\nInitialization Variables That Are Not Fields\nSometimes you may need to pass arguments to __init__ that are not instance fields.\nSuch arguments are called init-only variables by the dataclasses documentation. To\ndeclare an argument like that, the dataclasses module provides the pseudotype Init\nVar, which uses the same syntax of typing.ClassVar. The example given in the doc‐\numentation is a data class that has a field initialized from a database, and the database\nobject must be passed to the constructor.\nExample 5-18 shows the code that illustrates the “Init-only variables” section.\nExample 5-18. Example from the dataclasses module documentation\n@dataclass\nclass C:\n    i: int\n    j: int = None\n    database: InitVar[DatabaseType] = None\n    def __post_init__(self, database):\n        if self.j is None and database is not None:\n            self.j = database.lookup('j')\nc = C(10, database=my_database)\n186 \n| \nChapter 5: Data Class Builders",
      "content_length": 1909,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "8 Source: Dublin Core article in the English Wikipedia.\nNote how the database attribute is declared. InitVar will prevent @dataclass from\ntreating database as a regular field. It will not be set as an instance attribute, and the\ndataclasses.fields function will not list it. However, database will be one of the\narguments that the generated __init__ will accept, and it will be also passed to\n__post_init__. If you write that method, you must add a corresponding argument to\nthe method signature, as shown in Example 5-18.\nThis rather long overview of @dataclass covered the most useful features—some of\nthem appeared in previous sections, like “Main Features” on page 167 where we cov‐\nered all three data class builders in parallel. The dataclasses documentation and\nPEP 526—Syntax for Variable Annotations have all the details.\nIn the next section, I present a longer example with @dataclass.\n@dataclass Example: Dublin Core Resource Record\nOften, classes built with @dataclass will have more fields than the very short exam‐\nples presented so far. Dublin Core provides the foundation for a more typical @data\nclass example.\nThe Dublin Core Schema is a small set of vocabulary terms that can be used to describe\ndigital resources (video, images, web pages, etc.), as well as physical resources such as\nbooks or CDs, and objects like artworks.8\n—Dublin Core on Wikipedia\nThe standard defines 15 optional fields; the Resource class in Example 5-19 uses 8 of\nthem.\nExample 5-19. dataclass/resource.py: code for Resource, a class based on Dublin Core\nterms\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom enum import Enum, auto\nfrom datetime import date\nclass ResourceType(Enum):  \n    BOOK = auto()\n    EBOOK = auto()\n    VIDEO = auto()\n@dataclass\nMore About @dataclass \n| \n187",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "class Resource:\n    \"\"\"Media resource description.\"\"\"\n    identifier: str                                    \n    title: str = '<untitled>'                          \n    creators: list[str] = field(default_factory=list)\n    date: Optional[date] = None                        \n    type: ResourceType = ResourceType.BOOK             \n    description: str = ''\n    language: str = ''\n    subjects: list[str] = field(default_factory=list)\nThis Enum will provide type-safe values for the Resource.type field.\nidentifier is the only required field.\ntitle is the first field with a default. This forces all fields below to provide\ndefaults.\nThe value of date can be a datetime.date instance, or None.\nThe type field default is ResourceType.BOOK.\nExample 5-20 shows a doctest to demonstrate how a Resource record appears in\ncode.\nExample 5-20. dataclass/resource.py: code for Resource, a class based on Dublin Core\nterms\n    >>> description = 'Improving the design of existing code'\n    >>> book = Resource('978-0-13-475759-9', 'Refactoring, 2nd Edition',\n    ...     ['Martin Fowler', 'Kent Beck'], date(2018, 11, 19),\n    ...     ResourceType.BOOK, description, 'EN',\n    ...     ['computer programming', 'OOP'])\n    >>> book  # doctest: +NORMALIZE_WHITESPACE\n    Resource(identifier='978-0-13-475759-9', title='Refactoring, 2nd Edition',\n    creators=['Martin Fowler', 'Kent Beck'], date=datetime.date(2018, 11, 19),\n    type=<ResourceType.BOOK: 1>, description='Improving the design of existing code',\n    language='EN', subjects=['computer programming', 'OOP'])\nThe __repr__ generated by @dataclass is OK, but we can make it more readable.\nThis is the format we want from repr(book):\n    >>> book  # doctest: +NORMALIZE_WHITESPACE\n    Resource(\n        identifier = '978-0-13-475759-9',\n        title = 'Refactoring, 2nd Edition',\n        creators = ['Martin Fowler', 'Kent Beck'],\n        date = datetime.date(2018, 11, 19),\n        type = <ResourceType.BOOK: 1>,\n188 \n| \nChapter 5: Data Class Builders",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "description = 'Improving the design of existing code',\n        language = 'EN',\n        subjects = ['computer programming', 'OOP'],\n    )\nExample 5-21 is the code of __repr__ to produce the format shown in the last snip‐\npet. This example uses dataclass.fields to get the names of the data class fields.\nExample 5-21. dataclass/resource_repr.py: code for __repr__ method\nimplemented in the Resource class from Example 5-19\n    def __repr__(self):\n        cls = self.__class__\n        cls_name = cls.__name__\n        indent = ' ' * 4\n        res = [f'{cls_name}(']                            \n        for f in fields(cls):                             \n            value = getattr(self, f.name)                 \n            res.append(f'{indent}{f.name} = {value!r},')  \n        res.append(')')                                   \n        return '\\n'.join(res)                             \nStart the res list to build the output string with the class name and open\nparenthesis.\nFor each field f in the class…\n…get the named attribute from the instance.\nAppend an indented line with the name of the field and repr(value)—that’s\nwhat the !r does.\nAppend closing parenthesis.\nBuild a multiline string from res and return it.\nWith this example inspired by the soul of Dublin, Ohio, we conclude our tour of\nPython’s data class builders.\nData classes are handy, but your project may suffer if you overuse them. The next\nsection explains.\nMore About @dataclass \n| \n189",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "9 I am fortunate to have Martin Fowler as a colleague at Thoughtworks, so it took just 20 minutes to get his\npermission.\nData Class as a Code Smell\nWhether you implement a data class by writing all the code yourself or leveraging\none of the class builders described in this chapter, be aware that it may signal a prob‐\nlem in your design.\nIn Refactoring: Improving the Design of Existing Code, 2nd ed. (Addison-Wesley),\nMartin Fowler and Kent Beck present a catalog of “code smells”—patterns in code\nthat may indicate the need for refactoring. The entry titled “Data Class” starts like\nthis:\nThese are classes that have fields, getting and setting methods for fields, and nothing\nelse. Such classes are dumb data holders and are often being manipulated in far too\nmuch detail by other classes.\nIn Fowler’s personal website, there’s an illuminating post titled “Code Smell”. The\npost is very relevant to our discussion because he uses data class as one example of a\ncode smell and suggests how to deal with it. Here is the post, reproduced in full.9\nCode Smell\nBy Martin Fowler\nA code smell is a surface indication that usually corresponds to a deeper problem in\nthe system. The term was first coined by Kent Beck while helping me with my Refac‐\ntoring book.\nThe quick definition above contains a couple of subtle points. Firstly, a smell is by\ndefinition something that’s quick to spot—or sniffable as I’ve recently put it. A long\nmethod is a good example of this—just looking at the code and my nose twitches if I\nsee more than a dozen lines of Java.\nThe second is that smells don’t always indicate a problem. Some long methods are\njust fine. You have to look deeper to see if there is an underlying problem there—\nsmells aren’t inherently bad on their own—they are often an indicator of a problem\nrather than the problem themselves.\nThe best smells are something that’s easy to spot and most of the time lead you to\nreally interesting problems. Data classes (classes with all data and no behavior) are\ngood examples of this. You look at them and ask yourself what behavior should be in\nthis class. Then you start refactoring to move that behavior in there. Often simple\nquestions and initial refactorings can be the vital step in turning anemic objects into\nsomething that really has class.\n190 \n| \nChapter 5: Data Class Builders",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "One of the nice things about smells is that it’s easy for inexperienced people to spot\nthem, even if they don’t know enough to evaluate if there’s a real problem or to cor‐\nrect them. I’ve heard of lead developers who will pick a “smell of the week” and ask\npeople to look for the smell and bring it up with the senior members of the team.\nDoing it one smell at a time is a good way of gradually teaching people on the team to\nbe better programmers.\nThe main idea of object-oriented programming is to place behavior and data together\nin the same code unit: a class. If a class is widely used but has no significant behavior\nof its own, it’s possible that code dealing with its instances is scattered (and even\nduplicated) in methods and functions throughout the system—a recipe for mainte‐\nnance headaches. That’s why Fowler’s refactorings to deal with a data class involve\nbringing responsibilities back into it.\nTaking that into account, there are a couple of common scenarios where it makes\nsense to have a data class with little or no behavior.\nData Class as Scaffolding\nIn this scenario, the data class is an initial, simplistic implementation of a class to\njump-start a new project or module. With time, the class should get its own methods,\ninstead of relying on methods of other classes to operate on its instances. Scaffolding\nis temporary; eventually your custom class may become fully independent from the\nbuilder you used to start it.\nPython is also used for quick problem solving and experimentation, and then it’s OK\nto leave the scaffolding in place.\nData Class as Intermediate Representation\nA data class can be useful to build records about to be exported to JSON or some\nother interchange format, or to hold data that was just imported, crossing some sys‐\ntem boundary. Python’s data class builders all provide a method or function to con‐\nvert an instance to a plain dict, and you can always invoke the constructor with a\ndict used as keyword arguments expanded with **. Such a dict is very close to a\nJSON record.\nIn this scenario, the data class instances should be handled as immutable objects—\neven if the fields are mutable, you should not change them while they are in this\nintermediate form. If you do, you’re losing the key benefit of having data and behav‐\nior close together. When importing/exporting requires changing values, you should\nimplement your own builder methods instead of using the given “as dict” methods or\nstandard constructors.\nData Class as a Code Smell \n| \n191",
      "content_length": 2507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "10 I put this content here because it is the earliest chapter focusing on user-defined classes, and I thought pattern\nmatching with classes was too important to wait until Part II of the book. My philosophy: it’s more important\nto know how to use classes than to define classes.\nNow we change the subject to see how to write patterns that match instances of arbi‐\ntrary classes, and not just the sequences and mappings we’ve seen in “Pattern Match‐\ning with Sequences” on page 38 and “Pattern Matching with Mappings” on page 81.\nPattern Matching Class Instances\nClass patterns are designed to match class instances by type and—optionally—by\nattributes. The subject of a class pattern can be any class instance, not only instances\nof data classes.10\nThere are three variations of class patterns: simple, keyword, and positional. We’ll\nstudy them in that order.\nSimple Class Patterns\nWe’ve already seen an example with simple class patterns used as subpatterns in “Pat‐\ntern Matching with Sequences” on page 38:\n        case [str(name), _, _, (float(lat), float(lon))]:\nThat pattern matches a four-item sequence where the first item must be an instance\nof str, and the last item must be a 2-tuple with two instances of float.\nThe syntax for class patterns looks like a constructor invocation. The following is a\nclass pattern that matches float values without binding a variable (the case body can\nrefer to x directly if needed):\n    match x:\n        case float():\n            do_something_with(x)\nBut this is likely to be a bug in your code:\n    match x:\n        case float:  # DANGER!!!\n            do_something_with(x)\nIn the preceding example, case float: matches any subject, because Python sees\nfloat as a variable, which is then bound to the subject.\nThe simple pattern syntax of float(x) is a special case that applies only to nine\nblessed built-in types, listed at the end of the “Class Patterns” section of PEP 634—\nStructural Pattern Matching: Specification:\nbytes   dict   float   frozenset   int   list   set   str   tuple\n192 \n| \nChapter 5: Data Class Builders",
      "content_length": 2072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "In those classes, the variable that looks like a constructor argument—e.g., the x in\nfloat(x)—is bound to the whole subject instance or the part of the subject that\nmatches a subpattern, as exemplified by str(name) in the sequence pattern we saw\nearlier:\n        case [str(name), _, _, (float(lat), float(lon))]:\nIf the class is not one of those nine blessed built-ins, then the argument-like variables\nrepresent patterns to be matched against attributes of an instance of that class.\nKeyword Class Patterns\nTo understand how to use keyword class patterns, consider the following City class\nand five instances in Example 5-22.\nExample 5-22. City class and a few instances\nimport typing\nclass City(typing.NamedTuple):\n    continent: str\n    name: str\n    country: str\ncities = [\n    City('Asia', 'Tokyo', 'JP'),\n    City('Asia', 'Delhi', 'IN'),\n    City('North America', 'Mexico City', 'MX'),\n    City('North America', 'New York', 'US'),\n    City('South America', 'São Paulo', 'BR'),\n]\nGiven those definitions, the following function would return a list of Asian cities:\ndef match_asian_cities():\n    results = []\n    for city in cities:\n        match city:\n            case City(continent='Asia'):\n                results.append(city)\n    return results\nThe pattern City(continent='Asia') matches any City instance where the conti\nnent attribute value is equal to 'Asia', regardless of the values of the other attributes.\nIf you want to collect the value of the country attribute, you could write:\ndef match_asian_countries():\n    results = []\n    for city in cities:\nPattern Matching Class Instances \n| \n193",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "match city:\n            case City(continent='Asia', country=cc):\n                results.append(cc)\n    return results\nThe pattern City(continent='Asia', country=cc) matches the same Asian cities as\nbefore, but now the cc variable is bound to the country attribute of the instance. This\nalso works if the pattern variable is called country as well:\n        match city:\n            case City(continent='Asia', country=country):\n                results.append(country)\nKeyword class patterns are very readable, and work with any class that has public\ninstance attributes, but they are somewhat verbose.\nPositional class patterns are more convenient in some cases, but they require explicit\nsupport by the class of the subject, as we’ll see next.\nPositional Class Patterns\nGiven the definitions from Example 5-22, the following function would return a list\nof Asian cities, using a positional class pattern:\ndef match_asian_cities_pos():\n    results = []\n    for city in cities:\n        match city:\n            case City('Asia'):\n                results.append(city)\n    return results\nThe pattern City('Asia') matches any City instance where the first attribute value\nis 'Asia', regardless of the values of the other attributes.\nIf you want to collect the value of the country attribute, you could write:\ndef match_asian_countries_pos():\n    results = []\n    for city in cities:\n        match city:\n            case City('Asia', _, country):\n                results.append(country)\n    return results\nThe pattern City('Asia', _, country) matches the same cities as before, but now\nthe country variable is bound to the third attribute of the instance.\nI’ve mentioned “first” or “third” attribute, but what does that really mean?\n194 \n| \nChapter 5: Data Class Builders",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "What makes City or any class work with positional patterns is the presence of a spe‐\ncial class attribute named __match_args__, which the class builders in this chapter\nautomatically create. This is the value of __match_args__ in the City class:\n>>> City.__match_args__\n('continent', 'name', 'country')\nAs you can see, __match_args__ declares the names of the attributes in the order they\nwill be used in positional patterns.\nIn “Supporting Positional Pattern Matching” on page 377 we’ll write code to define\n__match_args__ for a class we’ll create without the help of a class builder.\nYou can combine keyword and positional arguments in a pattern.\nSome, but not all, of the instance attributes available for matching\nmay be listed in __match_args__. Therefore, sometimes you may\nneed to use keyword arguments in addition to positional argu‐\nments in a pattern.\nTime for a chapter summary.\nChapter Summary\nThe main topic of this chapter was the data class builders collections.namedtuple,\ntyping.NamedTuple, and dataclasses.dataclass. We saw that each generates data\nclasses from descriptions provided as arguments to a factory function, or from class\nstatements with type hints in the case of the latter two. In particular, both named\ntuple variants produce tuple subclasses, adding only the ability to access fields by\nname, and providing a _fields class attribute listing the field names as a tuple of\nstrings.\nNext we studied the main features of the three class builders side by side, including\nhow to extract instance data as a dict, how to get the names and default values of\nfields, and how to make a new instance from an existing one.\nThis prompted our first look into type hints, particularly those used to annotate\nattributes in a class statement, using the notation introduced in Python 3.6 with\nPEP 526—Syntax for Variable Annotations. Probably the most surprising aspect of\ntype hints in general is the fact that they have no effect at all at runtime. Python\nremains a dynamic language. External tools, like Mypy, are needed to take advantage\nof typing information to detect errors via static analysis of the source code. After a\nbasic overview of the syntax from PEP 526, we studied the effect of annotations in a\nplain class and in classes built by typing.NamedTuple and @dataclass.\nChapter Summary \n| \n195",
      "content_length": 2322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Next, we covered the most commonly used features provided by @dataclass and the\ndefault_factory option of the dataclasses.field function. We also looked into\nthe special pseudotype hints typing.ClassVar and dataclasses.InitVar that are\nimportant in the context of data classes. This main topic concluded with an example\nbased on the Dublin Core Schema, which illustrated how to use dataclasses.fields\nto iterate over the attributes of a Resource instance in a custom __repr__.\nThen, we warned against possible abuse of data classes defeating a basic principle of\nobject-oriented programming: data and the functions that touch it should be together\nin the same class. Classes with no logic may be a sign of misplaced logic.\nIn the last section, we saw how pattern matching works with subjects that are instan‐\nces of any class—not just classes built with the class builders presented in this\nchapter.\nFurther Reading\nPython’s standard documentation for the data class builders we covered is very good,\nand has quite a few small examples.\nFor @dataclass in particular, most of PEP 557—Data Classes was copied into the\ndataclasses module documentation. But PEP 557 has a few very informative sec‐\ntions that were not copied, including “Why not just use namedtuple?”, “Why not just\nuse typing.NamedTuple?”, and the “Rationale” section, which concludes with this\nQ&A:\nWhere is it not appropriate to use Data Classes?\nAPI compatibility with tuples or dicts is required. Type validation beyond that pro‐\nvided by PEPs 484 and 526 is required, or value validation or conversion is required.\n—Eric V. Smith, PEP 557 “Rationale”\nOver at RealPython.com, Geir Arne Hjelle wrote a very complete “Ultimate guide to\ndata classes in Python 3.7”.\nAt PyCon US 2018, Raymond Hettinger presented “Dataclasses: The code generator\nto end all code generators” (video).\nFor more features and advanced functionality, including validation, the attrs project\nled by Hynek Schlawack appeared years before dataclasses, and offers more fea‐\ntures, promising to “bring back the joy of writing classes by relieving you from the\ndrudgery of implementing object protocols (aka dunder methods).” The influence of\nattrs on @dataclass is acknowledged by Eric V. Smith in PEP 557. This probably\nincludes Smith’s most important API decision: the use of a class decorator instead of\na base class and/or a metaclass to do the job.\n196 \n| \nChapter 5: Data Class Builders",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Glyph—founder of the Twisted project—wrote an excellent introduction to attrs in\n“The One Python Library Everyone Needs”. The attrs documentation includes a dis‐\ncussion of alternatives.\nBook author, instructor, and mad computer scientist Dave Beazley wrote cluegen, yet\nanother data class generator. If you’ve seen any of Dave’s talks, you know he is a mas‐\nter of metaprogramming Python from first principles. So I found it inspiring to learn\nfrom the cluegen README.md file the concrete use case that motivated him to write\nan alternative to Python’s @dataclass, and his philosophy of presenting an approach\nto solve the problem, in contrast to providing a tool: the tool may be quicker to use at\nfirst, but the approach is more flexible and can take you as far as you want to go.\nRegarding data class as a code smell, the best source I found was Martin Fowler’s\nbook Refactoring, 2nd ed. This newest version is missing the quote from the epigraph\nof this chapter, “Data classes are like children…,” but otherwise it’s the best edition of\nFowler’s most famous book, particularly for Pythonistas because the examples are\nin modern JavaScript, which is closer to Python than Java—the language of the first\nedition.\nThe website Refactoring Guru also has a description of the data class code smell.\nSoapbox\nThe entry for “Guido” in “The Jargon File” is about Guido van Rossum. It says,\namong other things:\nMythically, Guido’s most important attribute besides Python itself is Guido’s time\nmachine, a device he is reputed to possess because of the unnerving frequency with\nwhich user requests for new features have been met with the response “I just imple‐\nmented that last night…”\nFor the longest time, one of the missing pieces in Python’s syntax has been a quick,\nstandard way to declare instance attributes in a class. Many object-oriented languages\nhave that. Here is part of a Point class definition in Smalltalk:\nObject subclass: #Point\n    instanceVariableNames: 'x y'\n    classVariableNames: ''\n    package: 'Kernel-BasicObjects'\nThe second line lists the names of the instance attributes x and y. If there were class\nattributes, they would be in the third line.\nPython has always offered an easy way to declare class attributes, if they have an ini‐\ntial value. But instance attributes are much more common, and Python coders have\nbeen forced to look into the __init__ method to find them, always afraid that there\nmay be instance attributes created elsewhere in the class—or even created by external\nfunctions or methods of other classes.\nFurther Reading \n| \n197",
      "content_length": 2571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "Now we have @dataclass, yay!\nBut they bring their own problems.\nFirst, when you use @dataclass, type hints are not optional. We’ve been promised for\nthe last seven years, since PEP 484—Type Hints that they would always be optional.\nNow we have a major new language feature that requires them. If you don’t like the\nwhole static typing trend, you may want to use attrs instead.\nSecond, the PEP 526 syntax for annotating instance and class attributes reverses the\nestablished convention of class statements: everything declared at the top-level of a\nclass block was a class attribute (methods are class attributes, too). With PEP 526\nand @dataclass, any attribute declared at the top level with a type hint becomes an\ninstance attribute:\n    @dataclass\n    class Spam:\n        repeat: int  # instance attribute\nHere, repeat is also an instance attribute:\n    @dataclass\n    class Spam:\n        repeat: int = 99  # instance attribute\nBut if there are no type hints, suddenly you are back in the good old times when dec‐\nlarations at the top level of the class belong to the class only:\n    @dataclass\n    class Spam:\n        repeat = 99  # class attribute!\nFinally, if you want to annotate that class attribute with a type, you can’t use regular\ntypes because then it will become an instance attribute. You must resort to that pseu‐\ndotype ClassVar annotation:\n    @dataclass\n    class Spam:\n        repeat: ClassVar[int] = 99  # aargh!\nHere we are talking about the exception to the exception to the rule. This seems\nrather unPythonic to me.\nI did not take part in the discussions leading to PEP 526 or PEP 557—Data Classes,\nbut here is an alternative syntax that I’d like to see:\n@dataclass\nclass HackerClubMember:\n    .name: str                                   \n    .guests: list = field(default_factory=list)\n    .handle: str = ''\n    all_handles = set()                          \n198 \n| \nChapter 5: Data Class Builders",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Instance attributes must be declared with a . prefix.\nAny attribute name that doesn’t have a . prefix is a class attribute (as they always\nhave been).\nThe language grammar would have to change to accept that. I find this quite readable,\nand it avoids the exception-to-the-exception issue.\nI wish I could borrow Guido’s time machine to go back to 2017 and sell this idea to\nthe core team.\nFurther Reading \n| \n199",
      "content_length": 411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "CHAPTER 6\nObject References, Mutability,\nand Recycling\n“You are sad,” the Knight said in an anxious tone: “let me sing you a song to comfort\nyou. […] The name of the song is called ‘HADDOCKS’ EYES’.”\n“Oh, that’s the name of the song, is it?” Alice said, trying to feel interested.\n“No, you don’t understand,” the Knight said, looking a little vexed. “That’s what the\nname is CALLED. The name really IS ‘THE AGED AGED MAN.’”\n—Adapted from Lewis Carroll, Through the Looking-Glass, and What Alice Found\nThere\nAlice and the Knight set the tone of what we will see in this chapter. The theme is the\ndistinction between objects and their names. A name is not the object; a name is a\nseparate thing.\nWe start the chapter by presenting a metaphor for variables in Python: variables are\nlabels, not boxes. If reference variables are old news to you, the analogy may still be\nhandy if you need to explain aliasing issues to others.\nWe then discuss the concepts of object identity, value, and aliasing. A surprising trait\nof tuples is revealed: they are immutable but their values may change. This leads to a\ndiscussion of shallow and deep copies. References and function parameters are our\nnext theme: the problem with mutable parameter defaults and the safe handling of\nmutable arguments passed by clients of our functions.\nThe last sections of the chapter cover garbage collection, the del command, and a\nselection of tricks that Python plays with immutable objects.\nThis is a rather dry chapter, but its topics lie at the heart of many subtle bugs in real\nPython programs.\n201",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "1 Lynn Andrea Stein is an award-winning computer science educator who currently teaches at Olin College of\nEngineering.\nWhat’s New in This Chapter\nThe topics covered here are very fundamental and stable. There were no changes\nworth mentioning in this second edition.\nI added an example of using is to test for a sentinel object, and a warning about\nmisuses of the is operator at the end of “Choosing Between == and is” on page 206.\nThis chapter used to be in Part IV, but I decided to bring it up earlier because it\nworks better as an ending to Part II, “Data Structures,” than an opening to “Object-\nOriented Idioms.”\nThe section on “Weak References” from the first edition of this\nbook is now a post at fluentpython.com.\nLet’s start by unlearning that a variable is like a box where you store data.\nVariables Are Not Boxes\nIn 1997, I took a summer course on Java at MIT. The professor, Lynn Stein,1 made\nthe point that the usual “variables as boxes” metaphor actually hinders the under‐\nstanding of reference variables in object-oriented languages. Python variables are like\nreference variables in Java; a better metaphor is to think of variables as labels with\nnames attached to objects. The next example and figure will help you understand\nwhy.\nExample 6-1 is a simple interaction that the “variables as boxes” idea cannot explain.\nFigure 6-1 illustrates why the box metaphor is wrong for Python, while sticky notes\nprovide a helpful picture of how variables actually work.\nExample 6-1. Variables a and b hold references to the same list, not copies of the list\n>>> a = [1, 2, 3]  \n>>> b = a          \n>>> a.append(4)    \n>>> b              \n[1, 2, 3, 4]\n202 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Create a list [1, 2, 3] and bind the variable a to it.\nBind the variable b to the same value that a is referencing.\nModify the list referenced by a, by appending another item.\nYou can see the effect via the b variable. If we think of b as a box that stored a\ncopy of the [1, 2, 3] from the a box, this behavior makes no sense.\nFigure 6-1. If you imagine variables are like boxes, you can’t make sense of assignment\nin Python; instead, think of variables as sticky notes, and Example 6-1 becomes easy to\nexplain.\nTherefore, the b = a statement does not copy the contents of box a into box b. It\nattaches the label b to the object that already has the label a.\nProf. Stein also spoke about assignment in a very deliberate way. For example, when\ntalking about a seesaw object in a simulation, she would say: “Variable s is assigned to\nthe seesaw,” but never “The seesaw is assigned to variable s.” With reference vari‐\nables, it makes much more sense to say that the variable is assigned to an object, and\nnot the other way around. After all, the object is created before the assignment.\nExample 6-2 proves that the righthand side of an assignment happens first.\nSince the verb “to assign” is used in contradictory ways, a useful alternative is “to\nbind”: Python’s assignment statement x = … binds the x name to the object created\nor referenced on the righthand side. And the object must exist before a name can be\nbound to it, as Example 6-2 proves.\nExample 6-2. Variables are bound to objects only after the objects are created\n>>> class Gizmo:\n...    def __init__(self):\n...         print(f'Gizmo id: {id(self)}')\n...\n>>> x = Gizmo()\nVariables Are Not Boxes \n| \n203",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "Gizmo id: 4301489152  \n>>> y = Gizmo() * 10  \nGizmo id: 4301489432  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for *: 'Gizmo' and 'int'\n>>>\n>>> dir()  \n['Gizmo', '__builtins__', '__doc__', '__loader__', '__name__',\n'__package__', '__spec__', 'x']\nThe output Gizmo id: … is a side effect of creating a Gizmo instance.\nMultiplying a Gizmo instance will raise an exception.\nHere is proof that a second Gizmo was actually instantiated before the multiplica‐\ntion was attempted.\nBut variable y was never created, because the exception happened while the\nrighthand side of the assignment was being evaluated.\nTo understand an assignment in Python, read the righthand side\nfirst: that’s where the object is created or retrieved. After that, the\nvariable on the left is bound to the object, like a label stuck to it.\nJust forget about the boxes.\nBecause variables are mere labels, nothing prevents an object from having several\nlabels assigned to it. When that happens, you have aliasing, our next topic.\nIdentity, Equality, and Aliases\nLewis Carroll is the pen name of Prof. Charles Lutwidge Dodgson. Mr. Carroll is not\nonly equal to Prof. Dodgson, they are one and the same. Example 6-3 expresses this\nidea in Python.\nExample 6-3. charles and lewis refer to the same object\n>>> charles = {'name': 'Charles L. Dodgson', 'born': 1832}\n>>> lewis = charles  \n>>> lewis is charles\nTrue\n>>> id(charles), id(lewis)  \n(4300473992, 4300473992)\n>>> lewis['balance'] = 950  \n204 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": ">>> charles\n{'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950}\nlewis is an alias for charles.\nThe is operator and the id function confirm it.\nAdding an item to lewis is the same as adding an item to charles.\nHowever, suppose an impostor—let’s call him Dr. Alexander Pedachenko—claims he\nis Charles L. Dodgson, born in 1832. His credentials may be the same, but Dr. Peda‐\nchenko is not Prof. Dodgson. Figure 6-2 illustrates this scenario.\nFigure 6-2. charles and lewis are bound to the same object; alex is bound to a sepa‐\nrate object of equal value.\nExample 6-4 implements and tests the alex object depicted in Figure 6-2.\nExample 6-4. alex and charles compare equal, but alex is not charles\n>>> alex = {'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950}  \n>>> alex == charles  \nTrue\n>>> alex is not charles  \nTrue\nalex refers to an object that is a replica of the object assigned to charles.\nThe objects compare equal because of the __eq__ implementation in the dict\nclass.\nBut they are distinct objects. This is the Pythonic way of writing the negative\nidentity comparison: a is not b.\nExample 6-3 is an example of aliasing. In that code, lewis and charles are aliases:\ntwo variables bound to the same object. On the other hand, alex is not an alias for\nIdentity, Equality, and Aliases \n| \n205",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "charles: these variables are bound to distinct objects. The objects bound to alex and\ncharles have the same value—that’s what == compares—but they have different\nidentities.\nIn The Python Language Reference, “3.1. Objects, values and types” states:\nAn object’s identity never changes once it has been created; you may think of it as the\nobject’s address in memory. The is operator compares the identity of two objects; the\nid() function returns an integer representing its identity.\nThe real meaning of an object’s ID is implementation dependent. In CPython, id()\nreturns the memory address of the object, but it may be something else in another\nPython interpreter. The key point is that the ID is guaranteed to be a unique integer\nlabel, and it will never change during the life of the object.\nIn practice, we rarely use the id() function while programming. Identity checks are\nmost often done with the is operator, which compares the object IDs, so our code\ndoesn’t need to call id() explicitly. Next, we’ll talk about is versus ==.\nFor tech reviewer Leonardo Rochael, the most frequent use for\nid() is while debugging, when the repr() of two objects look alike,\nbut you need to understand whether two references are aliases or\npoint to separate objects. If the references are in different contexts\n—such as different stack frames—using the is operator may not be\nviable.\nChoosing Between == and is\nThe == operator compares the values of objects (the data they hold), while is com‐\npares their identities.\nWhile programming, we often care more about values than object identities, so ==\nappears more frequently than is in Python code.\nHowever, if you are comparing a variable to a singleton, then it makes sense to use\nis. By far, the most common case is checking whether a variable is bound to None.\nThis is the recommended way to do it:\nx is None\nAnd the proper way to write its negation is:\nx is not None\nNone is the most common singleton we test with is. Sentinel objects are another\nexample of singletons we test with is. Here is one way to create and test a sentinel\nobject:\n206 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 2146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "2 In contrast, flat sequences like str, bytes, and array.array don’t contain references but directly hold their\ncontents—characters, bytes, and numbers—in contiguous memory.\nEND_OF_DATA = object()\n# ... many lines\ndef traverse(...):\n    # ... more lines\n    if node is END_OF_DATA:\n        return\n    # etc.\nThe is operator is faster than ==, because it cannot be overloaded, so Python does\nnot have to find and invoke special methods to evaluate it, and computing is as sim‐\nple as comparing two integer IDs. In contrast, a == b is syntactic sugar for\na.__eq__(b). The __eq__ method inherited from object compares object IDs, so it\nproduces the same result as is. But most built-in types override __eq__ with more\nmeaningful implementations that actually take into account the values of the object\nattributes. Equality may involve a lot of processing—for example, when comparing\nlarge collections or deeply nested structures.\nUsually we are more interested in object equality than identity.\nChecking for None is the only common use case for the is operator.\nMost other uses I see while reviewing code are wrong. If you are\nnot sure, use ==. It’s usually what you want, and also works with\nNone—albeit not as fast.\nTo wrap up this discussion of identity versus equality, we’ll see that the famously\nimmutable tuple is not as unchanging as you may expect.\nThe Relative Immutability of Tuples\nTuples, like most Python collections—lists, dicts, sets, etc.—are containers: they hold\nreferences to objects.2 If the referenced items are mutable, they may change even if\nthe tuple itself does not. In other words, the immutability of tuples really refers to the\nphysical contents of the tuple data structure (i.e., the references it holds), and does\nnot extend to the referenced objects.\nExample 6-5 illustrates the situation in which the value of a tuple changes as a result\nof changes to a mutable object referenced in it. What can never change in a tuple is\nthe identity of the items it contains.\nIdentity, Equality, and Aliases \n| \n207",
      "content_length": 2032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "Example 6-5. t1 and t2 initially compare equal, but changing a mutable item inside\ntuple t1 makes it different\n>>> t1 = (1, 2, [30, 40])  \n>>> t2 = (1, 2, [30, 40])  \n>>> t1 == t2  \nTrue\n>>> id(t1[-1])  \n4302515784\n>>> t1[-1].append(99)  \n>>> t1\n(1, 2, [30, 40, 99])\n>>> id(t1[-1])  \n4302515784\n>>> t1 == t2  \nFalse\nt1 is immutable, but t1[-1] is mutable.\nBuild a tuple t2 whose items are equal to those of t1.\nAlthough distinct objects, t1 and t2 compare equal, as expected.\nInspect the identity of the list at t1[-1].\nModify the t1[-1] list in place.\nThe identity of t1[-1] has not changed, only its value.\nt1 and t2 are now different.\nThis relative immutability of tuples is behind the riddle “A += Assignment Puzzler”\non page 54. It’s also the reason why some tuples are unhashable, as we’ve seen in\n“What Is Hashable” on page 84.\nThe distinction between equality and identity has further implications when you\nneed to copy an object. A copy is an equal object with a different ID. But if an object\ncontains other objects, should the copy also duplicate the inner objects, or is it OK to\nshare them? There’s no single answer. Read on for a discussion.\nCopies Are Shallow by Default\nThe easiest way to copy a list (or most built-in mutable collections) is to use the built-\nin constructor for the type itself. For example:\n>>> l1 = [3, [55, 44], (7, 8, 9)]\n>>> l2 = list(l1)  \n208 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": ">>> l2\n[3, [55, 44], (7, 8, 9)]\n>>> l2 == l1  \nTrue\n>>> l2 is l1  \nFalse\nlist(l1) creates a copy of l1.\nThe copies are equal…\n…but refer to two different objects.\nFor lists and other mutable sequences, the shortcut l2 = l1[:] also makes a copy.\nHowever, using the constructor or [:] produces a shallow copy (i.e., the outermost\ncontainer is duplicated, but the copy is filled with references to the same items held\nby the original container). This saves memory and causes no problems if all the items\nare immutable. But if there are mutable items, this may lead to unpleasant surprises.\nIn Example 6-6, we create a shallow copy of a list containing another list and a tuple,\nand then make changes to see how they affect the referenced objects.\nIf you have a connected computer on hand, I highly recommend\nwatching the interactive animation for Example 6-6 at the Online\nPython Tutor. As I write this, direct linking to a prepared example\nat pythontutor.com is not working reliably, but the tool is awesome,\nso taking the time to copy and paste the code is worthwhile.\nExample 6-6. Making a shallow copy of a list containing another list; copy and paste\nthis code to see it animated at the Online Python Tutor\nl1 = [3, [66, 55, 44], (7, 8, 9)]\nl2 = list(l1)      \nl1.append(100)     \nl1[1].remove(55)   \nprint('l1:', l1)\nprint('l2:', l2)\nl2[1] += [33, 22]  \nl2[2] += (10, 11)  \nprint('l1:', l1)\nprint('l2:', l2)\nl2 is a shallow copy of l1. This state is depicted in Figure 6-3.\nAppending 100 to l1 has no effect on l2.\nCopies Are Shallow by Default \n| \n209",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "Here we remove 55 from the inner list l1[1]. This affects l2 because l2[1] is\nbound to the same list as l1[1].\nFor a mutable object like the list referred by l2[1], the operator += changes the\nlist in place. This change is visible at l1[1], which is an alias for l2[1].\n+= on a tuple creates a new tuple and rebinds the variable l2[2] here. This is the\nsame as doing l2[2] = l2[2] + (10, 11). Now the tuples in the last position of\nl1 and l2 are no longer the same object. See Figure 6-4.\nFigure 6-3. Program state immediately after the assignment l2 = list(l1) in\nExample 6-6. l1 and l2 refer to distinct lists, but the lists share references to the same\ninner list object [66, 55, 44] and tuple (7, 8, 9). (Diagram generated by the\nOnline Python Tutor.)\nThe output of Example 6-6 is Example 6-7, and the final state of the objects is depic‐\nted in Figure 6-4.\nExample 6-7. Output of Example 6-6\nl1: [3, [66, 44], (7, 8, 9), 100]\nl2: [3, [66, 44], (7, 8, 9)]\nl1: [3, [66, 44, 33, 22], (7, 8, 9), 100]\nl2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)]\n210 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "Figure 6-4. Final state of l1 and l2: they still share references to the same list object,\nnow containing [66, 44, 33, 22], but the operation l2[2] += (10, 11) created a\nnew tuple with content (7, 8, 9, 10, 11), unrelated to the tuple (7, 8, 9) refer‐\nenced by l1[2]. (Diagram generated by the Online Python Tutor.)\nIt should be clear now that shallow copies are easy to make, but they may or may not\nbe what you want. How to make deep copies is our next topic.\nDeep and Shallow Copies of Arbitrary Objects\nWorking with shallow copies is not always a problem, but sometimes you need to\nmake deep copies (i.e., duplicates that do not share references of embedded objects).\nThe copy module provides the deepcopy and copy functions that return deep and\nshallow copies of arbitrary objects.\nTo illustrate the use of copy() and deepcopy(), Example 6-8 defines a simple class,\nBus, representing a school bus that is loaded with passengers and then picks up or\ndrops off passengers on its route.\nExample 6-8. Bus picks up and drops off passengers\nclass Bus:\n    def __init__(self, passengers=None):\n        if passengers is None:\n            self.passengers = []\n        else:\n            self.passengers = list(passengers)\n    def pick(self, name):\n        self.passengers.append(name)\nCopies Are Shallow by Default \n| \n211",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "def drop(self, name):\n        self.passengers.remove(name)\nNow, in the interactive Example 6-9, we will create a bus object (bus1) and two\nclones—a shallow copy (bus2) and a deep copy (bus3)—to observe what happens as\nbus1 drops off a student.\nExample 6-9. Effects of using copy versus deepcopy\n>>> import copy\n>>> bus1 = Bus(['Alice', 'Bill', 'Claire', 'David'])\n>>> bus2 = copy.copy(bus1)\n>>> bus3 = copy.deepcopy(bus1)\n>>> id(bus1), id(bus2), id(bus3)\n(4301498296, 4301499416, 4301499752)  \n>>> bus1.drop('Bill')\n>>> bus2.passengers\n['Alice', 'Claire', 'David']          \n>>> id(bus1.passengers), id(bus2.passengers), id(bus3.passengers)\n(4302658568, 4302658568, 4302657800)  \n>>> bus3.passengers\n['Alice', 'Bill', 'Claire', 'David']  \nUsing copy and deepcopy, we create three distinct Bus instances.\nAfter bus1 drops 'Bill', he is also missing from bus2.\nInspection of the passengers attributes shows that bus1 and bus2 share the same\nlist object, because bus2 is a shallow copy of bus1.\nbus3 is a deep copy of bus1, so its passengers attribute refers to another list.\nNote that making deep copies is not a simple matter in the general case. Objects may\nhave cyclic references that would cause a naïve algorithm to enter an infinite loop.\nThe deepcopy function remembers the objects already copied to handle cyclic refer‐\nences gracefully. This is demonstrated in Example 6-10.\nExample 6-10. Cyclic references: b refers to a, and then is appended to a; deepcopy still\nmanages to copy a\n>>> a = [10, 20]\n>>> b = [a, 30]\n>>> a.append(b)\n>>> a\n[10, 20, [[...], 30]]\n>>> from copy import deepcopy\n212 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": ">>> c = deepcopy(a)\n>>> c\n[10, 20, [[...], 30]]\nAlso, a deep copy may be too deep in some cases. For example, objects may refer\nto external resources or singletons that should not be copied. You can control the\nbehavior of both copy and deepcopy by implementing the __copy__() and\n__deepcopy__() special methods, as described in the copy module documentation.\nThe sharing of objects through aliases also explains how parameter passing works in\nPython, and the problem of using mutable types as parameter defaults. These issues\nwill be covered next.\nFunction Parameters as References\nThe only mode of parameter passing in Python is call by sharing. That is the same\nmode used in most object-oriented languages, including JavaScript, Ruby, and Java\n(this applies to Java reference types; primitive types use call by value). Call by sharing\nmeans that each formal parameter of the function gets a copy of each reference in the\narguments. In other words, the parameters inside the function become aliases of the\nactual arguments.\nThe result of this scheme is that a function may change any mutable object passed as\na parameter, but it cannot change the identity of those objects (i.e., it cannot alto‐\ngether replace an object with another). Example 6-11 shows a simple function using\n+= on one of its parameters. As we pass numbers, lists, and tuples to the function, the\nactual arguments passed are affected in different ways.\nExample 6-11. A function may change any mutable object it receives\n>>> def f(a, b):\n...     a += b\n...     return a\n...\n>>> x = 1\n>>> y = 2\n>>> f(x, y)\n3\n>>> x, y  \n(1, 2)\n>>> a = [1, 2]\n>>> b = [3, 4]\n>>> f(a, b)\n[1, 2, 3, 4]\n>>> a, b  \n([1, 2, 3, 4], [3, 4])\n>>> t = (10, 20)\nFunction Parameters as References \n| \n213",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": ">>> u = (30, 40)\n>>> f(t, u)  \n(10, 20, 30, 40)\n>>> t, u\n((10, 20), (30, 40))\nThe number x is unchanged.\nThe list a is changed.\nThe tuple t is unchanged.\nAnother issue related to function parameters is the use of mutable values for defaults,\nas discussed next.\nMutable Types as Parameter Defaults: Bad Idea\nOptional parameters with default values are a great feature of Python function defini‐\ntions, allowing our APIs to evolve while remaining backward compatible. However,\nyou should avoid mutable objects as default values for parameters.\nTo illustrate this point, in Example 6-12, we take the Bus class from Example 6-8 and\nchange its __init__ method to create HauntedBus. Here we tried to be clever, and\ninstead of having a default value of passengers=None, we have passengers=[], thus\navoiding the if in the previous __init__. This “cleverness” gets us into trouble.\nExample 6-12. A simple class to illustrate the danger of a mutable default\nclass HauntedBus:\n    \"\"\"A bus model haunted by ghost passengers\"\"\"\n    def __init__(self, passengers=[]):  \n        self.passengers = passengers  \n    def pick(self, name):\n        self.passengers.append(name)  \n    def drop(self, name):\n        self.passengers.remove(name)\nWhen the passengers argument is not passed, this parameter is bound to the\ndefault list object, which is initially empty.\nThis assignment makes self.passengers an alias for passengers, which is itself\nan alias for the default list, when no passengers argument is given.\n214 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "When the methods .remove() and .append() are used with self.passengers,\nwe are actually mutating the default list, which is an attribute of the function\nobject.\nExample 6-13 shows the eerie behavior of the HauntedBus.\nExample 6-13. Buses haunted by ghost passengers\n>>> bus1 = HauntedBus(['Alice', 'Bill'])  \n>>> bus1.passengers\n['Alice', 'Bill']\n>>> bus1.pick('Charlie')\n>>> bus1.drop('Alice')\n>>> bus1.passengers  \n['Bill', 'Charlie']\n>>> bus2 = HauntedBus()  \n>>> bus2.pick('Carrie')\n>>> bus2.passengers\n['Carrie']\n>>> bus3 = HauntedBus()  \n>>> bus3.passengers  \n['Carrie']\n>>> bus3.pick('Dave')\n>>> bus2.passengers  \n['Carrie', 'Dave']\n>>> bus2.passengers is bus3.passengers  \nTrue\n>>> bus1.passengers  \n['Bill', 'Charlie']\nbus1 starts with a two-passenger list.\nSo far, so good: no surprises with bus1.\nbus2 starts empty, so the default empty list is assigned to self.passengers.\nbus3 also starts empty, again the default list is assigned.\nThe default is no longer empty!\nNow Dave, picked by bus3, appears in bus2.\nThe problem: bus2.passengers and bus3.passengers refer to the same list.\nBut bus1.passengers is a distinct list.\nFunction Parameters as References \n| \n215",
      "content_length": 1174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "The problem is that HauntedBus instances that don’t get an initial passenger list end\nup sharing the same passenger list among themselves.\nSuch bugs may be subtle. As Example 6-13 demonstrates, when a HauntedBus is\ninstantiated with passengers, it works as expected. Strange things happen only when a\nHauntedBus starts empty, because then self.passengers becomes an alias for the\ndefault value of the passengers parameter. The problem is that each default value is\nevaluated when the function is defined—i.e., usually when the module is loaded—and\nthe default values become attributes of the function object. So if a default value is\na mutable object, and you change it, the change will affect every future call of the\nfunction.\nAfter running the lines in Example 6-13, you can inspect the HauntedBus.__init__\nobject and see the ghost students haunting its __defaults__ attribute:\n>>> dir(HauntedBus.__init__)  # doctest: +ELLIPSIS\n['__annotations__', '__call__', ..., '__defaults__', ...]\n>>> HauntedBus.__init__.__defaults__\n(['Carrie', 'Dave'],)\nFinally, we can verify that bus2.passengers is an alias bound to the first element of\nthe HauntedBus.__init__.__defaults__ attribute:\n>>> HauntedBus.__init__.__defaults__[0] is bus2.passengers\nTrue\nThe issue with mutable defaults explains why None is commonly used as the default\nvalue for parameters that may receive mutable values. In Example 6-8, __init__\nchecks whether the passengers argument is None. If it is, self.passengers is bound\nto a new empty list. If passengers is not None, the correct implementation binds a\ncopy of that argument to self.passengers. The next section explains why copying\nthe argument is a good practice.\nDefensive Programming with Mutable Parameters\nWhen you are coding a function that receives a mutable parameter, you should care‐\nfully consider whether the caller expects the argument passed to be changed.\nFor example, if your function receives a dict and needs to modify it while processing\nit, should this side effect be visible outside of the function or not? Actually it depends\non the context. It’s really a matter of aligning the expectation of the coder of the func‐\ntion and that of the caller.\nThe last bus example in this chapter shows how a TwilightBus breaks expectations\nby sharing its passenger list with its clients. Before studying the implementation, see\nin Example 6-14 how the TwilightBus class works from the perspective of a client of\nthe class.\n216 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "3 See Principle of least astonishment in the English Wikipedia.\nExample 6-14. Passengers disappear when dropped by a TwilightBus\n>>> basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']  \n>>> bus = TwilightBus(basketball_team)  \n>>> bus.drop('Tina')  \n>>> bus.drop('Pat')\n>>> basketball_team  \n['Sue', 'Maya', 'Diana']\nbasketball_team holds five student names.\nA TwilightBus is loaded with the team.\nThe bus drops one student, then another.\nThe dropped passengers vanished from the basketball team!\nTwilightBus violates the “Principle of least astonishment,” a best practice of interface\ndesign.3 It surely is astonishing that when the bus drops a student, their name is\nremoved from the basketball team roster.\nExample 6-15 is the implementation TwilightBus and an explanation of the\nproblem.\nExample 6-15. A simple class to show the perils of mutating received arguments\nclass TwilightBus:\n    \"\"\"A bus model that makes passengers vanish\"\"\"\n    def __init__(self, passengers=None):\n        if passengers is None:\n            self.passengers = []  \n        else:\n            self.passengers = passengers  \n    def pick(self, name):\n        self.passengers.append(name)\n    def drop(self, name):\n        self.passengers.remove(name)  \nFunction Parameters as References \n| \n217",
      "content_length": 1281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "Here we are careful to create a new empty list when passengers is None.\nHowever, this assignment makes self.passengers an alias for passengers,\nwhich is itself an alias for the actual argument passed to __init__ (i.e., basket\nball_team in Example 6-14).\nWhen the methods .remove() and .append() are used with self.passengers,\nwe are actually mutating the original list received as an argument to the con‐\nstructor.\nThe problem here is that the bus is aliasing the list that is passed to the constructor.\nInstead, it should keep its own passenger list. The fix is simple: in __init__, when the\npassengers parameter is provided, self.passengers should be initialized with a\ncopy of it, as we did correctly in Example 6-8:\n    def __init__(self, passengers=None):\n        if passengers is None:\n            self.passengers = []\n        else:\n            self.passengers = list(passengers) \nMake a copy of the passengers list, or convert it to a list if it’s not one.\nNow our internal handling of the passenger list will not affect the argument used to\ninitialize the bus. As a bonus, this solution is more flexible: now the argument passed\nto the passengers parameter may be a tuple or any other iterable, like a set or even\ndatabase results, because the list constructor accepts any iterable. As we create our\nown list to manage, we ensure that it supports the necessary .remove()\nand .append() operations we use in the .pick() and .drop() methods.\nUnless a method is explicitly intended to mutate an object received\nas an argument, you should think twice before aliasing the argu‐\nment object by simply assigning it to an instance variable in your\nclass. If in doubt, make a copy. Your clients will be happier. Of\ncourse, making a copy is not free: there is a cost in CPU and mem‐\nory. However, an API that causes subtle bugs is usually a bigger\nproblem than one that is a little slower or uses more resources.\nNow let’s talk about one of the most misunderstood of Python’s statements: del.\n218 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 2053,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "del and Garbage Collection\nObjects are never explicitly destroyed; however, when they become unreachable they\nmay be garbage-collected.\n—“Data Model” chapter of The Python Language Reference\nThe first strange fact about del is that it’s not a function, it’s a statement. We write\ndel x and not del(x)—although the latter also works, but only because the expres‐\nsions x and (x) usually mean the same thing in Python.\nThe second surprising fact is that del deletes references, not objects. Python’s\ngarbage collector may discard an object from memory as an indirect result of del, if\nthe deleted variable was the last reference to the object. Rebinding a variable may also\ncause the number of references to an object to reach zero, causing its destruction.\n>>> a = [1, 2]  \n>>> b = a       \n>>> del a       \n>>> b           \n[1, 2]\n>>> b = [3]     \nCreate object [1, 2] and bind a to it.\nBind b to the same [1, 2] object.\nDelete reference a.\n[1, 2] was not affected, because b still points to it.\nRebinding b to a different object removes the last remaining reference to [1, 2].\nNow the garbage collector can discard that object.\nThere is a __del__ special method, but it does not cause the dis‐\nposal of the instance, and should not be called by your code.\n__del__ is invoked by the Python interpreter when the instance is\nabout to be destroyed to give it a chance to release external\nresources. You will seldom need to implement __del__ in your\nown code, yet some Python programmers spend time coding it for\nno good reason. The proper use of __del__ is rather tricky. See the\n__del__ special method documentation in the “Data Model” chap‐\nter of The Python Language Reference.\nIn CPython, the primary algorithm for garbage collection is reference counting.\nEssentially, each object keeps count of how many references point to it. As soon as\ndel and Garbage Collection \n| \n219",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "that refcount reaches zero, the object is immediately destroyed: CPython calls the\n__del__ method on the object (if defined) and then frees the memory allocated to the\nobject. In CPython 2.0, a generational garbage collection algorithm was added to\ndetect groups of objects involved in reference cycles—which may be unreachable\neven with outstanding references to them, when all the mutual references are con‐\ntained within the group. Other implementations of Python have more sophisticated\ngarbage collectors that do not rely on reference counting, which means the __del__\nmethod may not be called immediately when there are no more references to the\nobject. See “PyPy, Garbage Collection, and a Deadlock” by A. Jesse Jiryu Davis for\ndiscussion of improper and proper use of __del__.\nTo demonstrate the end of an object’s life, Example 6-16 uses weakref.finalize to\nregister a callback function to be called when an object is destroyed.\nExample 6-16. Watching the end of an object when no more references point to it\n>>> import weakref\n>>> s1 = {1, 2, 3}\n>>> s2 = s1         \n>>> def bye():      \n...     print('...like tears in the rain.')\n...\n>>> ender = weakref.finalize(s1, bye)  \n>>> ender.alive  \nTrue\n>>> del s1\n>>> ender.alive  \nTrue\n>>> s2 = 'spam'  \n...like tears in the rain.\n>>> ender.alive\nFalse\ns1 and s2 are aliases referring to the same set, {1, 2, 3}.\nThis function must not be a bound method of the object about to be destroyed or\notherwise hold a reference to it.\nRegister the bye callback on the object referred by s1.\nThe .alive attribute is True before the finalize object is called.\nAs discussed, del did not delete the object, just the s1 reference to it.\nRebinding the last reference, s2, makes {1, 2, 3} unreachable. It is destroyed,\nthe bye callback is invoked, and ender.alive becomes False.\n220 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "4 This is clearly documented. Type help(tuple) in the Python console to read: “If the argument is a tuple, the\nreturn value is the same object.” I thought I knew everything about tuples before writing this book.\nThe point of Example 6-16 is to make explicit that del does not delete objects, but\nobjects may be deleted as a consequence of being unreachable after del is used.\nYou may be wondering why the {1, 2, 3} object was destroyed in Example 6-16.\nAfter all, the s1 reference was passed to the finalize function, which must have held\non to it in order to monitor the object and invoke the callback. This works because\nfinalize holds a weak reference to {1, 2, 3}. Weak references to an object do not\nincrease its reference count. Therefore, a weak reference does not prevent the target\nobject from being garbage collected. Weak references are useful in caching applica‐\ntions because you don’t want the cached objects to be kept alive just because they are\nreferenced by the cache.\nWeak references is a very specialized topic. That’s why I chose to\nskip it in this second edition. Instead, I published “Weak Refer‐\nences” on fluentpython.com.\nTricks Python Plays with Immutables\nThis optional section discusses some Python details that are not\nreally important for users of Python, and that may not apply to\nother Python implementations or even future versions of CPython.\nNevertheless, I’ve seen people stumble upon these corner cases and\nthen start using the is operator incorrectly, so I felt they were\nworth mentioning.\nI was surprised to learn that, for a tuple t, t[:] does not make a copy, but returns a\nreference to the same object. You also get a reference to the same tuple if you write\ntuple(t).4 Example 6-17 proves it.\nExample 6-17. A tuple built from another is actually the same exact tuple\n>>> t1 = (1, 2, 3)\n>>> t2 = tuple(t1)\n>>> t2 is t1  \nTrue\n>>> t3 = t1[:]\n>>> t3 is t1  \nTrue\nTricks Python Plays with Immutables \n| \n221",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "5 The harmless lie of having the copy method not copying anything is justified by interface compatibility: it\nmakes frozenset more compatible with set. Anyway, it makes no difference to the end user whether two\nidentical immutable objects are the same or are copies.\nt1 and t2 are bound to the same object.\nAnd so is t3.\nThe same behavior can be observed with instances of str, bytes, and frozenset.\nNote that a frozenset is not a sequence, so fs[:] does not work if fs is a frozenset.\nBut fs.copy() has the same effect: it cheats and returns a reference to the same\nobject, and not a copy at all, as Example 6-18 shows.5\nExample 6-18. String literals may create shared objects\n>>> t1 = (1, 2, 3)\n>>> t3 = (1, 2, 3)  \n>>> t3 is t1  \nFalse\n>>> s1 = 'ABC'\n>>> s2 = 'ABC'  \n>>> s2 is s1 \nTrue\nCreating a new tuple from scratch.\nt1 and t3 are equal, but not the same object.\nCreating a second str from scratch.\nSurprise: a and b refer to the same str!\nThe sharing of string literals is an optimization technique called interning. CPython\nuses a similar technique with small integers to avoid unnecessary duplication of num‐\nbers that appear frequently in programs like 0, 1, –1, etc. Note that CPython does not\nintern all strings or integers, and the criteria it uses to do so is an undocumented\nimplementation detail.\nNever depend on str or int interning! Always use == instead of is\nto compare strings or integers for equality. Interning is an optimi‐\nzation for internal use of the Python interpreter.\n222 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "6 A terrible use for this information would be to ask about it when interviewing candidates or authoring ques‐\ntions for “certification” exams. There are countless more important and useful facts to check for Python\nknowledge.\n7 Actually the type of an object may be changed by merely assigning a different class to its __class__ attribute,\nbut that is pure evil and I regret writing this footnote.\nThe tricks discussed in this section, including the behavior of frozenset.copy(),\nare harmless “lies” that save memory and make the interpreter faster. Do not worry\nabout them, they should not give you any trouble because they only apply to immuta‐\nble types. Probably the best use of these bits of trivia is to win bets with fellow\nPythonistas.6\nChapter Summary\nEvery Python object has an identity, a type, and a value. Only the value of an object\nmay change over time.7\nIf two variables refer to immutable objects that have equal values (a == b is True), in\npractice it rarely matters if they refer to copies or are aliases referring to the same\nobject, because the value of an immutable object does not change, with one excep‐\ntion. The exception being immutable collections such as tuples: if an immutable col‐\nlection holds references to mutable items, then its value may actually change when\nthe value of a mutable item changes. In practice, this scenario is not so common.\nWhat never changes in an immutable collection are the identities of the objects\nwithin. The frozenset class does not suffer from this problem because it can only\nhold hashable elements, and the value of hashable objects cannot ever change, by\ndefinition.\nThe fact that variables hold references has many practical consequences in Python\nprogramming:\n• Simple assignment does not create copies.\n• Augmented assignment with += or *= creates new objects if the lefthand variable\nis bound to an immutable object, but may modify a mutable object in place.\n• Assigning a new value to an existing variable does not change the object previ‐\nously bound to it. This is called a rebinding: the variable is now bound to a differ‐\nent object. If that variable was the last reference to the previous object, that object\nwill be garbage collected.\nChapter Summary \n| \n223",
      "content_length": 2236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "• Function parameters are passed as aliases, which means the function may change\nany mutable object received as an argument. There is no way to prevent this,\nexcept making local copies or using immutable objects (e.g., passing a tuple\ninstead of a list).\n• Using mutable objects as default values for function parameters is dangerous\nbecause if the parameters are changed in place, then the default is changed,\naffecting every future call that relies on the default.\nIn CPython, objects are discarded as soon as the number of references to them rea‐\nches zero. They may also be discarded if they form groups with cyclic references but\nnot outside references.\nIn some situations, it may be useful to hold a reference to an object that will not—by\nitself—keep an object alive. One example is a class that wants to keep track of all its\ncurrent instances. This can be done with weak references, a low-level mechanism\nunderlying the more useful collections WeakValueDictionary, WeakKeyDictionary,\nWeakSet, and the finalize function from the weakref module. For more on this,\nplease see “Weak References” at fluentpython.com.\nFurther Reading\nThe “Data Model” chapter of The Python Language Reference starts with a clear\nexplanation of object identities and values.\nWesley Chun, author of the Core Python series of books, presented Understanding\nPython’s Memory Model, Mutability, and Methods at EuroPython 2011, covering not\nonly the theme of this chapter but also the use of special methods.\nDoug Hellmann wrote the posts “copy – Duplicate Objects” and “weakref—Garbage-\nCollectable References to Objects” covering some of the topics we just discussed.\nMore information on the CPython generational garbage collector can be found in the\ngc module documentation, which starts with the sentence “This module provides an\ninterface to the optional garbage collector.” The “optional” qualifier here may be sur‐\nprising, but the “Data Model” chapter also states:\nAn implementation is allowed to postpone garbage collection or omit it altogether—it\nis a matter of implementation quality how garbage collection is implemented, as long\nas no objects are collected that are still reachable.\n224 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 2239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "Pablo Galindo wrote more in-depth treatment of Python’s GC in “Design of CPy‐\nthon’s Garbage Collector” in the Python Developer’s Guide, aimed at new and experi‐\nenced contributors to the CPython implementation.\nThe CPython 3.4 garbage collector improved handling of objects with a __del__\nmethod, as described in PEP 442—Safe object finalization.\nWikipedia has an article about string interning, mentioning the use of this technique\nin several languages, including Python.\nWikipedia also as an article on “Haddocks’ Eyes”, the Lewis Carroll song I quoted at\nthe top of this chapter. The Wikipedia editors wrote that the lyrics are used in works\non logic and philosophy “to elaborate on the symbolic status of the concept of name:\na name as identification marker may be assigned to anything, including another\nname, thus introducing different levels of symbolization.”\nSoapbox\nEqual Treatment to All Objects\nI learned Java before I discovered Python. The == operator in Java never felt right to\nme. It is much more common for programmers to care about equality than identity,\nbut for objects (not primitive types), the Java == compares references, and not object\nvalues. Even for something as basic as comparing strings, Java forces you to use\nthe .equals method. Even then, there is another catch: if you write a.equals(b) and\na is null, you get a null pointer exception. The Java designers felt the need to over‐\nload + for strings, so why not go ahead and overload == as well?\nPython gets this right. The == operator compares object values; is compares refer‐\nences. And because Python has operator overloading, == works sensibly with all\nobjects in the standard library, including None, which is a proper object, unlike Java’s\nnull.\nAnd of course, you can define __eq__ in your own classes to decide what == means\nfor your instances. If you don’t override __eq__, the method inherited from object\ncompares object IDs, so the fallback is that every instance of a user-defined class is\nconsidered different.\nThese are some of the things that made me switch from Java to Python as soon as I\nfinished reading The Python Tutorial one afternoon in September 1998.\nMutability\nThis chapter would not be necessary if all Python objects were immutable. When you\nare dealing with unchanging objects, it makes no difference whether variables hold\nthe actual objects or references to shared objects. If a == b is true, and neither object\nFurther Reading \n| \n225",
      "content_length": 2452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "can change, they might as well be the same. That’s why string interning is safe. Object\nidentity becomes important only when objects are mutable.\nIn “pure” functional programming, all data is immutable: appending to a collection\nactually creates a new collection. Elixir is one easy to learn, practical functional lan‐\nguage in which all built-in types are immutable, including lists.\nPython, however, is not a functional language, much less a pure one. Instances of\nuser-defined classes are mutable by default in Python—as in most object-oriented\nlanguages. When creating your own objects, you have to be extra careful to make\nthem immutable, if that is a requirement. Every attribute of the object must also be\nimmutable, otherwise you end up with something like the tuple: immutable as far as\nobject IDs go, but the value of a tuple may change if it holds a mutable object.\nMutable objects are also the main reason why programming with threads is so hard\nto get right: threads mutating objects without proper synchronization produce cor‐\nrupted data. Excessive synchronization, on the other hand, causes deadlocks. The\nErlang language and platform—which includes Elixir—was designed to maximize\nuptime in highly concurrent, distributed applications such as telecommunications\nswitches. Naturally, they chose immutable data by default.\nObject Destruction and Garbage Collection\nThere is no mechanism in Python to directly destroy an object, and this omission is\nactually a great feature: if you could destroy an object at any time, what would hap‐\npen to existing references pointing to it?\nGarbage collection in CPython is done primarily by reference counting, which is easy\nto implement, but is prone to memory leaking when there are reference cycles, so\nwith version 2.0 (October 2000) a generational garbage collector was implemented,\nand it is able to dispose of unreachable objects kept alive by reference cycles.\nBut the reference counting is still there as a baseline, and it causes the immediate dis‐\nposal of objects with zero references. This means that, in CPython—at least for now\n—it’s safe to write this:\nopen('test.txt', 'wt', encoding='utf-8').write('1, 2, 3')\nThat code is safe because the reference count of the file object will be zero after the\nwrite method returns, and Python will immediately close the file before destroying\nthe object representing it in memory. However, the same line is not safe in Jython or\nIronPython that use the garbage collector of their host runtimes (the Java VM and\nthe .NET CLR), which are more sophisticated but do not rely on reference counting\nand may take longer to destroy the object and close the file. In all cases, including\nCPython, the best practice is to explicitly close the file, and the most reliable way of\ndoing it is using the with statement, which guarantees that the file will be closed even\nif exceptions are raised while it is open. Using with, the previous snippet becomes:\n226 \n| \nChapter 6: Object References, Mutability, and Recycling",
      "content_length": 3013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "with open('test.txt', 'wt', encoding='utf-8') as fp:\n    fp.write('1, 2, 3')\nIf you are into the subject of garbage collectors, you may want to read Thomas Perl’s\npaper “Python Garbage Collector Implementations: CPython, PyPy and GaS”, from\nwhich I learned the bit about the safety of the open().write() in CPython.\nParameter Passing: Call by Sharing\nA popular way of explaining how parameter passing works in Python is the phrase:\n“Parameters are passed by value, but the values are references.” This is not wrong, but\ncauses confusion because the most common parameter passing modes in older lan‐\nguages are call by value (the function gets a copy of the argument) and call by refer‐\nence (the function gets a pointer to the argument). In Python, the function gets a copy\nof the arguments, but the arguments are always references. So the value of the refer‐\nenced objects may be changed, if they are mutable, but their identity cannot. Also,\nbecause the function gets a copy of the reference in an argument, rebinding it in the\nfunction body has no effect outside of the function. I adopted the term call by sharing\nafter reading up on the subject in Programming Language Pragmatics, 3rd ed., by\nMichael L. Scott (Morgan Kaufmann), section “8.3.1: Parameter Modes.”\nFurther Reading \n| \n227",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "PART Il\n\nFunctions as Objects",
      "content_length": 29,
      "extraction_method": "OCR"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "1 “Origins of Python’s ‘Functional’ Features”, from Guido’s The History of Python blog.\nCHAPTER 7\nFunctions as First-Class Objects\nI have never considered Python to be heavily influenced by functional languages, no\nmatter what people say or think. I was much more familiar with imperative languages\nsuch as C and Algol 68 and although I had made functions first-class objects, I didn’t\nview Python as a functional programming language.\n— Guido van Rossum, Python BDFL1\nFunctions in Python are first-class objects. Programming language researchers define\na “first-class object” as a program entity that can be:\n• Created at runtime\n• Assigned to a variable or element in a data structure\n• Passed as an argument to a function\n• Returned as the result of a function\nIntegers, strings, and dictionaries are other examples of first-class objects in Python\n—nothing fancy here. Having functions as first-class objects is an essential feature of\nfunctional languages, such as Clojure, Elixir, and Haskell. However, first-class func‐\ntions are so useful that they’ve been adopted by popular languages like JavaScript, Go,\nand Java (since JDK 8), none of which claim to be “functional languages.”\nThis chapter and most of Part III explore the practical applications of treating func‐\ntions as objects.\n231",
      "content_length": 1297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "The term “first-class functions” is widely used as shorthand for\n“functions as first-class objects.” It’s not ideal because it implies an\n“elite” among functions. In Python, all functions are first-class.\nWhat’s New in This Chapter\nThe section “The Nine Flavors of Callable Objects” on page 237 was titled “The Seven\nFlavors of Callable Objects” in the first edition of this book. The new callables are\nnative coroutines and asynchronous generators, introduced in Python 3.5 and 3.6,\nrespectively. Both are covered in Chapter 21, but they are mentioned here along with\nthe other callables for completeness.\n“Positional-Only Parameters” on page 242 is a new section, covering a feature added\nin Python 3.8.\nI moved the discussion of runtime access to function annotations to “Reading Type\nHints at Runtime” on page 537. When I wrote the first edition, PEP 484—Type Hints\nwas still under consideration, and people used annotations in different ways. Since\nPython 3.5, annotations should conform to PEP 484. Therefore, the best place to\ncover them is when discussing type hints.\nThe first edition of this book had sections about the introspection\nof function objects that were too low-level and distracted from the\nmain subject of this chapter. I merged those sections into a post\ntitled “Introspection of Function Parameters” at fluentpython.com.\nNow let’s see why Python functions are full-fledged objects.\nTreating a Function Like an Object\nThe console session in Example 7-1 shows that Python functions are objects. Here we\ncreate a function, call it, read its __doc__ attribute, and check that the function object\nitself is an instance of the function class.\nExample 7-1. Create and test a function, then read its __doc__ and check its type\n>>> def factorial(n):  \n...     \"\"\"returns n!\"\"\"\n...     return 1 if n < 2 else n * factorial(n - 1)\n...\n>>> factorial(42)\n1405006117752879898543142606244511569936384000000000\n>>> factorial.__doc__  \n232 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "'returns n!'\n>>> type(factorial)  \n<class 'function'>\nThis is a console session, so we’re creating a function at “runtime.”\n__doc__ is one of several attributes of function objects.\nfactorial is an instance of the function class.\nThe __doc__ attribute is used to generate the help text of an object. In the Python\nconsole, the command help(factorial) will display a screen like Figure 7-1.\nFigure 7-1. Help screen for factorial; the text is built from the __doc__ attribute of\nthe function.\nExample 7-2 shows the “first class” nature of a function object. We can assign it a\nvariable fact and call it through that name. We can also pass factorial as an argu‐\nment to the map function. Calling map(function, iterable) returns an iterable\nwhere each item is the result of calling the first argument (a function) to successive\nelements of the second argument (an iterable), range(10) in this example.\nExample 7-2. Use factorial through a different name, and pass factorial as an\nargument\n>>> fact = factorial\n>>> fact\n<function factorial at 0x...>\n>>> fact(5)\n120\n>>> map(factorial, range(11))\n<map object at 0x...>\n>>> list(map(factorial, range(11)))\n[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]\nTreating a Function Like an Object \n| \n233",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Having first-class functions enables programming in a functional style. One of the\nhallmarks of functional programming is the use of higher-order functions, our next\ntopic.\nHigher-Order Functions\nA function that takes a function as an argument or returns a function as the result is a\nhigher-order function. One example is map, shown in Example 7-2. Another is the\nbuilt-in function sorted: the optional key argument lets you provide a function to be\napplied to each item for sorting, as we saw in “list.sort Versus the sorted Built-In” on\npage 56. For example, to sort a list of words by length, pass the len function as the\nkey, as in Example 7-3.\nExample 7-3. Sorting a list of words by length\n>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']\n>>> sorted(fruits, key=len)\n['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry']\n>>>\nAny one-argument function can be used as the key. For example, to create a rhyme\ndictionary it might be useful to sort each word spelled backward. In Example 7-4,\nnote that the words in the list are not changed at all; only their reversed spelling is\nused as the sort criterion, so that the berries appear together.\nExample 7-4. Sorting a list of words by their reversed spelling\n>>> def reverse(word):\n...     return word[::-1]\n>>> reverse('testing')\n'gnitset'\n>>> sorted(fruits, key=reverse)\n['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']\n>>>\nIn the functional programming paradigm, some of the best known higher-order\nfunctions are map, filter, reduce, and apply. The apply function was deprecated in\nPython 2.3 and removed in Python 3 because it’s no longer necessary. If you need to\ncall a function with a dynamic set of arguments, you can write fn(*args, **kwargs)\ninstead of apply(fn, args, kwargs).\nThe map, filter, and reduce higher-order functions are still around, but better alter‐\nnatives are available for most of their use cases, as the next section shows.\n234 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "Modern Replacements for map, filter, and reduce\nFunctional languages commonly offer the map, filter, and reduce higher-order\nfunctions (sometimes with different names). The map and filter functions are still\nbuilt-ins in Python 3, but since the introduction of list comprehensions and genera‐\ntor expressions, they are not as important. A listcomp or a genexp does the job of map\nand filter combined, but is more readable. Consider Example 7-5.\nExample 7-5. Lists of factorials produced with map and filter compared to\nalternatives coded as list comprehensions\n>>> list(map(factorial, range(6)))  \n[1, 1, 2, 6, 24, 120]\n>>> [factorial(n) for n in range(6)]  \n[1, 1, 2, 6, 24, 120]\n>>> list(map(factorial, filter(lambda n: n % 2, range(6))))  \n[1, 6, 120]\n>>> [factorial(n) for n in range(6) if n % 2]  \n[1, 6, 120]\n>>>\nBuild a list of factorials from 0! to 5!.\nSame operation, with a list comprehension.\nList of factorials of odd numbers up to 5!, using both map and filter.\nList comprehension does the same job, replacing map and filter, and making\nlambda unnecessary.\nIn Python 3, map and filter return generators—a form of iterator—so their direct\nsubstitute is now a generator expression (in Python 2, these functions returned lists,\ntherefore their closest alternative was a listcomp).\nThe reduce function was demoted from a built-in in Python 2 to the functools\nmodule in Python 3. Its most common use case, summation, is better served by the\nsum built-in available since Python 2.3 was released in 2003. This is a big win in terms\nof readability and performance (see Example 7-6).\nExample 7-6. Sum of integers up to 99 performed with reduce and sum\n>>> from functools import reduce  \n>>> from operator import add  \n>>> reduce(add, range(100))  \n4950\n>>> sum(range(100))  \nHigher-Order Functions \n| \n235",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "4950\n>>>\nStarting with Python 3.0, reduce is no longer a built-in.\nImport add to avoid creating a function just to add two numbers.\nSum integers up to 99.\nSame task with sum—no need to import and call reduce and add.\nThe common idea of sum and reduce is to apply some operation to\nsuccessive items in a series, accumulating previous results, thus\nreducing a series of values to a single value.\nOther reducing built-ins are all and any:\nall(iterable)\nReturns True if there are no falsy elements in the iterable; all([]) returns True.\nany(iterable)\nReturns True if any element of the iterable is truthy; any([]) returns False.\nI give a fuller explanation of reduce in “Vector Take #4: Hashing and a Faster ==” on\npage 411 where an ongoing example provides a meaningful context for the use of this\nfunction. The reducing functions are summarized later in the book when iterables are\nin focus, in “Iterable Reducing Functions” on page 630.\nTo use a higher-order function, sometimes it is convenient to create a small, . one-off\nfunction. That is why anonymous functions exist. We’ll cover them next.\nAnonymous Functions\nThe lambda keyword creates an anonymous function within a Python expression.\nHowever, the simple syntax of Python limits the body of lambda functions to be pure\nexpressions. In other words, the body cannot contain other Python statements such\nas while, try, etc. Assignment with = is also a statement, so it cannot occur in a\nlambda. The new assignment expression syntax using := can be used—but if you need\nit, your lambda is probably too complicated and hard to read, and it should be refac‐\ntored into a regular function using def.\n236 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "The best use of anonymous functions is in the context of an argument list for a\nhigher-order function. For example, Example 7-7 is the rhyme index example from\nExample 7-4 rewritten with lambda, without defining a reverse function.\nExample 7-7. Sorting a list of words by their reversed spelling using lambda\n>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']\n>>> sorted(fruits, key=lambda word: word[::-1])\n['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']\n>>>\nOutside the limited context of arguments to higher-order functions, anonymous\nfunctions are rarely useful in Python. The syntactic restrictions tend to make nontriv‐\nial lambdas either unreadable or unworkable. If a lambda is hard to read, I strongly\nadvise you follow Fredrik Lundh’s refactoring advice.\nFredrik Lundh’s lambda Refactoring Recipe\nIf you find a piece of code hard to understand because of a lambda, Fredrik Lundh\nsuggests this refactoring procedure:\n1. Write a comment explaining what the heck that lambda does.\n2. Study the comment for a while, and think of a name that captures the essence of\nthe comment.\n3. Convert the lambda to a def statement, using that name.\n4. Remove the comment.\nThese steps are quoted from the “Functional Programming HOWTO”, a must read.\nThe lambda syntax is just syntactic sugar: a lambda expression creates a function\nobject just like the def statement. That is just one of several kinds of callable objects\nin Python. The following section reviews all of them.\nThe Nine Flavors of Callable Objects\nThe call operator () may be applied to other objects besides functions. To determine\nwhether an object is callable, use the callable() built-in function. As of Python 3.9,\nthe data model documentation lists nine callable types:\nThe Nine Flavors of Callable Objects \n| \n237",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "2 Calling a class usually creates an instance of that same class, but other behaviors are possible by overriding\n__new__. We’ll see an example of this in “Flexible Object Creation with __new__” on page 843.\nUser-defined functions\nCreated with def statements or lambda expressions.\nBuilt-in functions\nA function implemented in C (for CPython), like len or time.strftime.\nBuilt-in methods\nMethods implemented in C, like dict.get.\nMethods\nFunctions defined in the body of a class.\nClasses\nWhen invoked, a class runs its __new__ method to create an instance, then\n__init__ to initialize it, and finally the instance is returned to the caller. Because\nthere is no new operator in Python, calling a class is like calling a function.2\nClass instances\nIf a class defines a __call__ method, then its instances may be invoked as func‐\ntions—that’s the subject of the next section.\nGenerator functions\nFunctions or methods that use the yield keyword in their body. When called,\nthey return a generator object.\nNative coroutine functions\nFunctions or methods defined with async def. When called, they return a\ncoroutine object. Added in Python 3.5.\nAsynchronous generator functions\nFunctions or methods defined with async def that have yield in their body.\nWhen called, they return an asynchronous generator for use with async for.\nAdded in Python 3.6.\nGenerators, native coroutines, and asynchronous generator functions are unlike\nother callables in that their return values are never application data, but objects that\nrequire further processing to yield application data or perform useful work. Genera‐\ntor functions return iterators. Both are covered in Chapter 17. Native coroutine func‐\ntions and asynchronous generator functions return objects that only work with the\nhelp of an asynchronous programming framework, such as asyncio. They are the sub‐\nject of Chapter 21.\n238 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "3 Why build a BingoCage when we already have random.choice? The choice function may return the same\nitem multiple times, because the picked item is not removed from the collection given. Calling BingoCage\nnever returns duplicate results—as long as the instance is filled with unique values.\nGiven the variety of existing callable types in Python, the safest way\nto determine whether an object is callable is to use the callable()\nbuilt-in:\n>>> abs, str, 'Ni!'\n(<built-in function abs>, <class 'str'>, 'Ni!')\n>>> [callable(obj) for obj in (abs, str, 'Ni!')]\n[True, True, False]\nWe now move on to building class instances that work as callable objects.\nUser-Defined Callable Types\nNot only are Python functions real objects, but arbitrary Python objects may also be\nmade to behave like functions. Implementing a __call__ instance method is all it\ntakes.\nExample 7-8 implements a BingoCage class. An instance is built from any iterable,\nand stores an internal list of items, in random order. Calling the instance pops an\nitem.3\nExample 7-8. bingocall.py: A BingoCage does one thing: picks items from a shuffled list\nimport random\nclass BingoCage:\n    def __init__(self, items):\n        self._items = list(items)  \n        random.shuffle(self._items)  \n    def pick(self):  \n        try:\n            return self._items.pop()\n        except IndexError:\n            raise LookupError('pick from empty BingoCage')  \n    def __call__(self):  \n        return self.pick()\nUser-Defined Callable Types \n| \n239",
      "content_length": 1497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "__init__ accepts any iterable; building a local copy prevents unexpected side\neffects on any list passed as an argument.\nshuffle is guaranteed to work because self._items is a list.\nThe main method.\nRaise exception with custom message if self._items is empty.\nShortcut to bingo.pick(): bingo().\nHere is a simple demo of Example 7-8. Note how a bingo instance can be invoked as\na function, and the callable() built-in recognizes it as a callable object:\n>>> bingo = BingoCage(range(3))\n>>> bingo.pick()\n1\n>>> bingo()\n0\n>>> callable(bingo)\nTrue\nA class implementing __call__ is an easy way to create function-like objects that\nhave some internal state that must be kept across invocations, like the remaining\nitems in the BingoCage. Another good use case for __call__ is implementing decora‐\ntors. Decorators must be callable, and it is sometimes convenient to “remember”\nsomething between calls of the decorator (e.g., for memoization—caching the results\nof expensive computations for later use) or to split a complex implementation into\nseparate methods.\nThe functional approach to creating functions with internal state is to use closures.\nClosures, as well as decorators, are the subject of Chapter 9.\nNow let’s explore the powerful syntax Python offers to declare function parameters\nand pass arguments into them.\nFrom Positional to Keyword-Only Parameters\nOne of the best features of Python functions is the extremely flexible parameter han‐\ndling mechanism. Closely related are the use of * and ** to unpack iterables and\nmappings into separate arguments when we call a function. To see these features in\naction, see the code for Example 7-9 and tests showing its use in Example 7-10.\n240 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Example 7-9. tag generates HTML elements; a keyword-only argument class_ is used\nto pass “class” attributes as a workaround because class is a keyword in Python\ndef tag(name, *content, class_=None, **attrs):\n    \"\"\"Generate one or more HTML tags\"\"\"\n    if class_ is not None:\n        attrs['class'] = class_\n    attr_pairs = (f' {attr}=\"{value}\"' for attr, value\n                    in sorted(attrs.items()))\n    attr_str = ''.join(attr_pairs)\n    if content:\n        elements = (f'<{name}{attr_str}>{c}</{name}>'\n                    for c in content)\n        return '\\n'.join(elements)\n    else:\n        return f'<{name}{attr_str} />'\nThe tag function can be invoked in many ways, as Example 7-10 shows.\nExample 7-10. Some of the many ways of calling the tag function from Example 7-9\n>>> tag('br')  \n'<br />'\n>>> tag('p', 'hello')  \n'<p>hello</p>'\n>>> print(tag('p', 'hello', 'world'))\n<p>hello</p>\n<p>world</p>\n>>> tag('p', 'hello', id=33)  \n'<p id=\"33\">hello</p>'\n>>> print(tag('p', 'hello', 'world', class_='sidebar'))  \n<p class=\"sidebar\">hello</p>\n<p class=\"sidebar\">world</p>\n>>> tag(content='testing', name=\"img\")  \n'<img content=\"testing\" />'\n>>> my_tag = {'name': 'img', 'title': 'Sunset Boulevard',\n...           'src': 'sunset.jpg', 'class': 'framed'}\n>>> tag(**my_tag)  \n'<img class=\"framed\" src=\"sunset.jpg\" title=\"Sunset Boulevard\" />'\nA single positional argument produces an empty tag with that name.\nAny number of arguments after the first are captured by *content as a tuple.\nKeyword arguments not explicitly named in the tag signature are captured by\n**attrs as a dict.\nThe class_ parameter can only be passed as a keyword argument.\nFrom Positional to Keyword-Only Parameters \n| \n241",
      "content_length": 1704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "The first positional argument can also be passed as a keyword.\nPrefixing the my_tag dict with ** passes all its items as separate arguments,\nwhich are then bound to the named parameters, with the remaining caught by\n**attrs. In this case we can have a 'class' key in the arguments dict, because\nit is a string, and does not clash with the class reserved word.\nKeyword-only arguments are a feature of Python 3. In Example 7-9, the class_\nparameter can only be given as a keyword argument—it will never capture unnamed\npositional arguments. To specify keyword-only arguments when defining a function,\nname them after the argument prefixed with *. If you don’t want to support variable\npositional arguments but still want keyword-only arguments, put a * by itself in the\nsignature, like this:\n>>> def f(a, *, b):\n...     return a, b\n...\n>>> f(1, b=2)\n(1, 2)\n>>> f(1, 2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: f() takes 1 positional argument but 2 were given\nNote that keyword-only arguments do not need to have a default value: they can be\nmandatory, like b in the preceding example.\nPositional-Only Parameters\nSince Python 3.8, user-defined function signatures may specify positional-only\nparameters. This feature always existed for built-in functions, such as divmod(a, b),\nwhich can only be called with positional parameters, and not as divmod(a=10, b=4).\nTo define a function requiring positional-only parameters, use / in the parameter list.\nThis example from “What’s New In Python 3.8” shows how to emulate the divmod\nbuilt-in function:\ndef divmod(a, b, /):\n    return (a // b, a % b)\nAll arguments to the left of the / are positional-only. After the /, you may specify\nother arguments, which work as usual.\n242 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "The / in the parameter list is a syntax error in Python 3.7 or earlier.\nFor example, consider the tag function from Example 7-9. If we want the name\nparameter to be positional only, we can add a / after it in the function signature, like\nthis:\ndef tag(name, /, *content, class_=None, **attrs):\n    ...\nYou can find other examples of positional-only parameters in “What’s New In\nPython 3.8” and in PEP 570.\nAfter diving into Python’s flexible argument declaration features, the remainder of\nthis chapter covers the most useful packages in the standard library for programming\nin a functional style.\nPackages for Functional Programming\nAlthough Guido makes it clear that he did not design Python to be a functional pro‐\ngramming language, a functional coding style can be used to good extent, thanks to\nfirst-class functions, pattern matching, and the support of packages like operator\nand functools, which we cover in the next two sections.\nThe operator Module\nOften in functional programming it is convenient to use an arithmetic operator as a\nfunction. For example, suppose you want to multiply a sequence of numbers to calcu‐\nlate factorials without using recursion. To perform summation, you can use sum, but\nthere is no equivalent function for multiplication. You could use reduce—as we saw\nin “Modern Replacements for map, filter, and reduce” on page 235—but this requires\na function to multiply two items of the sequence. Example 7-11 shows how to solve\nthis using lambda.\nExample 7-11. Factorial implemented with reduce and an anonymous function\nfrom functools import reduce\ndef factorial(n):\n    return reduce(lambda a, b: a*b, range(1, n+1))\nPackages for Functional Programming \n| \n243",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "The operator module provides function equivalents for dozens of operators so you\ndon’t have to code trivial functions like lambda a, b: a*b. With it, we can rewrite\nExample 7-11 as Example 7-12.\nExample 7-12. Factorial implemented with reduce and operator.mul\nfrom functools import reduce\nfrom operator import mul\ndef factorial(n):\n    return reduce(mul, range(1, n+1))\nAnother group of one-trick lambdas that operator replaces are functions to pick\nitems from sequences or read attributes from objects: itemgetter and attrgetter\nare factories that build custom functions to do that.\nExample 7-13 shows a common use of itemgetter: sorting a list of tuples by the\nvalue of one field. In the example, the cities are printed sorted by country code (field\n1). Essentially, itemgetter(1) creates a function that, given a collection, returns the\nitem at index 1. That’s easier to write and read than lambda fields: fields[1],\nwhich does the same thing.\nExample 7-13. Demo of itemgetter to sort a list of tuples (data from Example 2-8)\n>>> metro_data = [\n...     ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),\n...     ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),\n...     ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),\n...     ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),\n...     ('São Paulo', 'BR', 19.649, (-23.547778, -46.635833)),\n... ]\n>>>\n>>> from operator import itemgetter\n>>> for city in sorted(metro_data, key=itemgetter(1)):\n...     print(city)\n...\n('São Paulo', 'BR', 19.649, (-23.547778, -46.635833))\n('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))\n('Tokyo', 'JP', 36.933, (35.689722, 139.691667))\n('Mexico City', 'MX', 20.142, (19.433333, -99.133333))\n('New York-Newark', 'US', 20.104, (40.808611, -74.020386))\nIf you pass multiple index arguments to itemgetter, the function it builds will return\ntuples with the extracted values, which is useful for sorting on multiple keys:\n>>> cc_name = itemgetter(1, 0)\n>>> for city in metro_data:\n...     print(cc_name(city))\n244 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 2062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "...\n('JP', 'Tokyo')\n('IN', 'Delhi NCR')\n('MX', 'Mexico City')\n('US', 'New York-Newark')\n('BR', 'São Paulo')\n>>>\nBecause itemgetter uses the [] operator, it supports not only sequences but also\nmappings and any class that implements __getitem__.\nA sibling of itemgetter is attrgetter, which creates functions to extract object\nattributes by name. If you pass attrgetter several attribute names as arguments, it\nalso returns a tuple of values. In addition, if any argument name contains a . (dot),\nattrgetter navigates through nested objects to retrieve the attribute. These behav‐\niors are shown in Example 7-14. This is not the shortest console session because we\nneed to build a nested structure to showcase the handling of dotted attributes by\nattrgetter.\nExample 7-14. Demo of attrgetter to process a previously defined list of namedtuple\ncalled metro_data (the same list that appears in Example 7-13)\n>>> from collections import namedtuple\n>>> LatLon = namedtuple('LatLon', 'lat lon')  \n>>> Metropolis = namedtuple('Metropolis', 'name cc pop coord')  \n>>> metro_areas = [Metropolis(name, cc, pop, LatLon(lat, lon))  \n...     for name, cc, pop, (lat, lon) in metro_data]\n>>> metro_areas[0]\nMetropolis(name='Tokyo', cc='JP', pop=36.933, coord=LatLon(lat=35.689722,\nlon=139.691667))\n>>> metro_areas[0].coord.lat  \n35.689722\n>>> from operator import attrgetter\n>>> name_lat = attrgetter('name', 'coord.lat')  \n>>>\n>>> for city in sorted(metro_areas, key=attrgetter('coord.lat')):  \n...     print(name_lat(city))  \n...\n('São Paulo', -23.547778)\n('Mexico City', 19.433333)\n('Delhi NCR', 28.613889)\n('Tokyo', 35.689722)\n('New York-Newark', 40.808611)\nUse namedtuple to define LatLon.\nAlso define Metropolis.\nPackages for Functional Programming \n| \n245",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "Build metro_areas list with Metropolis instances; note the nested tuple unpack‐\ning to extract (lat, lon) and use them to build the LatLon for the coord\nattribute of Metropolis.\nReach into element metro_areas[0] to get its latitude.\nDefine an attrgetter to retrieve the name and the coord.lat nested attribute.\nUse attrgetter again to sort list of cities by latitude.\nUse the attrgetter defined in  to show only the city name and latitude.\nHere is a partial list of functions defined in operator (names starting with _ are omit‐\nted, because they are mostly implementation details):\n>>> [name for name in dir(operator) if not name.startswith('_')]\n['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains',\n'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt',\n'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imatmul',\n'imod', 'imul', 'index', 'indexOf', 'inv', 'invert', 'ior',\n'ipow', 'irshift', 'is_', 'is_not', 'isub', 'itemgetter',\n'itruediv', 'ixor', 'le', 'length_hint', 'lshift', 'lt', 'matmul',\n'methodcaller', 'mod', 'mul', 'ne', 'neg', 'not_', 'or_', 'pos',\n'pow', 'rshift', 'setitem', 'sub', 'truediv', 'truth', 'xor']\nMost of the 54 names listed are self-evident. The group of names prefixed with i and\nthe name of another operator—e.g., iadd, iand, etc.—correspond to the augmented\nassignment operators—e.g., +=, &=, etc. These change their first argument in place, if\nit is mutable; if not, the function works like the one without the i prefix: it simply\nreturns the result of the operation.\nOf the remaining operator functions, methodcaller is the last we will cover. It is\nsomewhat similar to attrgetter and itemgetter in that it creates a function on the\nfly. The function it creates calls a method by name on the object given as argument,\nas shown in Example 7-15.\nExample 7-15. Demo of methodcaller: second test shows the binding of extra\narguments\n>>> from operator import methodcaller\n>>> s = 'The time has come'\n>>> upcase = methodcaller('upper')\n>>> upcase(s)\n'THE TIME HAS COME'\n>>> hyphenate = methodcaller('replace', ' ', '-')\n>>> hyphenate(s)\n'The-time-has-come'\n246 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "The first test in Example 7-15 is there just to show methodcaller at work, but if you\nneed to use the str.upper as a function, you can just call it on the str class and pass\na string as an argument, like this:\n>>> str.upper(s)\n'THE TIME HAS COME'\nThe second test in Example 7-15 shows that methodcaller can also do a partial appli‐\ncation to freeze some arguments, like the functools.partial function does. That is\nour next subject.*Bold Text*opmod07\nFreezing Arguments with functools.partial\nThe functools module provides several higher-order functions. We saw reduce in\n“Modern Replacements for map, filter, and reduce” on page 235. Another is partial:\ngiven a callable, it produces a new callable with some of the arguments of the original\ncallable bound to predetermined values. This is useful to adapt a function that takes\none or more arguments to an API that requires a callback with fewer arguments.\nExample 7-16 is a trivial demonstration.\nExample 7-16. Using partial to use a two-argument function where a one-argument\ncallable is required\n>>> from operator import mul\n>>> from functools import partial\n>>> triple = partial(mul, 3)  \n>>> triple(7)  \n21\n>>> list(map(triple, range(1, 10)))  \n[3, 6, 9, 12, 15, 18, 21, 24, 27]\nCreate new triple function from mul, binding the first positional argument to 3.\nTest it.\nUse triple with map; mul would not work with map in this example.\nA more useful example involves the unicode.normalize function that we saw in\n“Normalizing Unicode for Reliable Comparisons” on page 140. If you work with text\nfrom many languages, you may want to apply unicode.normalize('NFC', s) to any\nstring s before comparing or storing it. If you do that often, it’s handy to have an nfc\nfunction to do so, as in Example 7-17.\nPackages for Functional Programming \n| \n247",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Example 7-17. Building a convenient Unicode normalizing function with partial\n>>> import unicodedata, functools\n>>> nfc = functools.partial(unicodedata.normalize, 'NFC')\n>>> s1 = 'café'\n>>> s2 = 'cafe\\u0301'\n>>> s1, s2\n('café', 'café')\n>>> s1 == s2\nFalse\n>>> nfc(s1) == nfc(s2)\nTrue\npartial takes a callable as first argument, followed by an arbitrary number of posi‐\ntional and keyword arguments to bind.\nExample 7-18 shows the use of partial with the tag function from Example 7-9, to\nfreeze one positional argument and one keyword argument.\nExample 7-18. Demo of partial applied to the function tag from Example 7-9\n>>> from tagger import tag\n>>> tag\n<function tag at 0x10206d1e0>  \n>>> from functools import partial\n>>> picture = partial(tag, 'img', class_='pic-frame')  \n>>> picture(src='wumpus.jpeg')\n'<img class=\"pic-frame\" src=\"wumpus.jpeg\" />'  \n>>> picture\nfunctools.partial(<function tag at 0x10206d1e0>, 'img', class_='pic-frame')  \n>>> picture.func  \n<function tag at 0x10206d1e0>\n>>> picture.args\n('img',)\n>>> picture.keywords\n{'class_': 'pic-frame'}\nImport tag from Example 7-9 and show its ID.\nCreate the picture function from tag by fixing the first positional argument\nwith 'img' and the class_ keyword argument with 'pic-frame'.\npicture works as expected.\n248 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "4 The source code for functools.py reveals that functools.partial is implemented in C and is used by default.\nIf that is not available, a pure-Python implementation of partial is available since Python 3.4.\npartial() returns a functools.partial object.4\nA functools.partial object has attributes providing access to the original func‐\ntion and the fixed arguments.\nThe functools.partialmethod function does the same job as partial, but is\ndesigned to work with methods.\nThe functools module also includes higher-order functions designed to be used as\nfunction decorators, such as cache and singledispatch, among others. Those\nfunctions are covered in Chapter 9, which also explains how to implement custom\ndecorators.\nChapter Summary\nThe goal of this chapter was to explore the first-class nature of functions in Python.\nThe main ideas are that you can assign functions to variables, pass them to other\nfunctions, store them in data structures, and access function attributes, allowing\nframeworks and tools to act on that information.\nHigher-order functions, a staple of functional programming, are common in Python.\nThe sorted, min, and max built-ins, and functools.partial are examples of com‐\nmonly used higher-order functions in the language. Using map, filter, and reduce is\nnot as common as it used to be, thanks to list comprehensions (and similar con‐\nstructs like generator expressions) and the addition of reducing built-ins like sum,\nall, and any.\nCallables come in nine different flavors since Python 3.6, from the simple functions\ncreated with lambda to instances of classes implementing __call__. Generators and\ncoroutines are also callable, although their behavior is very different from other calla‐\nbles. All callables can be detected by the callable() built-in. Callables offer rich syn‐\ntax for declaring formal parameters, including keyword-only parameters, positional-\nonly parameters, and annotations.\nLastly, we covered some functions from the operator module and functools.par\ntial, which facilitate functional programming by minimizing the need for the func‐\ntionally challenged lambda syntax.\nChapter Summary \n| \n249",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "Further Reading\nThe next chapters continue our exploration of programming with function objects.\nChapter 8 is devoted to type hints in function parameters and return values. Chap‐\nter 9 dives into function decorators—a special kind of higher-order function—and\nthe closure mechanism that makes them work. Chapter 10 shows how first-class\nfunctions can simplify some classic object-oriented design patterns.\nIn The Python Language Reference, “3.2. The standard type hierarchy” presents the\nnine callable types, along with all the other built-in types.\nChapter 7 of the Python Cookbook, 3rd ed. (O’Reilly), by David Beazley and Brian K.\nJones, is an excellent complement to the current chapter as well as Chapter 9 of this\nbook, covering mostly the same concepts with a different approach.\nSee PEP 3102—Keyword-Only Arguments if you are interested in the rationale and\nuse cases for that feature.\nA great introduction to functional programming in Python is A. M. Kuchling’s\n“Python Functional Programming HOWTO”. The main focus of that text, however,\nis the use of iterators and generators, which are the subject of Chapter 17.\nThe StackOverflow question “Python: Why is functools.partial necessary?” has a\nhighly informative (and funny) reply by Alex Martelli, coauthor of the classic Python\nin a Nutshell (O’Reilly).\nReflecting on the question “Is Python a functional language?”, I created one of my\nfavorite talks, “Beyond Paradigms,” which I presented at PyCaribbean, PyBay, and\nPyConDE. See the slides and video from the Berlin presentation—where I met Miro‐\nslav Šedivý and Jürgen Gmach, two of the technical reviewers of this book.\nSoapbox\nIs Python a Functional Language?\nSometime in the year 2000 I attended a Zope workshop at Zope Corporation in the\nUnited States when Guido van Rossum dropped by the classroom (he was not the\ninstructor). In the Q&A that followed, somebody asked him which features of Python\nwere borrowed from other languages. Guido’s answer: “Everything that is good in\nPython was stolen from other languages.”\nShriram Krishnamurthi, professor of Computer Science at Brown University, starts\nhis “Teaching Programming Languages in a Post-Linnaean Age” paper with this:\nProgramming language “paradigms” are a moribund and tedious legacy of a bygone\nage. Modern language designers pay them no respect, so why do our courses slavishly\nadhere to them?\n250 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 2427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "5 There is also the problem of lost indentation when pasting code to web forums, but I digress.\nIn that paper, Python is mentioned by name in this passage:\nWhat else to make of a language like Python, Ruby, or Perl? Their designers have no\npatience for the niceties of these Linnaean hierarchies; they borrow features as they\nwish, creating mélanges that utterly defy characterization.\nKrishnamurthi argues that instead of trying to classify languages in some taxonomy,\nit’s more useful to consider them as aggregations of features. His ideas inspired my\ntalk “Beyond Paradigms,” mentioned at the end of “Further Reading” on page 250.\nEven if it was not Guido’s goal, endowing Python with first-class functions opened\nthe door to functional programming. In his post, “Origins of Python’s Functional\nFeatures”, he says that map, filter, and reduce were the motivation for adding\nlambda to Python in the first place. All of these features were contributed together by\nAmrit Prem for Python 1.0 in 1994, according to Misc/HISTORY in the CPython\nsource code.\nFunctions like map, filter, and reduce first appeared in Lisp, the original functional\nlanguage. However, Lisp does not limit what can be done inside a lambda, because\neverything in Lisp is an expression. Python uses a statement-oriented syntax in which\nexpressions cannot contain statements, and many language constructs are statements\n—including try/catch, which is what I miss most often when writing lambdas. This\nis the price to pay for Python’s highly readable syntax.5 Lisp has many strengths, but\nreadability is not one of them.\nIronically, stealing the list comprehension syntax from another functional language—\nHaskell—significantly diminished the need for map and filter, and also for lambda.\nBesides the limited anonymous function syntax, the biggest obstacle to wider adop‐\ntion of functional programming idioms in Python is the lack of tail-call elimination,\nan optimization that allows memory-efficient computation of a function that makes a\nrecursive call at the “tail” of its body. In another blog post, “Tail Recursion Elimina‐\ntion”, Guido gives several reasons why such optimization is not a good fit for Python.\nThat post is a great read for the technical arguments, but even more so because the\nfirst three and most important reasons given are usability issues. It is no accident that\nPython is a pleasure to use, learn, and teach. Guido made it so.\nSo there you have it: Python is not, by design, a functional language—whatever that\nmeans. Python just borrows a few good ideas from functional languages.\nThe Problem with Anonymous Functions\nBeyond the Python-specific syntax constraints, anonymous functions have a serious\ndrawback in any language: they have no name.\nFurther Reading \n| \n251",
      "content_length": 2770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "I am only half joking here. Stack traces are easier to read when functions have names.\nAnonymous functions are a handy shortcut, people have fun coding with them, but\nsometimes they get carried away—especially if the language and environment\nencourage deep nesting of anonymous functions, like JavaScript on Node.js do. Lots\nof nested anonymous functions make debugging and error handling hard. Asynchro‐\nnous programming in Python is more structured, perhaps because the limited lambda\nsyntax prevents its abuse and forces a more explicit approach. Promises, futures, and\ndeferreds are concepts used in modern asynchronous APIs. Along with coroutines,\nthey provide an escape from the so-called “callback hell.” I promise to write more\nabout asynchronous programming in the future, but this subject must be deferred to\nChapter 21.\n252 \n| \nChapter 7: Functions as First-Class Objects",
      "content_length": 882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "1 PEP 484—Type Hints, “Rationale and Goals”; bold emphasis retained from the original.\nCHAPTER 8\nType Hints in Functions\nIt should also be emphasized that Python will remain a dynamically typed language,\nand the authors have no desire to ever make type hints mandatory, even by\nconvention.\n—Guido van Rossum, Jukka Lehtosalo, and Łukasz Langa, PEP 484—Type Hints1\nType hints are the biggest change in the history of Python since the unification of\ntypes and classes in Python 2.2, released in 2001. However, type hints do not benefit\nall Python users equally. That’s why they should always be optional.\nPEP 484—Type Hints introduced syntax and semantics for explicit type declarations\nin function arguments, return values, and variables. The goal is to help developer\ntools find bugs in Python codebases via static analysis, i.e., without actually running\nthe code through tests.\nThe main beneficiaries are professional software engineers using IDEs (Integrated\nDevelopment Environments) and CI (Continuous Integration). The cost-benefit\nanalysis that makes type hints attractive to that group does not apply to all users of\nPython.\nPython’s user base is much wider than that. It includes scientists, traders, journalists,\nartists, makers, analysts, and students in many fields—among others. For most of\nthem, the cost of learning type hints is likely higher—unless they already know a\nlanguage with static types, subtyping, and generics. The benefits will be lower for\nmany of those users, given how they interact with Python, and the smaller size of\ntheir codebases and teams—often “teams” of one. Python’s default dynamic typing is\n253",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "simpler and more expressive when writing code for exploring data and ideas, as in\ndata science, creative computing, and learning,\nThis chapter focuses on Python’s type hints in function signatures. Chapter 15\nexplores type hints in the context of classes, and other typing module features.\nThe major topics in this chapter are:\n• A hands-on introduction to gradual typing with Mypy\n• The complementary perspectives of duck typing and nominal typing\n• Overview of the main categories of types that can appear in annotations—this is\nabout 60% of the chapter\n• Type hinting variadic parameters (*args, **kwargs)\n• Limitations and downsides of type hints and static typing\nWhat’s New in This Chapter\nThis chapter is completely new. Type hints appeared in Python 3.5 after I wrapped up\nthe first edition of Fluent Python.\nGiven the limitations of a static type system, the best idea of PEP 484 was to intro‐\nduce a gradual type system. Let’s begin by defining what that means.\nAbout Gradual Typing\nPEP 484 introduced a gradual type system to Python. Other languages with gradual\ntype systems are Microsoft’s TypeScript, Dart (the language of the Flutter SDK, cre‐\nated by Google), and Hack (a dialect of PHP supported by Facebook’s HHVM virtual\nmachine). The Mypy type checker itself started as a language: a gradually typed dia‐\nlect of Python with its own interpreter. Guido van Rossum convinced the creator of\nMypy, Jukka Lehtosalo, to make it a tool for checking annotated Python code.\nA gradual type system:\nIs optional\nBy default, the type checker should not emit warnings for code that has no type\nhints. Instead, the type checker assumes the Any type when it cannot determine\nthe type of an object. The Any type is considered compatible with all other types.\nDoes not catch type errors at runtime\nType hints are used by static type checkers, linters, and IDEs to raise warnings.\nThey do not prevent inconsistent values from being passed to functions or\nassigned to variables at runtime.\n254 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2032,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "2 A just-in-time compiler like the one in PyPy has much better data than type hints: it monitors the Python\nprogram as it runs, detects the concrete types in use, and generates optimized machine code for those\nconcrete types.\n3 For example, recursive types are not supported as of July 2021—see typing module issue #182, Define a JSON\ntype and Mypy issue #731, Support recursive types.\nDoes not enhance performance\nType annotations provide data that could, in theory, allow optimizations in the\ngenerated bytecode, but such optimizations are not implemented in any Python\nruntime that I am aware in of July 2021.2\nThe best usability feature of gradual typing is that annotations are always optional.\nWith static type systems, most type constraints are easy to express, many are cumber‐\nsome, some are hard, and a few are impossible.3 You may very well write an excellent\npiece of Python code, with good test coverage and passing tests, but still be unable to\nadd type hints that satisfy a type checker. That’s OK; just leave out the problematic\ntype hints and ship it!\nType hints are optional at all levels: you can have entire packages with no type hints,\nyou can silence the type checker when you import one of those packages into a mod‐\nule where you use type hints, and you can add special comments to make the type\nchecker ignore specific lines in your code.\nSeeking 100% coverage of type hints is likely to stimulate type hint‐\ning without proper thought, only to satisfy the metric. It will also\nprevent teams from making the most of the power and flexibility of\nPython. Code without type hints should naturally be accepted\nwhen annotations would make an API less user-friendly, or unduly\ncomplicate its implementation.\nGradual Typing in Practice\nLet’s see how gradual typing works in practice, starting with a simple function and\ngradually adding type hints to it, guided by Mypy.\nThere are several Python type checkers compatible with PEP 484,\nincluding Google’s pytype, Microsoft’s Pyright, Facebook’s Pyre—\nin addition to type checkers embedded in IDEs such as PyCharm. I\npicked Mypy for the examples because it’s the best known. How‐\never, one of the others may be a better fit for some projects or\nteams. Pytype, for example, is designed to handle codebases with\nno type hints and still provide useful advice. It is more lenient than\nMypy, and can also generate annotations for your code.\nGradual Typing in Practice \n| \n255",
      "content_length": 2436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "We will annotate a show_count function that returns a string with a count and a sin‐\ngular or plural word, depending on the count:\n>>> show_count(99, 'bird')\n'99 birds'\n>>> show_count(1, 'bird')\n'1 bird'\n>>> show_count(0, 'bird')\n'no birds'\nExample 8-1 shows the source code of show_count, without annotations.\nExample 8-1. show_count from messages.py without type hints\ndef show_count(count, word):\n    if count == 1:\n        return f'1 {word}'\n    count_str = str(count) if count else 'no'\n    return f'{count_str} {word}s'\nStarting with Mypy\nTo begin type checking, I run the mypy command on the messages.py module:\n…/no_hints/ $ pip install mypy\n[lots of messages omitted...]\n…/no_hints/ $ mypy messages.py\nSuccess: no issues found in 1 source file\nMypy with default settings finds no problem with Example 8-1.\nI am using Mypy 0.910, the most recent release as I review this in\nJuly 2021. The Mypy “Introduction” warns that it “is officially beta\nsoftware. There will be occasional changes that break backward\ncompatibility.” Mypy is giving me at least one report that is not the\nsame I got when I wrote this chapter in April 2020. By the time you\nread this, you may get different results than shown here.\nIf a function signature has no annotations, Mypy ignores it by default—unless config‐\nured otherwise.\nFor Example 8-2, I also have pytest unit tests. This is the code in messages_test.py.\nExample 8-2. messages_test.py without type hints\nfrom pytest import mark\nfrom messages import show_count\n256 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "@mark.parametrize('qty, expected', [\n    (1, '1 part'),\n    (2, '2 parts'),\n])\ndef test_show_count(qty, expected):\n    got = show_count(qty, 'part')\n    assert got == expected\ndef test_show_count_zero():\n    got = show_count(0, 'part')\n    assert got == 'no parts'\nNow let’s add type hints, guided by Mypy.\nMaking Mypy More Strict\nThe command-line option --disallow-untyped-defs makes Mypy flag any function\ndefinition that does not have type hints for all its parameters and for its return value.\nUsing --disallow-untyped-defs on the test file produces three errors and a note:\n…/no_hints/ $ mypy --disallow-untyped-defs messages_test.py\nmessages.py:14: error: Function is missing a type annotation\nmessages_test.py:10: error: Function is missing a type annotation\nmessages_test.py:15: error: Function is missing a return type annotation\nmessages_test.py:15: note: Use \"-> None\" if function does not return a value\nFound 3 errors in 2 files (checked 1 source file)\nFor the first steps with gradual typing, I prefer to use another option: --disallow-\nincomplete-defs. Initially, it tells me nothing:\n…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py\nSuccess: no issues found in 1 source file\nNow I can add just the return type to show_count in messages.py:\ndef show_count(count, word) -> str:\nThis is enough to make Mypy look at it. Using the same command line as before to\ncheck messages_test.py will lead Mypy to look at messages.py again:\n…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py\nmessages.py:14: error: Function is missing a type annotation\nfor one or more arguments\nFound 1 error in 1 file (checked 1 source file)\nNow I can gradually add type hints function by function, without getting warnings\nabout functions that I haven’t annotated. This is a fully annotated signature that satis‐\nfies Mypy:\ndef show_count(count: int, word: str) -> str:\nGradual Typing in Practice \n| \n257",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "Instead of typing command-line options like --disallow-\nincomplete-defs, you can save your favorite as described in the\nMypy configuration file documentation. You can have global set‐\ntings and per-module settings. Here is a simple mypy.ini to get\nstarted:\n[mypy]\npython_version = 3.9\nwarn_unused_configs = True\ndisallow_incomplete_defs = True\nA Default Parameter Value\nThe show_count function in Example 8-1 only works with regular nouns. If the plural\ncan’t be spelled by appending an 's', we should let the user provide the plural form,\nlike this:\n>>> show_count(3, 'mouse', 'mice')\n'3 mice'\nLet’s do a little “type-driven development.” First we add a test that uses that third\nargument. Don’t forget to add the return type hint to the test function, otherwise\nMypy will not check it.\ndef test_irregular() -> None:\n    got = show_count(2, 'child', 'children')\n    assert got == '2 children'\nMypy detects the error:\n…/hints_2/ $ mypy messages_test.py\nmessages_test.py:22: error: Too many arguments for \"show_count\"\nFound 1 error in 1 file (checked 1 source file)\nNow I edit show_count, adding the optional plural parameter in Example 8-3.\nExample 8-3. showcount from hints_2/messages.py with an optional parameter\ndef show_count(count: int, singular: str, plural: str = '') -> str:\n    if count == 1:\n        return f'1 {singular}'\n    count_str = str(count) if count else 'no'\n    if not plural:\n        plural = singular + 's'\n    return f'{count_str} {plural}'\nNow Mypy reports “Success.”\n258 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Here is one typing mistake that Python does not catch. Can you\nspot it?\ndef hex2rgb(color=str) -> tuple[int, int, int]:\nMypy’s error report is not very helpful:\ncolors.py:24: error: Function is missing a type\n    annotation for one or more arguments\nThe type hint for the color argument should be color: str. I\nwrote color=str, which is not an annotation: it sets the default\nvalue of color to str.\nIn my experience, it’s a common mistake and easy to overlook,\nespecially in complicated type hints.\nThe following details are considered good style for type hints:\n• No space between the parameter name and the :; one space after the :\n• Spaces on both sides of the = that precedes a default parameter value\nOn the other hand, PEP 8 says there should be no spaces around the = if there is no\ntype hint for that particular parameter.\nCode Style: Use flake8 and blue\nInstead of memorizing such silly rules, use tools like flake8 and blue. flake8 reports on\ncode styling, among many other issues, and blue rewrites source code according to\n(most) rules embedded in the black code formatting tool.\nGiven the goal of enforcing a “standard” coding style, blue is better than black\nbecause it follows Python’s own style of using single quotes by default, double quotes\nas an alternative:\n>>> \"I prefer single quotes\"\n'I prefer single quotes'\nThe preference for single quotes is embedded in repr(), among other places in CPy‐\nthon. The doctest module depends on repr() using single quotes by default.\nOne of the authors of blue is Barry Warsaw, coauthor of PEP 8, Python core devel‐\noper since 1994, and a member of Python’s Steering Council from 2019 to present\n(July 2021). We are in very good company when we choose single quotes by default.\nIf you must use black, use the black -S option. Then it will leave your quotes as they\nare.\nGradual Typing in Practice \n| \n259",
      "content_length": 1861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Using None as a Default\nIn Example 8-3, the parameter plural is annotated as str, and the default value is '',\nso there is no type conflict.\nI like that solution, but in other contexts None is a better default. If the optional\nparameter expects a mutable type, then None is the only sensible default—as we saw\nin “Mutable Types as Parameter Defaults: Bad Idea” on page 214.\nTo have None as the default for the plural parameter, here is what the signature\nwould look like:\nfrom typing import Optional\ndef show_count(count: int, singular: str, plural: Optional[str] = None) -> str:\nLet’s unpack that:\n• Optional[str] means plural may be a str or None.\n• You must explicitly provide the default value = None.\nIf you don’t assign a default value to plural, the Python runtime will treat it as a\nrequired parameter. Remember: at runtime, type hints are ignored.\nNote that we need to import Optional from the typing module. When importing\ntypes, it’s good practice to use the syntax from typing import X to reduce the length\nof the function signatures.\nOptional is not a great name, because that annotation does not\nmake the parameter optional. What makes it optional is assigning a\ndefault value to the parameter. Optional[str] just means: the type\nof this parameter may be str or NoneType. In the Haskell and Elm\nlanguages, a similar type is named Maybe.\nNow that we’ve had a first practical view of gradual typing, let’s consider what the\nconcept of type means in practice.\nTypes Are Defined by Supported Operations\nThere are many definitions of the concept of type in the literature. Here we assume\nthat type is a set of values and a set of functions that one can apply to these values.\n—PEP 483—The Theory of Type Hints\n260 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "4 Python doesn’t provide syntax to control the set of possible values for a type—except in Enum types. For exam‐\nple, using type hints you can’t define Quantity as an integer between 1 and 1000, or AirportCode as a 3-letter\ncombination. NumPy offers uint8, int16, and other machine-oriented numeric types, but in the Python\nstandard library we only have types with very small sets of values (NoneType, bool) or extremely large sets\n(float, int, str, all possible tuples, etc.).\nIn practice, it’s more useful to consider the set of supported operations as the defin‐\ning characteristic of a type.4\nFor example, from the point of view of applicable operations, what are the valid types\nfor x in the following function?\ndef double(x):\n    return x * 2\nThe x parameter type may be numeric (int, complex, Fraction, numpy.uint32, etc.)\nbut it may also be a sequence (str, tuple, list, array), an N-dimensional\nnumpy.array, or any other type that implements or inherits a __mul__ method that\naccepts an int argument.\nHowever, consider this annotated double. Please ignore the missing return type for\nnow, let’s focus on the parameter type:\nfrom collections import abc\ndef double(x: abc.Sequence):\n    return x * 2\nA type checker will reject that code. If you tell Mypy that x is of type abc.Sequence, it\nwill flag x * 2 as an error because the Sequence ABC does not implement or inherit\nthe __mul__ method. At runtime, that code will work with concrete sequences such as\nstr, tuple, list, array, etc., as well as numbers, because at runtime the type hints\nare ignored. But the type checker only cares about what is explicitly declared, and\nabc.Sequence has no __mul__.\nThat’s why the title of this section is “Types Are Defined by Supported Operations.”\nThe Python runtime accepts any object as the x argument for both versions of the\ndouble function. The computation x * 2 may work, or it may raise TypeError if the\noperation is not supported by x. In contrast, Mypy will declare x * 2 as wrong while\nanalyzing the annotated double source code, because it’s an unsupported operation\nfor the declared type: x: abc.Sequence.\nIn a gradual type system, we have the interplay of two different views of types:\nDuck typing\nThe view adopted by Smalltalk—the pioneering object-oriented language—as\nwell as Python, JavaScript, and Ruby. Objects have types, but variables (including\nTypes Are Defined by Supported Operations \n| \n261",
      "content_length": 2415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "5 Duck typing is an implicit form of structural typing, which Python ≥ 3.8 also supports with the introduction\nof typing.Protocol. This is covered later in this chapter—in “Static Protocols” on page 286—with more\ndetails in Chapter 13.\n6 Inheritance is often overused and hard to justify in examples that are realistic yet simple, so please accept this\nanimal example as a quick illustration of subtyping.\nparameters) are untyped. In practice, it doesn’t matter what the declared type of\nthe object is, only what operations it actually supports. If I can invoke\nbirdie.quack(), then birdie is a duck in this context. By definition, duck typ‐\ning is only enforced at runtime, when operations on objects are attempted. This\nis more flexible than nominal typing, at the cost of allowing more errors at run‐\ntime.5\nNominal typing\nThe view adopted by C++, Java, and C#, supported by annotated Python. Objects\nand variables have types. But objects only exist at runtime, and the type checker\nonly cares about the source code where variables (including parameters) are\nannotated with type hints. If Duck is a subclass of Bird, you can assign a Duck\ninstance to a parameter annotated as birdie: Bird. But in the body of the func‐\ntion, the type checker considers the call birdie.quack() illegal, because birdie\nis nominally a Bird, and that class does not provide the .quack() method. It\ndoesn’t matter if the actual argument at runtime is a Duck, because nominal typ‐\ning is enforced statically. The type checker doesn’t run any part of the program, it\nonly reads the source code. This is more rigid than duck typing, with the advan‐\ntage of catching some bugs earlier in a build pipeline, or even as the code is typed\nin an IDE.\nExample 8-4 is a silly example that contrasts duck typing and nominal typing, as well\nas static type checking and runtime behavior.6\nExample 8-4. birds.py\nclass Bird:\n    pass\nclass Duck(Bird):  \n    def quack(self):\n        print('Quack!')\ndef alert(birdie):  \n    birdie.quack()\ndef alert_duck(birdie: Duck) -> None:  \n    birdie.quack()\n262 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "def alert_bird(birdie: Bird) -> None:  \n    birdie.quack()\nDuck is a subclass of Bird.\nalert has no type hints, so the type checker ignores it.\nalert_duck takes one argument of type Duck.\nalert_bird takes one argument of type Bird.\nType checking birds.py with Mypy, we see a problem:\n…/birds/ $ mypy birds.py\nbirds.py:16: error: \"Bird\" has no attribute \"quack\"\nFound 1 error in 1 file (checked 1 source file)\nJust by analyzing the source code, Mypy sees that alert_bird is problematic: the type\nhint declares the birdie parameter with type Bird, but the body of the function calls\nbirdie.quack()—and the Bird class has no such method.\nNow let’s try to use the birds module in daffy.py in Example 8-5.\nExample 8-5. daffy.py\nfrom birds import *\ndaffy = Duck()\nalert(daffy)       \nalert_duck(daffy)  \nalert_bird(daffy)  \nValid call, because alert has no type hints.\nValid call, because alert_duck takes a Duck argument, and daffy is a Duck.\nValid call, because alert_bird takes a Bird argument, and daffy is also a Bird—\nthe superclass of Duck.\nRunning Mypy on daffy.py raises the same error about the quack call in the\nalert_bird function defined in birds.py:\n…/birds/ $ mypy daffy.py\nbirds.py:16: error: \"Bird\" has no attribute \"quack\"\nFound 1 error in 1 file (checked 1 source file)\nBut Mypy sees no problem with daffy.py itself: the three function calls are OK.\nNow, if you run daffy.py, this is what you get:\nTypes Are Defined by Supported Operations \n| \n263",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "…/birds/ $ python3 daffy.py\nQuack!\nQuack!\nQuack!\nEverything works! Duck typing FTW!\nAt runtime, Python doesn’t care about declared types. It uses duck typing only. Mypy\nflagged an error in alert_bird, but calling it with daffy works fine at runtime. This\nmay surprise many Pythonistas at first: a static type checker will sometimes find\nerrors in programs that we know will execute.\nHowever, if months from now you are tasked with extending the silly bird example,\nyou may be grateful for Mypy. Consider this woody.py module, which also uses\nbirds, in Example 8-6.\nExample 8-6. woody.py\nfrom birds import *\nwoody = Bird()\nalert(woody)\nalert_duck(woody)\nalert_bird(woody)\nMypy finds two errors while checking woody.py:\n…/birds/ $ mypy woody.py\nbirds.py:16: error: \"Bird\" has no attribute \"quack\"\nwoody.py:5: error: Argument 1 to \"alert_duck\" has incompatible type \"Bird\";\nexpected \"Duck\"\nFound 2 errors in 2 files (checked 1 source file)\nThe first error is in birds.py: the birdie.quack() call in alert_bird, which we’ve\nseen before. The second error is in woody.py: woody is an instance of Bird, so the call\nalert_duck(woody) is invalid because that function requires a Duck. Every Duck is a\nBird, but not every Bird is a Duck.\nAt runtime, none of the calls in woody.py succeed. The succession of failures is best\nillustrated in a console session with callouts in Example 8-7.\nExample 8-7. Runtime errors and how Mypy could have helped\n>>> from birds import *\n>>> woody = Bird()\n>>> alert(woody)  \nTraceback (most recent call last):\n  ...\nAttributeError: 'Bird' object has no attribute 'quack'\n>>>\n264 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": ">>> alert_duck(woody) \nTraceback (most recent call last):\n  ...\nAttributeError: 'Bird' object has no attribute 'quack'\n>>>\n>>> alert_bird(woody)  \nTraceback (most recent call last):\n  ...\nAttributeError: 'Bird' object has no attribute 'quack'\nMypy could not detect this error because there are no type hints in alert.\nMypy reported the problem: Argument 1 to \"alert_duck\" has incompatible\ntype \"Bird\"; expected \"Duck\".\nMypy has been telling us since Example 8-4 that the body of the alert_bird\nfunction is wrong: \"Bird\" has no attribute \"quack\".\nThis little experiment shows that duck typing is easier to get started and is more flexi‐\nble, but allows unsupported operations to cause errors at runtime. Nominal typing\ndetects errors before runtime, but sometimes can reject code that actually runs—such\nas the call alert_bird(daffy) in Example 8-5. Even if it sometimes works, the\nalert_bird function is misnamed: its body does require an object that supports\nthe .quack() method, which Bird doesn’t have.\nIn this silly example, the functions are one-liners. But in real code they could be\nlonger; they could pass the birdie argument to more functions, and the origin of the\nbirdie argument could be many function calls away, making it hard to pinpoint the\ncause of a runtime error. The type checker prevents many such errors from ever hap‐\npening at runtime.\nThe value of type hints is questionable in the tiny examples that fit\nin a book. The benefits grow with the size of the codebase. That’s\nwhy companies with millions of lines of Python code—like Drop‐\nbox, Google, and Facebook—invested in teams and tools to sup‐\nport the company-wide adoption of type hints, and have significant\nand increasing portions of their Python codebases type checked in\ntheir CI pipelines.\nIn this section we explored the relationship of types and operations in duck typing\nand nominal typing, starting with the simple double() function—which we left\nwithout proper type hints. Now we will tour the most important types used for anno‐\ntating functions. We’ll see a good way to add type hints to double() when we reach\n“Static Protocols” on page 286. But before we get to that, there are more fundamental\ntypes to know.\nTypes Are Defined by Supported Operations \n| \n265",
      "content_length": 2253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Types Usable in Annotations\nPretty much any Python type can be used in type hints, but there are restrictions and\nrecommendations. In addition, the typing module introduced special constructs\nwith semantics that are sometimes surprising.\nThis section covers all the major types you can use with annotations:\n• typing.Any\n• Simple types and classes\n• typing.Optional and typing.Union\n• Generic collections, including tuples and mappings\n• Abstract base classes\n• Generic iterables\n• Parameterized generics and TypeVar\n• typing.Protocols—the key to static duck typing\n• typing.Callable\n• typing.NoReturn—a good way to end this list\nWe’ll cover each of these in turn, starting with a type that is strange, apparently use‐\nless, but crucially important.\nThe Any Type\nThe keystone of any gradual type system is the Any type, also known as the dynamic\ntype. When a type checker sees an untyped function like this:\ndef double(x):\n    return x * 2\nit assumes this:\ndef double(x: Any) -> Any:\n    return x * 2\nThat means the x argument and the return value can be of any type, including differ‐\nent types. Any is assumed to support every possible operation.\nContrast Any with object. Consider this signature:\ndef double(x: object) -> object:\nThis function also accepts arguments of every type, because every type is a subtype-of\nobject.\n266 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "However, a type checker will reject this function:\ndef double(x: object) -> object:\n    return x * 2\nThe problem is that object does not support the __mul__ operation. This is what\nMypy reports:\n…/birds/ $ mypy double_object.py\ndouble_object.py:2: error: Unsupported operand types for * (\"object\" and \"int\")\nFound 1 error in 1 file (checked 1 source file)\nMore general types have narrower interfaces, i.e., they support fewer operations. The\nobject class implements fewer operations than abc.Sequence, which implements\nfewer operations than abc.MutableSequence, which implements fewer operations\nthan list.\nBut Any is a magic type that sits at the top and the bottom of the type hierarchy. It’s\nsimultaneously the most general type—so that an argument n: Any accepts values of\nevery type—and the most specialized type, supporting every possible operation. At\nleast, that’s how the type checker understands Any.\nOf course, no type can support every possible operation, so using Any prevents the\ntype checker from fulfilling its core mission: detecting potentially illegal operations\nbefore your program crashes with a runtime exception.\nSubtype-of versus consistent-with\nTraditional object-oriented nominal type systems rely on the is subtype-of relation‐\nship. Given a class T1 and a subclass T2, then T2 is subtype-of T1.\nConsider this code:\nclass T1:\n    ...\nclass T2(T1):\n    ...\ndef f1(p: T1) -> None:\n    ...\no2 = T2()\nf1(o2)  # OK\nTypes Usable in Annotations \n| \n267",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "7 MIT Professor, programming language designer, and Turing Award recipient. Wikipedia: Barbara Liskov.\nThe call f1(o2) is an application of the Liskov Substitution Principle—LSP. Barbara\nLiskov7 actually defined is subtype-of in terms of supported operations: if an object of\ntype T2 substitutes an object of type T1 and the program still behaves correctly, then\nT2 is subtype-of T1.\nContinuing from the previous code, this shows a violation of the LSP:\ndef f2(p: T2) -> None:\n    ...\no1 = T1()\nf2(o1)  # type error\nFrom the point of view of supported operations, this makes perfect sense: as a sub‐\nclass, T2 inherits and must support all operations that T1 does. So an instance of T2\ncan be used anywhere an instance of T1 is expected. But the reverse is not necessarily\ntrue: T2 may implement additional methods, so an instance of T1 may not be used\neverywhere an instance of T2 is expected. This focus on supported operations is\nreflected in the name behavioral subtyping, also used to refer to the LSP.\nIn a gradual type system, there is another relationship: consistent-with, which applies\nwherever subtype-of applies, with special provisions for type Any.\nThe rules for consistent-with are:\n1. Given T1 and a subtype T2, then T2 is consistent-with T1 (Liskov substitution).\n2. Every type is consistent-with Any: you can pass objects of every type to an argu‐\nment declared of type Any.\n3. Any is consistent-with every type: you can always pass an object of type Any where\nan argument of another type is expected.\nConsidering the previous definitions of the objects o1 and o2, here are examples of\nvalid code, illustrating rules #2 and #3:\ndef f3(p: Any) -> None:\n    ...\no0 = object()\no1 = T1()\no2 = T2()\nf3(o0)  #\nf3(o1)  #  all OK: rule #2\nf3(o2)  #\n268 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "def f4():  # implicit return type: `Any`\n    ...\no4 = f4()  # inferred type: `Any`\nf1(o4)  #\nf2(o4)  #  all OK: rule #3\nf3(o4)  #\nEvery gradual type system needs a wildcard type like Any.\nThe verb “to infer” is a fancy synomym for “to guess,” used in the\ncontext of type analysis. Modern type checkers in Python and other\nlanguages don’t require type annotations everywhere because they\ncan infer the type of many expressions. For example, if I write x =\nlen(s) * 10, the type checker doesn’t need an explicit local decla‐\nration to know that x is an int, as long as it can find type hints for\nthe len built-in.\nNow we can explore the rest of the types used in annotations.\nSimple Types and Classes\nSimple types like int, float, str, and bytes may be used directly in type hints. Con‐\ncrete classes from the standard library, external packages, or user defined—French\nDeck, Vector2d, and Duck—may also be used in type hints.\nAbstract base classes are also useful in type hints. We’ll get back to them as we study\ncollection types, and in “Abstract Base Classes” on page 278.\nAmong classes, consistent-with is defined like subtype-of: a subclass is consistent-with\nall its superclasses.\nHowever, “practicality beats purity,” so there is an important exception, which I dis‐\ncuss in the following tip.\nint Is Consistent-With complex\nThere is no nominal subtype relationship between the built-in\ntypes int, float, and complex: they are direct subclasses of object.\nBut PEP 484 declares that int is consistent-with float, and float\nis consistent-with complex. It makes sense in practice: int imple‐\nments all operations that float does, and int implements addi‐\ntional ones as well—bitwise operations like &, |, <<, etc. The end\nresult is: int is consistent-with complex. For i = 3, i.real is 3, and\ni.imag is 0.\nTypes Usable in Annotations \n| \n269",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "8 To be more precise, ord only accepts str or bytes with len(s) == 1. But the type system currently can’t\nexpress this constraint.\nOptional and Union Types\nWe saw the Optional special type in “Using None as a Default” on page 260. It solves\nthe problem of having None as a default, as in this example from that section:\nfrom typing import Optional\ndef show_count(count: int, singular: str, plural: Optional[str] = None) -> str:\nThe construct Optional[str] is actually a shortcut for Union[str, None], which\nmeans the type of plural may be str or None.\nBetter Syntax for Optional and Union in Python 3.10\nWe can write str | bytes instead of Union[str, bytes] since\nPython 3.10. It’s less typing, and there’s no need to import\nOptional or Union from typing. Contrast the old and new syntax\nfor the type hint of the plural parameter of show_count:\nplural: Optional[str] = None    # before\nplural: str | None = None       # after\nThe | operator also works with isinstance and issubclass to\nbuild the second argument: isinstance(x, int | str). For\nmore, see PEP 604—Complementary syntax for Union[].\nThe ord built-in function’s signature is a simple example of Union—it accepts str or\nbytes, and returns an int:8\ndef ord(c: Union[str, bytes]) -> int: ...\nHere is an example of a function that takes a str, but may return a str or a float:\nfrom typing import Union\ndef parse_token(token: str) -> Union[str, float]:\n    try:\n        return float(token)\n    except ValueError:\n        return token\nIf possible, avoid creating functions that return Union types, as they put an extra bur‐\nden on the user—forcing them to check the type of the returned value at runtime to\nknow what to do with it. But the parse_token in the preceding code is a reasonable\nuse case in the context of a simple expression evaluator.\n270 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "9 In ABC—the language that most influenced the initial design of Python—each list was constrained to accept\nvalues of a single type: the type of the first item you put into it.\nIn “Dual-Mode str and bytes APIs” on page 155, we saw functions\nthat accept either str or bytes arguments, but return str if the\nargument was str or bytes if the arguments was bytes. In those\ncases, the return type is determined by the input type, so Union is\nnot an accurate solution. To properly annotate such functions, we\nneed a type variable—presented in “Parameterized Generics and\nTypeVar” on page 282—or overloading, which we’ll see in “Overloa‐\nded Signatures” on page 520.\nUnion[] requires at least two types. Nested Union types have the same effect as a flat‐\ntened Union. So this type hint:\nUnion[A, B, Union[C, D, E]]\nis the same as:\nUnion[A, B, C, D, E]\nUnion is more useful with types that are not consistent among themselves. For exam‐\nple: Union[int, float] is redundant because int is consistent-with float. If you just\nuse float to annotate the parameter, it will accept int values as well.\nGeneric Collections\nMost Python collections are heterogeneous. For example, you can put any mixture of\ndifferent types in a list. However, in practice that’s not very useful: if you put\nobjects in a collection, you are likely to want to operate on them later, and usually\nthis means they must share at least one common method.9\nGeneric types can be declared with type parameters to specify the type of the items\nthey can handle.\nFor example, a list can be parameterized to constrain the type of the elements in it,\nas you can see in Example 8-8.\nExample 8-8. tokenize with type hints for Python ≥ 3.9\ndef tokenize(text: str) -> list[str]:\n    return text.upper().split()\nIn Python ≥ 3.9, it means that tokenize returns a list where every item is of type\nstr.\nTypes Usable in Annotations \n| \n271",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "The annotations stuff: list and stuff: list[Any] mean the same thing: stuff is\na list of objects of any type.\nIf you are using Python 3.8 or earlier, the concept is the same, but\nyou need more code to make it work—as explained in the optional\nbox “Legacy Support and Deprecated Collection Types” on page 272.\nPEP 585—Type Hinting Generics In Standard Collections lists collections from the\nstandard library accepting generic type hints. The following list shows only those col‐\nlections that use the simplest form of generic type hint, container[item]:\nlist        collections.deque        abc.Sequence   abc.MutableSequence\nset         abc.Container            abc.Set        abc.MutableSet\nfrozenset   abc.Collection\nThe tuple and mapping types support more complex type hints, as we’ll see in their\nrespective sections.\nAs of Python 3.10, there is no good way to annotate array.array, taking into\naccount the typecode constructor argument, which determines whether integers or\nfloats are stored in the array. An even harder problem is how to type check integer\nranges to prevent OverflowError at runtime when adding elements to arrays. For\nexample, an array with typecode='B' can only hold int values from 0 to 255. Cur‐\nrently, Python’s static type system is not up to this challenge.\nLegacy Support and Deprecated Collection Types\n(You may skip this box if you only use Python 3.9 or later.)\nFor Python 3.7 and 3.8, you need a __future__ import to make the [] notation work\nwith built-in collections such as list, as shown in Example 8-9.\nExample 8-9. tokenize with type hints for Python ≥ 3.7\nfrom __future__ import annotations\ndef tokenize(text: str) -> list[str]:\n    return text.upper().split()\nThe __future__ import does not work with Python 3.6 or earlier. Example 8-10\nshows how to annotate tokenize in a way that works with Python ≥ 3.5.\nExample 8-10. tokenize with type hints for Python ≥ 3.5\nfrom typing import List\n272 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "10 One of my contributions to the typing module documentation was to add dozens of deprecation warnings as\nI reorganized the entries below “Module Contents” into subsections, under the supervision of Guido van\nRossum.\ndef tokenize(text: str) -> List[str]:\n    return text.upper().split()\nTo provide the initial support for generic type hints, the authors of PEP 484 created\ndozens of generic types in the typing module. Table 8-1 shows some of them. For the\nfull list, visit the typing documentation.\nTable 8-1. Some collection types and their type hint equivalents\nCollection\nType hint equivalent\nlist\ntyping.List\nset\ntyping.Set\nfrozenset\ntyping.FrozenSet\ncollections.deque\ntyping.Deque\ncollections.abc.MutableSequence\ntyping.MutableSequence\ncollections.abc.Sequence\ntyping.Sequence\ncollections.abc.Set\ntyping.AbstractSet\ncollections.abc.MutableSet\ntyping.MutableSet\nPEP 585—Type Hinting Generics In Standard Collections started a multiyear process\nto improve the usability of generic type hints. We can summarize that process in four\nsteps:\n1. Introduce from __future__ import annotations in Python 3.7 to enable the\nuse of standard library classes as generics with list[str] notation.\n2. Make that behavior the default in Python 3.9: list[str] now works without the\nfuture import.\n3. Deprecate all the redundant generic types from the typing module.10 Depreca‐\ntion warnings will not be issued by the Python interpreter because type checkers\nshould flag the deprecated types when the checked program targets Python 3.9 or\nnewer.\n4. Remove those redundant generic types in the first version of Python released five\nyears after Python 3.9. At the current cadence, that could be Python 3.14, a.k.a\nPython Pi.\nNow let’s see how to annotate generic tuples.\nTypes Usable in Annotations \n| \n273",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "Tuple Types\nThere are three ways to annotate tuple types:\n• Tuples as records\n• Tuples as records with named fields\n• Tuples as immutable sequences\nTuples as records\nIf you’re using a tuple as a record, use the tuple built-in and declare the types of the\nfields within [].\nFor example, the type hint would be tuple[str, float, str] to accept a tuple with\ncity name, population, and country: ('Shanghai', 24.28, 'China').\nConsider a function that takes a pair of geographic coordinates and returns a Geo‐\nhash, used like this:\n>>> shanghai = 31.2304, 121.4737\n>>> geohash(shanghai)\n'wtw3sjq6q'\nExample 8-11 shows how geohash is defined, using the geolib package from PyPI.\nExample 8-11. coordinates.py with the geohash function\nfrom geolib import geohash as gh  # type: ignore  \nPRECISION = 9\ndef geohash(lat_lon: tuple[float, float]) -> str:  \n    return gh.encode(*lat_lon, PRECISION)\nThis comment stops Mypy from reporting that the geolib package doesn’t have\ntype hints.\nlat_lon parameter annotated as a tuple with two float fields.\nFor Python < 3.9, import and use typing.Tuple in type hints. It is\ndeprecated but will remain in the standard library at least until\n2024.\n274 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Tuples as records with named fields\nTo annotate a tuple with many fields, or specific types of tuple your code uses in\nmany places, I highly recommend using typing.NamedTuple, as seen in Chapter 5.\nExample 8-12 shows a variation of Example 8-11 with NamedTuple.\nExample 8-12. coordinates_named.py with the NamedTuple Coordinates and the geo\nhash function\nfrom typing import NamedTuple\nfrom geolib import geohash as gh  # type: ignore\nPRECISION = 9\nclass Coordinate(NamedTuple):\n    lat: float\n    lon: float\ndef geohash(lat_lon: Coordinate) -> str:\n    return gh.encode(*lat_lon, PRECISION)\nAs explained in “Overview of Data Class Builders” on page 164, typing.NamedTuple\nis a factory for tuple subclasses, so Coordinate is consistent-with tuple[float,\nfloat] but the reverse is not true—after all, Coordinate has extra methods added by\nNamedTuple, like ._asdict(), and could also have user-defined methods.\nIn practice, this means that it is type safe to pass a Coordinate instance to the dis\nplay function defined in the following:\ndef display(lat_lon: tuple[float, float]) -> str:\n    lat, lon = lat_lon\n    ns = 'N' if lat >= 0 else 'S'\n    ew = 'E' if lon >= 0 else 'W'\n    return f'{abs(lat):0.1f}°{ns}, {abs(lon):0.1f}°{ew}'\nTuples as immutable sequences\nTo annotate tuples of unspecified length that are used as immutable lists, you must\nspecify a single type, followed by a comma and ... (that’s Python’s ellipsis token,\nmade of three periods, not Unicode U+2026—HORIZONTAL ELLIPSIS).\nFor example, tuple[int, ...] is a tuple with int items.\nThe ellipsis indicates that any number of elements >= 1 is acceptable. There is no way\nto specify fields of different types for tuples of arbitrary length.\nThe annotations stuff: tuple[Any, ...] and stuff: tuple mean the same thing:\nstuff is a tuple of unspecified length with objects of any type.\nTypes Usable in Annotations \n| \n275",
      "content_length": 1883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "Here is a columnize function that transforms a sequence into a table of rows and cells\nin the form of a list of tuples with unspecified lengths. This is useful to display items\nin columns, like this:\n>>> animals = 'drake fawn heron ibex koala lynx tahr xerus yak zapus'.split()\n>>> table = columnize(animals)\n>>> table\n[('drake', 'koala', 'yak'), ('fawn', 'lynx', 'zapus'), ('heron', 'tahr'),\n ('ibex', 'xerus')]\n>>> for row in table:\n...     print(''.join(f'{word:10}' for word in row))\n...\ndrake     koala     yak\nfawn      lynx      zapus\nheron     tahr\nibex      xerus\nExample 8-13 shows the implementation of columnize. Note the return type:\nlist[tuple[str, ...]]\nExample 8-13. columnize.py returns a list of tuples of strings\nfrom collections.abc import Sequence\ndef columnize(\n    sequence: Sequence[str], num_columns: int = 0\n) -> list[tuple[str, ...]]:\n    if num_columns == 0:\n        num_columns = round(len(sequence) ** 0.5)\n    num_rows, reminder = divmod(len(sequence), num_columns)\n    num_rows += bool(reminder)\n    return [tuple(sequence[i::num_rows]) for i in range(num_rows)]\nGeneric Mappings\nGeneric mapping types are annotated as MappingType[KeyType, ValueType]. The\nbuilt-in dict and the mapping types in collections and collections.abc accept\nthat notation in Python ≥ 3.9. For earlier versions, you must use typing.Dict and\nother mapping types from the typing module, as described in “Legacy Support and\nDeprecated Collection Types” on page 272.\nExample 8-14 shows a practical use of a function returning an inverted index to\nsearch Unicode characters by name—a variation of Example 4-21 more suitable for\nserver-side code that we’ll study in Chapter 21.\nGiven starting and ending Unicode character codes, name_index returns a dict[str,\nset[str]], which is an inverted index mapping each word to a set of characters that\nhave that word in their names. For example, after indexing ASCII characters from 32\n276 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "to 64, here are the sets of characters mapped to the words 'SIGN' and 'DIGIT', and\nhow to find the character named 'DIGIT EIGHT':\n>>> index = name_index(32, 65)\n>>> index['SIGN']\n{'$', '>', '=', '+', '<', '%', '#'}\n>>> index['DIGIT']\n{'8', '5', '6', '2', '3', '0', '1', '4', '7', '9'}\n>>> index['DIGIT'] & index['EIGHT']\n{'8'}\nExample 8-14 shows the source code for charindex.py with the name_index function.\nBesides a dict[] type hint, this example has three features appearing for the first\ntime in the book.\nExample 8-14. charindex.py\nimport sys\nimport re\nimport unicodedata\nfrom collections.abc import Iterator\nRE_WORD = re.compile(r'\\w+')\nSTOP_CODE = sys.maxunicode + 1\ndef tokenize(text: str) -> Iterator[str]:  \n    \"\"\"return iterable of uppercased words\"\"\"\n    for match in RE_WORD.finditer(text):\n        yield match.group().upper()\ndef name_index(start: int = 32, end: int = STOP_CODE) -> dict[str, set[str]]:\n    index: dict[str, set[str]] = {}  \n    for char in (chr(i) for i in range(start, end)):\n        if name := unicodedata.name(char, ''):  \n            for word in tokenize(name):\n                index.setdefault(word, set()).add(char)\n    return index\ntokenize is a generator function. Chapter 17 is about generators.\nThe local variable index is annotated. Without the hint, Mypy says: Need type\nannotation for 'index' (hint: \"index: dict[<type>, <type>] = ...\").\nTypes Usable in Annotations \n| \n277",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "11 I use := when it makes sense in a few examples, but I don’t cover it in the book. Please see PEP 572—Assign‐\nment Expressions for all the gory details.\n12 Actually, dict is a virtual subclass of abc.MutableMapping. The concept of a virtual subclass is explained in\nChapter 13. For now, know that issubclass(dict, abc.MutableMapping) is True, despite the fact that dict\nis implemented in C and does not inherit anything from abc.MutableMapping, but only from object.\nI used the walrus operator := in the if condition. It assigns the result of the uni\ncodedata.name() call to name, and the whole expression evaluates to that result.\nWhen the result is '', that’s falsy, and the index is not updated.11\nWhen using a dict as a record, it is common to have all keys of the\nstr type, with values of different types depending on the keys. That\nis covered in “TypedDict” on page 526.\nAbstract Base Classes\nBe conservative in what you send, be liberal in what you accept.\n—Postel’s law, a.k.a. the Robustness Principle\nTable 8-1 lists several abstract classes from collections.abc. Ideally, a function\nshould accept arguments of those abstract types—or their typing equivalents before\nPython 3.9—and not concrete types. This gives more flexibility to the caller.\nConsider this function signature:\nfrom collections.abc import Mapping\ndef name2hex(name: str, color_map: Mapping[str, int]) -> str:\nUsing abc.Mapping allows the caller to provide an instance of dict, defaultdict,\nChainMap, a UserDict subclass, or any other type that is a subtype-of Mapping.\nIn contrast, consider this signature:\ndef name2hex(name: str, color_map: dict[str, int]) -> str:\nNow color_map must be a dict or one of its subtypes, such as defaultDict or\nOrderedDict. In particular, a subclass of collections.UserDict would not pass\nthe type check for color_map, despite being the recommended way to create\nuser-defined mappings, as we saw in “Subclassing UserDict Instead of dict” on page\n97. Mypy would reject a UserDict or an instance of a class derived from it, because\nUserDict is not a subclass of dict; they are siblings. Both are subclasses of\nabc.MutableMapping.12\n278 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Therefore, in general it’s better to use abc.Mapping or abc.MutableMapping in\nparameter type hints, instead of dict (or typing.Dict in legacy code). If the\nname2hex function doesn’t need to mutate the given color_map, the most accurate\ntype hint for color_map is abc.Mapping. That way, the caller doesn’t need to provide\nan object that implements methods like setdefault, pop, and update, which are part\nof the MutableMapping interface, but not of Mapping. This has to do with the second\npart of Postel’s law: “Be liberal in what you accept.”\nPostel’s law also tells us to be conservative in what we send. The return value of a\nfunction is always a concrete object, so the return type hint should be a concrete type,\nas in the example from “Generic Collections” on page 271—which uses list[str]:\ndef tokenize(text: str) -> list[str]:\n    return text.upper().split()\nUnder the entry of typing.List, the Python documentation says:\nGeneric version of list. Useful for annotating return types. To annotate arguments it\nis preferred to use an abstract collection type such as Sequence or Iterable.\nA similar comment appears in the entries for typing.Dict and typing.Set.\nRemember that most ABCs from collections.abc and other concrete classes from\ncollections, as well as built-in collections, support generic type hint notation like\ncollections.deque[str] starting with Python 3.9. The corresponding typing col‐\nlections are only needed to support code written in Python 3.8 or earlier. The full list\nof classes that became generic appears in the “Implementation” section of PEP 585—\nType Hinting Generics In Standard Collections.\nTo wrap up our discussion of ABCs in type hints, we need to talk about the numbers\nABCs.\nThe fall of the numeric tower\nThe numbers package defines the so-called numeric tower described in PEP 3141—A\nType Hierarchy for Numbers. The tower is linear hierarchy of ABCs, with Number at\nthe top:\n• Number\n• Complex\n• Real\n• Rational\n• Integral\nTypes Usable in Annotations \n| \n279",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "Those ABCs work perfectly well for runtime type checking, but they are not sup‐\nported for static type checking. The “Numeric Tower” section of PEP 484 rejects the\nnumbers ABCs and dictates that the built-in types complex, float, and int should\nbe treated as special cases, as explained in “int Is Consistent-With complex” on page\n269.\nWe’ll come back to this issue in “The numbers ABCs and Numeric Protocols” on\npage 478, in Chapter 13, which is devoted to contrasting protocols and ABCs.\nIn practice, if you want to annotate numeric arguments for static type checking, you\nhave a few options:\n1. Use one of the concrete types int, float, or complex—as recommended by PEP\n488.\n2. Declare a union type like Union[float, Decimal, Fraction].\n3. If you want to avoid hardcoding concrete types, use numeric protocols like Sup\nportsFloat, covered in “Runtime Checkable Static Protocols” on page 468.\nThe upcoming section “Static Protocols” on page 286 is a prerequisite for understand‐\ning the numeric protocols.\nMeanwhile, let’s get to one of the most useful ABCs for type hints: Iterable.\nIterable\nThe typing.List documentation I just quoted recommends Sequence and Iterable\nfor function parameter type hints.\nOne example of the Iterable argument appears in the math.fsum function from the\nstandard library:\ndef fsum(__seq: Iterable[float]) -> float:\nStub Files and the Typeshed Project\nAs of Python 3.10, the standard library has no annotations, but\nMypy, PyCharm, etc. can find the necessary type hints in the\nTypeshed project, in the form of stub files: special source files with\na .pyi extension that have annotated function and method signa‐\ntures, without the implementation—much like header files in C.\nThe signature for math.fsum is in /stdlib/2and3/math.pyi. The lead‐\ning underscores in __seq are a PEP 484 convention for positional-\nonly parameters, explained in “Annotating Positional Only and\nVariadic Parameters” on page 295.\n280 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "Example 8-15 is another example using an Iterable parameter that produces items\nthat are tuple[str, str]. Here is how the function is used:\n>>> l33t = [('a', '4'), ('e', '3'), ('i', '1'), ('o', '0')]\n>>> text = 'mad skilled noob powned leet'\n>>> from replacer import zip_replace\n>>> zip_replace(text, l33t)\n'm4d sk1ll3d n00b p0wn3d l33t'\nExample 8-15 shows how it’s implemented.\nExample 8-15. replacer.py\nfrom collections.abc import Iterable\nFromTo = tuple[str, str]  \ndef zip_replace(text: str, changes: Iterable[FromTo]) -> str:  \n    for from_, to in changes:\n        text = text.replace(from_, to)\n    return text\nFromTo is a type alias: I assigned tuple[str, str] to FromTo, to make the signa‐\nture of zip_replace more readable.\nchanges needs to be an Iterable[FromTo]; that’s the same as Itera\nble[tuple[str, str]], but shorter and easier to read.\nExplicit TypeAlias in Python 3.10\nPEP 613—Explicit Type Aliases introduced a special type, TypeA\nlias, to make the assignments that create type aliases more visible\nand easier to type check. Starting with Python 3.10, this is the pre‐\nferred way to create type aliases:\nfrom typing import TypeAlias\nFromTo: TypeAlias = tuple[str, str]\nabc.Iterable versus abc.Sequence\nBoth math.fsum and replacer.zip_replace must iterate over the entire Iterable\narguments to return a result. Given an endless iterable such as the itertools.cycle\ngenerator as input, these functions would consume all memory and crash the Python\nprocess. Despite this potential danger, it is fairly common in modern Python to offer\nfunctions that accept an Iterable input even if they must process it completely to\nreturn a result. That gives the caller the option of providing input data as a generator\nTypes Usable in Annotations \n| \n281",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "instead of a prebuilt sequence, potentially saving a lot of memory if the number of\ninput items is large.\nOn the other hand, the columnize function from Example 8-13 needs a Sequence\nparameter, and not an Iterable, because it must get the len() of the input to com‐\npute the number of rows up front.\nLike Sequence, Iterable is best used as a parameter type. It’s too vague as a return\ntype. A function should be more precise about the concrete type it returns.\nClosely related to Iterable is the Iterator type, used as a return type in\nExample 8-14. We’ll get back to it in Chapter 17, which is about generators and\nclassic iterators.\nParameterized Generics and TypeVar\nA parameterized generic is a generic type, written as list[T], where T is a type vari‐\nable that will be bound to a specific type with each usage. This allows a parameter\ntype to be reflected on the result type.\nExample 8-16 defines sample, a function that takes two arguments: a Sequence of ele‐\nments of type T, and an int. It returns a list of elements of the same type T, picked\nat random from the first argument.\nExample 8-16 shows the implementation.\nExample 8-16. sample.py\nfrom collections.abc import Sequence\nfrom random import shuffle\nfrom typing import TypeVar\nT = TypeVar('T')\ndef sample(population: Sequence[T], size: int) -> list[T]:\n    if size < 1:\n        raise ValueError('size must be >= 1')\n    result = list(population)\n    shuffle(result)\n    return result[:size]\nHere are two examples of why I used a type variable in sample:\n• If called with a tuple of type tuple[int, ...]—which is consistent-with\nSequence[int]—then the type parameter is int, so the return type is list[int].\n282 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "13 The implementation here is simpler than the one in the Python standard library statistics module.\n• If called with a str—which is consistent-with Sequence[str]—then the type\nparameter is str, so the return type is list[str].\nWhy Is TypeVar Needed?\nThe authors of PEP 484 wanted to introduce type hints by adding\nthe typing module and not changing anything else in the language.\nWith clever metaprogramming they could make the [] operator\nwork on classes like Sequence[T]. But the name of the T variable\ninside the brackets must be defined somewhere—otherwise the\nPython interpreter would need deep changes to support generic\ntype notation as special use of []. That’s why the typing.TypeVar\nconstructor is needed: to introduce the variable name in the cur‐\nrent namespace. Languages such as Java, C#, and TypeScript don’t\nrequire the name of type variable to be declared beforehand, so\nthey have no equivalent of Python’s TypeVar class.\nAnother example is the statistics.mode function from the standard library, which\nreturns the most common data point from a series.\nHere is one usage example from the documentation:\n>>> mode([1, 1, 2, 3, 3, 3, 3, 4])\n3\nWithout using a TypeVar, mode could have the signature shown in Example 8-17.\nExample 8-17. mode_float.py: mode that operates on float and subtypes13\nfrom collections import Counter\nfrom collections.abc import Iterable\ndef mode(data: Iterable[float]) -> float:\n    pairs = Counter(data).most_common(1)\n    if len(pairs) == 0:\n        raise ValueError('no mode for empty data')\n    return pairs[0][0]\nMany uses of mode involve int or float values, but Python has other numerical\ntypes, and it is desirable that the return type follows the element type of the given\nIterable. We can improve that signature using TypeVar. Let’s start with a simple,\nbut wrong, parameterized signature:\nTypes Usable in Annotations \n| \n283",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "from collections.abc import Iterable\nfrom typing import TypeVar\nT = TypeVar('T')\ndef mode(data: Iterable[T]) -> T:\nWhen it first appears in the signature, the type parameter T can be any type. The sec‐\nond time it appears, it will mean the same type as the first.\nTherefore, every iterable is consistent-with Iterable[T], including iterables of\nunhashable types that collections.Counter cannot handle. We need to restrict the\npossible types assigned to T. We’ll see two ways of doing that in the next two sections.\nRestricted TypeVar\nTypeVar accepts extra positional arguments to restrict the type parameter. We can\nimprove the signature of mode to accept specific number types, like this:\nfrom collections.abc import Iterable\nfrom decimal import Decimal\nfrom fractions import Fraction\nfrom typing import TypeVar\nNumberT = TypeVar('NumberT', float, Decimal, Fraction)\ndef mode(data: Iterable[NumberT]) -> NumberT:\nThat’s better than before, and it was the signature for mode in the statistics.pyi stub file\non typeshed on May 25, 2020.\nHowever, the statistics.mode documentation includes this example:\n>>> mode([\"red\", \"blue\", \"blue\", \"red\", \"green\", \"red\", \"red\"])\n'red'\nIn a hurry, we could just add str to the NumberT definition:\nNumberT = TypeVar('NumberT', float, Decimal, Fraction, str)\nThat certainly works, but NumberT is badly misnamed if it accepts str. More impor‐\ntantly, we can’t keep listing types forever, as we realize mode can deal with them. We\ncan do better with another feature of TypeVar, introduced next.\nBounded TypeVar\nLooking at the body of mode in Example 8-17, we see that the Counter class is used for\nranking. Counter is based on dict, therefore the element type of the data iterable\nmust be hashable.\nAt first, this signature may seem to work:\n284 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "14 I contributed this solution to typeshed, and that’s how mode is annotated on statistics.pyi as of May 26, 2020.\nfrom collections.abc import Iterable, Hashable\ndef mode(data: Iterable[Hashable]) -> Hashable:\nNow the problem is that the type of the returned item is Hashable: an ABC that\nimplements only the __hash__ method. So the type checker will not let us do any‐\nthing with the return value except call hash() on it. Not very useful.\nThe solution is another optional parameter of TypeVar: the bound keyword parame‐\nter. It sets an upper boundary for the acceptable types. In Example 8-18, we have\nbound=Hashable, which means the type parameter may be Hashable or any subtype-\nof it.14\nExample 8-18. mode_hashable.py: same as Example 8-17, with a more flexible\nsignature\nfrom collections import Counter\nfrom collections.abc import Iterable, Hashable\nfrom typing import TypeVar\nHashableT = TypeVar('HashableT', bound=Hashable)\ndef mode(data: Iterable[HashableT]) -> HashableT:\n    pairs = Counter(data).most_common(1)\n    if len(pairs) == 0:\n        raise ValueError('no mode for empty data')\n    return pairs[0][0]\nTo summarize:\n• A restricted type variable will be set to one of the types named in the TypeVar\ndeclaration.\n• A bounded type variable will be set to the inferred type of the expression—as\nlong as the inferred type is consistent-with the boundary declared in the bound=\nkeyword argument of TypeVar.\nIt is unfortunate that the keyword argument to declare a bounded\nTypeVar is named bound=, because the verb “to bind” is commonly\nused to mean setting the value of a variable, which in the reference\nsemantics of Python is best described as binding a name to the\nvalue. It would have been less confusing if the keyword argument\nwas named boundary=.\nTypes Usable in Annotations \n| \n285",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "The typing.TypeVar constructor has other optional parameters—covariant and con\ntravariant—that we’ll cover in Chapter 15, “Variance” on page 544.\nLet’s conclude this introduction to TypeVar with AnyStr.\nThe AnyStr predefined type variable\nThe typing module includes a predefined TypeVar named AnyStr. It’s defined like\nthis:\nAnyStr = TypeVar('AnyStr', bytes, str)\nAnyStr is used in many functions that accept either bytes or str, and return values\nof the given type.\nNow, on to typing.Protocol, a new feature of Python 3.8 that can support more\nPythonic use of type hints.\nStatic Protocols\nIn object-oriented programming, the concept of a “protocol” as an\ninformal interface is as old as Smalltalk, and is an essential part of\nPython from the beginning. However, in the context of type hints,\na protocol is a typing.Protocol subclass defining an interface that\na type checker can verify. Both kinds of protocols are covered in\nChapter 13. This is just a brief introduction in the context of func‐\ntion annotations.\nThe Protocol type, as presented in PEP 544—Protocols: Structural subtyping (static\nduck typing), is similar to interfaces in Go: a protocol type is defined by specifying\none or more methods, and the type checker verifies that those methods are imple‐\nmented where that protocol type is required.\nIn Python, a protocol definition is written as a typing.Protocol subclass. However,\nclasses that implement a protocol don’t need to inherit, register, or declare any rela‐\ntionship with the class that defines the protocol. It’s up to the type checker to find the\navailable protocol types and enforce their usage.\nHere is a problem that can be solved with the help of Protocol and TypeVar. Suppose\nyou want to create a function top(it, n) that returns the largest n elements of the\niterable it:\n>>> top([4, 1, 5, 2, 6, 7, 3], 3)\n[7, 6, 5]\n>>> l = 'mango pear apple kiwi banana'.split()\n>>> top(l, 3)\n['pear', 'mango', 'kiwi']\n286 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "15 How wonderful it is to open an interactive console and rely on duck typing to explore language features like I\njust did. I badly miss this kind of exploration when I use languages that don’t support it.\n>>>\n>>> l2 = [(len(s), s) for s in l]\n>>> l2\n[(5, 'mango'), (4, 'pear'), (5, 'apple'), (4, 'kiwi'), (6, 'banana')]\n>>> top(l2, 3)\n[(6, 'banana'), (5, 'mango'), (5, 'apple')]\nA parameterized generic top would look like what’s shown in Example 8-19.\nExample 8-19. top function with an undefined T type parameter\ndef top(series: Iterable[T], length: int) -> list[T]:\n    ordered = sorted(series, reverse=True)\n    return ordered[:length]\nThe problem is how to constrain T? It cannot be Any or object, because the series\nmust work with sorted. The sorted built-in actually accepts Iterable[Any], but\nthat’s because the optional parameter key takes a function that computes an arbitrary\nsort key from each element. What happens if you give sorted a list of plain objects\nbut don’t provide a key argument? Let’s try that:\n>>> l = [object() for _ in range(4)]\n>>> l\n[<object object at 0x10fc2fca0>, <object object at 0x10fc2fbb0>,\n<object object at 0x10fc2fbc0>, <object object at 0x10fc2fbd0>]\n>>> sorted(l)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: '<' not supported between instances of 'object' and 'object'\nThe error message shows that sorted uses the < operator on the elements of the itera‐\nble. Is this all it takes? Let’s do another quick experiment:15\n>>> class Spam:\n...     def __init__(self, n): self.n = n\n...     def __lt__(self, other): return self.n < other.n\n...     def __repr__(self): return f'Spam({self.n})'\n...\n>>> l = [Spam(n) for n in range(5, 0, -1)]\n>>> l\n[Spam(5), Spam(4), Spam(3), Spam(2), Spam(1)]\n>>> sorted(l)\n[Spam(1), Spam(2), Spam(3), Spam(4), Spam(5)]\nThat confirms it: I can sort a list of Spam because Spam implements __lt__—the spe‐\ncial method that supports the < operator.\nTypes Usable in Annotations \n| \n287",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "So the T type parameter in Example 8-19 should be limited to types that implement\n__lt__. In Example 8-18 we needed a type parameter that implemented __hash__, so\nwe were able to use typing.Hashable as the upper bound for the type parameter. But\nnow there is no suitable type in typing or abc to use, so we need to create it.\nExample 8-20 shows the new SupportsLessThan type, a Protocol.\nExample 8-20. comparable.py: definition of a SupportsLessThan Protocol type\nfrom typing import Protocol, Any\nclass SupportsLessThan(Protocol):  \n    def __lt__(self, other: Any) -> bool: ...  \nA protocol is a subclass of typing.Protocol.\nThe body of the protocol has one or more method definitions, with ... in their\nbodies.\nA type T is consistent-with a protocol P if T implements all the methods defined in P,\nwith matching type signatures.\nGiven SupportsLessThan, we can now define this working version of top in\nExample 8-21.\nExample 8-21. top.py: definition of the top function using a TypeVar with bound=Sup\nportsLessThan\nfrom collections.abc import Iterable\nfrom typing import TypeVar\nfrom comparable import SupportsLessThan\nLT = TypeVar('LT', bound=SupportsLessThan)\ndef top(series: Iterable[LT], length: int) -> list[LT]:\n    ordered = sorted(series, reverse=True)\n    return ordered[:length]\nLet’s test-drive top. Example 8-22 shows part of a test suite for use with pytest. It\ntries calling top first with a generator expression that yields tuple[int, str], and\nthen with a list of object. With the list of object, we expect to get a TypeError\nexception.\n288 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "16 Without this type hint, Mypy would infer the type of series as Generator[Tuple[builtins.int, buil\ntins.str*], None, None], which is verbose but consistent-with Iterator[tuple[int, str]], as we’ll see in\n“Generic Iterable Types” on page 639.\nExample 8-22. top_test.py: partial listing of the test suite for top\nfrom collections.abc import Iterator\nfrom typing import TYPE_CHECKING  \nimport pytest\nfrom top import top\n# several lines omitted\ndef test_top_tuples() -> None:\n    fruit = 'mango pear apple kiwi banana'.split()\n    series: Iterator[tuple[int, str]] = (  \n        (len(s), s) for s in fruit)\n    length = 3\n    expected = [(6, 'banana'), (5, 'mango'), (5, 'apple')]\n    result = top(series, length)\n    if TYPE_CHECKING:  \n        reveal_type(series)  \n        reveal_type(expected)\n        reveal_type(result)\n    assert result == expected\n# intentional type error\ndef test_top_objects_error() -> None:\n    series = [object() for _ in range(4)]\n    if TYPE_CHECKING:\n        reveal_type(series)\n    with pytest.raises(TypeError) as excinfo:\n        top(series, 3)  \n    assert \"'<' not supported\" in str(excinfo.value)\nThe typing.TYPE_CHECKING constant is always False at runtime, but type check‐\ners pretend it is True when they are type checking.\nExplicit type declaration for the series variable, to make the Mypy output easier\nto read.16\nThis if prevents the next three lines from executing when the test runs.\nreveal_type() cannot be called at runtime, because it is not a regular function\nbut a Mypy debugging facility—that’s why there is no import for it. Mypy will\nTypes Usable in Annotations \n| \n289",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "output one debugging message for each reveal_type() pseudofunction call,\nshowing the inferred type of the argument.\nThis line will be flagged as an error by Mypy.\nThe preceding tests pass—but they would pass anyway, with or without type hints in\ntop.py. More to the point, if I check that test file with Mypy, I see that the TypeVar is\nworking as intended. See the mypy command output in Example 8-23.\nAs of Mypy 0.910 (July 2021), the output of reveal_type does not\nshow precisely the types I declared in some cases, but compatible\ntypes instead. For example, I did not use typing.Iterator but\nused abc.Iterator. Please ignore this detail. The Mypy output is\nstill useful. I will pretend this issue of Mypy is fixed when discus‐\nsing the output.\nExample 8-23. Output of mypy top_test.py (lines split for readability)\n…/comparable/ $ mypy top_test.py\ntop_test.py:32: note:\n    Revealed type is \"typing.Iterator[Tuple[builtins.int, builtins.str]]\" \ntop_test.py:33: note:\n    Revealed type is \"builtins.list[Tuple[builtins.int, builtins.str]]\"\ntop_test.py:34: note:\n    Revealed type is \"builtins.list[Tuple[builtins.int, builtins.str]]\" \ntop_test.py:41: note:\n    Revealed type is \"builtins.list[builtins.object*]\" \ntop_test.py:43: error:\n    Value of type variable \"LT\" of \"top\" cannot be \"object\"  \nFound 1 error in 1 file (checked 1 source file)\nIn test_top_tuples, reveal_type(series) shows it is an Iterator[tuple[int,\nstr]]—which I explicitly declared.\nreveal_type(result) confirms that the type returned by the top call is what I\nwanted: given the type of series, the result is list[tuple[int, str]].\nIn test_top_objects_error, reveal_type(series) shows it is list[object*].\nMypy puts a * after any type that was inferred: I did not annotate the type of\nseries in this test.\nMypy flags the error that this test intentionally triggers: the element type of the\nIterable series cannot be object (it must be of type SupportsLessThan).\nA key advantage of a protocol type over ABCs is that a type doesn’t need any special\ndeclaration to be consistent-with a protocol type. This allows a protocol to be created\n290 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "17 I don’t know who invented the term static duck typing, but it became more popular with the Go language,\nwhich has interface semantics that are more like Python’s protocols than the nominal interfaces of Java.\nleveraging preexisting types, or types implemented in code that we do not control. I\ndon’t need to derive or register str, tuple, float, set, etc. with SupportsLessThan to\nuse them where a SupportsLessThan parameter is expected. They only need to\nimplement __lt__. And the type checker will still be able do its job, because Support\nsLessThan is explicitly defined as a Protocol—in contrast with the implicit protocols\nthat are common with duck typing, which are invisible to the type checker.\nThe special Protocol class was introduced in PEP 544—Protocols: Structural subtyp‐\ning (static duck typing). Example 8-21 demonstrates why this feature is known as\nstatic duck typing: the solution to annotate the series parameter of top was to say\n“The nominal type of series doesn’t matter, as long as it implements the __lt__\nmethod.” Python’s duck typing always allowed us to say that implicitly, leaving static\ntype checkers clueless. A type checker can’t read CPython’s source code in C, or per‐\nform console experiments to find out that sorted only requires that the elements\nsupport <.\nNow we can make duck typing explicit for static type checkers. That’s why it makes\nsense to say that typing.Protocol gives us static duck typing.17\nThere’s more to see about typing.Protocol. We’ll come back to it in Part IV, where\nChapter 13 contrasts structural typing, duck typing, and ABCs—another approach to\nformalizing protocols. In addition, “Overloaded Signatures” on page 520 (Chapter 15)\nexplains how to declare overloaded function signatures with @typing.overload, and\nincludes an extensive example using typing.Protocol and a bounded TypeVar.\ntyping.Protocol makes it possible to annotate the double func‐\ntion presented in “Types Are Defined by Supported Operations”\non page 260 without losing functionality. The key is to define a\nprotocol class with the __mul__ method. I invite you to do that as\nan exercise. The solution appears in “The Typed double Function”\non page 466 (Chapter 13).\nCallable\nTo annotate callback parameters or callable objects returned by higher-order func‐\ntions, the collections.abc module provides the Callable type, available in the typ\ning module for those not yet using Python 3.9. A Callable type is parameterized like\nthis:\nCallable[[ParamType1, ParamType2], ReturnType]\nTypes Usable in Annotations \n| \n291",
      "content_length": 2550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "18 REPL stands for Read-Eval-Print-Loop, the basic behavior of interactive interpreters.\nThe parameter list—[ParamType1, ParamType2]—can have zero or more types.\nHere is an example in the context of a repl function, part of a simple interactive\ninterpreter we’ll see in “Pattern Matching in lis.py: A Case Study” on page 669:18\ndef repl(input_fn: Callable[[Any], str] = input]) -> None:\nDuring normal usage, the repl function uses Python’s input built-in to read expres‐\nsions from the user. However, for automated testing or for integration with other\ninput sources, repl accepts an optional input_fn parameter: a Callable with the\nsame parameter and return types as input.\nThe built-in input has this signature on typeshed:\ndef input(__prompt: Any = ...) -> str: ...\nThe input signature is consistent-with this Callable type hint:\nCallable[[Any], str]\nThere is no syntax to annotate optional or keyword argument types. The documenta‐\ntion of typing.Callable says “such function types are rarely used as callback types.”\nIf you need a type hint to match a function with a flexible signature, replace the\nwhole parameter list with ...—like this:\nCallable[..., ReturnType]\nThe interaction of generic type parameters with a type hierarchy introduces a new\ntyping concept: variance.\nVariance in Callable types\nImagine a temperature control system with a simple update function as shown in\nExample 8-24. The update function calls the probe function to get the current tem‐\nperature, and calls display to show the temperature to the user. Both probe and dis\nplay are passed as arguments to update for didactic reasons. The goal of the example\nis to contrast two Callable annotations: one with a return type, the other with a\nparameter type.\nExample 8-24. Illustrating variance.\nfrom collections.abc import Callable\ndef update(  \n        probe: Callable[[], float],  \n        display: Callable[[float], None]  \n292 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": ") -> None:\n    temperature = probe()\n    # imagine lots of control code here\n    display(temperature)\ndef probe_ok() -> int:  \n    return 42\ndef display_wrong(temperature: int) -> None:  \n    print(hex(temperature))\nupdate(probe_ok, display_wrong)  # type error  \ndef display_ok(temperature: complex) -> None:  \n    print(temperature)\nupdate(probe_ok, display_ok)  # OK  \nupdate takes two callables as arguments.\nprobe must be a callable that takes no arguments and returns a float.\ndisplay takes a float argument and returns None.\nprobe_ok is consistent-with Callable[[], float] because returning an int does\nnot break code that expects a float.\ndisplay_wrong is not consistent-with Callable[[float], None] because there’s\nno guarantee that a function that expects an int can handle a float; for example,\nPython’s hex function accepts an int but rejects a float.\nMypy flags this line because display_wrong is incompatible with the type hint in\nthe display parameter of update.\ndisplay_ok is consistent-with Callable[[float], None] because a function that\naccepts a complex can also handle a float argument.\nMypy is happy with this line.\nTo summarize, it’s OK to provide a callback that returns an int when the code\nexpects a callback that returns a float, because an int value can always be used\nwhere a float is expected.\nFormally, we say that Callable[[], int] is subtype-of Callable[[], float]—as\nint is subtype-of float. This means that Callable is covariant on the return type\nTypes Usable in Annotations \n| \n293",
      "content_length": 1518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "because the subtype-of relationship of the types int and float is in the same direc‐\ntion as the relationship of the Callable types that use them as return types.\nOn the other hand, it’s a type error to provide a callback that takes a int argument\nwhen a callback that handles a float is required.\nFormally, Callable[[int], None] is not a subtype-of Callable[[float], None].\nAlthough int is subtype-of float, in the parameterized Callable type the relation‐\nship is reversed: Callable[[float], None] is subtype-of Callable[[int], None].\nTherefore we say that Callable is contravariant on the declared parameter types.\n“Variance” on page 544 in Chapter 15 explains variance with more details and examples\nof invariant, covariant, and contravariant types.\nFor now, rest assured that most parameterized generic types are\ninvariant, therefore simpler. For example, if I declare scores:\nlist[float], that tells me exactly what I can assign to scores. I\ncan’t assign objects declared as list[int] or list[complex]:\n• A list[int] object is not acceptable because it cannot hold\nfloat values which my code may need to put into scores.\n• A list[complex] object is not acceptable because my code\nmay need to sort scores to find the median, but complex does\nnot provide __lt__, therefore list[complex] is not sortable.\nNow we get to the last special type we’ll cover in this chapter.\nNoReturn\nThis is a special type used only to annotate the return type of functions that never\nreturn. Usually, they exist to raise exceptions. There are dozens of such functions in\nthe standard library.\nFor example, sys.exit() raises SystemExit to terminate the Python process.\nIts signature in typeshed is:\ndef exit(__status: object = ...) -> NoReturn: ...\nThe __status parameter is positional only, and it has a default value. Stub files don’t\nspell out the default values, they use ... instead. The type of __status is object,\nwhich means it may also be None, therefore it would be redundant to mark it\nOptional[object].\n294 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "In Chapter 24, Example 24-6 uses NoReturn in the __flag_unknown_attrs, a method\ndesigned to produce a user-friendly and comprehensive error message, and then raise\nAttributeError.\nThe last section in this epic chapter is about positional and variadic parameters.\nAnnotating Positional Only and Variadic Parameters\nRecall the tag function from Example 7-9. The last time we saw its signature was in\n“Positional-Only Parameters” on page 242:\ndef tag(name, /, *content, class_=None, **attrs):\nHere is tag, fully annotated, written in several lines—a common convention for long\nsignatures, with line breaks the way the blue formatter would do it:\nfrom typing import Optional\ndef tag(\n    name: str,\n    /,\n    *content: str,\n    class_: Optional[str] = None,\n    **attrs: str,\n) -> str:\nNote the type hint *content: str for the arbitrary positional parameters; this means\nall those arguments must be of type str. The type of the content local variable in the\nfunction body will be tuple[str, ...].\nThe type hint for the arbitrary keyword arguments is **attrs: str in this example,\ntherefore the type of attrs inside the function will be dict[str, str]. For a\ntype hint like **attrs: float, the type of attrs in the function would be\ndict[str, float].``\nIf the attrs parameter must accept values of different types, you’ll need to use a\nUnion[] or Any: **attrs: Any.\nThe / notation for positional-only parameters is only available in Python ≥ 3.8. In\nPython 3.7 or earlier, that’s a syntax error. The PEP 484 convention is to prefix each\npositional-only parameter name with two underscores. Here is the tag signature\nagain, now in two lines, using the PEP 484 convention:\nfrom typing import Optional\ndef tag(__name: str, *content: str, class_: Optional[str] = None,\n        **attrs: str) -> str:\nMypy understands and enforces both ways of declaring positional-only parameters.\nAnnotating Positional Only and Variadic Parameters \n| \n295",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "To close this chapter, let’s briefly consider the limits of type hints and the static type\nsystem they support.\nImperfect Typing and Strong Testing\nMaintainers of large corporate codebases report that many bugs are found by static\ntype checkers and fixed more cheaply than if the bugs were discovered only after the\ncode is running in production. However, it’s essential to note that automated testing\nwas standard practice and widely adopted long before static typing was introduced in\nthe companies that I know about.\nEven in the contexts where they are most beneficial, static typing cannot be trusted as\nthe ultimate arbiter of correctness. It’s not hard to find:\nFalse positives\nTools report type errors on code that is correct.\nFalse negatives\nTools don’t report type errors on code that is incorrect.\nAlso, if we are forced to type check everything, we lose some of the expressive power\nof Python:\n• Some handy features can’t be statically checked; for example, argument unpack‐\ning like config(**settings).\n• Advanced features like properties, descriptors, metaclasses, and metaprogram‐\nming in general are poorly supported or beyond comprehension for type\ncheckers.\n• Type checkers lag behind Python releases, rejecting or even crashing while ana‐\nlyzing code with new language features—for more than a year in some cases.\nCommon data constraints cannot be expressed in the type system—even simple ones.\nFor example, type hints are unable to ensure “quantity must be an integer > 0” or\n“label must be a string with 6 to 12 ASCII letters.” In general, type hints are not help‐\nful to catch errors in business logic.\nGiven those caveats, type hints cannot be the mainstay of software quality, and mak‐\ning them mandatory without exception would amplify the downsides.\nConsider a static type checker as one of the tools in a modern CI pipeline, along with\ntest runners, linters, etc. The point of a CI pipeline is to reduce software failures, and\nautomated tests catch many bugs that are beyond the reach of type hints. Any code\nyou can write in Python, you can test in Python—with or without type hints.\n296 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "The title and conclusion of this section were inspired by Bruce\nEckel’s article “Strong Typing vs. Strong Testing”, also published in\nthe anthology The Best Software Writing I, edited by Joel Spolsky\n(Apress). Bruce is a fan of Python and author of books about C++,\nJava, Scala, and Kotlin. In that post, he tells how he was a static typ‐\ning advocate until he learned Python and concluded: “If a Python\nprogram has adequate unit tests, it can be as robust as a C++, Java,\nor C# program with adequate unit tests (although the tests in\nPython will be faster to write).”\nThis wraps up our coverage of Python’s type hints for now. They are also the main\nfocus of Chapter 15, which covers generic classes, variance, overloaded signatures,\ntype casting, and more. Meanwhile, type hints will make guest appearances in several\nexamples throughout the book.\nChapter Summary\nWe started with a brief introduction to the concept of gradual typing and then\nswitched to a hands-on approach. It’s hard to see how gradual typing works without a\ntool that actually reads the type hints, so we developed an annotated function guided\nby Mypy error reports.\nBack to the idea of gradual typing, we explored how it is a hybrid of Python’s tradi‐\ntional duck typing and the nominal typing more familiar to users of Java, C++, and\nother statically typed languages.\nMost of the chapter was devoted to presenting the major groups of types used in\nannotations. Many of the types we covered are related to familiar Python object\ntypes, such as collections, tuples, and callables—extended to support generic notation\nlike Sequence[float]. Many of those types are temporary surrogates implemented in\nthe typing module before the standard types were changed to support generics in\nPython 3.9.\nSome of the types are special entities. Any, Optional, Union, and NoReturn have noth‐\ning to do with actual objects in memory, but exist only in the abstract domain of the\ntype system.\nWe studied parameterized generics and type variables, which bring more flexibility to\ntype hints without sacrificing type safety.\nParameterized generics become even more expressive with the use of Protocol.\nBecause it appeared only in Python 3.8, Protocol is not widely used yet—but it is\nhugely important. Protocol enables static duck typing: the essential bridge between\nPython’s duck-typed core and the nominal typing that allows static type checkers to\ncatch bugs.\nChapter Summary \n| \n297",
      "content_length": 2440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "19 “Benevolent Dictator For Life.” See Guido van van Rossum on the “Origin of BDFL”.\n20 From the YouTube video, “Type Hints by Guido van Rossum (March 2015)”. Quote starts at 13’40”. I did\nsome light editing for clarity.\nWhile covering some of these types, we experimented with Mypy to see type checking\nerrors and inferred types with the help of Mypy’s magic reveal_type() function.\nThe final section covered how to annotate positional-only and variadic parameters.\nType hints are a complex and evolving topic. Fortunately, they are an optional\nfeature. Let us keep Python accessible to the widest user base and stop preaching that\nall Python code should have type hints—as I’ve seen in public sermons by typing\nevangelists.\nOur BDFL19 emeritus led this push toward type hints in Python, so it’s only fair that\nthis chapter starts and ends with his words:\nI wouldn’t like a version of Python where I was morally obligated to add type hints all\nthe time. I really do think that type hints have their place but there are also plenty of\ntimes that it’s not worth it, and it’s so wonderful that you can choose to use them.20\n—Guido van Rossum\nFurther Reading\nBernát Gábor wrote in his excellent post, “The state of type hints in Python”:\nType hints should be used whenever unit tests are worth writing.\nI am a big fan of testing, but I also do a lot of exploratory coding. When I am explor‐\ning, tests and type hints are not helpful. They are a drag.\nGábor’s post is one of the best introductions to Python’s type hints that I found,\nalong with Geir Arne Hjelle’s “Python Type Checking (Guide)”. “Hypermodern\nPython Chapter 4: Typing” by Claudio Jolowicz is a shorter introduction that also\ncovers runtime type checking validation.\nFor deeper coverage, the Mypy documentation is the best source. It is valuable\nregardless of the type checker you are using, because it has tutorial and reference\npages about Python typing in general—not just about the Mypy tool itself. There you\nwill also find a handy cheat sheets and a very useful page about common issues and\nsolutions.\nThe typing module documentation is a good quick reference, but it doesn’t go into\nmuch detail. PEP 483—The Theory of Type Hints includes a deep explanation about\nvariance, using Callable to illustrate contravariance. The ultimate references are the\nPEP documents related to typing. There are more than 20 of them already. The\n298 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "intended audience of PEPs are Python core developers and Python’s Steering Coun‐\ncil, so they assume a lot of prior knowledge and are certainly not light reading.\nAs mentioned, Chapter 15 covers more typing topics, and “Further Reading” on page\n555 provides additional references, including Table 15-1, listing typing PEPs approved\nor under discussion as of late 2021.\n“Awesome Python Typing” is a valuable collection of links to tools and references.\nSoapbox\nJust Ride\nForget the ultralight, uncomfortable bikes, flashy jerseys, clunky shoes that clip onto\ntiny pedals, the grinding out of endless miles. Instead, ride like you did when you\nwere a kid—just get on your bike and discover the pure joy of riding it.\n—Grant Petersen, Just Ride: A Radically Practical Guide to Riding Your Bike\n(Workman Publishing)\nIf coding is not your whole profession, but a useful tool in your profession, or some‐\nthing you do to learn, tinker, and enjoy, you probably don’t need type hints any more\nthan most bikers need shoes with stiff soles and metal cleats.\nJust code.\nThe Cognitive Effect of Typing\nI worry about the effect type hints will have on Python coding style.\nI agree that users of most APIs benefit from type hints. But Python attracted me—\namong other reasons—because it provides functions that are so powerful that they\nreplace entire APIs, and we can write similarly powerful functions ourselves. Con‐\nsider the max() built-in. It’s powerful, yet easy to understand. But I will show in “Max\nOverload” on page 521 that it takes 14 lines of type hints to properly annotate it—not\ncounting a typing.Protocol and a few TypeVar definitions to support those type\nhints.\nI am concerned that strict enforcement of type hints in libraries will discourage pro‐\ngrammers from even considering writing such functions in the future.\nAccording to the English Wikipedia, “linguistic relativity”—a.k.a. the Sapir–Whorf\nhypothesis— is a “principle claiming that the structure of a language affects its speak‐\ners’ world view or cognition.” Wikipedia further explains:\n• The strong version says that language determines thought and that linguistic cate‐\ngories limit and determine cognitive categories.\nFurther Reading \n| \n299",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "• The weak version says that linguistic categories and usage only influence thought\nand decisions.\nLinguists generally agree the strong version is false, but there is empirical evidence\nsupporting the weak version.\nI am not aware of specific studies with programming languages, but in my experience\nthey’ve had a big impact on how I approach problems. The first programming lan‐\nguage I used professionally was Applesoft BASIC in the age of 8-bit computers.\nRecursion was not directly supported by BASIC—you had to roll your own call stack\nto use it. So I never considered using recursive algorithms or data structures. I knew\nat some conceptual level such things existed, but they weren’t part of my problem-\nsolving toolbox.\nDecades later when I started with Elixir, I enjoyed solving problems with recursion\nand overused it—until I discovered that many of my solutions would be simpler if I\nused existing functions from the Elixir Enum and Stream modules. I learned that\nidiomatic Elixir application-level code rarely has explicit recursive calls, but uses\nenums and streams that implement recursion under the hood.\nLinguistic relativity could explain the widespread idea (also unproven) that learning\ndifferent programming languages makes you a better programmer, particularly when\nthe languages support different programming paradigms. Practicing Elixir made me\nmore likely to apply functional patterns when I write Python or Go code.\nNow, back to Earth.\nThe requests package would probably have a very different API if Kenneth Reitz was\ndetermined (or told by his boss) to annotate all its functions. His goal was to write an\nAPI that was easy to use, flexible, and powerful. He succeeded, given the amazing\npopularity of requests—in May 2020, it’s #4 on PyPI Stats, with 2.6 million down‐\nloads a day. #1 is urllib3, a dependency of requests.\nIn 2017, the requests maintainers decided not to spend their time writing type hints.\nOne of the maintainers, Cory Benfield, had written an e-mail stating:\nI think that libraries with Pythonic APIs are the least likely to take up this typing sys‐\ntem because it will provide the least value to them.\nIn that message, Benfield gave this extreme example of a tentative type definition for\nthe files keyword argument of requests.request():\nOptional[\n  Union[\n    Mapping[\n      basestring,\n      Union[\n        Tuple[basestring, Optional[Union[basestring, file]]],\n        Tuple[basestring, Optional[Union[basestring, file]],\n              Optional[basestring]],\n300 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "Tuple[basestring, Optional[Union[basestring, file]],\n              Optional[basestring], Optional[Headers]]\n      ]\n    ],\n    Iterable[\n      Tuple[\n        basestring,\n        Union[\n          Tuple[basestring, Optional[Union[basestring, file]]],\n          Tuple[basestring, Optional[Union[basestring, file]],\n                Optional[basestring]],\n          Tuple[basestring, Optional[Union[basestring, file]],\n                Optional[basestring], Optional[Headers]]\n      ]\n    ]\n  ]\n]\nAnd that assumes this definition:\nHeaders = Union[\n  Mapping[basestring, basestring],\n  Iterable[Tuple[basestring, basestring]],\n]\nDo you think requests would be the way it is if the maintainers insisted on 100%\ntype hint coverage? SQLAlchemy is another important package that doesn’t play well\nwith type hints.\nWhat makes these libraries great is embracing the dynamic nature of Python.\nWhile there are benefits to type hints, there is also a price to pay.\nFirst, there is the significant investment of understanding how the type system works.\nThat’s a one-time cost.\nBut there is also a recurring cost, forever.\nWe lose some of the expressive power of Python if we insist on type checking every‐\nthing. Beautiful features like argument unpacking—e.g., config(**settings)—are\nbeyond comprehension for type checkers.\nIf you want to have a call like config(**settings) type checked, you must spell\nevery argument out. That brings me memories of Turbo Pascal code I wrote 35 years\nago.\nLibraries that use metaprogramming are hard or impossible to annotate. Surely meta‐\nprogramming can be abused, but it’s also what makes many Python packages a joy\nto use.\nIf type hints are mandated top-down without exceptions in large companies, I bet\nsoon we’ll see people using code generation to reduce boilerplate in Python source-\ncode—a common practice with less dynamic languages.\nFurther Reading \n| \n301",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "21 Source: “A Conversation with Alan Kay”.\nFor some projects and contexts, type hints just don’t make sense. Even in contexts\nwhere they mostly make sense, they don’t make sense all the time. Any reasonable\npolicy about the use of type hints must have exceptions.\nAlan Kay, the Turing Award laureate who pioneered object-oriented programming,\nonce said:\nSome people are completely religious about type systems and as a mathematician I\nlove the idea of type systems, but nobody has ever come up with one that has enough\nscope.21\nThank Guido for optional typing. Let’s use it as intended, and not aim to annotate\neverything into strict conformity to a coding style that looks like Java 1.5.\nDuck Typing FTW\nDuck typing fits my brain, and static duck typing is a good compromise allowing\nstatic type checking without losing a lot of flexibility that some nominal type systems\nonly provide with a lot of complexity—if ever.\nBefore PEP 544, this whole idea of type hints seemed utterly unPythonic to me. I was\nvery glad to see typing.Protocol land in Python. It brings balance to the force.\nGenerics or Specifics?\nFrom a Python perspective, the typing usage of the term “generic” is backward. Com‐\nmon meanings of “generic” are “applicable to an entire class or group” or “without a\nbrand name.”\nConsider list versus list[str]. The first is generic: it accepts any object. The sec‐\nond is specific: it only accepts str.\nThe term makes sense in Java, though. Before Java 1.5, all Java collections (except the\nmagic array) were “specific”: they could only hold Object references, so we had to\ncast the items that came out of a collection to use them. With Java 1.5, collections got\ntype parameters, and became “generic.”\n302 \n| \nChapter 8: Type Hints in Functions",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "1 That’s the 1995 Design Patterns book by the so-called Gang of Four (Gamma et al., Addison-Wesley).\nCHAPTER 9\nDecorators and Closures\nThere’s been a number of complaints about the choice of the name “decorator” for this\nfeature. The major one is that the name is not consistent with its use in the GoF book.1\nThe name decorator probably owes more to its use in the compiler area—a syntax tree\nis walked and annotated.\n—PEP 318—Decorators for Functions and Methods\nFunction decorators let us “mark” functions in the source code to enhance their\nbehavior in some way. This is powerful stuff, but mastering it requires understanding\nclosures—which is what we get when functions capture variables defined outside of\ntheir bodies.\nThe most obscure reserved keyword in Python is nonlocal, introduced in Python 3.0.\nYou can have a profitable life as a Python programmer without ever using it if you\nadhere to a strict regimen of class-centered object orientation. However, if you want\nto implement your own function decorators, you must understand closures, and then\nthe need for nonlocal becomes obvious.\nAside from their application in decorators, closures are also essential for any type of\nprogramming using callbacks, and for coding in a functional style when it makes\nsense.\nThe end goal of this chapter is to explain exactly how function decorators work, from\nthe simplest registration decorators to the rather more complicated parameterized\nones. However, before we reach that goal we need to cover:\n303",
      "content_length": 1505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "2 If you replace “function” with “class” in the previous sentence, you have a brief description of what a class\ndecorator does. Class decorators are covered in Chapter 24.\n• How Python evaluates decorator syntax\n• How Python decides whether a variable is local\n• Why closures exist and how they work\n• What problem is solved by nonlocal\nWith this grounding, we can tackle further decorator topics:\n• Implementing a well-behaved decorator\n• Powerful decorators in the standard library: @cache, @lru_cache, and @single\ndispatch\n• Implementing a parameterized decorator\nWhat’s New in This Chapter\nThe caching decorator functools.cache—new in Python 3.9—is simpler than the\ntraditional functools.lru_cache, so I present it first. The latter is covered in “Using\nlru_cache” on page 323, including the simplified form added in Python 3.8.\n“Single Dispatch Generic Functions” on page 324 was expanded and now uses type\nhints, the preferred way to use functools.singledispatch since Python 3.7.\n“Parameterized Decorators” on page 329 now includes a class-based example,\nExample 9-27.\nI moved Chapter 10, “Design Patterns with First-Class Functions” to the end of\nPart II to improve the flow of the book. “Decorator-Enhanced Strategy Pattern” on\npage 353 is now in that chapter, along with other variations of the Strategy design pat‐\ntern using callables.\nWe start with a very gentle introduction to decorators, and then proceed with the rest\nof the items listed in the chapter opening.\nDecorators 101\nA decorator is a callable that takes another function as an argument (the decorated\nfunction).\nA decorator may perform some processing with the decorated function, and returns\nit or replaces it with another function or callable object.2\n304 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "In other words, assuming an existing decorator named decorate, this code:\n@decorate\ndef target():\n    print('running target()')\nhas the same effect as writing this:\ndef target():\n    print('running target()')\ntarget = decorate(target)\nThe end result is the same: at the end of either of these snippets, the target name is\nbound to whatever function is returned by decorate(target)—which may be the\nfunction initially named target, or may be a different function.\nTo confirm that the decorated function is replaced, see the console session in\nExample 9-1.\nExample 9-1. A decorator usually replaces a function with a different one\n>>> def deco(func):\n...     def inner():\n...         print('running inner()')\n...     return inner  \n...\n>>> @deco\n... def target():  \n...     print('running target()')\n...\n>>> target()  \nrunning inner()\n>>> target  \n<function deco.<locals>.inner at 0x10063b598>\ndeco returns its inner function object.\ntarget is decorated by deco.\nInvoking the decorated target actually runs inner.\nInspection reveals that target is a now a reference to inner.\nStrictly speaking, decorators are just syntactic sugar. As we just saw, you can always\nsimply call a decorator like any regular callable, passing another function. Sometimes\nthat is actually convenient, especially when doing metaprogramming—changing pro‐\ngram behavior at runtime.\nDecorators 101 \n| \n305",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "Three essential facts make a good summary of decorators:\n• A decorator is a function or another callable.\n• A decorator may replace the decorated function with a different one.\n• Decorators are executed immediately when a module is loaded.\nNow let’s focus on the third point.\nWhen Python Executes Decorators\nA key feature of decorators is that they run right after the decorated function is\ndefined. That is usually at import time (i.e., when a module is loaded by Python).\nConsider registration.py in Example 9-2.\nExample 9-2. The registration.py module\nregistry = []  \ndef register(func):  \n    print(f'running register({func})')  \n    registry.append(func)  \n    return func  \n@register  \ndef f1():\n    print('running f1()')\n@register\ndef f2():\n    print('running f2()')\ndef f3():  \n    print('running f3()')\ndef main():  \n    print('running main()')\n    print('registry ->', registry)\n    f1()\n    f2()\n    f3()\nif __name__ == '__main__':\n    main()  \nregistry will hold references to functions decorated by @register.\nregister takes a function as an argument.\n306 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "Display what function is being decorated, for demonstration.\nInclude func in registry.\nReturn func: we must return a function; here we return the same received as\nargument.\nf1 and f2 are decorated by @register.\nf3 is not decorated.\nmain displays the registry, then calls f1(), f2(), and f3().\nmain() is only invoked if registration.py runs as a script.\nThe output of running registration.py as a script looks like this:\n$ python3 registration.py\nrunning register(<function f1 at 0x100631bf8>)\nrunning register(<function f2 at 0x100631c80>)\nrunning main()\nregistry -> [<function f1 at 0x100631bf8>, <function f2 at 0x100631c80>]\nrunning f1()\nrunning f2()\nrunning f3()\nNote that register runs (twice) before any other function in the module. When reg\nister is called, it receives the decorated function object as an argument—for exam‐\nple, <function f1 at 0x100631bf8>.\nAfter the module is loaded, the registry list holds references to the two decorated\nfunctions: f1 and f2. These functions, as well as f3, are only executed when explicitly\ncalled by main.\nIf registration.py is imported (and not run as a script), the output is this:\n>>> import registration\nrunning register(<function f1 at 0x10063b1e0>)\nrunning register(<function f2 at 0x10063b268>)\nAt this time, if you inspect registry, this is what you see:\n>>> registration.registry\n[<function f1 at 0x10063b1e0>, <function f2 at 0x10063b268>]\nThe main point of Example 9-2 is to emphasize that function decorators are executed\nas soon as the module is imported, but the decorated functions only run when they\nare explicitly invoked. This highlights the difference between what Pythonistas call\nimport time and runtime.\nWhen Python Executes Decorators \n| \n307",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "Registration Decorators\nConsidering how decorators are commonly employed in real code, Example 9-2 is\nunusual in two ways:\n• The decorator function is defined in the same module as the decorated functions.\nA real decorator is usually defined in one module and applied to functions in\nother modules.\n• The register decorator returns the same function passed as an argument. In\npractice, most decorators define an inner function and return it.\nEven though the register decorator in Example 9-2 returns the decorated function\nunchanged, that technique is not useless. Similar decorators are used in many Python\nframeworks to add functions to some central registry—for example, a registry map‐\nping URL patterns to functions that generate HTTP responses. Such registration dec‐\norators may or may not change the decorated function.\nWe will see a registration decorator applied in “Decorator-Enhanced Strategy Pat‐\ntern” on page 353 (Chapter 10).\nMost decorators do change the decorated function. They usually do it by defining an\ninner function and returning it to replace the decorated function. Code that uses\ninner functions almost always depends on closures to operate correctly. To under‐\nstand closures, we need to take a step back and review how variable scopes work in\nPython.\nVariable Scope Rules\nIn Example 9-3, we define and test a function that reads two variables: a local variable\na—defined as function parameter—and variable b that is not defined anywhere in the\nfunction.\nExample 9-3. Function reading a local and a global variable\n>>> def f1(a):\n...     print(a)\n...     print(b)\n...\n>>> f1(3)\n3\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in f1\nNameError: global name 'b' is not defined\n308 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "The error we got is not surprising. Continuing from Example 9-3, if we assign a value\nto a global b and then call f1, it works:\n>>> b = 6\n>>> f1(3)\n3\n6\nNow, let’s see an example that may surprise you.\nTake a look at the f2 function in Example 9-4. Its first two lines are the same as f1 in\nExample 9-3, then it makes an assignment to b. But it fails at the second print, before\nthe assignment is made.\nExample 9-4. Variable b is local, because it is assigned a value in the body of the\nfunction\n>>> b = 6\n>>> def f2(a):\n...     print(a)\n...     print(b)\n...     b = 9\n...\n>>> f2(3)\n3\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in f2\nUnboundLocalError: local variable 'b' referenced before assignment\nNote that the output starts with 3, which proves that the print(a) statement was exe‐\ncuted. But the second one, print(b), never runs. When I first saw this I was sur‐\nprised, thinking that 6 should be printed, because there is a global variable b and the\nassignment to the local b is made after print(b).\nBut the fact is, when Python compiles the body of the function, it decides that b is a\nlocal variable because it is assigned within the function. The generated bytecode\nreflects this decision and will try to fetch b from the local scope. Later, when the call\nf2(3) is made, the body of f2 fetches and prints the value of the local variable a, but\nwhen trying to fetch the value of local variable b, it discovers that b is unbound.\nThis is not a bug, but a design choice: Python does not require you to declare vari‐\nables, but assumes that a variable assigned in the body of a function is local. This is\nmuch better than the behavior of JavaScript, which does not require variable declara‐\ntions either, but if you do forget to declare that a variable is local (with var), you may\nclobber a global variable without knowing.\nVariable Scope Rules \n| \n309",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "If we want the interpreter to treat b as a global variable and still assign a new value to\nit within the function, we use the global declaration:\n>>> b = 6\n>>> def f3(a):\n...     global b\n...     print(a)\n...     print(b)\n...     b = 9\n...\n>>> f3(3)\n3\n6\n>>> b\n9\nIn the preceding examples, we can see two scopes in action:\nThe module global scope\nMade of names assigned to values outside of any class or function block.\nThe f3 function local scope\nMade of names assigned to values as parameters, or directly in the body of the\nfunction.\nThere is one other scope where variables can come from, which we call nonlocal and\nis fundamental for closures; we’ll see it in a bit.\nAfter this closer look at how variable scopes work in Python, we can tackle closures in\nthe next section, “Closures” on page 311. If you are curious about the bytecode differ‐\nences between the functions in Examples 9-3 and 9-4, see the following sidebar.\nComparing Bytecodes\nThe dis module provides an easy way to disassemble the bytecode of Python func‐\ntions. Read Examples 9-5 and 9-6 to see the bytecodes for f1 and f2 from Examples\n9-3 and 9-4.\nExample 9-5. Disassembly of the f1 function from Example 9-3\n>>> from dis import dis\n>>> dis(f1)\n  2           0 LOAD_GLOBAL              0 (print)  \n              3 LOAD_FAST                0 (a)  \n              6 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n              9 POP_TOP\n  3          10 LOAD_GLOBAL              0 (print)\n             13 LOAD_GLOBAL              1 (b)  \n             16 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n310 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "19 POP_TOP\n             20 LOAD_CONST               0 (None)\n             23 RETURN_VALUE\nLoad global name print.\nLoad local name a.\nLoad global name b.\nContrast the bytecode for f1 shown in Example 9-5 with the bytecode for f2 in\nExample 9-6.\nExample 9-6. Disassembly of the f2 function from Example 9-4\n>>> dis(f2)\n  2           0 LOAD_GLOBAL              0 (print)\n              3 LOAD_FAST                0 (a)\n              6 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n              9 POP_TOP\n  3          10 LOAD_GLOBAL              0 (print)\n             13 LOAD_FAST                1 (b)  \n             16 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n             19 POP_TOP\n  4          20 LOAD_CONST               1 (9)\n             23 STORE_FAST               1 (b)\n             26 LOAD_CONST               0 (None)\n             29 RETURN_VALUE\nLoad local name b. This shows that the compiler considers b a local variable, even\nif the assignment to b occurs later, because the nature of the variable—whether it\nis local or not—cannot change in the body of the function.\nThe CPython virtual machine (VM) that runs the bytecode is a stack machine, so\nLOAD and POP operations refer to the stack. It is beyond the scope of this book to fur‐\nther describe the Python opcodes, but they are documented along with the dis mod‐\nule in “dis—Disassembler for Python bytecode”.\nClosures\nIn the blogosphere, closures are sometimes confused with anonymous functions.\nMany confuse them because of the parallel history of those features: defining func‐\ntions inside functions is not so common or convenient, until you have anonymous\nfunctions. And closures only matter when you have nested functions. So a lot of peo‐\nple learn both concepts at the same time.\nClosures \n| \n311",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "Actually, a closure is a function—let’s call it f—with an extended scope that encom‐\npasses variables referenced in the body of f that are not global variables or local vari‐\nables of f. Such variables must come from the local scope of an outer function that\nencompasses f.\nIt does not matter whether the function is anonymous or not; what matters is that it\ncan access nonglobal variables that are defined outside of its body.\nThis is a challenging concept to grasp, and is better approached through an example.\nConsider an avg function to compute the mean of an ever-growing series of values;\nfor example, the average closing price of a commodity over its entire history. Every\nday a new price is added, and the average is computed taking into account all prices\nso far.\nStarting with a clean slate, this is how avg could be used:\n>>> avg(10)\n10.0\n>>> avg(11)\n10.5\n>>> avg(12)\n11.0\nWhere does avg come from, and where does it keep the history of previous values?\nFor starters, Example 9-7 is a class-based implementation.\nExample 9-7. average_oo.py: a class to calculate a running average\nclass Averager():\n    def __init__(self):\n        self.series = []\n    def __call__(self, new_value):\n        self.series.append(new_value)\n        total = sum(self.series)\n        return total / len(self.series)\nThe Averager class creates instances that are callable:\n>>> avg = Averager()\n>>> avg(10)\n10.0\n>>> avg(11)\n10.5\n>>> avg(12)\n11.0\n312 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "Now, Example 9-8 is a functional implementation, using the higher-order function\nmake_averager.\nExample 9-8. average.py: a higher-order function to calculate a running average\ndef make_averager():\n    series = []\n    def averager(new_value):\n        series.append(new_value)\n        total = sum(series)\n        return total / len(series)\n    return averager\nWhen invoked, make_averager returns an averager function object. Each time an\naverager is called, it appends the passed argument to the series, and computes the\ncurrent average, as shown in Example 9-9.\nExample 9-9. Testing Example 9-8\n>>> avg = make_averager()\n>>> avg(10)\n10.0\n>>> avg(11)\n10.5\n>>> avg(15)\n12.0\nNote the similarities of the examples: we call Averager() or make_averager() to get\na callable object avg that will update the historical series and calculate the current\nmean. In Example 9-7, avg is an instance of Averager, and in Example 9-8, it is the\ninner function, averager. Either way, we just call avg(n) to include n in the series\nand get the updated mean.\nIt’s obvious where the avg of the Averager class keeps the history: the self.series\ninstance attribute. But where does the avg function in the second example find the\nseries?\nNote that series is a local variable of make_averager because the assignment series\n= [] happens in the body of that function. But when avg(10) is called,\nmake_averager has already returned, and its local scope is long gone.\nWithin averager, series is a free variable. This is a technical term meaning a vari‐\nable that is not bound in the local scope. See Figure 9-1.\nClosures \n| \n313",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "Figure 9-1. The closure for averager extends the scope of that function to include the\nbinding for the free variable series.\nInspecting the returned averager object shows how Python keeps the names of local\nand free variables in the __code__ attribute that represents the compiled body of the\nfunction. Example 9-10 demonstrates.\nExample 9-10. Inspecting the function created by make_averager in Example 9-8\n>>> avg.__code__.co_varnames\n('new_value', 'total')\n>>> avg.__code__.co_freevars\n('series',)\nThe value for series is kept in the __closure__ attribute of the returned function\navg. Each item in avg.__closure__ corresponds to a name in avg. __code__\n.co_freevars. These items are cells, and they have an attribute called cell_con\ntents where the actual value can be found. Example 9-11 shows these attributes.\nExample 9-11. Continuing from Example 9-9\n>>> avg.__code__.co_freevars\n('series',)\n>>> avg.__closure__\n(<cell at 0x107a44f78: list object at 0x107a91a48>,)\n>>> avg.__closure__[0].cell_contents\n[10, 11, 12]\nTo summarize: a closure is a function that retains the bindings of the free variables\nthat exist when the function is defined, so that they can be used later when the func‐\ntion is invoked and the defining scope is no longer available.\nNote that the only situation in which a function may need to deal with external vari‐\nables that are nonglobal is when it is nested in another function and those variables\nare part of the local scope of the outer function.\n314 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "The nonlocal Declaration\nOur previous implementation of make_averager was not efficient. In Example 9-8,\nwe stored all the values in the historical series and computed their sum every time\naverager was called. A better implementation would only store the total and the\nnumber of items so far, and compute the mean from these two numbers.\nExample 9-12 is a broken implementation, just to make a point. Can you see where it\nbreaks?\nExample 9-12. A broken higher-order function to calculate a running average without\nkeeping all history\ndef make_averager():\n    count = 0\n    total = 0\n    def averager(new_value):\n        count += 1\n        total += new_value\n        return total / count\n    return averager\nIf you try Example 9-12, here is what you get:\n>>> avg = make_averager()\n>>> avg(10)\nTraceback (most recent call last):\n  ...\nUnboundLocalError: local variable 'count' referenced before assignment\n>>>\nThe problem is that the statement count += 1 actually means the same as count =\ncount + 1, when count is a number or any immutable type. So we are actually\nassigning to count in the body of averager, and that makes it a local variable. The\nsame problem affects the total variable.\nWe did not have this problem in Example 9-8 because we never assigned to the ser\nies name; we only called series.append and invoked sum and len on it. So we took\nadvantage of the fact that lists are mutable.\nBut with immutable types like numbers, strings, tuples, etc., all you can do is read,\nnever update. If you try to rebind them, as in count = count + 1, then you are\nimplicitly creating a local variable count. It is no longer a free variable, and therefore\nit is not saved in the closure.\nThe nonlocal Declaration \n| \n315",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "3 Thanks to tech reviewer Leonardo Rochael for suggesting this summary.\n4 Python does not have a program global scope, only module global scopes.\nTo work around this, the nonlocal keyword was introduced in Python 3. It lets you\ndeclare a variable as a free variable even when it is assigned within the function. If a\nnew value is assigned to a nonlocal variable, the binding stored in the closure is\nchanged. A correct implementation of our newest make_averager looks like\nExample 9-13.\nExample 9-13. Calculate a running average without keeping all history (fixed with the\nuse of nonlocal)\ndef make_averager():\n    count = 0\n    total = 0\n    def averager(new_value):\n        nonlocal count, total\n        count += 1\n        total += new_value\n        return total / count\n    return averager\nAfter studying the use of nonlocal, let’s summarize how Python’s variable lookup\nworks.\nVariable Lookup Logic\nWhen a function is defined, the Python bytecode compiler determines how to fetch a\nvariable x that appears in it, based on these rules:3\n• If there is a global x declaration, x comes from and is assigned to the x global\nvariable module.4\n• If there is a nonlocal x declaration, x comes from and is assigned to the x local\nvariable of the nearest surrounding function where x is defined.\n• If x is a parameter or is assigned a value in the function body, then x is the local\nvariable.\n• If x is referenced but is not assigned and is not a parameter:\n— x will be looked up in the local scopes of the surrounding function bodies\n(nonlocal scopes).\n316 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1590,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "— If not found in surrounding scopes, it will be read from the module global\nscope.\n— If not found in the global scope, it will be read from __builtins__.__dict__.\nNow that we have Python closures covered, we can effectively implement decorators\nwith nested functions.\nImplementing a Simple Decorator\nExample 9-14 is a decorator that clocks every invocation of the decorated function\nand displays the elapsed time, the arguments passed, and the result of the call.\nExample 9-14. clockdeco0.py: simple decorator to show the running time of functions\nimport time\ndef clock(func):\n    def clocked(*args):  \n        t0 = time.perf_counter()\n        result = func(*args)  \n        elapsed = time.perf_counter() - t0\n        name = func.__name__\n        arg_str = ', '.join(repr(arg) for arg in args)\n        print(f'[{elapsed:0.8f}s] {name}({arg_str}) -> {result!r}')\n        return result\n    return clocked  \nDefine inner function clocked to accept any number of positional arguments.\nThis line only works because the closure for clocked encompasses the func free\nvariable.\nReturn the inner function to replace the decorated function.\nExample 9-15 demonstrates the use of the clock decorator.\nExample 9-15. Using the clock decorator\nimport time\nfrom clockdeco0 import clock\n@clock\ndef snooze(seconds):\n    time.sleep(seconds)\n@clock\nImplementing a Simple Decorator \n| \n317",
      "content_length": 1369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "def factorial(n):\n    return 1 if n < 2 else n*factorial(n-1)\nif __name__ == '__main__':\n    print('*' * 40, 'Calling snooze(.123)')\n    snooze(.123)\n    print('*' * 40, 'Calling factorial(6)')\n    print('6! =', factorial(6))\nThe output of running Example 9-15 looks like this:\n$ python3 clockdeco_demo.py\n**************************************** Calling snooze(.123)\n[0.12363791s] snooze(0.123) -> None\n**************************************** Calling factorial(6)\n[0.00000095s] factorial(1) -> 1\n[0.00002408s] factorial(2) -> 2\n[0.00003934s] factorial(3) -> 6\n[0.00005221s] factorial(4) -> 24\n[0.00006390s] factorial(5) -> 120\n[0.00008297s] factorial(6) -> 720\n6! = 720\nHow It Works\nRemember that this code:\n@clock\ndef factorial(n):\n    return 1 if n < 2 else n*factorial(n-1)\nactually does this:\ndef factorial(n):\n    return 1 if n < 2 else n*factorial(n-1)\nfactorial = clock(factorial)\nSo, in both examples, clock gets the factorial function as its func argument (see\nExample 9-14). It then creates and returns the clocked function, which the Python\ninterpreter assigns to factorial (behind the scenes, in the first example). In fact, if\nyou import the clockdeco_demo module and check the __name__ of factorial, this\nis what you get:\n>>> import clockdeco_demo\n>>> clockdeco_demo.factorial.__name__\n'clocked'\n>>>\nSo factorial now actually holds a reference to the clocked function. From now on,\neach time factorial(n) is called, clocked(n) gets executed. In essence, clocked\ndoes the following:\n318 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "1. Records the initial time t0.\n2. Calls the original factorial function, saving the result.\n3. Computes the elapsed time.\n4. Formats and displays the collected data.\n5. Returns the result saved in step 2.\nThis is the typical behavior of a decorator: it replaces the decorated function with a\nnew function that accepts the same arguments and (usually) returns whatever the\ndecorated function was supposed to return, while also doing some extra processing.\nIn Design Patterns by Gamma et al., the short description of the\ndecorator pattern starts with: “Attach additional responsibilities to\nan object dynamically.” Function decorators fit that description.\nBut at the implementation level, Python decorators bear little\nresemblance to the classic decorator described in the original\nDesign Patterns work. “Soapbox” on page 338 has more on this\nsubject.\nThe clock decorator implemented in Example 9-14 has a few shortcomings: it does\nnot support keyword arguments, and it masks the __name__ and __doc__ of the deco‐\nrated function. Example 9-16 uses the functools.wraps decorator to copy the rele‐\nvant attributes from func to clocked. Also, in this new version, keyword arguments\nare correctly handled.\nExample 9-16. clockdeco.py: an improved clock decorator\nimport time\nimport functools\ndef clock(func):\n    @functools.wraps(func)\n    def clocked(*args, **kwargs):\n        t0 = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed = time.perf_counter() - t0\n        name = func.__name__\n        arg_lst = [repr(arg) for arg in args]\n        arg_lst.extend(f'{k}={v!r}' for k, v in kwargs.items())\n        arg_str = ', '.join(arg_lst)\n        print(f'[{elapsed:0.8f}s] {name}({arg_str}) -> {result!r}')\n        return result\n    return clocked\nImplementing a Simple Decorator \n| \n319",
      "content_length": 1805,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "5 To clarify, this is not a typo: memoization is a computer science term vaguely related to “memorization,” but\nnot the same.\nfunctools.wraps is just one of the ready-to-use decorators in the standard library. In\nthe next section, we’ll meet the most impressive decorator that functools provides:\ncache.\nDecorators in the Standard Library\nPython has three built-in functions that are designed to decorate methods: property,\nclassmethod, and staticmethod. We’ll discuss property in “Using a Property for\nAttribute Validation” on page 857 and the others in “classmethod Versus staticmethod”\non page 369.\nIn Example 9-16 we saw another important decorator: functools.wraps, a helper for\nbuilding well-behaved decorators. Some of the most interesting decorators in the\nstandard library are cache, lru_cache, and singledispatch—all from the functools\nmodule. We’ll cover them next.\nMemoization with functools.cache\nThe functools.cache decorator implements memoization:5 an optimization techni‐\nque that works by saving the results of previous invocations of an expensive function,\navoiding repeat computations on previously used arguments.\nfunctools.cache was added in Python 3.9. If you need to run\nthese examples in Python 3.8, replace @cache with @lru_cache. For\nprior versions of Python, you must invoke the decorator, writing\n@lru_cache(), as explained in “Using lru_cache” on page 323.\nA good demonstration is to apply @cache to the painfully slow recursive function to\ngenerate the nth number in the Fibonacci sequence, as shown in Example 9-17.\nExample 9-17. The very costly recursive way to compute the nth number in the\nFibonacci series\nfrom clockdeco import clock\n@clock\ndef fibonacci(n):\n    if n < 2:\n        return n\n320 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "return fibonacci(n - 2) + fibonacci(n - 1)\nif __name__ == '__main__':\n    print(fibonacci(6))\nHere is the result of running fibo_demo.py. Except for the last line, all output is gen‐\nerated by the clock decorator:\n$ python3 fibo_demo.py\n[0.00000042s] fibonacci(0) -> 0\n[0.00000049s] fibonacci(1) -> 1\n[0.00006115s] fibonacci(2) -> 1\n[0.00000031s] fibonacci(1) -> 1\n[0.00000035s] fibonacci(0) -> 0\n[0.00000030s] fibonacci(1) -> 1\n[0.00001084s] fibonacci(2) -> 1\n[0.00002074s] fibonacci(3) -> 2\n[0.00009189s] fibonacci(4) -> 3\n[0.00000029s] fibonacci(1) -> 1\n[0.00000027s] fibonacci(0) -> 0\n[0.00000029s] fibonacci(1) -> 1\n[0.00000959s] fibonacci(2) -> 1\n[0.00001905s] fibonacci(3) -> 2\n[0.00000026s] fibonacci(0) -> 0\n[0.00000029s] fibonacci(1) -> 1\n[0.00000997s] fibonacci(2) -> 1\n[0.00000028s] fibonacci(1) -> 1\n[0.00000030s] fibonacci(0) -> 0\n[0.00000031s] fibonacci(1) -> 1\n[0.00001019s] fibonacci(2) -> 1\n[0.00001967s] fibonacci(3) -> 2\n[0.00003876s] fibonacci(4) -> 3\n[0.00006670s] fibonacci(5) -> 5\n[0.00016852s] fibonacci(6) -> 8\n8\nThe waste is obvious: fibonacci(1) is called eight times, fibonacci(2) five times,\netc. But adding just two lines to use cache, performance is much improved. See\nExample 9-18.\nExample 9-18. Faster implementation using caching\nimport functools\nfrom clockdeco import clock\n@functools.cache  \n@clock  \ndef fibonacci(n):\nDecorators in the Standard Library \n| \n321",
      "content_length": 1398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "if n < 2:\n        return n\n    return fibonacci(n - 2) + fibonacci(n - 1)\nif __name__ == '__main__':\n    print(fibonacci(6))\nThis line works with Python 3.9 or later. See “Using lru_cache” on page 323 for\nalternatives supporting earlier versions of Python.\nThis is an example of stacked decorators: @cache is applied on the function\nreturned by @clock.\nStacked Decorators\nTo make sense of stacked decorators, recall that the @ is syntax\nsugar for applying the decorator function to the function below it.\nIf there’s more than one decorator, they behave like nested func‐\ntion calls. This:\n@alpha\n@beta\ndef my_fn():\n    ...\nis the same as this:\nmy_fn = alpha(beta(my_fn))\nIn other words, the beta decorator is applied first, and the function\nit returns is then passed to alpha.\nUsing cache in Example 9-18, the fibonacci function is called only once for each\nvalue of n:\n$ python3 fibo_demo_lru.py\n[0.00000043s] fibonacci(0) -> 0\n[0.00000054s] fibonacci(1) -> 1\n[0.00006179s] fibonacci(2) -> 1\n[0.00000070s] fibonacci(3) -> 2\n[0.00007366s] fibonacci(4) -> 3\n[0.00000057s] fibonacci(5) -> 5\n[0.00008479s] fibonacci(6) -> 8\n8\nIn another test, to compute fibonacci(30), Example 9-18 made the 31 calls needed\nin 0.00017s (total time), while the uncached Example 9-17 took 12.09s on an Intel\nCore i7 notebook, because it called fibonacci(1) 832,040 times, in a total of\n2,692,537 calls.\n322 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "All the arguments taken by the decorated function must be hashable, because the\nunderlying lru_cache uses a dict to store the results, and the keys are made from the\npositional and keyword arguments used in the calls.\nBesides making silly recursive algorithms viable, @cache really shines in applications\nthat need to fetch information from remote APIs.\nfunctools.cache can consume all available memory if there is a\nvery large number of cache entries. I consider it more suitable for\nuse in short-lived command-line scripts. In long-running pro‐\ncesses, I recommend using functools.lru_cache with a suitable\nmaxsize parameter, as explained in the next section.\nUsing lru_cache\nThe functools.cache decorator is actually a simple wrapper around the older func\ntools.lru_cache function, which is more flexible and compatible with Python 3.8\nand earlier versions.\nThe main advantage of @lru_cache is that its memory usage is bounded by the\nmaxsize parameter, which has a rather conservative default value of 128—which\nmeans the cache will hold at most 128 entries at any time.\nThe acronym LRU stands for Least Recently Used, meaning that older entries that\nhave not been read for a while are discarded to make room for new ones.\nSince Python 3.8, lru_cache can be applied in two ways. This is how to use it in the\nsimplest way:\n@lru_cache\ndef costly_function(a, b):\n    ...\nThe other way—available since Python 3.2—is to invoke it as a function, with ():\n@lru_cache()\ndef costly_function(a, b):\n    ...\nIn both cases, the default parameters would be used. These are:\nmaxsize=128\nSets the maximum number of entries to be stored. After the cache is full, the least\nrecently used entry is discarded to make room for each new entry. For optimal\nperformance, maxsize should be a power of 2. If you pass maxsize=None, the\nLRU logic is disabled, so the cache works faster but entries are never discarded,\nwhich may consume too much memory. That’s what @functools.cache does.\nDecorators in the Standard Library \n| \n323",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "typed=False\nDetermines whether the results of different argument types are stored separately.\nFor example, in the default setting, float and integer arguments that are consid‐\nered equal are stored only once, so there would be a single entry for the calls f(1)\nand f(1.0). If typed=True, those arguments would produce different entries,\npossibly storing distinct results.\nHere is an example invoking @lru_cache with nondefault parameters:\n@lru_cache(maxsize=2**20, typed=True)\ndef costly_function(a, b):\n    ...\nNow let’s study another powerful decorator: functools.singledispatch.\nSingle Dispatch Generic Functions\nImagine we are creating a tool to debug web applications. We want to generate\nHTML displays for different types of Python objects.\nWe could start with a function like this:\nimport html\ndef htmlize(obj):\n    content = html.escape(repr(obj))\n    return f'<pre>{content}</pre>'\nThat will work for any Python type, but now we want to extend it to generate custom\ndisplays for some types. Some examples:\nstr\nReplace embedded newline characters with '<br/>\\n' and use <p> tags instead\nof <pre>.\nint\nShow the number in decimal and hexadecimal (with a special case for bool).\nlist\nOutput an HTML list, formatting each item according to its type.\nfloat and Decimal\nOutput the value as usual, but also in the form of a fraction (why not?).\nThe behavior we want is shown in Example 9-19.\n324 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "Example 9-19. htmlize() generates HTML tailored to different object types\n>>> htmlize({1, 2, 3})  \n'<pre>{1, 2, 3}</pre>'\n>>> htmlize(abs)\n'<pre>&lt;built-in function abs&gt;</pre>'\n>>> htmlize('Heimlich & Co.\\n- a game')  \n'<p>Heimlich &amp; Co.<br/>\\n- a game</p>'\n>>> htmlize(42)  \n'<pre>42 (0x2a)</pre>'\n>>> print(htmlize(['alpha', 66, {3, 2, 1}]))  \n<ul>\n<li><p>alpha</p></li>\n<li><pre>66 (0x42)</pre></li>\n<li><pre>{1, 2, 3}</pre></li>\n</ul>\n>>> htmlize(True)  \n'<pre>True</pre>'\n>>> htmlize(fractions.Fraction(2, 3))  \n'<pre>2/3</pre>'\n>>> htmlize(2/3)   \n'<pre>0.6666666666666666 (2/3)</pre>'\n>>> htmlize(decimal.Decimal('0.02380952'))\n'<pre>0.02380952 (1/42)</pre>'\nThe original function is registered for object, so it serves as a catch-all to handle\nargument types that don’t match the other implementations.\nstr objects are also HTML-escaped but wrapped in <p></p>, with <br/> line\nbreaks inserted before each '\\n'.\nAn int is shown in decimal and hexadecimal, inside <pre></pre>.\nEach list item is formatted according to its type, and the whole sequence is ren‐\ndered as an HTML list.\nAlthough bool is an int subtype, it gets special treatment.\nShow Fraction as a fraction.\nShow float and Decimal with an approximate fractional equivalent.\nFunction singledispatch\nBecause we don’t have Java-style method overloading in Python, we can’t simply cre‐\nate variations of htmlize with different signatures for each data type we want to han‐\ndle differently. A possible solution in Python would be to turn htmlize into a\nDecorators in the Standard Library \n| \n325",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "dispatch function, with a chain of if/elif/… or match/case/… calling specialized\nfunctions like htmlize_str, htmlize_int, etc. This is not extensible by users of our\nmodule, and is unwieldy: over time, the htmlize dispatcher would become too big,\nand the coupling between it and the specialized functions would be very tight.\nThe functools.singledispatch decorator allows different modules to contribute to\nthe overall solution, and lets you easily provide specialized functions even for types\nthat belong to third-party packages that you can’t edit. If you decorate a plain func‐\ntion with @singledispatch, it becomes the entry point for a generic function: a group\nof functions to perform the same operation in different ways, depending on the type\nof the first argument. This is what is meant by the term single dispatch. If more argu‐\nments were used to select the specific functions, we’d have multiple dispatch.\nExample 9-20 shows how.\nfunctools.singledispatch exists since Python 3.4, but it only\nsupports type hints since Python 3.7. The last two functions in\nExample 9-20 illustrate the syntax that works in all versions of\nPython since 3.4.\nExample 9-20. @singledispatch creates a custom @htmlize.register to bundle\nseveral functions into a generic function\nfrom functools import singledispatch\nfrom collections import abc\nimport fractions\nimport decimal\nimport html\nimport numbers\n@singledispatch  \ndef htmlize(obj: object) -> str:\n    content = html.escape(repr(obj))\n    return f'<pre>{content}</pre>'\n@htmlize.register  \ndef _(text: str) -> str:  \n    content = html.escape(text).replace('\\n', '<br/>\\n')\n    return f'<p>{content}</p>'\n@htmlize.register  \ndef _(seq: abc.Sequence) -> str:\n    inner = '</li>\\n<li>'.join(htmlize(item) for item in seq)\n    return '<ul>\\n<li>' + inner + '</li>\\n</ul>'\n@htmlize.register  \ndef _(n: numbers.Integral) -> str:\n    return f'<pre>{n} (0x{n:x})</pre>'\n326 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "6 Unfortunately, Mypy 0.770 complains when it sees multiple functions with the same name.\n7 Despite the warning in “The fall of the numeric tower” on page 279, the number ABCs are not deprecated and\nyou find them in Python 3 code.\n@htmlize.register  \ndef _(n: bool) -> str:\n    return f'<pre>{n}</pre>'\n@htmlize.register(fractions.Fraction)  \ndef _(x) -> str:\n    frac = fractions.Fraction(x)\n    return f'<pre>{frac.numerator}/{frac.denominator}</pre>'\n@htmlize.register(decimal.Decimal)  \n@htmlize.register(float)\ndef _(x) -> str:\n    frac = fractions.Fraction(x).limit_denominator()\n    return f'<pre>{x} ({frac.numerator}/{frac.denominator})</pre>'\n@singledispatch marks the base function that handles the object type.\nEach specialized function is decorated with @«base».register.\nThe type of the first argument given at runtime determines when this particular\nfunction definition will be used. The name of the specialized functions is irrele‐\nvant; _ is a good choice to make this clear.6\nFor each additional type to get special treatment, register a new function with a\nmatching type hint in the first parameter.\nThe numbers ABCs are useful for use with singledispatch.7\nbool is a subtype-of numbers.Integral, but the singledispatch logic seeks the\nimplementation with the most specific matching type, regardless of the order\nthey appear in the code.\nDecorators in the Standard Library \n| \n327",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "8 Maybe one day you’ll also be able to express this with single unparameterized @htmlize.register and type\nhint using Union, but when I tried, Python raised a TypeError with a message saying that Union is not a class.\nSo, although PEP 484 syntax is supported by @singledispatch, the semantics are not there yet.\n9 NumPy, for example, implements several machine-oriented integer and floating-point types.\nIf you don’t want to, or cannot, add type hints to the decorated function, you can\npass a type to the @«base».register decorator. This syntax works in Python 3.4\nor later.\nThe @«base».register decorator returns the undecorated function, so it’s possi‐\nble to stack them to register two or more types on the same implementation.8\nWhen possible, register the specialized functions to handle ABCs (abstract classes)\nsuch as numbers.Integral and abc.MutableSequence, instead of concrete imple‐\nmentations like int and list. This allows your code to support a greater variety of\ncompatible types. For example, a Python extension can provide alternatives to the\nint type with fixed bit lengths as subclasses of numbers.Integral.9\nUsing ABCs or typing.Protocol with @singledispatch allows\nyour code to support existing or future classes that are actual or\nvirtual subclasses of those ABCs, or that implement those proto‐\ncols. The use of ABCs and the concept of a virtual subclass are sub‐\njects of Chapter 13.\nA notable quality of the singledispatch mechanism is that you can register special‐\nized functions anywhere in the system, in any module. If you later add a module with\na new user-defined type, you can easily provide a new custom function to handle that\ntype. And you can write custom functions for classes that you did not write and can’t\nchange.\nsingledispatch is a well-thought-out addition to the standard library, and it offers\nmore features than I can describe here. PEP 443—Single-dispatch generic functions is\na good reference, but it doesn’t mention the use of type hints, which were added later.\nThe functools module documentation has improved and has more up-to-date cov‐\nerage with several examples in its singledispatch entry.\n328 \n| \nChapter 9: Decorators and Closures",
      "content_length": 2190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "@singledispatch is not designed to bring Java-style method over‐\nloading to Python. A single class with many overloaded variations\nof a method is better than a single function with a lengthy stretch\nof if/elif/elif/elif blocks. But both solutions are flawed\nbecause they concentrate too much responsibility in a single code\nunit—the class or the function. The advantage of @singledispatch\nis supporting modular extension: each module can register a speci‐\nalized function for each type it supports. In a realistic use case, you\nwould not have all the implementations of generic functions in the\nsame module as in Example 9-20.\nWe’ve seen some decorators that take arguments, for example, @lru_cache() and\nhtmlize.register(float), created by @singledispatch in Example 9-20. The next\nsection shows how to build decorators that accept parameters.\nParameterized Decorators\nWhen parsing a decorator in source code, Python takes the decorated function and\npasses it as the first argument to the decorator function. So how do you make a deco‐\nrator accept other arguments? The answer is: make a decorator factory that takes\nthose arguments and returns a decorator, which is then applied to the function to be\ndecorated. Confusing? Sure. Let’s start with an example based on the simplest decora‐\ntor we’ve seen: register in Example 9-21.\nExample 9-21. Abridged registration.py module from Example 9-2, repeated here for\nconvenience\nregistry = []\ndef register(func):\n    print(f'running register({func})')\n    registry.append(func)\n    return func\n@register\ndef f1():\n    print('running f1()')\nprint('running main()')\nprint('registry ->', registry)\nf1()\nA Parameterized Registration Decorator\nTo make it easy to enable or disable the function registration performed by register,\nwe’ll make it accept an optional active parameter which, if False, skips registering\nParameterized Decorators \n| \n329",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "the decorated function. Example 9-22 shows how. Conceptually, the new register\nfunction is not a decorator but a decorator factory. When called, it returns the actual\ndecorator that will be applied to the target function.\nExample 9-22. To accept parameters, the new register decorator must be called as a\nfunction\nregistry = set()  \ndef register(active=True):  \n    def decorate(func):  \n        print('running register'\n              f'(active={active})->decorate({func})')\n        if active:   \n            registry.add(func)\n        else:\n            registry.discard(func)  \n        return func  \n    return decorate  \n@register(active=False)  \ndef f1():\n    print('running f1()')\n@register()  \ndef f2():\n    print('running f2()')\ndef f3():\n    print('running f3()')\nregistry is now a set, so adding and removing functions is faster.\nregister takes an optional keyword argument.\nThe decorate inner function is the actual decorator; note how it takes a function\nas an argument.\nRegister func only if the active argument (retrieved from the closure) is True.\nIf not active and func in registry, remove it.\nBecause decorate is a decorator, it must return a function.\nregister is our decorator factory, so it returns decorate.\n330 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "The @register factory must be invoked as a function, with the desired\nparameters.\nIf no parameters are passed, register must still be called as a function—@regis\nter()—i.e., to return the actual decorator, decorate.\nThe main point is that register() returns decorate, which is then applied to the\ndecorated function.\nThe code in Example 9-22 is in a registration_param.py module. If we import it, this\nis what we get:\n>>> import registration_param\nrunning register(active=False)->decorate(<function f1 at 0x10063c1e0>)\nrunning register(active=True)->decorate(<function f2 at 0x10063c268>)\n>>> registration_param.registry\n[<function f2 at 0x10063c268>]\nNote how only the f2 function appears in the registry; f1 does not appear because\nactive=False was passed to the register decorator factory, so the decorate that\nwas applied to f1 did not add it to the registry.\nIf, instead of using the @ syntax, we used register as a regular function, the syntax\nneeded to decorate a function f would be register()(f) to add f to the registry, or\nregister(active=False)(f) to not add it (or remove it). See Example 9-23 for a\ndemo of adding and removing functions to the registry.\nExample 9-23. Using the registration_param module listed in Example 9-22\n>>> from registration_param import *\nrunning register(active=False)->decorate(<function f1 at 0x10073c1e0>)\nrunning register(active=True)->decorate(<function f2 at 0x10073c268>)\n>>> registry  \n{<function f2 at 0x10073c268>}\n>>> register()(f3)  \nrunning register(active=True)->decorate(<function f3 at 0x10073c158>)\n<function f3 at 0x10073c158>\n>>> registry  \n{<function f3 at 0x10073c158>, <function f2 at 0x10073c268>}\n>>> register(active=False)(f2)  \nrunning register(active=False)->decorate(<function f2 at 0x10073c268>)\n<function f2 at 0x10073c268>\n>>> registry  \n{<function f3 at 0x10073c158>}\nWhen the module is imported, f2 is in the registry.\nThe register() expression returns decorate, which is then applied to f3.\nParameterized Decorators \n| \n331",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "The previous line added f3 to the registry.\nThis call removes f2 from the registry.\nConfirm that only f3 remains in the registry.\nThe workings of parameterized decorators are fairly involved, and the one we’ve just\ndiscussed is simpler than most. Parameterized decorators usually replace the decora‐\nted function, and their construction requires yet another level of nesting. Now we will\nexplore the architecture of one such function pyramid.\nThe Parameterized Clock Decorator\nIn this section, we’ll revisit the clock decorator, adding a feature: users may pass a\nformat string to control the output of the clocked function report. See Example 9-24.\nFor simplicity, Example 9-24 is based on the initial clock imple‐\nmentation from Example 9-14, and not the improved one from\nExample 9-16 that uses @functools.wraps, adding yet another\nfunction layer.\nExample 9-24. Module clockdeco_param.py: the parameterized clock decorator\nimport time\nDEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -> {result}'\ndef clock(fmt=DEFAULT_FMT):  \n    def decorate(func):      \n        def clocked(*_args): \n            t0 = time.perf_counter()\n            _result = func(*_args)  \n            elapsed = time.perf_counter() - t0\n            name = func.__name__\n            args = ', '.join(repr(arg) for arg in _args)  \n            result = repr(_result)  \n            print(fmt.format(**locals()))  \n            return _result  \n        return clocked  \n    return decorate  \nif __name__ == '__main__':\n    @clock()  \n    def snooze(seconds):\n        time.sleep(seconds)\n332 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "10 Tech reviewer Miroslav Šedivý noted: “It also means that code linters will complain about unused variables\nsince they tend to ignore uses of locals().” Yes, that’s yet another example of how static checking tools dis‐\ncourage the use of the dynamic features that attracted me and countless programmers to Python in the first\nplace. To make the linter happy, I could spell out each local variable twice in the call: fmt.format(elapsed=\nelapsed, name=name, args=args, result=result). I’d rather not. If you use static checking tools, it’s very\nimportant to know when to ignore them.\n    for i in range(3):\n        snooze(.123)\nclock is our parameterized decorator factory.\ndecorate is the actual decorator.\nclocked wraps the decorated function.\n_result is the actual result of the decorated function.\n_args holds the actual arguments of clocked, while args is str used for display.\nresult is the str representation of _result, for display.\nUsing **locals() here allows any local variable of clocked to be referenced in\nthe fmt.10\nclocked will replace the decorated function, so it should return whatever that\nfunction returns.\ndecorate returns clocked.\nclock returns decorate.\nIn this self test, clock() is called without arguments, so the decorator applied\nwill use the default format str.\nIf you run Example 9-24 from the shell, this is what you get:\n$ python3 clockdeco_param.py\n[0.12412500s] snooze(0.123) -> None\n[0.12411904s] snooze(0.123) -> None\n[0.12410498s] snooze(0.123) -> None\nTo exercise the new functionality, let’s have a look at Examples 9-25 and 9-26, which\nare two other modules using clockdeco_param, and the outputs they generate.\nParameterized Decorators \n| \n333",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "Example 9-25. clockdeco_param_demo1.py\nimport time\nfrom clockdeco_param import clock\n@clock('{name}: {elapsed}s')\ndef snooze(seconds):\n    time.sleep(seconds)\nfor i in range(3):\n    snooze(.123)\nOutput of Example 9-25:\n$ python3 clockdeco_param_demo1.py\nsnooze: 0.12414693832397461s\nsnooze: 0.1241159439086914s\nsnooze: 0.12412118911743164s\nExample 9-26. clockdeco_param_demo2.py\nimport time\nfrom clockdeco_param import clock\n@clock('{name}({args}) dt={elapsed:0.3f}s')\ndef snooze(seconds):\n    time.sleep(seconds)\nfor i in range(3):\n    snooze(.123)\nOutput of Example 9-26:\n$ python3 clockdeco_param_demo2.py\nsnooze(0.123) dt=0.124s\nsnooze(0.123) dt=0.124s\nsnooze(0.123) dt=0.124s\nLennart Regebro—a technical reviewer for the first edition—argues\nthat decorators are best coded as classes implementing __call__,\nand not as functions like the examples in this chapter. I agree that\napproach is better for nontrivial decorators, but to explain the basic\nidea of this language feature, functions are easier to understand.\nSee “Further Reading” on page 336, in particular, Graham Dumple‐\nton’s blog and wrapt module for industrial-strength techniques\nwhen building decorators.\nThe next section shows an example in the style recommended by Regebro and\nDumpleton.\n334 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1300,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "A Class-Based Clock Decorator\nAs a final example, Example 9-27 lists the implementation of a parameterized clock\ndecorator implemented as a class with __call__. Contrast Example 9-24 with\nExample 9-27. Which one do you prefer?\nExample 9-27. Module clockdeco_cls.py: parameterized clock decorator implemented as\nclass\nimport time\nDEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -> {result}'\nclass clock:  \n    def __init__(self, fmt=DEFAULT_FMT):  \n        self.fmt = fmt\n    def __call__(self, func):  \n        def clocked(*_args):\n            t0 = time.perf_counter()\n            _result = func(*_args)  \n            elapsed = time.perf_counter() - t0\n            name = func.__name__\n            args = ', '.join(repr(arg) for arg in _args)\n            result = repr(_result)\n            print(self.fmt.format(**locals()))\n            return _result\n        return clocked\nInstead of a clock outer function, the clock class is our parameterized decorator\nfactory. I named it with a lowercase c to make clear that this implementation is a\ndrop-in replacement for the one in Example 9-24.\nThe argument passed in the clock(my_format) is assigned to the fmt parameter\nhere. The class constructor returns an instance of clock, with my_format stored\nin self.fmt.\n__call__ makes the clock instance callable. When invoked, the instance replaces\nthe decorated function with clocked.\nclocked wraps the decorated function.\nThis ends our exploration of function decorators. We’ll see class decorators in\nChapter 24.\nParameterized Decorators \n| \n335",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "11 I wanted to make the code as simple as possible, so I did not follow Slatkin’s excellent advice in all examples.\nChapter Summary\nWe covered some difficult terrain in this chapter. I tried to make the journey as\nsmooth as possible, but we definitely entered the realm of metaprogramming.\nWe started with a simple @register decorator without an inner function, and fin‐\nished with a parameterized @clock() involving two levels of nested functions.\nRegistration decorators, though simple in essence, have real applications in Python\nframeworks. We will apply the registration idea in one implementation of the Strat‐\negy design pattern in Chapter 10.\nUnderstanding how decorators actually work required covering the difference\nbetween import time and runtime, then diving into variable scoping, closures, and the\nnew nonlocal declaration. Mastering closures and nonlocal is valuable not only to\nbuild decorators, but also to code event-oriented programs for GUIs or asynchronous\nI/O with callbacks, and to adopt a functional style when it makes sense.\nParameterized decorators almost always involve at least two nested functions, maybe\nmore if you want to use @functools.wraps to produce a decorator that provides bet‐\nter support for more advanced techniques. One such technique is stacked decorators,\nwhich we saw in Example 9-18. For more sophisticated decorators, a class-based\nimplementation may be easier to read and maintain.\nAs examples of parameterized decorators in the standard library, we visited the pow‐\nerful @cache and @singledispatch from the functools module.\nFurther Reading\nItem #26 of Brett Slatkin’s Effective Python, 2nd ed. (Addison-Wesley), covers best\npractices for function decorators and recommends always using functools.wraps—\nwhich we saw in Example 9-16.11\nGraham Dumpleton has a series of in-depth blog posts about techniques for imple‐\nmenting well-behaved decorators, starting with “How you implemented your Python\ndecorator is wrong”. His deep expertise in this matter is also nicely packaged in the\nwrapt module he wrote to simplify the implementation of decorators and dynamic\nfunction wrappers, which support introspection and behave correctly when further\ndecorated, when applied to methods, and when used as attribute descriptors. Chap‐\nter 23 in Part III is about descriptors.\nChapter 9, “Metaprogramming”, of the Python Cookbook, 3rd ed. by David Beazley\nand Brian K. Jones (O’Reilly), has several recipes, from elementary decorators to very\n336 \n| \nChapter 9: Decorators and Closures",
      "content_length": 2527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "sophisticated ones, including one that can be called as a regular decorator or as a dec‐\norator factory, e.g., @clock or @clock(). That’s “Recipe 9.6. Defining a Decorator\nThat Takes an Optional Argument” in that cookbook.\nMichele Simionato authored a package aiming to “simplify the usage of decorators\nfor the average programmer, and to popularize decorators by showing various\nnontrivial examples,” according to the docs. It’s available on PyPI as the decorator\npackage.\nCreated when decorators were still a new feature in Python, the Python Decorator\nLibrary wiki page has dozens of examples. Because that page started years ago, some\nof the techniques shown have been superseded, but the page is still an excellent\nsource of inspiration.\n“Closures in Python” is a short blog post by Fredrik Lundh that explains the termi‐\nnology of closures.\nPEP 3104—Access to Names in Outer Scopes describes the introduction of the\nnonlocal declaration to allow rebinding of names that are neither local nor global. It\nalso includes an excellent overview of how this issue is resolved in other dynamic lan‐\nguages (Perl, Ruby, JavaScript, etc.) and the pros and cons of the design options avail‐\nable to Python.\nOn a more theoretical level, PEP 227—Statically Nested Scopes documents the intro‐\nduction of lexical scoping as an option in Python 2.1 and as a standard in Python 2.2,\nexplaining the rationale and design choices for the implementation of closures in\nPython.\nPEP 443 provides the rationale and a detailed description of the single-dispatch\ngeneric functions’ facility. An old (March 2005) blog post by Guido van Rossum,\n“Five-Minute Multimethods in Python”, walks through an implementation of generic\nfunctions (a.k.a. multimethods) using decorators. His code supports multiple-\ndispatch (i.e., dispatch based on more than one positional argument). Guido’s multi‐\nmethods code is interesting, but it’s a didactic example. For a modern, production-\nready implementation of multiple dispatch generic functions, check out Reg by\nMartijn Faassen—author of the model-driven and REST-savvy Morepath web\nframework.\nFurther Reading \n| \n337",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "Soapbox\nDynamic Scope Versus Lexical Scope\nThe designer of any language with first-class functions faces this issue: being a first-\nclass object, a function is defined in a certain scope but may be invoked in other\nscopes. The question is: how to evaluate the free variables? The first and simplest\nanswer is “dynamic scope.” This means that free variables are evaluated by looking\ninto the environment where the function is invoked.\nIf Python had dynamic scope and no closures, we could improvise avg—similar to\nExample 9-8—like this:\n>>> ### this is not a real Python console session! ###\n>>> avg = make_averager()\n>>> series = []  \n>>> avg(10)\n10.0\n>>> avg(11)  \n10.5\n>>> avg(12)\n11.0\n>>> series = [1]  \n>>> avg(5)\n3.0\nBefore using avg, we have to define series = [] ourselves, so we must know\nthat averager (inside make_averager) refers to a list named series.\nBehind the scenes, series accumulates the values to be averaged.\nWhen series = [1] is executed, the previous list is lost. This could happen by\naccident, when handling two independent running averages at the same time.\nFunctions should be opaque, with their implementation hidden from users. But with\ndynamic scope, if a function uses free variables, the programmer has to know its\ninternals to set up an environment where it works correctly. After years of struggling\nwith the LaTeX document preparation language, the excellent Practical LaTeX book\nby George Grätzer (Springer) taught me that LaTeX variables use dynamic scope.\nThat’s why they were so confusing to me!\nEmacs Lisp also uses dynamic scope, at least by default. See “Dynamic Binding” in the\nEmacs Lisp manual for a short explanation.\nDynamic scope is easier to implement, which is probably why it was the path taken by\nJohn McCarthy when he created Lisp, the first language to have first-class functions.\nPaul Graham’s article “The Roots of Lisp” is an accessible explanation of John\n338 \n| \nChapter 9: Decorators and Closures",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "McCarthy’s original paper about the Lisp language, \"Recursive Functions of Symbolic\nExpressions and Their Computation by Machine, Part I”. McCarthy’s paper is a mas‐\nterpiece as great as Beethoven’s 9th Symphony. Paul Graham translated it for the rest\nof us, from mathematics to English and running code.\nPaul Graham’s commentary explains how tricky dynamic scoping is. Quoting from\n“The Roots of Lisp”:\nIt’s an eloquent testimony to the dangers of dynamic scope that even the very first\nexample of higher-order Lisp functions was broken because of it. It may be that\nMcCarthy was not fully aware of the implications of dynamic scope in 1960.\nDynamic scope remained in Lisp implementations for a surprisingly long time—\nuntil Sussman and Steele developed Scheme in 1975. Lexical scope does not compli‐\ncate the definition of eval very much, but it may make compilers harder to write.\nToday, lexical scope is the norm: free variables are evaluated considering the environ‐\nment where the function is defined. Lexical scope complicates the implementation of\nlanguages with first-class functions, because it requires the support of closures. On\nthe other hand, lexical scope makes source code easier to read. Most languages inven‐\nted since Algol have lexical scope. One notable exception is JavaScript, where the spe‐\ncial variable this is confusing because it can be lexically or dynamically scoped,\ndepending on how the code is written.\nFor many years, Python lambdas did not provide closures, contributing to the bad\nname of this feature among functional-programming geeks in the blogosphere. This\nwas fixed in Python 2.2 (December 2001), but the blogosphere has a long memory.\nSince then, lambda is embarrassing only because of its limited syntax.\nPython Decorators and the Decorator Design Pattern\nPython function decorators fit the general description of decorator given by Gamma\net al. in Design Patterns: “Attach additional responsibilities to an object dynamically.\nDecorators provide a flexible alternative to subclassing for extending functionality.”\nAt the implementation level, Python decorators do not resemble the classic decorator\ndesign pattern, but an analogy can be made.\nIn the design pattern, Decorator and Component are abstract classes. An instance of a\nconcrete decorator wraps an instance of a concrete component in order to add behav‐\niors to it. Quoting from Design Patterns:\nThe decorator conforms to the interface of the component it decorates so that its\npresence is transparent to the component’s clients. The decorator forwards requests\nto the component and may perform additional actions (such as drawing a border)\nbefore or after forwarding. Transparency lets you nest decorators recursively, thereby\nallowing an unlimited number of added responsibilities.” (p. 175)\nIn Python, the decorator function plays the role of a concrete Decorator subclass,\nand the inner function it returns is a decorator instance. The returned function wraps\nFurther Reading \n| \n339",
      "content_length": 2992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "the function to be decorated, which is analogous to the component in the design pat‐\ntern. The returned function is transparent because it conforms to the interface of the\ncomponent by accepting the same arguments. It forwards calls to the component and\nmay perform additional actions either before or after it. Borrowing from the previous\ncitation, we can adapt the last sentence to say that “Transparency lets you stack deco‐\nrators, thereby allowing an unlimited number of added behaviors.”\nNote that I am not suggesting that function decorators should be used to implement\nthe decorator pattern in Python programs. Although this can be done in specific sit‐\nuations, in general the decorator pattern is best implemented with classes to represent\nthe decorator and the components it will wrap.\n340 \n| \nChapter 9: Decorators and Closures",
      "content_length": 839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "1 From a slide in the talk “Root Cause Analysis of Some Faults in Design Patterns,” presented by Ralph Johnson\nat IME/CCSL, Universidade de São Paulo, Nov. 15, 2014.\nCHAPTER 10\nDesign Patterns with First-Class Functions\nConformity to patterns is not a measure of goodness.\n—Ralph Johnson, coauthor of the Design Patterns classic1\nIn software engineering, a design pattern is a general recipe for solving a common\ndesign problem. You don’t need to know design patterns to follow this chapter. I will\nexplain the patterns used in the examples.\nThe use of design patterns in programming was popularized by the landmark book\nDesign Patterns: Elements of Reusable Object-Oriented Software (Addison-Wesley) by\nErich Gamma, Richard Helm, Ralph Johnson, and John Vlissides—a.k.a. “the Gang\nof Four.” The book is a catalog of 23 patterns consisting of arrangements of classes\nexemplified with code in C++, but assumed to be useful in other object-oriented lan‐\nguages as well.\nAlthough design patterns are language independent, that does not mean every pat‐\ntern applies to every language. For example, Chapter 17 will show that it doesn’t\nmake sense to emulate the recipe of the Iterator pattern in Python, because the pat‐\ntern is embedded in the language and ready to use in the form of generators—which\ndon’t need classes to work, and require less code than the classic recipe.\nThe authors of Design Patterns acknowledge in their introduction that the implemen‐\ntation language determines which patterns are relevant:\nThe choice of programming language is important because it influences one’s point of\nview. Our patterns assume Smalltalk/C++-level language features, and that choice\ndetermines what can and cannot be implemented easily. If we assumed procedural\n341",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "2 Quoted from page 4 of Design Patterns.\nlanguages, we might have included design patterns called “Inheritance,” “Encapsula‐\ntion,” and “Polymorphism.” Similarly, some of our patterns are supported directly by\nthe less common object-oriented languages. CLOS has multi-methods, for example,\nwhich lessen the need for a pattern such as Visitor.2\nIn his 1996 presentation, “Design Patterns in Dynamic Languages”, Peter Norvig\nstates that 16 out of the 23 patterns in the original Design Patterns book become\neither “invisible or simpler” in a dynamic language (slide 9). He’s talking about the\nLisp and Dylan languages, but many of the relevant dynamic features are also present\nin Python. In particular, in the context of languages with first-class functions, Norvig\nsuggests rethinking the classic patterns known as Strategy, Command, Template\nMethod, and Visitor.\nThe goal of this chapter is to show how—in some cases—functions can do the same\nwork as classes, with code that is more readable and concise. We will refactor an\nimplementation of Strategy using functions as objects, removing a lot of boilerplate\ncode. We’ll also discuss a similar approach to simplifying the Command pattern.\nWhat’s New in This Chapter\nI moved this chapter to the end of Part III so I could apply a registration decorator in\n“Decorator-Enhanced Strategy Pattern” on page 353 and also use type hints in the\nexamples. Most type hints used in this chapter are not complicated, and they do help\nwith readability.\nCase Study: Refactoring Strategy\nStrategy is a good example of a design pattern that can be simpler in Python if you\nleverage functions as first-class objects. In the following section, we describe and\nimplement Strategy using the “classic” structure described in Design Patterns. If you\nare familiar with the classic pattern, you can skip to “Function-Oriented Strategy” on\npage 347 where we refactor the code using functions, significantly reducing the line\ncount.\nClassic Strategy\nThe UML class diagram in Figure 10-1 depicts an arrangement of classes that exem‐\nplifies the Strategy pattern.\n342 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 2149,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "Figure 10-1. UML class diagram for order discount processing implemented with the\nStrategy design pattern.\nThe Strategy pattern is summarized like this in Design Patterns:\nDefine a family of algorithms, encapsulate each one, and make them interchangeable.\nStrategy lets the algorithm vary independently from clients that use it.\nA clear example of Strategy applied in the ecommerce domain is computing dis‐\ncounts to orders according to the attributes of the customer or inspection of the\nordered items.\nConsider an online store with these discount rules:\n• Customers with 1,000 or more fidelity points get a global 5% discount per order.\n• A 10% discount is applied to each line item with 20 or more units in the same\norder.\n• Orders with at least 10 distinct items get a 7% global discount.\nFor brevity, let’s assume that only one discount may be applied to an order.\nThe UML class diagram for the Strategy pattern is depicted in Figure 10-1. Its partici‐\npants are:\nCase Study: Refactoring Strategy \n| \n343",
      "content_length": 1009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "Context\nProvides a service by delegating some computation to interchangeable compo‐\nnents that implement alternative algorithms. In the ecommerce example, the\ncontext is an Order, which is configured to apply a promotional discount accord‐\ning to one of several algorithms.\nStrategy\nThe interface common to the components that implement the different algo‐\nrithms. In our example, this role is played by an abstract class called Promotion.\nConcrete strategy\nOne of the concrete subclasses of Strategy. FidelityPromo, BulkPromo, and\nLargeOrderPromo are the three concrete strategies implemented.\nThe code in Example 10-1 follows the blueprint in Figure 10-1. As described in\nDesign Patterns, the concrete strategy is chosen by the client of the context class. In\nour example, before instantiating an order, the system would somehow select a pro‐\nmotional discount strategy and pass it to the Order constructor. The selection of the\nstrategy is outside the scope of the pattern.\nExample 10-1. Implementation of the Order class with pluggable discount strategies\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom decimal import Decimal\nfrom typing import NamedTuple, Optional\nclass Customer(NamedTuple):\n    name: str\n    fidelity: int\nclass LineItem(NamedTuple):\n    product: str\n    quantity: int\n    price: Decimal\n    def total(self) -> Decimal:\n        return self.price * self.quantity\nclass Order(NamedTuple):  # the Context\n    customer: Customer\n    cart: Sequence[LineItem]\n    promotion: Optional['Promotion'] = None\n    def total(self) -> Decimal:\n        totals = (item.total() for item in self.cart)\n344 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "return sum(totals, start=Decimal(0))\n    def due(self) -> Decimal:\n        if self.promotion is None:\n            discount = Decimal(0)\n        else:\n            discount = self.promotion.discount(self)\n        return self.total() - discount\n    def __repr__(self):\n        return f'<Order total: {self.total():.2f} due: {self.due():.2f}>'\nclass Promotion(ABC):  # the Strategy: an abstract base class\n    @abstractmethod\n    def discount(self, order: Order) -> Decimal:\n        \"\"\"Return discount as a positive dollar amount\"\"\"\nclass FidelityPromo(Promotion):  # first Concrete Strategy\n    \"\"\"5% discount for customers with 1000 or more fidelity points\"\"\"\n    def discount(self, order: Order) -> Decimal:\n        rate = Decimal('0.05')\n        if order.customer.fidelity >= 1000:\n            return order.total() * rate\n        return Decimal(0)\nclass BulkItemPromo(Promotion):  # second Concrete Strategy\n    \"\"\"10% discount for each LineItem with 20 or more units\"\"\"\n    def discount(self, order: Order) -> Decimal:\n        discount = Decimal(0)\n        for item in order.cart:\n            if item.quantity >= 20:\n                discount += item.total() * Decimal('0.1')\n        return discount\nclass LargeOrderPromo(Promotion):  # third Concrete Strategy\n    \"\"\"7% discount for orders with 10 or more distinct items\"\"\"\n    def discount(self, order: Order) -> Decimal:\n        distinct_items = {item.product for item in order.cart}\n        if len(distinct_items) >= 10:\n            return order.total() * Decimal('0.07')\n        return Decimal(0)\nNote that in Example 10-1, I coded Promotion as an abstract base class (ABC) to use\nthe @abstractmethod decorator and make the pattern more explicit.\nCase Study: Refactoring Strategy \n| \n345",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "Example 10-2 shows doctests used to demonstrate and verify the operation of a mod‐\nule implementing the rules described earlier.\nExample 10-2. Sample usage of Order class with different promotions applied\n    >>> joe = Customer('John Doe', 0)  \n    >>> ann = Customer('Ann Smith', 1100)\n    >>> cart = (LineItem('banana', 4, Decimal('.5')),  \n    ...         LineItem('apple', 10, Decimal('1.5')),\n    ...         LineItem('watermelon', 5, Decimal(5)))\n    >>> Order(joe, cart, FidelityPromo())  \n    <Order total: 42.00 due: 42.00>\n    >>> Order(ann, cart, FidelityPromo())  \n    <Order total: 42.00 due: 39.90>\n    >>> banana_cart = (LineItem('banana', 30, Decimal('.5')),  \n    ...                LineItem('apple', 10, Decimal('1.5')))\n    >>> Order(joe, banana_cart, BulkItemPromo())  \n    <Order total: 30.00 due: 28.50>\n    >>> long_cart = tuple(LineItem(str(sku), 1, Decimal(1)) \n    ...                  for sku in range(10))\n    >>> Order(joe, long_cart, LargeOrderPromo())  \n    <Order total: 10.00 due: 9.30>\n    >>> Order(joe, cart, LargeOrderPromo())\n    <Order total: 42.00 due: 42.00>\nTwo customers: joe has 0 fidelity points, ann has 1,100.\nOne shopping cart with three line items.\nThe FidelityPromo promotion gives no discount to joe.\nann gets a 5% discount because she has at least 1,000 points.\nThe banana_cart has 30 units of the \"banana\" product and 10 apples.\nThanks to the BulkItemPromo, joe gets a $1.50 discount on the bananas.\nlong_cart has 10 different items at $1.00 each.\njoe gets a 7% discount on the whole order because of LargerOrderPromo.\nExample 10-1 works perfectly well, but the same functionality can be implemented\nwith less code in Python by using functions as objects. The next section shows how.\n346 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "3 I had to reimplement Order with @dataclass due to a bug in Mypy. You may ignore this detail, because this\nclass works with NamedTuple as well, just like in Example 10-1. If Order is a NamedTuple, Mypy 0.910 crashes\nwhen checking the type hint for promotion. I tried adding # type ignore to that specific line, but Mypy\ncrashed anyway. Mypy handles the same type hint correctly if Order is built with @dataclass. Issue #9397 is\nunresolved as of July 19, 2021. Hopefully it will be fixed by the time you read this.\nFunction-Oriented Strategy\nEach concrete strategy in Example 10-1 is a class with a single method, discount.\nFurthermore, the strategy instances have no state (no instance attributes). You could\nsay they look a lot like plain functions, and you would be right. Example 10-3 is a\nrefactoring of Example 10-1, replacing the concrete strategies with simple functions\nand removing the Promo abstract class. Only small adjustments are needed in the\nOrder class.3\nExample 10-3. Order class with discount strategies implemented as functions\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass\nfrom decimal import Decimal\nfrom typing import Optional, Callable, NamedTuple\nclass Customer(NamedTuple):\n    name: str\n    fidelity: int\nclass LineItem(NamedTuple):\n    product: str\n    quantity: int\n    price: Decimal\n    def total(self):\n        return self.price * self.quantity\n@dataclass(frozen=True)\nclass Order:  # the Context\n    customer: Customer\n    cart: Sequence[LineItem]\n    promotion: Optional[Callable[['Order'], Decimal]] = None  \n    def total(self) -> Decimal:\n        totals = (item.total() for item in self.cart)\n        return sum(totals, start=Decimal(0))\n    def due(self) -> Decimal:\n        if self.promotion is None:\nCase Study: Refactoring Strategy \n| \n347",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "discount = Decimal(0)\n        else:\n            discount = self.promotion(self)  \n        return self.total() - discount\n    def __repr__(self):\n        return f'<Order total: {self.total():.2f} due: {self.due():.2f}>'\ndef fidelity_promo(order: Order) -> Decimal:  \n    \"\"\"5% discount for customers with 1000 or more fidelity points\"\"\"\n    if order.customer.fidelity >= 1000:\n        return order.total() * Decimal('0.05')\n    return Decimal(0)\ndef bulk_item_promo(order: Order) -> Decimal:\n    \"\"\"10% discount for each LineItem with 20 or more units\"\"\"\n    discount = Decimal(0)\n    for item in order.cart:\n        if item.quantity >= 20:\n            discount += item.total() * Decimal('0.1')\n    return discount\ndef large_order_promo(order: Order) -> Decimal:\n    \"\"\"7% discount for orders with 10 or more distinct items\"\"\"\n    distinct_items = {item.product for item in order.cart}\n    if len(distinct_items) >= 10:\n        return order.total() * Decimal('0.07')\n    return Decimal(0)\nThis type hint says: promotion may be None, or it may be a callable that takes an\nOrder argument and returns a Decimal.\nTo compute a discount, call the self.promotion callable, passing self as an\nargument. See the following tip for the reason.\nNo abstract class.\nEach strategy is a function.\n348 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 1342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "Why self.promotion(self)?\nIn the Order class, promotion is not a method. It’s an instance\nattribute that happens to be callable. So the first part of the expres‐\nsion, self.promotion, retrieves that callable. To invoke it, we must\nprovide an instance of Order, which in this case is self. That’s why\nself appears twice in that expression.\n“Methods Are Descriptors” on page 898 will explain the mechanism\nthat binds methods to instances automatically. It does not apply to\npromotion because it is not a method.\nThe code in Example 10-3 is shorter than Example 10-1. Using the new Order is also\na bit simpler, as shown in the Example 10-4 doctests.\nExample 10-4. Sample usage of Order class with promotions as functions\n    >>> joe = Customer('John Doe', 0)  \n    >>> ann = Customer('Ann Smith', 1100)\n    >>> cart = [LineItem('banana', 4, Decimal('.5')),\n    ...         LineItem('apple', 10, Decimal('1.5')),\n    ...         LineItem('watermelon', 5, Decimal(5))]\n    >>> Order(joe, cart, fidelity_promo)  \n    <Order total: 42.00 due: 42.00>\n    >>> Order(ann, cart, fidelity_promo)\n    <Order total: 42.00 due: 39.90>\n    >>> banana_cart = [LineItem('banana', 30, Decimal('.5')),\n    ...                LineItem('apple', 10, Decimal('1.5'))]\n    >>> Order(joe, banana_cart, bulk_item_promo)  \n    <Order total: 30.00 due: 28.50>\n    >>> long_cart = [LineItem(str(item_code), 1, Decimal(1))\n    ...               for item_code in range(10)]\n    >>> Order(joe, long_cart, large_order_promo)\n    <Order total: 10.00 due: 9.30>\n    >>> Order(joe, cart, large_order_promo)\n    <Order total: 42.00 due: 42.00>\nSame test fixtures as Example 10-1.\nTo apply a discount strategy to an Order, just pass the promotion function as an\nargument.\nA different promotion function is used here and in the next test.\nNote the callouts in Example 10-4—there is no need to instantiate a new promotion\nobject with each new order: the functions are ready to use.\nCase Study: Refactoring Strategy \n| \n349",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "4 See page 323 of Design Patterns.\n5 Ibid., p. 196.\nIt is interesting to note that in Design Patterns, the authors suggest: “Strategy objects\noften make good flyweights.”4 A definition of the Flyweight pattern in another part of\nthat work states: “A flyweight is a shared object that can be used in multiple contexts\nsimultaneously.”5 The sharing is recommended to reduce the cost of creating a new\nconcrete strategy object when the same strategy is applied over and over again with\nevery new context—with every new Order instance, in our example. So, to overcome\na drawback of the Strategy pattern—its runtime cost—the authors recommend apply‐\ning yet another pattern. Meanwhile, the line count and maintenance cost of your\ncode are piling up.\nA thornier use case, with complex concrete strategies holding internal state, may\nrequire all the pieces of the Strategy and Flyweight design patterns combined. But\noften concrete strategies have no internal state; they only deal with data from the\ncontext. If that is the case, then by all means use plain old functions instead of coding\nsingle-method classes implementing a single-method interface declared in yet\nanother class. A function is more lightweight than an instance of a user-defined class,\nand there is no need for Flyweight because each strategy function is created just once\nper Python process when it loads the module. A plain function is also “a shared\nobject that can be used in multiple contexts simultaneously.”\nNow that we have implemented the Strategy pattern with functions, other possibili‐\nties emerge. Suppose you want to create a “metastrategy” that selects the best avail‐\nable discount for a given Order. In the following sections we study additional\nrefactorings that implement this requirement using a variety of approaches that lev‐\nerage functions and modules as objects.\nChoosing the Best Strategy: Simple Approach\nGiven the same customers and shopping carts from the tests in Example 10-4, we\nnow add three additional tests in Example 10-5.\nExample 10-5. The best_promo function applies all discounts and returns the largest\n    >>> Order(joe, long_cart, best_promo)  \n    <Order total: 10.00 due: 9.30>\n    >>> Order(joe, banana_cart, best_promo)  \n    <Order total: 30.00 due: 28.50>\n    >>> Order(ann, cart, best_promo)  \n    <Order total: 42.00 due: 39.90>\n350 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 2404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "best_promo selected the larger_order_promo for customer joe.\nHere joe got the discount from bulk_item_promo for ordering lots of bananas.\nChecking out with a simple cart, best_promo gave loyal customer ann the dis‐\ncount for the fidelity_promo.\nThe implementation of best_promo is very simple. See Example 10-6.\nExample 10-6. best_promo finds the maximum discount iterating over a list of\nfunctions\npromos = [fidelity_promo, bulk_item_promo, large_order_promo]  \ndef best_promo(order: Order) -> Decimal:  \n    \"\"\"Compute the best discount available\"\"\"\n    return max(promo(order) for promo in promos)  \npromos: list of the strategies implemented as functions.\nbest_promo takes an instance of Order as argument, as do the other *_promo\nfunctions.\nUsing a generator expression, we apply each of the functions from promos to the\norder, and return the maximum discount computed.\nExample 10-6 is straightforward: promos is a list of functions. Once you get used to\nthe idea that functions are first-class objects, it naturally follows that building data\nstructures holding functions often makes sense.\nAlthough Example 10-6 works and is easy to read, there is some duplication that\ncould lead to a subtle bug: to add a new promotion strategy, we need to code the\nfunction and remember to add it to the promos list, or else the new promotion will\nwork when explicitly passed as an argument to Order, but will not be considered by\nbest_promotion.\nRead on for a couple of solutions to this issue.\nFinding Strategies in a Module\nModules in Python are also first-class objects, and the standard library provides sev‐\neral functions to handle them. The built-in globals is described as follows in the\nPython docs:\nCase Study: Refactoring Strategy \n| \n351",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "6 flake8 and VS Code both complain that these names are imported but not used. By definition, static analysis\ntools cannot understand the dynamic nature of Python. If we heed every advice from such tools, we’ll soon be\nwriting grim and verbose Java-like code with Python syntax.\nglobals()\nReturn a dictionary representing the current global symbol table. This is always\nthe dictionary of the current module (inside a function or method, this is the\nmodule where it is defined, not the module from which it is called).\nExample 10-7 is a somewhat hackish way of using globals to help best_promo auto‐\nmatically find the other available *_promo functions.\nExample 10-7. The promos list is built by introspection of the module global namespace\nfrom decimal import Decimal\nfrom strategy import Order\nfrom strategy import (\n    fidelity_promo, bulk_item_promo, large_order_promo  \n)\npromos = [promo for name, promo in globals().items()  \n                if name.endswith('_promo') and        \n                   name != 'best_promo'               \n]\ndef best_promo(order: Order) -> Decimal:              \n    \"\"\"Compute the best discount available\"\"\"\n    return max(promo(order) for promo in promos)\nImport the promotion functions so they are available in the global namespace.6\nIterate over each item in the dict returned by globals().\nSelect only values where the name ends with the _promo suffix and…\n…filter out best_promo itself, to avoid an infinite recursion when best_promo is\ncalled.\nNo changes in best_promo.\nAnother way of collecting the available promotions would be to create a module and\nput all the strategy functions there, except for best_promo.\nIn Example 10-8, the only significant change is that the list of strategy functions\nis built by introspection of a separate module called promotions. Note that\n352 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "Example 10-8 depends on importing the promotions module as well as inspect,\nwhich provides high-level introspection functions.\nExample 10-8. The promos list is built by introspection of a new promotions module\nfrom decimal import Decimal\nimport inspect\nfrom strategy import Order\nimport promotions\npromos = [func for _, func in inspect.getmembers(promotions, inspect.isfunction)]\ndef best_promo(order: Order) -> Decimal:\n    \"\"\"Compute the best discount available\"\"\"\n    return max(promo(order) for promo in promos)\nThe function inspect.getmembers returns the attributes of an object—in this case,\nthe promotions module—optionally filtered by a predicate (a boolean function). We\nuse inspect.isfunction to get only the functions from the module.\nExample 10-8 works regardless of the names given to the functions; all that matters is\nthat the promotions module contains only functions that calculate discounts given\norders. Of course, this is an implicit assumption of the code. If someone were to cre‐\nate a function with a different signature in the promotions module, then best_promo\nwould break while trying to apply it to an order.\nWe could add more stringent tests to filter the functions, by inspecting their argu‐\nments for instance. The point of Example 10-8 is not to offer a complete solution, but\nto highlight one possible use of module introspection.\nA more explicit alternative to dynamically collecting promotional discount functions\nwould be to use a simple decorator. That’s next.\nDecorator-Enhanced Strategy Pattern\nRecall that our main issue with Example 10-6 is the repetition of the function names\nin their definitions and then in the promos list used by the best_promo function to\ndetermine the highest discount applicable. The repetition is problematic because\nsomeone may add a new promotional strategy function and forget to manually add it\nto the promos list—in which case, best_promo will silently ignore the new strategy,\nintroducing a subtle bug in the system. Example 10-9 solves this problem with the\ntechnique covered in “Registration Decorators” on page 308.\nDecorator-Enhanced Strategy Pattern \n| \n353",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "Example 10-9. The promos list is filled by the Promotion decorator\nPromotion = Callable[[Order], Decimal]\npromos: list[Promotion] = []  \ndef promotion(promo: Promotion) -> Promotion:  \n    promos.append(promo)\n    return promo\ndef best_promo(order: Order) -> Decimal:\n    \"\"\"Compute the best discount available\"\"\"\n    return max(promo(order) for promo in promos)  \n@promotion  \ndef fidelity(order: Order) -> Decimal:\n    \"\"\"5% discount for customers with 1000 or more fidelity points\"\"\"\n    if order.customer.fidelity >= 1000:\n        return order.total() * Decimal('0.05')\n    return Decimal(0)\n@promotion\ndef bulk_item(order: Order) -> Decimal:\n    \"\"\"10% discount for each LineItem with 20 or more units\"\"\"\n    discount = Decimal(0)\n    for item in order.cart:\n        if item.quantity >= 20:\n            discount += item.total() * Decimal('0.1')\n    return discount\n@promotion\ndef large_order(order: Order) -> Decimal:\n    \"\"\"7% discount for orders with 10 or more distinct items\"\"\"\n    distinct_items = {item.product for item in order.cart}\n    if len(distinct_items) >= 10:\n        return order.total() * Decimal('0.07')\n    return Decimal(0)\nThe promos list is a module global, and starts empty.\nPromotion is a registration decorator: it returns the promo function unchanged,\nafter appending it to the promos list.\nNo changes needed to best_promo, because it relies on the promos list.\n354 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "Any function decorated by @promotion will be added to promos.\nThis solution has several advantages over the others presented before:\n• The promotion strategy functions don’t have to use special names—no need for\nthe _promo suffix.\n• The @promotion decorator highlights the purpose of the decorated function, and\nalso makes it easy to temporarily disable a promotion: just comment out the\ndecorator.\n• Promotional discount strategies may be defined in other modules, anywhere in\nthe system, as long as the @promotion decorator is applied to them.\nIn the next section, we discuss Command—another design pattern that is sometimes\nimplemented via single-method classes when plain functions would do.\nThe Command Pattern\nCommand is another design pattern that can be simplified by the use of functions\npassed as arguments. Figure 10-2 shows the arrangement of classes in the Command\npattern.\nFigure 10-2. UML class diagram for menu-driven text editor implemented with the\nCommand design pattern. Each command may have a different receiver: the object that\nimplements the action. For PasteCommand, the receiver is the Document. For OpenCom\nmand, the receiver is the application.\nThe Command Pattern \n| \n355",
      "content_length": 1200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "The goal of Command is to decouple an object that invokes an operation (the\ninvoker) from the provider object that implements it (the receiver). In the example\nfrom Design Patterns, each invoker is a menu item in a graphical application, and the\nreceivers are the document being edited or the application itself.\nThe idea is to put a Command object between the two, implementing an interface with\na single method, execute, which calls some method in the receiver to perform the\ndesired operation. That way the invoker does not need to know the interface of the\nreceiver, and different receivers can be adapted through different Command subclasses.\nThe invoker is configured with a concrete command and calls its execute method to\noperate it. Note in Figure 10-2 that MacroCommand may store a sequence of com‐\nmands; its execute() method calls the same method in each command stored.\nQuoting from Design Patterns, “Commands are an object-oriented replacement for\ncallbacks.” The question is: do we need an object-oriented replacement for callbacks?\nSometimes yes, but not always.\nInstead of giving the invoker a Command instance, we can simply give it a function.\nInstead of calling command.execute(), the invoker can just call command(). The Macro\nCommand can be implemented with a class implementing __call__. Instances of\nMacroCommand would be callables, each holding a list of functions for future invoca‐\ntion, as implemented in Example 10-10.\nExample 10-10. Each instance of MacroCommand has an internal list of commands\nclass MacroCommand:\n    \"\"\"A command that executes a list of commands\"\"\"\n    def __init__(self, commands):\n        self.commands = list(commands)  \n    def __call__(self):\n        for command in self.commands:  \n            command()\nBuilding a list from the commands arguments ensures that it is iterable and keeps\na local copy of the command references in each MacroCommand instance.\nWhen an instance of MacroCommand is invoked, each command in self.commands\nis called in sequence.\nMore advanced uses of the Command pattern—to support undo, for example—may\nrequire more than a simple callback function. Even then, Python provides a couple of\nalternatives that deserve consideration:\n356 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 2273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "7 “Root Cause Analysis of Some Faults in Design Patterns,” presented by Johnson at IME-USP, November 15,\n2014.\n• A callable instance like MacroCommand in Example 10-10 can keep whatever state\nis necessary, and provide extra methods in addition to __call__.\n• A closure can be used to hold the internal state of a function between calls.\nThis concludes our rethinking of the Command pattern with first-class functions. At\na high level, the approach here was similar to the one we applied to Strategy: replac‐\ning with callables the instances of a participant class that implemented a single-\nmethod interface. After all, every Python callable implements a single-method\ninterface, and that method is named __call__.\nChapter Summary\nAs Peter Norvig pointed out a couple of years after the classic Design Patterns book\nappeared, “16 of 23 patterns have qualitatively simpler implementation in Lisp or\nDylan than in C++ for at least some uses of each pattern” (slide 9 of Norvig’s “Design\nPatterns in Dynamic Languages” presentation). Python shares some of the dynamic\nfeatures of the Lisp and Dylan languages, in particular, first-class functions, our focus\nin this part of the book.\nFrom the same talk quoted at the start of this chapter, in reflecting on the 20th anni‐\nversary of Design Patterns: Elements of Reusable Object-Oriented Software, Ralph\nJohnson has stated that one of the failings of the book is: “Too much emphasis on\npatterns as end-points instead of steps in the design process.”7 In this chapter, we\nused the Strategy pattern as a starting point: a working solution that we could sim‐\nplify using first-class functions.\nIn many cases, functions or callable objects provide a more natural way of imple‐\nmenting callbacks in Python than mimicking the Strategy or the Command patterns\nas described by Gamma, Helm, Johnson, and Vlissides in Design Patterns. The refac‐\ntoring of Strategy and the discussion of Command in this chapter are examples of a\nmore general insight: sometimes you may encounter a design pattern or an API that\nrequires that components implement an interface with a single method, and that\nmethod has a generic-sounding name such as “execute,” “run,” or “do_it.” Such pat‐\nterns or APIs often can be implemented with less boilerplate code in Python using\nfunctions as first-class objects.\nChapter Summary \n| \n357",
      "content_length": 2348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "Further Reading\n“Recipe 8.21. Implementing the Visitor Pattern,” in the Python Cookbook, 3rd ed.,\npresents an elegant implementation of the Visitor pattern in which a NodeVisitor\nclass handles methods as first-class objects.\nOn the general topic of design patterns, the choice of readings for the Python pro‐\ngrammer is not as broad as what is available to other language communities.\nLearning Python Design Patterns, by Gennadiy Zlobin (Packt), is the only book that I\nhave seen entirely devoted to patterns in Python. But Zlobin’s work is quite short\n(100 pages) and covers 8 of the original 23 design patterns.\nExpert Python Programming, by Tarek Ziadé (Packt), is one of the best intermediate-\nlevel Python books in the market, and its final chapter, “Useful Design Patterns,”\npresents several of the classic patterns from a Pythonic perspective.\nAlex Martelli has given several talks about Python design patterns. There is a video of\nhis EuroPython 2011 presentation and a set of slides on his personal website. I’ve\nfound different slide decks and videos over the years, of varying lengths, so it is\nworthwhile to do a thorough search for his name with the words “Python Design Pat‐\nterns.” A publisher told me Martelli is working on a book about this subject. I will\ncertainly get it when it comes out.\nThere are many books about design patterns in the context of Java, but among them\nthe one I like most is Head First Design Patterns, 2nd ed., by Eric Freeman and Elisa‐\nbeth Robson (O’Reilly). It explains 16 of the 23 classic patterns. If you like the wacky\nstyle of the Head First series and need an introduction to this topic, you will love that\nwork. It is Java-centric, but the second edition was updated to reflect the addition of\nfirst-class functions in Java, making some of the examples closer to code we’d write in\nPython.\nFor a fresh look at patterns from the point of view of a dynamic language with duck\ntyping and first-class functions, Design Patterns in Ruby by Russ Olsen (Addison-\nWesley) has many insights that are also applicable to Python. In spite of their many\nsyntactic differences, at the semantic level Python and Ruby are closer to each other\nthan to Java or C++.\nIn “Design Patterns in Dynamic Languages” (slides), Peter Norvig shows how first-\nclass functions (and other dynamic features) make several of the original design pat‐\nterns either simpler or unnecessary.\nThe introduction of the original Design Patterns book by Gamma et al. is worth the\nprice of the book—more than the catalog of 23 patterns, which includes recipes rang‐\ning from very important to rarely useful. The widely quoted design principles,\n358 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 2714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "“Program to an interface, not an implementation” and “Favor object composition\nover class inheritance,” both come from that introduction.\nThe application of patterns to design originated with the architect Christopher\nAlexander et al., presented in the book A Pattern Language (Oxford University\nPress). Alexander’s idea is to create a standard vocabulary allowing teams to share\ncommon design decisions while designing buildings. M. J. Dominus wrote “‘Design\nPatterns’ Aren’t”, an intriguing slide deck and postscript text arguing that\nAlexander’s original vision of patterns is more profound, more human, and also\napplicable to software engineering.\nSoapbox\nPython has first-class functions and first-class types, features that Norvig claims affect\n10 of the 23 patterns (slide 10 of “Design Patterns in Dynamic Languages”). In Chap‐\nter 9, we saw that Python also has generic functions (“Single Dispatch Generic Func‐\ntions” on page 324), a limited form of the CLOS multimethods that Gamma et al.\nsuggest as a simpler way to implement the classic Visitor pattern. Norvig, on the\nother hand, says that multimethods simplify the Builder pattern (slide 10). Matching\ndesign patterns to language features is not an exact science.\nIn classrooms around the world, design patterns are frequently taught using Java\nexamples. I’ve heard more than one student claim that they were led to believe that\nthe original design patterns are useful in any implementation language. It turns out\nthat the “classic” 23 patterns from Design Patterns apply to “classic” Java very well in\nspite of being originally presented mostly in the context of C++—a few have Small‐\ntalk examples in the book. But that does not mean every one of those patterns applies\nequally well in any language. The authors are explicit right at the beginning of their\nbook that “some of our patterns are supported directly by the less common object-\noriented languages” (recall full quote on the first page of this chapter).\nThe Python bibliography about design patterns is very thin, compared to that of Java,\nC++, or Ruby. In “Further Reading” on page 358 I mentioned Learning Python\nDesign Patterns by Gennadiy Zlobin, which was published as recently as November\n2013. In contrast, Russ Olsen’s Design Patterns in Ruby was published in 2007 and\nhas 384 pages—284 more than Zlobin’s work.\nNow that Python is becoming increasingly popular in academia, let’s hope more will\nbe written about design patterns in the context of this language. Also, Java 8 intro‐\nduced method references and anonymous functions, and those highly anticipated\nfeatures are likely to prompt fresh approaches to patterns in Java—recognizing that as\nlanguages evolve, so must our understanding of how to apply the classic design\npatterns.\nFurther Reading \n| \n359",
      "content_length": 2793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "The call of the Wild\nAs we collaborated to put the final touches to this book, tech reviewer Leonardo\nRochael wondered:\nIf functions have a __call__ method, and methods are also callable, do __call__\nmethods also have a __call__ method?\nI don’t know if his discovery is useful, but it is a fun fact:\n>>> def turtle():\n...     return 'eggs'\n...\n>>> turtle()\n'eggs'\n>>> turtle.__call__()\n'eggs'\n>>> turtle.__call__.__call__()\n'eggs'\n>>> turtle.__call__.__call__.__call__()\n'eggs'\n>>> turtle.__call__.__call__.__call__.__call__()\n'eggs'\n>>> turtle.__call__.__call__.__call__.__call__.__call__()\n'eggs'\n>>> turtle.__call__.__call__.__call__.__call__.__call__.__call__()\n'eggs'\n>>> turtle.__call__.__call__.__call__.__call__.__call__.__call__.__call__()\n'eggs'\nTurtles all the way down!\n360 \n| \nChapter 10: Design Patterns with First-Class Functions",
      "content_length": 844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "PART III\n\nClasses and Protocols",
      "content_length": 31,
      "extraction_method": "OCR"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "1 From Faassen’s blog post, “What is Pythonic?”\nCHAPTER 11\nA Pythonic Object\nFor a library or framework to be Pythonic is to make it as easy and natural as possible\nfor a Python programmer to pick up how to perform a task.\n—Martijn Faassen, creator of Python and JavaScript frameworks.1\nThanks to the Python Data Model, your user-defined types can behave as naturally as\nthe built-in types. And this can be accomplished without inheritance, in the spirit of\nduck typing: you just implement the methods needed for your objects to behave as\nexpected.\nIn previous chapters, we studied the behavior of many built-in objects. We will now\nbuild user-defined classes that behave as real Python objects. Your application classes\nprobably don’t need and should not implement as many special methods as the\nexamples in this chapter. But if you are writing a library or a framework, the pro‐\ngrammers who will use your classes may expect them to behave like the classes that\nPython provides. Fulfilling that expectation is one way of being “Pythonic.”\nThis chapter starts where Chapter 1 ended, by showing how to implement several\nspecial methods that are commonly seen in Python objects of many different types.\nIn this chapter, we will see how to:\n• Support the built-in functions that convert objects to other types (e.g., repr(),\nbytes(), complex(), etc.)\n• Implement an alternative constructor as a class method\n• Extend the format mini-language used by f-strings, the format() built-in, and\nthe str.format() method\n363",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "• Provide read-only access to attributes\n• Make an object hashable for use in sets and as dict keys\n• Save memory with the use of __slots__\nWe’ll do all that as we develop Vector2d, a simple two-dimensional Euclidean vector\ntype. This code will be the foundation of an N-dimensional vector class in\nChapter 12.\nThe evolution of the example will be paused to discuss two conceptual topics:\n• How and when to use the @classmethod and @staticmethod decorators\n• Private and protected attributes in Python: usage, conventions, and limitations\nWhat’s New in This Chapter\nI added a new epigraph and a few words in the second paragraph of the chapter to\naddress the concept of “Pythonic”—which was only discussed at the very end in the\nfirst edition.\n“Formatted Displays” on page 370 was updated to mention f-strings, introduced in\nPython 3.6. It’s a small change because f-strings support the same formatting mini-\nlanguage as the format() built-in and the str.format() method, so any previously\nimplemented __format__ methods simply work with f-strings.\nThe rest of the chapter barely changed—the special methods are mostly the same\nsince Python 3.0, and the core ideas appeared in Python 2.2.\nLet’s get started with the object representation methods.\nObject Representations\nEvery object-oriented language has at least one standard way of getting a string repre‐\nsentation from any object. Python has two:\nrepr()\nReturn a string representing the object as the developer wants to see it. It’s what\nyou get when the Python console or a debugger shows an object.\nstr()\nReturn a string representing the object as the user wants to see it. It’s what you\nget when you print() an object.\nThe special methods __repr__ and __str__ support repr() and str(), as we saw in\nChapter 1.\n364 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "There are two additional special methods to support alternative representations of\nobjects: __bytes__ and __format__. The __bytes__ method is analogous to __str__:\nit’s called by bytes() to get the object represented as a byte sequence. Regarding\n__format__, it is used by f-strings, by the built-in function format(), and by the\nstr.format() method. They call obj.__format__(format_spec) to get string dis‐\nplays of objects using special formatting codes. We’ll cover __bytes__ in the next\nexample, and __format__ after that.\nIf you’re coming from Python 2, remember that in Python 3\n__repr__, __str__, and __format__ must always return Unicode\nstrings (type str). Only __bytes__ is supposed to return a byte\nsequence (type bytes).\nVector Class Redux\nIn order to demonstrate the many methods used to generate object representations,\nwe’ll use a Vector2d class similar to the one we saw in Chapter 1. We will build on it\nin this and future sections. Example 11-1 illustrates the basic behavior we expect\nfrom a Vector2d instance.\nExample 11-1. Vector2d instances have several representations\n    >>> v1 = Vector2d(3, 4)\n    >>> print(v1.x, v1.y)  \n    3.0 4.0\n    >>> x, y = v1  \n    >>> x, y\n    (3.0, 4.0)\n    >>> v1  \n    Vector2d(3.0, 4.0)\n    >>> v1_clone = eval(repr(v1))  \n    >>> v1 == v1_clone  \n    True\n    >>> print(v1)  \n    (3.0, 4.0)\n    >>> octets = bytes(v1)  \n    >>> octets\n    b'd\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x08@\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x10@'\n    >>> abs(v1)  \n    5.0\n    >>> bool(v1), bool(Vector2d(0, 0))  \n    (True, False)\nVector Class Redux \n| \n365",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "2 I used eval to clone the object here just to make a point about repr; to clone an instance, the copy.copy\nfunction is safer and faster.\nThe components of a Vector2d can be accessed directly as attributes (no getter\nmethod calls).\nA Vector2d can be unpacked to a tuple of variables.\nThe repr of a Vector2d emulates the source code for constructing the instance.\nUsing eval here shows that the repr of a Vector2d is a faithful representation of\nits constructor call.2\nVector2d supports comparison with ==; this is useful for testing.\nprint calls str, which for Vector2d produces an ordered pair display.\nbytes uses the __bytes__ method to produce a binary representation.\nabs uses the __abs__ method to return the magnitude of the Vector2d.\nbool uses the __bool__ method to return False for a Vector2d of zero magni‐\ntude or True otherwise.\nVector2d from Example 11-1 is implemented in vector2d_v0.py (Example 11-2). The\ncode is based on Example 1-2, except for the methods for the + and * operations,\nwhich we’ll see later in Chapter 16. We’ll add the method for == since it’s useful for\ntesting. At this point, Vector2d uses several special methods to provide operations\nthat a Pythonista expects in a well-designed object.\nExample 11-2. vector2d_v0.py: methods so far are all special methods\nfrom array import array\nimport math\nclass Vector2d:\n    typecode = 'd'  \n    def __init__(self, x, y):\n        self.x = float(x)    \n        self.y = float(y)\n    def __iter__(self):\n        return (i for i in (self.x, self.y))  \n366 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "3 This line could also be written as yield self.x; yield.self.y. I have a lot more to say about the __iter__\nspecial method, generator expressions, and the yield keyword in Chapter 17.\n    def __repr__(self):\n        class_name = type(self).__name__\n        return '{}({!r}, {!r})'.format(class_name, *self)  \n    def __str__(self):\n        return str(tuple(self))  \n    def __bytes__(self):\n        return (bytes([ord(self.typecode)]) +  \n                bytes(array(self.typecode, self)))  \n    def __eq__(self, other):\n        return tuple(self) == tuple(other)  \n    def __abs__(self):\n        return math.hypot(self.x, self.y)  \n    def __bool__(self):\n        return bool(abs(self))  \ntypecode is a class attribute we’ll use when converting Vector2d instances to/\nfrom bytes.\nConverting x and y to float in __init__ catches errors early, which is helpful in\ncase Vector2d is called with unsuitable arguments.\n__iter__ makes a Vector2d iterable; this is what makes unpacking work (e.g, x,\ny = my_vector). We implement it simply by using a generator expression to\nyield the components one after the other.3\n__repr__ builds a string by interpolating the components with {!r} to get their\nrepr; because Vector2d is iterable, *self feeds the x and y components to\nformat.\nFrom an iterable Vector2d, it’s easy to build a tuple for display as an ordered\npair.\nTo generate bytes, we convert the typecode to bytes and concatenate…\n…bytes converted from an array built by iterating over the instance.\nVector Class Redux \n| \n367",
      "content_length": 1523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "To quickly compare all components, build tuples out of the operands. This works\nfor operands that are instances of Vector2d, but has issues. See the following\nwarning.\nThe magnitude is the length of the hypotenuse of the right triangle formed by the\nx and y components.\n__bool__ uses abs(self) to compute the magnitude, then converts it to bool, so\n0.0 becomes False, nonzero is True.\nMethod __eq__ in Example 11-2 works for Vector2d operands but\nalso returns True when comparing Vector2d instances to other\niterables holding the same numeric values (e.g., Vector(3, 4) ==\n[3, 4]). This may be considered a feature or a bug. Further discus‐\nsion needs to wait until Chapter 16, when we cover operator over‐\nloading.\nWe have a fairly complete set of basic methods, but we still need a way to rebuild a\nVector2d from the binary representation produced by bytes().\nAn Alternative Constructor\nSince we can export a Vector2d as bytes, naturally we need a method that imports a\nVector2d from a binary sequence. Looking at the standard library for inspiration, we\nfind that array.array has a class method named .frombytes that suits our purpose\n—we saw it in “Arrays” on page 59. We adopt its name and use its functionality in a\nclass method for Vector2d in vector2d_v1.py (Example 11-3).\nExample 11-3. Part of vector2d_v1.py: this snippet shows only the frombytes class\nmethod, added to the Vector2d definition in vector2d_v0.py (Example 11-2)\n    @classmethod  \n    def frombytes(cls, octets):  \n        typecode = chr(octets[0])  \n        memv = memoryview(octets[1:]).cast(typecode)  \n        return cls(*memv)  \nThe classmethod decorator modifies a method so it can be called directly on a\nclass.\nNo self argument; instead, the class itself is passed as the first argument—con‐\nventionally named cls.\n368 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "4 We had a brief introduction to memoryview, explaining its .cast method, in “Memory Views” on page 62.\nRead the typecode from the first byte.\nCreate a memoryview from the octets binary sequence and use the typecode to\ncast it.4\nUnpack the memoryview resulting from the cast into the pair of arguments\nneeded for the constructor.\nI just used a classmethod decorator and it is very Python specific, so let’s have a word\nabout it.\nclassmethod Versus staticmethod\nThe classmethod decorator is not mentioned in the Python tutorial, and neither is\nstaticmethod. Anyone who has learned OO in Java may wonder why Python has\nboth of these decorators and not just one of them.\nLet’s start with classmethod. Example 11-3 shows its use: to define a method that\noperates on the class and not on instances. classmethod changes the way the method\nis called, so it receives the class itself as the first argument, instead of an instance. Its\nmost common use is for alternative constructors, like frombytes in Example 11-3.\nNote how the last line of frombytes actually uses the cls argument by invoking it to\nbuild a new instance: cls(*memv).\nIn contrast, the staticmethod decorator changes a method so that it receives no spe‐\ncial first argument. In essence, a static method is just like a plain function that hap‐\npens to live in a class body, instead of being defined at the module level.\nExample 11-4 contrasts the operation of classmethod and staticmethod.\nExample 11-4. Comparing behaviors of classmethod and staticmethod\n>>> class Demo:\n...     @classmethod\n...     def klassmeth(*args):\n...         return args  \n...     @staticmethod\n...     def statmeth(*args):\n...         return args  \n...\n>>> Demo.klassmeth()  \n(<class '__main__.Demo'>,)\n>>> Demo.klassmeth('spam')\nclassmethod Versus staticmethod \n| \n369",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "5 Leonardo Rochael, one of the technical reviewers of this book, disagrees with my low opinion of staticme\nthod, and recommends the blog post “The Definitive Guide on How to Use Static, Class or Abstract Methods\nin Python” by Julien Danjou as a counterargument. Danjou’s post is very good; I do recommend it. But it\nwasn’t enough to change my mind about staticmethod. You’ll have to decide for yourself.\n(<class '__main__.Demo'>, 'spam')\n>>> Demo.statmeth()   \n()\n>>> Demo.statmeth('spam')\n('spam',)\nklassmeth just returns all positional arguments.\nstatmeth does the same.\nNo matter how you invoke it, Demo.klassmeth receives the Demo class as the first\nargument.\nDemo.statmeth behaves just like a plain old function.\nThe classmethod decorator is clearly useful, but good use cases for\nstaticmethod are very rare in my experience. Maybe the function\nis closely related even if it never touches the class, so you may want\nto place it nearby in the code. Even then, defining the function\nright before or after the class in the same module is close enough\nmost of the time.5\nNow that we’ve seen what classmethod is good for (and that staticmethod is not\nvery useful), let’s go back to the issue of object representation and see how to support\nformatted output.\nFormatted Displays\nThe f-strings, the format() built-in function, and the str.format() method delegate\nthe actual formatting to each type by calling their .__format__(format_spec)\nmethod. The format_spec is a formatting specifier, which is either:\n• The second argument in format(my_obj, format_spec), or\n• Whatever appears after the colon in a replacement field delimited with {} inside\nan f-string or the fmt in fmt.str.format()\n370 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "For example:\n>>> brl = 1 / 4.82  # BRL to USD currency conversion rate\n>>> brl\n0.20746887966804978\n>>> format(brl, '0.4f')  \n'0.2075'\n>>> '1 BRL = {rate:0.2f} USD'.format(rate=brl)  \n'1 BRL = 0.21 USD'\n>>> f'1 USD = {1 / brl:0.2f} BRL'  \n'1 USD = 4.82 BRL'\nFormatting specifier is '0.4f'.\nFormatting specifier is '0.2f'. The rate part in the replacement field is not part\nof the formatting specifier. It determines which keyword argument of .format()\ngoes into that replacement field.\nAgain, the specifier is '0.2f'. The 1 / brl expression is not part of it.\nThe second and third callouts make an important point: a format string such as\n'{0.mass:5.3e}' actually uses two separate notations. The '0.mass' to the left of the\ncolon is the field_name part of the replacement field syntax, and it can be an arbi‐\ntrary expression in an f-string. The '5.3e' after the colon is the formatting specifier.\nThe notation used in the formatting specifier is called the Format Specification Mini-\nLanguage.\nIf f-strings, format(), and str.format() are new to you, classroom\nexperience tells me it’s best to study the format() built-in function\nfirst, which uses just the Format Specification Mini-Language.\nAfter you get the gist of that, read “Formatted string literals” and\n“Format String Syntax” to learn about the {:} replacement field\nnotation, used in f-strings and the str.format() method (includ‐\ning the !s, !r, and !a conversion flags). F-strings don’t make\nstr.format() obsolete: most of the time f-strings solve the prob‐\nlem, but sometimes it’s better to specify the formatting string else‐\nwhere, and not where it will be rendered.\nA few built-in types have their own presentation codes in the Format Specification\nMini-Language. For example—among several other codes—the int type supports b\nand x for base 2 and base 16 output, respectively, while float implements f for a\nfixed-point display and % for a percentage display:\n>>> format(42, 'b')\n'101010'\nFormatted Displays \n| \n371",
      "content_length": 1983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": ">>> format(2 / 3, '.1%')\n'66.7%'\nThe Format Specification Mini-Language is extensible because each class gets to\ninterpret the format_spec argument as it likes. For instance, the classes in the date\ntime module use the same format codes in the strftime() functions and in their\n__format__ methods. Here are a couple of examples using the format() built-in and\nthe str.format() method:\n>>> from datetime import datetime\n>>> now = datetime.now()\n>>> format(now, '%H:%M:%S')\n'18:49:05'\n>>> \"It's now {:%I:%M %p}\".format(now)\n\"It's now 06:49 PM\"\nIf a class has no __format__, the method inherited from object returns\nstr(my_object). Because Vector2d has a __str__, this works:\n>>> v1 = Vector2d(3, 4)\n>>> format(v1)\n'(3.0, 4.0)'\nHowever, if you pass a format specifier, object.__format__ raises TypeError:\n>>> format(v1, '.3f')\nTraceback (most recent call last):\n  ...\nTypeError: non-empty format string passed to object.__format__\nWe will fix that by implementing our own format mini-language. The first step will\nbe to assume the format specifier provided by the user is intended to format each\nfloat component of the vector. This is the result we want:\n>>> v1 = Vector2d(3, 4)\n>>> format(v1)\n'(3.0, 4.0)'\n>>> format(v1, '.2f')\n'(3.00, 4.00)'\n>>> format(v1, '.3e')\n'(3.000e+00, 4.000e+00)'\nExample 11-5 implements __format__ to produce the displays just shown.\nExample 11-5. Vector2d.__format__ method, take #1\n    # inside the Vector2d class\n    def __format__(self, fmt_spec=''):\n        components = (format(c, fmt_spec) for c in self)  \n        return '({}, {})'.format(*components)  \n372 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "Use the format built-in to apply the fmt_spec to each vector component, build‐\ning an iterable of formatted strings.\nPlug the formatted strings in the formula '(x, y)'.\nNow let’s add a custom formatting code to our mini-language: if the format specifier\nends with a 'p', we’ll display the vector in polar coordinates: <r, θ>, where r is the\nmagnitude and θ (theta) is the angle in radians. The rest of the format specifier\n(whatever comes before the 'p') will be used as before.\nWhen choosing the letter for the custom format code, I avoided\noverlapping with codes used by other types. In Format Specifica‐\ntion Mini-Language, we see that integers use the codes 'bcdoxXn',\nfloats use 'eEfFgGn%', and strings use 's'. So I picked 'p' for\npolar coordinates. Because each class interprets these codes inde‐\npendently, reusing a code letter in a custom format for a new type\nis not an error, but may be confusing to users.\nTo generate polar coordinates, we already have the __abs__ method for the magni‐\ntude, and we’ll code a simple angle method using the math.atan2() function to get\nthe angle. This is the code:\n    # inside the Vector2d class\n    def angle(self):\n        return math.atan2(self.y, self.x)\nWith that, we can enhance our __format__ to produce polar coordinates. See\nExample 11-6.\nExample 11-6. Vector2d.__format__ method, take #2, now with polar coordinates\n    def __format__(self, fmt_spec=''):\n        if fmt_spec.endswith('p'):  \n            fmt_spec = fmt_spec[:-1]  \n            coords = (abs(self), self.angle())  \n            outer_fmt = '<{}, {}>'  \n        else:\n            coords = self  \n            outer_fmt = '({}, {})'  \n        components = (format(c, fmt_spec) for c in coords)  \n        return outer_fmt.format(*components)  \nFormat ends with 'p': use polar coordinates.\nRemove 'p' suffix from fmt_spec.\nFormatted Displays \n| \n373",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "Build tuple of polar coordinates: (magnitude, angle).\nConfigure outer format with angle brackets.\nOtherwise, use x, y components of self for rectangular coordinates.\nConfigure outer format with parentheses.\nGenerate iterable with components as formatted strings.\nPlug formatted strings into outer format.\nWith Example 11-6, we get results similar to these:\n>>> format(Vector2d(1, 1), 'p')\n'<1.4142135623730951, 0.7853981633974483>'\n>>> format(Vector2d(1, 1), '.3ep')\n'<1.414e+00, 7.854e-01>'\n>>> format(Vector2d(1, 1), '0.5fp')\n'<1.41421, 0.78540>'\nAs this section shows, it’s not hard to extend the Format Specification Mini-\nLanguage to support user-defined types.\nNow let’s move to a subject that’s not just about appearances: we will make our\nVector2d hashable, so we can build sets of vectors, or use them as dict keys.\nA Hashable Vector2d\nAs defined, so far our Vector2d instances are unhashable, so we can’t put them in a\nset:\n>>> v1 = Vector2d(3, 4)\n>>> hash(v1)\nTraceback (most recent call last):\n  ...\nTypeError: unhashable type: 'Vector2d'\n>>> set([v1])\nTraceback (most recent call last):\n  ...\nTypeError: unhashable type: 'Vector2d'\nTo make a Vector2d hashable, we must implement __hash__ (__eq__ is also\nrequired, and we already have it). We also need to make vector instances immutable,\nas we’ve seen in “What Is Hashable” on page 84.\n374 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1386,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "6 The pros and cons of private attributes are the subject of the upcoming “Private and ‘Protected’ Attributes in\nPython” on page 382.\nRight now, anyone can do v1.x = 7, and there is nothing in the code to suggest that\nchanging a Vector2d is forbidden. This is the behavior we want:\n>>> v1.x, v1.y\n(3.0, 4.0)\n>>> v1.x = 7\nTraceback (most recent call last):\n  ...\nAttributeError: can't set attribute\nWe’ll do that by making the x and y components read-only properties in\nExample 11-7.\nExample 11-7. vector2d_v3.py: only the changes needed to make Vector2d immutable\nare shown here; see full listing in Example 11-11\nclass Vector2d:\n    typecode = 'd'\n    def __init__(self, x, y):\n        self.__x = float(x)  \n        self.__y = float(y)\n    @property  \n    def x(self):  \n        return self.__x  \n    @property  \n    def y(self):\n        return self.__y\n    def __iter__(self):\n        return (i for i in (self.x, self.y))  \n    # remaining methods: same as previous Vector2d\nUse exactly two leading underscores (with zero or one trailing underscore) to\nmake an attribute private.6\nThe @property decorator marks the getter method of a property.\nThe getter method is named after the public property it exposes: x.\nJust return self.__x.\nA Hashable Vector2d \n| \n375",
      "content_length": 1263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "Repeat the same formula for y property.\nEvery method that just reads the x, y components can stay as it was, reading the\npublic properties via self.x and self.y instead of the private attribute, so this\nlisting omits the rest of the code for the class.\nVector.x and Vector.y are examples of read-only properties.\nRead/write properties will be covered in Chapter 22, where we dive\ndeeper into @property.\nNow that our vectors are reasonably safe from accidental mutation, we can imple‐\nment the __hash__ method. It should return an int and ideally take into account the\nhashes of the object attributes that are also used in the __eq__ method, because\nobjects that compare equal should have the same hash. The __hash__ special method\ndocumentation suggests computing the hash of a tuple with the components, so\nthat’s what we do in Example 11-8.\nExample 11-8. vector2d_v3.py: implementation of hash\n    # inside class Vector2d:\n    def __hash__(self):\n        return hash((self.x, self.y))\nWith the addition of the __hash__ method, we now have hashable vectors:\n>>> v1 = Vector2d(3, 4)\n>>> v2 = Vector2d(3.1, 4.2)\n>>> hash(v1), hash(v2)\n(1079245023883434373, 1994163070182233067)\n>>> {v1, v2}\n{Vector2d(3.1, 4.2), Vector2d(3.0, 4.0)}\nIt’s not strictly necessary to implement properties or otherwise\nprotect the instance attributes to create a hashable type. Imple‐\nmenting __hash__ and __eq__ correctly is all it takes. But the value\nof a hashable object is never supposed to change, so this provided a\ngood excuse to talk about read-only properties.\nIf you are creating a type that has a sensible scalar numeric value, you may also\nimplement the __int__ and __float__ methods, invoked by the int() and float()\nconstructors, which are used for type coercion in some contexts. There is also a\n376 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "__complex__ method to support the complex() built-in constructor. Perhaps Vec\ntor2d should provide __complex__, but I’ll leave that as an exercise for you.\nSupporting Positional Pattern Matching\nSo far, Vector2d instances are compatible with keyword class patterns—covered in\n“Keyword Class Patterns” on page 193.\nIn Example 11-9, all of these keyword patterns work as expected.\nExample 11-9. Keyword patterns for Vector2d subjects—requires Python 3.10\ndef keyword_pattern_demo(v: Vector2d) -> None:\n    match v:\n        case Vector2d(x=0, y=0):\n            print(f'{v!r} is null')\n        case Vector2d(x=0):\n            print(f'{v!r} is vertical')\n        case Vector2d(y=0):\n            print(f'{v!r} is horizontal')\n        case Vector2d(x=x, y=y) if x==y:\n            print(f'{v!r} is diagonal')\n        case _:\n            print(f'{v!r} is awesome')\nHowever, if you try to use a positional pattern like this:\n        case Vector2d(_, 0):\n            print(f'{v!r} is horizontal')\nyou get:\nTypeError: Vector2d() accepts 0 positional sub-patterns (1 given)\nTo make Vector2d work with positional patterns, we need to add a class attribute\nnamed __match_args__ , listing the instance attributes in the order they will be used\nfor positional pattern matching:\nclass Vector2d:\n    __match_args__ = ('x', 'y')\n    # etc...\nNow we can save a few keystrokes when writing patterns to match Vector2d subjects,\nas you can see in Example 11-10.\nExample 11-10. Positional patterns for Vector2d subjects—requires Python 3.10\ndef positional_pattern_demo(v: Vector2d) -> None:\n    match v:\nSupporting Positional Pattern Matching \n| \n377",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "case Vector2d(0, 0):\n            print(f'{v!r} is null')\n        case Vector2d(0):\n            print(f'{v!r} is vertical')\n        case Vector2d(_, 0):\n            print(f'{v!r} is horizontal')\n        case Vector2d(x, y) if x==y:\n            print(f'{v!r} is diagonal')\n        case _:\n            print(f'{v!r} is awesome')\nThe __match_args__ class attribute does not need to include all public instance\nattributes. In particular, if the class __init__ has required and optional arguments\nthat are assigned to instance attributes, it may be reasonable to name the required\narguments in __match_args__, but not the optional ones.\nLet’s step back and review what we’ve coded so far in Vector2d.\nComplete Listing of Vector2d, Version 3\nWe have been working on Vector2d for a while, showing just snippets, so\nExample 11-11 is a consolidated, full listing of vector2d_v3.py, including the doctests\nI used when developing it.\nExample 11-11. vector2d_v3.py: the full monty\n\"\"\"\nA two-dimensional vector class\n    >>> v1 = Vector2d(3, 4)\n    >>> print(v1.x, v1.y)\n    3.0 4.0\n    >>> x, y = v1\n    >>> x, y\n    (3.0, 4.0)\n    >>> v1\n    Vector2d(3.0, 4.0)\n    >>> v1_clone = eval(repr(v1))\n    >>> v1 == v1_clone\n    True\n    >>> print(v1)\n    (3.0, 4.0)\n    >>> octets = bytes(v1)\n    >>> octets\n    b'd\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x08@\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x10@'\n    >>> abs(v1)\n    5.0\n    >>> bool(v1), bool(Vector2d(0, 0))\n    (True, False)\n378 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "Test of ``.frombytes()`` class method:\n    >>> v1_clone = Vector2d.frombytes(bytes(v1))\n    >>> v1_clone\n    Vector2d(3.0, 4.0)\n    >>> v1 == v1_clone\n    True\nTests of ``format()`` with Cartesian coordinates:\n    >>> format(v1)\n    '(3.0, 4.0)'\n    >>> format(v1, '.2f')\n    '(3.00, 4.00)'\n    >>> format(v1, '.3e')\n    '(3.000e+00, 4.000e+00)'\nTests of the ``angle`` method::\n    >>> Vector2d(0, 0).angle()\n    0.0\n    >>> Vector2d(1, 0).angle()\n    0.0\n    >>> epsilon = 10**-8\n    >>> abs(Vector2d(0, 1).angle() - math.pi/2) < epsilon\n    True\n    >>> abs(Vector2d(1, 1).angle() - math.pi/4) < epsilon\n    True\nTests of ``format()`` with polar coordinates:\n    >>> format(Vector2d(1, 1), 'p')  # doctest:+ELLIPSIS\n    '<1.414213..., 0.785398...>'\n    >>> format(Vector2d(1, 1), '.3ep')\n    '<1.414e+00, 7.854e-01>'\n    >>> format(Vector2d(1, 1), '0.5fp')\n    '<1.41421, 0.78540>'\nTests of `x` and `y` read-only properties:\n    >>> v1.x, v1.y\n    (3.0, 4.0)\n    >>> v1.x = 123\n    Traceback (most recent call last):\n      ...\n    AttributeError: can't set attribute 'x'\nComplete Listing of Vector2d, Version 3 \n| \n379",
      "content_length": 1120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "Tests of hashing:\n    >>> v1 = Vector2d(3, 4)\n    >>> v2 = Vector2d(3.1, 4.2)\n    >>> len({v1, v2})\n    2\n\"\"\"\nfrom array import array\nimport math\nclass Vector2d:\n    __match_args__ = ('x', 'y')\n    typecode = 'd'\n    def __init__(self, x, y):\n        self.__x = float(x)\n        self.__y = float(y)\n    @property\n    def x(self):\n        return self.__x\n    @property\n    def y(self):\n        return self.__y\n    def __iter__(self):\n        return (i for i in (self.x, self.y))\n    def __repr__(self):\n        class_name = type(self).__name__\n        return '{}({!r}, {!r})'.format(class_name, *self)\n    def __str__(self):\n        return str(tuple(self))\n    def __bytes__(self):\n        return (bytes([ord(self.typecode)]) +\n                bytes(array(self.typecode, self)))\n    def __eq__(self, other):\n        return tuple(self) == tuple(other)\n    def __hash__(self):\n        return hash((self.x, self.y))\n    def __abs__(self):\n380 \n| \nChapter 11: A Pythonic Object",
      "content_length": 972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "return math.hypot(self.x, self.y)\n    def __bool__(self):\n        return bool(abs(self))\n    def angle(self):\n        return math.atan2(self.y, self.x)\n    def __format__(self, fmt_spec=''):\n        if fmt_spec.endswith('p'):\n            fmt_spec = fmt_spec[:-1]\n            coords = (abs(self), self.angle())\n            outer_fmt = '<{}, {}>'\n        else:\n            coords = self\n            outer_fmt = '({}, {})'\n        components = (format(c, fmt_spec) for c in coords)\n        return outer_fmt.format(*components)\n    @classmethod\n    def frombytes(cls, octets):\n        typecode = chr(octets[0])\n        memv = memoryview(octets[1:]).cast(typecode)\n        return cls(*memv)\nTo recap, in this and the previous sections, we saw some essential special methods\nthat you may want to implement to have a full-fledged object.\nYou should only implement these special methods if your applica‐\ntion needs them. End users don’t care if the objects that make up\nthe application are “Pythonic” or not.\nOn the other hand, if your classes are part of a library for other\nPython programmers to use, you can’t really guess what they will\ndo with your objects, and they may expect more of the “Pythonic”\nbehaviors we are describing.\nAs coded in Example 11-11, Vector2d is a didactic example with a laundry list of spe‐\ncial methods related to object representation, not a template for every user-defined\nclass.\nIn the next section, we’ll take a break from Vector2d to discuss the design and draw‐\nbacks of the private attribute mechanism in Python—the double-underscore prefix in\nself.__x.\nComplete Listing of Vector2d, Version 3 \n| \n381",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "Private and “Protected” Attributes in Python\nIn Python, there is no way to create private variables like there is with the private\nmodifier in Java. What we have in Python is a simple mechanism to prevent acciden‐\ntal overwriting of a “private” attribute in a subclass.\nConsider this scenario: someone wrote a class named Dog that uses a mood instance\nattribute internally, without exposing it. You need to subclass Dog as Beagle. If you\ncreate your own mood instance attribute without being aware of the name clash, you\nwill clobber the mood attribute used by the methods inherited from Dog. This would\nbe a pain to debug.\nTo prevent this, if you name an instance attribute in the form __mood (two leading\nunderscores and zero or at most one trailing underscore), Python stores the name in\nthe instance __dict__ prefixed with a leading underscore and the class name, so in\nthe Dog class, __mood becomes _Dog__mood, and in Beagle it’s _Beagle__mood. This\nlanguage feature goes by the lovely name of name mangling.\nExample 11-12 shows the result in the Vector2d class from Example 11-7.\nExample 11-12. Private attribute names are “mangled” by prefixing the _ and the class\nname\n>>> v1 = Vector2d(3, 4)\n>>> v1.__dict__\n{'_Vector2d__y': 4.0, '_Vector2d__x': 3.0}\n>>> v1._Vector2d__x\n3.0\nName mangling is about safety, not security: it’s designed to prevent accidental access\nand not malicious prying. Figure 11-1 illustrates another safety device.\nAnyone who knows how private names are mangled can read the private attribute\ndirectly, as the last line of Example 11-12 shows—that’s actually useful for debugging\nand serialization. They can also directly assign a value to a private component of a\nVector2d by writing v1._Vector2d__x = 7. But if you are doing that in production\ncode, you can’t complain if something blows up.\nThe name mangling functionality is not loved by all Pythonistas, and neither is the\nskewed look of names written as self.__x. Some prefer to avoid this syntax and use\njust one underscore prefix to “protect” attributes by convention (e.g., self._x). Crit‐\nics of the automatic double-underscore mangling suggest that concerns about acci‐\ndental attribute clobbering should be addressed by naming conventions. Ian Bicking\n—creator of pip, virtualenv, and other projects—wrote:\n382 \n| \nChapter 11: A Pythonic Object",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "7 From the “Paste Style Guide”.\n8 In modules, a single _ in front of a top-level name does have an effect: if you write from mymod import *, the\nnames with a _ prefix are not imported from mymod. However, you can still write from mymod import _priva\ntefunc. This is explained in the Python Tutorial, section 6.1., “More on Modules”.\n9 One example is in the gettext module docs.\nNever, ever use two leading underscores. This is annoyingly private. If name clashes\nare a concern, use explicit name mangling instead (e.g., _MyThing_blahblah). This is\nessentially the same thing as double-underscore, only it’s transparent where double\nunderscore obscures.7\nFigure 11-1. A cover on a switch is a safety device, not a security one: it prevents acci‐\ndents, not sabotage.\nThe single underscore prefix has no special meaning to the Python interpreter when\nused in attribute names, but it’s a very strong convention among Python program‐\nmers that you should not access such attributes from outside the class.8 It’s easy to\nrespect the privacy of an object that marks its attributes with a single _, just as it’s\neasy respect the convention that variables in ALL_CAPS should be treated as constants.\nAttributes with a single _ prefix are called “protected” in some corners of the Python\ndocumentation.9 The practice of “protecting” attributes by convention with the form\nself._x is widespread, but calling that a “protected” attribute is not so common.\nSome even call that a “private” attribute.\nPrivate and “Protected” Attributes in Python \n| \n383",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "10 If this state of affairs depresses you, and makes you wish Python was more like Java in this regard, don’t read\nmy discussion of the relative strength of the Java private modifier in “Soapbox” on page 394.\nTo conclude: the Vector2d components are “private” and our Vector2d instances are\n“immutable”—with scare quotes—because there is no way to make them really pri‐\nvate and immutable.10\nWe’ll now come back to our Vector2d class. In the next section, we cover a special\nattribute (not a method) that affects the internal storage of an object, with potentially\nhuge impact on the use of memory but little effect on its public interface: __slots__.\nSaving Memory with __slots__\nBy default, Python stores the attributes of each instance in a dict named __dict__.\nAs we saw in “Practical Consequences of How dict Works” on page 102, a dict has a\nsignificant memory overhead—even with the optimizations mentioned in that sec‐\ntion. But if you define a class attribute named __slots__ holding a sequence of\nattribute names, Python uses an alternative storage model for the instance attributes:\nthe attributes named in __slots__ are stored in a hidden array or references that use\nless memory than a dict. Let’s see how that works through simple examples, starting\nwith Example 11-13.\nExample 11-13. The Pixel class uses __slots__\n>>> class Pixel:\n...     __slots__ = ('x', 'y')  \n...\n>>> p = Pixel()  \n>>> p.__dict__  \nTraceback (most recent call last):\n  ...\nAttributeError: 'Pixel' object has no attribute '__dict__'\n>>> p.x = 10  \n>>> p.y = 20\n>>> p.color = 'red'  \nTraceback (most recent call last):\n  ...\nAttributeError: 'Pixel' object has no attribute 'color'\n__slots__ must be present when the class is created; adding or changing it later\nhas no effect. The attribute names may be in a tuple or list, but I prefer a tuple\nto make it clear there’s no point in changing it.\n384 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "Create an instance of Pixel, because we see the effects of __slots__ on the\ninstances.\nFirst effect: instances of Pixel have no __dict__.\nSet the p.x and p.y attributes normally.\nSecond effect: trying to set an attribute not listed in __slots__ raises\nAttributeError.\nSo far, so good. Now let’s create a subclass of Pixel in Example 11-14 to see the\ncounterintuitive side of __slots__.\nExample 11-14. The OpenPixel is a subclass of Pixel\n>>> class OpenPixel(Pixel):  \n...     pass\n...\n>>> op = OpenPixel()\n>>> op.__dict__  \n{}\n>>> op.x = 8  \n>>> op.__dict__  \n{}\n>>> op.x  \n8\n>>> op.color = 'green'  \n>>> op.__dict__  \n{'color': 'green'}\nOpenPixel declares no attributes of its own.\nSurprise: instances of OpenPixel have a __dict__.\nIf you set attribute x (named in the __slots__ of the base class Pixel)…\n…it is not stored in the instance __dict__…\n…but it is stored in the hidden array of references in the instance.\nIf you set an attribute not named in the __slots__…\n…it is stored in the instance __dict__.\nSaving Memory with __slots__ \n| \n385",
      "content_length": 1047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "Example 11-14 shows that the effect of __slots__ is only partially inherited by a sub‐\nclass. To make sure that instances of a subclass have no __dict__, you must declare\n__slots__ again in the subclass.\nIf you declare __slots__ = () (an empty tuple), then the instances of the subclass\nwill have no __dict__ and will only accept the attributes named in the __slots__ of\nthe base class.\nIf you want a subclass to have additional attributes, name them in __slots__, as\nshown in Example 11-15.\nExample 11-15. The ColorPixel, another subclass of Pixel\n>>> class ColorPixel(Pixel):\n...    __slots__ = ('color',)  \n>>> cp = ColorPixel()\n>>> cp.__dict__  \nTraceback (most recent call last):\n  ...\nAttributeError: 'ColorPixel' object has no attribute '__dict__'\n>>> cp.x = 2\n>>> cp.color = 'blue'  \n>>> cp.flavor = 'banana'\nTraceback (most recent call last):\n  ...\nAttributeError: 'ColorPixel' object has no attribute 'flavor'\nEssentially, __slots__ of the superclasses are added to the __slots__ of the cur‐\nrent class. Don’t forget that single-item tuples must have a trailing comma.\nColorPixel instances have no __dict__.\nYou can set the attributes declared in the __slots__ of this class and super‐\nclasses, but no other.\nIt’s possible to “save memory and eat it too”: if you add the '__dict__' name to the\n__slots__ list, your instances will keep attributes named in __slots__ in the per-\ninstance array of references, but will also support dynamically created attributes,\nwhich will be stored in the usual __dict__. This is necessary if you want to use the\n@cached_property decorator (covered in “Step 5: Caching Properties with functools”\non page 855).\nOf course, having '__dict__' in __slots__ may entirely defeat its purpose, depend‐\ning on the number of static and dynamic attributes in each instance and how they are\nused. Careless optimization is worse than premature optimization: you add complex‐\nity but may not get any benefit.\n386 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "Another special per-instance attribute that you may want to keep is __weakref__,\nnecessary for an object to support weak references (mentioned briefly in “del and\nGarbage Collection” on page 219). That attribute exists by default in instances of\nuser-defined classes. However, if the class defines __slots__, and you need the\ninstances to be targets of weak references, then you need to include '__weakref__'\namong the attributes named in __slots__.\nNow let’s see the effect of adding __slots__ to Vector2d.\nSimple Measure of __slot__ Savings\nExample 11-16 shows the implementation of __slots__ in Vector2d.\nExample 11-16. vector2d_v3_slots.py: the __slots__ attribute is the only addition to\nVector2d\nclass Vector2d:\n    __match_args__ = ('x', 'y')  \n    __slots__ = ('__x', '__y')  \n    typecode = 'd'\n    # methods are the same as previous version\n__match_args__ lists the public attribute names for positional pattern matching.\nIn contrast, __slots__ lists the names of the instance attributes, which in this\ncase are private attributes.\nTo measure the memory savings, I wrote the mem_test.py script. It takes the name of\na module with a Vector2d class variant as command-line argument, and uses a list\ncomprehension to build a list with 10,000,000 instances of Vector2d. In the first run\nshown in Example 11-17, I use vector2d_v3.Vector2d (from Example 11-7); in the\nsecond run, I use the version with __slots__ from Example 11-16.\nExample 11-17. mem_test.py creates 10 million Vector2d instances using the class\ndefined in the named module\n$ time python3 mem_test.py vector2d_v3\nSelected Vector2d type: vector2d_v3.Vector2d\nCreating 10,000,000 Vector2d instances\nInitial RAM usage:      6,983,680\n  Final RAM usage:  1,666,535,424\nreal 0m11.990s\nuser 0m10.861s\nsys 0m0.978s\nSaving Memory with __slots__ \n| \n387",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "$ time python3 mem_test.py vector2d_v3_slots\nSelected Vector2d type: vector2d_v3_slots.Vector2d\nCreating 10,000,000 Vector2d instances\nInitial RAM usage:      6,995,968\n  Final RAM usage:    577,839,104\nreal 0m8.381s\nuser 0m8.006s\nsys 0m0.352s\nAs Example 11-17 reveals, the RAM footprint of the script grows to 1.55 GiB when\ninstance __dict__ is used in each of the 10 million Vector2d instances, but that is\nreduced to 551 MiB when Vector2d has a __slots__ attribute. The __slots__ ver‐\nsion is also faster. The mem_test.py script in this test basically deals with loading a\nmodule, checking memory usage, and formatting results. You can find its source\ncode in the fluentpython/example-code-2e repository.\nIf you are handling millions of objects with numeric data, you\nshould really be using NumPy arrays (see “NumPy” on page 64),\nwhich are not only memory efficient but have highly optimized\nfunctions for numeric processing, many of which operate on the\nentire array at once. I designed the Vector2d class just to provide\ncontext when discussing special methods, because I try to avoid\nvague foo and bar examples when I can.\nSummarizing the Issues with __slots__\nThe __slots__ class attribute may provide significant memory savings if properly\nused, but there are a few caveats:\n• You must remember to redeclare __slots__ in each subclass to prevent their\ninstances from having __dict__.\n• Instances will only be able to have the attributes listed in __slots__, unless you\ninclude '__dict__' in __slots__ (but doing so may negate the memory\nsavings).\n• Classes using __slots__ cannot use the @cached_property decorator, unless\nthey explicitly name '__dict__' in __slots__.\n• Instances cannot be targets of weak references, unless you add '__weakref__' in\n__slots__.\nThe last topic in this chapter has to do with overriding a class attribute in instances\nand subclasses.\n388 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Overriding Class Attributes\nA distinctive feature of Python is how class attributes can be used as default values for\ninstance attributes. In Vector2d there is the typecode class attribute. It’s used twice\nin the __bytes__ method, but we read it as self.typecode by design. Because Vec\ntor2d instances are created without a typecode attribute of their own, self.type\ncode will get the Vector2d.typecode class attribute by default.\nBut if you write to an instance attribute that does not exist, you create a new instance\nattribute—e.g., a typecode instance attribute—and the class attribute by the same\nname is untouched. However, from then on, whenever the code handling that\ninstance reads self.typecode, the instance typecode will be retrieved, effectively\nshadowing the class attribute by the same name. This opens the possibility of custom‐\nizing an individual instance with a different typecode.\nThe default Vector2d.typecode is 'd', meaning each vector component will be rep‐\nresented as an 8-byte double precision float when exporting to bytes. If we set the\ntypecode of a Vector2d instance to 'f' prior to exporting, each component will be\nexported as a 4-byte single precision float. Example 11-18 demonstrates.\nWe are discussing adding a custom instance attribute, therefore\nExample 11-18 uses the Vector2d implementation without\n__slots__, as listed in Example 11-11.\nExample 11-18. Customizing an instance by setting the typecode attribute that was\nformerly inherited from the class\n>>> from vector2d_v3 import Vector2d\n>>> v1 = Vector2d(1.1, 2.2)\n>>> dumpd = bytes(v1)\n>>> dumpd\nb'd\\x9a\\x99\\x99\\x99\\x99\\x99\\xf1?\\x9a\\x99\\x99\\x99\\x99\\x99\\x01@'\n>>> len(dumpd)  \n17\n>>> v1.typecode = 'f'  \n>>> dumpf = bytes(v1)\n>>> dumpf\nb'f\\xcd\\xcc\\x8c?\\xcd\\xcc\\x0c@'\n>>> len(dumpf)  \n9\n>>> Vector2d.typecode  \n'd'\nOverriding Class Attributes \n| \n389",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "Default bytes representation is 17 bytes long.\nSet typecode to 'f' in the v1 instance.\nNow the bytes dump is 9 bytes long.\nVector2d.typecode is unchanged; only the v1 instance uses typecode 'f'.\nNow it should be clear why the bytes export of a Vector2d is prefixed by the type\ncode: we wanted to support different export formats.\nIf you want to change a class attribute, you must set it on the class directly, not\nthrough an instance. You could change the default typecode for all instances (that\ndon’t have their own typecode) by doing this:\n>>> Vector2d.typecode = 'f'\nHowever, there is an idiomatic Python way of achieving a more permanent effect,\nand being more explicit about the change. Because class attributes are public, they are\ninherited by subclasses, so it’s common practice to subclass just to customize a class\ndata attribute. The Django class-based views use this technique extensively.\nExample 11-19 shows how.\nExample 11-19. The ShortVector2d is a subclass of Vector2d, which only overwrites\nthe default typecode\n>>> from vector2d_v3 import Vector2d\n>>> class ShortVector2d(Vector2d):  \n...     typecode = 'f'\n...\n>>> sv = ShortVector2d(1/11, 1/27)  \n>>> sv\nShortVector2d(0.09090909090909091, 0.037037037037037035)  \n>>> len(bytes(sv))  \n9\nCreate ShortVector2d as a Vector2d subclass just to overwrite the typecode\nclass attribute.\nBuild ShortVector2d instance sv for demonstration.\nInspect the repr of sv.\nCheck that the length of the exported bytes is 9, not 17 as before.\nThis example also explains why I did not hardcode the class_name in Vector2d.\n__repr__, but instead got it from type(self).__name__, like this:\n390 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "# inside class Vector2d:\n    def __repr__(self):\n        class_name = type(self).__name__\n        return '{}({!r}, {!r})'.format(class_name, *self)\nIf I had hardcoded the class_name, subclasses of Vector2d like ShortVector2d would\nhave to overwrite __repr__ just to change the class_name. By reading the name from\nthe type of the instance, I made __repr__ safer to inherit.\nThis ends our coverage of building a simple class that leverages the data model to\nplay well with the rest of Python: offering different object representations, providing\na custom formatting code, exposing read-only attributes, and supporting hash() to\nintegrate with sets and mappings.\nChapter Summary\nThe aim of this chapter was to demonstrate the use of special methods and conven‐\ntions in the construction of a well-behaved Pythonic class.\nIs vector2d_v3.py (shown in Example 11-11) more Pythonic than vector2d_v0.py\n(shown in Example 11-2)? The Vector2d class in vector2d_v3.py certainly exhibits\nmore Python features. But whether the first or the last Vector2d implementation is\nsuitable depends on the context where it would be used. Tim Peter’s “Zen of Python”\nsays:\nSimple is better than complex.\nAn object should be as simple as the requirements dictate—and not a parade of lan‐\nguage features. If the code is for an application, then it should focus on what is\nneeded to support the end users, not more. If the code is for a library for other pro‐\ngrammers to use, then it’s reasonable to implement special methods supporting\nbehaviors that Pythonistas expect. For example, __eq__ may not be necessary to sup‐\nport a business requirement, but it makes the class easier to test.\nMy goal in expanding the Vector2d code was to provide context for discussing\nPython special methods and coding conventions. The examples in this chapter have\ndemonstrated several of the special methods we first saw in Table 1-1 (Chapter 1):\n• String/bytes representation methods: __repr__, __str__, __format__, and\n__bytes__\n• Methods for reducing an object to a number: __abs__, __bool__, and __hash__\n• The __eq__ operator, to support testing and hashing (along with __hash__)\nChapter Summary \n| \n391",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "While supporting conversion to bytes, we also implemented an alternative construc‐\ntor, Vector2d.frombytes(), which provided the context for discussing the decora‐\ntors @classmethod (very handy) and @staticmethod (not so useful, module-level\nfunctions are simpler). The frombytes method was inspired by its namesake in the\narray.array class.\nWe saw that the Format Specification Mini-Language is extensible by implementing a\n__format__ method that parses a format_spec provided to the format(obj, for\nmat_spec) built-in or within replacement fields '{:«format_spec»}' in f-strings or\nstrings used with the str.format() method.\nIn preparation to make Vector2d instances hashable, we made an effort to make\nthem immutable, at least preventing accidental changes by coding the x and y\nattributes as private, and exposing them as read-only properties. We then\nimplemented __hash__ using the recommended technique of xor-ing the hashes of\nthe instance attributes.\nWe then discussed the memory savings and the caveats of declaring a __slots__\nattribute in Vector2d. Because using __slots__ has side effects, it really makes sense\nonly when handling a very large number of instances—think millions of instances,\nnot just thousands. In many such cases, using pandas may be the best option.\nThe last topic we covered was the overriding of a class attribute accessed via the\ninstances (e.g., self.typecode). We did that first by creating an instance attribute,\nand then by subclassing and overwriting at the class level.\nThroughout the chapter, I mentioned how design choices in the examples were\ninformed by studying the API of standard Python objects. If this chapter can be sum‐\nmarized in one sentence, this is it:\nTo build Pythonic objects, observe how real Python objects behave.\n—Ancient Chinese proverb\nFurther Reading\nThis chapter covered several special methods of the data model, so naturally the pri‐\nmary references are the same as the ones provided in Chapter 1, which gave a high-\nlevel view of the same topic. For convenience, I’ll repeat those four earlier\nrecommendations here, and add a few other ones:\nThe “Data Model” chapter of The Python Language Reference\nMost of the methods we used in this chapter are documented in “3.3.1. Basic cus‐\ntomization”.\n392 \n| \nChapter 11: A Pythonic Object",
      "content_length": 2301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "Python in a Nutshell, 3rd ed., by Alex Martelli, Anna Ravenscroft, and Steve Holden\nCovers the special methods in depth.\nPython Cookbook, 3rd ed., by David Beazley and Brian K. Jones\nModern Python practices demonstrated through recipes. Chapter 8, “Classes and\nObjects,” in particular has several solutions related to discussions in this chapter.\nPython Essential Reference, 4th ed., by David Beazley\nCovers the data model in detail, even if only Python 2.6 and 3.0 are covered (in\nthe fourth edition). The fundamental concepts are all the same and most of the\nData Model APIs haven’t changed at all since Python 2.2, when built-in types\nand user-defined classes were unified.\nIn 2015—the year I finished the first edition of Fluent Python—Hynek Schlawack\nstarted the attrs package. From the attrs documentation:\nattrs is the Python package that will bring back the joy of writing classes by relieving\nyou from the drudgery of implementing object protocols (aka dunder methods).\nI mentioned attrs as a more powerful alternative to @dataclass in “Further Read‐\ning” on page 196. The data class builders from Chapter 5 as well as attrs automati‐\ncally equip your classes with several special methods. But knowing how to code those\nspecial methods yourself is still essential to understand what those packages do, to\ndecide whether you really need them, and to override the methods they generate—\nwhen necessary.\nIn this chapter, we saw every special method related to object representation, except\n__index__ and __fspath__. We’ll discuss __index__ in Chapter 12, “A Slice-Aware\n__getitem__” on page 406. I will not cover __fspath__. To learn about it, see PEP 519\n—Adding a file system path protocol.\nAn early realization of the need for distinct string representations for objects\nappeared in Smalltalk. The 1996 article “How to Display an Object as a String: print‐\nString and displayString” by Bobby Woolf discusses the implementation of the print\nString and displayString methods in that language. From that article, I borrowed\nthe pithy descriptions “the way the developer wants to see it” and “the way the user\nwants to see it” when defining repr() and str() in “Object Representations” on page\n364.\nFurther Reading \n| \n393",
      "content_length": 2227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "11 See the “Simplest Thing that Could Possibly Work: A Conversation with Ward Cunningham, Part V”.\nSoapbox\nProperties Help Reduce Up-Front Costs\nIn the initial versions of Vector2d, the x and y attributes were public, as are all\nPython instance and class attributes by default. Naturally, users of vectors need to\naccess its components. Although our vectors are iterable and can be unpacked into a\npair of variables, it’s also desirable to write my_vector.x and my_vector.y to get each\ncomponent.\nWhen we felt the need to avoid accidental updates to the x and y attributes, we imple‐\nmented properties, but nothing changed elsewhere in the code and in the public\ninterface of Vector2d, as verified by the doctests. We are still able to access my_vec\ntor.x and my_vector.y.\nThis shows that we can always start our classes in the simplest possible way, with\npublic attributes, because when (or if) we later need to impose more control with get‐\nters and setters, these can be implemented through properties without changing any\nof the code that already interacts with our objects through the names (e.g., x and y)\nthat were initially simple public attributes.\nThis approach is the opposite of that encouraged by the Java language: a Java pro‐\ngrammer cannot start with simple public attributes and only later, if needed,\nimplement properties, because they don’t exist in the language. Therefore, writing\ngetters and setters is the norm in Java—even when those methods do nothing useful\n—because the API cannot evolve from simple public attributes to getters and setters\nwithout breaking all code that uses those attributes.\nIn addition, as Martelli, Ravenscroft, and Holden point out in Python in a Nutshell,\n3rd ed., typing getter/setter calls everywhere is goofy. You have to write stuff like:\n>>> my_object.set_foo(my_object.get_foo() + 1)\nJust to do this:\n>>> my_object.foo += 1\nWard Cunningham, inventor of the wiki and an Extreme Programming pioneer, rec‐\nommends asking: “What’s the simplest thing that could possibly work?” The idea is to\nfocus on the goal.11 Implementing setters and getters up-front is a distraction from\nthe goal. In Python, we can simply use public attributes, knowing we can change\nthem to properties later, if the need arises.\n394 \n| \nChapter 11: A Pythonic Object",
      "content_length": 2293,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "Safety Versus Security in Private Attributes\nPerl doesn’t have an infatuation with enforced privacy. It would prefer that you\nstayed out of its living room because you weren’t invited, not because it has a\nshotgun.\n—Larry Wall, creator of Perl\nPython and Perl are polar opposites in many regards, but Guido and Larry seem to\nagree on object privacy.\nHaving taught Python to many Java programmers over the years, I’ve found a lot of\nthem put too much faith in the privacy guarantees that Java offers. As it turns out, the\nJava private and protected modifiers normally provide protection against accidents\nonly (i.e., safety). They only offer security against malicious intent if the application is\nspecially configured and deployed on top of a Java SecurityManager, and that seldom\nhappens in practice, even in security conscious corporate settings.\nTo prove my point, I like to show this Java class (Example 11-20).\nExample 11-20. Confidential.java: a Java class with a private field named secret\npublic class Confidential {\n    private String secret = \"\";\n    public Confidential(String text) {\n        this.secret = text.toUpperCase();\n    }\n}\nIn Example 11-20, I store the text in the secret field after converting it to uppercase,\njust to make it obvious that whatever is in that field will be in all caps.\nThe actual demonstration consists of running expose.py with Jython. That script uses\nintrospection (“reflection” in Java parlance) to get the value of a private field. The\ncode is in Example 11-21.\nExample 11-21. expose.py: Jython code to read the content of a private field in\nanother class\n#!/usr/bin/env jython\n# NOTE: Jython is still Python 2.7 in late2020\nimport Confidential\nmessage = Confidential('top secret text')\nsecret_field = Confidential.getDeclaredField('secret')\nsecret_field.setAccessible(True)  # break the lock!\nprint 'message.secret =', secret_field.get(message)\nIf you run Example 11-21, this is what you get:\nFurther Reading \n| \n395",
      "content_length": 1964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "$ jython expose.py\nmessage.secret = TOP SECRET TEXT\nThe string 'TOP SECRET TEXT' was read from the secret private field of the Confi\ndential class.\nThere is no black magic here: expose.py uses the Java reflection API to get a reference\nto the private field named 'secret', and then calls 'secret_field.setAccessi\nble(True)' to make it readable. The same thing can be done with Java code, of\ncourse (but it takes more than three times as many lines to do it; see the file\nExpose.java in the _Fluent Python_ code repository).\nThe crucial call .setAccessible(True) will fail only if the Jython script or the Java\nmain program (e.g., Expose.class) is running under the supervision of a Security‐\nManager. But in the real world, Java applications are rarely deployed with a Security‐\nManager—except for Java applets when they were still supported by browsers.\nMy point is: in Java too, access control modifiers are mostly about safety and not\nsecurity, at least in practice. So relax and enjoy the power Python gives you. Use it\nresponsibly.\n396 \n| \nChapter 11: A Pythonic Object",
      "content_length": 1074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "CHAPTER 12\nSpecial Methods for Sequences\nDon’t check whether it is-a duck: check whether it quacks-like-a duck, walks-like-a\nduck, etc., etc., depending on exactly what subset of duck-like behavior you need to\nplay your language-games with. (comp.lang.python, Jul. 26, 2000)\n—Alex Martelli\nIn this chapter, we will create a class to represent a multidimensional Vector class—a\nsignificant step up from the two-dimensional Vector2d of Chapter 11. Vector will\nbehave like a standard Python immutable flat sequence. Its elements will be floats,\nand it will support the following by the end of this chapter:\n• Basic sequence protocol: __len__ and __getitem__\n• Safe representation of instances with many items\n• Proper slicing support, producing new Vector instances\n• Aggregate hashing, taking into account every contained element value\n• Custom formatting language extension\nWe’ll also implement dynamic attribute access with __getattr__ as a way of replac‐\ning the read-only properties we used in Vector2d—although this is not typical of\nsequence types.\nThe code-intensive presentation will be interrupted by a conceptual discussion about\nthe idea of protocols as an informal interface. We’ll talk about how protocols and\nduck typing are related, and its practical implications when you create your own\ntypes.\n397",
      "content_length": 1312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "What’s New in This Chapter\nThere are no major changes in this chapter. There is a new, brief discussion of\nthe typing.Protocol in a tip box near the end of “Protocols and Duck Typing” on\npage 402.\nIn “A Slice-Aware __getitem__” on page 406, the implementation of __getitem__ in\nExample 12-6 is more concise and robust than the example in the first edition, thanks\nto duck typing and operator.index. This change carried over to later implementa‐\ntions of Vector in this chapter and in Chapter 16.\nLet’s get started.\nVector: A User-Defined Sequence Type\nOur strategy to implement Vector will be to use composition, not inheritance. We’ll\nstore the components in an array of floats, and will implement the methods needed\nfor our Vector to behave like an immutable flat sequence.\nBut before we implement the sequence methods, let’s make sure we have a baseline\nimplementation of Vector that is compatible with our earlier Vector2d class—except\nwhere such compatibility would not make sense.\nVector Applications Beyond Three Dimensions\nWho needs a vector with 1,000 dimensions? N-dimensional vectors (with large values\nof N) are widely used in information retrieval, where documents and text queries are\nrepresented as vectors, with one dimension per word. This is called the Vector space\nmodel. In this model, a key relevance metric is the cosine similarity (i.e., the cosine of\nthe angle between the vector representing the query and the vector representing the\ndocument). As the angle decreases, the cosine approaches the maximum value of 1,\nand so does the relevance of the document to the query.\nHaving said that, the Vector class in this chapter is a didactic example and we’ll not\ndo much math here. Our goal is just to demonstrate some Python special methods in\nthe context of a sequence type.\nNumPy and SciPy are the tools you need for real-world vector math. The PyPI pack‐\nage gensim, by Radim Řehůřek, implements vector space modeling for natural lan‐\nguage processing and information retrieval, using NumPy and SciPy.\n398 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "Vector Take #1: Vector2d Compatible\nThe first version of Vector should be as compatible as possible with our earlier Vec\ntor2d class.\nHowever, by design, the Vector constructor is not compatible with the Vector2d\nconstructor. We could make Vector(3, 4) and Vector(3, 4, 5) work, by taking\narbitrary arguments with *args in __init__, but the best practice for a sequence con‐\nstructor is to take the data as an iterable argument in the constructor, like all built-in\nsequence types do. Example 12-1 shows some ways of instantiating our new Vector\nobjects.\nExample 12-1. Tests of Vector.__init__ and Vector.__repr__\n>>> Vector([3.1, 4.2])\nVector([3.1, 4.2])\n>>> Vector((3, 4, 5))\nVector([3.0, 4.0, 5.0])\n>>> Vector(range(10))\nVector([0.0, 1.0, 2.0, 3.0, 4.0, ...])\nApart from a new constructor signature, I made sure every test I did with Vector2d\n(e.g., Vector2d(3, 4)) passed and produced the same result with a two-component\nVector([3, 4]).\nWhen a Vector has more than six components, the string pro‐\nduced by repr() is abbreviated with ... as seen in the last line of\nExample 12-1. This is crucial in any collection type that may con‐\ntain a large number of items, because repr is used for debugging—\nand you don’t want a single large object to span thousands of lines\nin your console or log. Use the reprlib module to produce\nlimited-length representations, as in Example 12-2. The reprlib\nmodule was named repr in Python 2.7.\nExample 12-2 lists the implementation of our first version of Vector (this example\nbuilds on the code shown in Examples 11-2 and 11-3).\nExample 12-2. vector_v1.py: derived from vector2d_v1.py\nfrom array import array\nimport reprlib\nimport math\nclass Vector:\nVector Take #1: Vector2d Compatible \n| \n399",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "1 The iter() function is covered in Chapter 17, along with the __iter__ method.\n    typecode = 'd'\n    def __init__(self, components):\n        self._components = array(self.typecode, components)  \n    def __iter__(self):\n        return iter(self._components)  \n    def __repr__(self):\n        components = reprlib.repr(self._components)  \n        components = components[components.find('['):-1]  \n        return f'Vector({components})'\n    def __str__(self):\n        return str(tuple(self))\n    def __bytes__(self):\n        return (bytes([ord(self.typecode)]) +\n                bytes(self._components))  \n    def __eq__(self, other):\n        return tuple(self) == tuple(other)\n    def __abs__(self):\n        return math.hypot(*self)  \n    def __bool__(self):\n        return bool(abs(self))\n    @classmethod\n    def frombytes(cls, octets):\n        typecode = chr(octets[0])\n        memv = memoryview(octets[1:]).cast(typecode)\n        return cls(memv)  \nThe self._components instance “protected” attribute will hold an array with the\nVector components.\nTo allow iteration, we return an iterator over self._components.1\nUse reprlib.repr() to get a limited-length representation of self._components\n(e.g., array('d', [0.0, 1.0, 2.0, 3.0, 4.0, ...])).\nRemove the array('d', prefix, and the trailing ) before plugging the string into\na Vector constructor call.\n400 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "Build a bytes object directly from self._components.\nSince Python 3.8, math.hypot accepts N-dimensional points. I used this expres‐\nsion before: math.sqrt(sum(x * x for x in self)).\nThe only change needed from the earlier frombytes is in the last line: we pass the\nmemoryview directly to the constructor, without unpacking with * as we did\nbefore.\nThe way I used reprlib.repr deserves some elaboration. That function produces\nsafe representations of large or recursive structures by limiting the length of the out‐\nput string and marking the cut with '...'. I wanted the repr of a Vector to look like\nVector([3.0, 4.0, 5.0]) and not Vector(array('d', [3.0, 4.0, 5.0])),\nbecause the fact that there is an array inside a Vector is an implementation detail.\nBecause these constructor calls build identical Vector objects, I prefer the simpler\nsyntax using a list argument.\nWhen coding __repr__, I could have produced the simplified components display\nwith this expression: reprlib.repr(list(self._components)). However, this\nwould be wasteful, as I’d be copying every item from self._components to a list\njust to use the list repr. Instead, I decided to apply reprlib.repr to the self._com\nponents array directly, and then chop off the characters outside of the []. That’s what\nthe second line of __repr__ does in Example 12-2.\nBecause of its role in debugging, calling repr() on an object should\nnever raise an exception. If something goes wrong inside your\nimplementation of __repr__, you must deal with the issue and do\nyour best to produce some serviceable output that gives the user a\nchance of identifying the receiver (self).\nNote that the __str__, __eq__, and __bool__ methods are unchanged from Vec\ntor2d, and only one character was changed in frombytes (a * was removed in the last\nline). This is one of the benefits of making the original Vector2d iterable.\nBy the way, we could have subclassed Vector from Vector2d, but I chose not to do it\nfor two reasons. First, the incompatible constructors really make subclassing not\nadvisable. I could work around that with some clever parameter handling in\n__init__, but the second reason is more important: I want Vector to be a standalone\nexample of a class implementing the sequence protocol. That’s what we’ll do next,\nafter a discussion of the term protocol.\nVector Take #1: Vector2d Compatible \n| \n401",
      "content_length": 2357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "Protocols and Duck Typing\nAs early as Chapter 1, we saw that you don’t need to inherit from any special class to\ncreate a fully functional sequence type in Python; you just need to implement the\nmethods that fulfill the sequence protocol. But what kind of protocol are we talking\nabout?\nIn the context of object-oriented programming, a protocol is an informal interface,\ndefined only in documentation and not in code. For example, the sequence protocol\nin Python entails just the __len__ and __getitem__ methods. Any class Spam that\nimplements those methods with the standard signature and semantics can be used\nanywhere a sequence is expected. Whether Spam is a subclass of this or that is irrele‐\nvant; all that matters is that it provides the necessary methods. We saw that in\nExample 1-1, reproduced here in Example 12-3.\nExample 12-3. Code from Example 1-1, reproduced here for convenience\nimport collections\nCard = collections.namedtuple('Card', ['rank', 'suit'])\nclass FrenchDeck:\n    ranks = [str(n) for n in range(2, 11)] + list('JQKA')\n    suits = 'spades diamonds clubs hearts'.split()\n    def __init__(self):\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\n    def __len__(self):\n        return len(self._cards)\n    def __getitem__(self, position):\n        return self._cards[position]\nThe FrenchDeck class in Example 12-3 takes advantage of many Python facilities\nbecause it implements the sequence protocol, even if that is not declared anywhere in\nthe code. An experienced Python coder will look at it and understand that it is a\nsequence, even if it subclasses object. We say it is a sequence because it behaves like\none, and that is what matters.\nThis became known as duck typing, after Alex Martelli’s post quoted at the beginning\nof this chapter.\nBecause protocols are informal and unenforced, you can often get away with imple‐\nmenting just part of a protocol, if you know the specific context where a class will be\n402 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "used. For example, to support iteration, only __getitem__ is required; there is no\nneed to provide __len__.\nWith PEP 544—Protocols: Structural subtyping (static duck typ‐\ning), Python 3.8 supports protocol classes: typing constructs, which\nwe studied in “Static Protocols” on page 286. This new use of the\nword protocol in Python has a related but different meaning.\nWhen I need to differentiate them, I write static protocol to refer to\nthe protocols formalized in protocol classes, and dynamic protocol\nfor the traditional sense. One key difference is that static protocol\nimplementations must provide all methods defined in the protocol\nclass. “Two Kinds of Protocols” on page 434 in Chapter 13 has more\ndetails.\nWe’ll now implement the sequence protocol in Vector, initially without proper sup‐\nport for slicing, but later adding that.\nVector Take #2: A Sliceable Sequence\nAs we saw with the FrenchDeck example, supporting the sequence protocol is really\neasy if you can delegate to a sequence attribute in your object, like our self._compo\nnents array. These __len__ and __getitem__ one-liners are a good start:\nclass Vector:\n    # many lines omitted\n    # ...\n    def __len__(self):\n        return len(self._components)\n    def __getitem__(self, index):\n        return self._components[index]\nWith these additions, all of these operations now work:\n>>> v1 = Vector([3, 4, 5])\n>>> len(v1)\n3\n>>> v1[0], v1[-1]\n(3.0, 5.0)\n>>> v7 = Vector(range(7))\n>>> v7[1:4]\narray('d', [1.0, 2.0, 3.0])\nAs you can see, even slicing is supported—but not very well. It would be better if a\nslice of a Vector was also a Vector instance and not an array. The old FrenchDeck\nclass has a similar problem: when you slice it, you get a list. In the case of Vector, a\nlot of functionality is lost when slicing produces plain arrays.\nVector Take #2: A Sliceable Sequence \n| \n403",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "Consider the built-in sequence types: every one of them, when sliced, produces a new\ninstance of its own type, and not of some other type.\nTo make Vector produce slices as Vector instances, we can’t just delegate the slicing\nto array. We need to analyze the arguments we get in __getitem__ and do the right\nthing.\nNow, let’s see how Python turns the syntax my_seq[1:3] into arguments for\nmy_seq.__getitem__(...).\nHow Slicing Works\nA demo is worth a thousand words, so take a look at Example 12-4.\nExample 12-4. Checking out the behavior of __getitem__ and slices\n>>> class MySeq:\n...     def __getitem__(self, index):\n...         return index  \n...\n>>> s = MySeq()\n>>> s[1]  \n1\n>>> s[1:4]  \nslice(1, 4, None)\n>>> s[1:4:2]  \nslice(1, 4, 2)\n>>> s[1:4:2, 9]  \n(slice(1, 4, 2), 9)\n>>> s[1:4:2, 7:9]  \n(slice(1, 4, 2), slice(7, 9, None))\nFor this demonstration, __getitem__ merely returns whatever is passed to it.\nA single index, nothing new.\nThe notation 1:4 becomes slice(1, 4, None).\nslice(1, 4, 2) means start at 1, stop at 4, step by 2.\nSurprise: the presence of commas inside the [] means __getitem__ receives a\ntuple.\nThe tuple may even hold several slice objects.\nNow let’s take a closer look at slice itself in Example 12-5.\n404 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "Example 12-5. Inspecting the attributes of the slice class\n>>> slice  \n<class 'slice'>\n>>> dir(slice) \n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__',\n '__format__', '__ge__', '__getattribute__', '__gt__',\n '__hash__', '__init__', '__le__', '__lt__', '__ne__',\n '__new__', '__reduce__', '__reduce_ex__', '__repr__',\n '__setattr__', '__sizeof__', '__str__', '__subclasshook__',\n 'indices', 'start', 'step', 'stop']\nslice is a built-in type (we saw it first in “Slice Objects” on page 48).\nInspecting a slice, we find the data attributes start, stop, and step, and an\nindices method.\nIn Example 12-5, calling dir(slice) reveals an indices attribute, which turns out to\nbe a very interesting but little-known method. Here is what help(slice.indices)\nreveals:\nS.indices(len) -> (start, stop, stride)\nAssuming a sequence of length len, calculate the start and stop indices, and the\nstride length of the extended slice described by S. Out-of-bounds indices are\nclipped just like they are in a normal slice.\nIn other words, indices exposes the tricky logic that’s implemented in the built-in\nsequences to gracefully handle missing or negative indices and slices that are longer\nthan the original sequence. This method produces “normalized” tuples of nonnega‐\ntive start, stop, and stride integers tailored to a sequence of the given length.\nHere are a couple of examples, considering a sequence of len == 5, e.g., 'ABCDE':\n>>> slice(None, 10, 2).indices(5)  \n(0, 5, 2)\n>>> slice(-3, None, None).indices(5)  \n(2, 5, 1)\n'ABCDE'[:10:2] is the same as 'ABCDE'[0:5:2].\n'ABCDE'[-3:] is the same as 'ABCDE'[2:5:1].\nIn our Vector code, we’ll not need the slice.indices() method because when we\nget a slice argument we’ll delegate its handling to the _components array. But if you\ncan’t count on the services of an underlying sequence, this method can be a huge\ntime saver.\nVector Take #2: A Sliceable Sequence \n| \n405",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "Now that we know how to handle slices, let’s take a look at the improved Vec\ntor.__getitem__ implementation.\nA Slice-Aware __getitem__\nExample 12-6 lists the two methods needed to make Vector behave as a sequence:\n__len__ and __getitem__ (the latter now implemented to handle slicing correctly).\nExample 12-6. Part of vector_v2.py: __len__ and __getitem__ methods added to\nVector class from vector_v1.py (see Example 12-2)\n    def __len__(self):\n        return len(self._components)\n    def __getitem__(self, key):\n        if isinstance(key, slice):  \n            cls = type(self)  \n            return cls(self._components[key])  \n        index = operator.index(key)  \n        return self._components[index]  \nIf the key argument is a slice…\n…get the class of the instance (i.e., Vector) and…\n…invoke the class to build another Vector instance from a slice of the\n_components array.\nIf we can get an index from key…\n…return the specific item from _components.\nThe operator.index() function calls the __index__ special method. The function\nand the special method were defined in PEP 357—Allowing Any Object to be Used\nfor Slicing, proposed by Travis Oliphant to allow any of the numerous types of inte‐\ngers in NumPy to be used as indexes and slice arguments. The key difference between\noperator.index() and int() is that the former is intended for this specific purpose.\nFor example, int(3.14) returns 3, but operator.index(3.14) raises TypeError\nbecause a float should not be used as an index.\n406 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "Excessive use of isinstance may be a sign of bad OO design, but\nhandling slices in __getitem__ is a justified use case. In the first\nedition, I also used an isinstance test on key to test if it was an\ninteger. Using operator.index avoids this test, and raises Type\nError with a very informative message if we can’t get the index\nfrom key. See the last error message from Example 12-7.\nOnce the code in Example 12-6 is added to the Vector class, we have proper slicing\nbehavior, as Example 12-7 demonstrates.\nExample 12-7. Tests of enhanced Vector.__getitem__ from Example 12-6\n    >>> v7 = Vector(range(7))\n    >>> v7[-1]  \n    6.0\n    >>> v7[1:4]  \n    Vector([1.0, 2.0, 3.0])\n    >>> v7[-1:]  \n    Vector([6.0])\n    >>> v7[1,2]  \n    Traceback (most recent call last):\n      ...\n    TypeError: 'tuple' object cannot be interpreted as an integer\nAn integer index retrieves just one component value as a float.\nA slice index creates a new Vector.\nA slice of len == 1 also creates a Vector.\nVector does not support multidimensional indexing, so a tuple of indices or sli‐\nces raises an error.\nVector Take #3: Dynamic Attribute Access\nIn the evolution from Vector2d to Vector, we lost the ability to access vector compo‐\nnents by name (e.g., v.x, v.y). We are now dealing with vectors that may have a large\nnumber of components. Still, it may be convenient to access the first few components\nwith shortcut letters such as x, y, z instead of v[0], v[1], and v[2].\nHere is the alternative syntax we want to provide for reading the first four compo‐\nnents of a vector:\n>>> v = Vector(range(10))\n>>> v.x\n0.0\nVector Take #3: Dynamic Attribute Access \n| \n407",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "2 Attribute lookup is more complicated than this; we’ll see the gory details in Part V. For now, this simplified\nexplanation will do.\n3 Although __match_args__ exists to support pattern matching in Python 3.10, setting this attribute is harmless\nin previous versions of Python. In the first edition of this book, I named it shortcut_names. With the new\nname it does double duty: it supports positional patterns in case clauses, and it holds the names of the\ndynamic attributes supported by special logic in __getattr__ and __setattr__.\n>>> v.y, v.z, v.t\n(1.0, 2.0, 3.0)\nIn Vector2d, we provided read-only access to x and y using the @property decorator\n(Example 11-7). We could write four properties in Vector, but it would be tedious.\nThe __getattr__ special method provides a better way.\nThe __getattr__ method is invoked by the interpreter when attribute lookup fails.\nIn simple terms, given the expression my_obj.x, Python checks if the my_obj instance\nhas an attribute named x; if not, the search goes to the class (my_obj.__class__), and\nthen up the inheritance graph.2 If the x attribute is not found, then the __getattr__\nmethod defined in the class of my_obj is called with self and the name of the\nattribute as a string (e.g., 'x').\nExample 12-8 lists our __getattr__ method. Essentially it checks whether the\nattribute being sought is one of the letters xyzt and if so, returns the corresponding\nvector component.\nExample 12-8. Part of vector_v3.py: __getattr__ method added to the Vector class\n    __match_args__ = ('x', 'y', 'z', 't')  \n    def __getattr__(self, name):\n        cls = type(self)  \n        try:\n            pos = cls.__match_args__.index(name)  \n        except ValueError:  \n            pos = -1\n        if 0 <= pos < len(self._components):  \n            return self._components[pos]\n        msg = f'{cls.__name__!r} object has no attribute {name!r}'  \n        raise AttributeError(msg)\nSet __match_args__ to allow positional pattern matching on the dynamic\nattributes supported by __getattr__.3\nGet the Vector class for later use.\n408 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "Try to get the position of name in __match_args__.\n.index(name) raises ValueError when name is not found; set pos to -1. (I’d\nrather use a method like str.find here, but tuple doesn’t implement it.)\nIf the pos is within range of the available components, return the component.\nIf we get this far, raise AttributeError with a standard message text.\nIt’s not hard to implement __getattr__, but in this case it’s not enough. Consider\nthe bizarre interaction in Example 12-9.\nExample 12-9. Inappropriate behavior: assigning to v.x raises no error, but introduces\nan inconsistency\n>>> v = Vector(range(5))\n>>> v\nVector([0.0, 1.0, 2.0, 3.0, 4.0])\n>>> v.x  \n0.0\n>>> v.x = 10  \n>>> v.x  \n10\n>>> v\nVector([0.0, 1.0, 2.0, 3.0, 4.0])  \nAccess element v[0] as v.x.\nAssign new value to v.x. This should raise an exception.\nReading v.x shows the new value, 10.\nHowever, the vector components did not change.\nCan you explain what is happening? In particular, why does v.x return 10 the second\ntime if that value is not in the vector components array? If you don’t know right off\nthe bat, study the explanation of __getattr__ given right before Example 12-8. It’s a\nbit subtle, but a very important foundation to understand a lot of what comes later in\nthe book.\nAfter you’ve given it some thought, proceed and we’ll explain exactly what happened.\nThe inconsistency in Example 12-9 was introduced because of the way __getattr__\nworks: Python only calls that method as a fallback, when the object does not have the\nnamed attribute. However, after we assign v.x = 10, the v object now has an x\nattribute, so __getattr__ will no longer be called to retrieve v.x: the interpreter will\nVector Take #3: Dynamic Attribute Access \n| \n409",
      "content_length": 1713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "just return the value 10 that is bound to v.x. On the other hand, our implementation\nof __getattr__ pays no attention to instance attributes other than self._compo\nnents, from where it retrieves the values of the “virtual attributes” listed in\n__match_args__.\nWe need to customize the logic for setting attributes in our Vector class in order to\navoid this inconsistency.\nRecall that in the latest Vector2d examples from Chapter 11, trying to assign to the .x\nor .y instance attributes raised AttributeError. In Vector, we want the same excep‐\ntion with any attempt at assigning to all single-letter lowercase attribute names, just\nto avoid confusion. To do that, we’ll implement __setattr__, as listed in\nExample 12-10.\nExample 12-10. Part of vector_v3.py: __setattr__ method in the Vector class\n    def __setattr__(self, name, value):\n        cls = type(self)\n        if len(name) == 1:  \n            if name in cls.__match_args__:  \n                error = 'readonly attribute {attr_name!r}'\n            elif name.islower():  \n                error = \"can't set attributes 'a' to 'z' in {cls_name!r}\"\n            else:\n                error = ''  \n            if error:  \n                msg = error.format(cls_name=cls.__name__, attr_name=name)\n                raise AttributeError(msg)\n        super().__setattr__(name, value)  \nSpecial handling for single-character attribute names.\nIf name is one of __match_args__, set specific error message.\nIf name is lowercase, set error message about all single-letter names.\nOtherwise, set blank error message.\nIf there is a nonblank error message, raise AttributeError.\nDefault case: call __setattr__ on superclass for standard behavior.\n410 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "The super() function provides a way to access methods of super‐\nclasses dynamically, a necessity in a dynamic language supporting\nmultiple inheritance like Python. It’s used to delegate some task\nfrom a method in a subclass to a suitable method in a superclass, as\nseen in Example 12-10. There is more about super in “Multiple\nInheritance and Method Resolution Order” on page 494.\nWhile choosing the error message to display with AttributeError, my first check\nwas the behavior of the built-in complex type, because they are immutable and have a\npair of data attributes, real and imag. Trying to change either of those in a complex\ninstance raises AttributeError with the message \"can't set attribute\". On the\nother hand, trying to set a read-only attribute protected by a property as we did in “A\nHashable Vector2d” on page 374 produces the message \"read-only attribute\". I\ndrew inspiration from both wordings to set the error string in __setitem__, but was\nmore explicit about the forbidden attributes.\nNote that we are not disallowing setting all attributes, only single-letter, lowercase\nones, to avoid confusion with the supported read-only attributes x, y, z, and t.\nKnowing that declaring __slots__ at the class level prevents set‐\nting new instance attributes, it’s tempting to use that feature\ninstead of implementing __setattr__ as we did. However, because\nof all the caveats discussed in “Summarizing the Issues with\n__slots__” on page 388, using __slots__ just to prevent instance\nattribute creation is not recommended. __slots__ should be used\nonly to save memory, and only if that is a real issue.\nEven without supporting writing to the Vector components, here is an important\ntakeaway from this example: very often when you implement __getattr__, you need\nto code __setattr__ as well, to avoid inconsistent behavior in your objects.\nIf we wanted to allow changing components, we could implement __setitem__ to\nenable v[0] = 1.1 and/or __setattr__ to make v.x = 1.1 work. But Vector will\nremain immutable because we want to make it hashable in the coming section.\nVector Take #4: Hashing and a Faster ==\nOnce more we get to implement a __hash__ method. Together with the existing\n__eq__, this will make Vector instances hashable.\nThe __hash__ in Vector2d (Example 11-8) computed the hash of a tuple built with\nthe two components, self.x and self.y. Now we may be dealing with thousands of\ncomponents, so building a tuple may be too costly. Instead, I will apply the ^ (xor)\nVector Take #4: Hashing and a Faster == \n| \n411",
      "content_length": 2535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "4 The sum, any, and all cover the most common uses of reduce. See the discussion in “Modern Replacements\nfor map, filter, and reduce” on page 235.\noperator to the hashes of every component in succession, like this: v[0] ^ v[1] ^\nv[2]. That is what the functools.reduce function is for. Previously I said that\nreduce is not as popular as before,4 but computing the hash of all vector components\nis a good use case for it. Figure 12-1 depicts the general idea of the reduce function.\nFigure 12-1. Reducing functions—reduce, sum, any, all—produce a single aggregate\nresult from a sequence or from any finite iterable object.\nSo far we’ve seen that functools.reduce() can be replaced by sum(), but now let’s\nproperly explain how it works. The key idea is to reduce a series of values to a single\nvalue. The first argument to reduce() is a two-argument function, and the second\nargument is an iterable. Let’s say we have a two-argument function fn and a list lst.\nWhen you call reduce(fn, lst), fn will be applied to the first pair of elements—\nfn(lst[0], lst[1])—producing a first result, r1. Then fn is applied to r1 and the\nnext element—fn(r1, lst[2])—producing a second result, r2. Now fn(r2,\nlst[3]) is called to produce r3 … and so on until the last element, when a single\nresult, rN, is returned.\nHere is how you could use reduce to compute 5! (the factorial of 5):\n>>> 2 * 3 * 4 * 5  # the result we want: 5! == 120\n120\n>>> import functools\n>>> functools.reduce(lambda a,b: a*b, range(1, 6))\n120\nBack to our hashing problem, Example 12-11 shows the idea of computing the aggre‐\ngate xor by doing it in three ways: with a for loop and two reduce calls.\nExample 12-11. Three ways of calculating the accumulated xor of integers from 0 to 5\n>>> n = 0\n>>> for i in range(1, 6):  \n...     n ^= i\n...\n412 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": ">>> n\n1\n>>> import functools\n>>> functools.reduce(lambda a, b: a^b, range(6))  \n1\n>>> import operator\n>>> functools.reduce(operator.xor, range(6))  \n1\nAggregate xor with a for loop and an accumulator variable.\nfunctools.reduce using an anonymous function.\nfunctools.reduce replacing custom lambda with operator.xor.\nFrom the alternatives in Example 12-11, the last one is my favorite, and the for loop\ncomes second. What is your preference?\nAs seen in “The operator Module” on page 243, operator provides the functionality\nof all Python infix operators in function form, lessening the need for lambda.\nTo code Vector.__hash__ in my preferred style, we need to import the functools\nand operator modules. Example 12-12 shows the relevant changes.\nExample 12-12. Part of vector_v4.py: two imports and __hash__ method added to the\nVector class from vector_v3.py\nfrom array import array\nimport reprlib\nimport math\nimport functools  \nimport operator  \nclass Vector:\n    typecode = 'd'\n    # many lines omitted in book listing...\n    def __eq__(self, other):  \n        return tuple(self) == tuple(other)\n    def __hash__(self):\n        hashes = (hash(x) for x in self._components)  \n        return functools.reduce(operator.xor, hashes, 0)  \n    # more lines omitted...\nVector Take #4: Hashing and a Faster == \n| \n413",
      "content_length": 1310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "Import functools to use reduce.\nImport operator to use xor.\nNo change to __eq__; I listed it here because it’s good practice to keep __eq__\nand __hash__ close in source code, because they need to work together.\nCreate a generator expression to lazily compute the hash of each component.\nFeed hashes to reduce with the xor function to compute the aggregate hash\ncode; the third argument, 0, is the initializer (see the next warning).\nWhen using reduce, it’s good practice to provide the third argu‐\nment, reduce(function, iterable, initializer), to prevent\nthis exception: TypeError: reduce() of empty sequence with\nno initial value (excellent message: explains the problem and\nhow to fix it). The initializer is the value returned if the\nsequence is empty and is used as the first argument in the reducing\nloop, so it should be the identity value of the operation. As exam‐\nples, for +, |, ^ the initializer should be 0, but for *, & it should\nbe 1.\nAs implemented, the __hash__ method in Example 12-12 is a perfect example of a\nmap-reduce computation (Figure 12-2).\nFigure 12-2. Map-reduce: apply function to each item to generate a new series (map),\nthen compute the aggregate (reduce).\nThe mapping step produces one hash for each component, and the reduce step aggre‐\ngates all hashes with the xor operator. Using map instead of a genexp makes the map‐\nping step even more visible:\n414 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "5 We will seriously consider the matter of Vector([1, 2]) == (1, 2) in “Operator Overloading 101” on page\n562.\n    def __hash__(self):\n        hashes = map(hash, self._components)\n        return functools.reduce(operator.xor, hashes)\nThe solution with map would be less efficient in Python 2, where\nthe map function builds a new list with the results. But in Python\n3, map is lazy: it creates a generator that yields the results on\ndemand, thus saving memory—just like the generator expression\nwe used in the __hash__ method of Example 12-8.\nWhile we are on the topic of reducing functions, we can replace our quick implemen‐\ntation of __eq__ with another one that will be cheaper in terms of processing and\nmemory, at least for large vectors. As introduced in Example 11-2, we have this very\nconcise implementation of __eq__:\n    def __eq__(self, other):\n        return tuple(self) == tuple(other)\nThis works for Vector2d and for Vector—it even considers Vector([1, 2]) equal to\n(1, 2), which may be a problem, but we’ll overlook that for now.5 But for Vector\ninstances that may have thousands of components, it’s very inefficient. It builds two\ntuples copying the entire contents of the operands just to use the __eq__ of the tuple\ntype. For Vector2d (with only two components), it’s a good shortcut, but not for the\nlarge multidimensional vectors. A better way of comparing one Vector to another\nVector or iterable would be Example 12-13.\nExample 12-13. The Vector.__eq__ implementation using zip in a for loop for more\nefficient comparison\n    def __eq__(self, other):\n        if len(self) != len(other):  \n            return False\n        for a, b in zip(self, other):  \n            if a != b:  \n                return False\n        return True  \nIf the len of the objects are different, they are not equal.\nzip produces a generator of tuples made from the items in each iterable argu‐\nment. See “The Awesome zip” on page 416 if zip is new to you. In \n, the len\nVector Take #4: Hashing and a Faster == \n| \n415",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "comparison is needed because zip stops producing values without warning as\nsoon as one of the inputs is exhausted.\nAs soon as two components are different, exit returning False.\nOtherwise, the objects are equal.\nThe zip function is named after the zipper fastener because the\nphysical device works by interlocking pairs of teeth taken from\nboth zipper sides, a good visual analogy for what zip(left,\nright) does. No relation to compressed files.\nExample 12-13 is efficient, but the all function can produce the same aggregate com‐\nputation of the for loop in one line: if all comparisons between corresponding com‐\nponents in the operands are True, the result is True. As soon as one comparison is\nFalse, all returns False. Example 12-14 shows how __eq__ looks using all.\nExample 12-14. The Vector.__eq__ implementation using zip and all: same logic as\nExample 12-13\n    def __eq__(self, other):\n        return len(self) == len(other) and all(a == b for a, b in zip(self, other))\nNote that we first check that the operands have equal length, because zip will stop at\nthe shortest operand.\nExample 12-14 is the implementation we choose for __eq__ in vector_v4.py.\nThe Awesome zip\nHaving a for loop that iterates over items without fiddling with index variables is\ngreat and prevents lots of bugs, but demands some special utility functions. One of\nthem is the zip built-in, which makes it easy to iterate in parallel over two or more\niterables by returning tuples that you can unpack into variables, one for each item in\nthe parallel inputs. See Example 12-15.\nExample 12-15. The zip built-in at work\n>>> zip(range(3), 'ABC')  \n<zip object at 0x10063ae48>\n>>> list(zip(range(3), 'ABC'))  \n[(0, 'A'), (1, 'B'), (2, 'C')]\n>>> list(zip(range(3), 'ABC', [0.0, 1.1, 2.2, 3.3]))  \n[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2)]\n416 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": ">>> from itertools import zip_longest  \n>>> list(zip_longest(range(3), 'ABC', [0.0, 1.1, 2.2, 3.3], fillvalue=-1))\n[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2), (-1, -1, 3.3)]\nzip returns a generator that produces tuples on demand.\nBuild a list just for display; usually we iterate over the generator.\nzip stops without warning when one of the iterables is exhausted.\nThe itertools.zip_longest function behaves differently: it uses an optional\nfillvalue (None by default) to complete missing values so it can generate tuples\nuntil the last iterable is exhausted.\nNew zip() Option in Python 3.10\nI wrote in the first edition of this book that zip silently stop‐\nping at the shortest iterable was surprising—not a good trait\nfor an API. Silently ignoring part of the input can cause subtle\nbugs. Instead, zip should raise ValueError if the iterables are\nnot all of the same length, which is what happens when\nunpacking an iterable to a tuple of variables of different length\n—in line with Python’s fail fast policy. PEP 618—Add\nOptional Length-Checking To zip added an optional strict\nargument to zip to make it behave in that way. It is imple‐\nmented in Python 3.10.\nThe zip function can also be used to transpose a matrix represented as nested itera‐\nbles. For example:\n>>> a = [(1, 2, 3),\n...      (4, 5, 6)]\n>>> list(zip(*a))\n[(1, 4), (2, 5), (3, 6)]\n>>> b = [(1, 2),\n...      (3, 4),\n...      (5, 6)]\n>>> list(zip(*b))\n[(1, 3, 5), (2, 4, 6)]\nIf you want to grok zip, spend some time figuring out how these examples work.\nThe enumerate built-in is another generator function often used in for loops to avoid\ndirect handling of index variables. If you’re not familiar with enumerate, you should\ndefinitely check it out in the “Built-in functions” documentation. The zip and\nVector Take #4: Hashing and a Faster == \n| \n417",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "6 The Wolfram Mathworld website has an article on hypersphere; on Wikipedia, “hypersphere” redirects to the\n“n-sphere” entry.\nenumerate built-ins, along with several other generator functions in the standard\nlibrary, are covered in “Generator Functions in the Standard Library” on page 619.\nWe wrap up this chapter by bringing back the __format__ method from Vector2d to\nVector.\nVector Take #5: Formatting\nThe __format__ method of Vector will resemble that of Vector2d, but instead of\nproviding a custom display in polar coordinates, Vector will use spherical coordi‐\nnates—also known as “hyperspherical” coordinates, because now we support n\ndimensions, and spheres are “hyperspheres” in 4D and beyond.6 Accordingly, we’ll\nchange the custom format suffix from 'p' to 'h'.\nAs we saw in “Formatted Displays” on page 370, when extending\nthe Format Specification Mini-Language, it’s best to avoid reusing\nformat codes supported by built-in types. In particular, our exten‐\nded mini-language also uses the float formatting codes 'eEfFgGn%'\nin their original meaning, so we definitely must avoid these. Inte‐\ngers use 'bcdoxXn' and strings use 's'. I picked 'p' for Vector2d\npolar coordinates. Code 'h' for hyperspherical coordinates is a\ngood choice.\nFor example, given a Vector object in 4D space (len(v) == 4), the 'h' code will pro‐\nduce a display like <r, Φ₁, Φ₂, Φ₃>, where r is the magnitude (abs(v)), and the\nremaining numbers are the angular components Φ₁, Φ₂, Φ₃.\nHere are some samples of the spherical coordinate format in 4D, taken from the\ndoctests of vector_v5.py (see Example 12-16):\n>>> format(Vector([-1, -1, -1, -1]), 'h')\n'<2.0, 2.0943951023931957, 2.186276035465284, 3.9269908169872414>'\n>>> format(Vector([2, 2, 2, 2]), '.3eh')\n'<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'\n>>> format(Vector([0, 1, 0, 0]), '0.5fh')\n'<1.00000, 1.57080, 0.00000, 0.00000>'\nBefore we can implement the minor changes required in __format__, we need to\ncode a pair of support methods: angle(n) to compute one of the angular coordinates\n(e.g., Φ₁), and angles() to return an iterable of all angular coordinates. I will not\n418 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "describe the math here; if you’re curious, Wikipedia’s “n-sphere” entry has the for‐\nmulas I used to calculate the spherical coordinates from the Cartesian coordinates in\nthe Vector components array.\nExample 12-16 is a full listing of vector_v5.py consolidating all we’ve implemented\nsince “Vector Take #1: Vector2d Compatible” on page 399 and introducing custom\nformatting.\nExample 12-16. vector_v5.py: doctests and all code for the final Vector class; callouts\nhighlight additions needed to support __format__\n\"\"\"\nA multidimensional ``Vector`` class, take 5\nA ``Vector`` is built from an iterable of numbers::\n    >>> Vector([3.1, 4.2])\n    Vector([3.1, 4.2])\n    >>> Vector((3, 4, 5))\n    Vector([3.0, 4.0, 5.0])\n    >>> Vector(range(10))\n    Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])\nTests with two dimensions (same results as ``vector2d_v1.py``)::\n    >>> v1 = Vector([3, 4])\n    >>> x, y = v1\n    >>> x, y\n    (3.0, 4.0)\n    >>> v1\n    Vector([3.0, 4.0])\n    >>> v1_clone = eval(repr(v1))\n    >>> v1 == v1_clone\n    True\n    >>> print(v1)\n    (3.0, 4.0)\n    >>> octets = bytes(v1)\n    >>> octets\n    b'd\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x08@\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x10@'\n    >>> abs(v1)\n    5.0\n    >>> bool(v1), bool(Vector([0, 0]))\n    (True, False)\nTest of ``.frombytes()`` class method:\n    >>> v1_clone = Vector.frombytes(bytes(v1))\n    >>> v1_clone\nVector Take #5: Formatting \n| \n419",
      "content_length": 1398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "Vector([3.0, 4.0])\n    >>> v1 == v1_clone\n    True\nTests with three dimensions::\n    >>> v1 = Vector([3, 4, 5])\n    >>> x, y, z = v1\n    >>> x, y, z\n    (3.0, 4.0, 5.0)\n    >>> v1\n    Vector([3.0, 4.0, 5.0])\n    >>> v1_clone = eval(repr(v1))\n    >>> v1 == v1_clone\n    True\n    >>> print(v1)\n    (3.0, 4.0, 5.0)\n    >>> abs(v1)  # doctest:+ELLIPSIS\n    7.071067811...\n    >>> bool(v1), bool(Vector([0, 0, 0]))\n    (True, False)\nTests with many dimensions::\n    >>> v7 = Vector(range(7))\n    >>> v7\n    Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])\n    >>> abs(v7)  # doctest:+ELLIPSIS\n    9.53939201...\nTest of ``.__bytes__`` and ``.frombytes()`` methods::\n    >>> v1 = Vector([3, 4, 5])\n    >>> v1_clone = Vector.frombytes(bytes(v1))\n    >>> v1_clone\n    Vector([3.0, 4.0, 5.0])\n    >>> v1 == v1_clone\n    True\nTests of sequence behavior::\n    >>> v1 = Vector([3, 4, 5])\n    >>> len(v1)\n    3\n    >>> v1[0], v1[len(v1)-1], v1[-1]\n    (3.0, 5.0, 5.0)\n420 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "Test of slicing::\n    >>> v7 = Vector(range(7))\n    >>> v7[-1]\n    6.0\n    >>> v7[1:4]\n    Vector([1.0, 2.0, 3.0])\n    >>> v7[-1:]\n    Vector([6.0])\n    >>> v7[1,2]\n    Traceback (most recent call last):\n      ...\n    TypeError: 'tuple' object cannot be interpreted as an integer\nTests of dynamic attribute access::\n    >>> v7 = Vector(range(10))\n    >>> v7.x\n    0.0\n    >>> v7.y, v7.z, v7.t\n    (1.0, 2.0, 3.0)\nDynamic attribute lookup failures::\n    >>> v7.k\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'Vector' object has no attribute 'k'\n    >>> v3 = Vector(range(3))\n    >>> v3.t\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'Vector' object has no attribute 't'\n    >>> v3.spam\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'Vector' object has no attribute 'spam'\nTests of hashing::\n    >>> v1 = Vector([3, 4])\n    >>> v2 = Vector([3.1, 4.2])\n    >>> v3 = Vector([3, 4, 5])\n    >>> v6 = Vector(range(6))\n    >>> hash(v1), hash(v3), hash(v6)\n    (7, 2, 1)\nMost hash codes of non-integers vary from a 32-bit to 64-bit CPython build::\nVector Take #5: Formatting \n| \n421",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": ">>> import sys\n    >>> hash(v2) == (384307168202284039 if sys.maxsize > 2**32 else 357915986)\n    True\nTests of ``format()`` with Cartesian coordinates in 2D::\n    >>> v1 = Vector([3, 4])\n    >>> format(v1)\n    '(3.0, 4.0)'\n    >>> format(v1, '.2f')\n    '(3.00, 4.00)'\n    >>> format(v1, '.3e')\n    '(3.000e+00, 4.000e+00)'\nTests of ``format()`` with Cartesian coordinates in 3D and 7D::\n    >>> v3 = Vector([3, 4, 5])\n    >>> format(v3)\n    '(3.0, 4.0, 5.0)'\n    >>> format(Vector(range(7)))\n    '(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0)'\nTests of ``format()`` with spherical coordinates in 2D, 3D and 4D::\n    >>> format(Vector([1, 1]), 'h')  # doctest:+ELLIPSIS\n    '<1.414213..., 0.785398...>'\n    >>> format(Vector([1, 1]), '.3eh')\n    '<1.414e+00, 7.854e-01>'\n    >>> format(Vector([1, 1]), '0.5fh')\n    '<1.41421, 0.78540>'\n    >>> format(Vector([1, 1, 1]), 'h')  # doctest:+ELLIPSIS\n    '<1.73205..., 0.95531..., 0.78539...>'\n    >>> format(Vector([2, 2, 2]), '.3eh')\n    '<3.464e+00, 9.553e-01, 7.854e-01>'\n    >>> format(Vector([0, 0, 0]), '0.5fh')\n    '<0.00000, 0.00000, 0.00000>'\n    >>> format(Vector([-1, -1, -1, -1]), 'h')  # doctest:+ELLIPSIS\n    '<2.0, 2.09439..., 2.18627..., 3.92699...>'\n    >>> format(Vector([2, 2, 2, 2]), '.3eh')\n    '<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'\n    >>> format(Vector([0, 1, 0, 0]), '0.5fh')\n    '<1.00000, 1.57080, 0.00000, 0.00000>'\n\"\"\"\nfrom array import array\nimport reprlib\nimport math\nimport functools\nimport operator\n422 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "import itertools  \nclass Vector:\n    typecode = 'd'\n    def __init__(self, components):\n        self._components = array(self.typecode, components)\n    def __iter__(self):\n        return iter(self._components)\n    def __repr__(self):\n        components = reprlib.repr(self._components)\n        components = components[components.find('['):-1]\n        return f'Vector({components})'\n    def __str__(self):\n        return str(tuple(self))\n    def __bytes__(self):\n        return (bytes([ord(self.typecode)]) +\n                bytes(self._components))\n    def __eq__(self, other):\n        return (len(self) == len(other) and\n                all(a == b for a, b in zip(self, other)))\n    def __hash__(self):\n        hashes = (hash(x) for x in self)\n        return functools.reduce(operator.xor, hashes, 0)\n    def __abs__(self):\n        return math.hypot(*self)\n    def __bool__(self):\n        return bool(abs(self))\n    def __len__(self):\n        return len(self._components)\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            cls = type(self)\n            return cls(self._components[key])\n        index = operator.index(key)\n        return self._components[index]\n    __match_args__ = ('x', 'y', 'z', 't')\n    def __getattr__(self, name):\n        cls = type(self)\nVector Take #5: Formatting \n| \n423",
      "content_length": 1324,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "try:\n            pos = cls.__match_args__.index(name)\n        except ValueError:\n            pos = -1\n        if 0 <= pos < len(self._components):\n            return self._components[pos]\n        msg = f'{cls.__name__!r} object has no attribute {name!r}'\n        raise AttributeError(msg)\n    def angle(self, n):  \n        r = math.hypot(*self[n:])\n        a = math.atan2(r, self[n-1])\n        if (n == len(self) - 1) and (self[-1] < 0):\n            return math.pi * 2 - a\n        else:\n            return a\n    def angles(self):  \n        return (self.angle(n) for n in range(1, len(self)))\n    def __format__(self, fmt_spec=''):\n        if fmt_spec.endswith('h'):  # hyperspherical coordinates\n            fmt_spec = fmt_spec[:-1]\n            coords = itertools.chain([abs(self)],\n                                     self.angles())  \n            outer_fmt = '<{}>'  \n        else:\n            coords = self\n            outer_fmt = '({})'  \n        components = (format(c, fmt_spec) for c in coords)  \n        return outer_fmt.format(', '.join(components))  \n    @classmethod\n    def frombytes(cls, octets):\n        typecode = chr(octets[0])\n        memv = memoryview(octets[1:]).cast(typecode)\n        return cls(memv)\nImport itertools to use chain function in __format__.\nCompute one of the angular coordinates, using formulas adapted from the\nn-sphere article.\nCreate a generator expression to compute all angular coordinates on demand.\nUse itertools.chain to produce genexp to iterate seamlessly over the magni‐\ntude and the angular coordinates.\nConfigure a spherical coordinate display with angular brackets.\n424 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "Configure a Cartesian coordinate display with parentheses.\nCreate a generator expression to format each coordinate item on demand.\nPlug formatted components separated by commas inside brackets or\nparentheses.\nWe are making heavy use of generator expressions in __format__,\nangle, and angles, but our focus here is in providing __format__\nto bring Vector to the same implementation level as Vector2d.\nWhen we cover generators in Chapter 17, we’ll use some of the\ncode in Vector as examples, and then the generator tricks will be\nexplained in detail.\nThis concludes our mission for this chapter. The Vector class will be enhanced with\ninfix operators in Chapter 16, but our goal here was to explore techniques for coding\nspecial methods that are useful in a wide variety of collection classes.\nChapter Summary\nThe Vector example in this chapter was designed to be compatible with Vector2d,\nexcept for the use of a different constructor signature accepting a single iterable argu‐\nment, just like the built-in sequence types do. The fact that Vector behaves as a\nsequence just by implementing __getitem__ and __len__ prompted a discussion of\nprotocols, the informal interfaces used in duck-typed languages.\nWe then looked at how the my_seq[a:b:c] syntax works behind the scenes, by creat‐\ning a slice(a, b, c) object and handing it to __getitem__. Armed with this knowl‐\nedge, we made Vector respond correctly to slicing, by returning new Vector\ninstances, just like a Pythonic sequence is expected to do.\nThe next step was to provide read-only access to the first few Vector components\nusing notation such as my_vec.x. We did it by implementing __getattr__. Doing\nthat opened the possibility of tempting the user to assign to those special components\nby writing my_vec.x = 7, revealing a potential bug. We fixed it by implementing\n__setattr__ as well, to forbid assigning values to single-letter attributes. Very often,\nwhen you code a __getattr__ you need to add __setattr__ too, in order to avoid\ninconsistent behavior.\nImplementing the __hash__ function provided the perfect context for using func\ntools.reduce, because we needed to apply the xor operator ^ in succession to the\nhashes of all Vector components to produce an aggregate hash code for the whole\nChapter Summary \n| \n425",
      "content_length": 2283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "Vector. After applying reduce in __hash__, we used the all reducing built-in to cre‐\nate a more efficient __eq__ method.\nThe last enhancement to Vector was to reimplement the __format__ method from\nVector2d by supporting spherical coordinates as an alternative to the default Carte‐\nsian coordinates. We used quite a bit of math and several generators to code __for\nmat__ and its auxiliary functions, but these are implementation details—and we’ll\ncome back to the generators in Chapter 17. The goal of that last section was to sup‐\nport a custom format, thus fulfilling the promise of a Vector that could do everything\na Vector2d did, and more.\nAs we did in Chapter 11, here we often looked at how standard Python objects\nbehave, to emulate them and provide a “Pythonic” look-and-feel to Vector.\nIn Chapter 16, we will implement several infix operators on Vector. The math will be\nmuch simpler than in the angle() method here, but exploring how infix operators\nwork in Python is a great lesson in OO design. But before we get to operator over‐\nloading, we’ll step back from working on one class and look at organizing multiple\nclasses with interfaces and inheritance, the subjects of Chapters 13 and 14.\nFurther Reading\nMost special methods covered in the Vector example also appear in the Vector2d\nexample from Chapter 11, so the references in “Further Reading” on page 392 are all\nrelevant here.\nThe powerful reduce higher-order function is also known as fold, accumulate, aggre‐\ngate, compress, and inject. For more information, see Wikipedia’s “Fold (higher-\norder function)” article, which presents applications of that higher-order function\nwith emphasis on functional programming with recursive data structures. The article\nalso includes a table listing fold-like functions in dozens of programming languages.\n“What’s New in Python 2.5” has a short explanation of __index__, designed to sup‐\nport __getitem__ methods, as we saw in “A Slice-Aware __getitem__” on page 406.\nPEP 357—Allowing Any Object to be Used for Slicing details the need for it from the\nperspective of an implementor of a C-extension—Travis Oliphant, the primary crea‐\ntor of NumPy. Oliphant’s many contributions to Python made it a leading scientific\ncomputing language, which then positioned it to lead the way in machine learning\napplications.\n426 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "Soapbox\nProtocols as Informal Interfaces\nProtocols are not an invention of Python. The Smalltalk team, which also coined the\nexpression “object-oriented,” used “protocol” as a synonym for what we now call\ninterfaces. Some Smalltalk programming environments allowed programmers to tag a\ngroup of methods as a protocol, but that was merely a documentation and navigation\naid, and not enforced by the language. That’s why I believe “informal interface” is a\nreasonable short explanation for “protocol” when I speak to an audience that is more\nfamiliar with formal (and compiler enforced) interfaces.\nEstablished protocols naturally evolve in any language that uses dynamic typing, that\nis, when type checking is done at runtime because there is no static type information\nin method signatures and variables. Ruby is another important object-oriented lan‐\nguage that has dynamic typing and uses protocols.\nIn the Python documentation, you can often tell when a protocol is being discussed\nwhen you see language like “a file-like object.” This is a quick way of saying “some‐\nthing that behaves sufficiently like a file, by implementing the parts of the file inter‐\nface that are relevant in the context.”\nYou may think that implementing only part of a protocol is sloppy, but it has the\nadvantage of keeping things simple. Section 3.3 of the “Data Model” chapter suggests:\nWhen implementing a class that emulates any built-in type, it is important that the\nemulation only be implemented to the degree that it makes sense for the object being\nmodeled. For example, some sequences may work well with retrieval of individual\nelements, but extracting a slice may not make sense.\nWhen we don’t need to code nonsense methods just to fulfill some overdesigned\ninterface contract and keep the compiler happy, it becomes easier to follow the KISS\nprinciple.\nOn the other hand, if you want to use a type checker to verify your protocol imple‐\nmentations, then a stricter definition of protocol is required. That’s what typing.Pro\ntocol provides.\nI’ll have more to say about protocols and interfaces in Chapter 13, where they are the\nmain focus.\nOrigins of Duck Typing\nI believe the Ruby community, more than any other, helped popularize the term\n“duck typing,” as they preached to the Java masses. But the expression has been used\nin Python discussions before either Ruby or Python were “popular.” According to\nWikipedia, an early example of the duck analogy in object-oriented programming is a\nmessage to the Python-list by Alex Martelli from July 26, 2000: “polymorphism (was\nFurther Reading \n| \n427",
      "content_length": 2587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "7 I adapted the code for this presentation: in 2003, reduce was a built-in, but in Python 3 we need to import it;\nalso, I replaced the names x and y with my_list and sub, for sub-list.\nRe: Type checking in python?)”. That’s where the quote at the beginning of this chap‐\nter comes from. If you are curious about the literary origins of the “duck typing”\nterm, and the applications of this OO concept in many languages, check out Wikipe‐\ndia’s “Duck typing” entry.\nA Safe__format__, with Enhanced Usability\nWhile implementing __format__, I did not take any precautions regarding Vector\ninstances with a very large number of components, as we did in __repr__ using\nreprlib. The reasoning is that repr() is for debugging and logging, so it must always\ngenerate some serviceable output, while __format__ is used to display output to end\nusers who presumably want to see the entire Vector. If you think this is dangerous,\nthen it would be cool to implement a further extension to the Format Specifier Mini-\nLanguage.\nHere is how I’d do it: by default, any formatted Vector would display a reasonable\nbut limited number of components, say 30. If there are more elements than that, the\ndefault behavior would be similar to what the reprlib does: chop the excess and\nput ... in its place. However, if the format specifier ended with the special * code,\nmeaning “all,” then the size limitation would be disabled. So a user who’s unaware of\nthe problem of very long displays will not be bitten by it by accident. But if the default\nlimitation becomes a nuisance, then the presence of the ... could lead the user to\nsearch the documentation and discover the * formatting code.\nThe Search for a Pythonic Sum\nThere’s no single answer to “What is Pythonic?” just as there’s no single answer to\n“What is beautiful?” Saying, as I often do, that it means using “idiomatic Python” is\nnot 100% satisfactory, because what may be “idiomatic” for you may not be for me.\nOne thing I know: “idiomatic” does not mean using the most obscure language fea‐\ntures.\nIn the Python-list, there’s a thread titled “Pythonic Way to Sum n-th List Element?”\nfrom April 2003. It’s relevant to our discussion of reduce in this chapter.\nThe original poster, Guy Middleton, asked for an improvement on this solution, stat‐\ning he did not like to use lambda:7\n>>> my_list = [[1, 2, 3], [40, 50, 60], [9, 8, 7]]\n>>> import functools\n>>> functools.reduce(lambda a, b: a+b, [sub[1] for sub in my_list])\n60\n428 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 2510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "That code uses lots of idioms: lambda, reduce, and a list comprehension. It would\nprobably come last in a popularity contest, because it offends people who hate lambda\nand those who despise list comprehensions—pretty much both sides of a divide.\nIf you’re going to use lambda, there’s probably no reason to use a list comprehension\n—except for filtering, which is not the case here.\nHere is a solution of my own that will please the lambda lovers:\n>>> functools.reduce(lambda a, b: a + b[1], my_list, 0)\n60\nI did not take part in the original thread, and I wouldn’t use that in real code, because\nI don’t like lambda too much myself, but I wanted to show an example without a list\ncomprehension.\nThe first answer came from Fernando Perez, creator of IPython, highlighting that\nNumPy supports n-dimensional arrays and n-dimensional slicing:\n>>> import numpy as np\n>>> my_array = np.array(my_list)\n>>> np.sum(my_array[:, 1])\n60\nI think Perez’s solution is cool, but Guy Middleton praised this next solution, by Paul\nRubin and Skip Montanaro:\n>>> import operator\n>>> functools.reduce(operator.add, [sub[1] for sub in my_list], 0)\n60\nThen Evan Simpson asked, “What’s wrong with this?”:\n>>> total = 0\n>>> for sub in my_list:\n...     total += sub[1]\n...\n>>> total\n60\nLots of people agreed that was quite Pythonic. Alex Martelli went as far as saying\nthat’s probably how Guido would code it.\nI like Evan Simpson’s code, but I also like David Eppstein’s comment on it:\nIf you want the sum of a list of items, you should write it in a way that looks like “the\nsum of a list of items,” not in a way that looks like “loop over these items, maintain\nanother variable t, perform a sequence of additions.” Why do we have high-level lan‐\nguages if not to express our intentions at a higher level and let the language worry\nabout what low-level operations are needed to implement it?\nFurther Reading \n| \n429",
      "content_length": 1891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "Then Alex Martelli comes back to suggest:\n“The sum” is so frequently needed that I wouldn’t mind at all if Python singled it out\nas a built-in. But “reduce(operator.add, …” just isn’t a great way to express it, in my\nopinion (and yet as an old APL’er, and FP-liker, I should like it—but I don’t).\nAlex goes on to suggest a sum() function, which he contributed. It became a built-in\nin Python 2.3, released only three months after that conversation took place. So\nAlex’s preferred syntax became the norm:\n>>> sum([sub[1] for sub in my_list])\n60\nBy the end of the next year (November 2004), Python 2.4 was launched with genera‐\ntor expressions, providing what is now in my opinion the most Pythonic answer to\nGuy Middleton’s original question:\n>>> sum(sub[1] for sub in my_list)\n60\nThis is not only more readable than reduce but also avoids the trap of the empty\nsequence: sum([]) is 0, simple as that.\nIn the same conversation, Alex Martelli suggests the reduce built-in in Python 2 was\nmore trouble than it was worth, because it encouraged coding idioms that were hard\nto explain. He was most convincing: the function was demoted to the functools\nmodule in Python 3.\nStill, functools.reduce has its place. It solved the problem of our Vector.__hash__\nin a way that I would call Pythonic.\n430 \n| \nChapter 12: Special Methods for Sequences",
      "content_length": 1337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "1 Design Patterns: Elements of Reusable Object-Oriented Software, “Introduction,” p. 18.\nCHAPTER 13\nInterfaces, Protocols, and ABCs\nProgram to an interface, not an implementation.\n—Gamma, Helm, Johnson, Vlissides, First Principle of Object-Oriented Design1\nObject-oriented programming is all about interfaces. The best approach to under‐\nstanding a type in Python is knowing the methods it provides—its interface—as dis‐\ncussed in “Types Are Defined by Supported Operations” on page 260 (Chapter 8).\nDepending on the programming language, we have one or more ways of defining and\nusing interfaces. Since Python 3.8, we have four ways. They are depicted in the\nTyping Map (Figure 13-1). We can summarize them like this:\nDuck typing\nPython’s default approach to typing from the beginning. We’ve been studying\nduck typing since Chapter 1.\nGoose typing\nThe approach supported by abstract base classes (ABCs) since Python 2.6, which\nrelies on runtime checks of objects against ABCs. Goose typing is a major subject\nin this chapter.\nStatic typing\nThe traditional approach of statically-typed languages like C and Java; supported\nsince Python 3.5 by the typing module, and enforced by external type checkers\ncompliant with PEP 484—Type Hints. This is not the theme of this chapter. Most\nof Chapter 8 and the upcoming Chapter 15 are about static typing.\n431",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "Static duck typing\nAn approach made popular by the Go language; supported by subclasses of typ\ning.Protocol—new in Python 3.8—also enforced by external type checkers. We\nfirst saw this in “Static Protocols” on page 286 (Chapter 8).\nThe Typing Map\nThe four typing approaches depicted in Figure 13-1 are complementary: they have\ndifferent pros and cons. It doesn’t make sense to dismiss any of them.\nFigure 13-1. The top half describes runtime type checking approaches using just the\nPython interpreter; the bottom requires an external static type checker such as MyPy or\nan IDE like PyCharm. The left quadrants cover typing based on the object’s structure—\ni.e., the methods provided by the object, regardless of the name of its class or super‐\nclasses; the right quadrants depend on objects having explicitly named types: the name\nof the object’s class, or the name of its superclasses.\nEach of these four approaches rely on interfaces to work, but static typing can be\ndone—poorly—using only concrete types instead of interface abstractions like proto‐\ncols and abstract base classes. This chapter is about duck typing, goose typing, and\nstatic duck typing—typing disciplines that revolve around interfaces.\n432 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "This chapter is split in four main sections, addressing three of the four quadrants in\nthe Typing Map (Figure 13-1):\n• “Two Kinds of Protocols” on page 434 compares the two forms of structural\ntyping with protocols—i.e., the lefthand side of the Typing Map.\n• “Programming Ducks” on page 435 dives deeper into Python’s usual duck typing,\nincluding how to make it safer while preserving its major strength: flexibility.\n• “Goose Typing” on page 442 explains the use of ABCs for stricter runtime type\nchecking. This is the longest section, not because it’s more important, but\nbecause there are more sections about duck typing, static duck typing, and static\ntyping elsewhere in the book.\n• “Static Protocols” on page 466 covers usage, implementation, and design of typ\ning.Protocol subclasses—useful for static and runtime type checking.\nWhat’s New in This Chapter\nThis chapter was heavily edited and is about 24% longer than the corresponding\nChapter 11 in the first edition of Fluent Python. Although some sections and many\nparagraphs are the same, there’s a lot of new content. These are the highlights:\n• The chapter introduction and the Typing Map (Figure 13-1) are new. That’s the\nkey to most new content in this chapter—and all other chapters related to typing\nin Python ≥ 3.8.\n• “Two Kinds of Protocols” on page 434 explains the similarities and differences\nbetween dynamic and static protocols.\n• “Defensive Programming and ‘Fail Fast’” on page 440 mostly reproduces content\nfrom the first edition, but was updated and now has a section title to highlight its\nimportance.\n• “Static Protocols” on page 466 is all new. It builds on the initial presentation in\n“Static Protocols” on page 286 (Chapter 8).\n• Updated class diagrams of collections.abc in Figures 13-2, 13-3, and 13-4 to\ninclude the Collection ABC, from Python 3.6.\nThe first edition of Fluent Python had a section encouraging use of the numbers ABCs\nfor goose typing. In “The numbers ABCs and Numeric Protocols” on page 478, I\nexplain why you should use numeric static protocols from the typing module\ninstead, if you plan to use static type checkers as well as runtime checks in the style of\ngoose typing.\nWhat’s New in This Chapter \n| \n433",
      "content_length": 2210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "Two Kinds of Protocols\nThe word protocol has different meanings in computer science depending on context.\nA network protocol such as HTTP specifies commands that a client can send to a\nserver, such as GET, PUT, and HEAD. We saw in “Protocols and Duck Typing” on page\n402 that an object protocol specifies methods which an object must provide to fulfill a\nrole. The FrenchDeck example in Chapter 1 demonstrated one object protocol, the\nsequence protocol: the methods that allow a Python object to behave as a sequence.\nImplementing a full protocol may require several methods, but often it is OK to\nimplement only part of it. Consider the Vowels class in Example 13-1.\nExample 13-1. Partial sequence protocol implementation with __getitem__\n>>> class Vowels:\n...     def __getitem__(self, i):\n...         return 'AEIOU'[i]\n...\n>>> v = Vowels()\n>>> v[0]\n'A'\n>>> v[-1]\n'U'\n>>> for c in v: print(c)\n...\nA\nE\nI\nO\nU\n>>> 'E' in v\nTrue\n>>> 'Z' in v\nFalse\nImplementing __getitem__ is enough to allow retrieving items by index, and also to\nsupport iteration and the in operator. The __getitem__ special method is really the\nkey to the sequence protocol. Take a look at this entry from the Python/C API Refer‐\nence Manual, “Sequence Protocol” section:\nint PySequence_Check(PyObject *o)\nReturn 1 if the object provides sequence protocol, and 0 otherwise. Note that it\nreturns 1 for Python classes with a __getitem__() method unless they are dict\nsubclasses […].\nWe expect a sequence to also support len(), by implementing __len__. Vowels has\nno __len__ method, but it still behaves as a sequence in some contexts. And that may\n434 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "be enough for our purposes. That is why I like to say that a protocol is an “informal\ninterface.” That is also how protocols are understood in Smalltalk, the first object-\noriented programming environment to use that term.\nExcept in pages about network programming, most uses of the word “protocol” in\nthe Python documentation refer to these informal interfaces.\nNow, with the adoption of PEP 544—Protocols: Structural subtyping (static duck\ntyping) in Python 3.8, the word “protocol” has another meaning in Python—closely\nrelated, but different. As we saw in “Static Protocols” on page 286 (Chapter 8), PEP\n544 allows us to create subclasses of typing.Protocol to define one or more methods\nthat a class must implement (or inherit) to satisfy a static type checker.\nWhen I need to be specific, I will adopt these terms:\nDynamic protocol\nThe informal protocols Python always had. Dynamic protocols are implicit,\ndefined by convention, and described in the documentation. Python’s most\nimportant dynamic protocols are supported by the interpreter itself, and are doc‐\numented in the “Data Model” chapter of The Python Language Reference.\nStatic protocol\nA protocol as defined by PEP 544—Protocols: Structural subtyping (static duck\ntyping), since Python 3.8. A static protocol has an explicit definition: a typ\ning.Protocol subclass.\nThere are two key differences between them:\n• An object may implement only part of a dynamic protocol and still be useful; but\nto fulfill a static protocol, the object must provide every method declared in the\nprotocol class, even if your program doesn’t need them all.\n• Static protocols can be verified by static type checkers, but dynamic protocols\ncan’t.\nBoth kinds of protocols share the essential characteristic that a class never needs to\ndeclare that it supports a protocol by name, i.e., by inheritance.\nIn addition to static protocols, Python provides another way of defining an explicit\ninterface in code: an abstract base class (ABC).\nThe rest of this chapter covers dynamic and static protocols, as well as ABCs.\nProgramming Ducks\nLet’s start our discussion of dynamic protocols with two of the most important in\nPython: the sequence and iterable protocols. The interpreter goes out of its way to\nProgramming Ducks \n| \n435",
      "content_length": 2268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "handle objects that provide even a minimal implementation of those protocols, as the\nnext section explains.\nPython Digs Sequences\nThe philosophy of the Python Data Model is to cooperate with essential dynamic\nprotocols as much as possible. When it comes to sequences, Python tries hard to\nwork with even the simplest implementations.\nFigure 13-2 shows how the Sequence interface is formalized as an ABC. The Python\ninterpreter and built-in sequences like list, str, etc., do not rely on that ABC at all. I\nam using it only to describe what a full-fledged Sequence is expected to support.\nFigure 13-2. UML class diagram for the Sequence ABC and related abstract classes\nfrom collections.abc. Inheritance arrows point from a subclass to its superclasses.\nNames in italic are abstract methods. Before Python 3.6, there was no Collection\nABC—Sequence was a direct subclass of Container, Iterable, and Sized.\nMost ABCs in the collections.abc module exist to formalize\ninterfaces that are implemented by built-in objects and are implic‐\nitly supported by the interpreter—both of which predate the ABCs\nthemselves. The ABCs are useful as starting points for new classes,\nand to support explicit type checking at runtime (a.k.a. goose typ‐\ning) as well as type hints for static type checkers.\nStudying Figure 13-2, we see that a correct subclass of Sequence must implement\n__getitem__ and __len__ (from Sized). All the other methods in Sequence are con‐\ncrete, so subclasses can inherit their implementations—or provide better ones.\nNow, recall the Vowels class in Example 13-1. It does not inherit from abc.Sequence\nand it only implements __getitem__.\nThere is no __iter__ method, yet Vowels instances are iterable because—as a fallback\n—if Python finds a __getitem__ method, it tries to iterate over the object by calling\nthat method with integer indexes starting with 0. Because Python is smart enough to\n436 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "iterate over Vowels instances, it can also make the in operator work even when the\n__contains__ method is missing: it does a sequential scan to check if an item is\npresent.\nIn summary, given the importance of sequence-like data structures, Python manages\nto make iteration and the in operator work by invoking __getitem__ when __iter__\nand __contains__ are unavailable.\nThe original FrenchDeck from Chapter 1 does not subclass abc.Sequence either, but\nit does implement both methods of the sequence protocol: __getitem__ and __len__.\nSee Example 13-2.\nExample 13-2. A deck as a sequence of cards (same as Example 1-1)\nimport collections\nCard = collections.namedtuple('Card', ['rank', 'suit'])\nclass FrenchDeck:\n    ranks = [str(n) for n in range(2, 11)] + list('JQKA')\n    suits = 'spades diamonds clubs hearts'.split()\n    def __init__(self):\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\n    def __len__(self):\n        return len(self._cards)\n    def __getitem__(self, position):\n        return self._cards[position]\nSeveral of the examples in Chapter 1 work because of the special treatment Python\ngives to anything vaguely resembling a sequence. The iterable protocol in Python\nrepresents an extreme form of duck typing: the interpreter tries two different meth‐\nods to iterate over objects.\nTo be clear, the behaviors I described in this section are implemented in the inter‐\npreter itself, mostly in C. They do not depend on methods from the Sequence ABC.\nFor example, the concrete methods __iter__ and __contains__ in the Sequence\nclass emulate the built-in behaviors of the Python interpreter. If you are curious,\ncheck the source code of these methods in Lib/_collections_abc.py.\nNow let’s study another example emphasizing the dynamic nature of protocols—and\nwhy static type checkers have no chance of dealing with them.\nProgramming Ducks \n| \n437",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "2 The “Monkey patch” article on Wikipedia has a funny example in Python.\nMonkey Patching: Implementing a Protocol at Runtime\nMonkey patching is dynamically changing a module, class, or function at runtime, to\nadd features or fix bugs. For example, the gevent networking library monkey patches\nparts of Python’s standard library to allow lightweight concurrency without threads\nor async/await.2\nThe FrenchDeck class from Example 13-2 is missing an essential feature: it cannot be\nshuffled. Years ago when I first wrote the FrenchDeck example, I did implement a\nshuffle method. Later I had a Pythonic insight: if a FrenchDeck acts like a sequence,\nthen it doesn’t need its own shuffle method because there is already random.shuf\nfle, documented as “Shuffle the sequence x in place.”\nThe standard random.shuffle function is used like this:\n>>> from random import shuffle\n>>> l = list(range(10))\n>>> shuffle(l)\n>>> l\n[5, 2, 9, 7, 8, 3, 1, 4, 0, 6]\nWhen you follow established protocols, you improve your chances\nof leveraging existing standard library and third-party code, thanks\nto duck typing.\nHowever, if we try to shuffle a FrenchDeck instance, we get an exception, as in\nExample 13-3.\nExample 13-3. random.shuffle cannot handle FrenchDeck\n>>> from random import shuffle\n>>> from frenchdeck import FrenchDeck\n>>> deck = FrenchDeck()\n>>> shuffle(deck)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \".../random.py\", line 265, in shuffle\n    x[i], x[j] = x[j], x[i]\nTypeError: 'FrenchDeck' object does not support item assignment\nThe error message is clear: 'FrenchDeck' object does not support item assign\nment. The problem is that shuffle operates in place, by swapping items inside the\n438 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "collection, and FrenchDeck only implements the immutable sequence protocol.\nMutable sequences must also provide a __setitem__ method.\nBecause Python is dynamic, we can fix this at runtime, even at the interactive console.\nExample 13-4 shows how to do it.\nExample 13-4. Monkey patching FrenchDeck to make it mutable and compatible with\nrandom.shuffle (continuing from Example 13-3)\n>>> def set_card(deck, position, card):  \n...     deck._cards[position] = card\n...\n>>> FrenchDeck.__setitem__ = set_card  \n>>> shuffle(deck)  \n>>> deck[:5]\n[Card(rank='3', suit='hearts'), Card(rank='4', suit='diamonds'), Card(rank='4',\nsuit='clubs'), Card(rank='7', suit='hearts'), Card(rank='9', suit='spades')]\nCreate a function that takes deck, position, and card as arguments.\nAssign that function to an attribute named __setitem__ in the FrenchDeck class.\ndeck can now be shuffled because I added the necessary method of the mutable\nsequence protocol.\nThe signature of the __setitem__ special method is defined in The Python Language\nReference in “3.3.6. Emulating container types”. Here I named the arguments deck,\nposition, card—and not self, key, value as in the language reference—to show\nthat every Python method starts life as a plain function, and naming the first argu‐\nment self is merely a convention. This is OK in a console session, but in a Python\nsource file it’s much better to use self, key, and value as documented.\nThe trick is that set_card knows that the deck object has an attribute named _cards,\nand _cards must be a mutable sequence. The set_card function is then attached to\nthe FrenchDeck class as the __setitem__ special method. This is an example of mon‐\nkey patching: changing a class or module at runtime, without touching the source\ncode. Monkey patching is powerful, but the code that does the actual patching is very\ntightly coupled with the program to be patched, often handling private and undocu‐\nmented attributes.\nBesides being an example of monkey patching, Example 13-4 highlights the dynamic\nnature of protocols in dynamic duck typing: random.shuffle doesn’t care about the\nclass of the argument, it only needs the object to implement methods from the muta‐\nble sequence protocol. It doesn’t even matter if the object was “born” with the neces‐\nsary methods or if they were somehow acquired later.\nProgramming Ducks \n| \n439",
      "content_length": 2349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "3 That’s why automated testing is necessary.\nDuck typing doesn’t need to be wildly unsafe or hard to debug. The next section\nshows some useful code patterns to detect dynamic protocols without resorting to\nexplicit checks.\nDefensive Programming and “Fail Fast”\nDefensive programming is like defensive driving: a set of practices to enhance safety\neven when faced with careless programmers—or drivers.\nMany bugs cannot be caught except at runtime—even in mainstream statically typed\nlanguages.3 In a dynamically typed language, “fail fast” is excellent advice for safer\nand easier-to-maintain programs. Failing fast means raising runtime errors as soon\nas possible, for example, rejecting invalid arguments right a the beginning of a func‐\ntion body.\nHere is one example: when you write code that accepts a sequence of items to process\ninternally as a list, don’t enforce a list argument by type checking. Instead, take\nthe argument and immediately build a list from it. One example of this code pattern\nis the __init__ method in Example 13-10, later in this chapter:\n    def __init__(self, iterable):\n        self._balls = list(iterable)\nThat way you make your code more flexible, because the list() constructor handles\nany iterable that fits in memory. If the argument is not iterable, the call will fail fast\nwith a very clear TypeError exception, right when the object is initialized. If you want\nto be more explict, you can wrap the list() call with try/except to customize the\nerror message—but I’d use that extra code only on an external API, because the prob‐\nlem would be easy to see for maintainers of the codebase. Either way, the offending\ncall will appear near the end of the traceback, making it straightforward to fix. If you\ndon’t catch the invalid argument in the class constructor, the program will blow up\nlater, when some other method of the class needs to operate on self._balls and it is\nnot a list. Then the root cause will be harder to find.\nOf course, calling list() on the argument would be bad if the data shouldn’t be\ncopied, either because it’s too large or because the function, by design, needs to\nchange it in place for the benefit of the caller, like random.shuffle does. In that case,\na runtime check like isinstance(x, abc.MutableSequence) would be the way to go.\nIf you are afraid to get an infinite generator—not a common issue—you can begin by\ncalling len() on the argument. This would reject iterators, while safely dealing with\ntuples, arrays, and other existing or future classes that fully implement the Sequence\n440 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "interface. Calling len() is usually very cheap, and an invalid argument will raise an\nerror immediately.\nOn the other hand, if any iterable is acceptable, then call iter(x) as soon as possible\nto obtain an iterator, as we’ll see in “Why Sequences Are Iterable: The iter Function”\non page 596. Again, if x is not iterable, this will fail fast with an easy-to-debug\nexception.\nIn the cases I just described, a type hint could catch some problems earlier, but not all\nproblems. Recall that the type Any is consistent-with every other type. Type inference\nmay cause a variable to be tagged with the Any type. When that happens, the type\nchecker is in the dark. In addition, type hints are not enforced at runtime. Fail fast is\nthe last line of defense.\nDefensive code leveraging duck types can also include logic to handle different types\nwithout using isinstance() or hasattr() tests.\nOne example is how we might emulate the handling of the field_names argument in\ncollections.namedtuple: field_names accepts a single string with identifiers sepa‐\nrated by spaces or commas, or a sequence of identifiers. Example 13-5 shows how I’d\ndo it using duck typing.\nExample 13-5. Duck typing to handle a string or an iterable of strings\n    try:  \n        field_names = field_names.replace(',', ' ').split()  \n    except AttributeError:  \n        pass  \n    field_names = tuple(field_names)  \n    if not all(s.isidentifier() for s in field_names):  \n        raise ValueError('field_names must all be valid identifiers')\nAssume it’s a string (EAFP = it’s easier to ask forgiveness than permission).\nConvert commas to spaces and split the result into a list of names.\nSorry, field_names doesn’t quack like a str: it has no .replace, or it returns\nsomething we can’t .split.\nIf AttributeError was raised, then field_names is not a str and we assume it\nwas already an iterable of names.\nTo make sure it’s an iterable and to keep our own copy, create a tuple out of what\nwe have. A tuple is more compact than list, and it also prevents my code from\nchanging the names by mistake.\nProgramming Ducks \n| \n441",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "4 Bjarne Stroustrup, The Design and Evolution of C++, p. 278 (Addison-Wesley).\n5 Retrieved October 18, 2020.\nUse str.isidentifier to ensure every name is valid.\nExample 13-5 shows one situation where duck typing is more expressive than static\ntype hints. There is no way to spell a type hint that says “field_names must be a\nstring of identifiers separated by spaces or commas.” This is the relevant part of the\nnamedtuple signature on typeshed (see the full source at stdlib/3/collections/\n__init__.pyi):\n    def namedtuple(\n        typename: str,\n        field_names: Union[str, Iterable[str]],\n        *,\n        # rest of signature omitted\nAs you can see, field_names is annotated as Union[str, Iterable[str]], which is\nOK as far as it goes, but is not enough to catch all possible problems.\nAfter reviewing dynamic protocols, we move to a more explicit form of runtime type\nchecking: goose typing.\nGoose Typing\nAn abstract class represents an interface.\n—Bjarne Stroustrup, creator of C++4\nPython doesn’t have an interface keyword. We use abstract base classes (ABCs) to\ndefine interfaces for explicit type checking at runtime—also supported by static type\ncheckers.\nThe Python Glossary entry for abstract base class has a good explanation of the value\nthey bring to duck-typed languages:\nAbstract base classes complement duck typing by providing a way to define interfaces\nwhen other techniques like hasattr() would be clumsy or subtly wrong (for example,\nwith magic methods). ABCs introduce virtual subclasses, which are classes that don’t\ninherit from a class but are still recognized by isinstance() and issubclass(); see\nthe abc module documentation.5\nGoose typing is a runtime type checking approach that leverages ABCs. I will let Alex\nMartelli explain in “Waterfowl and ABCs” on page 443.\n442 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "I am very grateful to my friends Alex Martelli and Anna Raven‐\nscroft. I showed them the first outline of Fluent Python at OSCON\n2013, and they encouraged me to submit it for publication with\nO’Reilly. Both later contributed with thorough tech reviews. Alex\nwas already the most cited person in this book, and then he offered\nto write this essay. Take it away, Alex!\nWaterfowl and ABCs\nBy Alex Martelli\nI’ve been credited on Wikipedia for helping spread the helpful meme and sound-bite\n“duck typing” (i.e, ignoring an object’s actual type, focusing instead on ensuring that\nthe object implements the method names, signatures, and semantics required for its\nintended use).\nIn Python, this mostly boils down to avoiding the use of isinstance to check the\nobject’s type (not to mention the even worse approach of checking, for example,\nwhether type(foo) is bar—which is rightly anathema as it inhibits even the sim‐\nplest forms of inheritance!).\nThe overall duck typing approach remains quite useful in many contexts—and yet, in\nmany others, an often preferable one has evolved over time. And herein lies a tale…\nIn recent generations, the taxonomy of genus and species (including, but not limited\nto, the family of waterfowl known as Anatidae) has mostly been driven by phenetics—\nan approach focused on similarities of morphology and behavior…chiefly, observable\ntraits. The analogy to “duck typing” was strong.\nHowever, parallel evolution can often produce similar traits, both morphological and\nbehavioral ones, among species that are actually unrelated, but just happened to\nevolve in similar, though separate, ecological niches. Similar “accidental similarities”\nhappen in programming, too—for example, consider the classic object-oriented pro‐\ngramming example:\nclass Artist:\n    def draw(self): ...\nclass Gunslinger:\n    def draw(self): ...\nclass Lottery:\n    def draw(self): ...\nClearly, the mere existence of a method named draw, callable without arguments,\nis far from sufficient to assure us that two objects x and y, such that x.draw() and\ny.draw() can be called, are in any way exchangeable or abstractly equivalent—\nGoose Typing \n| \n443",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "6 You can also, of course, define your own ABCs—but I would discourage all but the most advanced Pythonis‐\ntas from going that route, just as I would discourage them from defining their own custom metaclasses…and\neven for said “most advanced Pythonistas,” those of us sporting deep mastery of every fold and crease in the\nlanguage, these are not tools for frequent use. Such “deep metaprogramming,” if ever appropriate, is intended\nfor authors of broad frameworks meant to be independently extended by vast numbers of separate develop‐\nment teams…less than 1% of “most advanced Pythonistas” may ever need that! — A.M.\nnothing about the similarity of the semantics resulting from such calls can be infer‐\nred. Rather, we need a knowledgeable programmer to somehow positively assert that\nsuch an equivalence holds at some level!\nIn biology (and other disciplines), this issue has led to the emergence (and, on many\nfacets, the dominance) of an approach that’s an alternative to phenetics, known as\ncladistics—focusing taxonomical choices on characteristics that are inherited from\ncommon ancestors, rather than ones that are independently evolved. (Cheap and\nrapid DNA sequencing can make cladistics highly practical in many more cases in\nrecent years.)\nFor example, sheldgeese (once classified as being closer to other geese) and shelducks\n(once classified as being closer to other ducks) are now grouped together within\nthe subfamily Tadornidae (implying they’re closer to each other than to any other\nAnatidae, as they share a closer common ancestor). Furthermore, DNA analysis has\nshown, in particular, that the white-winged wood duck is not as close to the Muscovy\nduck (the latter being a shelduck) as similarity in looks and behavior had long sug‐\ngested—so the wood duck was reclassified into its own genus, and entirely out of the\nsubfamily!\nDoes this matter? It depends on the context! For such purposes as deciding how best\nto cook a waterfowl once you’ve bagged it, for example, specific observable traits (not\nall of them—plumage, for example, is de minimis in such a context), mostly texture\nand flavor (old-fashioned phenetics!), may be far more relevant than cladistics. But\nfor other issues, such as susceptibility to different pathogens (whether you’re trying to\nraise waterfowl in captivity, or preserve them in the wild), DNA closeness can matter\nmuch more.\nSo, by very loose analogy with these taxonomic revolutions in the world of water‐\nfowls, I’m recommending supplementing (not entirely replacing—in certain contexts\nit shall still serve) good old duck typing with…goose typing!\nWhat goose typing means is: isinstance(obj, cls) is now just fine…as long as cls\nis an abstract base class—in other words, cls’s metaclass is abc.ABCMeta.\nYou can find many useful existing abstract classes in collections.abc (and addi‐\ntional ones in the numbers module of The Python Standard Library).6\nAmong the many conceptual advantages of ABCs over concrete classes (e.g., Scott\nMeyer’s “all non-leaf classes should be abstract”; see Item 33 in his book, More\n444 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 3118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Effective C++, Addison-Wesley), Python’s ABCs add one major practical advantage:\nthe register class method, which lets end-user code “declare” that a certain class\nbecomes a “virtual” subclass of an ABC (for this purpose, the registered class must\nmeet the ABC’s method name and signature requirements, and more importantly,\nthe underlying semantic contract—but it need not have been developed with any\nawareness of the ABC, and in particular need not inherit from it!). This goes a long\nway toward breaking the rigidity and strong coupling that make inheritance some‐\nthing to use with much more caution than typically practiced by most object-oriented\nprogrammers.\nSometimes you don’t even need to register a class for an ABC to recognize it as a\nsubclass!\nThat’s the case for the ABCs whose essence boils down to a few special methods. For\nexample:\n>>> class Struggle:\n...     def __len__(self): return 23\n...\n>>> from collections import abc\n>>> isinstance(Struggle(), abc.Sized)\nTrue\nAs you see, abc.Sized recognizes Struggle as “a subclass,” with no need for registra‐\ntion, as implementing the special method named __len__ is all it takes (it’s supposed\nto be implemented with the proper syntax—callable without arguments—and seman‐\ntics—returning a nonnegative integer denoting an object’s “length”; any code that\nimplements a specially named method, such as __len__, with arbitrary, non-\ncompliant syntax and semantics has much worse problems anyway).\nSo, here’s my valediction: whenever you’re implementing a class embodying any of\nthe concepts represented in the ABCs in numbers, collections.abc, or other frame‐\nwork you may be using, be sure (if needed) to subclass it from, or register it into, the\ncorresponding ABC. At the start of your programs using some library or framework\ndefining classes which have omitted to do that, perform the registrations yourself;\nthen, when you must check for (most typically) an argument being, e.g, “a sequence,”\ncheck whether:\nisinstance(the_arg, collections.abc.Sequence)\nAnd, don’t define custom ABCs (or metaclasses) in production code. If you feel the\nurge to do so, I’d bet it’s likely to be a case of the “all problems look like a nail”–syn‐\ndrome for somebody who just got a shiny new hammer—you (and future maintain‐\ners of your code) will be much happier sticking with straightforward and simple code,\neschewing such depths. Valē!\nGoose Typing \n| \n445",
      "content_length": 2410,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "To summarize, goose typing entails:\n• Subclassing from ABCs to make it explict that you are implementing a previously\ndefined interface.\n• Runtime type checking using ABCs instead of concrete classes as the second\nargument for isinstance and issubclass.\nAlex makes the point that inheriting from an ABC is more than implementing the\nrequired methods: it’s also a clear declaration of intent by the developer. That intent\ncan also be made explicit through registering a virtual subclass.\nDetails of using register are covered in “A Virtual Subclass of an\nABC” on page 460, later in this chapter. For now, here is a brief\nexample: given the FrenchDeck class, if I want it to pass a check like\nissubclass(FrenchDeck, Sequence), I can make it a virtual sub‐\nclass of the Sequence ABC with these lines:\nfrom collections.abc import Sequence\nSequence.register(FrenchDeck)\nThe use of isinstance and issubclass becomes more acceptable if you are checking\nagainst ABCs instead of concrete classes. If used with concrete classes, type checks\nlimit polymorphism—an essential feature of object-oriented programming. But with\nABCs these tests are more flexible. After all, if a component does not implement an\nABC by subclassing—but does implement the required methods—it can always be\nregistered after the fact so it passes those explicit type checks.\nHowever, even with ABCs, you should beware that excessive use of isinstance\nchecks may be a code smell—a symptom of bad OO design.\nIt’s usually not OK to have a chain of if/elif/elif with isinstance checks per‐\nforming different actions depending on the type of object: you should be using poly‐\nmorphism for that—i.e., design your classes so that the interpreter dispatches calls to\nthe proper methods, instead of you hardcoding the dispatch logic in if/elif/elif\nblocks.\nOn the other hand, it’s OK to perform an isinstance check against an ABC if you\nmust enforce an API contract: “Dude, you have to implement this if you want to call\nme,” as technical reviewer Lennart Regebro put it. That’s particularly useful in sys‐\ntems that have a plug-in architecture. Outside of frameworks, duck typing is often\nsimpler and more flexible than type checks.\nFinally, in his essay, Alex reinforces more than once the need for restraint in the cre‐\nation of ABCs. Excessive use of ABCs would impose ceremony in a language that\n446 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "became popular because it is practical and pragmatic. During the Fluent Python\nreview process, Alex wrote in an e-mail:\nABCs are meant to encapsulate very general concepts, abstractions, introduced by a\nframework—things like “a sequence” and “an exact number.” [Readers] most likely\ndon’t need to write any new ABCs, just use existing ones correctly, to get 99.9% of the\nbenefits without serious risk of misdesign.\nNow let’s see goose typing in practice.\nSubclassing an ABC\nFollowing Martelli’s advice, we’ll leverage an existing ABC, collections.MutableSe\nquence, before daring to invent our own. In Example 13-6, FrenchDeck2 is explicitly\ndeclared a subclass of collections.MutableSequence.\nExample 13-6. frenchdeck2.py: FrenchDeck2, a subclass of collections.MutableSe\nquence\nfrom collections import namedtuple, abc\nCard = namedtuple('Card', ['rank', 'suit'])\nclass FrenchDeck2(abc.MutableSequence):\n    ranks = [str(n) for n in range(2, 11)] + list('JQKA')\n    suits = 'spades diamonds clubs hearts'.split()\n    def __init__(self):\n        self._cards = [Card(rank, suit) for suit in self.suits\n                                        for rank in self.ranks]\n    def __len__(self):\n        return len(self._cards)\n    def __getitem__(self, position):\n        return self._cards[position]\n    def __setitem__(self, position, value):  \n        self._cards[position] = value\n    def __delitem__(self, position):  \n        del self._cards[position]\n    def insert(self, position, value):  \n        self._cards.insert(position, value)\nGoose Typing \n| \n447",
      "content_length": 1554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "__setitem__ is all we need to enable shuffling…\n…but subclassing MutableSequence forces us to implement __delitem__, an\nabstract method of that ABC.\nWe are also required to implement insert, the third abstract method of\nMutableSequence.\nPython does not check for the implementation of the abstract methods at import\ntime (when the frenchdeck2.py module is loaded and compiled), but only at runtime\nwhen we actually try to instantiate FrenchDeck2. Then, if we fail to implement any\nof the abstract methods, we get a TypeError exception with a message such as\n\"Can't instantiate abstract class FrenchDeck2 with abstract methods\n__delitem__, insert\". That’s why we must implement __delitem__ and insert,\neven if our FrenchDeck2 examples do not need those behaviors: the MutableSequence\nABC demands them.\nAs Figure 13-3 shows, not all methods of the Sequence and MutableSequence ABCs\nare abstract.\nFigure 13-3. UML class diagram for the MutableSequence ABC and its superclasses\nfrom collections.abc (inheritance arrows point from subclasses to ancestors; names\nin italic are abstract classes and abstract methods).\nTo write FrenchDeck2 as a subclass of MutableSequence, I had to pay the price of\nimplementing __delitem__ and insert, which my examples did not require. In\nreturn, FrenchDeck2 inherits five concrete methods from Sequence: __contains__,\n__iter__, __reversed__, index, and count. From MutableSequence, it gets another\nsix methods: append, reverse, extend, pop, remove, and __iadd__—which supports\nthe += operator for in place concatenation.\nThe concrete methods in each collections.abc ABC are implemented in terms of\nthe public interface of the class, so they work without any knowledge of the internal\nstructure of instances.\n448 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "7 Multiple inheritance was considered harmful and excluded from Java, except for interfaces: Java interfaces can\nextend multiple interfaces, and Java classes can implement multiple interfaces.\nAs the coder of a concrete subclass, you may be able to override\nmethods inherited from ABCs with more efficient implementa‐\ntions. For example, __contains__ works by doing a sequential scan\nof the sequence, but if your concrete sequence keeps its items sor‐\nted, you can write a faster __contains__ that does a binary search\nusing the bisect function from the standard library. See “Manag‐\ning Ordered Sequences with Bisect” at fluentpython.com to learn\nmore about it.\nTo use ABCs well, you need to know what’s available. We’ll review the collections\nABCs next.\nABCs in the Standard Library\nSince Python 2.6, the standard library provides several ABCs. Most are defined in the\ncollections.abc module, but there are others. You can find ABCs in the io and\nnumbers packages, for example. But the most widely used are in collections.abc.\nThere are two modules named abc in the standard library. Here we\nare talking about collections.abc. To reduce loading time, since\nPython 3.4 that module is implemented outside of the collections\npackage—in Lib/_collections_abc.py—so it’s imported separately\nfrom collections. The other abc module is just abc (i.e., Lib/\nabc.py) where the abc.ABC class is defined. Every ABC depends on\nthe abc module, but we don’t need to import it ourselves except to\ncreate a brand-new ABC.\nFigure 13-4 is a summary UML class diagram (without attribute names) of 17 ABCs\ndefined in collections.abc. The documentation of collections.abc has a nice\ntable summarizing the ABCs, their relationships, and their abstract and concrete\nmethods (called “mixin methods”). There is plenty of multiple inheritance going on\nin Figure 13-4. We’ll devote most of Chapter 14 to multiple inheritance, but for now\nit’s enough to say that it is usually not a problem when ABCs are concerned.7\nGoose Typing \n| \n449",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "Figure 13-4. UML class diagram for ABCs in collections.abc.\nLet’s review the clusters in Figure 13-4:\nIterable, Container, Sized\nEvery collection should either inherit from these ABCs or implement compatible\nprotocols. Iterable supports iteration with __iter__, Container supports the\nin operator with __contains__, and Sized supports len() with __len__.\nCollection\nThis ABC has no methods of its own, but was added in Python 3.6 to make it\neasier to subclass from Iterable, Container, and Sized.\nSequence, Mapping, Set\nThese are the main immutable collection types, and each has a mutable subclass.\nA detailed diagram for MutableSequence is in Figure 13-3; for MutableMapping\nand MutableSet, see Figures 3-1 and 3-2 in Chapter 3.\nMappingView\nIn Python 3, the objects returned from the mapping methods .items(), .keys(),\nand .values() implement the interfaces defined in ItemsView, KeysView, and\nValuesView, respectively. The first two also implement the rich interface of Set,\nwith all the operators we saw in “Set Operations” on page 107.\nIterator\nNote that iterator subclasses Iterable. We discuss this further in Chapter 17.\n450 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "Callable, Hashable\nThese are not collections, but collections.abc was the first package to define\nABCs in the standard library, and these two were deemed important enough\nto be included. They support type checking objects that must be callable or\nhashable.\nFor callable detection, the callable(obj) built-in function is more convenient than\ninsinstance(obj, Callable).\nIf insinstance(obj, Hashable) returns False, you can be certain that obj is not\nhashable. But if the return is True, it may be a false positive. The next box explains.\nisinstance with Hashable and Iterable Can Be Misleading\nIt’s easy to misinterpret the results of the isinstance and issubclass tests against\nthe Hashable and Iterable ABCs.\nIf isinstance(obj, Hashable) returns True, that only means that the class of obj\nimplements or inherits __hash__. But if obj is a tuple containing unhashable items,\nthen obj is not hashable, despite the positive result of the isinstance check. Tech\nreviewer Jürgen Gmach pointed out that duck typing provides the most accurate way\nto determine if an instance is hashable: call hash(obj). That call will raise TypeError\nif obj is not hashable.\nOn the other hand, even when isinstance(obj, Iterable) returns False, Python\nmay still be able to iterate over obj using __getitem__ with 0-based indices, as we\nsaw in Chapter 1 and “Python Digs Sequences” on page 436. The documentation for\ncollections.abc.Iterable states:\nThe only reliable way to determine whether an object is iterable is to call iter(obj).\nAfter looking at some existing ABCs, let’s practice goose typing by implementing an\nABC from scratch and putting it to use. The goal here is not to encourage everyone to\nstart creating ABCs left and right, but to learn how to read the source code of the\nABCs you’ll find in the standard library and other packages.\nDefining and Using an ABC\nThis warning appeared in the “Interfaces” chapter of the first edition of Fluent\nPython:\nABCs, like descriptors and metaclasses, are tools for building frameworks. Therefore,\nonly a small minority of Python developers can create ABCs without imposing unrea‐\nsonable limitations and needless work on fellow programmers.\nGoose Typing \n| \n451",
      "content_length": 2194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "8 Perhaps the client needs to audit the randomizer; or the agency wants to provide a rigged one. You never\nknow…\nNow ABCs have more potential use cases in type hints to support static typing. As\ndiscussed in “Abstract Base Classes” on page 278, using ABCs instead of concrete\ntypes in function argument type hints gives more flexibility to the caller.\nTo justify creating an ABC, we need to come up with a context for using it as an\nextension point in a framework. So here is our context: imagine you need to display\nadvertisements on a website or a mobile app in random order, but without repeating\nan ad before the full inventory of ads is shown. Now let’s assume we are building an\nad management framework called ADAM. One of its requirements is to support user-\nprovided nonrepeating random-picking classes.8 To make it clear to ADAM users what\nis expected of a “nonrepeating random-picking” component, we’ll define an ABC.\nIn the literature about data structures, “stack” and “queue” describe abstract inter‐\nfaces in terms of physical arrangements of objects. I will follow suit and use a real-\nworld metaphor to name our ABC: bingo cages and lottery blowers are machines\ndesigned to pick items at random from a finite set, without repeating, until the set is\nexhausted.\nThe ABC will be named Tombola, after the Italian name of bingo and the tumbling\ncontainer that mixes the numbers.\nThe Tombola ABC has four methods. The two abstract methods are:\n.load(…)\nPut items into the container.\n.pick()\nRemove one item at random from the container, returning it.\nThe concrete methods are:\n.loaded()\nReturn True if there is at least one item in the container.\n.inspect()\nReturn a tuple built from the items currently in the container, without changing\nits contents (the internal ordering is not preserved).\nFigure 13-5 shows the Tombola ABC and three concrete implementations.\n452 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "9 «registered» and «virtual subclass» are not standard UML terms. I am using them to represent a class relation‐\nship that is specific to Python.\nFigure 13-5. UML diagram for an ABC and three subclasses. The name of the Tombola\nABC and its abstract methods are written in italics, per UML conventions. The dashed\narrow is used for interface implementation—here I am using it to show that TomboList\nnot only implements the Tombola interface, but is also registered as virtual subclass of\nTombola—as we will see later in this chapter.9\nExample 13-7 shows the definition of the Tombola ABC.\nExample 13-7. tombola.py: Tombola is an ABC with two abstract methods and two\nconcrete methods\nimport abc\nclass Tombola(abc.ABC):  \n    @abc.abstractmethod\n    def load(self, iterable):  \n        \"\"\"Add items from an iterable.\"\"\"\n    @abc.abstractmethod\n    def pick(self):  \n        \"\"\"Remove item at random, returning it.\n        This method should raise `LookupError` when the instance is empty.\n        \"\"\"\n    def loaded(self):  \n        \"\"\"Return `True` if there's at least 1 item, `False` otherwise.\"\"\"\nGoose Typing \n| \n453",
      "content_length": 1118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "10 Before ABCs existed, abstract methods would raise NotImplementedError to signal that subclasses were\nresponsible for their implementation. In Smalltalk-80, abstract method bodies would invoke subclassRespon\nsibility, a method inherited from object that would produce an error with the message, “My subclass\nshould have overridden one of my messages.”\n        return bool(self.inspect())  \n    def inspect(self):\n        \"\"\"Return a sorted tuple with the items currently inside.\"\"\"\n        items = []\n        while True:  \n            try:\n                items.append(self.pick())\n            except LookupError:\n                break\n        self.load(items)  \n        return tuple(items)\nTo define an ABC, subclass abc.ABC.\nAn abstract method is marked with the @abstractmethod decorator, and often its\nbody is empty except for a docstring.10\nThe docstring instructs implementers to raise LookupError if there are no items\nto pick.\nAn ABC may include concrete methods.\nConcrete methods in an ABC must rely only on the interface defined by the ABC\n(i.e., other concrete or abstract methods or properties of the ABC).\nWe can’t know how concrete subclasses will store the items, but we can build the\ninspect result by emptying the Tombola with successive calls to .pick()…\n…then use .load(…) to put everything back.\n454 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "An abstract method can actually have an implementation. Even if\nit does, subclasses will still be forced to override it, but they will be\nable to invoke the abstract method with super(), adding function‐\nality to it instead of implementing from scratch. See the abc mod‐\nule documentation for details on @abstractmethod usage.\nThe code for the .inspect() method in Example 13-7 is silly, but it shows that we\ncan rely on .pick() and .load(…) to inspect what’s inside the Tombola by picking all\nitems and loading them back—without knowing how the items are actually stored.\nThe point of this example is to highlight that it’s OK to provide concrete methods in\nABCs, as long as they only depend on other methods in the interface. Being aware of\ntheir internal data structures, concrete subclasses of Tombola may always over‐\nride .inspect() with a smarter implementation, but they don’t have to.\nThe .loaded() method in Example 13-7 has one line, but it’s expensive: it\ncalls .inspect() to build the tuple just to apply bool() on it. This works, but a con‐\ncrete subclass can do much better, as we’ll see.\nNote that our roundabout implementation of .inspect() requires that we catch a\nLookupError thrown by self.pick(). The fact that self.pick() may raise LookupEr\nror is also part of its interface, but there is no way to make this explicit in Python,\nexcept in the documentation (see the docstring for the abstract pick method in\nExample 13-7).\nI chose the LookupError exception because of its place in the Python hierarchy of\nexceptions in relation to IndexError and KeyError, the most likely exceptions to be\nraised by the data structures used to implement a concrete Tombola. Therefore,\nimplementations can raise LookupError, IndexError, KeyError, or a custom subclass\nof LookupError to comply. See Figure 13-6.\nGoose Typing \n| \n455",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "11 The complete tree is in section “5.4. Exception hierarchy” of The Python Standard Library docs.\nFigure 13-6. Part of the Exception class hierarchy.11\nLookupError is the exception we handle in Tombola.inspect.\nIndexError is the LookupError subclass raised when we try to get an item from a\nsequence with an index beyond the last position.\nKeyError is raised when we use a nonexistent key to get an item from a mapping.\nWe now have our very own Tombola ABC. To witness the interface checking per‐\nformed by an ABC, let’s try to fool Tombola with a defective implementation in\nExample 13-8.\nExample 13-8. A fake Tombola doesn’t go undetected\n>>> from tombola import Tombola\n>>> class Fake(Tombola):  \n...     def pick(self):\n...         return 13\n...\n>>> Fake  \n<class '__main__.Fake'>\n>>> f = Fake()  \nTraceback (most recent call last):\n456 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "12 The @abc.abstractmethod entry in the abc module documentation.\n  File \"<stdin>\", line 1, in <module>\nTypeError: Can't instantiate abstract class Fake with abstract method load\nDeclare Fake as a subclass of Tombola.\nThe class was created, no errors so far.\nTypeError is raised when we try to instantiate Fake. The message is very clear:\nFake is considered abstract because it failed to implement load, one of the\nabstract methods declared in the Tombola ABC.\nSo we have our first ABC defined, and we put it to work validating a class. We’ll soon\nsubclass the Tombola ABC, but first we must cover some ABC coding rules.\nABC Syntax Details\nThe standard way to declare an ABC is to subclass abc.ABC or any other ABC.\nBesides the ABC base class, and the @abstractmethod decorator, the abc module\ndefines the @abstractclassmethod, @abstractstaticmethod, and @abstractprop\nerty decorators. However, these last three were deprecated in Python 3.3, when it\nbecame possible to stack decorators on top of @abstractmethod, making the others\nredundant. For example, the preferred way to declare an abstract class method is:\nclass MyABC(abc.ABC):\n    @classmethod\n    @abc.abstractmethod\n    def an_abstract_classmethod(cls, ...):\n        pass\nThe order of stacked function decorators matters, and in the case\nof @abstractmethod, the documentation is explicit:\nWhen abstractmethod() is applied in combination with\nother method descriptors, it should be applied as the\ninnermost decorator…12\nIn other words, no other decorator may appear between @abstract\nmethod and the def statement.\nNow that we’ve got these ABC syntax issues covered, let’s put Tombola to use by\nimplementing two concrete descendants of it.\nGoose Typing \n| \n457",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "Subclassing an ABC\nGiven the Tombola ABC, we’ll now develop two concrete subclasses that satisfy its\ninterface. These classes were pictured in Figure 13-5, along with the virtual subclass\nto be discussed in the next section.\nThe BingoCage class in Example 13-9 is a variation of Example 7-8 using a better ran‐\ndomizer. This BingoCage implements the required abstract methods load and pick.\nExample 13-9. bingo.py: BingoCage is a concrete subclass of Tombola\nimport random\nfrom tombola import Tombola\nclass BingoCage(Tombola):  \n    def __init__(self, items):\n        self._randomizer = random.SystemRandom()  \n        self._items = []\n        self.load(items)  \n    def load(self, items):\n        self._items.extend(items)\n        self._randomizer.shuffle(self._items)  \n    def pick(self):  \n        try:\n            return self._items.pop()\n        except IndexError:\n            raise LookupError('pick from empty BingoCage')\n    def __call__(self):  \n        self.pick()\nThis BingoCage class explicitly extends Tombola.\nPretend we’ll use this for online gaming. random.SystemRandom implements the\nrandom API on top of the os.urandom(…) function, which provides random bytes\n“suitable for cryptographic use,” according to the os module docs.\nDelegate initial loading to the .load(…) method.\nInstead of the plain random.shuffle() function, we use the .shuffle() method\nof our SystemRandom instance.\n458 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "pick is implemented as in Example 7-8.\n__call__ is also from Example 7-8. It’s not needed to satisfy the Tombola inter‐\nface, but there’s no harm in adding extra methods.\nBingoCage inherits the expensive loaded and the silly inspect methods from Tom\nbola. Both could be overridden with much faster one-liners, as in Example 13-10.\nThe point is: we can be lazy and just inherit the suboptimal concrete methods from\nan ABC. The methods inherited from Tombola are not as fast as they could be for\nBingoCage, but they do provide correct results for any Tombola subclass that correctly\nimplements pick and load.\nExample 13-10 shows a very different but equally valid implementation of the Tom\nbola interface. Instead of shuffling the “balls” and popping the last, LottoBlower\npops from a random position.\nExample 13-10. lotto.py: LottoBlower is a concrete subclass that overrides the inspect\nand loaded methods from Tombola\nimport random\nfrom tombola import Tombola\nclass LottoBlower(Tombola):\n    def __init__(self, iterable):\n        self._balls = list(iterable)  \n    def load(self, iterable):\n        self._balls.extend(iterable)\n    def pick(self):\n        try:\n            position = random.randrange(len(self._balls))  \n        except ValueError:\n            raise LookupError('pick from empty LottoBlower')\n        return self._balls.pop(position)  \n    def loaded(self):  \n        return bool(self._balls)\n    def inspect(self):  \n        return tuple(self._balls)\nGoose Typing \n| \n459",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "13 “Defensive Programming with Mutable Parameters” on page 216 in Chapter 6 was devoted to the aliasing\nissue we just avoided here.\nThe initializer accepts any iterable: the argument is used to build a list.\nThe random.randrange(…) function raises ValueError if the range is empty, so\nwe catch that and throw LookupError instead, to be compatible with Tombola.\nOtherwise the randomly selected item is popped from self._balls.\nOverride loaded to avoid calling inspect (as Tombola.loaded does in\nExample 13-7). We can make it faster by working with self._balls directly—no\nneed to build a whole new tuple.\nOverride inspect with a one-liner.\nExample 13-10 illustrates an idiom worth mentioning: in __init__, self._balls\nstores list(iterable) and not just a reference to iterable (i.e., we did not merely\nassign self._balls = iterable, aliasing the argument). As mentioned in “Defen‐\nsive Programming and ‘Fail Fast’” on page 440, this makes our LottoBlower flexible\nbecause the iterable argument may be any iterable type. At the same time, we make\nsure to store its items in a list so we can pop items. And even if we always get lists as\nthe iterable argument, list(iterable) produces a copy of the argument, which is\na good practice considering we will be removing items from it and the client might\nnot expect that the provided list will be changed.13\nWe now come to the crucial dynamic feature of goose typing: declaring virtual sub‐\nclasses with the register method.\nA Virtual Subclass of an ABC\nAn essential characteristic of goose typing—and one reason why it deserves a water‐\nfowl name—is the ability to register a class as a virtual subclass of an ABC, even if it\ndoes not inherit from it. When doing so, we promise that the class faithfully imple‐\nments the interface defined in the ABC—and Python will believe us without check‐\ning. If we lie, we’ll be caught by the usual runtime exceptions.\nThis is done by calling a register class method on the ABC. The registered class\nthen becomes a virtual subclass of the ABC, and will be recognized as such by issub\nclass, but it does not inherit any methods or attributes from the ABC.\n460 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "Virtual subclasses do not inherit from their registered ABCs, and\nare not checked for conformance to the ABC interface at any time,\nnot even when they are instantiated. Also, static type checkers can’t\nhandle virtual subclasses at this time. For details, see Mypy issue\n2922—ABCMeta.register support.\nThe register method is usually invoked as a plain function (see “Usage of register in\nPractice” on page 463), but it can also be used as a decorator. In Example 13-11, we use\nthe decorator syntax and implement TomboList, a virtual subclass of Tombola, depic‐\nted in Figure 13-7.\nFigure 13-7. UML class diagram for the TomboList, a real subclass of list and a vir‐\ntual subclass of Tombola.\nExample 13-11. tombolist.py: class TomboList is a virtual subclass of Tombola\nfrom random import randrange\nfrom tombola import Tombola\n@Tombola.register  \nclass TomboList(list):  \n    def pick(self):\n        if self:  \n            position = randrange(len(self))\n            return self.pop(position)  \nGoose Typing \n| \n461",
      "content_length": 1014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "14 The same trick I used with load() doesn’t work with loaded(), because the list type does not implement\n__bool__, the method I’d have to bind to loaded. The bool() built-in doesn’t need __bool__ to work because\nit can also use __len__. See “4.1. Truth Value Testing” in the “Built-in Types” chapter of the Python\ndocumentation.\n        else:\n            raise LookupError('pop from empty TomboList')\n    load = list.extend  \n    def loaded(self):\n        return bool(self)  \n    def inspect(self):\n        return tuple(self)\n# Tombola.register(TomboList)  \nTombolist is registered as a virtual subclass of Tombola.\nTombolist extends list.\nTombolist inherits its boolean behavior from list, and that returns True if the\nlist is not empty.\nOur pick calls self.pop, inherited from list, passing a random item index.\nTombolist.load is the same as list.extend.\nloaded delegates to bool.14\nIt’s always possible to call register in this way, and it’s useful to do so when you\nneed to register a class that you do not maintain, but which does fulfill the\ninterface.\nNote that because of the registration, the functions issubclass and isinstance act\nas if TomboList is a subclass of Tombola:\n>>> from tombola import Tombola\n>>> from tombolist import TomboList\n>>> issubclass(TomboList, Tombola)\nTrue\n>>> t = TomboList(range(100))\n>>> isinstance(t, Tombola)\nTrue\n462 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "15 There is a whole section explaining the __mro__ class attribute in “Multiple Inheritance and Method Resolu‐\ntion Order” on page 494. Right now, this quick explanation will do.\nHowever, inheritance is guided by a special class attribute named __mro__—the\nMethod Resolution Order. It basically lists the class and its superclasses in the order\nPython uses to search for methods.15 If you inspect the __mro__ of TomboList, you’ll\nsee that it lists only the “real” superclasses—list and object:\n>>> TomboList.__mro__\n(<class 'tombolist.TomboList'>, <class 'list'>, <class 'object'>)\nTombola is not in Tombolist.__mro__, so Tombolist does not inherit any methods\nfrom Tombola.\nThis concludes our Tombola ABC case study. In the next section, we’ll address how\nthe register ABC function is used in the wild.\nUsage of register in Practice\nIn Example 13-11, we used Tombola.register as a class decorator. Prior to Python\n3.3, register could not be used like that—it had to be called as a plain function after\nthe class definition, as suggested by the comment at the end of Example 13-11. How‐\never, even now, it’s more widely deployed as a function to register classes defined\nelsewhere. For example, in the source code for the collections.abc module, the\nbuilt-in types tuple, str, range, and memoryview are registered as virtual subclasses\nof Sequence, like this:\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\nSeveral other built-in types are registered to ABCs in _collections_abc.py. Those regis‐\ntrations happen only when that module is imported, which is OK because you’ll have\nto import it anyway to get the ABCs. For example, you need to import MutableMap\nping from collections.abc to perform a check like isinstance(my_dict, Mutable\nMapping).\nSubclassing an ABC or registering with an ABC are both explicit ways of making our\nclasses pass issubclass checks—as well as isinstance checks, which also rely on\nissubclass. But some ABCs support structural typing as well. The next section\nexplains.\nGoose Typing \n| \n463",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "16 The concept of type consistency was explained in “Subtype-of versus consistent-with” on page 267.\nStructural Typing with ABCs\nABCs are mostly used with nominal typing. When a class Sub explicitly inherits from\nAnABC, or is registered with AnABC, the name of AnABC is linked to the Sub class—and\nthat’s how at runtime, issubclass(AnABC, Sub) returns True.\nIn contrast, structural typing is about looking at the structure of an object’s public\ninterface to determine its type: an object is consistent-with a type if it implements the\nmethods defined in the type.16 Dynamic and static duck typing are two approaches to\nstructural typing.\nIt turns out that some ABCs also support structural typing. In his essay, “Waterfowl\nand ABCs” on page 443, Alex shows that a class can be recognized as a subclass of an\nABC even without registration. Here is his example again, with an added test using\nissubclass:\n>>> class Struggle:\n...     def __len__(self): return 23\n...\n>>> from collections import abc\n>>> isinstance(Struggle(), abc.Sized)\nTrue\n>>> issubclass(Struggle, abc.Sized)\nTrue\nClass Struggle is considered a subclass of abc.Sized by the issubclass function\n(and, consequently, by isinstance as well) because abc.Sized implements a special\nclass method named __subclasshook__.\nThe __subclasshook__ for Sized checks whether the class argument has an attribute\nnamed __len__. If it does, then it is considered a virtual subclass of Sized. See\nExample 13-12.\nExample 13-12. Definition of Sized from the source code of Lib/_collections_abc.py\nclass Sized(metaclass=ABCMeta):\n    __slots__ = ()\n    @abstractmethod\n    def __len__(self):\n        return 0\n    @classmethod\n    def __subclasshook__(cls, C):\n464 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "if cls is Sized:\n            if any(\"__len__\" in B.__dict__ for B in C.__mro__):  \n                return True  \n        return NotImplemented  \nIf there is an attribute named __len__ in the __dict__ of any class listed in\nC.__mro__ (i.e., C and its superclasses)…\n…return True, signaling that C is a virtual subclass of Sized.\nOtherwise return NotImplemented to let the subclass check proceed.\nIf you are interested in the details of the subclass check, see the\nsource code for the ABCMeta.__subclasscheck__ method in\nPython 3.6: Lib/abc.py. Beware: it has lots of ifs and two recursive\ncalls. In Python 3.7, Ivan Levkivskyi and Inada Naoki rewrote in C\nmost of the logic for the abc module, for better performance. See\nPython issue #31333. The current implementation of ABC\nMeta.__subclasscheck__ simply calls _abc_subclasscheck. The\nrelevant C source code is in cpython/Modules/_abc.c#L605.\nThat’s how __subclasshook__ allows ABCs to support structural typing. You can\nformalize an interface with an ABC, you can make isinstance checks against that\nABC, and still have a completely unrelated class pass an issubclass check because it\nimplements a certain method (or because it does whatever it takes to convince a\n__subclasshook__ to vouch for it).\nIs it a good idea to implement __subclasshook__ in our own ABCs? Probably not.\nAll the implementations of __subclasshook__ I’ve seen in the Python source code\nare in ABCs like Sized that declare just one special method, and they simply check\nfor that special method name. Given their “special” status, you can be pretty sure that\nany method named __len__ does what you expect. But even in the realm of special\nmethods and fundamental ABCs, it can be risky to make such assumptions. For\nexample, mappings implement __len__, __getitem__, and __iter__, but they are\nrightly not considered subtypes of Sequence, because you can’t retrieve items using\ninteger offsets or slices. That’s why the abc.Sequence class does not implement\n__subclasshook__.\nFor ABCs that you and I may write, a __subclasshook__ would be even less depend‐\nable. I am not ready to believe that any class named Spam that implements or inherits\nload, pick, inspect, and loaded is guaranteed to behave as a Tombola. It’s better to\nlet the programmer affirm it by subclassing Spam from Tombola, or registering it with\nGoose Typing \n| \n465",
      "content_length": 2355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "17 OK, double() is not very useful, except as an example. But the Python standard library has many functions\nthat could not be properly annotated before static protocols were added in Python 3.8. I helped fix a couple of\nbugs in typeshed by adding type hints using protocols. For example, the pull request that fixed “Should Mypy\nwarn about potential invalid arguments to max?” leveraged a _SupportsLessThan protocol, which I used to\nenhance the annotations for max, min, sorted, and list.sort.\nTombola.register(Spam). Of course, your __subclasshook__ could also check\nmethod signatures and other features, but I just don’t think it’s worthwhile.\nStatic Protocols\nStatic protocols were introduced in “Static Protocols” on page 286\n(Chapter 8). I considered delaying all coverage of protocols until\nthis chapter, but decided that the initial presentation of type hints\nin functions had to include protocols because duck typing is an\nessential part of Python, and static type checking without protocols\ndoesn’t handle Pythonic APIs very well.\nWe will wrap up this chapter by illustrating static protocols with two simple exam‐\nples, and a discussion of numeric ABCs and protocols. Let’s start by showing how a\nstatic protocol makes it possible to annotate and type check the double() function we\nfirst saw in “Types Are Defined by Supported Operations” on page 260.\nThe Typed double Function\nWhen introducing Python to programmers more used to statically typed languages,\none of my favorite examples is this simple double function:\n>>> def double(x):\n...     return x * 2\n...\n>>> double(1.5)\n3.0\n>>> double('A')\n'AA'\n>>> double([10, 20, 30])\n[10, 20, 30, 10, 20, 30]\n>>> from fractions import Fraction\n>>> double(Fraction(2, 5))\nFraction(4, 5)\nBefore static protocols were introduced, there was no practical way to add type hints\nto double without limiting its possible uses.17\n466 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "Thanks to duck typing, double works even with types from the future, such as the\nenhanced Vector class that we’ll see in “Overloading * for Scalar Multiplication” on\npage 572 (Chapter 16):\n>>> from vector_v7 import Vector\n>>> double(Vector([11.0, 12.0, 13.0]))\nVector([22.0, 24.0, 26.0])\nThe initial implementation of type hints in Python was a nominal type system: the\nname of a type in an annotation had to match the name of the type of the actual argu‐\nments—or the name of one of its superclasses. Since it’s impossible to name all types\nthat implement a protocol by supporting the required operations, duck typing could\nnot be described by type hints before Python 3.8.\nNow, with typing.Protocol we can tell Mypy that double takes an argument x that\nsupports x * 2. Example 13-13 shows how.\nExample 13-13. double_protocol.py: definition of double using a Protocol\nfrom typing import TypeVar, Protocol\nT = TypeVar('T')  \nclass Repeatable(Protocol):\n    def __mul__(self: T, repeat_count: int) -> T: ...  \nRT = TypeVar('RT', bound=Repeatable)  \ndef double(x: RT) -> RT:  \n    return x * 2\nWe’ll use this T in the __mul__ signature.\n__mul__ is the essence of the Repeatable protocol. The self parameter is usually\nnot annotated—its type is assumed to be the class. Here we use T to make sure\nthe result type is the same as the type of self. Also, note that repeat_count is\nlimited to int in this protocol.\nThe RT type variable is bounded by the Repeatable protocol: the type checker\nwill require that the actual type implements Repeatable.\nNow the type checker is able to verify that the x parameter is an object that can\nbe multiplied by an integer, and the return value has the same type as x.\nStatic Protocols \n| \n467",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "This example shows why PEP 544 is titled “Protocols: Structural subtyping (static\nduck typing).” The nominal type of the actual argument x given to double is irrele‐\nvant as long as it quacks—that is, as long as it implements __mul__.\nRuntime Checkable Static Protocols\nIn the Typing Map (Figure 13-1), typing.Protocol appears in the static checking\narea—the bottom half of the diagram. However, when defining a typing.Protocol\nsubclass, you can use the @runtime_checkable decorator to make that protocol sup‐\nport isinstance/issubclass checks at runtime. This works because typing.Proto\ncol is an ABC, therefore it supports the __subclasshook__ we saw in “Structural\nTyping with ABCs” on page 464.\nAs of Python 3.9, the typing module includes seven ready-to-use protocols that are\nruntime checkable. Here are two of them, quoted directly from the typing documen‐\ntation:\nclass typing.SupportsComplex\nAn ABC with one abstract method, __complex__.\nclass typing.SupportsFloat\nAn ABC with one abstract method, __float__.\nThese protocols are designed to check numeric types for “convertibility”: if an object\no implements __complex__, then you should be able to get a complex by invoking\ncomplex(o)—because the __complex__ special method exists to support the\ncomplex() built-in function.\nExample 13-14 shows the source code for the typing.SupportsComplex protocol.\nExample 13-14. typing.SupportsComplex protocol source code\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n    __slots__ = ()\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n468 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "18 The __slots__ attribute is irrelevant to the current discussion—it’s an optimization we covered in “Saving\nMemory with __slots__” on page 384.\nThe key is the __complex__ abstract method.18 During static type checking, an object\nwill be considered consistent-with the SupportsComplex protocol if it implements a\n__complex__ method that takes only self and returns a complex.\nThanks to the @runtime_checkable class decorator applied to SupportsComplex, that\nprotocol can also be used with isinstance checks in Example 13-15.\nExample 13-15. Using SupportsComplex at runtime\n>>> from typing import SupportsComplex\n>>> import numpy as np\n>>> c64 = np.complex64(3+4j)  \n>>> isinstance(c64, complex)   \nFalse\n>>> isinstance(c64, SupportsComplex)  \nTrue\n>>> c = complex(c64)  \n>>> c\n(3+4j)\n>>> isinstance(c, SupportsComplex) \nFalse\n>>> complex(c)\n(3+4j)\ncomplex64 is one of five complex number types provided by NumPy.\nNone of the NumPy complex types subclass the built-in complex.\nBut NumPy’s complex types implement __complex__, so they comply with the\nSupportsComplex protocol.\nTherefore, you can create built-in complex objects from them.\nSadly, the complex built-in type does not implement __complex__, although\ncomplex(c) works fine if c is a complex.\nAs a result of that last point, if you want to test whether an object c is a complex or\nSupportsComplex, you can provide a tuple of types as the second argument to\nisinstance, like this:\nisinstance(c, (complex, SupportsComplex))\nStatic Protocols \n| \n469",
      "content_length": 1506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "An alternative would be to use the Complex ABC, defined in the numbers module.\nThe built-in complex type and the NumPy complex64 and complex128 types are all\nregistered as virtual subclasses of numbers.Complex, therefore this works:\n>>> import numbers\n>>> isinstance(c, numbers.Complex)\nTrue\n>>> isinstance(c64, numbers.Complex)\nTrue\nI recommended using the numbers ABCs in the first edition of Fluent Python, but\nnow that’s no longer good advice, because those ABCs are not recognized by the\nstatic type checkers, as we’ll see in “The numbers ABCs and Numeric Protocols” on\npage 478.\nIn this section I wanted to demonstrate that a runtime checkable protocol works with\nisinstance, but it turns out this is example not a particularly good use case of isin\nstance, as the sidebar “Duck Typing Is Your Friend” on page 470 explains.\nIf you’re using an external type checker, there is one advantage of\nexplict isinstance checks: when you write an if statement where\nthe condition is isinstance(o, MyType), then Mypy can infer that\ninside the if block, the type of the o object is consistent-with\nMyType.\nDuck Typing Is Your Friend\nVery often at runtime, duck typing is the best approach for type checking: instead of\ncalling isinstance or hasattr, just try the operations you need to do on the object,\nand handle exceptions as needed. Here is a concrete example.\nContinuing the previous discussion—given an object o that I need to use as a com‐\nplex number, this would be one approach:\nif isinstance(o, (complex, SupportsComplex)):\n    # do something that requires `o` to be convertible to complex\nelse:\n    raise TypeError('o must be convertible to complex')\nThe goose typing approach would be to use the numbers.Complex ABC:\nif isinstance(o, numbers.Complex):\n    # do something with `o`, an instance of `Complex`\nelse:\n    raise TypeError('o must be an instance of Complex')\nHowever, I prefer to leverage duck typing and do this using the EAFP principle—it’s\neasier to ask for forgiveness than permission:\n470 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "try:\n    c = complex(o)\nexcept TypeError as exc:\n    raise TypeError('o must be convertible to complex') from exc\nAnd, if all you’re going to do is raise a TypeError anyway, then I’d omit the try/\nexcept/raise statements and just write this:\nc = complex(o)\nIn this last case, if o is not an acceptable type, Python will raise an exception with a\nvery clear message. For example, this is what I get if o is a tuple:\nTypeError: complex() first argument must be a string or a number, not 'tuple'\nI find the duck typing approach much better in this case.\nNow that we’ve seen how to use static protocols at runtime with preexisting types like\ncomplex and numpy.complex64, we need to discuss the limitations of runtime checka‐\nble protocols.\nLimitations of Runtime Protocol Checks\nWe’ve seen that type hints are generally ignored at runtime, and this also affects the\nuse of isinstance or issubclass checks against static protocols.\nFor example, any class with a __float__ method is considered—at runtime—a vir‐\ntual subclass of SupportsFloat, even if the __float__ method does not return a\nfloat.\nCheck out this console session:\n>>> import sys\n>>> sys.version\n'3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02) \\n[Clang 6.0 (clang-600.0.57)]'\n>>> c = 3+4j\n>>> c.__float__\n<method-wrapper '__float__' of complex object at 0x10a16c590>\n>>> c.__float__()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can't convert complex to float\nIn Python 3.9, the complex type does have a __float__ method, but it exists only to\nraise a TypeError with an explicit error message. If that __float__ method had\nannotations, the return type would be NoReturn—which we saw in “NoReturn” on\npage 294.\nStatic Protocols \n| \n471",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "19 Thanks to Ivan Levkivskyi, coauthor of PEP 544 (on Protocols), for pointing out that type checking is not just\na matter of checking whether the type of x is T: it’s about determining that the type of x is consistent-with T,\nwhich may be expensive. It’s no wonder that Mypy takes a few seconds to type check even short Python\nscripts.\nBut type hinting complex.__float__ on typeshed would not solve this problem\nbecause Python’s runtime generally ignores type hints—and can’t access the typeshed\nstub files anyway.\nContinuing from the previous Python 3.9 session:\n>>> from typing import SupportsFloat\n>>> c = 3+4j\n>>> isinstance(c, SupportsFloat)\nTrue\n>>> issubclass(complex, SupportsFloat)\nTrue\nSo we have misleading results: the runtime checks against SupportsFloat suggest\nthat you can convert a complex to float, but in fact that raises a type error.\nThe specific isssue with the complex type is fixed in Python\n3.10.0b4 with the removal of the complex.__float__ method.\nBut the overall issue remains: isinstance/issubclass checks only\nlook at the presence or absence of methods, without checking their\nsignatures, much less their type annotations. And this is not about\nto change, because such type checks at runtime would have an\nunacceptable performance cost.19\nNow let’s see how to implement a static protocol in a user-defined class.\nSupporting a Static Protocol\nRecall the Vector2d class we built in Chapter 11. Given that a complex number and a\nVector2d instance both consist of a pair of floats, it makes sense to support conver‐\nsion from Vector2d to complex.\nExample 13-16 shows the implementation of the __complex__ method to enhance\nthe last version of Vector2d we saw in Example 11-11. For completeness, we can sup‐\nport the inverse operation with a fromcomplex class method to build a Vector2d from\na complex.\n472 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "Example 13-16. vector2d_v4.py: methods for converting to and from complex\n    def __complex__(self):\n        return complex(self.x, self.y)\n    @classmethod\n    def fromcomplex(cls, datum):\n        return cls(datum.real, datum.imag)  \nThis assumes that datum has .real and .imag attributes. We’ll see a better imple‐\nmentation in Example 13-17.\nGiven the preceding code, and the __abs__ method the Vector2d already had in\nExample 11-11, we get these features:\n>>> from typing import SupportsComplex, SupportsAbs\n>>> from vector2d_v4 import Vector2d\n>>> v = Vector2d(3, 4)\n>>> isinstance(v, SupportsComplex)\nTrue\n>>> isinstance(v, SupportsAbs)\nTrue\n>>> complex(v)\n(3+4j)\n>>> abs(v)\n5.0\n>>> Vector2d.fromcomplex(3+4j)\nVector2d(3.0, 4.0)\nFor runtime type checking, Example 13-16 is fine, but for better static coverage and\nerror reporting with Mypy, the __abs__, __complex__, and fromcomplex methods\nshould get type hints, as shown in Example 13-17.\nExample 13-17. vector2d_v5.py: adding annotations to the methods under study\n    def __abs__(self) -> float:  \n        return math.hypot(self.x, self.y)\n    def __complex__(self) -> complex:  \n        return complex(self.x, self.y)\n    @classmethod\n    def fromcomplex(cls, datum: SupportsComplex) -> Vector2d:  \n        c = complex(datum)  \n        return cls(c.real, c.imag)\nStatic Protocols \n| \n473",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "20 Read the Python Steering Council decision on python-dev.\nThe float return annotation is needed, otherwise Mypy infers Any, and doesn’t\ncheck the body of the method.\nEven without the annotation, Mypy was able to infer that this returns a complex.\nThe annotation prevents a warning, depending on your Mypy configuration.\nHere SupportsComplex ensures the datum is convertible.\nThis explicit conversion is necessary, because the SupportsComplex type does not\ndeclare .real and .imag attributes, used in the next line. For example, Vector2d\ndoesn’t have those attributes, but implements __complex__.\nThe return type of fromcomplex can be Vector2d if from __future__ import anno\ntations appears at the top of the module. That import causes type hints to be stored\nas strings, without being evaluated at import time, when function definitions are\nevaluated. Without the __future__ import of annotations, Vector2d is an invalid\nreference at this point (the class is not fully defined yet) and should be written as a\nstring: 'Vector2d', as if it were a forward reference. This __future__ import was\nintroduced in PEP 563—Postponed Evaluation of Annotations, implemented in\nPython 3.7. That behavior was scheduled to become default in 3.10, but the change\nwas delayed to a later version.20 When that happens, the import will be redundant,\nbut harmless.\nNext, let’s see how to create—and later, extend—a new static protocol.\nDesigning a Static Protocol\nWhile studying goose typing, we saw the Tombola ABC in “Defining and Using an\nABC” on page 451. Here we’ll see how to define a similar interface using a static\nprotocol.\nThe Tombola ABC specifies two methods: pick and load. We could define a static\nprotocol with these two methods as well, but I learned from the Go community that\nsingle-method protocols make static duck typing more useful and flexible. The Go\nstandard library has several interfaces like Reader, an interface for I/O that requires\njust a read method. After a while, if you realize a more complete protocol is required,\nyou can combine two or more protocols to define a new one.\nUsing a container that picks items at random may or may not require reloading the\ncontainer, but it certainly needs a method to do the actual pick, so that’s the method I\n474 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "will choose for the minimal RandomPicker protocol. The code for that protocol is in\nExample 13-18, and its use is demonstrated by tests in Example 13-19.\nExample 13-18. randompick.py: definition of RandomPicker\nfrom typing import Protocol, runtime_checkable, Any\n@runtime_checkable\nclass RandomPicker(Protocol):\n    def pick(self) -> Any: ...\nThe pick method returns Any. In “Implementing a Generic Static\nProtocol” on page 552, we will see how to make RandomPicker a\ngeneric type with a parameter to let users of the protocol specify\nthe return type of the pick method.\nExample 13-19. randompick_test.py: RandomPicker in use\nimport random\nfrom typing import Any, Iterable, TYPE_CHECKING\nfrom randompick import RandomPicker  \nclass SimplePicker:  \n    def __init__(self, items: Iterable) -> None:\n        self._items = list(items)\n        random.shuffle(self._items)\n    def pick(self) -> Any:  \n        return self._items.pop()\ndef test_isinstance() -> None:  \n    popper: RandomPicker = SimplePicker([1])  \n    assert isinstance(popper, RandomPicker)  \ndef test_item_type() -> None:  \n    items = [1, 2]\n    popper = SimplePicker(items)\n    item = popper.pick()\n    assert item in items\n    if TYPE_CHECKING:\n        reveal_type(item)  \n    assert isinstance(item, int)\nStatic Protocols \n| \n475",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "It’s not necessary to import the static protocol to define a class that implements\nit. Here I imported RandomPicker only to use it on test_isinstance later.\nSimplePicker implements RandomPicker—but it does not subclass it. This is\nstatic duck typing in action.\nAny is the default return type, so this annotation is not strictly necessary, but it\ndoes make it more clear that we are implementing the RandomPicker protocol as\ndefined in Example 13-18.\nDon’t forget to add -> None hints to your tests if you want Mypy to look at them.\nI added a type hint for the popper variable to show that Mypy understands that\nSimplePicker is consistent-with.\nThis test proves that an instance of SimplePicker is also an instance of Random\nPicker. This works because of the @runtime_checkable decorator applied to\nRandomPicker, and because SimplePicker has a pick method as required.\nThis test invokes the pick method from a SimplePicker, verifies that it returns\none of the items given to SimplePicker, and then does static and runtime checks\non the returned item.\nThis line generates a note in the Mypy output.\nAs we saw in Example 8-22, reveal_type is a “magic” function recognized by Mypy.\nThat’s why it is not imported and we can only call it inside if blocks protected by\ntyping.TYPE_CHECKING, which is only True in the eyes of a static type checker, but is\nFalse at runtime.\nBoth tests in Example 13-19 pass. Mypy does not see any errors in that code either,\nand shows the result of the reveal_type on the item returned by pick:\n$ mypy randompick_test.py\nrandompick_test.py:24: note: Revealed type is 'Any'\nHaving created our first protocol, let’s study some advice on the matter.\nBest Practices for Protocol Design\nAfter 10 years of experience with static duck typing in Go, it is clear that narrow pro‐\ntocols are more useful—often such protocols have a single method, rarely more than\na couple of methods. Martin Fowler wrote a post defining role interface, a useful idea\nto keep in mind when designing protocols.\n476 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "21 Every method is callable, so this guideline doesn’t say much. Perhaps “provide one or two methods”? Any‐\nway, it’s a guideline, not a strict rule.\nAlso, sometimes you see a protocol defined near the function that uses it—that is,\ndefined in “client code” instead of being defined in a library. This is makes it easy to\ncreate new types to call that function, which is good for extensibility and testing with\nmocks.\nThe practices of narrow protocols and client-code protocols both avoid unnecessary\ntight coupling, in line with the Interface Segregation Principle, which we can summa‐\nrize as “Clients should not be forced to depend upon interfaces that they do not use.”\nThe page “Contributing to typeshed” recommends this naming convention for static\nprotocols (the following three points are quoted verbatim):\n• Use plain names for protocols that represent a clear concept (e.g., Iterator,\nContainer).\n• Use SupportsX for protocols that provide callable methods (e.g., SupportsInt,\nSupportsRead, SupportsReadSeek).21\n• Use HasX for protocols that have readable and/or writable attributes or getter/\nsetter methods (e.g., HasItems, HasFileno).\nThe Go standard library has a naming convention that I like: for single method pro‐\ntocols, if the method name is a verb, append “-er” or “-or” to make it a noun. For\nexample, instead of SupportsRead, have Reader. More examples include Formatter,\nAnimator, and Scanner. For inspiration, see “Go (Golang) Standard Library Inter‐\nfaces (Selected)” by Asuka Kenji.\nOne good reason to create minimalistic protocols is the ability to extend them later, if\nneeded. We’ll now see that it’s not hard to create a derived protocol with an addi‐\ntional method.\nExtending a Protocol\nAs I mentioned at the start of the previous section, Go developers advocate to err on\nthe side of minimalism when defining interfaces—their name for static protocols.\nMany of the most widely used Go interfaces have a single method.\nWhen practice reveals that a protocol with more methods is useful, instead of adding\nmethods to the original protocol, it’s better to derive a new protocol from it. Extend‐\ning a static protocol in Python has a few caveats, as Example 13-20 shows.\nStatic Protocols \n| \n477",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "22 For details and rationale, please see the section about @runtime_checkable in PEP 544—Protocols: Structural\nsubtyping (static duck typing).\n23 Again, please read “Merging and extending protocols” in PEP 544 for details and rationale.\nExample 13-20. randompickload.py: extending RandomPicker\nfrom typing import Protocol, runtime_checkable\nfrom randompick import RandomPicker\n@runtime_checkable  \nclass LoadableRandomPicker(RandomPicker, Protocol):  \n    def load(self, Iterable) -> None: ...  \nIf you want the derived protocol to be runtime checkable, you must apply the\ndecorator again—its behavior is not inherited.22\nEvery protocol must explicitly name typing.Protocol as one of its base classes\nin addition to the protocol we are extending. This is different from the way\ninheritance works in Python.23\nBack to “regular” object-oriented programming: we only need to declare the\nmethod that is new in this derived protocol. The pick method declaration is\ninherited from RandomPicker.\nThis concludes the final example of defining and using a static protocol in this\nchapter.\nTo wrap the chapter, we’ll go over numeric ABCs and their possible replacement with\nnumeric protocols.\nThe numbers ABCs and Numeric Protocols\nAs we saw in “The fall of the numeric tower” on page 279, the ABCs in the numbers\npackage of the standard library work fine for runtime type checking.\nIf you need to check for an integer, you can use isinstance(x, numbers.Integral)\nto accept int, bool (which subclasses int) or other integer types that are provided by\nexternal libraries that register their types as virtual subclasses of the numbers ABCs.\nFor example, NumPy has 21 integer types—as well as several variations of floating-\npoint types registered as numbers.Real, and complex numbers with various bit\nwidths registered as numbers.Complex.\n478 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "24 See Issue #41974—Remove complex.__float__, complex.__floordiv__, etc.\nSomewhat surprisingly, decimal.Decimal is not registered as a vir‐\ntual subclass of numbers.Real. The reason is that, if you need the\nprecision of Decimal in your program, then you want to be pro‐\ntected from accidental mixing of decimals with floating-point\nnumbers that are less precise.\nSadly, the numeric tower was not designed for static type checking. The root ABC—\nnumbers.Number—has no methods, so if you declare x: Number, Mypy will not let\nyou do arithmetic or call any methods on x.\nIf the numbers ABCs are not supported, what are the options?\nA good place to look for typing solutions is the typeshed project. As part of the\nPython standard library, the statistics module has a corresponding statistics.pyi\nstub file with type hints for on typeshed. There you’ll find the following definitions,\nwhich are used to annotate several functions:\n_Number = Union[float, Decimal, Fraction]\n_NumberT = TypeVar('_NumberT', float, Decimal, Fraction)\nThat approach is correct, but limited. It does not support numeric types outside of\nthe standard library, which the numbers ABCs do support at runtime—when the\nnumeric types are registered as virtual subclasses.\nThe current trend is to recommend the numeric protocols provided by the typing\nmodule, which we discussed in “Runtime Checkable Static Protocols” on page 468.\nUnfortunately, at runtime, the numeric protocols may let you down. As mentioned in\n“Limitations of Runtime Protocol Checks” on page 471, the complex type in Python\n3.9 implements __float__, but the method exists only to raise TypeError with an\nexplicit message: “can’t convert complex to float.” It implements __int__ as well, for\nthe same reason. The presence of those methods makes isinstance return mislead‐\ning results in Python 3.9. In Python 3.10, the methods of complex that uncondition‐\nally raised TypeError were removed.24\nOn the other hand, NumPy’s complex types implement __float__ and __int__\nmethods that work, only issuing a warning when each of them is used for the first\ntime:\n>>> import numpy as np\n>>> cd = np.cdouble(3+4j)\n>>> cd\n(3+4j)\n>>> float(cd)\nStatic Protocols \n| \n479",
      "content_length": 2191,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "25 I did not test all the other float and integer variants NumPy offers.\n26 The NumPy number types are all registered against the appropriate numbers ABCs, which Mypy ignores.\n27 That’s a well-meaning lie on the part of typeshed: as of Python 3.9, the built-in complex type does not actually\nhave a __complex__ method.\n<stdin>:1: ComplexWarning: Casting complex values to real\ndiscards the imaginary part\n3.0\nThe opposite problem also happens: built-ins complex, float, and int, and also\nnumpy.float16 and numpy.uint8, don’t have a __complex__ method, so isin\nstance(x, SupportsComplex) returns False for them.25 The NumPy complex types,\nsuch as np.complex64, do implement __complex__ to convert to a built-in complex.\nHowever, in practice, the complex() built-in constructor handles instances of all\nthese types with no errors or warnings:\n>>> import numpy as np\n>>> from typing import SupportsComplex\n>>> sample = [1+0j, np.complex64(1+0j), 1.0, np.float16(1.0), 1, np.uint8(1)]\n>>> [isinstance(x, SupportsComplex) for x in sample]\n[False, True, False, False, False, False]\n>>> [complex(x) for x in sample]\n[(1+0j), (1+0j), (1+0j), (1+0j), (1+0j), (1+0j)]\nThis shows that isinstance checks against SupportsComplex suggest that those con‐\nversions to complex would fail, but they all succeed. In the typing-sig mailing list,\nGuido van Rossum pointed out that the built-in complex accepts a single argument,\nand that’s why those conversions work.\nOn the other hand, Mypy accepts arguments of all those six types in a call to a\nto_complex() function defined like this:\ndef to_complex(n: SupportsComplex) -> complex:\n    return complex(n)\nAs I write this, NumPy has no type hints, so its number types are all Any.26 On the\nother hand, Mypy is somehow “aware” that the built-in int and float can be con‐\nverted to complex, even though on typeshed only the built-in complex class has a\n__complex__ method.27\nIn conclusion, although numeric types should not be hard to type check, the current\nsituation is this: the type hints PEP 484 eschews the numeric tower and implicitly\nrecommends that type checkers hardcode the subtype relationships among built-in\ncomplex, float, and int. Mypy does that, and it also pragmatically accepts that int\nand float are consistent-with SupportsComplex, even though they don’t implement\n__complex__.\n480 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "I only found unexpected results when using isinstance checks\nwith numeric Supports* protocols while experimenting with con‐\nversions to or from complex. If you don’t use complex numbers,\nyou can rely on those protocols instead of the numbers ABCs.\nThe main takeaways for this section are:\n• The numbers ABCs are fine for runtime type checking, but unsuitable for static\ntyping.\n• The numeric static protocols SupportsComplex, SupportsFloat, etc. work well\nfor static typing, but are unreliable for runtime type checking when complex\nnumbers are involved.\nWe are now ready for a quick review of what we saw in this chapter.\nChapter Summary\nThe Typing Map (Figure 13-1) is the key to making sense of this chapter. After a brief\nintroduction to the four approaches to typing, we contrasted dynamic and static pro‐\ntocols, which respectively support duck typing and static duck typing. Both kinds of\nprotocols share the essential characteristic that a class is never required to explicitly\ndeclare support for any specific protocol. A class supports a protocol simply by\nimplementing the necessary methods.\nThe next major section was “Programming Ducks” on page 435, where we explored\nthe lengths to which the Python interpreter goes to make the sequence and iterable\ndynamic protocols work, including partial implementations of both. We then saw\nhow a class can be made to implement a protocol at runtime through the addition of\nextra methods via monkey patching. The duck typing section ended with hints for\ndefensive programming, including detection of structural types without explicit isin\nstance or hasattr checks using try/except and failing fast.\nAfter Alex Martelli introduced goose typing in “Waterfowl and ABCs” on page 443,\nwe saw how to subclass existing ABCs, surveyed important ABCs in the standard\nlibrary, and created an ABC from scratch, which we then implemented by traditional\nsubclassing and by registration. To close this section, we saw how the __subclass\nhook__ special method enables ABCs to support structural typing by recognizing\nunrelated classes that provide methods fulfilling the interface defined in the ABC.\nThe last major section was “Static Protocols” on page 466, where we resumed cover‐\nage of static duck typing, which started in Chapter 8, in “Static Protocols” on page\n286. We saw how the @runtime_checkable decorator also leverages __subclass\nhook__ to support structural typing at runtime—even though the best use of static\nChapter Summary \n| \n481",
      "content_length": 2486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "28 Thanks to tech reviewer Jürgen Gmach for recommending the “Interfaces and Protocols” post.\nprotocols is with static type checkers, which can take into account type hints to make\nstructural typing more reliable. Next we talked about the design and coding of a static\nprotocol and how to extend it. The chapter ended with “The numbers ABCs and\nNumeric Protocols” on page 478, which tells the sad story of the derelict state of the\nnumeric tower and a few existing shortcomings of the proposed alternative: the\nnumeric static protocols such as SupportsFloat and others added to the typing\nmodule in Python 3.8.\nThe main message of this chapter is that we have four complementary ways of pro‐\ngramming with interfaces in modern Python, each with different advantages and\ndrawbacks. You are likely to find suitable use cases for each typing scheme in any\nmodern Python codebase of significant size. Rejecting any one of these approaches\nwill make your work as a Python programmer harder than it needs to be.\nHaving said that, Python achieved widespread popularity while supporting only duck\ntyping. Other popular languages such as JavaScript, PHP, and Ruby, as well as Lisp,\nSmalltalk, Erlang, and Clojure—not popular but very influential—are all languages\nthat had and still have tremendous impact by leveraging the power and simplicity of\nduck typing.\nFurther Reading\nFor a quick look at typing pros and cons, as well as the importance of typing.Proto\ncol for the health of statically checked codebases, I highly recommend Glyph Lefko‐\nwitz’s post “I Want A New Duck: typing.Protocol and the future of duck typing”. I\nalso learned a lot from his post “Interfaces and Protocols”, comparing typing.Proto\ncol and zope.interface—an earlier mechanism for defining interfaces in loosely\ncoupled plug-in systems, used by the Plone CMS, the Pyramid web framework, and\nthe Twisted asynchronous programming framework, a project founded by Glyph.28\nGreat books about Python have—almost by definition—great coverage of duck typ‐\ning. Two of my favorite Python books had updates released after the first edition of\nFluent Python: The Quick Python Book, 3rd ed., (Manning), by Naomi Ceder; and\nPython in a Nutshell, 3rd ed., by Alex Martelli, Anna Ravenscroft, and Steve Holden\n(O’Reilly).\nFor a discussion of the pros and cons of dynamic typing, see Guido van Rossum’s\ninterview with Bill Venners in “Contracts in Python: A Conversation with Guido van\nRossum, Part IV”. An insightful and balanced take on this debate is Martin Fowler’s\npost “Dynamic Typing”. He also wrote “Role Interface”, which I mentioned in “Best\nPractices for Protocol Design” on page 476. Although it is not about duck typing, that\n482 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "post is highly relevant for Python protocol design, as he contrasts narrow role inter‐\nfaces with the broader public interfaces of classes in general.\nThe Mypy documentation is often the best source of information for anything related\nto static typing in Python, including static duck typing, addressed in their “Protocols\nand structural subtyping” chapter.\nThe remaining references are all about goose typing. Beazley and Jones’s Python\nCookbook, 3rd ed. (O’Reilly) has a section about defining an ABC (Recipe 8.12). The\nbook was written before Python 3.4, so they don’t use the now preferred syntax of\ndeclaring ABCs by subclassing from abc.ABC (instead, they use the metaclass key‐\nword, which we’ll only really need in Chapter 24). Apart from this small detail, the\nrecipe covers the major ABC features very well.\nThe Python Standard Library by Example by Doug Hellmann (Addison-Wesley), has\na chapter about the abc module. It’s also available on the web in Doug’s excellent\nPyMOTW—Python Module of the Week. Hellmann also uses the old style of ABC\ndeclaration: PluginBase(metaclass=abc.ABCMeta) instead of the simpler Plugin\nBase(abc.ABC) available since Python 3.4.\nWhen using ABCs, multiple inheritance is not only common but practically inevita‐\nble, because each of the fundamental collection ABCs—Sequence, Mapping, and Set\n—extends Collection, which in turn extends multiple ABCs (see Figure 13-4).\nTherefore, Chapter 14 is an important follow-up to this one.\nPEP 3119—Introducing Abstract Base Classes gives the rationale for ABCs. PEP 3141\n—A Type Hierarchy for Numbers presents the ABCs of the numbers module, but the\ndiscussion in the Mypy issue #3186 “int is not a Number?” includes some arguments\nabout why the numeric tower is unsuitable for static type checking. Alex Waygood\nwrote a comprehensive answer on StackOverflow, discussing ways to annotate\nnumeric types. I’ll keep watching Mypy issue #3186 for the next chapters of this saga,\nhoping for a happy ending that will make static typing and goose typing compatible\n—as they should be.\nFurther Reading \n| \n483",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "Soapbox\nThe MVP Journey of Python Static Typing\nI work for Thoughtworks, a worldwide leader in Agile software development. At\nThoughtworks, we often recommend that our clients should aim to create and deploy\nMVPs: minimal viable products: “a simple version of a product that is given to users\nin order to validate the key business assumptions,” as defined by my colleague Paulo\nCaroli in “Lean Inception”, a post in Martin Fowler’s collective blog.\nGuido van Rossum and the other core developers who designed and implemented\nstatic typing have followed an MVP strategy since 2006. First, PEP 3107—Function\nAnnotations was implemented in Python 3.0 with very limited semantics: just syntax\nto attach annotations to function parameters and returns. This was done explicitly to\nallow for experimentation and collect feedback—key benefits of an MVP.\nEight years later, PEP 484—Type Hints was proposed and approved. Its implementa‐\ntion in Python 3.5 required no changes in the language or standard library—except\nfor adding the typing module, on which no other part of the standard library depen‐\nded. PEP 484 supported only nominal types with generics—similar to Java—but with\nthe actual static checking done by external tools. Important features were missing,\nlike variable annotations, generic built-in types, and protocols. Despite those limita‐\ntions, this typing MVP was valuable enough to attract investment and adoption by\ncompanies with very large Python codebases, like Dropbox, Google, and Facebook; as\nwell as support from professional IDEs, like PyCharm, Wing, and VS Code.\nPEP 526—Syntax for Variable Annotations was the first evolutionary step that\nrequired changes to the interpreter in Python 3.6. More changes to the Python 3.7\ninterpreter were made to support PEP 563—Postponed Evaluation of Annotations\nand PEP 560—Core support for typing module and generic types, which allowed\nbuilt-in and standard library collections to accept generic type hints out of the box in\nPython 3.9, thanks to PEP 585—Type Hinting Generics In Standard Collections.\nDuring those years, some Python users—including me—were underwhelmed by the\ntyping support. After I learned Go, the lack of static duck typing in Python was\nincomprehensible, in a language where duck typing had always been a core strength.\nBut that is the nature of MVPs: they may not satisfy all potential users, but they can\nbe implemented with less effort, and guide further development with feedback from\nactual usage in the field.\nIf there is one thing we all learned from Python 3, it’s that incremental progress is\nsafer than big-bang releases. I am glad we did not have to wait for Python 4—if it ever\ncomes—to make Python more attractive to large enterprises, where the benefits of\nstatic typing outweigh the added complexity.\n484 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "Typing Approaches in Popular Languages\nFigure 13-8 is a variation of the Typing Map (Figure 13-1) with the names of a few\npopular languages that support each of the typing approaches.\nFigure 13-8. Four approaches to type checking and some languages that support them.\nTypeScript and Python ≥ 3.8 are the only languages in my small and arbitrary sample\nthat support all four approaches.\nGo is clearly a statically typed language in the Pascal tradition, but it pioneered static\nduck typing—at least among languages that are widely used today. I also put Go in\nthe goose typing quadrant because of its type assertions, which allow checking and\nadapting to different types at runtime.\nIf I had to draw a similar diagram in the year 2000, only the duck typing and the static\ntyping quadrants would have languages in them. I am not aware of languages that\nsupported static duck typing or goose typing 20 years ago. The fact that each of the\nfour quadrants has at least three popular languages suggests that a lot of people see\nvalue in each of the four approaches to typing.\nFurther Reading \n| \n485",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "Monkey Patching\nMonkey patching has a bad reputation. If abused, it can lead to systems that are hard\nto understand and maintain. The patch is usually tightly coupled with its target, mak‐\ning it brittle. Another problem is that two libraries that apply monkey patches may\nstep on each other’s toes, with the second library to run destroying patches of the\nfirst.\nBut monkey patching can also be useful, for example, to make a class implement a\nprotocol at runtime. The Adapter design pattern solves the same problem by imple‐\nmenting a whole new class.\nIt’s easy to monkey patch Python code, but there are limitations. Unlike Ruby and\nJavaScript, Python does not let you monkey patch the built-in types. I actually con‐\nsider this an advantage, because you can be certain that a str object will always have\nthose same methods. This limitation reduces the chance that external libraries apply\nconflicting patches.\nMetaphors and Idioms in Interfaces\nA metaphor fosters understanding by making constraints and affordances clear.\nThat’s the value of the words “stack” and “queue” in describing those fundamental\ndata structures: they make clear which operations are allowed, i.e., how items can be\nadded or removed. On the other hand, Alan Cooper et al. write in About Face, the\nEssentials of Interaction Design, 4th ed. (Wiley):\nStrict adherence to metaphors ties interfaces unnecessarily tightly to the workings of\nthe physical world.\nHe’s referring to user interfaces, but the admonition applies to APIs as well. But\nCooper does grant that when a “truly appropriate” metaphor “falls on our lap,” we\ncan use it (he writes “falls on our lap” because it’s so hard to find fitting metaphors\nthat you should not spend time actively looking for them). I believe the bingo\nmachine imagery I used in this chapter is appropriate and I stand by it.\nAbout Face is by far the best book about UI design I’ve read—and I’ve read a few.\nLetting go of metaphors as a design paradigm, and replacing it with “idiomatic inter‐\nfaces” was the most valuable thing I learned from Cooper’s work.\nIn About Face, Cooper does not deal with APIs, but the more I think about his ideas,\nthe more I see how they apply to Python. The fundamental protocols of the language\nare what Cooper calls “idioms.” Once we learn what a “sequence” is, we can apply\nthat knowledge in different contexts. This is a main theme of Fluent Python: high‐\nlighting the fundamental idioms of the language, so your code is concise, effective,\nand readable—for a fluent Pythonista.\n486 \n| \nChapter 13: Interfaces, Protocols, and ABCs",
      "content_length": 2578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "1 Alan Kay, “The Early History of Smalltalk,” in SIGPLAN Not. 28, 3 (March 1993), 69–95. Also available\nonline. Thanks to my friend Christiano Anderson, who shared this reference as I was writing this chapter.\nCHAPTER 14\nInheritance: For Better or for Worse\n[...] we needed a better theory about inheritance entirely (and still do). For example,\ninheritance and instancing (which is a kind of inheritance) muddles both pragmatics\n(such as factoring code to save space) and semantics (used for way too many tasks such\nas: specialization, generalization, speciation, etc.).\n—Alan Kay, “The Early History of Smalltalk”1\nThis chapter is about inheritance and subclassing. I will assume a basic understand‐\ning of these concepts, which you may know from reading The Python Tutorial or\nfrom experience with another mainstream object-oriented language, such as Java, C#,\nor C++. Here we’ll focus on four characteristics of Python:\n• The super() function\n• The pitfalls of subclassing from built-in types\n• Multiple inheritance and method resolution order\n• Mixin classes\nMultiple inheritance is the ability of a class to have more than one base class. C++\nsupports it; Java and C# don’t. Many consider multiple inheritance more trouble than\nit’s worth. It was deliberately left out of Java after its perceived abuse in early C++\ncodebases.\nThis chapter introduces multiple inheritance for those who have never used it, and\nprovides some guidance on how to cope with single or multiple inheritance if you\nmust use it.\n487",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "As of 2021, there is a significant backlash against overuse of inheritance in general—\nnot only multiple inheritance—because superclasses and subclasses are tightly cou‐\npled. Tight coupling means that changes to one part of the program may have unex‐\npected and far-reaching effects in other parts, making systems brittle and hard to\nunderstand.\nHowever, we have to maintain existing systems designed with complex class hierar‐\nchies, or use frameworks that force us to use inheritance—even multiple inheritance\nsometimes.\nI will illustrate practical uses of multiple inheritance with the standard library, the\nDjango web framework, and the Tkinter GUI toolkit.\nWhat’s New in This Chapter\nThere are no new Python features related to the subject of this chapter, but I heavily\nedited it based on feedback from technical reviewers of the second edition, especially\nLeonardo Rochael and Caleb Hattingh.\nI wrote a new opening section focusing on the super() built-in function, and\nchanged the examples in “Multiple Inheritance and Method Resolution Order” on\npage 494 for a deeper exploration of how super() works to support cooperative\nmultiple inheritance.\n“Mixin Classes” on page 500 is also new. “Multiple Inheritance in the Real World” on\npage 502 was reorganized and covers simpler mixin examples from the standard\nlibrary, before the complex Django and the complicated Tkinter hierarchies.\nAs the chapter title suggests, the caveats of inheritance have always been one of the\nmain themes of this chapter. But more and more developers consider it so problem‐\natic that I’ve added a couple of paragraphs about avoiding inheritance to the end of\n“Chapter Summary” on page 514 and “Further Reading” on page 515.\nWe’ll start with an overview of the mysterious super() function.\nThe super() Function\nConsistent use the of the super() built-in function is essential for maintainable\nobject-oriented Python programs.\nWhen a subclass overrides a method of a superclass, the overriding method usually\nneeds to call the corresponding method of the superclass. Here’s the recommended\n488 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "2 I only changed the docstring in the example, because the original is misleading. It says: “Store items in the\norder the keys were last added,” but that is not what the clearly named LastUpdatedOrderedDict does.\nway to do it, from an example in the collections module documentation, section\n“OrderedDict Examples and Recipes”:2\nclass LastUpdatedOrderedDict(OrderedDict):\n    \"\"\"Store items in the order they were last updated\"\"\"\n    def __setitem__(self, key, value):\n        super().__setitem__(key, value)\n        self.move_to_end(key)\nTo do its job, LastUpdatedOrderedDict overrides __setitem__ to:\n1. Use super().__setitem__ to call that method on the superclass, to let it insert or\nupdate the key/value pair.\n2. Call self.move_to_end to ensure the updated key is in the last position.\nInvoking an overridden __init__ method is particularly important to allow super‐\nclasses to do their part in initializing the instance.\nIf you learned object-oriented programming in Java, you may\nrecall that a Java constructor method automatically calls the no-\nargument constructor of the superclass. Python doesn’t do this.\nYou must get used to writing this pattern:\n    def __init__(self, a, b) :\n        super().__init__(a, b)\n        ...  # more initialization code\nYou may have seen code that doesn’t use super(), but instead calls the method\ndirectly on the superclass, like this:\nclass NotRecommended(OrderedDict):\n    \"\"\"This is a counter example!\"\"\"\n    def __setitem__(self, key, value):\n        OrderedDict.__setitem__(self, key, value)\n        self.move_to_end(key)\nThis alternative works in this particular case, but is not recommended for two rea‐\nsons. First, it hardcodes the base class. The name OrderedDict appears in the class\nstatement and also inside __setitem__. If in the future someone changes the\nclass statement to change the base class or add another one, they may forget to\nupdate the body of __setitem__, introducing a bug.\nThe super() Function \n| \n489",
      "content_length": 1974,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "3 It is also possible to provide only the first argument, but this not useful and may soon be deprecated, with the\nblessing of Guido van Rossum who created super() in the first place. See the discussion at “Is it time to dep‐\nrecate unbound super methods?”.\nThe second reason is that super implements logic to handle class hierarchies with\nmultiple inheritance. We’ll come back to that in “Multiple Inheritance and Method\nResolution Order” on page 494. To conclude this refresher about super, it is useful to\nreview how we had to call it in Python 2, because the old signature with two argu‐\nments is revealing:\nclass LastUpdatedOrderedDict(OrderedDict):\n    \"\"\"This code works in Python 2 and Python 3\"\"\"\n    def __setitem__(self, key, value):\n        super(LastUpdatedOrderedDict, self).__setitem__(key, value)\n        self.move_to_end(key)\nBoth arguments of super are now optional. The Python 3 bytecode compiler auto‐\nmatically provides them by inspecting the surrounding context when super() is\ninvoked in a method. The arguments are:\ntype\nThe start of the search path for the superclass implementing the desired method.\nBy default, it is the class that owns the method where the super() call appears.\nobject_or_type\nThe object (for instance method calls) or class (for class method calls) to be the\nreceiver of the method call. By default, it is self if the super() call happens in an\ninstance method.\nWhether you or the compiler provides those arguments, the super() call returns a\ndynamic proxy object that finds a method (such as __setitem__ in the example) in a\nsuperclass of the type parameter, and binds it to the object_or_type, so that we\ndon’t need to pass the receiver (self) explicitly when invoking the method.\nIn Python 3, you can still explicitly provide the first and second arguments\nto super().3 But they are needed only in special cases, such as skipping over part of\nthe MRO for testing or debugging, or for working around undesired behavior in a\nsuperclass.\nNow let’s discuss the caveats when subclassing built-in types.\nSubclassing Built-In Types Is Tricky\nIt was not possible to subclass built-in types such as list or dict in the earliest ver‐\nsions of Python. Since Python 2.2, it’s possible, but there is a major caveat: the code of\n490 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "the built-ins (written in C) usually does not call methods overridden by user-defined\nclasses. A good short description of the problem is in the documentation for PyPy, in\nthe “Differences between PyPy and CPython” section, “Subclasses of built-in types”:\nOfficially, CPython has no rule at all for when exactly overridden method of subclasses\nof built-in types get implicitly called or not. As an approximation, these methods are\nnever called by other built-in methods of the same object. For example, an overridden\n__getitem__() in a subclass of dict will not be called by e.g. the built-in get()\nmethod.\nExample 14-1 illustrates the problem.\nExample 14-1. Our __setitem__ override is ignored by the __init__ and __update__\nmethods of the built-in dict\n>>> class DoppelDict(dict):\n...     def __setitem__(self, key, value):\n...         super().__setitem__(key, [value] * 2)  \n...\n>>> dd = DoppelDict(one=1)  \n>>> dd\n{'one': 1}\n>>> dd['two'] = 2  \n>>> dd\n{'one': 1, 'two': [2, 2]}\n>>> dd.update(three=3)  \n>>> dd\n{'three': 3, 'one': 1, 'two': [2, 2]}\nDoppelDict.__setitem__ duplicates values when storing (for no good reason,\njust to have a visible effect). It works by delegating to the superclass.\nThe __init__ method inherited from dict clearly ignored that __setitem__ was\noverridden: the value of 'one' is not duplicated.\nThe [] operator calls our __setitem__ and works as expected: 'two' maps to the\nduplicated value [2, 2].\nThe update method from dict does not use our version of __setitem__ either:\nthe value of 'three' was not duplicated.\nThis built-in behavior is a violation of a basic rule of object-oriented programming:\nthe search for methods should always start from the class of the receiver (self), even\nwhen the call happens inside a method implemented in a superclass. This is what is\ncalled “late binding,” which Alan Kay—of Smalltalk fame—considers a key feature of\nobject-oriented programming: in any call of the form x.method(), the exact method\nSubclassing Built-In Types Is Tricky \n| \n491",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "4 It is interesting to note that C++ has the notion of virtual and nonvirtual methods. Virtual methods are late\nbound, but nonvirtual methods are bound at compile time. Although every method that we can write in\nPython is late bound like a virtual method, built-in objects written in C seem to have nonvirtual methods by\ndefault, at least in CPython.\nto be called must be determined at runtime, based on the class of the receiver x.4\nThis sad state of affairs contributes to the issues we saw in “Inconsistent Usage of\n__missing__ in the Standard Library” on page 94.\nThe problem is not limited to calls within an instance—whether self.get() calls\nself.__getitem__()—but also happens with overridden methods of other classes\nthat should be called by the built-in methods. Example 14-2 is adapted from the PyPy\ndocumentation.\nExample 14-2. The __getitem__ of AnswerDict is bypassed by dict.update\n>>> class AnswerDict(dict):\n...     def __getitem__(self, key):  \n...         return 42\n...\n>>> ad = AnswerDict(a='foo')  \n>>> ad['a']  \n42\n>>> d = {}\n>>> d.update(ad)  \n>>> d['a']  \n'foo'\n>>> d\n{'a': 'foo'}\nAnswerDict.__getitem__ always returns 42, no matter what the key.\nad is an AnswerDict loaded with the key-value pair ('a', 'foo').\nad['a'] returns 42, as expected.\nd is an instance of plain dict, which we update with ad.\nThe dict.update method ignored our AnswerDict.__getitem__.\nSubclassing built-in types like dict or list or str directly is error-\nprone because the built-in methods mostly ignore user-defined\noverrides. Instead of subclassing the built-ins, derive your classes\nfrom the collections module using UserDict, UserList, and\nUserString, which are designed to be easily extended.\n492 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "5 If you are curious, the experiment is in the 14-inheritance/strkeydict_dictsub.py file in the fluentpython/\nexample-code-2e repository.\nIf you subclass collections.UserDict instead of dict, the issues exposed in Exam‐\nples 14-1 and 14-2 are both fixed. See Example 14-3.\nExample 14-3. DoppelDict2 and AnswerDict2 work as expected because they extend\nUserDict and not dict\n>>> import collections\n>>>\n>>> class DoppelDict2(collections.UserDict):\n...     def __setitem__(self, key, value):\n...         super().__setitem__(key, [value] * 2)\n...\n>>> dd = DoppelDict2(one=1)\n>>> dd\n{'one': [1, 1]}\n>>> dd['two'] = 2\n>>> dd\n{'two': [2, 2], 'one': [1, 1]}\n>>> dd.update(three=3)\n>>> dd\n{'two': [2, 2], 'three': [3, 3], 'one': [1, 1]}\n>>>\n>>> class AnswerDict2(collections.UserDict):\n...     def __getitem__(self, key):\n...         return 42\n...\n>>> ad = AnswerDict2(a='foo')\n>>> ad['a']\n42\n>>> d = {}\n>>> d.update(ad)\n>>> d['a']\n42\n>>> d\n{'a': 42}\nAs an experiment to measure the extra work required to subclass a built-in, I rewrote\nthe StrKeyDict class from Example 3-9 to subclass dict instead of UserDict. In\norder to make it pass the same suite of tests, I had to implement __init__, get, and\nupdate because the versions inherited from dict refused to cooperate with the over‐\nridden __missing__, __contains__, and __setitem__. The UserDict subclass from\nExample 3-9 has 16 lines, while the experimental dict subclass ended up with 33\nlines.5\nSubclassing Built-In Types Is Tricky \n| \n493",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "6 By the way, in this regard, PyPy behaves more “correctly” than CPython, at the expense of introducing a\nminor incompatibility. See “Differences between PyPy and CPython” for details.\nTo be clear: this section covered an issue that applies only to method delegation\nwithin the C language code of the built-in types, and only affects classes derived\ndirectly from those types. If you subclass a base class coded in Python, such as\nUserDict or MutableMapping, you will not be troubled by this.6\nNow let’s focus on an issue that arises with multiple inheritance: if a class has two\nsuperclasses, how does Python decide which attribute to use when we call\nsuper().attr, but both superclasses have an attribute with that name?\nMultiple Inheritance and Method Resolution Order\nAny language implementing multiple inheritance needs to deal with potential nam‐\ning conflicts when superclasses implement a method by the same name. This is called\nthe “diamond problem,” illustrated in Figure 14-1 and Example 14-4.\nFigure 14-1. Left: Activation sequence for the leaf1.ping() call. Right: Activation\nsequence for the leaf1.pong() call.\nExample 14-4. diamond.py: classes Leaf, A, B, Root form the graph in Figure 14-1\nclass Root:  \n    def ping(self):\n        print(f'{self}.ping() in Root')\n    def pong(self):\n        print(f'{self}.pong() in Root')\n    def __repr__(self):\n494 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "cls_name = type(self).__name__\n        return f'<instance of {cls_name}>'\nclass A(Root):  \n    def ping(self):\n        print(f'{self}.ping() in A')\n        super().ping()\n    def pong(self):\n        print(f'{self}.pong() in A')\n        super().pong()\nclass B(Root):  \n    def ping(self):\n        print(f'{self}.ping() in B')\n        super().ping()\n    def pong(self):\n        print(f'{self}.pong() in B')\nclass Leaf(A, B):  \n    def ping(self):\n        print(f'{self}.ping() in Leaf')\n        super().ping()\nRoot provides ping, pong, and __repr__ to make the output easier to read.\nThe ping and pong methods in class A both call super().\nOnly the ping method in class B calls super().\nClass Leaf implements only ping, and it calls super().\nNow let’s see the effect of calling the ping and pong methods on an instance of Leaf\n(Example 14-5).\nExample 14-5. Doctests for calling ping and pong on a Leaf object\n    >>> leaf1 = Leaf()  \n    >>> leaf1.ping()    \n    <instance of Leaf>.ping() in Leaf\n    <instance of Leaf>.ping() in A\n    <instance of Leaf>.ping() in B\n    <instance of Leaf>.ping() in Root\n    >>> leaf1.pong()   \nMultiple Inheritance and Method Resolution Order \n| \n495",
      "content_length": 1183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "7 Classes also have a .mro() method, but that’s an advanced feature of metaclass programming, mentioned in\n“Classes as Objects” on page 908. The content of the __mro__ attribute is what matters during normal usage\nof a class.\n    <instance of Leaf>.pong() in A\n    <instance of Leaf>.pong() in B\nleaf1 is an instance of Leaf.\nCalling leaf1.ping() activates the ping methods in Leaf, A, B, and Root, because\nthe ping methods in the first three classes call super().ping().\nCalling leaf1.pong() activates pong in A via inheritance, which then calls\nsuper.pong(), activating B.pong.\nThe activation sequences shown in Example 14-5 and Figure 14-1 are determined by\ntwo factors:\n• The method resolution order of the Leaf class.\n• The use of super() in each method.\nEvery class has an attribute called __mro__ holding a tuple of references to the super‐\nclasses in method resolution order, from the current class all the way to the object\nclass.7 For the Leaf class, this is the __mro__:\n>>> Leaf.__mro__  # doctest:+NORMALIZE_WHITESPACE\n    (<class 'diamond1.Leaf'>, <class 'diamond1.A'>, <class 'diamond1.B'>,\n     <class 'diamond1.Root'>, <class 'object'>)\nLooking at Figure 14-1, you may think the MRO describes a\nbreadth-first search, but that’s just a coincidence for that particular\nclass hierarchy. The MRO is computed by a published algorithm\ncalled C3. Its use in Python is detailed in Michele Simionato’s “The\nPython 2.3 Method Resolution Order”. It’s a challenging read, but\nSimionato writes: “unless you make strong use of multiple inheri‐\ntance and you have non-trivial hierarchies, you don’t need to\nunderstand the C3 algorithm, and you can easily skip this paper.”\nThe MRO only determines the activation order, but whether a particular method will\nbe activated in each of the classes depends on whether each implementation calls\nsuper() or not.\nConsider the experiment with the pong method. The Leaf class does not override it,\ntherefore calling leaf1.pong() activates the implementation in the next class of\n496 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "Leaf.__mro__: the A class. Method A.pong calls super().pong(). The B class is next\nin the MRO, therefore B.pong is activated. But that method doesn’t call\nsuper().pong(), so the activation sequence ends here.\nThe MRO takes into account not only the inheritance graph but also the order in\nwhich superclasses are listed in a subclass declaration. In other words, if in dia‐\nmond.py (Example 14-4) the Leaf class was declared as Leaf(B, A), then class B\nwould appear before A in Leaf.__mro__. This would affect the activation order of the\nping methods, and would also cause leaf1.pong() to activate B.pong via inheritance,\nbut A.pong and Root.pong would never run, because B.pong doesn’t call super().\nWhen a method calls super(), it is a cooperative method. Cooperative methods\nenable cooperative multiple inheritance. These terms are intentional: in order to work,\nmultiple inheritance in Python requires the active cooperation of the methods\ninvolved. In the B class, ping cooperates, but pong does not.\nA noncooperative method can be the cause of subtle bugs. Many\ncoders reading Example 14-4 may expect that when method\nA.pong calls super.pong(), that will ultimately activate Root.pong.\nBut if B.pong is activated before, it drops the ball. That’s why it is\nrecommended that every method m of a nonroot class should call\nsuper().m().\nCooperative methods must have compatible signatures, because you never know\nwhether A.ping will be called before or after B.ping. The activation sequence\ndepends on the order of A and B in the declaration of each subclass that inherits from\nboth.\nPython is a dynamic language, so the interaction of super() with the MRO is also\ndynamic. Example 14-6 shows a surprising result of this dynamic behavior.\nExample 14-6. diamond2.py: classes to demonstrate the dynamic nature of super()\nfrom diamond import A  \nclass U():  \n    def ping(self):\n        print(f'{self}.ping() in U')\n        super().ping()  \nclass LeafUA(U, A):  \n    def ping(self):\n        print(f'{self}.ping() in LeafUA')\n        super().ping()\nMultiple Inheritance and Method Resolution Order \n| \n497",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "Class A comes from diamond.py (Example 14-4).\nClass U is unrelated to A or Root from the diamond module.\nWhat does super().ping() do? Answer: it depends. Read on.\nLeafUA subclasses U and A in this order.\nIf you create an instance of U and try to call ping, you get an error:\n    >>> u = U()\n    >>> u.ping()\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'super' object has no attribute 'ping'\nThe 'super' object returned by super() has no attribute 'ping' because the MRO\nof U has two classes: U and object, and the latter has no attribute named 'ping'.\nHowever, the U.ping method is not completely hopeless. Check this out:\n    >>> leaf2 = LeafUA()\n    >>> leaf2.ping()\n    <instance of LeafUA>.ping() in LeafUA\n    <instance of LeafUA>.ping() in U\n    <instance of LeafUA>.ping() in A\n    <instance of LeafUA>.ping() in Root\n    >>> LeafUA.__mro__  # doctest:+NORMALIZE_WHITESPACE\n    (<class 'diamond2.LeafUA'>, <class 'diamond2.U'>,\n     <class 'diamond.A'>, <class 'diamond.Root'>, <class 'object'>)\nThe super().ping() call in LeafUA activates U.ping, which cooperates by calling\nsuper().ping() too, activating A.ping, and eventually Root.ping.\nNote the base classes of LeafUA are (U, A) in that order. If instead the bases were\n(A, U), then leaf2.ping() would never reach U.ping, because the super().ping()\nin A.ping would activate Root.ping, and that method does not call super().\nIn a real program, a class like U could be a mixin class: a class intended to be used\ntogether with other classes in multiple inheritance, to provide additional functional‐\nity. We’ll study that shortly, in “Mixin Classes” on page 500.\nTo wrap up this discussion of the MRO, Figure 14-2 illustrates part of the complex\nmultiple inheritance graph of the Tkinter GUI toolkit from the Python standard\nlibrary.\n498 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "Figure 14-2. Left: UML diagram of the Tkinter Text widget class and superclasses.\nRight: The long and winding path of Text.__mro__ is drawn with dashed arrows.\nTo study the picture, start at the Text class at the bottom. The Text class implements\na full-featured, multiline editable text widget. It provides rich functionality in itself,\nbut also inherits many methods from other classes. The lefthand side shows a plain\nUML class diagram. On the right, it’s decorated with arrows showing the MRO, as\nlisted in Example 14-7 with the help of a print_mro convenience function.\nExample 14-7. MRO of tkinter.Text\n>>> def print_mro(cls):\n...     print(', '.join(c.__name__ for c in cls.__mro__))\n>>> import tkinter\n>>> print_mro(tkinter.Text)\nText, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView, object\nNow let’s talk about mixins.\nMultiple Inheritance and Method Resolution Order \n| \n499",
      "content_length": 895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "Mixin Classes\nA mixin class is designed to be subclassed together with at least one other class in a\nmultiple inheritance arrangement. A mixin is not supposed to be the only base class\nof a concrete class, because it does not provide all the functionality for a concrete\nobject, but only adds or customizes the behavior of child or sibling classes.\nMixin classes are a convention with no explicit language support in\nPython and C++. Ruby allows the explicit definition and use of\nmodules that work as mixins—collections of methods that may be\nincluded to add functionality to a class. C#, PHP, and Rust imple‐\nment traits, which are also an explicit form of mixin.\nLet’s see a simple but handy example of a mixin class.\nCase-Insensitive Mappings\nExample 14-8 shows UpperCaseMixin, a class designed to provide case-insensitive\naccess to mappings with string keys, by uppercasing those keys when they are added\nor looked up.\nExample 14-8. uppermixin.py: UpperCaseMixin supports case-insensitive mappings\nimport collections\ndef _upper(key):  \n    try:\n        return key.upper()\n    except AttributeError:\n        return key\nclass UpperCaseMixin:  \n    def __setitem__(self, key, item):\n        super().__setitem__(_upper(key), item)\n    def __getitem__(self, key):\n        return super().__getitem__(_upper(key))\n    def get(self, key, default=None):\n        return super().get(_upper(key), default)\n    def __contains__(self, key):\n        return super().__contains__(_upper(key))\n500 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 1536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "This helper function takes a key of any type, and tries to return key.upper(); if\nthat fails, it returns the key unchanged.\nThe mixin implements four essential methods of mappings, always calling\nsuper(), with the key uppercased, if possible.\nSince every method ot UpperCaseMixin calls super(), this mixin depends on a sibling\nclass that implements or inherits methods with the same signature. To make its con‐\ntribution, a mixin usually needs to appear before other classes in the MRO of a sub‐\nclass that uses it. In practice, that means mixins must appear first in the tuple of base\nclasses in a class declaration. Example 14-9 shows two examples.\nExample 14-9. uppermixin.py: two classes that use UpperCaseMixin\nclass UpperDict(UpperCaseMixin, collections.UserDict):  \n    pass\nclass UpperCounter(UpperCaseMixin, collections.Counter):  \n    \"\"\"Specialized 'Counter' that uppercases string keys\"\"\"  \nUpperDict needs no implementation of its own, but UpperCaseMixin must be the\nfirst base class, otherwise the methods from UserDict would be called instead.\nUpperCaseMixin also works with Counter.\nInstead of pass, it’s better to provide a docstring to satisfy the need for a body in\nthe class statement syntax.\nHere are some doctests from uppermixin.py, for UpperDict:\n    >>> d = UpperDict([('a', 'letter A'), (2, 'digit two')])\n    >>> list(d.keys())\n    ['A', 2]\n    >>> d['b'] = 'letter B'\n    >>> 'b' in d\n    True\n    >>> d['a'], d.get('B')\n    ('letter A', 'letter B')\n    >>> list(d.keys())\n    ['A', 2, 'B']\nAnd a quick demonstration of UpperCounter:\n    >>> c = UpperCounter('BaNanA')\n    >>> c.most_common()\n    [('A', 3), ('N', 2), ('B', 1)]\nMixin Classes \n| \n501",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "8 Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, Design Patterns: Elements of Reusable\nObject-Oriented Software (Addison-Wesley).\n9 As previously mentioned, Java 8 allows interfaces to provide method implementations as well. The new fea‐\nture is called “Default Methods” in the official Java Tutorial.\nUpperDict and UpperCounter seem almost magical, but I had to carefully study the\ncode of UserDict and Counter to make UpperCaseMixin work with them.\nFor example, my first version of UpperCaseMixin did not provide the get method.\nThat version worked with UserDict but not with Counter. The UserDict class inher‐\nits get from collections.abc.Mapping, and that get calls __getitem__, which I\nimplemented. But keys were not uppercased when an UpperCounter was loaded upon\n__init__. That happened because Counter.__init__ uses Counter.update, which in\nturn relies on the get method inherited from dict. However, the get method in the\ndict class does not call __getitem__. This is the heart of the issue discussed in\n“Inconsistent Usage of __missing__ in the Standard Library” on page 94. It is also a\nstark reminder of the brittle and puzzling nature of programs leveraging inheritance,\neven at a small scale.\nThe next section covers several examples of multiple inheritance, often featuring\nmixin classes.\nMultiple Inheritance in the Real World\nIn the Design Patterns book,8 almost all the code is in C++, but the only example of\nmultiple inheritance is the Adapter pattern. In Python, multiple inheritance is not the\nnorm either, but there are important examples that I will comment on in this section.\nABCs Are Mixins Too\nIn the Python standard library, the most visible use of multiple inheritance is the col\nlections.abc package. That is not controversial: after all, even Java supports multi‐\nple inheritance of interfaces, and ABCs are interface declarations that may optionally\nprovide concrete method implementations.9\nPython’s official documentation of collections.abc uses the term mixin method for\nthe concrete methods implemented in many of the collection ABCs. The ABCs that\nprovide mixin methods play two roles: they are interface definitions and also mixin\nclasses. For example, the implementation of collections.UserDict relies on several\nof the mixin methods provided by collections.abc.MutableMapping.\n502 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "ThreadingMixIn and ForkingMixIn\nThe http.server package provides HTTPServer and ThreadingHTTPServer classes. The\nlatter was added in Python 3.7. Its documentation says:\nclass http.server.ThreadingHTTPServer(server_address, RequestHandlerClass)\nThis class is identical to HTTPServer but uses threads to handle requests by using\nthe ThreadingMixIn. This is useful to handle web browsers pre-opening sockets,\non which HTTPServer would wait indefinitely.\nThis is the complete source code for the ThreadingHTTPServer class in Python 3.10:\nclass ThreadingHTTPServer(socketserver.ThreadingMixIn, HTTPServer):\n    daemon_threads = True\nThe source code of socketserver.ThreadingMixIn has 38 lines, including comments\nand docstrings. Example 14-10 shows a summary of its implementation.\nExample 14-10. Part of Lib/socketserver.py in Python 3.10\nclass ThreadingMixIn:\n    \"\"\"Mixin class to handle each request in a new thread.\"\"\"\n    # 8 lines omitted in book listing\n    def process_request_thread(self, request, client_address):  \n        ... # 6 lines omitted in book listing\n    def process_request(self, request, client_address):  \n        ... # 8 lines omitted in book listing\n    def server_close(self):  \n        super().server_close()\n        self._threads.join()\nprocess_request_thread does not call super() because it is a new method, not\nan override. Its implementation calls three instance methods that HTTPServer\nprovides or inherits.\nThis overrides the process_request method that HTTPServer inherits from sock\netserver.BaseServer, starting a thread and delegating the actual work to pro\ncess_request_thread running in that thread. It does not call super().\nserver_close calls super().server_close() to stop taking requests, then waits\nfor the threads started by process_request to finish their jobs.\nThe ThreadingMixIn appears in the socketserver module documentation next to\nForkingMixin. The latter is designed to support concurrent servers based on\nMultiple Inheritance in the Real World \n| \n503",
      "content_length": 2003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "os.fork(), an API for launching a child process, available in POSIX-compliant\nUnix-like systems.\nDjango Generic Views Mixins\nYou don’t need to know Django to follow this section. I am using a\nsmall part of the framework as a practical example of multiple\ninheritance, and I will try to give all the necessary background,\nassuming you have some experience with server-side web develop‐\nment in any language or framework.\nIn Django, a view is a callable object that takes a request argument—an object repre‐\nsenting an HTTP request—and returns an object representing an HTTP response.\nThe different responses are what interests us in this discussion. They can be as simple\nas a redirect response, with no content body, or as complex as a catalog page in an\nonline store, rendered from an HTML template and listing multiple merchandise\nwith buttons for buying, and links to detail pages.\nOriginally, Django provided a set of functions, called generic views, that imple‐\nmented some common use cases. For example, many sites need to show search\nresults that include information from numerous items, with the listing spanning mul‐\ntiple pages, and for each item a link to a page with detailed information about it.\nIn Django, a list view and a detail view are designed to work together to solve this\nproblem: a list view renders search results, and a detail view produces a page for each\nindividual item.\nHowever, the original generic views were functions, so they were not extensible. If\nyou needed to do something similar but not exactly like a generic list view, you’d\nhave to start from scratch.\nThe concept of class-based views was introduced in Django 1.3, along with a set of\ngeneric view classes organized as base classes, mixins, and ready-to-use concrete\nclasses. In Django 3.2, the base classes and mixins are in the base module of\nthe django.views.generic package, pictured in Figure 14-3. At the top of the\ndiagram we see two classes that take care of very distinct responsibilities: View and\nTemplateResponseMixin.\n504 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "10 Django programmers know that the as_view class method is the most visible part of the View interface, but\nit’s not relevant to us here.\nFigure 14-3. UML class diagram for the django.views.generic.base module.\nA great resource to study these classes is the Classy Class-Based\nViews website, where you can easily navigate through them, see all\nmethods in each class (inherited, overridden, and added methods),\nview diagrams, browse their documentation, and jump to their\nsource code on GitHub.\nView is the base class of all views (it could be an ABC), and it provides core function‐\nality like the dispatch method, which delegates to “handler” methods like get, head,\npost, etc., implemented by concrete subclasses to handle the different HTTP verbs.10\nThe RedirectView class inherits only from View, and you can see that it implements\nget, head, post, etc.\nConcrete subclasses of View are supposed to implement the handler methods, so why\naren’t those methods part of the View interface? The reason: subclasses are free to\nMultiple Inheritance in the Real World \n| \n505",
      "content_length": 1071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "11 If you are into design patterns, you’ll notice that the Django dispatch mechanism is a dynamic variation of the\nTemplate Method pattern. It’s dynamic because the View class does not force subclasses to implement all han‐\ndlers, but dispatch checks at runtime if a concrete handler is available for the specific request.\nimplement just the handlers they want to support. A TemplateView is used only to\ndisplay content, so it only implements get. If an HTTP POST request is sent to a Tem\nplateView, the inherited View.dispatch method checks that there is no post han‐\ndler, and produces an HTTP 405 Method Not Allowed response.11\nThe TemplateResponseMixin provides functionality that is of interest only to views\nthat need to use a template. A RedirectView, for example, has no content body, so it\nhas no need of a template and it does not inherit from this mixin. TemplateResponse\nMixin provides behaviors to TemplateView and other template-rendering views, such\nas ListView, DetailView, etc., defined in the django.views.generic subpackages.\nFigure 14-4 depicts the django.views.generic.list module and part of the base\nmodule.\nFor Django users, the most important class in Figure 14-4 is ListView, which is an\naggregate class, with no code at all (its body is just a docstring). When instantiated, a\nListView has an object_list instance attribute through which the template can iter‐\nate to show the page contents, usually the result of a database query returning multi‐\nple objects. All the functionality related to generating this iterable of objects comes\nfrom the MultipleObjectMixin. That mixin also provides the complex pagination\nlogic—to display part of the results in one page and links to more pages.\nSuppose you want to create a view that will not render a template, but will produce a\nlist of objects in JSON format. That’s why the BaseListView exists. It provides an\neasy-to-use extension point that brings together View and MultipleObjectMixin\nfunctionality, without the overhead of the template machinery.\nThe Django class-based views API is a better example of multiple inheritance than\nTkinter. In particular, it is easy to make sense of its mixin classes: each has a well-\ndefined purpose, and they are all named with the …Mixin suffix.\n506 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "Figure 14-4. UML class diagram for the django.views.generic.list module. Here\nthe three classes of the base module are collapsed (see Figure 14-3). The ListView class\nhas no methods or attributes: it’s an aggregate class.\nClass-based views were not universally embraced by Django users. Many do use them\nin a limited way, as opaque boxes, but when it’s necessary to create something new, a\nlot of Django coders continue writing monolithic view functions that take care of all\nthose responsibilities, instead of trying to reuse the base views and mixins.\nIt does take some time to learn how to leverage class-based views and how to extend\nthem to fulfill specific application needs, but I found that it was worthwhile to study\nthem. They eliminate a lot of boilerplate code, make it easier to reuse solutions, and\neven improve team communication—for example, by defining standard names to\ntemplates, and to the variables passed to template contexts. Class-based views are\nDjango views “on rails.”\nMultiple Inheritance in Tkinter\nAn extreme example of multiple inheritance in Python’s standard library is the\nTkinter GUI toolkit. I used part of the Tkinter widget hierarchy to illustrate the MRO\nin Figure 14-2. Figure 14-5 shows all the widget classes in the tkinter base package\n(there are more widgets in the tkinter.ttk subpackage).\nMultiple Inheritance in the Real World \n| \n507",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "Figure 14-5. Summary UML diagram for the Tkinter GUI class hierarchy; classes tag‐\nged «mixin» are designed to provide concrete methods to other classes via multiple\ninheritance.\nTkinter is 25 years old as I write this. It is not an example of current best practices.\nBut it shows how multiple inheritance was used when coders did not appreciate its\ndrawbacks. And it will serve as a counterexample when we cover some good practices\nin the next section.\nConsider these classes from Figure 14-5:\n➊ Toplevel: The class of a top-level window in a Tkinter application.\n➋ Widget: The superclass of every visible object that can be placed on a window.\n➌ Button: A plain button widget.\n➍ Entry: A single-line editable text field.\n➎ Text: A multiline editable text field.\nHere are the MROs of those classes, displayed by the print_mro function from\nExample 14-7:\n>>> import tkinter\n>>> print_mro(tkinter.Toplevel)\nToplevel, BaseWidget, Misc, Wm, object\n508 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 1001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": ">>> print_mro(tkinter.Widget)\nWidget, BaseWidget, Misc, Pack, Place, Grid, object\n>>> print_mro(tkinter.Button)\nButton, Widget, BaseWidget, Misc, Pack, Place, Grid, object\n>>> print_mro(tkinter.Entry)\nEntry, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, object\n>>> print_mro(tkinter.Text)\nText, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView, object\nBy current standards, the class hierarchy of Tkinter is very deep.\nFew parts of the Python standard library have more than three or\nfour levels of concrete classes, and the same can be said of the Java\nclass library. However, it is interesting to note that the some of the\ndeepest hierarchies in the Java class library are precisely in\nthe packages related to GUI programming: java.awt and\njavax.swing. Squeak, the modern, free version of Smalltalk,\nincludes the powerful and innovative Morphic GUI toolkit, also\nwith a deep class hierarchy. In my experience, GUI toolkits are\nwhere inheritance is most useful.\nNote how these classes relate to others:\n• Toplevel is the only graphical class that does not inherit from Widget, because it\nis the top-level window and does not behave like a widget; for example, it cannot\nbe attached to a window or frame. Toplevel inherits from Wm, which provides\ndirect access functions of the host window manager, like setting the window title\nand configuring its borders.\n• Widget inherits directly from BaseWidget and from Pack, Place, and Grid. These\nlast three classes are geometry managers: they are responsible for arranging\nwidgets inside a window or frame. Each encapsulates a different layout strategy\nand widget placement API.\n• Button, like most widgets, descends only from Widget, but indirectly from Misc,\nwhich provides dozens of methods to every widget.\n• Entry subclasses Widget and XView, which support horizontal scrolling.\n• Text subclasses from Widget, XView, and YView for vertical scrolling.\nWe’ll now discuss some good practices of multiple inheritance and see whether\nTkinter goes along with them.\nMultiple Inheritance in the Real World \n| \n509",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "12 The principle appears on p. 20 of the introduction to the book.\nCoping with Inheritance\nWhat Alan Kay wrote in the epigraph remains true: there’s still no general theory\nabout inheritance that can guide practicing programmers. What we have are rules of\nthumb, design patterns, “best practices,” clever acronyms, taboos, etc. Some of these\nprovide useful guidelines, but none of them are universally accepted or always\napplicable.\nIt’s easy to create incomprehensible and brittle designs using inheritance, even\nwithout multiple inheritance. Because we don’t have a comprehensive theory, here\nare a few tips to avoid spaghetti class graphs.\nFavor Object Composition over Class Inheritance\nThe title of this subsection is the second principle of object-oriented design from the\nDesign Patterns book,12 and is the best advice I can offer here. Once you get comforta‐\nble with inheritance, it’s too easy to overuse it. Placing objects in a neat hierarchy\nappeals to our sense of order; programmers do it just for fun.\nFavoring composition leads to more flexible designs. For example, in the case of the\ntkinter.Widget class, instead of inheriting the methods from all geometry managers,\nwidget instances could hold a reference to a geometry manager, and invoke its meth‐\nods. After all, a Widget should not “be” a geometry manager, but could use the serv‐\nices of one via delegation. Then you could add a new geometry manager without\ntouching the widget class hierarchy and without worrying about name clashes. Even\nwith single inheritance, this principle enhances flexibility, because subclassing is a\nform of tight coupling, and tall inheritance trees tend to be brittle.\nComposition and delegation can replace the use of mixins to make behaviors avail‐\nable to different classes, but cannot replace the use of interface inheritance to define a\nhierarchy of types.\nUnderstand Why Inheritance Is Used in Each Case\nWhen dealing with multiple inheritance, it’s useful to keep straight the reasons why\nsubclassing is done in each particular case. The main reasons are:\n• Inheritance of interface creates a subtype, implying an “is-a” relationship. This is\nbest done with ABCs.\n• Inheritance of implementation avoids code duplication by reuse. Mixins can help\nwith this.\n510 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "13 Grady Booch et al., Object-Oriented Analysis and Design with Applications, 3rd ed. (Addison-Wesley), p. 109.\nIn practice, both uses are often simultaneous, but whenever you can make the intent\nclear, do it. Inheritance for code reuse is an implementation detail, and it can often be\nreplaced by composition and delegation. On the other hand, interface inheritance is\nthe backbone of a framework. Interface inheritance should use only ABCs as base\nclasses, if possible.\nMake Interfaces Explicit with ABCs\nIn modern Python, if a class is intended to define an interface, it should be an explicit\nABC or a typing.Protocol subclass. An ABC should subclass only abc.ABC or other\nABCs. Multiple inheritance of ABCs is not problematic.\nUse Explicit Mixins for Code Reuse\nIf a class is designed to provide method implementations for reuse by multiple unre‐\nlated subclasses, without implying an “is-a” relationship, it should be an explicit\nmixin class. Conceptually, a mixin does not define a new type; it merely bundles\nmethods for reuse. A mixin should never be instantiated, and concrete classes should\nnot inherit only from a mixin. Each mixin should provide a single specific behavior,\nimplementing few and very closely related methods. Mixins should avoid keeping\nany internal state; i.e., a mixin class should not have instance attributes.\nThere is no formal way in Python to state that a class is a mixin, so it is highly recom‐\nmended that they are named with a Mixin suffix.\nProvide Aggregate Classes to Users\nA class that is constructed primarily by inheriting from mixins and does not add its\nown structure or behavior is called an aggregate class.\n—Booch et al.13\nIf some combination of ABCs or mixins is particularly useful to client code, provide a\nclass that brings them together in a sensible way.\nFor example, here is the complete source code for the Django ListView class on the\nbottom right of Figure 14-4:\nclass ListView(MultipleObjectTemplateResponseMixin, BaseListView):\n    \"\"\"\n    Render some list of objects, set by `self.model` or `self.queryset`.\n    `self.queryset` can actually be any iterable of items, not just a queryset.\n    \"\"\"\nCoping with Inheritance \n| \n511",
      "content_length": 2190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "14 PEP 591 also introduces a Final annotation for variables or attributes that should not be reassigned or over‐\nridden.\nThe body of ListView is empty, but the class provides a useful service: it brings\ntogether a mixin and a base class that should be used together.\nAnother example is tkinter.Widget, which has four base classes and no methods or\nattributes of its own—just a docstring. Thanks to the Widget aggregate class, we can\ncreate new a widget with the required mixins, without having to figure out in which\norder they should be declared to work as intended.\nNote that aggregate classes don’t have to be completely empty, but they often are.\nSubclass Only Classes Designed for Subclassing\nIn one comment about this chapter, technical reviewer Leonardo Rochael suggested\nthe following warning.\nSubclassing any complex class and overriding its methods is error-\nprone because the superclass methods may ignore the subclass\noverrides in unexpected ways. As much as possible, avoid overrid‐\ning methods, or at least restrain yourself to subclassing classes\nwhich are designed to be easily extended, and only in the ways in\nwhich they were designed to be extended.\nThat’s great advice, but how do we know whether or how a class was designed to be\nextended?\nThe first answer is documentation (sometimes in the form of docstrings or even\ncomments in code). For example, Python’s socketserver package is described as “a\nframework for network servers.” Its BaseServer class is designed for subclassing, as\nthe name suggests. More importantly, the documentation and the docstring in the\nsource code of the class explicitly note which of its methods are intended to be over‐\nridden by subclasses.\nIn Python ≥ 3.8, a new way of making those design constraints explicit is provided by\nPEP 591—Adding a final qualifier to typing. The PEP introduces a @final decorator\nthat can be applied to classes or individual methods, so that IDEs or type checkers\ncan report misguided attempts to subclass those classes or override those methods.14\n512 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "Avoid Subclassing from Concrete Classes\nSubclassing concrete classes is more dangerous than subclassing ABCs and mixins,\nbecause instances of concrete classes usually have internal state that can easily be cor‐\nrupted when you override methods that depend on that state. Even if your methods\ncooperate by calling super(), and the internal state is held in private attributes using\nthe __x syntax, there are still countless ways a method override can introduce bugs.\nIn “Waterfowl and ABCs” on page 443, Alex Martelli quotes Scott Meyer’s More\nEffective C++, which says: “all non-leaf classes should be abstract.” In other words,\nMeyer recommends that only abstract classes should be subclassed.\nIf you must use subclassing for code reuse, then the code intended for reuse should\nbe in mixin methods of ABCs or in explicitly named mixin classes.\nWe will now analyze Tkinter from the point of view of these recommendations.\nTkinter: The Good, the Bad, and the Ugly\nMost advice in the previous section is not followed by Tkinter, with the notable\nexception of “Provide Aggregate Classes to Users” on page 511. Even then, it’s not a\ngreat example, because composition would probably work better for integrating the\ngeometry managers into Widget, as discussed in “Favor Object Composition over\nClass Inheritance” on page 510.\nKeep in mind that Tkinter has been part of the standard library since Python 1.1 was\nreleased in 1994. Tkinter is a layer on top of the excellent Tk GUI toolkit of the Tcl\nlanguage. The Tcl/Tk combo is not originally object-oriented, so the Tk API is basi‐\ncally a vast catalog of functions. However, the toolkit is object-oriented in its design,\nif not in its original Tcl implementation.\nThe docstring of tkinter.Widget starts with the words “Internal class.” This suggests\nthat Widget should probably be an ABC. Although Widget has no methods of its own,\nit does define an interface. Its message is: “You can count on every Tkinter widget\nproviding basic widget methods (__init__, destroy, and dozens of Tk API func‐\ntions), in addition to the methods of all three geometry managers.” We can agree that\nthis is not a great interface definition (it’s just too broad), but it is an interface, and\nWidget “defines” it as the union of the interfaces of its superclasses.\nThe Tk class, which encapsulates the GUI application logic, inherits from Wm and\nMisc, neither of which are abstract or mixin (Wm is not a proper mixin because\nTopLevel subclasses only from it). The name of the Misc class is—by itself—a very\nstrong code smell. Misc has more than 100 methods, and all widgets inherit from it.\nWhy is it necessary that every single widget has methods for clipboard handling, text\nselection, timer management, and the like? You can’t really paste into a button or\nCoping with Inheritance \n| \n513",
      "content_length": 2817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "select text from a scrollbar. Misc should be split into several specialized mixin classes,\nand not all widgets should inherit from every one of those mixins.\nTo be fair, as a Tkinter user, you don’t need to know or use multiple inheritance at\nall. It’s an implementation detail hidden behind the widget classes that you will\ninstantiate or subclass in your own code. But you will suffer the consequences of\nexcessive multiple inheritance when you type dir(tkinter.Button) and try to find\nthe method you need among the 214 attributes listed. And you’ll need to face the\ncomplexity if you decide to implement a new Tk widget.\nDespite the problems, Tkinter is stable, flexible, and provides a\nmodern look-and-feel if you use the tkinter.ttk package and its\nthemed widgets. Also, some of the original widgets, like Canvas\nand Text, are incredibly powerful. You can turn a Canvas object\ninto a simple drag-and-drop drawing application in a matter of\nhours. Tkinter and Tcl/Tk are definitely worth a look if you are\ninterested in GUI programming.\nThis concludes our tour through the labyrinth of inheritance.\nChapter Summary\nThis chapter started with a review of the super() function in the context of single\ninheritance. We then discussed the problem with subclassing built-in types: their\nnative methods implemented in C do not call overridden methods in subclasses,\nexcept in very few special cases. That’s why, when we need a custom list, dict, or\nstr type, it’s easier to subclass UserList, UserDict, or UserString—all defined in the\ncollections module, which actually wrap the corresponding built-in types and dele‐\ngate operations to them—three examples of favoring composition over inheritance in\nthe standard library. If the desired behavior is very different from what the built-ins\noffer, it may be easier to subclass the appropriate ABC from collections.abc and\nwrite your own implementation.\nThe rest of the chapter was devoted to the double-edged sword of multiple inheri‐\ntance. First we saw how the method resolution order, encoded in the __mro__ class\nattribute, addresses the problem of potential naming conflicts in inherited methods.\nWe also saw how the super() built-in behaves, sometimes unexpectedly, in hierar‐\nchies with multiple inheritance. The behavior of super() is designed to support\nmixin classes, which we then studied through the simple example of the UpperCase\nMixin for case-insensitive mappings.\nWe saw how multiple inhertance and mixin methods are used in Python’s ABCs, as\nwell as in the socketserver threading and forking mixins. More complex uses of\n514 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "multiple inheritance were exemplified by Django’s class-based views and the Tkinter\nGUI toolkit. Although Tkinter is not an example of modern best practices, it is an\nexample of overly complex class hierarchies we may find in legacy systems.\nTo close the chapter, I presented seven recommendations to cope with inheritance,\nand applied some of that advice in a commentary of the Tkinter class hierarchy.\nRejecting inheritance—even single inheritance—is a modern trend. One of the most\nsuccessful languages created in the 21st century is Go. It doesn’t have a construct\ncalled “class,” but you can build types that are structs of encapsulated fields and you\ncan attach methods to those structs. Go allows the definition of interfaces that are\nchecked by the compiler using structural typing, a.k.a. static duck typing—very simi‐\nlar to what we now have with protocol types since Python 3.8. Go has special syntax\nfor building types and interfaces by composition, but it does not support inheritance\n—not even among interfaces.\nSo perhaps the best advice about inheritance is: avoid it if you can. But often, we\ndon’t have a choice: the frameworks we use impose their own design choices.\nFurther Reading\nWhen it comes to reading clarity, properly-done composition is superior to inheri‐\ntance. Since code is much more often read than written, avoid subclassing in general,\nbut especially don’t mix the various types of inheritance, and don’t use subclassing for\ncode sharing.\n—Hynek Schlawack, Subclassing in Python Redux\nDuring the final review of this book, technical reviewer Jürgen Gmach recommended\nHynek Schlawack’s post “Subclassing in Python Redux”—the source of the preceding\nquote. Schlawack is the author of the popular attrs package, and was a core contribu‐\ntor to the Twisted asynchronous programming framework, a project started by Glyph\nLefkowitz in 2002. Over time, the core team realized they had overused subclassing in\ntheir design, according to Schlawack. His post is long, and cites other important posts\nand talks. Highly recommended.\nIn that same conclusion, Hynek Schlawack wrote: “Don’t forget that more often than\nnot, a function is all you need.” I agree, and that is precisely why Fluent Python covers\nfunctions in depth before classes and inheritance. My goal was to show how much\nyou can accomplish with functions leveraging existing classes from the standard\nlibrary, before creating your own classes.\nSubclassing built-ins, the super function, and advanced features like descriptors and\nmetaclasses are all introduced in Guido van Rossum’s paper “Unifying types and\nclasses in Python 2.2”. Nothing really important has changed in these features since\nthen. Python 2.2 was an amazing feat of language evolution, adding several powerful\nFurther Reading \n| \n515",
      "content_length": 2789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "new features in a coherent whole, without breaking backward compatibility. The new\nfeatures were 100% opt-in. To use them, we just had to explicitly subclass object—\ndirectly or indirectly—to create a so-called “new style class.” In Python 3, every class\nsubclasses object.\nThe Python Cookbook, 3rd ed. by David Beazley and Brian K. Jones (O’Reilly) has\nseveral recipes showing the use of super() and mixin classes. You can start from the\nilluminating section “8.7. Calling a Method on a Parent Class”, and follow the inter‐\nnal references from there.\nRaymond Hettinger’s post “Python’s super() considered super!” explains the work‐\nings of super and multiple inheritance in Python from a positive perspective. It was\nwritten in response to “Python’s Super is nifty, but you can’t use it (Previously:\nPython’s Super Considered Harmful)” by James Knight. Martijn Pieters’ response to\n“How to use super() with one argument?” includes a concise and deep explanation of\nsuper, including its relationship with descriptors, a concept we’ll only study in Chap‐\nter 23. That’s the nature of super. It is simple to use in basic use cases, but is a power‐\nful and complex tool that touches some of Python’s most advanced dynamic features,\nrarely found in other languages.\nDespite the titles of those posts, the problem is not really the super built-in—which\nin Python 3 is not as ugly as it was in Python 2. The real issue is multiple inheritance,\nwhich is inherently complicated and tricky. Michele Simionato goes beyond criticiz‐\ning and actually offers a solution in his “Setting Multiple Inheritance Straight”: he\nimplements traits, an explict form of mixins that originated in the Self language. Sim‐\nionato has a long series of blog posts about multiple inheritance in Python, including\n“The wonders of cooperative inheritance, or using super in Python 3”; “Mixins con‐\nsidered harmful,” part 1 and part 2; and “Things to Know About Python Super,” part\n1, part 2, and part 3. The oldest posts use the Python 2 super syntax, but are still rele‐\nvant.\nI read the first edition of Grady Booch et al., Object-Oriented Analysis and Design, 3rd\ned., and highly recommend it as a general primer on object-oriented thinking, inde‐\npendent of programming language. It is a rare book that covers multiple inheritance\nwithout prejudice.\nNow more than ever it’s fashionable to avoid inheritance, so here are two references\nabout how to do that. Brandon Rhodes wrote “The Composition Over Inheritance\nPrinciple”, part of his excellent Python Design Patterns guide. Augie Fackler and\nNathaniel Manista presented “The End Of Object Inheritance & The Beginning Of A\nNew Modularity” at PyCon 2013. Fackler and Manista talk about organizing systems\naround interfaces and functions that handle objects implementing those interfaces,\navoiding the tight coupling and failure modes of classes and inheritance. That\nreminds me a lot of the Go way, but they advocate it for Python.\n516 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 3010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "15 Alan Kay, “The Early History of Smalltalk,” in SIGPLAN Not. 28, 3 (March 1993), 69–95. Also available\nonline. Thanks to my friend Christiano Anderson, who shared this reference as I was writing this chapter.\nSoapbox\nThink about the Classes You Really Need\n[We] started to push on the inheritance idea as a way to let novices build on frame‐\nworks that could only be designed by experts.\n—Alan Kay, “The Early History of Smalltalk”15\nThe vast majority of programmers write applications, not frameworks. Even those\nwho do write frameworks are likely to spend a lot (if not most) of their time writing\napplications. When we write applications, we normally don’t need to code class hier‐\narchies. At most, we write classes that subclass from ABCs or other classes provided\nby the framework. As application developers, it’s very rare that we need to write a\nclass that will act as the superclass of another. The classes we code are almost always\nleaf classes (i.e., leaves of the inheritance tree).\nIf, while working as an application developer, you find yourself building multilevel\nclass hierarchies, it’s likely that one or more of the following applies:\n• You are reinventing the wheel. Go look for a framework or library that provides\ncomponents you can reuse in your application.\n• You are using a badly designed framework. Go look for an alternative.\n• You are overengineering. Remember the KISS principle.\n• You became bored coding applications and decided to start a new framework.\nCongratulations and good luck!\nIt’s also possible that all of the above apply to your situation: you became bored and\ndecided to reinvent the wheel by building your own overengineered and badly\ndesigned framework, which is forcing you to code class after class to solve trivial\nproblems. Hopefully you are having fun, or at least getting paid for it.\nMisbehaving Built-Ins: Bug or Feature?\nThe built-in dict, list, and str types are essential building blocks of Python itself,\nso they must be fast—any performance issues in them would severely impact pretty\nmuch everything else. That’s why CPython adopted the shortcuts that cause its built-\nin methods to misbehave by not cooperating with methods overridden by subclasses.\nA possible way out of this dilemma would be to offer two implementations for each\nof those types: one “internal,” optimized for use by the interpreter, and an external,\neasily extensible one.\nFurther Reading \n| \n517",
      "content_length": 2429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "16 My friend and technical reviewer Leonardo Rochael explains better than I could: “The continued existence,\nbut persistent lack of arrival, of Perl 6 was draining willpower out of the evolution of Perl itself. Now Perl\ncontinues to be developed as a separate language (it’s up to version 5.34 as of now) with no shadow of depre‐\ncation because of the language formerly known as Perl 6.”\nBut wait, this is what we have already: UserDict, UserList, and UserString are not\nas fast as the built-ins but are easily extensible. The pragmatic approach taken by\nCPython means we also get to use, in our own applications, the highly optimized\nimplementations that are hard to subclass. Which makes sense, considering that it’s\nnot so often that we need a custom mapping, list, or string, but we use dict, list,\nand str every day. We just need to be aware of the trade-offs involved.\nInheritance Across Languages\nAlan Kay coined the term “object-oriented,” and Smalltalk had only single inheri‐\ntance, although there are forks with various forms of multiple inheritance support,\nincluding the modern Squeak and Pharo Smalltalk dialects that support traits—a lan‐\nguage construct that fulfills the role of a mixin class, while avoiding some of the issues\nwith multiple inheritance.\nThe first popular language to implement multiple inheritance was C++, and the fea‐\nture was abused enough that Java—intended as a C++ replacement—was designed\nwithout support for multiple inheritance of implementation (i.e., no mixin classes).\nThat is, until Java 8 introduced default methods that make interfaces very similar to\nthe abstract classes used to define interfaces in C++ and in Python. After Java, proba‐\nbly the most widely deployed JVM language is Scala, and it implements traits.\nOther languages supporting traits are the latest stable versions of PHP and Groovy, as\nwell as Rust and Raku—the language formerly known as Perl 6.16 So it’s fair to say\nthat traits are trendy in 2021.\nRuby offers an original take on multiple inheritance: it does not support it, but intro‐\nduces mixins as a language feature. A Ruby class can include a module in its body, so\nthe methods defined in the module become part of the class implementation. This is a\n“pure” form of mixin, with no inheritance involved, and it’s clear that a Ruby mixin\nhas no influence on the type of the class where it’s used. This provides the benefits of\nmixins, while avoiding many of its usual problems.\nTwo new object-oriented languages that are getting a lot of attention severely limit\ninheritance: Go and Julia. Both are about programming “objects,” and support\npolymorphism, but they avoid the term “class.”\nGo has no inheritance at all. Julia has a type hierarchy but subtypes cannot inherit\nstructure, only behaviors, and only abstract types can be subtyped. In addition, Julia\nmethods are implemented using multiple dispatch—a more advanced form of the\nmechanism we saw in “Single Dispatch Generic Functions” on page 324.\n518 \n| \nChapter 14: Inheritance: For Better or for Worse",
      "content_length": 3037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "1 From YouTube video of “A Language Creators’ Conversation: Guido van Rossum, James Gosling, Larry Wall,\nand Anders Hejlsberg,” streamed live on April 2, 2019. Quote starts at 1:32:05, edited for brevity. Full tran‐\nscript available at https://github.com/fluentpython/language-creators.\nCHAPTER 15\nMore About Type Hints\nI learned a painful lesson that for small programs, dynamic typing is great. For large\nprograms you need a more disciplined approach. And it helps if the language gives you\nthat discipline rather than telling you “Well, you can do whatever you want”.\n—Guido van Rossum, a fan of Monty Python1\nThis chapter is a sequel to Chapter 8, covering more of Python’s gradual type system.\nThe main topics are:\n• Overloaded function signatures\n• typing.TypedDict for type hinting dicts used as records\n• Type casting\n• Runtime access to type hints\n• Generic types\n— Declaring a generic class\n— Variance: invariant, covariant, and contravariant types\n— Generic static protocols\nWhat’s New in This Chapter\nThis chapter is new in the second edition of Fluent Python. Let’s start with overloads.\n519",
      "content_length": 1104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "Overloaded Signatures\nPython functions may accept different combinations of arguments. The @typ\ning.overload decorator allows annotating those different combinations. This is par‐\nticularly important when the return type of the function depends on the type of two\nor more parameters.\nConsider the sum built-in function. This is the text of help(sum):\n>>> help(sum)\nsum(iterable, /, start=0)\n    Return the sum of a 'start' value (default: 0) plus an iterable of numbers\n    When the iterable is empty, return the start value.\n    This function is intended specifically for use with numeric values and may\n    reject non-numeric types.\nThe sum built-in is written in C, but typeshed has overloaded type hints for it, in\nbuiltins.pyi:\n@overload\ndef sum(__iterable: Iterable[_T]) -> Union[_T, int]: ...\n@overload\ndef sum(__iterable: Iterable[_T], start: _S) -> Union[_T, _S]: ...\nFirst let’s look at the overall syntax of overloads. That’s all the code about the sum\nyou’ll find in the stub file (.pyi). The implementation would be in a different file. The\nellipsis (...) has no function other than to fulfill the syntactic requirement for a\nfunction body, similar to pass. So .pyi files are valid Python files.\nAs mentioned in “Annotating Positional Only and Variadic Parameters” on page 295,\nthe two leading underscores in __iterable are a PEP 484 convention for positional-\nonly arguments that is enforced by Mypy. It means you can call sum(my_list), but\nnot sum(__iterable = my_list).\nThe type checker tries to match the given arguments with each overloaded signature,\nin order. The call sum(range(100), 1000) doesn’t match the first overload, because\nthat signature has only one parameter. But it matches the second.\nYou can also use @overload in a regular Python module, by writing the overloaded\nsignatures right before the function’s actual signature and implementation.\nExample 15-1 shows how sum would appear annotated and implemented in a Python\nmodule.\nExample 15-1. mysum.py: definition of the sum function with overloaded signatures\nimport functools\nimport operator\nfrom collections.abc import Iterable\n520 \n| \nChapter 15: More About Type Hints",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "from typing import overload, Union, TypeVar\nT = TypeVar('T')\nS = TypeVar('S')  \n@overload\ndef sum(it: Iterable[T]) -> Union[T, int]: ...  \n@overload\ndef sum(it: Iterable[T], /, start: S) -> Union[T, S]: ...  \ndef sum(it, /, start=0):  \n    return functools.reduce(operator.add, it, start)\nWe need this second TypeVar in the second overload.\nThis signature is for the simple case: sum(my_iterable). The result type may be T\n—the type of the elements that my_iterable yields—or it may be int if the itera‐\nble is empty, because the default value of the start parameter is 0.\nWhen start is given, it can be of any type S, so the result type is Union[T, S].\nThis is why we need S. If we reused T, then the type of start would have to be\nthe same type as the elements of Iterable[T].\nThe signature of the actual function implementation has no type hints.\nThat’s a lot of lines to annotate a one-line function. Probably overkill, I know. At\nleast it wasn’t a foo function.\nIf you want to learn about @overload by reading code, typeshed has hundreds of\nexamples. On typeshed, the stub file for Python’s built-ins has 186 overloads as I write\nthis—more than any other in the standard library.\nTake Advantage of Gradual Typing\nAiming for 100% of annotated code may lead to type hints that add\nlots of noise but little value. Refactoring to simplify type hinting\ncan lead to cumbersome APIs. Sometimes it’s better to be prag‐\nmatic and leave a piece of code without type hints.\nThe handy APIs we call Pythonic are often hard to annotate. In the next section we’ll\nsee an example: six overloads are needed to properly annotate the flexible max built-in\nfunction.\nMax Overload\nIt is difficult to add type hints to functions that leverage the powerful dynamic fea‐\ntures of Python.\nOverloaded Signatures \n| \n521",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "While studying typeshed, I found bug report #4051: Mypy failed to warn that it is ille‐\ngal to pass None as one of the arguments to the built-in max() function, or to pass an\niterable that at some point yields None. In either case, you get a runtime exception like\nthis one:\nTypeError: '>' not supported between instances of 'int' and 'NoneType'\nThe documentation of max starts with this sentence:\nReturn the largest item in an iterable or the largest of two or more arguments.\nTo me, that’s a very intuitive description.\nBut if I must annotate a function described in those terms, I have to ask: which is it?\nAn iterable or two or more arguments?\nThe reality is more complicated because max also takes two optional keyword argu‐\nments: key and default.\nI coded max in Python to make it easier to see the relationship between how it works\nand the overloaded annotations (the built-in max is in C); see Example 15-2.\nExample 15-2. mymax.py: Python rewrite of max function\n# imports and definitions omitted, see next listing\nMISSING = object()\nEMPTY_MSG = 'max() arg is an empty sequence'\n# overloaded type hints omitted, see next listing\ndef max(first, *args, key=None, default=MISSING):\n    if args:\n        series = args\n        candidate = first\n    else:\n        series = iter(first)\n        try:\n            candidate = next(series)\n        except StopIteration:\n            if default is not MISSING:\n                return default\n            raise ValueError(EMPTY_MSG) from None\n    if key is None:\n        for current in series:\n            if candidate < current:\n                candidate = current\n    else:\n        candidate_key = key(candidate)\n        for current in series:\n            current_key = key(current)\n522 \n| \nChapter 15: More About Type Hints",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "2 I am grateful to Jelle Zijlstra—a typeshed maintainer—who taught me several things, including how to reduce\nmy original nine overloads to six.\n            if candidate_key < current_key:\n                candidate = current\n                candidate_key = current_key\n    return candidate\nThe focus of this example is not the logic of max, so I will not spend time with its\nimplementation, other than explaining MISSING. The MISSING constant is a unique\nobject instance used as a sentinel. It is the default value for the default= keyword\nargument, so that max can accept default=None and still distinguish between these\ntwo situations:\n1. The user did not provide a value for default=, so it is MISSING, and max raises\nValueError if first is an empty iterable.\n2. The user provided some value for default=, including None, so max returns that\nvalue if first is an empty iterable.\nTo fix issue #4051, I wrote the code in Example 15-3.2\nExample 15-3. mymax.py: top of the module, with imports, definitions, and overloads\nfrom collections.abc import Callable, Iterable\nfrom typing import Protocol, Any, TypeVar, overload, Union\nclass SupportsLessThan(Protocol):\n    def __lt__(self, other: Any) -> bool: ...\nT = TypeVar('T')\nLT = TypeVar('LT', bound=SupportsLessThan)\nDT = TypeVar('DT')\nMISSING = object()\nEMPTY_MSG = 'max() arg is an empty sequence'\n@overload\ndef max(__arg1: LT, __arg2: LT, *args: LT, key: None = ...) -> LT:\n    ...\n@overload\ndef max(__arg1: T, __arg2: T, *args: T, key: Callable[[T], LT]) -> T:\n    ...\n@overload\ndef max(__iterable: Iterable[LT], *, key: None = ...) -> LT:\n    ...\n@overload\ndef max(__iterable: Iterable[T], *, key: Callable[[T], LT]) -> T:\nOverloaded Signatures \n| \n523",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "...\n@overload\ndef max(__iterable: Iterable[LT], *, key: None = ...,\n        default: DT) -> Union[LT, DT]:\n    ...\n@overload\ndef max(__iterable: Iterable[T], *, key: Callable[[T], LT],\n        default: DT) -> Union[T, DT]:\n    ...\nMy Python implementation of max is about the same length as all those typing\nimports and declarations. Thanks to duck typing, my code has no isinstance\nchecks, and provides the same error checking as those type hints—but only at run‐\ntime, of course.\nA key benefit of @overload is declaring the return type as precisely as possible,\naccording to the types of the arguments given. We’ll see that benefit next by studying\nthe overloads for max in groups of one or two at a time.\nArguments implementing SupportsLessThan, but key and default not provided\n@overload\ndef max(__arg1: LT, __arg2: LT, *_args: LT, key: None = ...) -> LT:\n    ...\n# ... lines omitted ...\n@overload\ndef max(__iterable: Iterable[LT], *, key: None = ...) -> LT:\n    ...\nIn these cases, the inputs are either separate arguments of type LT implementing\nSupportsLessThan, or an Iterable of such items. The return type of max is the same\nas the actual arguments or items, as we saw in “Bounded TypeVar” on page 284.\nSample calls that match these overloads:\nmax(1, 2, -3)  # returns 2\nmax(['Go', 'Python', 'Rust'])  # returns 'Rust'\nArgument key provided, but no default\n@overload\ndef max(__arg1: T, __arg2: T, *_args: T, key: Callable[[T], LT]) -> T:\n    ...\n# ... lines omitted ...\n@overload\ndef max(__iterable: Iterable[T], *, key: Callable[[T], LT]) -> T:\n    ...\nThe inputs can be separate items of any type T or a single Iterable[T], and key=\nmust be a callable that takes an argument of the same type T, and returns a value that\n524 \n| \nChapter 15: More About Type Hints",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "implements SupportsLessThan. The return type of max is the same as the actual argu‐\nments.\nSample calls that match these overloads:\nmax(1, 2, -3, key=abs)  # returns -3\nmax(['Go', 'Python', 'Rust'], key=len)  # returns 'Python'\nArgument default provided, but no key\n@overload\ndef max(__iterable: Iterable[LT], *, key: None = ...,\n        default: DT) -> Union[LT, DT]:\n    ...\nThe input is an iterable of items of type LT implementing SupportsLessThan. The\ndefault= argument is the return value when the Iterable is empty. Therefore the\nreturn type of max must be a Union of type LT and the type of the default argument.\nSample calls that match these overloads:\nmax([1, 2, -3], default=0)  # returns 2\nmax([], default=None)  # returns None\nArguments key and default provided\n@overload\ndef max(__iterable: Iterable[T], *, key: Callable[[T], LT],\n        default: DT) -> Union[T, DT]:\n    ...\nThe inputs are:\n• An Iterable of items of any type T\n• Callable that takes an argument of type T and returns a value of type LT that\nimplements SupportsLessThan\n• A default value of any type DT\nThe return type of max must be a Union of type T or the type of the default\nargument:\nmax([1, 2, -3], key=abs, default=None)  # returns -3\nmax([], key=abs, default=None)  # returns None\nTakeaways from Overloading max\nType hints allow Mypy to flag a call like max([None, None]) with this error message:\nmymax_demo.py:109: error: Value of type variable \"_LT\" of \"max\"\n  cannot be \"None\"\nOverloaded Signatures \n| \n525",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "On the other hand, having to write so many lines to support the type checker may\ndiscourage people from writing convenient and flexible functions like max. If I had to\nreinvent the min function as well, I could refactor and reuse most of the implementa‐\ntion of max. But I’d have to copy and paste all overloaded declarations—even though\nthey would be identical for min, except for the function name.\nMy friend João S. O. Bueno—one of the smartest Python devs I know—tweeted this:\nAlthough it is this hard to express the signature of max—it fits in one’s mind quite\neasily. My understanding is that the expressiveness of annotation markings is very\nlimited, compared to that of Python.\nNow let’s study the TypedDict typing construct. It is not as useful as I imagined at\nfirst, but has its uses. Experimenting with TypedDict demonstrates the limitations of\nstatic typing for handling dynamic structures, such as JSON data.\nTypedDict\nIt’s tempting to use TypedDict to protect against errors while han‐\ndling dynamic data structures like JSON API responses. But the\nexamples here make clear that correct handling of JSON must be\ndone at runtime, and not with static type checking. For runtime\nchecking of JSON-like structures using type hints, check out the\npydantic package on PyPI.\nPython dictionaries are sometimes used as records, with the keys used as field names\nand field values of different types.\nFor example, consider a record describing a book in JSON or Python:\n{\"isbn\": \"0134757599\",\n \"title\": \"Refactoring, 2e\",\n \"authors\": [\"Martin Fowler\", \"Kent Beck\"],\n \"pagecount\": 478}\nBefore Python 3.8, there was no good way to annotate a record like that, because the\nmapping types we saw in “Generic Mappings” on page 276 limit all values to have the\nsame type.\nHere are two lame attempts to annotate a record like the preceding JSON object:\nDict[str, Any]\nThe values may be of any type.\n526 \n| \nChapter 15: More About Type Hints",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "Dict[str, Union[str, int, List[str]]]\nHard to read, and doesn’t preserve the relationship between field names and\ntheir respective field types: title is supposed to be a str, it can’t be an int or a\nList[str].\nPEP 589—TypedDict: Type Hints for Dictionaries with a Fixed Set of Keys addressed\nthat problem. Example 15-4 shows a simple TypedDict.\nExample 15-4. books.py: the BookDict definition\nfrom typing import TypedDict\nclass BookDict(TypedDict):\n    isbn: str\n    title: str\n    authors: list[str]\n    pagecount: int\nAt first glance, typing.TypedDict may seem like a data class builder, similar to\ntyping.NamedTuple—covered in Chapter 5.\nThe syntactic similarity is misleading. TypedDict is very different. It exists only for\nthe benefit of type checkers, and has no runtime effect.\nTypedDict provides two things:\n• Class-like syntax to annotate a dict with type hints for the value of each “field.”\n• A constructor that tells the type checker to expect a dict with the keys and values\nas specified.\nAt runtime, a TypedDict constructor such as BookDict is a placebo: it has the same\neffect as calling the dict constructor with the same arguments.\nThe fact that BookDict creates a plain dict also means that:\n• The “fields” in the pseudoclass definition don’t create instance attributes.\n• You can’t write initializers with default values for the “fields.”\n• Method definitions are not allowed.\nLet’s explore the behavior of a BookDict at runtime (Example 15-5).\nExample 15-5. Using a BookDict, but not quite as intended\n>>> from books import BookDict\n>>> pp = BookDict(title='Programming Pearls',  \n...               authors='Jon Bentley',  \nTypedDict \n| \n527",
      "content_length": 1662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "...               isbn='0201657880',\n...               pagecount=256)\n>>> pp  \n{'title': 'Programming Pearls', 'authors': 'Jon Bentley', 'isbn': '0201657880',\n 'pagecount': 256}\n>>> type(pp)\n<class 'dict'>\n>>> pp.title  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'dict' object has no attribute 'title'\n>>> pp['title']\n'Programming Pearls'\n>>> BookDict.__annotations__  \n{'isbn': <class 'str'>, 'title': <class 'str'>, 'authors': typing.List[str],\n 'pagecount': <class 'int'>}\nYou can call BookDict like a dict constructor with keyword arguments, or pass‐\ning a dict argument—including a dict literal.\nOops…I forgot authors takes a list. But gradual typing means no type checking\nat runtime.\nThe result of calling BookDict is a plain dict…\n…therefore you can’t read the data using object.field notation.\nThe type hints are in BookDict.__annotations__, and not in pp.\nWithout a type checker, TypedDict is as useful as comments: it may help people read\nthe code, but that’s it. In contrast, the class builders from Chapter 5 are useful even if\nyou don’t use a type checker, because at runtime they generate or enhance a custom\nclass that you can instantiate. They also provide several useful methods or functions\nlisted in Table 5-1.\nExample 15-6 builds a valid BookDict and tries some operations on it. This shows\nhow TypedDict enables Mypy to catch errors, shown in Example 15-7.\nExample 15-6. demo_books.py: legal and illegal operations on a BookDict\nfrom books import BookDict\nfrom typing import TYPE_CHECKING\ndef demo() -> None:  \n    book = BookDict(  \n        isbn='0134757599',\n        title='Refactoring, 2e',\n        authors=['Martin Fowler', 'Kent Beck'],\n528 \n| \nChapter 15: More About Type Hints",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "pagecount=478\n    )\n    authors = book['authors'] \n    if TYPE_CHECKING:  \n        reveal_type(authors)  \n    authors = 'Bob'  \n    book['weight'] = 4.2\n    del book['title']\nif __name__ == '__main__':\n    demo()\nRemember to add a return type, so that Mypy doesn’t ignore the function.\nThis is a valid BookDict: all the keys are present, with values of the correct types.\nMypy will infer the type of authors from the annotation for the 'authors' key\nin BookDict.\ntyping.TYPE_CHECKING is only True when the program is being type checked. At\nruntime, it’s always false.\nThe previous if statement prevents reveal_type(authors) from being called at\nruntime. reveal_type is not a runtime Python function, but a debugging facility\nprovided by Mypy. That’s why there is no import for it. See its output in\nExample 15-7.\nThe last three lines of the demo function are illegal. They will cause error mes‐\nsages in Example 15-7.\nType checking demo_books.py from Example 15-6, we get Example 15-7.\nExample 15-7. Type checking demo_books.py\n…/typeddict/ $ mypy demo_books.py\ndemo_books.py:13: note: Revealed type is 'built-ins.list[built-ins.str]'  \ndemo_books.py:14: error: Incompatible types in assignment\n                  (expression has type \"str\", variable has type \"List[str]\")  \ndemo_books.py:15: error: TypedDict \"BookDict\" has no key 'weight'  \ndemo_books.py:16: error: Key 'title' of TypedDict \"BookDict\" cannot be deleted  \nFound 3 errors in 1 file (checked 1 source file)\nTypedDict \n| \n529",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "3 As of May 2020, pytype allows it. But its FAQ says it will be disallowed in the future. See the question, “Why\ndidn’t pytype catch that I changed the type of an annotated variable?” in the pytype FAQ.\n4 I prefer to use the lxml package to generate and parse XML: it’s easy to get started, full-featured, and fast.\nUnfortunately, lxml and Python’s own ElementTree don’t fit the limited RAM of my hypothetical microcon‐\ntroller.\nThis note is the result of reveal_type(authors).\nThe type of the authors variable was inferred from the type of the book['au\nthors'] expression that initialized it. You can’t assign a str to a variable of type\nList[str]. Type checkers usually don’t allow the type of a variable to change.3\nCannot assign to a key that is not part of the BookDict definition.\nCannot delete a key that is part of the BookDict definition.\nNow let’s see BookDict used in function signatures, to type check function calls.\nImagine you need to generate XML from book records, similar to this:\n<BOOK>\n  <ISBN>0134757599</ISBN>\n  <TITLE>Refactoring, 2e</TITLE>\n  <AUTHOR>Martin Fowler</AUTHOR>\n  <AUTHOR>Kent Beck</AUTHOR>\n  <PAGECOUNT>478</PAGECOUNT>\n</BOOK>\nIf you were writing MicroPython code to be embedded in a tiny microcontroller, you\nmight write a function like what’s shown in Example 15-8.4\nExample 15-8. books.py: to_xml function\nAUTHOR_ELEMENT = '<AUTHOR>{}</AUTHOR>'\ndef to_xml(book: BookDict) -> str:  \n    elements: list[str] = []  \n    for key, value in book.items():\n        if isinstance(value, list):  \n            elements.extend(\n                AUTHOR_ELEMENT.format(n) for n in value)  \n        else:\n            tag = key.upper()\n            elements.append(f'<{tag}>{value}</{tag}>')\n    xml = '\\n\\t'.join(elements)\n    return f'<BOOK>\\n\\t{xml}\\n</BOOK>'\n530 \n| \nChapter 15: More About Type Hints",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "5 The Mypy documentation discusses this in its “Common issues and solutions” page, in the section, “Types of\nempty collections”.\n6 Brett Cannon, Guido van Rossum, and others have been discussing how to type hint json.loads() since\n2016 in Mypy issue #182: Define a JSON type.\nThe whole point of the example: using BookDict in the function signature.\nIt’s often necessary to annotate collections that start empty, otherwise Mypy can’t\ninfer the type of the elements.5\nMypy understands isinstance checks, and treats value as a list in this block.\nWhen I used key == 'authors' as the condition for the if guarding this block,\nMypy found an error in this line: \"object\" has no attribute \"__iter__\",\nbecause it inferred the type of value returned from book.items() as object,\nwhich doesn’t support the __iter__ method required by the generator expres‐\nsion. With the isinstance check, this works because Mypy knows that value is a\nlist in this block.\nExample 15-9 shows a function that parses a JSON str and returns a BookDict.\nExample 15-9. books_any.py: from_json function\ndef from_json(data: str) -> BookDict:\n    whatever = json.loads(data)  \n    return whatever  \nThe return type of json.loads() is Any.6\nI can return whatever—of type Any—because Any is consistent-with every type,\nincluding the declared return type, BookDict.\nThe second point of Example 15-9 is very important to keep in mind: Mypy will not\nflag any problem in this code, but at runtime the value in whatever may not conform\nto the BookDict structure—in fact, it may not be a dict at all!\nIf you run Mypy with --disallow-any-expr, it will complain about the two lines in\nthe body of from_json:\n…/typeddict/ $ mypy books_any.py --disallow-any-expr\nbooks_any.py:30: error: Expression has type \"Any\"\nbooks_any.py:31: error: Expression has type \"Any\"\nFound 2 errors in 1 file (checked 1 source file)\nTypedDict \n| \n531",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "Lines 30 and 31 mentioned in the previous snippet are the body of the from_json\nfunction. We can silence the type error by adding a type hint to the initialization of\nthe whatever variable, as in Example 15-10.\nExample 15-10. books.py: from_json function with variable annotation\ndef from_json(data: str) -> BookDict:\n    whatever: BookDict = json.loads(data)  \n    return whatever  \n--disallow-any-expr does not cause errors when an expression of type Any is\nimmediately assigned to a variable with a type hint.\nNow whatever is of type BookDict, the declared return type.\nDon’t be lulled into a false sense of type safety by Example 15-10!\nLooking at the code at rest, the type checker cannot predict that\njson.loads() will return anything that resembles a BookDict.\nOnly runtime validation can guarantee that.\nStatic type checking is unable to prevent errors with code that is inherently dynamic,\nsuch as json.loads(), which builds Python objects of different types at runtime, as\nExamples 15-11, 15-12, and 15-13 demonstrate.\nExample 15-11. demo_not_book.py: from_json returns an invalid BookDict, and\nto_xml accepts it\nfrom books import to_xml, from_json\nfrom typing import TYPE_CHECKING\ndef demo() -> None:\n    NOT_BOOK_JSON = \"\"\"\n        {\"title\": \"Andromeda Strain\",\n         \"flavor\": \"pistachio\",\n         \"authors\": true}\n    \"\"\"\n    not_book = from_json(NOT_BOOK_JSON)  \n    if TYPE_CHECKING:  \n        reveal_type(not_book)\n        reveal_type(not_book['authors'])\n    print(not_book)  \n    print(not_book['flavor'])  \n    xml = to_xml(not_book)  \n    print(xml)  \n532 \n| \nChapter 15: More About Type Hints",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "if __name__ == '__main__':\n    demo()\nThis line does not produce a valid BookDict—see the content of NOT_BOOK_JSON.\nLet’s have Mypy reveal a couple of types.\nThis should not be a problem: print can handle object and every other type.\nBookDict has no 'flavor' key, but the JSON source does…what will happen?\nRemember the signature: def to_xml(book: BookDict) -> str:.\nWhat will the XML output look like?\nNow we check demo_not_book.py with Mypy (Example 15-12).\nExample 15-12. Mypy report for demo_not_book.py, reformatted for clarity\n…/typeddict/ $ mypy demo_not_book.py\ndemo_not_book.py:12: note: Revealed type is\n   'TypedDict('books.BookDict', {'isbn': built-ins.str,\n                                 'title': built-ins.str,\n                                 'authors': built-ins.list[built-ins.str],\n                                 'pagecount': built-ins.int})'  \ndemo_not_book.py:13: note: Revealed type is 'built-ins.list[built-ins.str]'  \ndemo_not_book.py:16: error: TypedDict \"BookDict\" has no key 'flavor'  \nFound 1 error in 1 file (checked 1 source file)\nThe revealed type is the nominal type, not the runtime content of not_book.\nAgain, this is the nominal type of not_book['authors'], as defined in BookDict.\nNot the runtime type.\nThis error is for line print(not_book['flavor']): that key does not exist in the\nnominal type.\nNow let’s run demo_not_book.py, showing the output in Example 15-13.\nExample 15-13. Output of running demo_not_book.py\n…/typeddict/ $ python3 demo_not_book.py\n{'title': 'Andromeda Strain', 'flavor': 'pistachio', 'authors': True}  \npistachio  \n<BOOK>  \nTypedDict \n| \n533",
      "content_length": 1606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "<TITLE>Andromeda Strain</TITLE>\n        <FLAVOR>pistachio</FLAVOR>\n        <AUTHORS>True</AUTHORS>\n</BOOK>\nThis is not really a BookDict.\nThe value of not_book['flavor'].\nto_xml takes a BookDict argument, but there is no runtime checking: garbage in,\ngarbage out.\nExample 15-13 shows that demo_not_book.py outputs nonsense, but has no runtime\nerrors. Using a TypedDict while handling JSON data did not provide much type\nsafety.\nIf you look at the code for to_xml in Example 15-8 through the lens of duck typing,\nthe argument book must provide an .items() method that returns an iterable of\ntuples like (key, value) where:\n• key must have an .upper() method\n• value can be anything\nThe point of this demonstration: when handling data with a dynamic structure, such\nas JSON or XML, TypedDict is absolutely not a replacement for data validation at\nruntime. For that, use pydantic.\nTypedDict has more features, including support for optional keys, a limited form of\ninheritance, and an alternative declaration syntax. If you want to know more about it,\nplease review PEP 589—TypedDict: Type Hints for Dictionaries with a Fixed Set of\nKeys.\nNow let’s turn our attention to a function that is best avoided, but sometimes is\nunavoidable: typing.cast.\nType Casting\nNo type system is perfect, and neither are the static type checkers, the type hints in\nthe typeshed project, or the type hints in the third-party packages that have them.\nThe typing.cast() special function provides one way to handle type checking mal‐\nfunctions or incorrect type hints in code we can’t fix. The Mypy 0.930 documentation\nexplains:\nCasts are used to silence spurious type checker warnings and give the type checker a\nlittle help when it can’t quite understand what is going on.\n534 \n| \nChapter 15: More About Type Hints",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "7 The use of enumerate in the example is intended to confuse the type checker. A simpler implementation\nyielding strings directly instead of going through the enumerate index is correctly analyzed by Mypy, and the\ncast() is not needed.\nAt runtime, typing.cast does absolutely nothing. This is its implementation:\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\nPEP 484 requires type checkers to “blindly believe” the type stated in the cast. The\n“Casts” section of PEP 484 gives an example where the type checker needs the guid‐\nance of cast:\nfrom typing import cast\ndef find_first_str(a: list[object]) -> str:\n    index = next(i for i, x in enumerate(a) if isinstance(x, str))\n    # We only get here if there's at least one string\n    return cast(str, a[index])\nThe next() call on the generator expression will either return the index of a str item\nor raise StopIteration. Therefore, find_first_str will always return a str if no\nexception is raised, and str is the declared return type.\nBut if the last line were just return a[index], Mypy would infer the return type as\nobject because the a argument is declared as list[object]. So the cast() is\nrequired to guide Mypy.7\nHere is another example with cast, this time to correct an outdated type hint for\nPython’s standard library. In Example 21-12, I create an asyncio Server object and I\nwant to get the address the server is listening to. I coded this line:\naddr = server.sockets[0].getsockname()\nBut Mypy reported this error:\nValue of type \"Optional[List[socket]]\" is not indexable\nThe type hint for Server.sockets on typeshed in May 2021 is valid for Python 3.6,\nwhere the sockets attribute could be None. But in Python 3.7, sockets became a\nproperty with a getter that always returns a list—which may be empty if the server\nhas no sockets. And since Python 3.8, the getter returns a tuple (used as an immuta‐\nble sequence).\nType Casting \n| \n535",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "8 I reported typeshed issue #5535, “Wrong type hint for asyncio.base_events.Server sockets attribute.” and it\nwas quickly fixed by Sebastian Rittau. However, I decided to keep the example because it illustrates a com‐\nmon use case for cast, and the cast I wrote is harmless.\n9 To be honest, I originally appended a # type: ignore comment to the line with server.sockets[0] because\nafter a little research I found similar lines the asyncio documentation and in a test case, so I suspected the\nproblem was not in my code.\n10 19 May 2020 message to the typing-sig mailing list.\nSince I can’t fix typeshed right now,8 I added a cast, like this:\nfrom asyncio.trsock import TransportSocket\nfrom typing import cast\n# ... many lines omitted ...\n    socket_list = cast(tuple[TransportSocket, ...], server.sockets)\n    addr = socket_list[0].getsockname()\nUsing cast in this case required a couple of hours to understand the problem and\nread asyncio source code to find the correct type of the sockets: the TransportSocket\nclass from the undocumented asyncio.trsock module. I also had to add two import\nstatements and another line of code for readability.9 But the code is safer.\nThe careful reader may note that sockets[0] could raise IndexError if sockets is\nempty. However, as far as I understand asyncio, that cannot happen in\nExample 21-12 because the server is ready to accept connections by the time I read\nits sockets attribute, therefore it will not be empty. Anyway, IndexError is a runtime\nerror. Mypy can’t spot the problem even in a trivial case like print([][0]).\nDon’t get too comfortable using cast to silence Mypy, because\nMypy is usually right when it reports an error. If you are using\ncast very often, that’s a code smell. Your team may be misusing\ntype hints, or you may have low-quality dependencies in your\ncodebase.\nDespite the downsides, there are valid uses for cast. Here is something Guido van\nRossum wrote about it:\nWhat’s wrong with the occasional cast() call or # type: ignore comment?10\n536 \n| \nChapter 15: More About Type Hints",
      "content_length": 2049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "11 The syntax # type: ignore[code] allows you to specify which Mypy error code is being silenced, but the\ncodes are not always easy to interpret. See “Error codes” in the Mypy documentation.\n12 I will not go into the implementation of clip, but you can read the whole module in clip_annot.py if you’re\ncurious.\nIt is unwise to completely ban the use of cast, especially because the other work‐\narounds are worse:\n• # type: ignore is less informative.11\n• Using Any is contagious: since Any is consistent-with all types, abusing it may pro‐\nduce cascading effects through type inference, undermining the type checker’s\nability to detect errors in other parts of the code.\nOf course, not all typing mishaps can be fixed with cast. Sometimes we need #\ntype: ignore, the occasional Any, or even leaving a function without type hints.\nNext, let’s talk about using annotations at runtime.\nReading Type Hints at Runtime\nAt import time, Python reads the type hints in functions, classes, and modules, and\nstores them in attributes named __annotations__. For instance, consider the clip\nfunction in Example 15-14.12\nExample 15-14. clipannot.py: annotated signature of a clip function\ndef clip(text: str, max_len: int = 80) -> str:\nThe type hints are stored as a dict in the __annotations__ attribute of the function:\n>>> from clip_annot import clip\n>>> clip.__annotations__\n{'text': <class 'str'>, 'max_len': <class 'int'>, 'return': <class 'str'>}\nThe 'return' key maps to the return type hint after the -> symbol in Example 15-14.\nNote that the annotations are evaluated by the interpreter at import time, just as\nparameter default values are also evaluated. That’s why the values in the annotations\nare the Python classes str and int, and not the strings 'str' and 'int'. The import\ntime evaluation of annotations is the standard as of Python 3.10, but that may change\nif PEP 563 or PEP 649 become the standard behavior.\nReading Type Hints at Runtime \n| \n537",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "Problems with Annotations at Runtime\nThe increased use of type hints raised two problems:\n• Importing modules uses more CPU and memory when many type hints are used.\n• Referring to types not yet defined requires using strings instead of actual types.\nBoth issues are relevant. The first is because of what we just saw: annotations are\nevaluated by the interpreter at import time and stored in the __annotations__\nattribute. Let’s focus now on the second issue.\nStoring annotations as strings is sometimes required because of the “forward refer‐\nence” problem: when a type hint needs to refer to a class defined below in the same\nmodule. However, a common manifestation of the problem in source code doesn’t\nlook like a forward reference at all: that’s when a method returns a new object of the\nsame class. Since the class object is not defined until Python completely evaluates the\nclass body, type hints must use the name of the class as a string. Here is an example:\nclass Rectangle:\n    # ... lines omitted ...\n    def stretch(self, factor: float) -> 'Rectangle':\n        return Rectangle(width=self.width * factor)\nWriting forward referencing type hints as strings is the standard and required prac‐\ntice as of Python 3.10. Static type checkers were designed to deal with that issue from\nthe beginning.\nBut at runtime, if you write code to read the return annotation for stretch, you will\nget a string 'Rectangle' instead of a reference to the actual type, the Rectangle class.\nNow your code needs to figure out what that string means.\nThe typing module includes three functions and a class categorized as Introspection\nhelpers, the most important being typing.get_type_hints. Part of its documenta‐\ntion states:\nget_type_hints(obj, globals=None, locals=None, include_extras=False)\n[…] This is often the same as obj.__annotations__. In addition, forward refer‐\nences encoded as string literals are handled by evaluating them in globals and\nlocals namespaces. […]\nSince Python 3.10, the new inspect.get_annotations(…) func‐\ntion should be used instead of typing.get_type_hints. However,\nsome readers may not be using Python 3.10 yet, so in the examples\nI’ll use typing.get_type_hints, which is available since the typ\ning module was added in Python 3.5.\n538 \n| \nChapter 15: More About Type Hints",
      "content_length": 2298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "PEP 563—Postponed Evaluation of Annotations was approved to make it unneces‐\nsary to write annotations as strings, and to reduce the runtime costs of type hints. Its\nmain idea is described in these two sentences of the “Abstract”:\nThis PEP proposes changing function annotations and variable annotations so that\nthey are no longer evaluated at function definition time. Instead, they are preserved in\nannotations in string form.\nBeginning with Python 3.7, that’s how annotations are handled in any module that\nstarts with this import statement:\nfrom __future__ import annotations\nTo demonstrate its effect, I put a copy of the same clip function from Example 15-14\nin a clip_annot_post.py module with that __future__ import line at the top.\nAt the console, here’s what I get when I import that module and read the annotations\nfrom clip:\n>>> from clip_annot_post import clip\n>>> clip.__annotations__\n{'text': 'str', 'max_len': 'int', 'return': 'str'}\nAs you can see, all the type hints are now plain strings, despite the fact they are not\nwritten as quoted strings in the definition of clip (Example 15-14).\nThe typing.get_type_hints function is able to resolve many type hints, including\nthose in clip:\n>>> from clip_annot_post import clip\n>>> from typing import get_type_hints\n>>> get_type_hints(clip)\n{'text': <class 'str'>, 'max_len': <class 'int'>, 'return': <class 'str'>}\nCalling get_type_hints gives us the real types—even in some cases where the origi‐\nnal type hint is written as a quoted string. That’s the recommended way to read type\nhints at runtime.\nThe PEP 563 behavior was scheduled to become default in Python 3.10, with no\n__future__ import needed. However, the maintainers of FastAPI and pydantic raised\nthe alarm that the change would break their code which relies on type hints at run‐\ntime, and cannot use get_type_hints reliably.\nIn the ensuing discussion on the python-dev mailing list, Łukasz Langa—the author\nof PEP 563—described some limitations of that function:\n[…] it turned out that typing.get_type_hints() has limits that make its use in gen‐\neral costly at runtime, and more importantly insufficient to resolve all types. The most\ncommon example deals with non-global context in which types are generated (e.g.,\ninner classes, classes within functions, etc.). But one of the crown examples of forward\nreferences: classes with methods accepting or returning objects of their own type, also\nReading Type Hints at Runtime \n| \n539",
      "content_length": 2459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "13 Message “PEP 563 in light of PEP 649”, posted April 16, 2021.\nisn’t properly handled by typing.get_type_hints() if a class generator is used.\nThere’s some trickery we can do to connect the dots but in general it’s not great.13\nPython’s Steering Council decided to postpone making PEP 563 the default behavior\nuntil Python 3.11 or later, giving more time to developers to come up with a solution\nthat addresses the issues PEP 563 tried to solve, without breaking widespread uses of\ntype hints at runtime. PEP 649—Deferred Evaluation Of Annotations Using Descrip‐\ntors is under consideration as a possible solution, but a different compromise may be\nreached.\nTo summarize: reading type hints at runtime is not 100% reliable as of Python 3.10\nand is likely to change in 2022.\nCompanies using Python at a very large scale want the benefits of\nstatic typing, but they don’t want to pay the price for the evaluation\nof the type hints at import time. Static checking happens at devel‐\noper workstations and dedicated CI servers, but loading modules\nhappens at a much higher frequency and volume in the production\ncontainers, and this cost is not negligible at scale.\nThis creates tension in the Python community between those who\nwant type hints to be stored as strings only—to reduce the loading\ncosts—versus those who also want to use type hints at runtime, like\nthe creators and users of pydantic and FastAPI, who would rather\nhave type objects stored instead of having to evaluate those annota‐\ntions, a challenging task.\nDealing with the Problem\nGiven the unstable situation at present, if you need to read annotations at runtime, I\nrecommend:\n• Avoid reading __annotations__ directly; instead, use inspect.get_annota\ntions (from Python 3.10) or typing.get_type_hints (since Python 3.5).\n• Write a custom function of your own as a thin wrapper around in spect\n.get_annotations or typing.get_type_hints, and have the rest of your code‐\nbase call that custom function, so that future changes are localized to a single\nfunction.\nTo demonstrate the second point, here are the first lines of the Checked class defined\nin Example 24-5, which we’ll study in Chapter 24:\n540 \n| \nChapter 15: More About Type Hints",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "class Checked:\n    @classmethod\n    def _fields(cls) -> dict[str, type]:\n        return get_type_hints(cls)\n    # ... more lines ...\nThe Checked._fields class method protects other parts of the module from depend‐\ning directly on typing.get_type_hints. If get_type_hints changes in the future,\nrequiring additional logic, or you want to replace it with inspect.get_annotations,\nthe change is limited to Checked._fields and does not affect the rest of your\nprogram.\nGiven the ongoing discussions and proposed changes for runtime\ninspection of type hints, the official “Annotations Best Practices”\ndocument is required reading, and is likely to be updated on the\nroad to Python 3.11. That how-to was written by Larry Hastings,\nthe author of PEP 649—Deferred Evaluation Of Annotations Using\nDescriptors, an alternative proposal to address the runtime issues\nraised by PEP 563—Postponed Evaluation of Annotations.\nThe remaining sections of this chapter cover generics, starting with how to define a\ngeneric class that can be parameterized by its users.\nImplementing a Generic Class\nIn Example 13-7 we defined the Tombola ABC: an interface for classes that work like\na bingo cage. The LottoBlower class from Example 13-10 is a concrete implementa‐\ntion. Now we’ll study a generic version of LottoBlower used like in Example 15-15.\nExample 15-15. generic_lotto_demo.py: using a generic lottery blower class\nfrom generic_lotto import LottoBlower\nmachine = LottoBlower[int](range(1, 11))  \nfirst = machine.pick()  \nremain = machine.inspect()  \nTo instantiate a generic class, we give it an actual type parameter, like int here.\nMypy will correctly infer that first is an int…\n… and that remain is a tuple of integers.\nImplementing a Generic Class \n| \n541",
      "content_length": 1746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "In addition, Mypy reports violations of the parameterized type with helpful messages,\nsuch as what’s shown in Example 15-16.\nExample 15-16. generic_lotto_errors.py: errors reported by Mypy\nfrom generic_lotto import LottoBlower\nmachine = LottoBlower[int]([1, .2])\n## error: List item 1 has incompatible type \"float\";  \n##        expected \"int\"\nmachine = LottoBlower[int](range(1, 11))\nmachine.load('ABC')\n## error: Argument 1 to \"load\" of \"LottoBlower\"  \n##        has incompatible type \"str\";\n##        expected \"Iterable[int]\"\n## note:  Following member(s) of \"str\" have conflicts:\n## note:      Expected:\n## note:          def __iter__(self) -> Iterator[int]\n## note:      Got:\n## note:          def __iter__(self) -> Iterator[str]\nUpon instantiation of LottoBlower[int], Mypy flags the float.\nWhen calling .load('ABC'), Mypy explains why a str won’t do: str.__iter__\nreturns an Iterator[str], but LottoBlower[int] requires an Iterator[int].\nExample 15-17 is the implementation.\nExample 15-17. generic_lotto.py: a generic lottery blower class\nimport random\nfrom collections.abc import Iterable\nfrom typing import TypeVar, Generic\nfrom tombola import Tombola\nT = TypeVar('T')\nclass LottoBlower(Tombola, Generic[T]):  \n    def __init__(self, items: Iterable[T]) -> None:  \n        self._balls = list[T](items)\n    def load(self, items: Iterable[T]) -> None:  \n        self._balls.extend(items)\n542 \n| \nChapter 15: More About Type Hints",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "def pick(self) -> T:  \n        try:\n            position = random.randrange(len(self._balls))\n        except ValueError:\n            raise LookupError('pick from empty LottoBlower')\n        return self._balls.pop(position)\n    def loaded(self) -> bool:  \n        return bool(self._balls)\n    def inspect(self) -> tuple[T, ...]:  \n        return tuple(self._balls)\nGeneric class declarations often use multiple inheritance, because we need to\nsubclass Generic to declare the formal type parameters—in this case, T.\nThe items argument in __init__ is of type Iterable[T], which becomes Itera\nble[int] when an instance is declared as LottoBlower[int].\nThe load method is likewise constrained.\nThe return type of T now becomes int in a LottoBlower[int].\nNo type variable here.\nFinally, T sets the type of the items in the returned tuple.\nThe “User-defined generic types” section of the typing module\ndocumentation is short, presents good examples, and provides a\nfew more details that I do not cover here.\nNow that we’ve seen how to implement a generic class, let’s define the terminology to\ntalk about generics.\nImplementing a Generic Class \n| \n543",
      "content_length": 1144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "14 The terms are from Joshua Bloch’s classic book, Effective Java, 3rd ed. (Addison-Wesley). The definitions and\nexamples are mine.\nBasic Jargon for Generic Types\nHere are a few definitions that I found useful when studying generics:14\nGeneric type\nA type declared with one or more type variables.\nExamples: LottoBlower[T], abc.Mapping[KT, VT]\nFormal type parameter\nThe type variables that appear in a generic type declaration.\nExample: KT and VT in the previous example abc.Mapping[KT, VT]\nParameterized type\nA type declared with actual type parameters.\nExamples: LottoBlower[int], abc.Mapping[str, float]\nActual type parameter\nThe actual types given as parameters when a parameterized type is declared.\nExample: the int in LottoBlower[int]\nThe next topic is about how to make generic types more flexible, introducing the\nconcepts of covariance, contravariance, and invariance.\nVariance\nDepending on your experience with generics in other languages,\nthis may be the most challenging section in the book. The concept\nof variance is abstract, and a rigorous presentation would make\nthis section look like pages from a math book.\nIn practice, variance is mostly relevant to library authors who want\nto support new generic container types or provide callback-based\nAPIs. Even then, you can avoid much complexity by supporting\nonly invariant containers—which is mostly what we have now in\nthe Python standard library. So, on a first reading, you can skip the\nwhole section or just read the sections about invariant types.\nWe first saw the concept of variance in “Variance in Callable types” on page 292,\napplied to parameterized generic Callable types. Here we’ll expand the concept to\ncover generic collection types, using a “real world” analogy to make this abstract con‐\ncept more concrete.\n544 \n| \nChapter 15: More About Type Hints",
      "content_length": 1831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "15 I first saw the cafeteria analogy for variance in Erik Meijer’s Foreword in The Dart Programming Language\nbook by Gilad Bracha (Addison-Wesley).\n16 Much better than banning books!\nImagine that a school cafeteria has a rule that only juice dispensers can be installed.15\nGeneral beverage dispensers are not allowed because they may serve sodas, which are\nbanned by the school board.16\nAn Invariant Dispenser\nLet’s try to model the cafeteria scenario with a generic BeverageDispenser class that\ncan be parameterized on the type of beverage. See Example 15-18.\nExample 15-18. invariant.py: type definitions and install function\nfrom typing import TypeVar, Generic\nclass Beverage:  \n    \"\"\"Any beverage.\"\"\"\nclass Juice(Beverage):\n    \"\"\"Any fruit juice.\"\"\"\nclass OrangeJuice(Juice):\n    \"\"\"Delicious juice from Brazilian oranges.\"\"\"\nT = TypeVar('T')  \nclass BeverageDispenser(Generic[T]):  \n    \"\"\"A dispenser parameterized on the beverage type.\"\"\"\n    def __init__(self, beverage: T) -> None:\n        self.beverage = beverage\n    def dispense(self) -> T:\n        return self.beverage\ndef install(dispenser: BeverageDispenser[Juice]) -> None:  \n    \"\"\"Install a fruit juice dispenser.\"\"\"\nBeverage, Juice, and OrangeJuice form a type hierarchy.\nSimple TypeVar declaration.\nBeverageDispenser is parameterized on the type of beverage.\nVariance \n| \n545",
      "content_length": 1347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "install is a module-global function. Its type hint enforces the rule that only a\njuice dispenser is acceptable.\nGiven the definitions in Example 15-18, the following code is legal:\njuice_dispenser = BeverageDispenser(Juice())\ninstall(juice_dispenser)\nHowever, this is not legal:\nbeverage_dispenser = BeverageDispenser(Beverage())\ninstall(beverage_dispenser)\n## mypy: Argument 1 to \"install\" has\n## incompatible type \"BeverageDispenser[Beverage]\"\n##          expected \"BeverageDispenser[Juice]\"\nA dispenser that serves any Beverage is not acceptable because the cafeteria requires\na dispenser that is specialized for Juice.\nSomewhat surprisingly, this code is also illegal:\norange_juice_dispenser = BeverageDispenser(OrangeJuice())\ninstall(orange_juice_dispenser)\n## mypy: Argument 1 to \"install\" has\n## incompatible type \"BeverageDispenser[OrangeJuice]\"\n##          expected \"BeverageDispenser[Juice]\"\nA dispenser specialized for OrangeJuice is not allowed either. Only BeverageDis\npenser[Juice] will do. In the typing jargon, we say that BeverageDis\npenser(Generic[T]) is invariant when BeverageDispenser[OrangeJuice] is not\ncompatible with BeverageDispenser[Juice]—despite the fact that OrangeJuice is a\nsubtype-of Juice.\nPython mutable collection types—such as list and set—are invariant. The Lotto\nBlower class from Example 15-17 is also invariant.\nA Covariant Dispenser\nIf we want to be more flexible and model dispensers as a generic class that can accept\nsome beverage type and also its subtypes, we must make it covariant. Example 15-19\nshows how we’d declare BeverageDispenser.\nExample 15-19. covariant.py: type definitions and install function\nT_co = TypeVar('T_co', covariant=True)  \nclass BeverageDispenser(Generic[T_co]):  \n    def __init__(self, beverage: T_co) -> None:\n        self.beverage = beverage\n546 \n| \nChapter 15: More About Type Hints",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "def dispense(self) -> T_co:\n        return self.beverage\ndef install(dispenser: BeverageDispenser[Juice]) -> None:  \n    \"\"\"Install a fruit juice dispenser.\"\"\"\nSet covariant=True when declaring the type variable; _co is a conventional suf‐\nfix for covariant type parameters on typeshed.\nUse T_co to parameterize the Generic special class.\nType hints for install are the same as in Example 15-18.\nThe following code works because now both the Juice dispenser and the Orange\nJuice dispenser are valid in a covariant BeverageDispenser:\njuice_dispenser = BeverageDispenser(Juice())\ninstall(juice_dispenser)\norange_juice_dispenser = BeverageDispenser(OrangeJuice())\ninstall(orange_juice_dispenser)\nBut a dispenser for an arbitrary Beverage is not acceptable:\nbeverage_dispenser = BeverageDispenser(Beverage())\ninstall(beverage_dispenser)\n## mypy: Argument 1 to \"install\" has\n## incompatible type \"BeverageDispenser[Beverage]\"\n##          expected \"BeverageDispenser[Juice]\"\nThat’s covariance: the subtype relationship of the parameterized dispensers varies in\nthe same direction as the subtype relationship of the type parameters.\nA Contravariant Trash Can\nNow we’ll model the cafeteria rule for deploying a trash can. Let’s assume food and\ndrinks are served in biodegradable packages, and leftovers as well as single-use uten‐\nsils are also biodegradable. The trash cans must be suitable for biodegradable refuse.\nVariance \n| \n547",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "For the sake of this didactic example, let’s make simplifying\nassumptions to classify trash in a neat hierarchy:\n• Refuse is the most general type of trash. All trash is refuse.\n• Biodegradable is a specific type of trash that can be decom‐\nposed by organisms over time. Some Refuse is not Biodegrad\nable.\n• Compostable is a specific type of Biodegradable trash that can\nbe efficiently turned into organic fertilizer in a compost bin or\nin a composting facility. Not all Biodegradable trash is Compo\nstable in our definition.\nIn order to model the rule for an acceptable trash can in the cafeteria, we need to\nintroduce the concept of “contravariance” through an example using it, as shown in\nExample 15-20.\nExample 15-20. contravariant.py: type definitions and install function\nfrom typing import TypeVar, Generic\nclass Refuse:  \n    \"\"\"Any refuse.\"\"\"\nclass Biodegradable(Refuse):\n    \"\"\"Biodegradable refuse.\"\"\"\nclass Compostable(Biodegradable):\n    \"\"\"Compostable refuse.\"\"\"\nT_contra = TypeVar('T_contra', contravariant=True)  \nclass TrashCan(Generic[T_contra]):  \n    def put(self, refuse: T_contra) -> None:\n        \"\"\"Store trash until dumped.\"\"\"\ndef deploy(trash_can: TrashCan[Biodegradable]):\n    \"\"\"Deploy a trash can for biodegradable refuse.\"\"\"\nA type hierarchy for refuse: Refuse is the most general type, Compostable is the\nmost specific.\nT_contra is a conventional name for a contravariant type variable.\nTrashCan is contravariant on the type of refuse.\nGiven those definitions, these types of trash cans are acceptable:\n548 \n| \nChapter 15: More About Type Hints",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "bio_can: TrashCan[Biodegradable] = TrashCan()\ndeploy(bio_can)\ntrash_can: TrashCan[Refuse] = TrashCan()\ndeploy(trash_can)\nThe more general TrashCan[Refuse] is acceptable because it can take any kind of\nrefuse, including Biodegradable. However, a TrashCan[Compostable] will not do,\nbecause it cannot take Biodegradable:\ncompost_can: TrashCan[Compostable] = TrashCan()\ndeploy(compost_can)\n## mypy: Argument 1 to \"deploy\" has\n## incompatible type \"TrashCan[Compostable]\"\n##          expected \"TrashCan[Biodegradable]\"\nLet’s summarize the concepts we just saw.\nVariance Review\nVariance is a subtle property. The following sections recap the concept of invariant,\ncovariant, and contravariant types, and provide some rules of thumb to reason about\nthem.\nInvariant types\nA generic type L is invariant when there is no supertype or subtype relationship\nbetween two parameterized types, regardless of the relationship that may exist\nbetween the actual parameters. In other words, if L is invariant, then L[A] is not a\nsupertype or a subtype of L[B]. They are inconsistent in both ways.\nAs mentioned, Python’s mutable collections are invariant by default. The list type is\na good example: list[int] is not consistent-with list[float] and vice versa.\nIn general, if a formal type parameter appears in type hints of method arguments,\nand the same parameter appears in method return types, that parameter must be\ninvariant to ensure type safety when updating and reading from the collection.\nFor example, here is part of the type hints for the list built-in on typeshed:\nclass list(MutableSequence[_T], Generic[_T]):\n    @overload\n    def __init__(self) -> None: ...\n    @overload\n    def __init__(self, iterable: Iterable[_T]) -> None: ...\n    # ... lines omitted ...\n    def append(self, __object: _T) -> None: ...\n    def extend(self, __iterable: Iterable[_T]) -> None: ...\n    def pop(self, __index: int = ...) -> _T: ...\n    # etc...\nVariance \n| \n549",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "Note that _T appears in the arguments of __init__, append, and extend, and as the\nreturn type of pop. There is no way to make such a class type safe if it is covariant or\ncontravariant in _T.\nCovariant types\nConsider two types A and B, where B is consistent-with A, and neither of them is Any.\nSome authors use the <: and :> symbols to denote type relationships like this:\nA :> B\nA is a supertype-of or the same as B.\nB <: A\nB is a subtype-of or the same as A.\nGiven A :> B, a generic type C is covariant when C[A] :> C[B].\nNote the direction of the :> symbol is the same in both cases where A is to the left\nof B. Covariant generic types follow the subtype relationship of the actual type\nparameters.\nImmutable containers can be covariant. For example, this is how the typing.Frozen\nSet class is documented as a covariant with a type variable using the conventional\nname T_co:\nclass FrozenSet(frozenset, AbstractSet[T_co]):\nApplying the :> notation to parameterized types, we have:\n           float :> int\nfrozenset[float] :> frozenset[int]\nIterators are another example of covariant generics: they are not read-only collections\nlike a frozenset, but they only produce output. Any code expecting an abc.Itera\ntor[float] yielding floats can safely use an abc.Iterator[int] yielding integers.\nCallable types are covariant on the return type for a similar reason.\nContravariant types\nGiven A :> B, a generic type K is contravariant if K[A] <: K[B].\nContravariant generic types reverse the subtype relationship of the actual type\nparameters.\nThe TrashCan class exemplifies this:\n          Refuse :> Biodegradable\nTrashCan[Refuse] <: TrashCan[Biodegradable]\n550 \n| \nChapter 15: More About Type Hints",
      "content_length": 1695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "A contravariant container is usually a write-only data structure, also known as a\n“sink.” There are no examples of such collections in the standard library, but there\nare a few types with contravariant type parameters.\nCallable[[ParamType, …], ReturnType] is contravariant on the parameter types,\nbut covariant on the ReturnType, as we saw in “Variance in Callable types” on page\n292. In addition, Generator, Coroutine, and AsyncGenerator have one contravariant\ntype parameter. The Generator type is described in “Generic Type Hints for Classic\nCoroutines” on page 650; Coroutine and AsyncGenerator are described in Chapter 21.\nFor the present discussion about variance, the main point is that the contravariant\nformal parameter defines the type of the arguments used to invoke or send data to\nthe object, while different covariant formal parameters define the types of outputs\nproduced by the object—the yield type or the return type, depending on the object.\nThe meanings of “send” and “yield” are explained in “Classic Coroutines” on page 641.\nWe can derive useful guidelines from these observations of covariant outputs and\ncontravariant inputs.\nVariance rules of thumb\nFinally, here are a few rules of thumb to reason about when thinking through\nvariance:\n• If a formal type parameter defines a type for data that comes out of the object, it\ncan be covariant.\n• If a formal type parameter defines a type for data that goes into the object after\nits initial construction, it can be contravariant.\n• If a formal type parameter defines a type for data that comes out of the object\nand the same parameter defines a type for data that goes into the object, it must\nbe invariant.\n• To err on the safe side, make formal type parameters invariant.\nCallable[[ParamType, …], ReturnType] demonstrates rules #1 and #2: The Return\nType is covariant, and each ParamType is contravariant.\nBy default, TypeVar creates formal parameters that are invariant, and that’s how the\nmutable collections in the standard library are annotated.\n“Generic Type Hints for Classic Coroutines” on page 650 continues the present discus‐\nsion about variance.\nNext, let’s see how to define generic static protocols, applying the idea of covariance\nto a couple of new examples.\nVariance \n| \n551",
      "content_length": 2263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "Implementing a Generic Static Protocol\nThe Python 3.10 standard library provides a few generic static protocols. One of them\nis SupportsAbs, implemented like this in the typing module:\n@runtime_checkable\nclass SupportsAbs(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its\n        return type.\"\"\"\n    __slots__ = ()\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\nT_co is declared according to the naming convention:\nT_co = TypeVar('T_co', covariant=True)\nThanks to SupportsAbs, Mypy recognizes this code as valid, as you can see in\nExample 15-21.\nExample 15-21. abs_demo.py: use of the generic SupportsAbs protocol\nimport math\nfrom typing import NamedTuple, SupportsAbs\nclass Vector2d(NamedTuple):\n    x: float\n    y: float\n    def __abs__(self) -> float:  \n        return math.hypot(self.x, self.y)\ndef is_unit(v: SupportsAbs[float]) -> bool:  \n    \"\"\"'True' if the magnitude of 'v' is close to 1.\"\"\"\n    return math.isclose(abs(v), 1.0)  \nassert issubclass(Vector2d, SupportsAbs)  \nv0 = Vector2d(0, 1)  \nsqrt2 = math.sqrt(2)\nv1 = Vector2d(sqrt2 / 2, sqrt2 / 2)\nv2 = Vector2d(1, 1)\nv3 = complex(.5, math.sqrt(3) / 2)\nv4 = 1  \nassert is_unit(v0)\nassert is_unit(v1)\nassert not is_unit(v2)\nassert is_unit(v3)\n552 \n| \nChapter 15: More About Type Hints",
      "content_length": 1303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "assert is_unit(v4)\nprint('OK')\nDefining __abs__ makes Vector2d consistent-with SupportsAbs.\nParameterizing SupportsAbs with float ensures…\n…that Mypy accepts abs(v) as the first argument for math.isclose.\nThanks to @runtime_checkable in the definition of SupportsAbs, this is a valid\nruntime assertion.\nThe remaining code all passes Mypy checks and runtime assertions.\nThe int type is also consistent-with SupportsAbs. According to typeshed,\nint.__abs__ returns an int, which is consistent-with the float type parameter\ndeclared in the is_unit type hint for the v argument.\nSimilarly, we can write a generic version of the RandomPicker protocol presented in\nExample 13-18, which was defined with a single method pick returning Any.\nExample 15-22 shows how to make a generic RandomPicker covariant on the return\ntype of pick.\nExample 15-22. generic_randompick.py: definition of generic RandomPicker\nfrom typing import Protocol, runtime_checkable, TypeVar\nT_co = TypeVar('T_co', covariant=True)  \n@runtime_checkable\nclass RandomPicker(Protocol[T_co]):  \n    def pick(self) -> T_co: ...  \nDeclare T_co as covariant.\nThis makes RandomPicker generic with a covariant formal type parameter.\nUse T_co as the return type.\nImplementing a Generic Static Protocol \n| \n553",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "The generic RandomPicker protocol can be covariant because its only formal parame‐\nter is used in a return type.\nWith this, we can call it a chapter.\nChapter Summary\nThe chapter started with a simple example of using @overload, followed by a much\nmore complex example that we studied in detail: the overloaded signatures required\nto correctly annotate the max built-in function.\nThe typing.TypedDict special construct came next. I chose to cover it here, and not\nin Chapter 5 where we saw typing.NamedTuple, because TypedDict is not a class\nbuilder; it’s merely a way to add type hints to a variable or argument that requires a\ndict with a specific set of string keys, and specific types for each key—which happens\nwhen we use a dict as a record, often in the context of handling with JSON data.\nThat section was a bit long because using TypedDict can give a false sense of security,\nand I wanted to show how runtime checks and error handling are really inevitable\nwhen trying to make statically structured records out of mappings that are dynamic\nin nature.\nNext we talked about typing.cast, a function designed to let us guide the work of\nthe type checker. It’s important to carefully consider when to use cast, because over‐\nusing it hinders the type checker.\nRuntime access to type hints came next. The key point was to use typing.\nget_type_hints instead of reading the __annotations__ attribute directly. However,\nthat function may be unreliable with some annotations, and we saw that Python core\ndevelopers are still working on a way to make type hints usable at runtime, while\nreducing their impact on CPU and memory usage.\nThe final sections were about generics, starting with the LottoBlower generic class—\nwhich we later learned is an invariant generic class. That example was followed by\ndefinitions of four basic terms: generic type, formal type parameter, parameterized\ntype, and actual type parameter.\nThe major topic of variance was presented next, using cafeteria beverage dispensers\nand trash cans as “real life” examples of invariant, covariant, and contravariant\ngeneric types. Next we reviewed, formalized, and further applied those concepts to\nexamples in Python’s standard library.\nLastly, we saw how a generic static protocol is defined, first considering the typ\ning.SupportsAbs protocol, and then applying the same idea to the RandomPicker\nexample, making it more strict than the original protocol from Chapter 13.\n554 \n| \nChapter 15: More About Type Hints",
      "content_length": 2481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "Python’s type system is a huge and rapidly evolving subject. This\nchapter is not comprehensive. I chose to focus on topics that are\neither widely applicable, particularly challenging, or conceptually\nimportant and therefore likely to be relevant for a long time.\nFurther Reading\nPython’s static type system was complex as initially designed, and is getting more\ncomplex with each passing year. Table 15-1 lists all the PEPs that I am aware of as of\nMay 2021. It would take a whole book to cover everything.\nTable 15-1. PEPs about type hints, with links in the titles. PEP with numbers marked with *\nare important enough to be mentioned in the opening paragraph of the typing\ndocumentation. Question marks in the Python column indicate PEPs under discussion or\nnot yet implemented; “n/a” appears in informational PEPs with no specific Python version.\nPEP\nTitle\nPython Year\n3107\nFunction Annotations\n3.0\n2006\n483*\nThe Theory of Type Hints\nn/a\n2014\n484*\nType Hints\n3.5\n2014\n482\nLiterature Overview for Type Hints\nn/a\n2015\n526*\nSyntax for Variable Annotations\n3.6\n2016\n544*\nProtocols: Structural subtyping (static duck typing)\n3.8\n2017\n557\nData Classes\n3.7\n2017\n560\nCore support for typing module and generic types\n3.7\n2017\n561\nDistributing and Packaging Type Information\n3.7\n2017\n563\nPostponed Evaluation of Annotations\n3.7\n2017\n586*\nLiteral Types\n3.8\n2018\n585\nType Hinting Generics In Standard Collections\n3.9\n2019\n589*\nTypedDict: Type Hints for Dictionaries with a Fixed Set of Keys\n3.8\n2019\n591*\nAdding a final qualifier to typing\n3.8\n2019\n593\nFlexible function and variable annotations\n?\n2019\n604\nAllow writing union types as X | Y\n3.10\n2019\n612\nParameter Specification Variables\n3.10\n2019\n613\nExplicit Type Aliases\n3.10\n2020\n645\nAllow writing optional types as x?\n?\n2020\n646\nVariadic Generics\n?\n2020\n647\nUser-Defined Type Guards\n3.10\n2021\n649\nDeferred Evaluation Of Annotations Using Descriptors\n?\n2021\n655\nMarking individual TypedDict items as required or potentially-missing\n?\n2021\nFurther Reading \n| \n555",
      "content_length": 2009,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "17 As a reader of footnotes, you may recall that I credited Erik Meijer for the cafeteria analogy to explain\nvariance.\n18 That book was written for Dart 1. There are significant changes in Dart 2, including in the type system. Nev‐\nertheless, Bracha is an important researcher in the field of programming language design, and I found the\nbook valuable for his perspective on the design of Dart.\nPython’s official documentation hardly keeps up with all that, so Mypy’s documenta‐\ntion is an essential reference. Robust Python by Patrick Viafore (O’Reilly) is the first\nbook with extensive coverage of Python’s static type system that I know about, pub‐\nlished August 2021. You may be reading the second such book right now.\nThe subtle topic of variance has its own section in PEP 484, and is also covered in the\n“Generics” page of Mypy, as well as in its invaluable “Common Issues” page.\nPEP 362—Function Signature Object is worth reading if you intend to use the\ninspect module that complements the typing.get_type_hints function.\nIf you are interested in the history of Python, you may like to know that Guido van\nRossum posted “Adding Optional Static Typing to Python” on December 23, 2004.\n“Python 3 Types in the Wild: A Tale of Two Type Systems” is a research paper by\nIngkarat Rak-amnouykit and others from the Rensselaer Polytechnic Institute and\nIBM TJ Watson Research Center. The paper surveys the use of type hints in open\nsource projects on GitHub, showing that most projects don’t use them, and also that\nmost projects that have type hints apparently don’t use a type checker. I found most\ninteresting the discussion of the different semantics of Mypy and Google’s pytype,\nwhich they conclude are “essentially two different type systems.”\nTwo seminal papers about gradual typing are Gilad Bracha’s “Pluggable Type Sys‐\ntems”, and “Static Typing Where Possible, Dynamic Typing When Needed: The End\nof the Cold War Between Programming Languages” by Eric Meijer and Peter Dray‐\nton.17\nI learned a lot reading the relevant parts of some books about other languages that\nimplement some of the same ideas:\n• Atomic Kotlin by Bruce Eckel and Svetlana Isakova (Mindview)\n• Effective Java, 3rd ed., by Joshua Bloch (Addison-Wesley)\n• Programming with Types: TypeScript Examples by Vlad Riscutia (Manning)\n• Programming TypeScript by Boris Cherny (O’Reilly)\n• The Dart Programming Language by Gilad Bracha (Addison-Wesley)18\nFor some critical views on type systems, I recommend Victor Youdaiken’s posts “Bad\nideas in type theory” and “Types considered harmful II”.\n556 \n| \nChapter 15: More About Type Hints",
      "content_length": 2607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "Finally, I was surprised to find “Generics Considered Harmful” by Ken Arnold, a\ncore contributor to Java from the beginning, as well as coauthor of the first four edi‐\ntions of the official The Java Programming Language book (Addison-Wesley)—in\ncollaboration with James Gosling, the lead designer of Java.\nSadly, Arnold’s criticism applies to Python’s static type system as well. While reading\nthe many rules and special cases of the typing PEPs, I was constantly reminded of this\npassage from Gosling’s post:\nWhich brings up the problem that I always cite for C++: I call it the “Nth order excep‐\ntion to the exception rule.” It sounds like this: “You can do x, except in case y, unless y\ndoes z, in which case you can if …”\nFortunately, Python has a key advantage over Java and C++: an optional type system.\nWe can squelch type checkers and omit type hints when they become too\ncumbersome.\nSoapbox\nTyping Rabbit Holes\nWhen using a type checker, we are sometimes forced to discover and import classes\nwe did not need to know about, and our code has no need to reference—except to\nwrite type hints. Such classes are undocumented, probably because they are consid‐\nered implementation details by the authors of the packages. Here are two examples\nfrom the standard library.\nTo use cast() in the server.sockets example in “Type Casting” on page 534, I had\nto scour the vast asyncio documentation and then browse the source code of several\nmodules in that package to discover the undocumented TransportSocket class in the\nequally undocumented asyncio.trsock module. Using socket.socket instead of\nTransportSocket would be incorrect, because the latter is explicitly not a subtype of\nthe former, according to a docstring in the source code.\nI fell into a similar rabbit hole when I added type hints to Example 19-13, a simple\ndemonstration of multiprocessing. That example uses SimpleQueue objects, which\nyou get by calling multiprocessing.SimpleQueue(). However, I could not use that\nname in a type hint, because it turns out that multiprocessing.SimpleQueue is not a\nclass! It’s a bound method of the undocumented multiprocessing.BaseContext\nclass, which builds and returns an instance of the SimpleQueue class defined in the\nundocumented multiprocessing.queues module.\nIn each of those cases I had to spend a couple of hours to find the right undocumen‐\nted class to import, just to write a single type hint. This kind of research is part of the\njob when writing a book. But when writing application code, I’d probably avoid such\nFurther Reading \n| \n557",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "19 See the last paragraph of the section “Covariance and Contravariance” in PEP 484.\nscavenger hunts for a single offending line and just write # type: ignore. Sometimes\nthat’s the only cost-effective solution.\nVariance Notation in Other Languages\nVariance is a difficult topic, and Python’s type hints syntax is not as good as it could\nbe. This is evidenced by this direct quote from PEP 484:\nCovariance or contravariance is not a property of a type variable, but a property of a\ngeneric class defined using this variable.19\nIf that is the case, why are covariance and contravariance declared with TypeVar and\nnot on the generic class?\nThe authors of PEP 484 worked under the severe self-imposed constraint that type\nhints should be supported without making any change to the interpreter. This\nrequired the introduction of TypeVar to define type variables, and also the abuse of []\nto provide Klass[T] syntax for generics—instead of the Klass<T> notation used in\nother popular languages, including C#, Java, Kotlin, and TypeScript. None of these\nlanguages require type variables to be declared before use.\nIn addition, the syntax of Kotlin and C# makes it clear whether the type parameter is\ncovariant, contravariant, or invariant exactly where it makes sense: in the class or\ninterface declaration.\nIn Kotlin, we could declare the BeverageDispenser like this:\nclass BeverageDispenser<out T> {\n    // etc...\n}\nThe out modifier in the formal type parameter means T is an “output” type, therefore\nBeverageDispenser is covariant.\n558 \n| \nChapter 15: More About Type Hints",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "You can probably guess how TrashCan would be declared:\nclass TrashCan<in T> {\n    // etc...\n}\nGiven T as an “input” formal type parameter, then TrashCan is contravariant.\nIf neither in nor out appear, then the class is invariant on the parameter.\nIt’s easy to recall the “Variance rules of thumb” on page 551 when out and in are\nused in the formal type parameters.\nThis suggests that a good naming convention for covariant and contravariant type\nvariables in Python would be:\nT_out = TypeVar('T_out', covariant=True)\nT_in = TypeVar('T_in', contravariant=True)\nThen we could define the classes like this:\nclass BeverageDispenser(Generic[T_out]):\n    ...\nclass TrashCan(Generic[T_in]):\n    ...\nIs it too late to change the naming convention established in PEP 484?\nFurther Reading \n| \n559",
      "content_length": 786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "1 Source: “The C Family of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup, and James Gosling”.\nCHAPTER 16\nOperator Overloading\nThere are some things that I kind of feel torn about, like operator overloading. I left\nout operator overloading as a fairly personal choice because I had seen too many\npeople abuse it in C++.\n—James Gosling, creator of Java1\nIn Python, you can compute compound interest using a formula written like this:\ninterest = principal * ((1 + rate) ** periods - 1)\nOperators that appear between operands, like 1 + rate, are infix operators. In\nPython, the infix operators can handle any arbitrary type. Thus, if you are dealing\nwith real money, you can make sure that principal, rate, and periods are exact\nnumbers—instances of the Python decimal.Decimal class—and that formula will\nwork as written, producing an exact result.\nBut in Java, if you switch from float to BigDecimal to get exact results, you can’t use\ninfix operators anymore, because they only work with the primitive types. This is the\nsame formula coded to work with BigDecimal numbers in Java:\nBigDecimal interest = principal.multiply(BigDecimal.ONE.add(rate)\n                        .pow(periods).subtract(BigDecimal.ONE));\nIt’s clear that infix operators make formulas more readable. Operator overloading is\nnecessary to support infix operator notation with user-defined or extension types,\nsuch as NumPy arrays. Having operator overloading in a high-level, easy-to-use lan‐\nguage was probably a key reason for the huge success of Python in data science,\nincluding financial and scientific applications.\n561",
      "content_length": 1608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "2 The remaining ABCs in Python’s standard library are still valuable for goose typing and static typing. The\nissue with the numbers ABCs is explained in “The numbers ABCs and Numeric Protocols” on page 478.\nIn “Emulating Numeric Types” on page 9 (Chapter 1) we saw some trivial implemen‐\ntations of operators in a bare-bones Vector class. The __add__ and __mul__ methods\nin Example 1-2 were written to show how special methods support operator over‐\nloading, but there are subtle problems in their implementations that we overlooked.\nAlso, in Example 11-2, we noted that the Vector2d.__eq__ method considers this to\nbe True: Vector(3, 4) == [3, 4]—which may or not make sense. We will address\nthese matters in this chapter, as well as:\n• How an infix operator method should signal it cannot handle an operand\n• Using duck typing or goose typing to deal with operands of various types\n• The special behavior of the rich comparison operators (e.g., ==, >, <=, etc.)\n• The default handling of augmented assignment operators such as +=, and how to\noverload them\nWhat’s New in This Chapter\nGoose typing is a key part of Python, but the numbers ABCs are not supported in\nstatic typing, so I changed Example 16-11 to use duck typing instead of an explicit\nisinstance check against numbers.Real.2\nI covered the @ matrix multiplication operator in the first edition of Fluent Python as\nan upcoming change when 3.5 was still in alpha. Accordingly, that operator is no\nlonger in a side note, but is integrated in the flow of the chapter in “Using @ as an\nInfix Operator” on page 574. I leveraged goose typing to make the implementation\nof __matmul__ safer than the one in the first edition, without compromising on\nflexibility.\n“Further Reading” on page 587 now has a couple of new references—including a blog\npost by Guido van Rossum. I also added mentions of two libraries that showcase\neffective use of operator overloading outside the domain of mathematics: pathlib\nand Scapy.\nOperator Overloading 101\nOperator overloading allows user-defined objects to interoperate with infix operators\nsuch as + and |, or unary operators like - and ~. More generally, function invocation\n562 \n| \nChapter 16: Operator Overloading",
      "content_length": 2207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "3 See https://en.wikipedia.org/wiki/Bitwise_operation#NOT for an explanation of the bitwise not.\n(()), attribute access (.), and item access/slicing ([]) are also operators in Python,\nbut this chapter covers unary and infix operators.\nOperator overloading has a bad name in some circles. It is a language feature that can\nbe (and has been) abused, resulting in programmer confusion, bugs, and unexpected\nperformance bottlenecks. But if used well, it leads to pleasurable APIs and readable\ncode. Python strikes a good balance among flexibility, usability, and safety by impos‐\ning some limitations:\n• We cannot change the meaning of the operators for the built-in types.\n• We cannot create new operators, only overload existing ones.\n• A few operators can’t be overloaded: is, and, or, not (but the bitwise &, |, ~, can).\nIn Chapter 12, we already had one infix operator in Vector: ==, supported by the\n__eq__ method. In this chapter, we’ll improve the implementation of __eq__ to better\nhandle operands of types other than Vector. However, the rich comparison operators\n(==, !=, >, <, >=, <=) are special cases in operator overloading, so we’ll start by over‐\nloading four arithmetic operators in Vector: the unary - and +, followed by the infix\n+ and *.\nLet’s start with the easiest topic: unary operators.\nUnary Operators\nThe Python Language Reference, “6.5. Unary arithmetic and bitwise operations” lists\nthree unary operators, shown here with their associated special methods:\n-, implemented by __neg__\nArithmetic unary negation. If x is -2 then -x == 2.\n+, implemented by __pos__\nArithmetic unary plus. Usually x == +x, but there are a few cases when that’s not\ntrue. See “When x and +x Are Not Equal” on page 565 if you’re curious.\n~, implemented by __invert__\nBitwise not, or bitwise inverse of an integer, defined as ~x == -(x+1). If x is 2\nthen ~x == -3.3\nThe “Data Model” chapter of The Python Language Reference also lists the abs()\nbuilt-in function as a unary operator. The associated special method is __abs__, as\nwe’ve seen before.\nUnary Operators \n| \n563",
      "content_length": 2070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": "It’s easy to support the unary operators. Simply implement the appropriate special\nmethod, which will take just one argument: self. Use whatever logic makes sense in\nyour class, but stick to the general rule of operators: always return a new object. In\nother words, do not modify the receiver (self), but create and return a new instance\nof a suitable type.\nIn the case of - and +, the result will probably be an instance of the same class as\nself. For unary +, if the receiver is immutable you should return self; otherwise,\nreturn a copy of self. For abs(), the result should be a scalar number.\nAs for ~, it’s difficult to say what would be a sensible result if you’re not dealing with\nbits in an integer. In the pandas data analysis package, the tilde negates boolean filter‐\ning conditions; see “Boolean indexing” in the pandas documentation for examples.\nAs promised before, we’ll implement several new operators on the Vector class from\nChapter 12. Example 16-1 shows the __abs__ method we already had in\nExample 12-16, and the newly added __neg__ and __pos__ unary operator method.\nExample 16-1. vector_v6.py: unary operators - and + added to Example 12-16\n    def __abs__(self):\n        return math.hypot(*self)\n    def __neg__(self):\n        return Vector(-x for x in self)  \n    def __pos__(self):\n        return Vector(self)  \nTo compute -v, build a new Vector with every component of self negated.\nTo compute +v, build a new Vector with every component of self.\nRecall that Vector instances are iterable, and the Vector.__init__ takes an iterable\nargument, so the implementations of __neg__ and __pos__ are short and sweet.\nWe’ll not implement __invert__, so if the user tries ~v on a Vector instance, Python\nwill raise TypeError with a clear message: “bad operand type for unary ~: 'Vector'.”\nThe following sidebar covers a curiosity that may help you win a bet about unary +\nsomeday.\n564 \n| \nChapter 16: Operator Overloading",
      "content_length": 1939,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": "When x and +x Are Not Equal\nEverybody expects that x == +x, and that is true almost all the time in Python, but I\nfound two cases in the standard library where x != +x.\nThe first case involves the decimal.Decimal class. You can have x != +x if x is a\nDecimal instance created in an arithmetic context and +x is then evaluated in a con‐\ntext with different settings. For example, x is calculated in a context with a certain\nprecision, but the precision of the context is changed and then +x is evaluated. See\nExample 16-2 for a demonstration.\nExample 16-2. A change in the arithmetic context precision may cause x to differ\nfrom +x\n>>> import decimal\n>>> ctx = decimal.getcontext()  \n>>> ctx.prec = 40  \n>>> one_third = decimal.Decimal('1') / decimal.Decimal('3')  \n>>> one_third  \nDecimal('0.3333333333333333333333333333333333333333')\n>>> one_third == +one_third  \nTrue\n>>> ctx.prec = 28  \n>>> one_third == +one_third  \nFalse\n>>> +one_third  \nDecimal('0.3333333333333333333333333333')\nGet a reference to the current global arithmetic context.\nSet the precision of the arithmetic context to 40.\nCompute 1/3 using the current precision.\nInspect the result; there are 40 digits after the decimal point.\none_third == +one_third is True.\nLower precision to 28—the default for Decimal arithmetic.\nNow one_third == +one_third is False.\nInspect +one_third; there are 28 digits after the '.' here.\nUnary Operators \n| \n565",
      "content_length": 1412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": null,
      "content": "The fact is that each occurrence of the expression +one_third produces a new Deci\nmal instance from the value of one_third, but using the precision of the current\narithmetic context.\nYou can find the second case where x != +x in the collections.Counter documen‐\ntation. The Counter class implements several arithmetic operators, including infix +\nto add the tallies from two Counter instances. However, for practical reasons,\nCounter addition discards from the result any item with a negative or zero count.\nAnd the prefix + is a shortcut for adding an empty Counter, therefore it produces a\nnew Counter, preserving only the tallies that are greater than zero. See Example 16-3.\nExample 16-3. Unary + produces a new Counter without zeroed or negative tallies\n>>> ct = Counter('abracadabra')\n>>> ct\nCounter({'a': 5, 'r': 2, 'b': 2, 'd': 1, 'c': 1})\n>>> ct['r'] = -3\n>>> ct['d'] = 0\n>>> ct\nCounter({'a': 5, 'b': 2, 'c': 1, 'd': 0, 'r': -3})\n>>> +ct\nCounter({'a': 5, 'b': 2, 'c': 1})\nAs you can see, +ct returns a counter where all tallies are greater than zero.\nNow, back to our regularly scheduled programming.\nOverloading + for Vector Addition\nThe Vector class is a sequence type, and the section “3.3.6. Emulating container\ntypes” in the “Data Model” chapter of the official Python documentation says that\nsequences should support the + operator for concatenation and * for repetition.\nHowever, here we will implement + and * as mathematical vector operations, which\nare a bit harder but more meaningful for a Vector type.\nIf users want to concatenate or repeat Vector instances, they can\nconvert them to tuples or lists, apply the operator, and convert\nback—thanks to the fact that Vector is iterable and can be con‐\nstructed from an iterable:\n>>> v_concatenated = Vector(list(v1) + list(v2))\n>>> v_repeated = Vector(tuple(v1) * 5)\nAdding two Euclidean vectors results in a new vector in which the components are\nthe pairwise additions of the components of the operands. To illustrate:\n566 \n| \nChapter 16: Operator Overloading",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": null,
      "content": ">>> v1 = Vector([3, 4, 5])\n>>> v2 = Vector([6, 7, 8])\n>>> v1 + v2\nVector([9.0, 11.0, 13.0])\n>>> v1 + v2 == Vector([3 + 6, 4 + 7, 5 + 8])\nTrue\nWhat happens if we try to add two Vector instances of different lengths? We could\nraise an error, but considering practical applications (such as information retrieval),\nit’s better to fill out the shortest Vector with zeros. This is the result we want:\n>>> v1 = Vector([3, 4, 5, 6])\n>>> v3 = Vector([1, 2])\n>>> v1 + v3\nVector([4.0, 6.0, 5.0, 6.0])\nGiven these basic requirements, we can implement __add__ like in Example 16-4.\nExample 16-4. Vector.__add__ method, take #1\n    # inside the Vector class\n    def __add__(self, other):\n        pairs = itertools.zip_longest(self, other, fillvalue=0.0)  \n        return Vector(a + b for a, b in pairs)  \npairs is a generator that produces tuples (a, b), where a is from self, and b is\nfrom other. If self and other have different lengths, fillvalue supplies the\nmissing values for the shortest iterable.\nA new Vector is built from a generator expression, producing one addition for\neach (a, b) from pairs.\nNote how __add__ returns a new Vector instance, and does not change self or\nother.\nSpecial methods implementing unary or infix operators should\nnever change the value of the operands. Expressions with such\noperators are expected to produce results by creating new objects.\nOnly augmented assignment operators may change the first\noperand (self), as discussed in “Augmented Assignment Opera‐\ntors” on page 580.\nExample 16-4 allows adding Vector to a Vector2d, and Vector to a tuple or to any\niterable that produces numbers, as Example 16-5 proves.\nOverloading + for Vector Addition \n| \n567",
      "content_length": 1682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": null,
      "content": "Example 16-5. Vector.__add__ take #1 supports non-Vector objects, too\n>>> v1 = Vector([3, 4, 5])\n>>> v1 + (10, 20, 30)\nVector([13.0, 24.0, 35.0])\n>>> from vector2d_v3 import Vector2d\n>>> v2d = Vector2d(1, 2)\n>>> v1 + v2d\nVector([4.0, 6.0, 5.0])\nBoth uses of + in Example 16-5 work because __add__ uses zip_longest(…), which\ncan consume any iterable, and the generator expression to build the new Vector\nmerely performs a + b with the pairs produced by zip_longest(…), so an iterable\nproducing any number items will do.\nHowever, if we swap the operands (Example 16-6), the mixed-type additions fail.\nExample 16-6. Vector.__add__ take #1 fails with non-Vector left operands\n>>> v1 = Vector([3, 4, 5])\n>>> (10, 20, 30) + v1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only concatenate tuple (not \"Vector\") to tuple\n>>> from vector2d_v3 import Vector2d\n>>> v2d = Vector2d(1, 2)\n>>> v2d + v1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'Vector2d' and 'Vector'\nTo support operations involving objects of different types, Python implements a spe‐\ncial dispatching mechanism for the infix operator special methods. Given an expres‐\nsion a + b, the interpreter will perform these steps (also see Figure 16-1):\n1. If a has __add__, call a.__add__(b) and return result unless it’s NotImplemented.\n2. If a doesn’t have __add__, or calling it returns NotImplemented, check if b has\n__radd__, then call b.__radd__(a) and return result unless it’s NotImplemented.\n3. If b doesn’t have __radd__, or calling it returns NotImplemented, raise TypeError\nwith an unsupported operand types message.\n568 \n| \nChapter 16: Operator Overloading",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": null,
      "content": "4 The Python documentation uses both terms. The “Data Model” chapter uses “reflected,” but “9.1.2.2. Imple‐\nmenting the arithmetic operations” in the numbers module docs mention “forward” and “reverse” methods,\nand I find this terminology better, because “forward” and “reversed” clearly name each of the directions,\nwhile “reflected” doesn’t have an obvious opposite.\nThe __radd__ method is called the “reflected” or “reversed” ver‐\nsion of __add__. I prefer to call them “reversed” special methods.4\nFigure 16-1. Flowchart for computing a + b with __add__ and __radd__.\nTherefore, to make the mixed-type additions in Example 16-6 work, we need to\nimplement the Vector.__radd__ method, which Python will invoke as a fallback if\nthe left operand does not implement __add__, or if it does but returns NotImplemen\nted to signal that it doesn’t know how to handle the right operand.\nOverloading + for Vector Addition \n| \n569",
      "content_length": 921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": null,
      "content": "Do not confuse NotImplemented with NotImplementedError. The\nfirst, NotImplemented, is a special singleton value that an infix\noperator special method should return to tell the interpreter it\ncannot handle a given operand. In contrast, NotImplementedError\nis an exception that stub methods in abstract classes may raise to\nwarn that subclasses must implement them.\nThe simplest implementation of __radd__ that works is shown in Example 16-7.\nExample 16-7. The Vector methods __add__ and __radd__\n    # inside the Vector class\n    def __add__(self, other):  \n        pairs = itertools.zip_longest(self, other, fillvalue=0.0)\n        return Vector(a + b for a, b in pairs)\n    def __radd__(self, other):  \n        return self + other\nNo changes to __add__ from Example 16-4; listed here because __radd__ uses it.\n__radd__ just delegates to __add__.\nOften, __radd__ can be as simple as that: just invoke the proper operator, therefore\ndelegating to __add__ in this case. This applies to any commutative operator; + is\ncommutative when dealing with numbers or our vectors, but it’s not commutative\nwhen concatenating sequences in Python.\nIf __radd__ simply calls __add__, here is another way to achieve the same effect:\n    def __add__(self, other):\n        pairs = itertools.zip_longest(self, other, fillvalue=0.0)\n        return Vector(a + b for a, b in pairs)\n    __radd__ = __add__\nThe methods in Example 16-7 work with Vector objects, or any iterable with\nnumeric items, such as a Vector2d, a tuple of integers, or an array of floats. But if\nprovided with a noniterable object, __add__ raises an exception with a message that is\nnot very helpful, as in Example 16-8.\nExample 16-8. Vector.__add__ method needs an iterable operand\n>>> v1 + 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n570 \n| \nChapter 16: Operator Overloading",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": null,
      "content": "File \"vector_v6.py\", line 328, in __add__\n    pairs = itertools.zip_longest(self, other, fillvalue=0.0)\nTypeError: zip_longest argument #2 must support iteration\nEven worse, we get a misleading message if an operand is iterable but its items cannot\nbe added to the float items in the Vector. See Example 16-9.\nExample 16-9. Vector.__add__ method needs an iterable with numeric items\n>>> v1 + 'ABC'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"vector_v6.py\", line 329, in __add__\n    return Vector(a + b for a, b in pairs)\n  File \"vector_v6.py\", line 243, in __init__\n    self._components = array(self.typecode, components)\n  File \"vector_v6.py\", line 329, in <genexpr>\n    return Vector(a + b for a, b in pairs)\nTypeError: unsupported operand type(s) for +: 'float' and 'str'\nI tried to add Vector and a str, but the message complains about float and str.\nThe problems in Examples 16-8 and 16-9 actually go deeper than obscure error mes‐\nsages: if an operator special method cannot return a valid result because of type\nincompatibility, it should return NotImplemented and not raise TypeError. By\nreturning NotImplemented, you leave the door open for the implementer of the other\noperand type to perform the operation when Python tries the reversed method call.\nIn the spirit of duck typing, we will refrain from testing the type of the other\noperand, or the type of its elements. We’ll catch the exceptions and return NotImple\nmented. If the interpreter has not yet reversed the operands, it will try that. If the\nreverse method call returns NotImplemented, then Python will raise TypeError with a\nstandard error message like “unsupported operand type(s) for +: Vector and str.”\nThe final implementation of the special methods for Vector addition is in\nExample 16-10.\nExample 16-10. vector_v6.py: operator + methods added to vector_v5.py\n(Example 12-16)\n    def __add__(self, other):\n        try:\n            pairs = itertools.zip_longest(self, other, fillvalue=0.0)\n            return Vector(a + b for a, b in pairs)\n        except TypeError:\n            return NotImplemented\nOverloading + for Vector Addition \n| \n571",
      "content_length": 2156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": null,
      "content": "def __radd__(self, other):\n        return self + other\nNote that __add__ now catches a TypeError and returns NotImplemented.\nIf an infix operator method raises an exception, it aborts the opera‐\ntor dispatch algorithm. In the particular case of TypeError, it is\noften better to catch it and return NotImplemented. This allows\nthe interpreter to try calling the reversed operator method, which\nmay correctly handle the computation with the swapped operands,\nif they are of different types.\nAt this point, we have safely overloaded the + operator by writing __add__ and\n__radd__. We will now tackle another infix operator: *.\nOverloading * for Scalar Multiplication\nWhat does Vector([1, 2, 3]) * x mean? If x is a number, that would be a scalar\nproduct, and the result would be a new Vector with each component multiplied by x\n—also known as an elementwise multiplication:\n>>> v1 = Vector([1, 2, 3])\n>>> v1 * 10\nVector([10.0, 20.0, 30.0])\n>>> 11 * v1\nVector([11.0, 22.0, 33.0])\nAnother kind of product involving Vector operands would be the\ndot product of two vectors—or matrix multiplication, if you take\none vector as a 1 × N matrix and the other as an N × 1 matrix. We\nwill implement that operator in our Vector class in “Using @ as an\nInfix Operator” on page 574.\nBack to our scalar product, again we start with the simplest __mul__ and __rmul__\nmethods that could possibly work:\n    # inside the Vector class\n    def __mul__(self, scalar):\n        return Vector(n * scalar for n in self)\n    def __rmul__(self, scalar):\n        return self * scalar\nThose methods do work, except when provided with incompatible operands. The\nscalar argument has to be a number that when multiplied by a float produces\n572 \n| \nChapter 16: Operator Overloading",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": null,
      "content": "another float (because our Vector class uses an array of floats internally). So a com\nplex number will not do, but the scalar can be an int, a bool (because bool is a sub‐\nclass of int), or even a fractions.Fraction instance. In Example 16-11, the __mul__\nmethod does not make an explicit type check on scalar, but instead converts it into\na float, and returns NotImplemented if that fails. That’s a clear example of duck\ntyping.\nExample 16-11. vector_v7.py: operator * methods added\nclass Vector:\n    typecode = 'd'\n    def __init__(self, components):\n        self._components = array(self.typecode, components)\n    # many methods omitted in book listing, see vector_v7.py\n    # in https://github.com/fluentpython/example-code-2e\n    def __mul__(self, scalar):\n        try:\n            factor = float(scalar)\n        except TypeError:  \n            return NotImplemented  \n        return Vector(n * factor for n in self)\n    def __rmul__(self, scalar):\n        return self * scalar  \nIf scalar cannot be converted to float…\n…we don’t know how to handle it, so we return NotImplemented to let Python\ntry __rmul__ on the scalar operand.\nIn this example, __rmul__ works fine by just performing self * scalar, dele‐\ngating to the __mul__ method.\nWith Example 16-11, we can multiply Vectors by scalar values of the usual, and not\nso usual, numeric types:\n>>> v1 = Vector([1.0, 2.0, 3.0])\n>>> 14 * v1\nVector([14.0, 28.0, 42.0])\n>>> v1 * True\nVector([1.0, 2.0, 3.0])\n>>> from fractions import Fraction\n>>> v1 * Fraction(1, 3)\nVector([0.3333333333333333, 0.6666666666666666, 1.0])\nOverloading * for Scalar Multiplication \n| \n573",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": null,
      "content": "5 See “Soapbox” on page 588 for a discussion of the problem.\nNow that we can multiply Vector by scalars, let’s see how to implement Vector by\nVector products.\nIn the first edition of Fluent Python, I used goose typing in\nExample 16-11: I checked the scalar argument of __mul__ with\nisinstance(scalar, numbers.Real). Now I avoid using the num\nbers ABCs because they are not supported by PEP 484, and using\ntypes at runtime that cannot also be statically checked seems a bad\nidea to me.\nAlternatively, I could have checked against the typing.Supports\nFloat protocol that we saw in “Runtime Checkable Static Proto‐\ncols” on page 468. I chose duck typing in that example because I\nthink fluent Pythonistas should be comfortable with that coding\npattern.\nOn the other hand, __matmul__ in Example 16-12 is a good exam‐\nple of goose typing, new in this second edition.\nUsing @ as an Infix Operator\nThe @ sign is well-known as the prefix of function decorators, but since 2015, it can\nalso be used as an infix operator. For years, the dot product was written as\nnumpy.dot(a, b) in NumPy. The function call notation makes longer formulas\nharder to translate from mathematical notation to Python,5 so the numerical comput‐\ning community lobbied for PEP 465—A dedicated infix operator for matrix multipli‐\ncation, which was implemented in Python 3.5. Today, you can write a @ b to\ncompute the dot product of two NumPy arrays.\nThe @ operator is supported by the special methods __matmul__, __rmatmul__, and\n__imatmul__, named for “matrix multiplication.” These methods are not used any‐\nwhere in the standard library at this time, but are recognized by the interpreter since\nPython 3.5, so the NumPy team—and the rest of us—can support the @ operator in\nuser-defined types. The parser was also changed to handle the new operator (a @ b\nwas a syntax error in Python 3.4).\nThese simple tests show how @ should work with Vector instances:\n>>> va = Vector([1, 2, 3])\n>>> vz = Vector([5, 6, 7])\n>>> va @ vz == 38.0  # 1*5 + 2*6 + 3*7\nTrue\n>>> [10, 20, 30] @ vz\n574 \n| \nChapter 16: Operator Overloading",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": null,
      "content": "380.0\n>>> va @ 3\nTraceback (most recent call last):\n...\nTypeError: unsupported operand type(s) for @: 'Vector' and 'int'\nExample 16-12 shows the code of the relevant special methods.\nExample 16-12. vector_v7.py: operator @ methods\nclass Vector:\n    # many methods omitted in book listing\n    def __matmul__(self, other):\n        if (isinstance(other, abc.Sized) and  \n            isinstance(other, abc.Iterable)):\n            if len(self) == len(other):  \n                return sum(a * b for a, b in zip(self, other))  \n            else:\n                raise ValueError('@ requires vectors of equal length.')\n        else:\n            return NotImplemented\n    def __rmatmul__(self, other):\n        return self @ other\nBoth operands must implement __len__ and __iter__…\n…and have the same length to allow…\n…a beautiful application of sum, zip, and generator expression.\nNew zip() Feature in Python 3.10\nThe zip built-in accepts a strict keyword-only optional argument\nsince Python 3.10. When strict=True, the function raises ValueEr\nror when the iterables have different lengths. The default is False.\nThis new strict behavior is in line with Python’s fail fast philoso‐\nphy. In Example 16-12, I’d replace the inner if with a try/except\nValueError and add strict=True to the zip call.\nExample 16-12 is a good example of goose typing in practice. If we tested the other\noperand against Vector, we’d deny users the flexibility of using lists or arrays as\noperands to @. As long as one operand is a Vector, our @ implementation supports\nother operands that are instances of abc.Sized and abc.Iterable. Both of these\nABCs implement the __subclasshook__, therefore any object providing __len__ and\n__iter__ satisfies our test—no need to actually subclass those ABCs or even register\nUsing @ as an Infix Operator \n| \n575",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": null,
      "content": "with them, as explained in “Structural Typing with ABCs” on page 464. In particular,\nour Vector class does not subclass either abc.Sized or abc.Iterable, but it does\npass the isinstance checks against those ABCs because it has the necessary methods.\nLet’s review the arithmetic operators supported by Python, before diving into the\nspecial category of “Rich Comparison Operators” on page 577.\nWrapping-Up Arithmetic Operators\nImplementing +, *, and @, we saw the most common patterns for coding infix opera‐\ntors. The techniques we described are applicable to all operators listed in Table 16-1\n(the in-place operators will be covered in “Augmented Assignment Operators” on\npage 580).\nTable 16-1. Infix operator method names (the in-place operators are used for augmented\nassignment; comparison operators are in Table 16-2)\nOperator\nForward\nReverse\nIn-place\nDescription\n+\n__add__\n__radd__\n__iadd__\nAddition or concatenation\n-\n__sub__\n__rsub__\n__isub__\nSubtraction\n*\n__mul__\n__rmul__\n__imul__\nMultiplication or\nrepetition\n/\n__truediv__\n__rtruediv__\n__itruediv__\nTrue division\n//\n__floordiv__\n__rfloordiv__\n__ifloordiv__\nFloor division\n%\n__mod__\n__rmod__\n__imod__\nModulo\ndivmod()\n__divmod__\n__rdivmod__\n__idivmod__\nReturns tuple of floor\ndivision quotient and\nmodulo\n**, pow()\n__pow__\n__rpow__\n__ipow__\nExponentiationa\n@\n__matmul__\n__rmatmul__\n__imatmul__\nMatrix multiplication\n&\n__and__\n__rand__\n__iand__\nBitwise and\n|\n__or__\n__ror__\n__ior__\nBitwise or\n^\n__xor__\n__rxor__\n__ixor__\nBitwise xor\n<<\n__lshift__\n__rlshift__\n__ilshift__\nBitwise shift left\n>>\n__rshift__\n__rrshift__\n__irshift__\nBitwise shift right\na pow takes an optional third argument, modulo: pow(a, b, modulo), also supported by the special methods when\ninvoked directly (e.g., a.__pow__(b, modulo)).\nThe rich comparison operators use a different set of rules.\n576 \n| \nChapter 16: Operator Overloading",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": null,
      "content": "Rich Comparison Operators\nThe handling of the rich comparison operators ==, !=, >, <, >=, and <= by the Python\ninterpreter is similar to what we just saw, but differs in two important aspects:\n• The same set of methods is used in forward and reverse operator calls. The rules\nare summarized in Table 16-2. For example, in the case of ==, both the forward\nand reverse calls invoke __eq__, only swapping arguments; and a forward call to\n__gt__ is followed by a reverse call to __lt__ with the arguments swapped.\n• In the case of == and !=, if the reverse method is missing, or returns NotImplemen\nted, Python compares the object IDs instead of raising TypeError.\nTable 16-2. Rich comparison operators: reverse methods invoked when the initial method\ncall returns NotImplemented\nGroup\nInfix operator\nForward method call Reverse method call Fallback\nEquality\na == b\na.__eq__(b)\nb.__eq__(a)\nReturn id(a) == id(b)\na != b\na.__ne__(b)\nb.__ne__(a)\nReturn not (a == b)\nOrdering\na > b\na.__gt__(b)\nb.__lt__(a)\nRaise TypeError\na < b\na.__lt__(b)\nb.__gt__(a)\nRaise TypeError\na >= b\na.__ge__(b)\nb.__le__(a)\nRaise TypeError\na <= b\na.__le__(b)\nb.__ge__(a)\nRaise TypeError\nGiven these rules, let’s review and improve the behavior of the Vector.__eq__\nmethod, which was coded as follows in vector_v5.py (Example 12-16):\nclass Vector:\n    # many lines omitted\n    def __eq__(self, other):\n        return (len(self) == len(other) and\n                all(a == b for a, b in zip(self, other)))\nThat method produces the results in Example 16-13.\nExample 16-13. Comparing a Vector to a Vector, a Vector2d, and a tuple\n>>> va = Vector([1.0, 2.0, 3.0])\n>>> vb = Vector(range(1, 4))\n>>> va == vb  \nTrue\n>>> vc = Vector([1, 2])\n>>> from vector2d_v3 import Vector2d\n>>> v2d = Vector2d(1, 2)\n>>> vc == v2d  \nRich Comparison Operators \n| \n577",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": null,
      "content": "True\n>>> t3 = (1, 2, 3)\n>>> va == t3  \nTrue\nTwo Vector instances with equal numeric components compare equal.\nA Vector and a Vector2d are also equal if their components are equal.\nA Vector is also considered equal to a tuple or any iterable with numeric items\nof equal value.\nThe result in Example 16-13 is probably not desirable. Do we really want a Vector to\nbe considered equal to a tuple containing the same numbers? I have no hard rule\nabout this; it depends on the application context. The “Zen of Python” says:\nIn the face of ambiguity, refuse the temptation to guess.\nExcessive liberality in the evaluation of operands may lead to surprising results, and\nprogrammers hate surprises.\nTaking a clue from Python itself, we can see that [1,2] == (1, 2) is False. There‐\nfore, let’s be conservative and do some type checking. If the second operand is a\nVector instance (or an instance of a Vector subclass), then use the same logic as the\ncurrent __eq__. Otherwise, return NotImplemented and let Python handle that. See\nExample 16-14.\nExample 16-14. vector_v8.py: improved __eq__ in the Vector class\n    def __eq__(self, other):\n        if isinstance(other, Vector):  \n            return (len(self) == len(other) and\n                    all(a == b for a, b in zip(self, other)))\n        else:\n            return NotImplemented  \nIf the other operand is an instance of Vector (or of a Vector subclass), perform\nthe comparison as before.\nOtherwise, return NotImplemented.\nIf you run the tests in Example 16-13 with the new Vector.__eq__ from\nExample 16-14, what you get now is shown in Example 16-15.\n578 \n| \nChapter 16: Operator Overloading",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": null,
      "content": "Example 16-15. Same comparisons as Example 16-13: last result changed\n>>> va = Vector([1.0, 2.0, 3.0])\n>>> vb = Vector(range(1, 4))\n>>> va == vb  \nTrue\n>>> vc = Vector([1, 2])\n>>> from vector2d_v3 import Vector2d\n>>> v2d = Vector2d(1, 2)\n>>> vc == v2d  \nTrue\n>>> t3 = (1, 2, 3)\n>>> va == t3  \nFalse\nSame result as before, as expected.\nSame result as before, but why? Explanation coming up.\nDifferent result; this is what we wanted. But why does it work? Read on…\nAmong the three results in Example 16-15, the first one is no news, but the last two\nwere caused by __eq__ returning NotImplemented in Example 16-14. Here is what\nhappens in the example with a Vector and a Vector2d, vc == v2d, step-by-step:\n1. To evaluate vc == v2d, Python calls Vector.__eq__(vc, v2d).\n2. Vector.__eq__(vc, v2d) verifies that v2d is not a Vector and returns\nNotImplemented.\n3. Python gets the NotImplemented result, so it tries Vector2d.__eq__(v2d, vc).\n4. Vector2d.__eq__(v2d, vc) turns both operands into tuples and compares\nthem: the result is True (the code for Vector2d.__eq__ is in Example 11-11).\nAs for the comparison va == t3, between Vector and tuple in Example 16-15, the\nactual steps are:\n1. To evaluate va == t3, Python calls Vector.__eq__(va, t3).\n2. Vector.__eq__(va, t3) verifies that t3 is not a Vector and returns\nNotImplemented.\n3. Python gets the NotImplemented result, so it tries tuple.__eq__(t3, va).\nRich Comparison Operators \n| \n579",
      "content_length": 1438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": null,
      "content": "6 The logic for object.__eq__ and object.__ne__ is in function object_richcompare in Objects/typeobject.c\nin the CPython source code.\n4. tuple.__eq__(t3, va) has no idea what a Vector is, so it returns\nNotImplemented.\n5. In the special case of ==, if the reversed call returns NotImplemented, Python\ncompares object IDs as a last resort.\nWe don’t need to implement __ne__ for != because the fallback behavior of __ne__\ninherited from object suits us: when __eq__ is defined and does not return NotImple\nmented, __ne__ returns that result negated.\nIn other words, given the same objects we used in Example 16-15, the results for !=\nare consistent:\n>>> va != vb\nFalse\n>>> vc != v2d\nFalse\n>>> va != (1, 2, 3)\nTrue\nThe __ne__ inherited from object works like the following code—except that the\noriginal is written in C:6\n    def __ne__(self, other):\n        eq_result = self == other\n        if eq_result is NotImplemented:\n            return NotImplemented\n        else:\n            return not eq_result\nAfter covering the essentials of infix operator overloading, let’s turn to a different\nclass of operators: the augmented assignment operators.\nAugmented Assignment Operators\nOur Vector class already supports the augmented assignment operators += and *=.\nThat’s because augmented assignment works with immutable receivers by creating\nnew instances and rebinding the lefthand variable.\nExample 16-16 shows them in action.\nExample 16-16. Using += and *= with Vector instances\n>>> v1 = Vector([1, 2, 3])\n>>> v1_alias = v1  \n580 \n| \nChapter 16: Operator Overloading",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": null,
      "content": ">>> id(v1)  \n4302860128\n>>> v1 += Vector([4, 5, 6])  \n>>> v1  \nVector([5.0, 7.0, 9.0])\n>>> id(v1)  \n4302859904\n>>> v1_alias  \nVector([1.0, 2.0, 3.0])\n>>> v1 *= 11  \n>>> v1  \nVector([55.0, 77.0, 99.0])\n>>> id(v1)\n4302858336\nCreate an alias so we can inspect the Vector([1, 2, 3]) object later.\nRemember the ID of the initial Vector bound to v1.\nPerform augmented addition.\nThe expected result…\n…but a new Vector was created.\nInspect v1_alias to confirm the original Vector was not altered.\nPerform augmented multiplication.\nAgain, the expected result, but a new Vector was created.\nIf a class does not implement the in-place operators listed in Table 16-1, the augmen‐\nted assignment operators work as syntactic sugar: a += b is evaluated exactly as a =\na + b. That’s the expected behavior for immutable types, and if you have __add__,\nthen += will work with no additional code.\nHowever, if you do implement an in-place operator method such as __iadd__, that\nmethod is called to compute the result of a += b. As the name says, those operators\nare expected to change the lefthand operand in place, and not create a new object as\nthe result.\nThe in-place special methods should never be implemented for\nimmutable types like our Vector class. This is fairly obvious, but\nworth stating anyway.\nAugmented Assignment Operators \n| \n581",
      "content_length": 1327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": null,
      "content": "To show the code of an in-place operator, we will extend the BingoCage class from\nExample 13-9 to implement __add__ and __iadd__.\nWe’ll call the subclass AddableBingoCage. Example 16-17 is the behavior we want for\nthe + operator.\nExample 16-17. The + operator creates a new AddableBingoCage instance\n    >>> vowels = 'AEIOU'\n    >>> globe = AddableBingoCage(vowels)  \n    >>> globe.inspect()\n    ('A', 'E', 'I', 'O', 'U')\n    >>> globe.pick() in vowels  \n    True\n    >>> len(globe.inspect())  \n    4\n    >>> globe2 = AddableBingoCage('XYZ')  \n    >>> globe3 = globe + globe2\n    >>> len(globe3.inspect())  \n    7\n    >>> void = globe + [10, 20]  \n    Traceback (most recent call last):\n      ...\n    TypeError: unsupported operand type(s) for +: 'AddableBingoCage' and 'list'\nCreate a globe instance with five items (each of the vowels).\nPop one of the items, and verify it is one of the vowels.\nConfirm that the globe is down to four items.\nCreate a second instance, with three items.\nCreate a third instance by adding the previous two. This instance has seven\nitems.\nAttempting to add an AddableBingoCage to a list fails with TypeError. That\nerror message is produced by the Python interpreter when our __add__ method\nreturns NotImplemented.\nBecause an AddableBingoCage is mutable, Example 16-18 shows how it will work\nwhen we implement __iadd__.\n582 \n| \nChapter 16: Operator Overloading",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": null,
      "content": "Example 16-18. An existing AddableBingoCage can be loaded with += (continuing\nfrom Example 16-17)\n    >>> globe_orig = globe  \n    >>> len(globe.inspect())  \n    4\n    >>> globe += globe2  \n    >>> len(globe.inspect())\n    7\n    >>> globe += ['M', 'N']  \n    >>> len(globe.inspect())\n    9\n    >>> globe is globe_orig  \n    True\n    >>> globe += 1  \n    Traceback (most recent call last):\n      ...\n    TypeError: right operand in += must be 'Tombola' or an iterable\nCreate an alias so we can check the identity of the object later.\nglobe has four items here.\nAn AddableBingoCage instance can receive items from another instance of the\nsame class.\nThe righthand operand of += can also be any iterable.\nThroughout this example, globe has always referred to the same object as\nglobe_orig.\nTrying to add a noniterable to an AddableBingoCage fails with a proper error\nmessage.\nNote that the += operator is more liberal than + with regard to the second operand.\nWith +, we want both operands to be of the same type (AddableBingoCage, in this\ncase), because if we accepted different types, this might cause confusion as to the type\nof the result. With the +=, the situation is clearer: the lefthand object is updated in\nplace, so there’s no doubt about the type of the result.\nI validated the contrasting behavior of + and += by observing how\nthe list built-in type works. Writing my_list + x, you can only\nconcatenate one list to another list, but if you write my_list +=\nx, you can extend the lefthand list with items from any iterable x\non the righthand side. This is how the list.extend() method\nworks: it accepts any iterable argument.\nAugmented Assignment Operators \n| \n583",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": null,
      "content": "7 The iter built-in function will be covered in the next chapter. Here I could have used tuple(other), and it\nwould work, but at the cost of building a new tuple when all the .load(…) method needs is to iterate over its\nargument.\nNow that we are clear on the desired behavior for AddableBingoCage, we can look at\nits implementation in Example 16-19. Recall that BingoCage, from Example 13-9, is a\nconcrete subclass of the Tombola ABC from Example 13-7.\nExample 16-19. bingoaddable.py: AddableBingoCage extends BingoCage to support +\nand +=\nfrom tombola import Tombola\nfrom bingo import BingoCage\nclass AddableBingoCage(BingoCage):  \n    def __add__(self, other):\n        if isinstance(other, Tombola):  \n            return AddableBingoCage(self.inspect() + other.inspect())\n        else:\n            return NotImplemented\n    def __iadd__(self, other):\n        if isinstance(other, Tombola):\n            other_iterable = other.inspect()  \n        else:\n            try:\n                other_iterable = iter(other)  \n            except TypeError:  \n                msg = ('right operand in += must be '\n                       \"'Tombola' or an iterable\")\n                raise TypeError(msg)\n        self.load(other_iterable)  \n        return self  \nAddableBingoCage extends BingoCage.\nOur __add__ will only work with an instance of Tombola as the second operand.\nIn __iadd__, retrieve items from other, if it is an instance of Tombola.\nOtherwise, try to obtain an iterator over other.7\nIf that fails, raise an exception explaining what the user should do. When possi‐\nble, error messages should explicitly guide the user to the solution.\n584 \n| \nChapter 16: Operator Overloading",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 615,
      "chapter": null,
      "content": "If we got this far, we can load the other_iterable into self.\nVery important: augmented assignment special methods of mutable objects must\nreturn self. That’s what users expect.\nWe can summarize the whole idea of in-place operators by contrasting the return\nstatements that produce results in __add__ and __iadd__ in Example 16-19:\n__add__\nThe result is produced by calling the constructor AddableBingoCage to build a\nnew instance.\n__iadd__\nThe result is produced by returning self, after it has been modified.\nTo wrap up this example, a final observation on Example 16-19: by design, no\n__radd__ was coded in AddableBingoCage, because there is no need for it. The for‐\nward method __add__ will only deal with righthand operands of the same type, so if\nPython is trying to compute a + b, where a is an AddableBingoCage and b is not, we\nreturn NotImplemented—maybe the class of b can make it work. But if the expression\nis b + a and b is not an AddableBingoCage, and it returns NotImplemented, then it’s\nbetter to let Python give up and raise TypeError because we cannot handle b.\nIn general, if a forward infix operator method (e.g., __mul__) is\ndesigned to work only with operands of the same type as self, it’s\nuseless to implement the corresponding reverse method (e.g.,\n__rmul__) because that, by definition, will only be invoked when\ndealing with an operand of a different type.\nThis concludes our exploration of operator overloading in Python.\nChapter Summary\nWe started this chapter by reviewing some restrictions Python imposes on operator\noverloading: no redefining of operators in the built-in types themselves, overloading\nlimited to existing operators, with a few operators left out (is, and, or, not).\nWe got down to business with the unary operators, implementing __neg__ and\n__pos__. Next came the infix operators, starting with +, supported by the __add__\nmethod. We saw that unary and infix operators are supposed to produce results by\ncreating new objects, and should never change their operands. To support operations\nwith other types, we return the NotImplemented special value—not an exception—\nallowing the interpreter to try again by swapping the operands and calling the reverse\nChapter Summary \n| \n585",
      "content_length": 2226,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": null,
      "content": "special method for that operator (e.g., __radd__). The algorithm Python uses to han‐\ndle infix operators is summarized in the flowchart in Figure 16-1.\nMixing operand types requires detecting operands we can’t handle. In this chapter,\nwe did this in two ways: in the duck typing way, we just went ahead and tried the\noperation, catching a TypeError exception if it happened; later, in __mul__ and\n__matmul__, we did it with an explicit isinstance test. There are pros and cons to\nthese approaches: duck typing is more flexible, but explicit type checking is more\npredictable.\nIn general, libraries should leverage duck typing—opening the door for objects\nregardless of their types, as long as they support the necessary operations. However,\nPython’s operator dispatch algorithm may produce misleading error messages or\nunexpected results when combined with duck typing. For this reason, the discipline\nof type checking using isinstance calls against ABCs is often useful when writing\nspecial methods for operator overloading. That’s the technique dubbed goose typing\nby Alex Martelli—which we saw in “Goose Typing” on page 442. Goose typing is a\ngood compromise between flexibility and safety, because existing or future user-\ndefined types can be declared as actual or virtual subclasses of an ABC. In addition, if\nan ABC implements the __subclasshook__, then objects pass isinstance checks\nagainst that ABC by providing the required methods—no subclassing or registration\nrequired.\nThe next topic we covered was the rich comparison operators. We implemented ==\nwith __eq__ and discovered that Python provides a handy implementation of != in\nthe __ne__ inherited from the object base class. The way Python evaluates these\noperators along with >, <, >=, and <= is slightly different, with special logic for choos‐\ning the reverse method, and fallback handling for == and !=, which never generate\nerrors because Python compares the object IDs as a last resort.\nIn the last section, we focused on augmented assignment operators. We saw that\nPython handles them by default as a combination of plain operator followed by\nassignment, that is: a += b is evaluated exactly as a = a + b. That always creates a\nnew object, so it works for mutable or immutable types. For mutable objects, we can\nimplement in-place special methods such as __iadd__ for +=, and alter the value of\nthe lefthand operand. To show this at work, we left behind the immutable Vector\nclass and worked on implementing a BingoCage subclass to support += for adding\nitems to the random pool, similar to the way the list built-in supports += as a short‐\ncut for the list.extend() method. While doing this, we discussed how + tends to be\nstricter than += regarding the types it accepts. For sequence types, + usually requires\nthat both operands are of the same type, while += often accepts any iterable as the\nrighthand operand.\n586 \n| \nChapter 16: Operator Overloading",
      "content_length": 2929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 617,
      "chapter": null,
      "content": "Further Reading\nGuido van Rossum wrote a good defense of operator overloading in “Why operators\nare useful”. Trey Hunner blogged “Tuple ordering and deep comparisons in Python”,\narguing that the rich comparison operators in Python are more flexible and powerful\nthan programmers may realize when coming from other languages.\nOperator overloading is one area of Python programming where isinstance tests are\ncommon. The best practice around such tests is goose typing, covered in “Goose Typ‐\ning” on page 442. If you skipped that, make sure to read it.\nThe main reference for the operator special methods is the “Data Model” chapter of\nthe Python documentation. Another relevant reading is “9.1.2.2. Implementing the\narithmetic operations” in the numbers module of The Python Standard Library.\nA clever example of operator overloading appeared in the pathlib package, added in\nPython 3.4. Its Path class overloads the / operator to build filesystem paths from\nstrings, as shown in this example from the documentation:\n>>> p = Path('/etc')\n>>> q = p / 'init.d' / 'reboot'\n>>> q\nPosixPath('/etc/init.d/reboot')\nAnother nonarithmetic example of operator overloading is in the Scapy library, used\nto “send, sniff, dissect, and forge network packets.” In Scapy, the / operator builds\npackets by stacking fields from different network layers. See “Stacking layers” for\ndetails.\nIf you are about to implement comparison operators, study functools.total_order\ning. That is a class decorator that automatically generates methods for all rich com‐\nparison operators in any class that defines at least a couple of them. See the functools\nmodule docs.\nIf you are curious about operator method dispatching in languages with dynamic\ntyping, two seminal readings are “A Simple Technique for Handling Multiple Poly‐\nmorphism” by Dan Ingalls (member of the original Smalltalk team), and “Arithmetic\nand Double Dispatching in Smalltalk-80” by Kurt J. Hebel and Ralph Johnson (John‐\nson became famous as one of the authors of the original Design Patterns book). Both\npapers provide deep insight into the power of polymorphism in languages with\ndynamic typing, like Smalltalk, Python, and Ruby. Python does not use double dis‐\npatching for handling operators as described in those articles. The Python algorithm\nusing forward and reverse operators is easier for user-defined classes to support than\ndouble dispatching, but requires special handling by the interpreter. In contrast,\nclassic double dispatching is a general technique you can use in Python or any\nFurther Reading \n| \n587",
      "content_length": 2563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": null,
      "content": "object-oriented language beyond the specific context of infix operators, and in fact\nIngalls, Hebel, and Johnson use very different examples to describe it.\nThe article, “The C Family of Languages: Interview with Dennis Ritchie, Bjarne\nStroustrup, and James Gosling”, from which I quoted the epigraph for this chapter,\nappeared in Java Report, 5(7), July 2000, and C++ Report, 12(7), July/August 2000,\nalong with two other snippets I used in this chapter’s “Soapbox” (next). If you are\ninto programming language design, do yourself a favor and read that interview.\nSoapbox\nOperator Overloading: Pros and Cons\nJames Gosling, quoted at the start of this chapter, made the conscious decision to\nleave operator overloading out when he designed Java. In that same interview (“The\nC Family of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup, and James\nGosling”) he says:\nProbably about 20 to 30 percent of the population think of operator overloading as\nthe spawn of the devil; somebody has done something with operator overloading that\nhas just really ticked them off, because they’ve used like + for list insertion and it\nmakes life really, really confusing. A lot of that problem stems from the fact that\nthere are only about half a dozen operators you can sensibly overload, and yet there\nare thousands or millions of operators that people would like to define—so you have\nto pick, and often the choices conflict with your sense of intuition.\nGuido van Rossum picked the middle way in supporting operator overloading: he did\nnot leave the door open for users creating new arbitrary operators like <=> or :-),\nwhich prevents a Tower of Babel of custom operators, and allows the Python parser\nto be simple. Python also does not let you overload the operators of the built-in types,\nanother limitation that promotes readability and predictable performance.\nGosling goes on to say:\nThen there’s a community of about 10 percent that have actually used operator over‐\nloading appropriately and who really care about it, and for whom it’s actually really\nimportant; this is almost exclusively people who do numerical work, where the nota‐\ntion is very important to appealing to people’s intuition, because they come into it\nwith an intuition about what the + means, and the ability to say “a + b” where a and b\nare complex numbers or matrices or something really does make sense.\nOf course, there are benefits to disallowing operator overloading in a language. I’ve\nseen the argument that C is better than C++ for systems programming because oper‐\nator overloading in C++ can make costly operations seem trivial. Two successful\nmodern languages that compile to binary executables made opposite choices: Go\ndoesn’t have operator overloading, but Rust does.\n588 \n| \nChapter 16: Operator Overloading",
      "content_length": 2799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": null,
      "content": "But overloaded operators, when used sensibly, do make code easier to read and write.\nIt’s a great feature to have in a modern high-level language.\nA Glimpse at Lazy Evaluation\nIf you look closely at the traceback in Example 16-9, you’ll see evidence of the lazy\nevaluation of generator expressions. Example 16-20 is that same traceback, now with\ncallouts.\nExample 16-20. Same as Example 16-9\n>>> v1 + 'ABC'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"vector_v6.py\", line 329, in __add__\n    return Vector(a + b for a, b in pairs)  \n  File \"vector_v6.py\", line 243, in __init__\n    self._components = array(self.typecode, components)  \n  File \"vector_v6.py\", line 329, in <genexpr>\n    return Vector(a + b for a, b in pairs)  \nTypeError: unsupported operand type(s) for +: 'float' and 'str'\nThe Vector call gets a generator expression as its components argument. No\nproblem at this stage.\nThe components genexp is passed to the array constructor. Within the array\nconstructor, Python tries to iterate over the genexp, causing the evaluation of the\nfirst item a + b. That’s when the TypeError occurs.\nThe exception propagates to the Vector constructor call, where it is reported.\nThis shows how the generator expression is evaluated at the latest possible moment,\nand not where it is defined in the source code.\nIn contrast, if the Vector constructor was invoked as Vector([a + b for a, b in\npairs]), then the exception would happen right there, because the list comprehen‐\nsion tried to build a list to be passed as the argument to the Vector() call. The body\nof Vector.__init__ would not be reached at all.\nChapter 17 will cover generator expressions in detail, but I did not want to let this\naccidental demonstration of their lazy nature go unnoticed.\nFurther Reading \n| \n589",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 621,
      "chapter": null,
      "content": "PART IV\n\nControl Flow",
      "content_length": 21,
      "extraction_method": "OCR"
    },
    {
      "page_number": 622,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 623,
      "chapter": null,
      "content": "1 From “Revenge of the Nerds”, a blog post.\nCHAPTER 17\nIterators, Generators,\nand Classic Coroutines\nWhen I see patterns in my programs, I consider it a sign of trouble. The shape of a\nprogram should reflect only the problem it needs to solve. Any other regularity in the\ncode is a sign, to me at least, that I’m using abstractions that aren’t powerful enough—\noften that I’m generating by hand the expansions of some macro that I need to write.\n—Paul Graham, Lisp hacker and venture capitalist1\nIteration is fundamental to data processing: programs apply computations to data\nseries, from pixels to nucleotides. If the data doesn’t fit in memory, we need to fetch\nthe items lazily—one at a time and on demand. That’s what an iterator does. This\nchapter shows how the Iterator design pattern is built into the Python language so\nyou never need to code it by hand.\nEvery standard collection in Python is iterable. An iterable is an object that provides\nan iterator, which Python uses to support operations like:\n• for loops\n• List, dict, and set comprehensions\n• Unpacking assignments\n• Construction of collection instances\n593",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": null,
      "content": "This chapter covers the following topics:\n• How Python uses the iter() built-in function to handle iterable objects\n• How to implement the classic Iterator pattern in Python\n• How the classic Iterator pattern can be replaced by a generator function or gen‐\nerator expression\n• How a generator function works in detail, with line-by-line descriptions\n• Leveraging the general-purpose generator functions in the standard library\n• Using yield from expressions to combine generators\n• Why generators and classic coroutines look alike but are used in very different\nways and should not be mixed\nWhat’s New in This Chapter\n“Subgenerators with yield from” on page 632 grew from one to six pages. It now\nincludes simpler experiments demonstrating the behavior of generators with yield\nfrom, and an example of traversing a tree data structure, developed step-by-step.\nNew sections explain the type hints for Iterable, Iterator, and Generator types.\nThe last major section of this chapter, “Classic Coroutines” on page 641, is a 9-page\nintroduction to a topic that filled a 40-page chapter in the first edition. I updated and\nmoved the “Classic Coroutines” chapter to a post in the companion website because\nit was the most challenging chapter for readers, but its subject matter is less relevant\nafter Python 3.5 introduced native coroutines—which we’ll study in Chapter 21.\nWe’ll get started studying how the iter() built-in function makes sequences iterable.\nA Sequence of Words\nWe’ll start our exploration of iterables by implementing a Sentence class: you give its\nconstructor a string with some text, and then you can iterate word by word. The first\nversion will implement the sequence protocol, and it’s iterable because all sequences\nare iterable—as we’ve seen since Chapter 1. Now we’ll see exactly why.\nExample 17-1 shows a Sentence class that extracts words from a text by index.\nExample 17-1. sentence.py: a Sentence as a sequence of words\nimport re\nimport reprlib\n594 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 625,
      "chapter": null,
      "content": "2 We first used reprlib in “Vector Take #1: Vector2d Compatible” on page 399.\nRE_WORD = re.compile(r'\\w+')\nclass Sentence:\n    def __init__(self, text):\n        self.text = text\n        self.words = RE_WORD.findall(text)  \n    def __getitem__(self, index):\n        return self.words[index]  \n    def __len__(self):  \n        return len(self.words)\n    def __repr__(self):\n        return 'Sentence(%s)' % reprlib.repr(self.text)  \n.findall returns a list with all nonoverlapping matches of the regular expres‐\nsion, as a list of strings.\nself.words holds the result of .findall, so we simply return the word at the\ngiven index.\nTo complete the sequence protocol, we implement __len__ although it is not\nneeded to make an iterable.\nreprlib.repr is a utility function to generate abbreviated string representations\nof data structures that can be very large.2\nBy default, reprlib.repr limits the generated string to 30 characters. See the console\nsession in Example 17-2 to see how Sentence is used.\nExample 17-2. Testing iteration on a Sentence instance\n>>> s = Sentence('\"The time has come,\" the Walrus said,')  \n>>> s\nSentence('\"The time ha... Walrus said,')  \n>>> for word in s:  \n...     print(word)\nThe\ntime\nhas\ncome\nthe\nA Sequence of Words \n| \n595",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": null,
      "content": "Walrus\nsaid\n>>> list(s)  \n['The', 'time', 'has', 'come', 'the', 'Walrus', 'said']\nA sentence is created from a string.\nNote the output of __repr__ using ... generated by reprlib.repr.\nSentence instances are iterable; we’ll see why in a moment.\nBeing iterable, Sentence objects can be used as input to build lists and other\niterable types.\nIn the following pages, we’ll develop other Sentence classes that pass the tests in\nExample 17-2. However, the implementation in Example 17-1 is different from the\nothers because it’s also a sequence, so you can get words by index:\n>>> s[0]\n'The'\n>>> s[5]\n'Walrus'\n>>> s[-1]\n'said'\nPython programmers know that sequences are iterable. Now we’ll see precisely why.\nWhy Sequences Are Iterable: The iter Function\nWhenever Python needs to iterate over an object x, it automatically calls iter(x).\nThe iter built-in function:\n1. Checks whether the object implements __iter__, and calls that to obtain an\niterator.\n2. If __iter__ is not implemented, but __getitem__ is, then iter() creates an iter‐\nator that tries to fetch items by index, starting from 0 (zero).\n3. If that fails, Python raises TypeError, usually saying 'C' object is not itera\nble, where C is the class of the target object.\nThat is why all Python sequences are iterable: by definition, they all implement\n__getitem__. In fact, the standard sequences also implement __iter__, and yours\nshould too, because iteration via __getitem__ exists for backward compatibility and\nmay be gone in the future—although it is not deprecated as of Python 3.10, and I\ndoubt it will ever be removed.\n596 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": null,
      "content": "As mentioned in “Python Digs Sequences” on page 436, this is an extreme form of\nduck typing: an object is considered iterable not only when it implements the special\nmethod __iter__, but also when it implements __getitem__. Take a look:\n>>> class Spam:\n...     def __getitem__(self, i):\n...         print('->', i)\n...         raise IndexError()\n...\n>>> spam_can = Spam()\n>>> iter(spam_can)\n<iterator object at 0x10a878f70>\n>>> list(spam_can)\n-> 0\n[]\n>>> from collections import abc\n>>> isinstance(spam_can, abc.Iterable)\nFalse\nIf a class provides __getitem__, the iter() built-in accepts an instance of that class\nas iterable and builds an iterator from the instance. Python’s iteration machinery will\ncall __getitem__ with indexes starting from 0, and will take an IndexError as a\nsignal that there are no more items.\nNote that although spam_can is iterable (its __getitem__ could provide items), it is\nnot recognized as such by an isinstance against abc.Iterable.\nIn the goose-typing approach, the definition for an iterable is simpler but not as flexi‐\nble: an object is considered iterable if it implements the __iter__ method. No sub‐\nclassing or registration is required, because abc.Iterable implements the\n__subclasshook__, as seen in “Structural Typing with ABCs” on page 464. Here is a\ndemonstration:\n>>> class GooseSpam:\n...     def __iter__(self):\n...         pass\n...\n>>> from collections import abc\n>>> issubclass(GooseSpam, abc.Iterable)\nTrue\n>>> goose_spam_can = GooseSpam()\n>>> isinstance(goose_spam_can, abc.Iterable)\nTrue\nAs of Python 3.10, the most accurate way to check whether an\nobject x is iterable is to call iter(x) and handle a TypeError excep‐\ntion if it isn’t. This is more accurate than using isinstance(x,\nabc.Iterable), because iter(x) also considers the legacy\n__getitem__ method, while the Iterable ABC does not.\nWhy Sequences Are Iterable: The iter Function \n| \n597",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": null,
      "content": "Explicitly checking whether an object is iterable may not be worthwhile if right after\nthe check you are going to iterate over the object. After all, when the iteration is\nattempted on a noniterable, the exception Python raises is clear enough: TypeError:\n'C' object is not iterable. If you can do better than just raising TypeError, then\ndo so in a try/except block instead of doing an explicit check. The explicit check\nmay make sense if you are holding on to the object to iterate over it later; in this case,\ncatching the error early makes debugging easier.\nThe iter() built-in is more often used by Python itself than by our own code.\nThere’s a second way we can use it, but it’s not widely known.\nUsing iter with a Callable\nWe can call iter() with two arguments to create an iterator from a function or any\ncallable object. In this usage, the first argument must be a callable to be invoked\nrepeatedly (with no arguments) to produce values, and the second argument is a sen‐\ntinel: a marker value which, when returned by the callable, causes the iterator to raise\nStopIteration instead of yielding the sentinel.\nThe following example shows how to use iter to roll a six-sided die until a 1 is rolled:\n>>> def d6():\n...     return randint(1, 6)\n...\n>>> d6_iter = iter(d6, 1)\n>>> d6_iter\n<callable_iterator object at 0x10a245270>\n>>> for roll in d6_iter:\n...     print(roll)\n...\n4\n3\n6\n3\nNote that the iter function here returns a callable_iterator. The for loop in the\nexample may run for a very long time, but it will never display 1, because that is the\nsentinel value. As usual with iterators, the d6_iter object in the example becomes\nuseless once exhausted. To start over, we must rebuild the iterator by invoking\niter() again.\nThe documentation for iter includes the following explanation and example code:\nOne useful application of the second form of iter() is to build a block-reader. For\nexample, reading fixed-width blocks from a binary database file until the end of file is\nreached:\nfrom functools import partial\n598 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": null,
      "content": "with open('mydata.db', 'rb') as f:\n    read64 = partial(f.read, 64)\n    for block in iter(read64, b''):\n        process_block(block)\nFor clarity, I’ve added the read64 assignment, which is not in the original example.\nThe partial() function is necessary because the callable given to iter() must not\nrequire arguments. In the example, an empty bytes object is the sentinel, because\nthat’s what f.read returns when there are no more bytes to read.\nThe next section details the relationship between iterables and iterators.\nIterables Versus Iterators\nFrom the explanation in “Why Sequences Are Iterable: The iter Function” on page\n596 we can extrapolate a definition:\niterable\nAny object from which the iter built-in function can obtain an iterator. Objects\nimplementing an __iter__ method returning an iterator are iterable. Sequences\nare always iterable, as are objects implementing a __getitem__ method that\naccepts 0-based indexes.\nIt’s important to be clear about the relationship between iterables and iterators:\nPython obtains iterators from iterables.\nHere is a simple for loop iterating over a str. The str 'ABC' is the iterable here. You\ndon’t see it, but there is an iterator behind the curtain:\n>>> s = 'ABC'\n>>> for char in s:\n...     print(char)\n...\nA\nB\nC\nIf there was no for statement and we had to emulate the for machinery by hand with\na while loop, this is what we’d have to write:\n>>> s = 'ABC'\n>>> it = iter(s)  \n>>> while True:\n...     try:\n...         print(next(it))  \n...     except StopIteration:  \n...         del it  \n...         break  \n...\nA\nIterables Versus Iterators \n| \n599",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": null,
      "content": "B\nC\nBuild an iterator it from the iterable.\nRepeatedly call next on the iterator to obtain the next item.\nThe iterator raises StopIteration when there are no further items.\nRelease reference to it—the iterator object is discarded.\nExit the loop.\nStopIteration signals that the iterator is exhausted. This exception is handled inter‐\nnally by the iter() built-in that is part of the logic of for loops and other iteration\ncontexts like list comprehensions, iterable unpacking, etc.\nPython’s standard interface for an iterator has two methods:\n__next__\nReturns the next item in the series, raising StopIteration if there are no more.\n__iter__\nReturns self; this allows iterators to be used where an iterable is expected, for\nexample, in a for loop.\nThat interface is formalized in the collections.abc.Iterator ABC, which declares\nthe __next__ abstract method, and subclasses Iterable—where the abstract\n__iter__ method is declared. See Figure 17-1.\nFigure 17-1. The Iterable and Iterator ABCs. Methods in italic are abstract. A con‐\ncrete Iterable.__iter__ should return a new Iterator instance. A concrete Itera\ntor must implement __next__. The Iterator.__iter__ method just returns the\ninstance itself.\nThe source code for collections.abc.Iterator is in Example 17-3.\n600 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": null,
      "content": "Example 17-3. abc.Iterator class; extracted from Lib/_collections_abc.py\nclass Iterator(Iterable):\n    __slots__ = ()\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n    def __iter__(self):\n        return self\n    @classmethod\n    def __subclasshook__(cls, C):  \n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')  \n        return NotImplemented\n__subclasshook__ supports structural type checks with isinstance and issub\nclass. We saw it in “Structural Typing with ABCs” on page 464.\n_check_methods traverses the __mro__ of the class to check whether the methods\nare implemented in its base classes. It’s defined in that same Lib/_collec‐\ntions_abc.py module. If the methods are implemented, the C class will be recog‐\nnized as a virtual subclass of Iterator. In other words, issubclass(C,\nIterable) will return True.\nThe Iterator ABC abstract method is it.__next__() in Python 3\nand it.next() in Python 2. As usual, you should avoid calling spe‐\ncial methods directly. Just use the next(it): this built-in function\ndoes the right thing in Python 2 and 3—which is useful for those\nmigrating codebases from 2 to 3.\nThe Lib/types.py module source code in Python 3.9 has a comment that says:\n# Iterators in Python aren't a matter of type but of protocol.  A large\n# and changing number of builtin types implement *some* flavor of\n# iterator.  Don't check the type!  Use hasattr to check for both\n# \"__iter__\" and \"__next__\" attributes instead.\nIn fact, that’s exactly what the __subclasshook__ method of the abc.Iterator ABC\ndoes.\nIterables Versus Iterators \n| \n601",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": null,
      "content": "Given the advice from Lib/types.py and the logic implemented in\nLib/_collections_abc.py, the best way to check if an object x is an\niterator is to call isinstance(x, abc.Iterator). Thanks to Itera\ntor.__subclasshook__, this test works even if the class of x is not\na real or virtual subclass of Iterator.\nBack to our Sentence class from Example 17-1, you can clearly see how the iterator is\nbuilt by iter() and consumed by next() using the Python console:\n>>> s3 = Sentence('Life of Brian')  \n>>> it = iter(s3)  \n>>> it  # doctest: +ELLIPSIS\n<iterator object at 0x...>\n>>> next(it)  \n'Life'\n>>> next(it)\n'of'\n>>> next(it)\n'Brian'\n>>> next(it)  \nTraceback (most recent call last):\n  ...\nStopIteration\n>>> list(it)  \n[]\n>>> list(iter(s3))  \n['Life', 'of', 'Brian']\nCreate a sentence s3 with three words.\nObtain an iterator from s3.\nnext(it) fetches the next word.\nThere are no more words, so the iterator raises a StopIteration exception.\nOnce exhausted, an iterator will always raise StopIteration, which makes it\nlook like it’s empty.\nTo go over the sentence again, a new iterator must be built.\nBecause the only methods required of an iterator are __next__ and __iter__, there is\nno way to check whether there are remaining items, other than to call next() and\ncatch StopIteration. Also, it’s not possible to “reset” an iterator. If you need to start\nover, you need to call iter() on the iterable that built the iterator in the first place.\nCalling iter() on the iterator itself won’t help either, because—as mentioned—\n602 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 633,
      "chapter": null,
      "content": "3 Thanks to tech reviewer Leonardo Rochael for this fine example.\nIterator.__iter__ is implemented by returning self, so this will not reset a deple‐\nted iterator.\nThat minimal interface is sensible, because in reality not all iterators are resettable.\nFor example, if an iterator is reading packets from the network, there’s no way to\nrewind it.3\nThe first version of Sentence from Example 17-1 was iterable thanks to the special\ntreatment the iter() built-in gives to sequences. Next, we will implement Sentence\nvariations that implement __iter__ to return iterators.\nSentence Classes with __iter__\nThe next variations of Sentence implement the standard iterable protocol, first by\nimplementing the Iterator design pattern, and then with generator functions.\nSentence Take #2: A Classic Iterator\nThe next Sentence implementation follows the blueprint of the classic Iterator design\npattern from the Design Patterns book. Note that it is not idiomatic Python, as the\nnext refactorings will make very clear. But it is useful to show the distinction between\nan iterable collection and an iterator that works with it.\nThe Sentence class in Example 17-4 is iterable because it implements the __iter__\nspecial method, which builds and returns a SentenceIterator. That’s how an itera‐\nble and an iterator are related.\nExample 17-4. sentence_iter.py: Sentence implemented using the Iterator pattern\nimport re\nimport reprlib\nRE_WORD = re.compile(r'\\w+')\nclass Sentence:\n    def __init__(self, text):\n        self.text = text\n        self.words = RE_WORD.findall(text)\n    def __repr__(self):\n        return f'Sentence({reprlib.repr(self.text)})'\nSentence Classes with __iter__ \n| \n603",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 634,
      "chapter": null,
      "content": "def __iter__(self):  \n        return SentenceIterator(self.words)  \nclass SentenceIterator:\n    def __init__(self, words):\n        self.words = words  \n        self.index = 0  \n    def __next__(self):\n        try:\n            word = self.words[self.index]  \n        except IndexError:\n            raise StopIteration()  \n        self.index += 1  \n        return word  \n    def __iter__(self):  \n        return self\nThe __iter__ method is the only addition to the previous Sentence implemen‐\ntation. This version has no __getitem__, to make it clear that the class is iterable\nbecause it implements __iter__.\n__iter__ fulfills the iterable protocol by instantiating and returning an iterator.\nSentenceIterator holds a reference to the list of words.\nself.index determines the next word to fetch.\nGet the word at self.index.\nIf there is no word at self.index, raise StopIteration.\nIncrement self.index.\nReturn the word.\nImplement self.__iter__.\nThe code in Example 17-4 passes the tests in Example 17-2.\nNote that implementing __iter__ in SentenceIterator is not actually needed\nfor this example to work, but it is the right thing to do: iterators are supposed to\nimplement both __next__ and __iter__, and doing so makes our iterator pass\nthe issubclass(SentenceIterator, abc.Iterator) test. If we had subclassed\n604 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 635,
      "chapter": null,
      "content": "SentenceIterator from abc.Iterator, we’d inherit the concrete abc.Itera\ntor.__iter__ method.\nThat is a lot of work (for us spoiled Python programmers, anyway). Note how most\ncode in SentenceIterator deals with managing the internal state of the iterator.\nSoon we’ll see how to avoid that bookkeeping. But first, a brief detour to address an\nimplementation shortcut that may be tempting, but is just wrong.\nDon’t Make the Iterable an Iterator for Itself\nA common cause of errors in building iterables and iterators is to confuse the two. To\nbe clear: iterables have an __iter__ method that instantiates a new iterator every\ntime. Iterators implement a __next__ method that returns individual items, and an\n__iter__ method that returns self.\nTherefore, iterators are also iterable, but iterables are not iterators.\nIt may be tempting to implement __next__ in addition to __iter__ in the Sentence\nclass, making each Sentence instance at the same time an iterable and iterator over\nitself. But this is rarely a good idea. It’s also a common antipattern, according to Alex\nMartelli who has a lot of experience reviewing Python code at Google.\nThe “Applicability” section about the Iterator design pattern in the Design Patterns\nbook says:\nUse the Iterator pattern\n• to access an aggregate object’s contents without exposing its internal\nrepresentation.\n• to support multiple traversals of aggregate objects.\n• to provide a uniform interface for traversing different aggregate structures (that\nis, to support polymorphic iteration).\nTo “support multiple traversals,” it must be possible to obtain multiple independent\niterators from the same iterable instance, and each iterator must keep its own internal\nstate, so a proper implementation of the pattern requires each call to iter(my_itera\nble) to create a new, independent, iterator. That is why we need the SentenceItera\ntor class in this example.\nNow that the classic Iterator pattern is properly demonstrated, we can let it go.\nPython incorporated the yield keyword from Barbara Liskov’s CLU language, so we\ndon’t need to “generate by hand” the code to implement iterators.\nThe next sections present more idiomatic versions of Sentence.\nSentence Classes with __iter__ \n| \n605",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 636,
      "chapter": null,
      "content": "4 When reviewing this code, Alex Martelli suggested the body of this method could simply be return\niter(self.words). He is right: the result of calling self.words.__iter__() would also be an iterator, as it\nshould be. However, I used a for loop with yield here to introduce the syntax of a generator function, which\nrequires the yield keyword, as we’ll see in the next section. During review of the second edition of this book,\nLeonardo Rochael suggested yet another shortcut for the body of __iter__: yield from self.words. We’ll\nalso cover yield from later in this chapter.\nSentence Take #3: A Generator Function\nA Pythonic implementation of the same functionality uses a generator, avoiding all\nthe work to implement the SentenceIterator class. A proper explanation of the gen‐\nerator comes right after Example 17-5.\nExample 17-5. sentence_gen.py: Sentence implemented using a generator\nimport re\nimport reprlib\nRE_WORD = re.compile(r'\\w+')\nclass Sentence:\n    def __init__(self, text):\n        self.text = text\n        self.words = RE_WORD.findall(text)\n    def __repr__(self):\n        return 'Sentence(%s)' % reprlib.repr(self.text)\n    def __iter__(self):\n        for word in self.words:  \n            yield word  \n        \n# done! \nIterate over self.words.\nYield the current word.\nExplicit return is not necessary; the function can just “fall through” and return\nautomatically. Either way, a generator function doesn’t raise StopIteration: it\nsimply exits when it’s done producing values.4\nNo need for a separate iterator class!\n606 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 637,
      "chapter": null,
      "content": "5 Sometimes I add a gen prefix or suffix when naming generator functions, but this is not a common practice.\nAnd you can’t do that if you’re implementing an iterable, of course: the necessary special method must be\nnamed __iter__.\n6 Thanks to David Kwast for suggesting this example.\nHere again we have a different implementation of Sentence that passes the tests in\nExample 17-2.\nBack in the Sentence code in Example 17-4, __iter__ called the SentenceIterator\nconstructor to build an iterator and return it. Now the iterator in Example 17-5 is in\nfact a generator object, built automatically when the __iter__ method is called,\nbecause __iter__ here is a generator function.\nA full explanation of generators follows.\nHow a Generator Works\nAny Python function that has the yield keyword in its body is a generator function: a\nfunction which, when called, returns a generator object. In other words, a generator\nfunction is a generator factory.\nThe only syntax distinguishing a plain function from a generator\nfunction is the fact that the latter has a yield keyword somewhere\nin its body. Some argued that a new keyword like gen should be\nused instead of def to declare generator functions, but Guido did\nnot agree. His arguments are in PEP 255 — Simple Generators.5\nExample 17-6 shows the behavior of a simple generator function.6\nExample 17-6. A generator function that yields three numbers\n>>> def gen_123():\n...     yield 1  \n...     yield 2\n...     yield 3\n...\n>>> gen_123  # doctest: +ELLIPSIS\n<function gen_123 at 0x...>  \n>>> gen_123()   # doctest: +ELLIPSIS\n<generator object gen_123 at 0x...>  \n>>> for i in gen_123():  \n...     print(i)\n1\n2\nSentence Classes with __iter__ \n| \n607",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 638,
      "chapter": null,
      "content": "3\n>>> g = gen_123()  \n>>> next(g)  \n1\n>>> next(g)\n2\n>>> next(g)\n3\n>>> next(g)  \nTraceback (most recent call last):\n  ...\nStopIteration\nThe body of a generator function often has yield inside a loop, but not necessar‐\nily; here I just repeat yield three times.\nLooking closely, we see gen_123 is a function object.\nBut when invoked, gen_123() returns a generator object.\nGenerator objects implement the Iterator interface, so they are also iterable.\nWe assign this new generator object to g, so we can experiment with it.\nBecause g is an iterator, calling next(g) fetches the next item produced by yield.\nWhen the generator function returns, the generator object raises StopIteration.\nA generator function builds a generator object that wraps the body of the function.\nWhen we invoke next() on the generator object, execution advances to the next\nyield in the function body, and the next() call evaluates to the value yielded when\nthe function body is suspended. Finally, the enclosing generator object created by\nPython raises StopIteration when the function body returns, in accordance with the\nIterator protocol.\nI find it helpful to be rigorous when talking about values obtained\nfrom a generator. It’s confusing to say a generator “returns” values.\nFunctions return values. Calling a generator function returns a\ngenerator. A generator yields values. A generator doesn’t “return”\nvalues in the usual way: the return statement in the body of a gen‐\nerator function causes StopIteration to be raised by the generator\nobject. If you return x in the generator, the caller can retrieve the\nvalue of x from the StopIteration exception, but usually that is\ndone automatically using the yield from syntax, as we’ll see in\n“Returning a Value from a Coroutine” on page 646.\n608 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 639,
      "chapter": null,
      "content": "Example 17-7 makes the interaction between a for loop and the body of the function\nmore explicit.\nExample 17-7. A generator function that prints messages when it runs\n>>> def gen_AB():\n...     print('start')\n...     yield 'A'          \n...     print('continue')\n...     yield 'B'          \n...     print('end.')      \n...\n>>> for c in gen_AB():     \n...     print('-->', c)    \n...\nstart     \n--> A     \ncontinue  \n--> B     \nend.      \n>>>       \nThe first implicit call to next() in the for loop at \n will print 'start' and stop\nat the first yield, producing the value 'A'.\nThe second implicit call to next() in the for loop will print 'continue' and stop\nat the second yield, producing the value 'B'.\nThe third call to next() will print 'end.' and fall through the end of the func‐\ntion body, causing the generator object to raise StopIteration.\nTo iterate, the for machinery does the equivalent of g = iter(gen_AB()) to get\na generator object, and then next(g) at each iteration.\nThe loop prints --> and the value returned by next(g). This output will appear\nonly after the output of the print calls inside the generator function.\nThe text start comes from print('start') in the generator body.\nyield 'A' in the generator body yields the value A consumed by the for loop,\nwhich gets assigned to the c variable and results in the output --> A.\nIteration continues with a second call to next(g), advancing the generator body\nfrom yield 'A' to yield 'B'. The text continue is output by the second print\nin the generator body.\nSentence Classes with __iter__ \n| \n609",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 640,
      "chapter": null,
      "content": "yield 'B' yields the value B consumed by the for loop, which gets assigned to\nthe c loop variable, so the loop prints --> B.\nIteration continues with a third call to next(it), advancing to the end of the\nbody of the function. The text end. appears in the output because of the third\nprint in the generator body.\nWhen the generator function runs to the end, the generator object raises StopIt\neration. The for loop machinery catches that exception, and the loop termi‐\nnates cleanly.\nNow hopefully it’s clear how Sentence.__iter__ in Example 17-5 works: __iter__\nis a generator function which, when called, builds a generator object that implements\nthe Iterator interface, so the SentenceIterator class is no longer needed.\nThat second version of Sentence is more concise than the first, but it’s not as lazy as\nit could be. Nowadays, laziness is considered a good trait, at least in programming\nlanguages and APIs. A lazy implementation postpones producing values to the last\npossible moment. This saves memory and may avoid wasting CPU cycles, too.\nWe’ll build lazy Sentence classes next.\nLazy Sentences\nThe final variations of Sentence are lazy, taking advantage of a lazy function from the\nre module.\nSentence Take #4: Lazy Generator\nThe Iterator interface is designed to be lazy: next(my_iterator) yields one item at\na time. The opposite of lazy is eager: lazy evaluation and eager evaluation are techni‐\ncal terms in programming language theory.\nOur Sentence implementations so far have not been lazy because the __init__\neagerly builds a list of all words in the text, binding it to the self.words attribute.\nThis requires processing the entire text, and the list may use as much memory as the\ntext itself (probably more; it depends on how many nonword characters are in the\ntext). Most of this work will be in vain if the user only iterates over the first couple of\nwords. If you wonder, “Is there a lazy way of doing this in Python?” the answer is\noften “Yes.”\nThe re.finditer function is a lazy version of re.findall. Instead of a list, re.fin\nditer returns a generator yielding re.MatchObject instances on demand. If there are\nmany matches, re.finditer saves a lot of memory. Using it, our third version of\n610 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 641,
      "chapter": null,
      "content": "Sentence is now lazy: it only reads the next word from the text when it is needed. The\ncode is in Example 17-8.\nExample 17-8. sentence_gen2.py: Sentence implemented using a generator function\ncalling the re.finditer generator function\nimport re\nimport reprlib\nRE_WORD = re.compile(r'\\w+')\nclass Sentence:\n    def __init__(self, text):\n        self.text = text  \n    def __repr__(self):\n        return f'Sentence({reprlib.repr(self.text)})'\n    def __iter__(self):\n        for match in RE_WORD.finditer(self.text):  \n            yield match.group()  \nNo need to have a words list.\nfinditer builds an iterator over the matches of RE_WORD on self.text, yielding\nMatchObject instances.\nmatch.group() extracts the matched text from the MatchObject instance.\nGenerators are a great shortcut, but the code can be made even more concise with a\ngenerator expression.\nSentence Take #5: Lazy Generator Expression\nWe can replace simple generator functions like the one in the previous Sentence\nclass (Example 17-8) with a generator expression. As a list comprehension builds\nlists, a generator expression builds generator objects. Example 17-9 contrasts their\nbehavior.\nExample 17-9. The gen_AB generator function is used by a list comprehension, then by\na generator expression\n>>> def gen_AB():  \n...     print('start')\n...     yield 'A'\nLazy Sentences \n| \n611",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 642,
      "chapter": null,
      "content": "...     print('continue')\n...     yield 'B'\n...     print('end.')\n...\n>>> res1 = [x*3 for x in gen_AB()]  \nstart\ncontinue\nend.\n>>> for i in res1:  \n...     print('-->', i)\n...\n--> AAA\n--> BBB\n>>> res2 = (x*3 for x in gen_AB())  \n>>> res2\n<generator object <genexpr> at 0x10063c240>\n>>> for i in res2:  \n...     print('-->', i)\n...\nstart      \n--> AAA\ncontinue\n--> BBB\nend.\nThis is the same gen_AB function from Example 17-7.\nThe list comprehension eagerly iterates over the items yielded by the generator\nobject returned by gen_AB(): 'A' and 'B'. Note the output in the next lines:\nstart, continue, end.\nThis for loop iterates over the res1 list built by the list comprehension.\nThe generator expression returns res2, a generator object. The generator is not\nconsumed here.\nOnly when the for loop iterates over res2, this generator gets items from\ngen_AB. Each iteration of the for loop implicitly calls next(res2), which in turn\ncalls next() on the generator object returned by gen_AB(), advancing it to the\nnext yield.\nNote how the output of gen_AB() interleaves with the output of the print in the\nfor loop.\nWe can use a generator expression to further reduce the code in the Sentence class.\nSee Example 17-10.\n612 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 643,
      "chapter": null,
      "content": "Example 17-10. sentence_genexp.py: Sentence implemented using a generator\nexpression\nimport re\nimport reprlib\nRE_WORD = re.compile(r'\\w+')\nclass Sentence:\n    def __init__(self, text):\n        self.text = text\n    def __repr__(self):\n        return f'Sentence({reprlib.repr(self.text)})'\n    def __iter__(self):\n        return (match.group() for match in RE_WORD.finditer(self.text))\nThe only difference from Example 17-8 is the __iter__ method, which here is not a\ngenerator function (it has no yield) but uses a generator expression to build a gener‐\nator and then returns it. The end result is the same: the caller of __iter__ gets a gen‐\nerator object.\nGenerator expressions are syntactic sugar: they can always be replaced by generator\nfunctions, but sometimes are more convenient. The next section is about generator\nexpression usage.\nWhen to Use Generator Expressions\nI used several generator expressions when implementing the Vector class in\nExample 12-16. Each of these methods has a generator expression: __eq__, __hash__,\n__abs__, angle, angles, format, __add__, and __mul__. In all those methods, a list\ncomprehension would also work, at the cost of using more memory to store the inter‐\nmediate list values.\nIn Example 17-10, we saw that a generator expression is a syntactic shortcut to create\na generator without defining and calling a function. On the other hand, generator\nfunctions are more flexible: we can code complex logic with multiple statements, and\nwe can even use them as coroutines, as we’ll see in “Classic Coroutines” on page 641.\nFor the simpler cases, a generator expression is easier to read at a glance, as the\nVector example shows.\nMy rule of thumb in choosing the syntax to use is simple: if the generator expression\nspans more than a couple of lines, I prefer to code a generator function for the sake of\nreadability.\nWhen to Use Generator Expressions \n| \n613",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 644,
      "chapter": null,
      "content": "Syntax Tip\nWhen a generator expression is passed as the single argument to a\nfunction or constructor, you don’t need to write a set of parenthe‐\nses for the function call and another to enclose the generator\nexpression. A single pair will do, like in the Vector call from the\n__mul__ method in Example 12-16, reproduced here:\ndef __mul__(self, scalar):\n    if isinstance(scalar, numbers.Real):\n        return Vector(n * scalar for n in self)\n    else:\n        return NotImplemented\nHowever, if there are more function arguments after the generator\nexpression, you need to enclose it in parentheses to avoid a Syntax\nError.\nThe Sentence examples we’ve seen demonstrate generators playing the role of the\nclassic Iterator pattern: retrieving items from a collection. But we can also use genera‐\ntors to yield values independent of a data source. The next section shows an example.\nBut first, a short discussion on the overlapping concepts of iterator and generator.\nContrasting Iterators and Generators\nIn the official Python documentation and codebase, the terminology around iterators\nand generators is inconsistent and evolving. I’ve adopted the following definitions:\niterator\nGeneral term for any object that implements a __next__ method. Iterators are\ndesigned to produce data that is consumed by the client code, i.e., the code that\ndrives the iterator via a for loop or other iterative feature, or by explicitly calling\nnext(it) on the iterator—although this explicit usage is much less common. In\npractice, most iterators we use in Python are generators.\ngenerator\nAn iterator built by the Python compiler. To create a generator, we don’t imple‐\nment __next__. Instead, we use the yield keyword to make a generator function,\nwhich is a factory of generator objects. A generator expression is another way to\nbuild a generator object. Generator objects provide __next__, so they are itera‐\ntors. Since Python 3.5, we also have asynchronous generators declared with async\ndef. We’ll study them in Chapter 21, “Asynchronous Programming”.\nThe Python Glossary recently introduced the term generator iterator to refer to gener‐\nator objects built by generator functions, while the entry for generator expression says\nit returns an “iterator.”\n614 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 645,
      "chapter": null,
      "content": "But the objects returned in both cases are generator objects, according to Python:\n>>> def g():\n...     yield 0\n...\n>>> g()\n<generator object g at 0x10e6fb290>\n>>> ge = (c for c in 'XYZ')\n>>> ge\n<generator object <genexpr> at 0x10e936ce0>\n>>> type(g()), type(ge)\n(<class 'generator'>, <class 'generator'>)\nAn Arithmetic Progression Generator\nThe classic Iterator pattern is all about traversal: navigating some data structure. But\na standard interface based on a method to fetch the next item in a series is also useful\nwhen the items are produced on the fly, instead of retrieved from a collection. For\nexample, the range built-in generates a bounded arithmetic progression (AP) of inte‐\ngers. What if you need to generate an AP of numbers of any type, not only integers?\nExample 17-11 shows a few console tests of an ArithmeticProgression class we will\nsee in a moment. The signature of the constructor in Example 17-11 is Arithmetic\nProgression(begin, step[, end]). The complete signature of the range built-in is\nrange(start, stop[, step]). I chose to implement a different signature because\nthe step is mandatory but end is optional in an arithmetic progression. I also\nchanged the argument names from start/stop to begin/end to make it clear that I\nopted for a different signature. In each test in Example 17-11, I call list() on the\nresult to inspect the generated values.\nExample 17-11. Demonstration of an ArithmeticProgression class\n    >>> ap = ArithmeticProgression(0, 1, 3)\n    >>> list(ap)\n    [0, 1, 2]\n    >>> ap = ArithmeticProgression(1, .5, 3)\n    >>> list(ap)\n    [1.0, 1.5, 2.0, 2.5]\n    >>> ap = ArithmeticProgression(0, 1/3, 1)\n    >>> list(ap)\n    [0.0, 0.3333333333333333, 0.6666666666666666]\n    >>> from fractions import Fraction\n    >>> ap = ArithmeticProgression(0, Fraction(1, 3), 1)\n    >>> list(ap)\n    [Fraction(0, 1), Fraction(1, 3), Fraction(2, 3)]\n    >>> from decimal import Decimal\n    >>> ap = ArithmeticProgression(0, Decimal('.1'), .3)\nAn Arithmetic Progression Generator \n| \n615",
      "content_length": 2020,
      "extraction_method": "Direct"
    },
    {
      "page_number": 646,
      "chapter": null,
      "content": "7 In Python 2, there was a coerce() built-in function, but it’s gone in Python 3. It was deemed unnecessary\nbecause the numeric coercion rules are implicit in the arithmetic operator methods. So the best way I could\nthink of to coerce the initial value to be of the same type as the rest of the series was to perform the addition\nand use its type to convert the result. I asked about this in the Python-list and got an excellent response from\nSteven D’Aprano.\n    >>> list(ap)\n    [Decimal('0'), Decimal('0.1'), Decimal('0.2')]\nNote that the type of the numbers in the resulting arithmetic progression follows the\ntype of begin + step, according to the numeric coercion rules of Python arithmetic.\nIn Example 17-11, you see lists of int, float, Fraction, and Decimal numbers.\nExample 17-12 lists the implementation of the ArithmeticProgression class.\nExample 17-12. The ArithmeticProgression class\nclass ArithmeticProgression:\n    def __init__(self, begin, step, end=None):       \n        self.begin = begin\n        self.step = step\n        self.end = end  # None -> \"infinite\" series\n    def __iter__(self):\n        result_type = type(self.begin + self.step)   \n        result = result_type(self.begin)             \n        forever = self.end is None                   \n        index = 0\n        while forever or result < self.end:          \n            yield result                             \n            index += 1\n            result = self.begin + self.step * index  \n__init__ requires two arguments: begin and step; end is optional, if it’s None,\nthe series will be unbounded.\nGet the type of adding self.begin and self.step. For example, if one is int and\nthe other is float, result_type will be float.\nThis line makes a result with the same numeric value of self.begin, but\ncoerced to the type of the subsequent additions.7\nFor readability, the forever flag will be True if the self.end attribute is None,\nresulting in an unbounded series.\n616 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 647,
      "chapter": null,
      "content": "8 The 17-it-generator/ directory in the Fluent Python code repository includes doctests and a script, arit‐\nprog_runner.py, which runs the tests against all variations of the aritprog*.py scripts.\nThis loop runs forever or until the result matches or exceeds self.end. When\nthis loop exits, so does the function.\nThe current result is produced.\nThe next potential result is calculated. It may never be yielded, because the while\nloop may terminate.\nIn the last line of Example 17-12, instead of adding self.step to the previous result\neach time around the loop, I opted to ignore the previous result and each new\nresult by adding self.begin to self.step multiplied by index. This avoids the\ncumulative effect of floating-point errors after successive additions. These simple\nexperiments make the difference clear:\n>>> 100 * 1.1\n110.00000000000001\n>>> sum(1.1 for _ in range(100))\n109.99999999999982\n>>> 1000 * 1.1\n1100.0\n>>> sum(1.1 for _ in range(1000))\n1100.0000000000086\nThe ArithmeticProgression class from Example 17-12 works as intended, and is a\nanother example of using a generator function to implement the __iter__ special\nmethod. However, if the whole point of a class is to build a generator by implement‐\ning __iter__, we can replace the class with a generator function. A generator\nfunction is, after all, a generator factory.\nExample 17-13 shows a generator function called aritprog_gen that does the same\njob as ArithmeticProgression but with less code. The tests in Example 17-11 all pass\nif you just call aritprog_gen instead of ArithmeticProgression.8\nExample 17-13. The aritprog_gen generator function\ndef aritprog_gen(begin, step, end=None):\n    result = type(begin + step)(begin)\n    forever = end is None\n    index = 0\n    while forever or result < end:\n        yield result\n        index += 1\n        result = begin + step * index\nAn Arithmetic Progression Generator \n| \n617",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 648,
      "chapter": null,
      "content": "Example 17-13 is elegant, but always remember: there are plenty of ready-to-use gen‐\nerators in the standard library, and the next section will show a shorter implementa‐\ntion using the itertools module.\nArithmetic Progression with itertools\nThe itertools module in Python 3.10 has 20 generator functions that can be com‐\nbined in a variety of interesting ways.\nFor example, the itertools.count function returns a generator that yields numbers.\nWithout arguments, it yields a series of integers starting with 0. But you can provide\noptional start and step values to achieve a result similar to our aritprog_gen\nfunctions:\n>>> import itertools\n>>> gen = itertools.count(1, .5)\n>>> next(gen)\n1\n>>> next(gen)\n1.5\n>>> next(gen)\n2.0\n>>> next(gen)\n2.5\nitertools.count never stops, so if you call list(count()),\nPython will try to build a list that would fill all the memory chips\never made. In practice, your machine will become very grumpy\nlong before the call fails.\nOn the other hand, there is the itertools.takewhile function: it returns a generator\nthat consumes another generator and stops when a given predicate evaluates to\nFalse. So we can combine the two and write this:\n>>> gen = itertools.takewhile(lambda n: n < 3, itertools.count(1, .5))\n>>> list(gen)\n[1, 1.5, 2.0, 2.5]\nLeveraging takewhile and count, Example 17-14 is even more concise.\nExample 17-14. aritprog_v3.py: this works like the previous aritprog_gen functions\nimport itertools\ndef aritprog_gen(begin, step, end=None):\n    first = type(begin + step)(begin)\n    ap_gen = itertools.count(first, step)\n618 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 649,
      "chapter": null,
      "content": "if end is None:\n        return ap_gen\n    return itertools.takewhile(lambda n: n < end, ap_gen)\nNote that aritprog_gen in Example 17-14 is not a generator function: it has no\nyield in its body. But it returns a generator, just as a generator function does.\nHowever, recall that itertools.count adds the step repeatedly, so the floating-point\nseries it produces are not as precise as Example 17-13.\nThe point of Example 17-14 is: when implementing generators, know what is avail‐\nable in the standard library, otherwise there’s a good chance you’ll reinvent the wheel.\nThat’s why the next section covers several ready-to-use generator functions.\nGenerator Functions in the Standard Library\nThe standard library provides many generators, from plain-text file objects providing\nline-by-line iteration, to the awesome os.walk function, which yields filenames while\ntraversing a directory tree, making recursive filesystem searches as simple as a for\nloop.\nThe os.walk generator function is impressive, but in this section I want to focus on\ngeneral-purpose functions that take arbitrary iterables as arguments and return gen‐\nerators that yield selected, computed, or rearranged items. In the following tables, I\nsummarize two dozen of them, from the built-in, itertools, and functools mod‐\nules. For convenience, I grouped them by high-level functionality, regardless of\nwhere they are defined.\nThe first group contains the filtering generator functions: they yield a subset of items\nproduced by the input iterable, without changing the items themselves. Like take\nwhile, most functions listed in Table 17-1 take a predicate, which is a one-argument\nBoolean function that will be applied to each item in the input to determine whether\nthe item is included in the output.\nTable 17-1. Filtering generator functions\nModule\nFunction\nDescription\nitertools\ncompress(it, selector_it)\nConsumes two iterables in parallel; yields items from it\nwhenever the corresponding item in selector_it is\ntruthy\nitertools\ndropwhile(predicate, it)\nConsumes it, skipping items while predicate computes\ntruthy, then yields every remaining item (no further checks\nare made)\n(built-in)\nfilter(predicate, it)\nApplies predicate to each item of iterable, yielding\nthe item if predicate(item) is truthy; if predicate is\nNone, only truthy items are yielded\nGenerator Functions in the Standard Library \n| \n619",
      "content_length": 2374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 650,
      "chapter": null,
      "content": "9 Here, the term “mapping” is unrelated to dictionaries, but has to do with the map built-in.\nModule\nFunction\nDescription\nitertools\nfilterfalse(predicate, it)\nSame as filter, with the predicate logic negated:\nyields items whenever predicate computes falsy\nitertools\nislice(it, stop) or \nislice(it, start, stop, \nstep=1)\nYields items from a slice of it, similar to s[:stop] or\ns[start:stop:step] except it can be any iterable,\nand the operation is lazy\nitertools\ntakewhile(predicate, it)\nYields items while predicate computes truthy, then stops\nand no further checks are made\nThe console listing in Example 17-15 shows the use of all the functions in Table 17-1.\nExample 17-15. Filtering generator functions examples\n>>> def vowel(c):\n...     return c.lower() in 'aeiou'\n...\n>>> list(filter(vowel, 'Aardvark'))\n['A', 'a', 'a']\n>>> import itertools\n>>> list(itertools.filterfalse(vowel, 'Aardvark'))\n['r', 'd', 'v', 'r', 'k']\n>>> list(itertools.dropwhile(vowel, 'Aardvark'))\n['r', 'd', 'v', 'a', 'r', 'k']\n>>> list(itertools.takewhile(vowel, 'Aardvark'))\n['A', 'a']\n>>> list(itertools.compress('Aardvark', (1, 0, 1, 1, 0, 1)))\n['A', 'r', 'd', 'a']\n>>> list(itertools.islice('Aardvark', 4))\n['A', 'a', 'r', 'd']\n>>> list(itertools.islice('Aardvark', 4, 7))\n['v', 'a', 'r']\n>>> list(itertools.islice('Aardvark', 1, 7, 2))\n['a', 'd', 'a']\nThe next group contains the mapping generators: they yield items computed from\neach individual item in the input iterable—or iterables, in the case of map and star\nmap.9 The generators in Table 17-2 yield one result per item in the input iterables. If\nthe input comes from more than one iterable, the output stops as soon as the first\ninput iterable is exhausted.\n620 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 651,
      "chapter": null,
      "content": "Table 17-2. Mapping generator functions\nModule\nFunction\nDescription\nitertools\naccumulate(it, \n[func])\nYields accumulated sums; if func is provided, yields the result of applying\nit to the first pair of items, then to the first result and next item, etc.\n(built-in)\nenumerate(iterable, \nstart=0)\nYields 2-tuples of the form (index, item), where index is counted\nfrom start, and item is taken from the iterable\n(built-in)\nmap(func, it1, \n[it2, …, itN])\nApplies func to each item of it, yielding the result; if N iterables are\ngiven, func must take N arguments and the iterables will be consumed in\nparallel\nitertools\nstarmap(func, it)\nApplies func to each item of it, yielding the result; the input iterable\nshould yield iterable items iit, and func is applied as func(*iit)\nExample 17-16 demonstrates some uses of itertools.accumulate.\nExample 17-16. itertools.accumulate generator function examples\n>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]\n>>> import itertools\n>>> list(itertools.accumulate(sample))  \n[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]\n>>> list(itertools.accumulate(sample, min))  \n[5, 4, 2, 2, 2, 2, 2, 0, 0, 0]\n>>> list(itertools.accumulate(sample, max))  \n[5, 5, 5, 8, 8, 8, 8, 8, 9, 9]\n>>> import operator\n>>> list(itertools.accumulate(sample, operator.mul))  \n[5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]\n>>> list(itertools.accumulate(range(1, 11), operator.mul))\n[1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]  \nRunning sum.\nRunning minimum.\nRunning maximum.\nRunning product.\nFactorials from 1! to 10!.\nThe remaining functions of Table 17-2 are shown in Example 17-17.\nExample 17-17. Mapping generator function examples\n>>> list(enumerate('albatroz', 1))  \n[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')]\nGenerator Functions in the Standard Library \n| \n621",
      "content_length": 1811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 652,
      "chapter": null,
      "content": ">>> import operator\n>>> list(map(operator.mul, range(11), range(11)))  \n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n>>> list(map(operator.mul, range(11), [2, 4, 8]))  \n[0, 4, 16]\n>>> list(map(lambda a, b: (a, b), range(11), [2, 4, 8]))  \n[(0, 2), (1, 4), (2, 8)]\n>>> import itertools\n>>> list(itertools.starmap(operator.mul, enumerate('albatroz', 1)))  \n['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz']\n>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]\n>>> list(itertools.starmap(lambda a, b: b / a,\n...     enumerate(itertools.accumulate(sample), 1)))  \n[5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333,\n5.0, 4.375, 4.888888888888889, 4.5]\nNumber the letters in the word, starting from 1.\nSquares of integers from 0 to 10.\nMultiplying numbers from two iterables in parallel: results stop when the short‐\nest iterable ends.\nThis is what the zip built-in function does.\nRepeat each letter in the word according to its place in it, starting from 1.\nRunning average.\nNext, we have the group of merging generators—all of these yield items from multi‐\nple input iterables. chain and chain.from_iterable consume the input iterables\nsequentially (one after the other), while product, zip, and zip_longest consume the\ninput iterables in parallel. See Table 17-3.\nTable 17-3. Generator functions that merge multiple input iterables\nModule\nFunction\nDescription\nitertools\nchain(it1, …, itN)\nYields all items from it1, then from it2, etc., seamlessly\nitertools\nchain.from_iterable(it)\nYields all items from each iterable produced by it, one after the\nother, seamlessly; it will be an iterable where the items are\nalso iterables, for example, a list of tuples\nitertools\nproduct(it1, …, itN, \nrepeat=1)\nCartesian product: yields N-tuples made by combining items\nfrom each input iterable, like nested for loops could produce;\nrepeat allows the input iterables to be consumed more than\nonce\n622 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 653,
      "chapter": null,
      "content": "Module\nFunction\nDescription\n(built-in)\nzip(it1, …, itN, \nstrict=False)\nYields N-tuples built from items taken from the iterables in\nparallel, silently stopping when the first iterable is exhausted,\nunless strict=True is givena\nitertools\nzip_longest(it1, …, itN, \nfillvalue=None)\nYields N-tuples built from items taken from the iterables in\nparallel, stopping only when the last iterable is exhausted, filling\nthe blanks with the fillvalue\na The strict keyword-only argument is new in Python 3.10. When strict=True, ValueError is raised if any iterable\nhas a different length. The default is False, for backward compatibility.\nExample 17-18 shows the use of the itertools.chain and zip generator functions\nand their siblings. Recall that the zip function is named after the zip fastener or zip‐\nper (no relation to compression). Both zip and itertools.zip_longest were intro‐\nduced in “The Awesome zip” on page 416.\nExample 17-18. Merging generator function examples\n>>> list(itertools.chain('ABC', range(2)))  \n['A', 'B', 'C', 0, 1]\n>>> list(itertools.chain(enumerate('ABC')))  \n[(0, 'A'), (1, 'B'), (2, 'C')]\n>>> list(itertools.chain.from_iterable(enumerate('ABC')))  \n[0, 'A', 1, 'B', 2, 'C']\n>>> list(zip('ABC', range(5), [10, 20, 30, 40]))  \n[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]\n>>> list(itertools.zip_longest('ABC', range(5)))  \n[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]\n>>> list(itertools.zip_longest('ABC', range(5), fillvalue='?'))  \n[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)]\nGenerator Functions in the Standard Library \n| \n623",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 654,
      "chapter": null,
      "content": "chain is usually called with two or more iterables.\nchain does nothing useful when called with a single iterable.\nBut chain.from_iterable takes each item from the iterable, and chains them in\nsequence, as long as each item is itself iterable.\nAny number of iterables can be consumed by zip in parallel, but the generator\nalways stops as soon as the first iterable ends. In Python ≥ 3.10, if the\nstrict=True argument is given and an iterable ends before the others, ValueEr\nror is raised.\nitertools.zip_longest works like zip, except it consumes all input iterables to\nthe end, padding output tuples with None, as needed.\nThe fillvalue keyword argument specifies a custom padding value.\nThe itertools.product generator is a lazy way of computing Cartesian products,\nwhich we built using list comprehensions with more than one for clause in\n“Cartesian Products” on page 27. Generator expressions with multiple for clauses\ncan also be used to produce Cartesian products lazily. Example 17-19 demonstrates\nitertools.product.\nExample 17-19. itertools.product generator function examples\n>>> list(itertools.product('ABC', range(2)))  \n[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]\n>>> suits = 'spades hearts diamonds clubs'.split()\n>>> list(itertools.product('AK', suits))  \n[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'),\n('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')]\n>>> list(itertools.product('ABC'))  \n[('A',), ('B',), ('C',)]\n>>> list(itertools.product('ABC', repeat=2))  \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'),\n('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]\n>>> list(itertools.product(range(2), repeat=3))\n[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),\n(1, 0, 1), (1, 1, 0), (1, 1, 1)]\n>>> rows = itertools.product('AB', range(2), repeat=2)\n>>> for row in rows: print(row)\n...\n('A', 0, 'A', 0)\n('A', 0, 'A', 1)\n('A', 0, 'B', 0)\n('A', 0, 'B', 1)\n('A', 1, 'A', 0)\n624 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 655,
      "chapter": null,
      "content": "('A', 1, 'A', 1)\n('A', 1, 'B', 0)\n('A', 1, 'B', 1)\n('B', 0, 'A', 0)\n('B', 0, 'A', 1)\n('B', 0, 'B', 0)\n('B', 0, 'B', 1)\n('B', 1, 'A', 0)\n('B', 1, 'A', 1)\n('B', 1, 'B', 0)\n('B', 1, 'B', 1)\nThe Cartesian product of a str with three characters and a range with two inte‐\ngers yields six tuples (because 3 * 2 is 6).\nThe product of two card ranks ('AK') and four suits is a series of eight tuples.\nGiven a single iterable, product yields a series of one-tuples—not very useful.\nThe repeat=N keyword argument tells the product to consume each input itera‐\nble N times.\nSome generator functions expand the input by yielding more than one value per\ninput item. They are listed in Table 17-4.\nTable 17-4. Generator functions that expand each input item into multiple output items\nModule\nFunction\nDescription\nitertools\ncombinations(it, out_len)\nYields combinations of out_len items from the items\nyielded by it\nitertools\ncombinations_with_replacement(it, \nout_len)\nYields combinations of out_len items from the items\nyielded by it, including combinations with repeated\nitems\nitertools\ncount(start=0, step=1)\nYields numbers starting at start, incremented by\nstep, indefinitely\nitertools\ncycle(it)\nYields items from it, storing a copy of each, then\nyields the entire sequence repeatedly, indefinitely\nitertools\npairwise(it)\nYields successive overlapping pairs taken from the\ninput iterablea\nitertools\npermutations(it, out_len=None)\nYields permutations of out_len items from the items\nyielded by it; by default, out_len is\nlen(list(it))\nitertools\nrepeat(item, [times])\nYields the given item repeatedly, indefinitely unless a\nnumber of times is given\na itertools.pairwise was added in Python 3.10.\nGenerator Functions in the Standard Library \n| \n625",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 656,
      "chapter": null,
      "content": "The count and repeat functions from itertools return generators that conjure\nitems out of nothing: neither of them takes an iterable as input. We saw iter\ntools.count in “Arithmetic Progression with itertools” on page 618. The cycle\ngenerator makes a backup of the input iterable and yields its items repeatedly.\nExample 17-20 illustrates the use of count, cycle, pairwise, and repeat.\nExample 17-20. count, cycle, pairwise, and repeat\n>>> ct = itertools.count()  \n>>> next(ct)  \n0\n>>> next(ct), next(ct), next(ct)  \n(1, 2, 3)\n>>> list(itertools.islice(itertools.count(1, .3), 3))  \n[1, 1.3, 1.6]\n>>> cy = itertools.cycle('ABC')  \n>>> next(cy)\n'A'\n>>> list(itertools.islice(cy, 7))  \n['B', 'C', 'A', 'B', 'C', 'A', 'B']\n>>> list(itertools.pairwise(range(7)))  \n[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]\n>>> rp = itertools.repeat(7)  \n>>> next(rp), next(rp)\n(7, 7)\n>>> list(itertools.repeat(8, 4))  \n[8, 8, 8, 8]\n>>> list(map(operator.mul, range(11), itertools.repeat(5)))  \n[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\nBuild a count generator ct.\nRetrieve the first item from ct.\nI can’t build a list from ct, because ct never stops, so I fetch the next three\nitems.\nI can build a list from a count generator if it is limited by islice or takewhile.\nBuild a cycle generator from 'ABC' and fetch its first item, 'A'.\nA list can only be built if limited by islice; the next seven items are retrieved\nhere.\nFor each item in the input, pairwise yields a 2-tuple with that item and the next\n—if there is a next item. Available in Python ≥ 3.10.\n626 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 657,
      "chapter": null,
      "content": "Build a repeat generator that will yield the number 7 forever.\nA repeat generator can be limited by passing the times argument: here the num‐\nber 8 will be produced 4 times.\nA common use of repeat: providing a fixed argument in map; here it provides the\n5 multiplier.\nThe combinations, combinations_with_replacement, and permutations generator\nfunctions—together with product—are called the combinatorics generators in the\nitertools documentation page. There is a close relationship between iter\ntools.product and the remaining combinatoric functions as well, as Example 17-21\nshows.\nExample 17-21. Combinatoric generator functions yield multiple values per input item\n>>> list(itertools.combinations('ABC', 2))  \n[('A', 'B'), ('A', 'C'), ('B', 'C')]\n>>> list(itertools.combinations_with_replacement('ABC', 2))  \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]\n>>> list(itertools.permutations('ABC', 2))  \n[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]\n>>> list(itertools.product('ABC', repeat=2))  \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'),\n('C', 'A'), ('C', 'B'), ('C', 'C')]\nAll combinations of len()==2 from the items in 'ABC'; item ordering in the gen‐\nerated tuples is irrelevant (they could be sets).\nAll combinations of len()==2 from the items in 'ABC', including combinations\nwith repeated items.\nAll permutations of len()==2 from the items in 'ABC'; item ordering in the gen‐\nerated tuples is relevant.\nCartesian product from 'ABC' and 'ABC' (that’s the effect of repeat=2).\nThe last group of generator functions we’ll cover in this section are designed to yield\nall items in the input iterables, but rearranged in some way. Here are two functions\nthat return multiple generators: itertools.groupby and itertools.tee. The other\ngenerator function in this group, the reversed built-in, is the only one covered in\nthis section that does not accept any iterable as input, but only sequences. This makes\nsense: because reversed will yield the items from last to first, it only works with a\nsequence with a known length. But it avoids the cost of making a reversed copy of the\nGenerator Functions in the Standard Library \n| \n627",
      "content_length": 2215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 658,
      "chapter": null,
      "content": "sequence by yielding each item as needed. I put the itertools.product function\ntogether with the merging generators in Table 17-3 because they all consume more\nthan one iterable, while the generators in Table 17-5 all accept at most one input\niterable.\nTable 17-5. Rearranging generator functions\nModule\nFunction\nDescription\nitertools\ngroupby(it, \nkey=None)\nYields 2-tuples of the form (key, group), where key is the grouping\ncriterion and group is a generator yielding the items in the group\n(built-in)\nreversed(seq)\nYields items from seq in reverse order, from last to first; seq must be a\nsequence or implement the __reversed__ special method\nitertools\ntee(it, n=2)\nYields a tuple of n generators, each yielding the items of the input iterable\nindependently\nExample 17-22 demonstrates the use of itertools.groupby and the reversed built-\nin. Note that itertools.groupby assumes that the input iterable is sorted by the\ngrouping criterion, or at least that the items are clustered by that criterion—even if\nnot completely sorted. Tech reviewer Miroslav Šedivý suggested this use case: you\ncan sort the datetime objects chronologically, then groupby weekday to get a group\nof Monday data, followed by Tuesday data, etc., and then by Monday (of the next\nweek) again, and so on.\nExample 17-22. itertools.groupby\n>>> list(itertools.groupby('LLLLAAGGG'))  \n[('L', <itertools._grouper object at 0x102227cc0>),\n('A', <itertools._grouper object at 0x102227b38>),\n('G', <itertools._grouper object at 0x102227b70>)]\n>>> for char, group in itertools.groupby('LLLLAAAGG'):  \n...     print(char, '->', list(group))\n...\nL -> ['L', 'L', 'L', 'L']\nA -> ['A', 'A',]\nG -> ['G', 'G', 'G']\n>>> animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear',\n...            'bat', 'dolphin', 'shark', 'lion']\n>>> animals.sort(key=len)  \n>>> animals\n['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark',\n'giraffe', 'dolphin']\n>>> for length, group in itertools.groupby(animals, len):  \n...     print(length, '->', list(group))\n...\n3 -> ['rat', 'bat']\n4 -> ['duck', 'bear', 'lion']\n5 -> ['eagle', 'shark']\n628 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 659,
      "chapter": null,
      "content": "7 -> ['giraffe', 'dolphin']\n>>> for length, group in itertools.groupby(reversed(animals), len): \n...     print(length, '->', list(group))\n...\n7 -> ['dolphin', 'giraffe']\n5 -> ['shark', 'eagle']\n4 -> ['lion', 'bear', 'duck']\n3 -> ['bat', 'rat']\n>>>\ngroupby yields tuples of (key, group_generator).\nHandling groupby generators involves nested iteration: in this case, the outer for\nloop and the inner list constructor.\nSort animals by length.\nAgain, loop over the key and group pair, to display the key and expand the group\ninto a list.\nHere the reverse generator iterates over animals from right to left.\nThe last of the generator functions in this group is iterator.tee, which has a unique\nbehavior: it yields multiple generators from a single input iterable, each yielding\nevery item from the input. Those generators can be consumed independently, as\nshown in Example 17-23.\nExample 17-23. itertools.tee yields multiple generators, each yielding every item of\nthe input generator\n>>> list(itertools.tee('ABC'))\n[<itertools._tee object at 0x10222abc8>, <itertools._tee object at 0x10222ac08>]\n>>> g1, g2 = itertools.tee('ABC')\n>>> next(g1)\n'A'\n>>> next(g2)\n'A'\n>>> next(g2)\n'B'\n>>> list(g1)\n['B', 'C']\n>>> list(g2)\n['C']\n>>> list(zip(*itertools.tee('ABC')))\n[('A', 'A'), ('B', 'B'), ('C', 'C')]\nGenerator Functions in the Standard Library \n| \n629",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 660,
      "chapter": null,
      "content": "Note that several examples in this section used combinations of generator functions.\nThis is a great feature of these functions: because they take generators as arguments\nand return generators, they can be combined in many different ways.\nNow we’ll review another group of iterable-savvy functions in the standard library.\nIterable Reducing Functions\nThe functions in Table 17-6 all take an iterable and return a single result. They are\nknown as “reducing,” “folding,” or “accumulating” functions. We can implement\nevery one of the built-ins listed here with functools.reduce, but they exist as built-\nins because they address some common use cases more easily. A longer explanation\nabout functools.reduce appeared in “Vector Take #4: Hashing and a Faster ==” on\npage 411.\nIn the case of all and any, there is an important optimization functools.reduce\ndoes not support: all and any short-circuit—i.e., they stop consuming the iterator as\nsoon as the result is determined. See the last test with any in Example 17-24.\nTable 17-6. Built-in functions that read iterables and return single values\nModule\nFunction\nDescription\n(built-in)\nall(it)\nReturns True if all items in it are truthy, otherwise False;\nall([]) returns True\n(built-in)\nany(it)\nReturns True if any item in it is truthy, otherwise False;\nany([]) returns False\n(built-in)\nmax(it, [key=,] \n[default=])\nReturns the maximum value of the items in it;a key is an ordering\nfunction, as in sorted; default is returned if the iterable is\nempty\n(built-in)\nmin(it, [key=,] \n[default=])\nReturns the minimum value of the items in it.b key is an ordering\nfunction, as in sorted; default is returned if the iterable is\nempty\nfunctools\nreduce(func, it, [ini\ntial])\nReturns the result of applying func to the first pair of items, then to\nthat result and the third item, and so on; if given, initial forms\nthe initial pair with the first item\n(built-in)\nsum(it, start=0)\nThe sum of all items in it, with the optional start value added\n(use math.fsum for better precision when adding floats)\na May also be called as max(arg1, arg2, …, [key=?]), in which case the maximum among the arguments is\nreturned.\nb May also be called as min(arg1, arg2, …, [key=?]), in which case the minimum among the arguments is\nreturned.\nThe operation of all and any is exemplified in Example 17-24.\n630 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 661,
      "chapter": null,
      "content": "Example 17-24. Results of all and any for some sequences\n>>> all([1, 2, 3])\nTrue\n>>> all([1, 0, 3])\nFalse\n>>> all([])\nTrue\n>>> any([1, 2, 3])\nTrue\n>>> any([1, 0, 3])\nTrue\n>>> any([0, 0.0])\nFalse\n>>> any([])\nFalse\n>>> g = (n for n in [0, 0.0, 7, 8])\n>>> any(g)  \nTrue\n>>> next(g)  \n8\nany iterated over g until g yielded 7; then any stopped and returned True.\nThat’s why 8 was still remaining.\nAnother built-in that takes an iterable and returns something else is sorted. Unlike\nreversed, which is a generator function, sorted builds and returns a new list. After\nall, every single item of the input iterable must be read so they can be sorted, and the\nsorting happens in a list, therefore sorted just returns that list after it’s done. I\nmention sorted here because it does consume an arbitrary iterable.\nOf course, sorted and the reducing functions only work with iterables that eventually\nstop. Otherwise, they will keep on collecting items and never return a result.\nIf you’ve gotten this far, you’ve seen the most important and useful\ncontent of this chapter. The remaining sections cover advanced\ngenerator features that most of us don’t see or need very often,\nsuch as the yield from construct and classic coroutines.\nThere are also sections about type hinting iterables, iterators, and\nclassic coroutines.\nThe yield from syntax provides a new way of combining generators. That’s next.\nIterable Reducing Functions \n| \n631",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 662,
      "chapter": null,
      "content": "Subgenerators with yield from\nThe yield from expression syntax was introduced in Python 3.3 to allow a generator\nto delegate work to a subgenerator.\nBefore yield from was introduced, we used a for loop when a generator needed to\nyield values produced from another generator:\n>>> def sub_gen():\n...     yield 1.1\n...     yield 1.2\n...\n>>> def gen():\n...     yield 1\n...     for i in sub_gen():\n...         yield i\n...     yield 2\n...\n>>> for x in gen():\n...     print(x)\n...\n1\n1.1\n1.2\n2\nWe can get the same result using yield from, as you can see in Example 17-25.\nExample 17-25. Test-driving yield from\n>>> def sub_gen():\n...     yield 1.1\n...     yield 1.2\n...\n>>> def gen():\n...     yield 1\n...     yield from sub_gen()\n...     yield 2\n...\n>>> for x in gen():\n...     print(x)\n...\n1\n1.1\n1.2\n2\nIn Example 17-25, the for loop is the client code, gen is the delegating generator, and\nsub_gen is the subgenerator. Note that yield from pauses gen, and sub_gen takes\nover until it is exhausted. The values yielded by sub_gen pass through gen directly to\n632 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 663,
      "chapter": null,
      "content": "10 chain and most itertools functions are written in C.\nthe client for loop. Meanwhile, gen is suspended and cannot see the values passing\nthrough it. Only when sub_gen is done, gen resumes.\nWhen the subgenerator contains a return statement with a value, that value can be\ncaptured in the delegating generator by using yield from as part of an expression.\nExample 17-26 demonstrates.\nExample 17-26. yield from gets the return value of the subgenerator\n>>> def sub_gen():\n...     yield 1.1\n...     yield 1.2\n...     return 'Done!'\n...\n>>> def gen():\n...     yield 1\n...     result = yield from sub_gen()\n...     print('<--', result)\n...     yield 2\n...\n>>> for x in gen():\n...     print(x)\n...\n1\n1.1\n1.2\n<-- Done!\n2\nNow that we’ve seen the basics of yield from, let’s study a couple of simple but prac‐\ntical examples of its use.\nReinventing chain\nWe saw in Table 17-3 that itertools provides a chain generator that yields items\nfrom several iterables, iterating over the first, then the second, and so on up to the\nlast. This is a homemade implementation of chain with nested for loops in Python:10\n>>> def chain(*iterables):\n...     for it in iterables:\n...         for i in it:\n...             yield i\n...\n>>> s = 'ABC'\n>>> r = range(3)\nSubgenerators with yield from \n| \n633",
      "content_length": 1276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 664,
      "chapter": null,
      "content": ">>> list(chain(s, r))\n['A', 'B', 'C', 0, 1, 2]\nThe chain generator in the preceding code is delegating to each iterable it in turn, by\ndriving each it in the inner for loop. That inner loop can be replaced with a yield\nfrom expression, as shown in the next console listing:\n>>> def chain(*iterables):\n...     for i in iterables:\n...         yield from i\n...\n>>> list(chain(s, t))\n['A', 'B', 'C', 0, 1, 2]\nThe use of yield from in this example is correct, and the code reads better, but it\nseems like syntactic sugar with little real gain. Now let’s develop a more interesting\nexample.\nTraversing a Tree\nIn this section, we’ll see yield from in a script to traverse a tree structure. I will build\nit in baby steps.\nThe tree structure for this example is Python’s exception hierarchy. But the pattern\ncan be adapted to show a directory tree or any other tree structure.\nStarting from BaseException at level zero, the exception hierarchy is five levels deep\nas of Python 3.10. Our first baby step is to show level zero.\nGiven a root class, the tree generator in Example 17-27 yields its name and stops.\nExample 17-27. tree/step0/tree.py: yield the name of the root class and stop\ndef tree(cls):\n    yield cls.__name__\ndef display(cls):\n    for cls_name in tree(cls):\n        print(cls_name)\nif __name__ == '__main__':\n    display(BaseException)\nThe output of Example 17-27 is just one line:\nBaseException\n634 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 665,
      "chapter": null,
      "content": "The next baby step takes us to level 1. The tree generator will yield the name of the\nroot class and the names of each direct subclass. The names of the subclasses are\nindented to reveal the hierarchy. This is the output we want:\n$ python3 tree.py\nBaseException\n    Exception\n    GeneratorExit\n    SystemExit\n    KeyboardInterrupt\nExample 17-28 produces that output.\nExample 17-28. tree/step1/tree.py: yield the name of root class and direct subclasses\ndef tree(cls):\n    yield cls.__name__, 0                        \n    for sub_cls in cls.__subclasses__():         \n        yield sub_cls.__name__, 1                \ndef display(cls):\n    for cls_name, level in tree(cls):\n        indent = ' ' * 4 * level                 \n        print(f'{indent}{cls_name}')\nif __name__ == '__main__':\n    display(BaseException)\nTo support the indented output, yield the name of the class and its level in the\nhierarchy.\nUse the __subclasses__ special method to get a list of subclasses.\nYield name of subclass and level 1.\nBuild indentation string of 4 spaces times level. At level zero, this will be an\nempty string.\nIn Example 17-29, I refactor tree to separate the special case of the root class from\nthe subclasses, which are now handled in the sub_tree generator. At yield from, the\ntree generator is suspended, and sub_tree takes over yielding values.\nSubgenerators with yield from \n| \n635",
      "content_length": 1382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 666,
      "chapter": null,
      "content": "Example 17-29. tree/step2/tree.py: tree yields the root class name, then delegates to\nsub_tree\ndef tree(cls):\n    yield cls.__name__, 0\n    yield from sub_tree(cls)              \ndef sub_tree(cls):\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls.__name__, 1         \ndef display(cls):\n    for cls_name, level in tree(cls):     \n        indent = ' ' * 4 * level\n        print(f'{indent}{cls_name}')\nif __name__ == '__main__':\n    display(BaseException)\nDelegate to sub_tree to yield the names of the subclasses.\nYield the name of each subclass and level 1. Because of the yield from\nsub_tree(cls) inside tree, these values bypass the tree generator function\ncompletely…\n… and are received directly here.\nIn keeping with the baby steps method, I’ll write the simplest code I can imagine to\nreach level 2. For depth-first tree traversal, after yielding each node in level 1, I want\nto yield the children of that node in level 2, before resuming level 1. A nested for\nloop takes care of that, as in Example 17-30.\nExample 17-30. tree/step3/tree.py: sub_tree traverses levels 1 and 2 depth-first\ndef tree(cls):\n    yield cls.__name__, 0\n    yield from sub_tree(cls)\ndef sub_tree(cls):\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls.__name__, 1\n        for sub_sub_cls in sub_cls.__subclasses__():\n            yield sub_sub_cls.__name__, 2\n636 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 667,
      "chapter": null,
      "content": "def display(cls):\n    for cls_name, level in tree(cls):\n        indent = ' ' * 4 * level\n        print(f'{indent}{cls_name}')\nif __name__ == '__main__':\n    display(BaseException)\nThis is the result of running step3/tree.py from Example 17-30:\n$ python3 tree.py\nBaseException\n    Exception\n        TypeError\n        StopAsyncIteration\n        StopIteration\n        ImportError\n        OSError\n        EOFError\n        RuntimeError\n        NameError\n        AttributeError\n        SyntaxError\n        LookupError\n        ValueError\n        AssertionError\n        ArithmeticError\n        SystemError\n        ReferenceError\n        MemoryError\n        BufferError\n        Warning\n    GeneratorExit\n    SystemExit\n    KeyboardInterrupt\nYou may already know where this is going, but I will stick to baby steps one more\ntime: let’s reach level 3 by adding yet another nested for loop. The rest of the pro‐\ngram is unchanged, so Example 17-31 shows only the sub_tree generator.\nExample 17-31. sub_tree generator from tree/step4/tree.py\ndef sub_tree(cls):\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls.__name__, 1\n        for sub_sub_cls in sub_cls.__subclasses__():\n            yield sub_sub_cls.__name__, 2\n            for sub_sub_sub_cls in sub_sub_cls.__subclasses__():\n                yield sub_sub_sub_cls.__name__, 3\nSubgenerators with yield from \n| \n637",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 668,
      "chapter": null,
      "content": "There is a clear pattern in Example 17-31. We do a for loop to get the subclasses of\nlevel N. Each time around the loop, we yield a subclass of level N, then start another\nfor loop to visit level N+1.\nIn “Reinventing chain” on page 633, we saw how we can replace a nested for loop\ndriving a generator with yield from on the same generator. We can apply that idea\nhere, if we make sub_tree accept a level parameter, and yield from it recursively,\npassing the current subclass as the new root class with the next level number. See\nExample 17-32.\nExample 17-32. tree/step5/tree.py: recursive sub_tree goes as far as memory allows\ndef tree(cls):\n    yield cls.__name__, 0\n    yield from sub_tree(cls, 1)\ndef sub_tree(cls, level):\n    for sub_cls in cls.__subclasses__():\n        yield sub_cls.__name__, level\n        yield from sub_tree(sub_cls, level+1)\ndef display(cls):\n    for cls_name, level in tree(cls):\n        indent = ' ' * 4 * level\n        print(f'{indent}{cls_name}')\nif __name__ == '__main__':\n    display(BaseException)\nExample 17-32 can traverse trees of any depth, limited only by Python’s recursion\nlimit. The default limit allows 1,000 pending functions.\nAny good tutorial about recursion will stress the importance of having a base case to\navoid infinite recursion. A base case is a conditional branch that returns without\nmaking a recursive call. The base case is often implemented with an if statement. In\nExample 17-32, sub_tree has no if, but there is an implicit conditional in the for\nloop: if cls.__subclasses__() returns an empty list, the body of the loop is not exe‐\ncuted, therefore no recursive call happens. The base case is when the cls class has no\nsubclasses. In that case, sub_tree yields nothing. It just returns.\nExample 17-32 works as intended, but we can make it more concise by recalling the\npattern we observed when we reached level 3 (Example 17-31): we yield a subclass\nwith level N, then start a nested for loop to visit level N+1. In Example 17-32 we\n638 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 669,
      "chapter": null,
      "content": "replaced that nested loop with yield from. Now we can merge tree and sub_tree\ninto a single generator. Example 17-33 is the last step for this example.\nExample 17-33. tree/step6/tree.py: recursive calls of tree pass an incremented level\nargument\ndef tree(cls, level=0):\n    yield cls.__name__, level\n    for sub_cls in cls.__subclasses__():\n        yield from tree(sub_cls, level+1)\ndef display(cls):\n    for cls_name, level in tree(cls):\n        indent = ' ' * 4 * level\n        print(f'{indent}{cls_name}')\nif __name__ == '__main__':\n    display(BaseException)\nAt the start of “Subgenerators with yield from” on page 632, we saw how yield from\nconnects the subgenerator directly to the client code, bypassing the delegating gener‐\nator. That connection becomes really important when generators are used as corou‐\ntines and not only produce but also consume values from the client code, as we’ll see\nin “Classic Coroutines” on page 641.\nAfter this first encounter with yield from, let’s turn to type hinting iterables and\niterators.\nGeneric Iterable Types\nPython’s standard library has many functions that accept iterable arguments. In your\ncode, such functions can be annotated like the zip_replace function we saw in\nExample 8-15, using collections.abc.Iterable (or typing.Iterable if you must\nsupport Python 3.8 or earlier, as explained in “Legacy Support and Deprecated Col‐\nlection Types” on page 272). See Example 17-34.\nExample 17-34. replacer.py returns an iterator of tuples of strings\nfrom collections.abc import Iterable\nFromTo = tuple[str, str]  \ndef zip_replace(text: str, changes: Iterable[FromTo]) -> str:  \n    for from_, to in changes:\nGeneric Iterable Types \n| \n639",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 670,
      "chapter": null,
      "content": "text = text.replace(from_, to)\n    return text\nDefine type alias; not required, but makes the next type hint more readable.\nStarting with Python 3.10, FromTo should have a type hint of typing.TypeAlias\nto clarify the reason for this line: FromTo: TypeAlias = tuple[str, str].\nAnnotate changes to accept an Iterable of FromTo tuples.\nIterator types don’t appear as often as Iterable types, but they are also simple to\nwrite. Example 17-35 shows the familiar Fibonacci generator, annotated.\nExample 17-35. fibo_gen.py: fibonacci returns a generator of integers\nfrom collections.abc import Iterator\ndef fibonacci() -> Iterator[int]:\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\nNote that the type Iterator is used for generators coded as functions with yield, as\nwell as iterators written “by hand” as classes with __next__. There is also a collec\ntions.abc.Generator type (and the corresponding deprecated typing.Generator)\nthat we can use to annotate generator objects, but it is needlessly verbose for genera‐\ntors used as iterators.\nExample 17-36, when checked with Mypy, reveals that the Iterator type is really a\nsimplified special case of the Generator type.\nExample 17-36. itergentype.py: two ways to annotate iterators\nfrom collections.abc import Iterator\nfrom keyword import kwlist\nfrom typing import TYPE_CHECKING\nshort_kw = (k for k in kwlist if len(k) < 5)  \nif TYPE_CHECKING:\n    reveal_type(short_kw)  \nlong_kw: Iterator[str] = (k for k in kwlist if len(k) >= 4)  \nif TYPE_CHECKING:  \n    reveal_type(long_kw)\n640 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1614,
      "extraction_method": "Direct"
    },
    {
      "page_number": 671,
      "chapter": null,
      "content": "11 As of version 0.910, Mypy still uses the deprecated typing types.\nGenerator expression that yields Python keywords with less than 5 characters.\nMypy infers: typing.Generator[builtins.str*, None, None].11\nThis also yields strings, but I added an explicit type hint.\nRevealed type: typing.Iterator[builtins.str].\nabc.Iterator[str] is consistent-with abc.Generator[str, None, None], therefore\nMypy issues no errors for type checking in Example 17-36.\nIterator[T] is a shortcut for Generator[T, None, None]. Both annotations mean “a\ngenerator that yields items of type T, but that does not consume or return values.”\nGenerators able to consume and return values are coroutines, our next topic.\nClassic Coroutines\nPEP 342—Coroutines via Enhanced Generators introduced\nthe .send() and other features that made it possible to use genera‐\ntors as coroutines. PEP 342 uses the word “coroutine” with the\nsame meaning I am using here.\nIt is unfortunate that Python’s official documentation and standard\nlibrary now use inconsistent terminology to refer to generators\nused as coroutines, forcing me to adopt the “classic coroutine”\nqualifier to contrast with the newer “native coroutine” objects.\nAfter Python 3.5 came out, the trend is to use “coroutine” as a syn‐\nonym for “native coroutine.” But PEP 342 is not deprecated, and\nclassic coroutines still work as originally designed, although they\nare no longer supported by asyncio.\nUnderstanding classic coroutines in Python is confusing because they are actually\ngenerators used in a different way. So let’s step back and consider another feature of\nPython that can be used in two ways.\nWe saw in “Tuples Are Not Just Immutable Lists” on page 30 that we can use tuple\ninstances as records or as immutable sequences. When used as a record, a tuple is\nexpected to have a specific number of items, and each item may have a different type.\nWhen used as immutable lists, a tuple can have any length, and all items are expected\nto have the same type. That’s why there are two different ways to annotate tuples with\ntype hints:\nClassic Coroutines \n| \n641",
      "content_length": 2091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 672,
      "chapter": null,
      "content": "# A city record with name, country, and population:\ncity: tuple[str, str, int]\n# An immutable sequence of domain names:\ndomains: tuple[str, ...]\nSomething similar happens with generators. They are commonly used as iterators,\nbut they can also be used as coroutines. A coroutine is really a generator function,\ncreated with the yield keyword in its body. And a coroutine object is physically a\ngenerator object. Despite sharing the same underlying implementation in C, the use\ncases of generators and coroutines in Python are so different that there are two ways\nto type hint them:\n# The `readings` variable can be bound to an iterator\n# or generator object that yields `float` items:\nreadings: Iterator[float]\n# The `sim_taxi` variable can be bound to a coroutine\n# representing a taxi cab in a discrete event simulation.\n# It yields events, receives `float` timestamps, and returns\n# the number of trips made during the simulation:\nsim_taxi: Generator[Event, float, int]\nAdding to the confusion, the typing module authors decided to name that type Gen\nerator, when in fact it describes the API of a generator object intended to be used as\na coroutine, while generators are more often used as simple iterators.\nThe typing documentation describes the formal type parameters of Generator like\nthis:\nGenerator[YieldType, SendType, ReturnType]\nThe SendType is only relevant when the generator is used as a coroutine. That type\nparameter is the type of x in the call gen.send(x). It is an error to call .send() on a\ngenerator that was coded to behave as an iterator instead of a coroutine. Likewise,\nReturnType is only meaningful to annotate a coroutine, because iterators don’t return\nvalues like regular functions. The only sensible operation on a generator used as an\niterator is to call next(it) directly or indirectly via for loops and other forms of iter‐\nation. The YieldType is the type of the value returned by a call to next(it).\nThe Generator type has the same type parameters as typing.Coroutine:\nCoroutine[YieldType, SendType, ReturnType]\nThe typing.Coroutine documentation actually says: “The variance and order of type\nvariables correspond to those of Generator.” But typing.Coroutine (deprecated)\nand collections.abc.Coroutine (generic since Python 3.9) are intended to anno‐\ntate only native coroutines, not classic coroutines. If you want to use type hints with\n642 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 673,
      "chapter": null,
      "content": "12 Slide 33, “Keeping It Straight,” in “A Curious Course on Coroutines and Concurrency”.\n13 This example is inspired by a snippet from Jacob Holm in the Python-ideas list, message titled “Yield-From:\nFinalization guarantees”. Some variations appear later in the thread, and Holm further explains his thinking\nin message 003912.\nclassic coroutines, you’ll suffer through the confusion of annotating them as Genera\ntor[YieldType, SendType, ReturnType].\nDavid Beazley created some of the best talks and most comprehensive workshops\nabout classic coroutines. In his PyCon 2009 course handout, he has a slide titled\n“Keeping It Straight,” which reads:\n• Generators produce data for iteration\n• Coroutines are consumers of data\n• To keep your brain from exploding, don’t mix the two concepts together\n• Coroutines are not related to iteration\n• Note: There is a use of having `yield` produce a value in a coroutine, but it’s not\ntied to iteration.12\nNow let’s see how classic coroutines work.\nExample: Coroutine to Compute a Running Average\nWhile discussing closures in Chapter 9, we studied objects to compute a running\naverage. Example 9-7 shows a class and Example 9-13 presents a higher-order func‐\ntion returning a function that keeps the total and count variables across invocations\nin a closure. Example 17-37 shows how to do the same with a coroutine.13\nExample 17-37. coroaverager.py: coroutine to compute a running average\nfrom collections.abc import Generator\ndef averager() -> Generator[float, float, None]:  \n    total = 0.0\n    count = 0\n    average = 0.0\n    while True:  \n        term = yield average  \n        total += term\n        count += 1\n        average = total/count\nClassic Coroutines \n| \n643",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 674,
      "chapter": null,
      "content": "14 In fact, it never returns unless some exception breaks the loop. Mypy 0.910 accepts both None and typing\n.NoReturn as the generator return type parameter—but it also accepts str in that position, so apparently it\ncan’t fully analyze the coroutine code at this time.\nThis function returns a generator that yields float values, accepts float values\nvia .send(), and does not return a useful value.14\nThis infinite loop means the coroutine will keep on yielding averages as long as\nthe client code sends values.\nThe yield statement here suspends the coroutine, yields a result to the client,\nand—later—gets a value sent by the caller to the coroutine, starting another iter‐\nation of the infinite loop.\nIn a coroutine, total and count can be local variables: no instance attributes or clo‐\nsures are needed to keep the context while the coroutine is suspended waiting for the\nnext .send(). That’s why coroutines are attractive replacements for callbacks in asyn‐\nchronous programming—they keep local state between activations.\nExample 17-38 runs doctests to show the averager coroutine in operation.\nExample 17-38. coroaverager.py: doctest for the running average coroutine in\nExample 17-37\n    >>> coro_avg = averager()  \n    >>> next(coro_avg)  \n    0.0\n    >>> coro_avg.send(10)  \n    10.0\n    >>> coro_avg.send(30)\n    20.0\n    >>> coro_avg.send(5)\n    15.0\nCreate the coroutine object.\nStart the coroutine. This yields the initial value of average: 0.0.\nNow we are in business: each call to .send() yields the current average.\nIn Example 17-38, the call next(coro_avg) makes the coroutine advance to the\nyield, yielding the initial value for average. You can also start the coroutine by call‐\ning coro_avg.send(None)—this is actually what the next() built-in does. But you\ncan’t send any value other than None, because the coroutine can only accept a sent\n644 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 675,
      "chapter": null,
      "content": "value when it is suspended at a yield line. Calling next() or .send(None) to advance\nto the first yield is known as “priming the coroutine.”\nAfter each activation, the coroutine is suspended precisely at the yield keyword,\nwaiting for a value to be sent. The line coro_avg.send(10) provides that value, caus‐\ning the coroutine to activate. The yield expression resolves to the value 10, assigning\nit to the term variable. The rest of the loop updates the total, count, and average\nvariables. The next iteration in the while loop yields the average, and the coroutine\nis again suspended at the yield keyword.\nThe attentive reader may be anxious to know how the execution of an averager\ninstance (e.g., coro_avg) may be terminated, because its body is an infinite loop. We\ndon’t usually need to terminate a generator, because it is garbage collected as soon as\nthere are no more valid references to it. If you need to explicitly terminate it, use\nthe .close() method, as shown in Example 17-39.\nExample 17-39. coroaverager.py: continuing from Example 17-38\n    >>> coro_avg.send(20)  \n    16.25\n    >>> coro_avg.close()  \n    >>> coro_avg.close()  \n    >>> coro_avg.send(5)  \n    Traceback (most recent call last):\n      ...\n    StopIteration\ncoro_avg is the instance created in Example 17-38.\nThe .close() method raises GeneratorExit at the suspended yield expression.\nIf not handled in the coroutine function, the exception terminates it. Generator\nExit is caught by the generator object that wraps the coroutine—that’s why we\ndon’t see it.\nCalling .close() on a previously closed coroutine has no effect.\nTrying .send() on a closed coroutine raises StopIteration.\nBesides the .send() method, PEP 342—Coroutines via Enhanced Generators also\nintroduced a way for a coroutine to return a value. The next section shows how.\nClassic Coroutines \n| \n645",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 676,
      "chapter": null,
      "content": "15 I considered renaming the field, but count is the best name for the local variable in the coroutine, and is the\nname I used for this variable in similar examples in the book, so it makes sense to use the same name in the\nResult field. I don’t hesitate to use # type: ignore to avoid the limitations and annoyances of static type\ncheckers when submission to the tool would make the code worse or needlessly complicated.\nReturning a Value from a Coroutine\nWe’ll now study another coroutine to compute an average. This version will not yield\npartial results. Instead, it returns a tuple with the number of terms and the average.\nI’ve split the listing in two parts: Example 17-40 and Example 17-41.\nExample 17-40. coroaverager2.py: top of the file\nfrom collections.abc import Generator\nfrom typing import Union, NamedTuple\nclass Result(NamedTuple):  \n    count: int  # type: ignore  \n    average: float\nclass Sentinel:  \n    def __repr__(self):\n        return f'<Sentinel>'\nSTOP = Sentinel()  \nSendType = Union[float, Sentinel]  \nThe averager2 coroutine in Example 17-41 will return an instance of Result.\nThe Result is actually a subclass of tuple, which has a .count() method that I\ndon’t need. The # type: ignore comment prevents Mypy from complaining\nabout having a count field.15\nA class to make a sentinel value with a readable __repr__.\nThe sentinel value that I’ll use to make the coroutine stop collecting data and\nreturn a result.\nI’ll use this type alias for the second type parameter of the coroutine Generator\nreturn type, the SendType parameter.\nThe SendType definition also works in Python 3.10, but if you don’t need to support\nearlier versions, it is better to write it like this, after importing TypeAlias from\ntyping:\n646 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 677,
      "chapter": null,
      "content": "SendType: TypeAlias = float | Sentinel\nUsing | instead of typing.Union is so concise and readable that I’d probably not cre‐\nate that type alias, but instead I’d write the signature of averager2 like this:\ndef averager2(verbose: bool=False) -> Generator[None, float | Sentinel, Result]:\nNow, let’s study the coroutine code itself (Example 17-41).\nExample 17-41. coroaverager2.py: a coroutine that returns a result value\ndef averager2(verbose: bool = False) -> Generator[None, SendType, Result]:  \n    total = 0.0\n    count = 0\n    average = 0.0\n    while True:\n        term = yield  \n        if verbose:\n            print('received:', term)\n        if isinstance(term, Sentinel):  \n            break\n        total += term  \n        count += 1\n        average = total / count\n    return Result(count, average)  \nFor this coroutine, the yield type is None because it does not yield data. It receives\ndata of the SendType and returns a Result tuple when done.\nUsing yield like this only makes sense in coroutines, which are designed to con‐\nsume data. This yields None, but receives a term from .send(term).\nIf the term is a Sentinel, break from the loop. Thanks to this isinstance\ncheck…\n…Mypy allows me to add term to the total without flagging an error that I can’t\nadd a float to an object that may be a float or a Sentinel.\nThis line will be reached only if a Sentinel is sent to the coroutine.\nNow let’s see how we can use this coroutine, starting with a simple example that\ndoesn’t actually produce a result (Example 17-42).\nExample 17-42. coroaverager2.py: doctest showing .cancel()\n    >>> coro_avg = averager2()\n    >>> next(coro_avg)\n    >>> coro_avg.send(10)  \nClassic Coroutines \n| \n647",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 678,
      "chapter": null,
      "content": ">>> coro_avg.send(30)\n    >>> coro_avg.send(6.5)\n    >>> coro_avg.close()  \nRecall that averager2 does not yield partial results. It yields None, which\nPython’s console omits.\nCalling .close() in this coroutine makes it stop but does not return a result,\nbecause the GeneratorExit exception is raised at the yield line in the coroutine,\nso the return statement is never reached.\nNow let’s make it work in Example 17-43.\nExample 17-43. coroaverager2.py: doctest showing StopIteration with a Result\n    >>> coro_avg = averager2()\n    >>> next(coro_avg)\n    >>> coro_avg.send(10)\n    >>> coro_avg.send(30)\n    >>> coro_avg.send(6.5)\n    >>> try:\n    ...     coro_avg.send(STOP)  \n    ... except StopIteration as exc:\n    ...     result = exc.value  \n    ...\n    >>> result  \n    Result(count=3, average=15.5)\nSending the STOP sentinel makes the coroutine break from the loop and return a\nResult. The generator object that wraps the coroutine then raises StopItera\ntion.\nThe StopIteration instance has a value attribute bound to the value of the\nreturn statement that terminated the coroutine.\nBelieve it or not!\nThis idea of “smuggling” the return value out of the coroutine wrapped in a StopIter\nation exception is a bizarre hack. Nevertheless, this bizarre hack is part of PEP 342—\nCoroutines via Enhanced Generators, and is documented with the StopIteration\nexception, and in the “Yield expressions” section of Chapter 6 of The Python Lan‐\nguage Reference.\nA delegating generator can get the return value of a coroutine directly using the\nyield from syntax, as shown in Example 17-44.\n648 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 679,
      "chapter": null,
      "content": "Example 17-44. coroaverager2.py: doctest showing StopIteration with a Result\n    >>> def compute():\n    ...     res = yield from averager2(True)  \n    ...     print('computed:', res)  \n    ...     return res  \n    ...\n    >>> comp = compute()  \n    >>> for v in [None, 10, 20, 30, STOP]:  \n    ...     try:\n    ...         comp.send(v)  \n    ...     except StopIteration as exc:  \n    ...         result = exc.value\n    received: 10\n    received: 20\n    received: 30\n    received: <Sentinel>\n    computed: Result(count=3, average=20.0)\n    >>> result  \n    Result(count=3, average=20.0)\nres will collect the return value of averager2; the yield from machinery\nretrieves the return value when it handles the StopIteration exception that\nmarks the termination of the coroutine. When True, the verbose parameter\nmakes the coroutine print the value received, to make its operation visible.\nKeep an eye out for the output of this line when this generator runs.\nReturn the result. This will also be wrapped in StopIteration.\nCreate the delegating coroutine object.\nThis loop will drive the delegating coroutine.\nFirst value sent is None, to prime the coroutine; last is the sentinel to stop it.\nCatch StopIteration to fetch the return value of compute.\nAfter the lines output by averager2 and compute, we get the Result instance.\nEven though the examples here don’t do much, the code is hard to follow. Driving\nthe coroutine with .send() calls and retrieving results is complicated, except with\nyield from—but we can only use that syntax inside a delegating generator/corou‐\ntine, which must ultimately be driven by some nontrivial code, as shown in\nExample 17-44.\nClassic Coroutines \n| \n649",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 680,
      "chapter": null,
      "content": "16 Since Python 3.7, typing.Generator and other types that correspond to ABCs in collections.abc were\nrefactored with a wrapper around the corresponding ABC, so their generic parameters aren’t visible in the\ntyping.py source file. That’s why I refer to Python 3.6 source code here.\nThe previous examples show that using coroutines directly is cumbersome and con‐\nfusing. Add exception handling and the coroutine .throw() method, and examples\nbecome even more convoluted. I won’t cover .throw() in this book because—\nlike .send()—it is only useful to drive coroutines “by hand,” but I don’t recommend\ndoing that, unless you are creating a new coroutine-based framework from scratch.\nIf you are interested in deeper coverage of classic coroutines—\nincluding the .throw() method—please check out “Classic Corou‐\ntines” at the fluentpython.com companion website. That post\nincludes Python-like pseudocode detailing how yield from drives\ngenerators and coroutines, as well as a a small discrete event simu‐\nlation demonstrating a form of concurrency using coroutines\nwithout an asynchronous programming framework.\nIn practice, productive work with coroutines requires the support of a specialized\nframework. That is what asyncio provided for classic coroutines way back in Python\n3.3. With the advent of native coroutines in Python 3.5, the Python core developers\nare gradually phasing out support for classic coroutines in asyncio. But the underly‐\ning mechanisms are very similar. The async def syntax makes native coroutines eas‐\nier to spot in code, which is a great benefit. Inside, native coroutines use await\ninstead of yield from to delegate to other coroutines. Chapter 21 is all about that.\nNow let’s wrap up the chapter with a mind-bending section about covariance and\ncontravariance in type hints for coroutines.\nGeneric Type Hints for Classic Coroutines\nBack in “Contravariant types” on page 550, I mentioned typing.Generator as one of\nthe few standard library types with a contravariant type parameter. Now that we’ve\nstudied classic coroutines, we are ready to make sense of this generic type.\nHere is how typing.Generator was declared in the typing.py module of Python 3.6:16\nT_co = TypeVar('T_co', covariant=True)\nV_co = TypeVar('V_co', covariant=True)\nT_contra = TypeVar('T_contra', contravariant=True)\n# many lines omitted\nclass Generator(Iterator[T_co], Generic[T_co, T_contra, V_co],\n                extra=_G_base):\n650 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 681,
      "chapter": null,
      "content": "That generic type declaration means that a Generator type hint requires those three\ntype parameters we’ve seen before:\nmy_coro : Generator[YieldType, SendType, ReturnType]\nFrom the type variables in the formal parameters, we see that YieldType and Return\nType are covariant, but SendType is contravariant. To understand why, consider that\nYieldType and ReturnType are “output” types. Both describe data that comes out of\nthe coroutine object—i.e., the generator object when used as a coroutine object.\nIt makes sense that these are covariant, because any code expecting a coroutine that\nyields floats can use a coroutine that yields integers. That’s why Generator is cova‐\nriant on its YieldType parameter. The same reasoning applies to the ReturnType\nparameter—also covariant.\nUsing the notation introduced in “Covariant types” on page 550, the covariance of\nthe first and third parameters is expressed by the :> symbols pointing in the same\ndirection:\n                       float :> int\nGenerator[float, Any, float] :> Generator[int, Any, int]\nYieldType and ReturnType are examples of the first rule of “Variance rules of\nthumb” on page 551:\n1. If a formal type parameter defines a type for data that comes out of the object, it\ncan be covariant.\nOn the other hand, SendType is an “input” parameter: it is the type of the value argu‐\nment for the .send(value) method of the coroutine object. Client code that needs to\nsend floats to a coroutine cannot use a coroutine with int as the SendType because\nfloat is not a subtype of int. In other words, float is not consistent-with int. But\nthe client can use a coroutine with complex as the SendType, because float is a sub‐\ntype of complex, therefore float is consistent-with complex.\nThe :> notation makes the contravariance of the second parameter visible:\n                     float :> int\nGenerator[Any, float, Any] <: Generator[Any, int, Any]\nThis is an example of the second Variance Rule of Thumb:\n2. If a formal type parameter defines a type for data that goes into the object after its\ninitial construction, it can be contravariant.\nThis merry discussion of variance completes the longest chapter in the book.\nClassic Coroutines \n| \n651",
      "content_length": 2195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 682,
      "chapter": null,
      "content": "17 According to the Jargon file, to grok is not merely to learn something, but to absorb it so “it becomes part of\nyou, part of your identity.”\nChapter Summary\nIteration is so deeply embedded in the language that I like to say that Python groks\niterators.17 The integration of the Iterator pattern in the semantics of Python is a\nprime example of how design patterns are not equally applicable in all programming\nlanguages. In Python, a classic Iterator implemented “by hand” as in Example 17-4\nhas no practical use, except as a didactic example.\nIn this chapter, we built a few versions of a class to iterate over individual words in\ntext files that may be very long. We saw how Python uses the iter() built-in to create\niterators from sequence-like objects. We build a classic iterator as a class with\n__next__(), and then we used generators to make each successive refactoring of the\nSentence class more concise and readable.\nWe then coded a generator of arithmetic progressions and showed how to leverage\nthe itertools module to make it simpler. An overview of most general-purpose gen‐\nerator functions in the standard library followed.\nWe then studied yield from expressions in the context of simple generators with the\nchain and tree examples.\nThe last major section was about classic coroutines, a topic of waning importance\nafter native coroutines were added in Python 3.5. Although difficult to use in prac‐\ntice, classic coroutines are the foundation of native coroutines, and the yield from\nexpression is the direct precursor of await.\nAlso covered were type hints for Iterable, Iterator, and Generator types—with the\nlatter providing a concrete and rare example of a contravariant type parameter.\nFurther Reading\nA detailed technical explanation of generators appears in The Python Language Refer‐\nence in “6.2.9. Yield expressions”. The PEP where generator functions were defined is\nPEP 255—Simple Generators.\nThe itertools module documentation is excellent because of all the examples\nincluded. Although the functions in that module are implemented in C, the docu‐\nmentation shows how some of them would be written in Python, often by leveraging\nother functions in the module. The usage examples are also great; for instance, there\nis a snippet showing how to use the accumulate function to amortize a loan with\ninterest, given a list of payments over time. There is also an “Itertools Recipes”\n652 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 683,
      "chapter": null,
      "content": "section with additional high-performance functions that use the itertools functions\nas building blocks.\nBeyond Python’s standard library, I recommend the More Itertools package, which\nfollows the fine itertools tradition in providing powerful generators with plenty of\nexamples and some useful recipes.\nChapter 4, “Iterators and Generators,” of Python Cookbook, 3rd ed., by David Beazley\nand Brian K. Jones (O’Reilly), has 16 recipes covering this subject from many differ‐\nent angles, focusing on practical applications. It includes some illuminating recipes\nwith yield from.\nSebastian Rittau—currently a top contributor of typeshed—explains why iterators\nshould be iterable, as he noted in 2006 that, “Java: Iterators are not Iterable”.\nThe yield from syntax is explained with examples in the “What’s New in Python\n3.3” section of PEP 380—Syntax for Delegating to a Subgenerator. My post “Classic\nCoroutines” at fluentpython.com explains yield from in depth, including Python\npseudocode of its implementation in C.\nDavid Beazley is the ultimate authority on Python generators and coroutines. The\nPython Cookbook, 3rd ed., (O’Reilly) he coauthored with Brian Jones has numerous\nrecipes with coroutines. Beazley’s PyCon tutorials on the subject are famous for their\ndepth and breadth. The first was at PyCon US 2008: “Generator Tricks for Systems\nProgrammers”. PyCon US 2009 saw the legendary “A Curious Course on Coroutines\nand Concurrency” (hard-to-find video links for all three parts: part 1, part 2, and part\n3). His tutorial from PyCon 2014 in Montréal was “Generators: The Final Frontier”,\nin which he tackles more concurrency examples—so it’s really more about topics in\nChapter 21. Dave can’t resist making brains explode in his classes, so in the last part\nof “The Final Frontier,” coroutines replace the classic Visitor pattern in an arithmetic\nexpression evaluator.\nCoroutines allow new ways of organizing code, and just as recursion or polymor‐\nphism (dynamic dispatch), it takes some time getting used to their possibilities. An\ninteresting example of classic algorithm rewritten with coroutines is in the post\n“Greedy algorithm with coroutines”, by James Powell.\nBrett Slatkin’s Effective Python, 1st ed. (Addison-Wesley) has an excellent short chap‐\nter titled “Consider Coroutines to Run Many Functions Concurrently.” That chapter\nis not in the second edition of Effective Python, but it is still available online as a sam‐\nple chapter. Slatkin presents the best example of driving coroutines with yield from\nthat I’ve seen: an implementation of John Conway’s Game of Life in which corou‐\ntines manage the state of each cell as the game runs. I refactored the code for the\nGame of Life example—separating the functions and classes that implement the game\nfrom the testing snippets used in Slatkin’s original code. I also rewrote the tests as\nFurther Reading \n| \n653",
      "content_length": 2882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 684,
      "chapter": null,
      "content": "18 Gamma et. al., Design Patterns: Elements of Reusable Object-Oriented Software, p. 261.\ndoctests, so you can see the output of the various coroutines and classes without run‐\nning the script. The refactored example is posted as a GitHub gist.\nSoapbox\nThe Minimalistic Iterator Interface in Python\nIn the “Implementation” section of the Iterator pattern,18 the Gang of Four wrote:\nThe minimal interface to Iterator consists of the operations First, Next, IsDone, and\nCurrentItem.\nHowever, that very sentence has a footnote that reads:\nWe can make this interface even smaller by merging Next, IsDone, and CurrentItem\ninto a single operation that advances to the next object and returns it. If the traversal\nis finished, then this operation returns a special value (0, for instance) that marks the\nend of the iteration.\nThis is close to what we have in Python: the single method __next__ does the job. But\ninstead of using a sentinel, which could be overlooked by mistake, the StopIteration\nexception signals the end of the iteration. Simple and correct: that’s the Python way.\nPluggable Generators\nAnyone who manages large datasets finds many uses for generators. This is the story\nof the first time I built a practical solution around generators.\nYears ago I worked at BIREME, a digital library run by PAHO/WHO (Pan-American\nHealth Organization/World Health Organization) in São Paulo, Brazil. Among the\nbibliographic datasets created by BIREME are LILACS (Latin American and Carib‐\nbean Health Sciences index) and SciELO (Scientific Electronic Library Online), two\ncomprehensive databases indexing the research literature about health sciences pro‐\nduced in the region.\nSince the late 1980s, the database system used to manage LILACS is CDS/ISIS, a non-\nrelational document database created by UNESCO. One of my jobs was to research\nalternatives for a possible migration of LILACS—and eventually the much larger\nSciELO—to a modern, open source, document database such as CouchDB or Mon‐\ngoDB. At the time, I wrote a paper explaining the semistructured data model and\ndifferent ways to represent CDS/ISIS data with JSON-like records: “From ISIS to\nCouchDB: Databases and Data Models for Bibliographic Records”.\nAs part of that research, I wrote a Python script to read a CDS/ISIS file and write a\nJSON file suitable for importing to CouchDB or MongoDB. Initially, the script read\n654 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 2446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 685,
      "chapter": null,
      "content": "19 The code is in Python 2 because one of its optional dependencies is a Java library named Bruma, which we\ncan import when we run the script with Jython—which does not yet support Python 3.\n20 The library used to read the complex .mst binary is actually written in Java, so this functionality is only avail‐\nable when isis2json.py is executed with the Jython interpreter, version 2.5 or newer. For further details, see the\nREADME.rst file in the repository. The dependencies are imported inside the generator functions that need\nthem, so the script can run even if only one of the external libraries is available.\nfiles in the ISO-2709 format exported by CDS/ISIS. The reading and writing had to be\ndone incrementally because the full datasets were much bigger than main memory.\nThat was easy enough: each iteration of the main for loop read one record from\nthe .iso file, massaged it, and wrote it to the .json output.\nHowever, for operational reasons, it was deemed necessary that isis2json.py supported\nanother CDS/ISIS data format: the binary .mst files used in production at BIREME—\nto avoid the costly export to ISO-2709. Now I had a problem: the libraries used to\nread ISO-2709 and .mst files had very different APIs. And the JSON writing loop was\nalready complicated because the script accepted a variety of command-line options to\nrestructure each output record. Reading data using two different APIs in the same\nfor loop where the JSON was produced would be unwieldy.\nThe solution was to isolate the reading logic into a pair of generator functions: one\nfor each supported input format. In the end, I split the isis2json.py script into four\nfunctions. You can see the Python 2 source code with dependencies in the fluentpy‐\nthon/isis2json repository on GitHub.19\nHere is a high-level overview of how the script is structured:\nmain\nThe main function uses argparse to read command-line options that configure\nthe structure of the output records. Based on the input filename extension, a suit‐\nable generator function is selected to read the data and yield the records, one by\none.\niter_iso_records\nThis generator function reads .iso files (assumed to be in the ISO-2709 format). It\ntakes two arguments: the filename and isis_json_type, one of the options\nrelated to the record structure. Each iteration of its for loop reads one record,\ncreates an empty dict, populates it with field data, and yields the dict.\niter_mst_records\nThis other generator functions reads .mst files.20 If you look at the source code\nfor isis2json.py, you’ll see that it’s not as simple as iter_iso_records, but its\ninterface and overall structure is the same: it takes a filename and an\nisis_json_type argument and enters a for loop, which builds and yields one\ndict per iteration, representing a single record.\nFurther Reading \n| \n655",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 686,
      "chapter": null,
      "content": "write_json\nThis function performs the actual writing of the JSON records, one at a time. It\ntakes numerous arguments, but the first one—input_gen—is a reference to a\ngenerator function: either iter_iso_records or iter_mst_records. The main\nfor loop in write_json iterates over the dictionaries yielded by the selected\ninput_gen generator, restructures it in different ways as determined by the\ncommand-line options, and appends the JSON record to the output file.\nBy leveraging generator functions, I was able to decouple the reading from the writ‐\ning. Of course, the simplest way to decouple them would be to read all records to\nmemory, then write them to disk. But that was not a viable option because of the size\nof the datasets. Using generators, the reading and writing is interleaved, so the script\ncan process files of any size. Also, the special logic for reading a record in the differ‐\nent input formats is separated from the logic of restructuring each record for writing.\nNow, if we need isis2json.py to support an additional input format—say, MARCXML,\na DTD used by the US Library of Congress to represent ISO-2709 data—it will be\neasy to add a third generator function to implement the reading logic, without chang‐\ning anything in the complicated write_json function.\nThis is not rocket science, but it’s a real example where generators enabled an effi‐\ncient and flexible solution to process databases as a stream of records, keeping mem‐\nory usage low regardless of the size of the dataset.\n656 \n| \nChapter 17: Iterators, Generators, and Classic Coroutines",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 687,
      "chapter": null,
      "content": "1 PyCon US 2013 keynote: “What Makes Python Awesome”; the part about with starts at 23:00 and ends at\n26:15.\nCHAPTER 18\nwith, match, and else Blocks\nContext managers may end up being almost as important as the subroutine itself.\nWe’ve only scratched the surface with them. […] Basic has a with statement, there are\nwith statements in lots of languages. But they don’t do the same thing, they all do\nsomething very shallow, they save you from repeated dotted [attribute] lookups, they\ndon’t do setup and tear down. Just because it’s the same name don’t think it’s the same\nthing. The with statement is a very big deal.1\n—Raymond Hettinger, eloquent Python evangelist\nThis chapter is about control flow features that are not so common in other lan‐\nguages, and for this reason tend to be overlooked or underused in Python. They are:\n• The with statement and context manager protocol\n• Pattern matching with match/case\n• The else clause in for, while, and try statements\nThe with statement sets up a temporary context and reliably tears it down, under the\ncontrol of a context manager object. This prevents errors and reduces boilerplate\ncode, making APIs at the same time safer and easier to use. Python programmers are\nfinding lots of uses for with blocks beyond automatic file closing.\nWe’ve seen pattern matching in previous chapters, but here we’ll see how the gram‐\nmar of a language can be expressed as sequence patterns. That observation explains\nwhy match/case is an effective tool to create language processors that are easy to\nunderstand and extend. We’ll study a complete interpreter for a small but functional\n657",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 688,
      "chapter": null,
      "content": "subset of the Scheme language. The same ideas could be applied to develop a tem‐\nplate language or a DSL (Domain-Specific Language) to encode business rules in a\nlarger system.\nThe else clause is not a big deal, but it does help convey intention when properly\nused together with for, while, and try.\nWhat’s New in This Chapter\n“Pattern Matching in lis.py: A Case Study” on page 669 is a new section.\nI updated “The contextlib Utilities” on page 663 to cover a few features of the context\nlib module added since Python 3.6, and the new parenthesized context managers\nsyntax introduced in Python 3.10.\nLet’s start with the powerful with statement.\nContext Managers and with Blocks\nContext manager objects exist to control a with statement, just like iterators exist to\ncontrol a for statement.\nThe with statement was designed to simplify some common uses of try/finally,\nwhich guarantees that some operation is performed after a block of code, even if the\nblock is terminated by return, an exception, or a sys.exit() call. The code in the\nfinally clause usually releases a critical resource or restores some previous state that\nwas temporarily changed.\nThe Python community is finding new, creative uses for context managers. Some\nexamples from the standard library are:\n• Managing transactions in the sqlite3 module—see “Using the connection as a\ncontext manager”.\n• Safely handling locks, conditions, and semaphores—as described in the thread\ning module documentation.\n• Setting up custom environments for arithmetic operations with Decimal objects\n—see the decimal.localcontext documentation.\n• Patching objects for testing—see the unittest.mock.patch function.\nThe context manager interface consists of the __enter__ and __exit__ methods. At\nthe top of the with, Python calls the __enter__ method of the context manager\nobject. When the with block completes or terminates for any reason, Python calls\n__exit__ on the context manager object.\n658 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 689,
      "chapter": null,
      "content": "The most common example is making sure a file object is closed. Example 18-1 is a\ndetailed demonstration of using with to close a file.\nExample 18-1. Demonstration of a file object as a context manager\n>>> with open('mirror.py') as fp:  \n...     src = fp.read(60)  \n...\n>>> len(src)\n60\n>>> fp  \n<_io.TextIOWrapper name='mirror.py' mode='r' encoding='UTF-8'>\n>>> fp.closed, fp.encoding  \n(True, 'UTF-8')\n>>> fp.read(60)  \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: I/O operation on closed file.\nfp is bound to the opened text file because the file’s __enter__ method returns\nself.\nRead 60 Unicode characters from fp.\nThe fp variable is still available—with blocks don’t define a new scope, as func‐\ntions do.\nWe can read the attributes of the fp object.\nBut we can’t read more text from fp because at the end of the with block, the\nTextIOWrapper.__exit__ method was called, and it closed the file.\nThe first callout in Example 18-1 makes a subtle but crucial point: the context man‐\nager object is the result of evaluating the expression after with, but the value bound\nto the target variable (in the as clause) is the result returned by the __enter__\nmethod of the context manager object.\nIt just happens that the open() function returns an instance of TextIOWrapper, and\nits __enter__ method returns self. But in a different class, the __enter__ method\nmay also return some other object instead of the context manager instance.\nWhen control flow exits the with block in any way, the __exit__ method is invoked\non the context manager object, not on whatever was returned by __enter__.\nContext Managers and with Blocks \n| \n659",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 690,
      "chapter": null,
      "content": "The as clause of the with statement is optional. In the case of open, we always need it\nto get a reference to the file, so that we can call methods on it. But some context man‐\nagers return None because they have no useful object to give back to the user.\nExample 18-2 shows the operation of a perfectly frivolous context manager designed\nto highlight the distinction between the context manager and the object returned by\nits __enter__ method.\nExample 18-2. Test-driving the LookingGlass context manager class\n    >>> from mirror import LookingGlass\n    >>> with LookingGlass() as what:  \n    ...      print('Alice, Kitty and Snowdrop')  \n    ...      print(what)\n    ...\n    pordwonS dna yttiK ,ecilA\n    YKCOWREBBAJ\n    >>> what  \n    'JABBERWOCKY'\n    >>> print('Back to normal.')  \n    Back to normal.\nThe context manager is an instance of LookingGlass; Python calls __enter__ on\nthe context manager and the result is bound to what.\nPrint a str, then the value of the target variable what. The output of each print\nwill come out reversed.\nNow the with block is over. We can see that the value returned by __enter__,\nheld in what, is the string 'JABBERWOCKY'.\nProgram output is no longer reversed.\nExample 18-3 shows the implementation of LookingGlass.\nExample 18-3. mirror.py: code for the LookingGlass context manager class\nimport sys\nclass LookingGlass:\n    def __enter__(self):  \n        self.original_write = sys.stdout.write  \n        sys.stdout.write = self.reverse_write  \n        return 'JABBERWOCKY'  \n    def reverse_write(self, text):  \n660 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 691,
      "chapter": null,
      "content": "self.original_write(text[::-1])\n    def __exit__(self, exc_type, exc_value, traceback):  \n        sys.stdout.write = self.original_write  \n        if exc_type is ZeroDivisionError:  \n            print('Please DO NOT divide by zero!')\n            return True  \n        \nPython invokes __enter__ with no arguments besides self.\nHold the original sys.stdout.write method, so we can restore it later.\nMonkey-patch sys.stdout.write, replacing it with our own method.\nReturn the 'JABBERWOCKY' string just so we have something to put in the target\nvariable what.\nOur replacement to sys.stdout.write reverses the text argument and calls the\noriginal implementation.\nPython calls __exit__ with None, None, None if all went well; if an exception is\nraised, the three arguments get the exception data, as described after this\nexample.\nRestore the original method to sys.stdout.write.\nIf the exception is not None and its type is ZeroDivisionError, print a message…\n…and return True to tell the interpreter that the exception was handled.\nIf __exit__ returns None or any falsy value, any exception raised in the with\nblock will be propagated.\nWhen real applications take over standard output, they often want\nto replace sys.stdout with another file-like object for a while, then\nswitch back to the original. The contextlib.redirect_stdout\ncontext manager does exactly that: just pass it the file-like object\nthat will stand in for sys.stdout.\nThe interpreter calls the __enter__ method with no arguments—beyond the implicit\nself. The three arguments passed to __exit__ are:\nContext Managers and with Blocks \n| \n661",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 692,
      "chapter": null,
      "content": "2 The three arguments received by self are exactly what you get if you call sys.exc_info() in the finally\nblock of a try/finally statement. This makes sense, considering that the with statement is meant to replace\nmost uses of try/finally, and calling sys.exc_info() was often necessary to determine what clean-up\naction would be required.\nexc_type\nThe exception class (e.g., ZeroDivisionError).\nexc_value\nThe exception instance. Sometimes, parameters passed to the exception con‐\nstructor—such as the error message—can be found in exc_value.args.\ntraceback\nA traceback object.2\nFor a detailed look at how a context manager works, see Example 18-4, where\nLookingGlass is used outside of a with block, so we can manually call its __enter__\nand __exit__ methods.\nExample 18-4. Exercising LookingGlass without a with block\n    >>> from mirror import LookingGlass\n    >>> manager = LookingGlass()  \n    >>> manager  # doctest: +ELLIPSIS\n    <mirror.LookingGlass object at 0x...>\n    >>> monster = manager.__enter__()  \n    >>> monster == 'JABBERWOCKY'  \n    eurT\n    >>> monster\n    'YKCOWREBBAJ'\n    >>> manager  # doctest: +ELLIPSIS\n    >... ta tcejbo ssalGgnikooL.rorrim<\n    >>> manager.__exit__(None, None, None)  \n    >>> monster\n    'JABBERWOCKY'\nInstantiate and inspect the manager instance.\nCall the manager’s __enter__ method and store result in monster.\nmonster is the string 'JABBERWOCKY'. The True identifier appears reversed\nbecause all output via stdout goes through the write method we patched in\n__enter__.\nCall manager.__exit__ to restore the previous stdout.write.\n662 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 693,
      "chapter": null,
      "content": "Parenthesized Context Managers in Python 3.10\nPython 3.10 adopted a new, more powerful parser, allowing new\nsyntax beyond what was possible with the older LL(1) parser. One\nsyntax enhancement was to allow parenthesized context managers,\nlike this:\nwith (\n    CtxManager1() as example1,\n    CtxManager2() as example2,\n    CtxManager3() as example3,\n):\n    ...\nPrior to 3.10, we’d have to write that as nested with blocks.\nThe standard library includes the contextlib package with handy functions, classes,\nand decorators for building, combining, and using context managers.\nThe contextlib Utilities\nBefore rolling your own context manager classes, take a look at contextlib—“Utilit‐\nies for with-statement contexts” in the Python documentation. Maybe what you are\nabout to build already exists, or there is a class or some callable that will make your\njob easier.\nBesides the redirect_stdout context manager mentioned right after Example 18-3,\nredirect_stderr was added in Python 3.5—it does the same as the former, but for\noutput directed to stderr.\nThe contextlib package also includes:\nclosing\nA function to build context managers out of objects that provide a close()\nmethod but don’t implement the __enter__/__exit__ interface.\nsuppress\nA context manager to temporarily ignore exceptions given as arguments.\nnullcontext\nA context manager that does nothing, to simplify conditional logic around\nobjects that may not implement a suitable context manager. It serves as a stand-\nin when conditional code before the with block may or may not provide a con‐\ntext manager for the with statement—added in Python 3.7.\nThe contextlib module provides classes and a decorator that are more widely appli‐\ncable than the decorators just mentioned:\nContext Managers and with Blocks \n| \n663",
      "content_length": 1778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 694,
      "chapter": null,
      "content": "@contextmanager\nA decorator that lets you build a context manager from a simple generator func‐\ntion, instead of creating a class and implementing the interface. See “Using\n@contextmanager” on page 664.\nAbstractContextManager\nAn ABC that formalizes the context manager interface, and makes it a bit easier\nto create context manager classes by subclassing—added in Python 3.6.\nContextDecorator\nA base class for defining class-based context managers that can also be used as\nfunction decorators, running the entire function within a managed context.\nExitStack\nA context manager that lets you enter a variable number of context managers.\nWhen the with block ends, ExitStack calls the stacked context managers’\n__exit__ methods in LIFO order (last entered, first exited). Use this class when\nyou don’t know beforehand how many context managers you need to enter in\nyour with block; for example, when opening all files from an arbitrary list of files\nat the same time.\nWith Python 3.7, contextlib added AbstractAsyncContextManager, @asynccontext\nmanager, and AsyncExitStack. They are similar to the equivalent utilities without the\nasync part of the name, but designed for use with the new async with statement,\ncovered in Chapter 21.\nThe most widely used of these utilities is the @contextmanager decorator, so it\ndeserves more attention. That decorator is also interesting because it shows a use for\nthe yield statement unrelated to iteration.\nUsing @contextmanager\nThe @contextmanager decorator is an elegant and practical tool that brings together\nthree distinctive Python features: a function decorator, a generator, and the with\nstatement.\nUsing @contextmanager reduces the boilerplate of creating a context manager:\ninstead of writing a whole class with __enter__/__exit__ methods, you just imple‐\nment a generator with a single yield that should produce whatever you want the\n__enter__ method to return.\nIn a generator decorated with @contextmanager, yield splits the body of the function\nin two parts: everything before the yield will be executed at the beginning of the\nwith block when the interpreter calls __enter__; the code after yield will run when\n__exit__ is called at the end of the block.\n664 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 2251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 695,
      "chapter": null,
      "content": "Example 18-5 replaces the LookingGlass class from Example 18-3 with a generator\nfunction.\nExample 18-5. mirror_gen.py: a context manager implemented with a generator\nimport contextlib\nimport sys\n@contextlib.contextmanager  \ndef looking_glass():\n    original_write = sys.stdout.write  \n    def reverse_write(text):  \n        original_write(text[::-1])\n    sys.stdout.write = reverse_write  \n    yield 'JABBERWOCKY'  \n    sys.stdout.write = original_write  \nApply the contextmanager decorator.\nPreserve the original sys.stdout.write method.\nreverse_write can call original_write later because it is available in its closure.\nReplace sys.stdout.write with reverse_write.\nYield the value that will be bound to the target variable in the as clause of the\nwith statement. The generator pauses at this point while the body of the with\nexecutes.\nWhen control exits the with block, execution continues after the yield; here the\noriginal sys.stdout.write is restored.\nExample 18-6 shows the looking_glass function in operation.\nExample 18-6. Test-driving the looking_glass context manager function\n    >>> from mirror_gen import looking_glass\n    >>> with looking_glass() as what:  \n    ...      print('Alice, Kitty and Snowdrop')\n    ...      print(what)\n    ...\n    pordwonS dna yttiK ,ecilA\n    YKCOWREBBAJ\n    >>> what\n    'JABBERWOCKY'\nContext Managers and with Blocks \n| \n665",
      "content_length": 1371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 696,
      "chapter": null,
      "content": "3 The actual class is named _GeneratorContextManager. If you want to see exactly how it works, read its source\ncode in Lib/contextlib.py in Python 3.10.\n    >>> print('back to normal')\n    back to normal\nThe only difference from Example 18-2 is the name of the context manager: look\ning_glass instead of LookingGlass.\nThe contextlib.contextmanager decorator wraps the function in a class that imple‐\nments the __enter__ and __exit__ methods.3\nThe __enter__ method of that class:\n1. Calls the generator function to get a generator object—let’s call it gen.\n2. Calls next(gen) to drive it to the yield keyword.\n3. Returns the value yielded by next(gen), to allow the user to bind it to a variable\nin the with/as form.\nWhen the with block terminates, the __exit__ method:\n1. Checks whether an exception was passed as exc_type; if so, gen.throw(excep\ntion) is invoked, causing the exception to be raised in the yield line inside the\ngenerator function body.\n2. Otherwise, next(gen) is called, resuming the execution of the generator function\nbody after the yield.\nExample 18-5 has a flaw: if an exception is raised in the body of the with block, the\nPython interpreter will catch it and raise it again in the yield expression inside look\ning_glass. But there is no error handling there, so the looking_glass generator will\nterminate without ever restoring the original sys.stdout.write method, leaving the\nsystem in an invalid state.\nExample 18-7 adds special handling of the ZeroDivisionError exception, making it\nfunctionally equivalent to the class-based Example 18-3.\nExample 18-7. mirror_gen_exc.py: generator-based context manager implementing\nexception handling—same external behavior as Example 18-3\nimport contextlib\nimport sys\n@contextlib.contextmanager\n666 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 697,
      "chapter": null,
      "content": "4 This tip is quoted literally from a comment by Leonardo Rochael, one of the tech reviewers for this book.\nNicely said, Leo!\ndef looking_glass():\n    original_write = sys.stdout.write\n    def reverse_write(text):\n        original_write(text[::-1])\n    sys.stdout.write = reverse_write\n    msg = ''  \n    try:\n        yield 'JABBERWOCKY'\n    except ZeroDivisionError:  \n        msg = 'Please DO NOT divide by zero!'\n    finally:\n        sys.stdout.write = original_write  \n        if msg:\n            print(msg)  \nCreate a variable for a possible error message; this is the first change in relation\nto Example 18-5.\nHandle ZeroDivisionError by setting an error message.\nUndo monkey-patching of sys.stdout.write.\nDisplay error message, if it was set.\nRecall that the __exit__ method tells the interpreter that it has handled the exception\nby returning a truthy value; in that case, the interpreter suppresses the exception. On\nthe other hand, if __exit__ does not explicitly return a value, the interpreter gets the\nusual None, and propagates the exception. With @contextmanager, the default behav‐\nior is inverted: the __exit__ method provided by the decorator assumes any excep‐\ntion sent into the generator is handled and should be suppressed.\nHaving a try/finally (or a with block) around the yield is an\nunavoidable price of using @contextmanager, because you never\nknow what the users of your context manager are going to do\ninside the with block.4\nContext Managers and with Blocks \n| \n667",
      "content_length": 1494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 698,
      "chapter": null,
      "content": "5 At least I and the other technical reviewers didn’t know it until Caleb Hattingh told us. Thanks, Caleb!\nA little-known feature of @contextmanager is that the generators decorated with it\ncan also be used as decorators themselves.5 That happens because @contextmanager\nis implemented with the contextlib.ContextDecorator class.\nExample 18-8 shows the looking_glass context manager from Example 18-5 used as\ndecorator.\nExample 18-8. The looking_glass context manager also works as a decorator\n    >>> @looking_glass()\n    ... def verse():\n    ...     print('The time has come')\n    ...\n    >>> verse()  \n    emoc sah emit ehT\n    >>> print('back to normal')  \n    back to normal\nlooking_glass does its job before and after the body of verse runs.\nThis confirms that the original sys.write was restored.\nContrast Example 18-8 with Example 18-6, where looking_glass is used as a context\nmanager.\nAn interesting real-life example of @contextmanager outside of the standard library is\nMartijn Pieters’ in-place file rewriting using a context manager. Example 18-9 shows\nhow it’s used.\nExample 18-9. A context manager for rewriting files in place\nimport csv\nwith inplace(csvfilename, 'r', newline='') as (infh, outfh):\n    reader = csv.reader(infh)\n    writer = csv.writer(outfh)\n    for row in reader:\n        row += ['new', 'columns']\n        writer.writerow(row)\nThe inplace function is a context manager that gives you two handles—infh and\noutfh in the example—to the same file, allowing your code to read and write to it at\n668 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 699,
      "chapter": null,
      "content": "6 People complain about too many parentheses in Lisp, but thoughtful indentation and a good editor mostly\ntake care of that issue. The main readability problem is using the same (f …) notation for function calls and\nspecial forms like (define …), (if …), and (quote …) that don’t behave at all like function calls.\nthe same time. It’s easier to use than the standard library’s fileinput.input function\n(which also provides a context manager, by the way).\nIf you want to study Martijn’s inplace source code (listed in the post), find the yield\nkeyword: everything before it deals with setting up the context, which entails creating\na backup file, then opening and yielding references to the readable and writable file\nhandles that will be returned by the __enter__ call. The __exit__ processing after\nthe yield closes the file handles and restores the file from the backup if something\nwent wrong.\nThis concludes our overview of the with statement and context managers. Let’s turn\nto match/case in the context of a complete example.\nPattern Matching in lis.py: A Case Study\nIn “Pattern Matching Sequences in an Interpreter” on page 43 we saw examples of\nsequence patterns extracted from the evaluate function of Peter Norvig’s lis.py inter‐\npreter, ported to Python 3.10. In this section I want to give a broader overview of how\nlis.py works, and also explore all the case clauses of evaluate, explaining not only the\npatterns but also what the interpreter does in each case.\nBesides showing more pattern matching, I wrote this section for three reasons:\n1. Norvig’s lis.py is a beautiful example of idiomatic Python code.\n2. The simplicity of Scheme is a master class of language design.\n3. Learning how an interpreter works gave me a deeper understanding of Python\nand programming languages in general—interpreted or compiled.\nBefore looking at the Python code, let’s get a little taste of Scheme so you can make\nsense of this case study—in case you haven’t seen Scheme or Lisp before.\nScheme Syntax\nIn Scheme there is no distinction between expressions and statements, like we have in\nPython. Also, there are no infix operators. All expressions use prefix notation like (+\nx 13) instead of x + 13. The same prefix notation is used for function calls—e.g.,\n(gcd x 13)—and special forms—e.g., (define x 13), which we’d write as the\nassignment statement x = 13 in Python. The notation used by Scheme and most Lisp\ndialects is known as S-expression.6\nPattern Matching in lis.py: A Case Study \n| \n669",
      "content_length": 2497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 700,
      "chapter": null,
      "content": "7 To make iteration through recursion practical and efficient, Scheme and other functional languages imple‐\nment proper tail calls. For more about this, see “Soapbox” on page 691.\nExample 18-10 shows a simple example in Scheme.\nExample 18-10. Greatest common divisor in Scheme\n(define (mod m n)\n    (- m (* n (quotient m n))))\n(define (gcd m n)\n    (if (= n 0)\n        m\n        (gcd n (mod m n))))\n(display (gcd 18 45))\nExample 18-10 shows three Scheme expressions: two function definitions—mod and\ngcd—and a call to display, which will output 9, the result of (gcd 18 45).\nExample 18-11 is the same code in Python (shorter than an English explanation of\nthe recursive Euclidean algorithm).\nExample 18-11. Same as Example 18-10, written in Python\ndef mod(m, n):\n    return m - (m // n * n)\ndef gcd(m, n):\n    if n == 0:\n        return m\n    else:\n        return gcd(n, mod(m, n))\nprint(gcd(18, 45))\nIn idiomatic Python, I’d use the % operator instead of reinventing mod, and it would\nbe more efficient to use a while loop instead of recursion. But I wanted to show two\nfunction definitions, and make the examples as similar as possible, to help you read\nthe Scheme code.\nScheme has no iterative control flow commands like while or for. Iteration is done\nwith recursion. Note how there are no assignments in the Scheme and Python exam‐\nples. Extensive use of recursion and minimal use of assignment are hallmarks of pro‐\ngramming in a functional style.7\n670 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 701,
      "chapter": null,
      "content": "8 But Norvig’s second interpreter, lispy.py, supports strings as a data type, as well as advanced features like syn‐\ntactic macros, continuations, and proper tail calls. However, lispy.py is almost three times longer than lis.py—\nand much harder to understand.\nNow let’s review the code of the Python 3.10 version of lis.py. The complete source\ncode with tests is in the 18-with-match/lispy/py3.10/ directory of the GitHub reposi‐\ntory fluentpython/example-code-2e.\nImports and Types\nExample 18-12 shows the first lines of lis.py. The use of TypeAlias and the | type\nunion operator require Python 3.10.\nExample 18-12. lis.py: top of the file\nimport math\nimport operator as op\nfrom collections import ChainMap\nfrom itertools import chain\nfrom typing import Any, TypeAlias, NoReturn\nSymbol: TypeAlias = str\nAtom: TypeAlias = float | int | Symbol\nExpression: TypeAlias = Atom | list\nThe types defined are:\nSymbol\nJust an alias for str. In lis.py, Symbol is used for identifiers; there is no string data\ntype with operations such as slicing, splitting, etc.8\nAtom\nA simple syntactic element, such as a number or a Symbol—as opposed to a com‐\nposite structure made of distinct parts, like a list.\nExpression\nThe building blocks of Scheme programs are expressions made of atoms and\nlists, possibly nested.\nThe Parser\nNorvig’s parser is 36 lines of code showcasing the power of Python applied to han‐\ndling the simple recursive syntax of S-expression—without string data, comments,\nmacros, and other features of standard Scheme that make parsing more complicated\n(Example 18-13).\nPattern Matching in lis.py: A Case Study \n| \n671",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 702,
      "chapter": null,
      "content": "Example 18-13. lis.py: the main parsing functions\ndef parse(program: str) -> Expression:\n    \"Read a Scheme expression from a string.\"\n    return read_from_tokens(tokenize(program))\ndef tokenize(s: str) -> list[str]:\n    \"Convert a string into a list of tokens.\"\n    return s.replace('(', ' ( ').replace(')', ' ) ').split()\ndef read_from_tokens(tokens: list[str]) -> Expression:\n    \"Read an expression from a sequence of tokens.\"\n    # more parsing code omitted in book listing\nThe main function of that group is parse, which takes an S-expression as a str and\nreturns an Expression object, as defined in Example 18-12: an Atom or a list that\nmay contain more atoms and nested lists.\nNorvig uses a smart trick in tokenize: he adds spaces before and after each parenthe‐\nsis in the input and then splits it, resulting in a list of syntactic tokens with '(' and\n')' as separate tokens. This shortcut works because there is no string type in the little\nScheme of lis.py, so every '(' or ')' is an expression delimiter. The recursive parsing\ncode is in read_from_tokens, a 14-line function that you can read in the fluentpy‐\nthon/example-code-2e repository. I will skip it because I want to focus on the other\nparts of the interpreter.\nHere are some doctests extracted from lispy/py3.10/examples_test.py:\n>>> from lis import parse\n>>> parse('1.5')\n1.5\n>>> parse('ni!')\n'ni!'\n>>> parse('(gcd 18 45)')\n['gcd', 18, 45]\n>>> parse('''\n... (define double\n...     (lambda (n)\n...         (* n 2)))\n... ''')\n['define', 'double', ['lambda', ['n'], ['*', 'n', 2]]]\nThe parsing rules for this subset of Scheme are simple:\n1. A token that looks like a number is parsed as a float or int.\n2. Anything else that is not '(' or ')' is parsed as a Symbol—a str to be used as an\nidentifier. This includes source text like +, set!, and make-counter that are valid\nidentifiers in Scheme but not in Python.\n672 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 703,
      "chapter": null,
      "content": "3. Expressions inside '(' and ')' are recursively parsed as lists containing atoms or\nas nested lists that may contain atoms and more nested lists.\nUsing the terminology of the Python interpreter, the output of parse is an AST\n(Abstract Syntax Tree): a convenient representation of the Scheme program as nested\nlists forming a tree-like structure, where the outermost list is the trunk, inner lists are\nthe branches, and atoms are the leaves (Figure 18-1).\nFigure 18-1. A Scheme lambda expression represented as source code (concrete syntax),\nas a tree, and as a sequence of Python objects (abstract syntax).\nThe Environment\nThe Environment class extends collections.ChainMap, adding a change method to\nupdate a value inside one of the chained dicts, which ChainMap instances hold in a list\nof mappings: the self.maps attribute. The change method is needed to support the\nScheme (set! …) form, described later; see Example 18-14.\nExample 18-14. lis.py: the Environment class\nclass Environment(ChainMap[Symbol, Any]):\n    \"A ChainMap that allows changing an item in-place.\"\n    def change(self, key: Symbol, value: Any) -> None:\n        \"Find where key is defined and change the value there.\"\n        for map in self.maps:\n            if key in map:\nPattern Matching in lis.py: A Case Study \n| \n673",
      "content_length": 1297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 704,
      "chapter": null,
      "content": "9 The # type: ignore[index] comment is there because of typeshed issue #6042, which is unresolved as I\nreview this chapter. ChainMap is annotated as MutableMapping, but the type hint in the maps attribute says it’s\na list of Mapping, indirectly making the whole ChainMap immutable as far as Mypy is concerned.\n                map[key] = value  # type: ignore[index]\n                return\n        raise KeyError(key)\nNote that the change method only updates existing keys.9 Trying to change a key that\nis not found raises KeyError.\nThis doctest shows how Environment works:\n>>> from lis import Environment\n>>> inner_env = {'a': 2}\n>>> outer_env = {'a': 0, 'b': 1}\n>>> env = Environment(inner_env, outer_env)\n>>> env['a']  \n2\n>>> env['a'] = 111  \n>>> env['c'] = 222\n>>> env\nEnvironment({'a': 111, 'c': 222}, {'a': 0, 'b': 1})\n>>> env.change('b', 333)  \n>>> env\nEnvironment({'a': 111, 'c': 222}, {'a': 0, 'b': 333})\nWhen reading values, Environment works as ChainMap: keys are searched in the\nnested mappings from left to right. That’s why the value of a in the outer_env is\nshadowed by the value in inner_env.\nAssigning with [] overwrites or inserts new items, but always in the first map‐\nping, inner_env in this example.\nenv.change('b', 333) seeks the 'b' key and assigns a new value to it in-place,\nin the outer_env.\nNext is the standard_env() function, which builds and returns an Environment\nloaded with predefined functions, similar to Python’s __builtins__ module that is\nalways available (Example 18-15).\nExample 18-15. lis.py: standard_env() builds and returns the global environment\ndef standard_env() -> Environment:\n    \"An environment with some Scheme standard procedures.\"\n    env = Environment()\n    env.update(vars(math))   # sin, cos, sqrt, pi, ...\n674 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 705,
      "chapter": null,
      "content": "10 As I studied Norvig’s lis.py and lispy.py, I started a fork named mylis that adds some features, including a\nREPL that accepts partial S-expressions and prompts for the continuation, similar to how Python’s REPL\nknows we are not finished and presents the secondary prompt (...) until we enter a complete expression or\nstatement that can be evaluated. mylis also handles a few errors gracefully, but it’s still easy to crash. It’s not\nnearly as robust as Python’s REPL.\n    env.update({\n            '+': op.add,\n            '-': op.sub,\n            '*': op.mul,\n            '/': op.truediv,\n            # omitted here: more operator definitions\n            'abs': abs,\n            'append': lambda *args: list(chain(*args)),\n            'apply': lambda proc, args: proc(*args),\n            'begin': lambda *x: x[-1],\n            'car': lambda x: x[0],\n            'cdr': lambda x: x[1:],\n            # omitted here: more function definitions\n            'number?': lambda x: isinstance(x, (int, float)),\n            'procedure?': callable,\n            'round': round,\n            'symbol?': lambda x: isinstance(x, Symbol),\n    })\n    return env\nTo summarize, the env mapping is loaded with:\n• All functions from Python’s math module\n• Selected operators from Python’s op module\n• Simple but powerful functions built with Python’s lambda\n• Python built-ins renamed, like callable as procedure?, or directly mapped, like\nround\nThe REPL\nNorvig’s REPL (read-eval-print-loop) is easy to understand but not user-friendly (see\nExample 18-16). If no command-line arguments are given to lis.py, the repl() func‐\ntion is invoked by main()—defined at the end of the module. At the lis.py> prompt,\nwe must enter correct and complete expressions; if we forget to close one parenthesis,\nlis.py crashes.10\nPattern Matching in lis.py: A Case Study \n| \n675",
      "content_length": 1842,
      "extraction_method": "Direct"
    },
    {
      "page_number": 706,
      "chapter": null,
      "content": "Example 18-16. The REPL functions\ndef repl(prompt: str = 'lis.py> ') -> NoReturn:\n    \"A prompt-read-eval-print loop.\"\n    global_env = Environment({}, standard_env())\n    while True:\n        ast = parse(input(prompt))\n        val = evaluate(ast, global_env)\n        if val is not None:\n            print(lispstr(val))\ndef lispstr(exp: object) -> str:\n    \"Convert a Python object back into a Lisp-readable string.\"\n    if isinstance(exp, list):\n        return '(' + ' '.join(map(lispstr, exp)) + ')'\n    else:\n        return str(exp)\nHere is a quick explanation about these two functions:\nrepl(prompt: str = 'lis.py> ') -> NoReturn\nCalls standard_env() to provide built-in functions for the global environment,\nthen enters an infinite loop, reading and parsing each input line, evaluating it in\nthe global environment, and displaying the result—unless it’s None. The\nglobal_env may be modified by evaluate. For example, when a user defines a\nnew global variable or named function, it is stored in the first mapping of the\nenvironment—the empty dict in the Environment constructor call in the first\nline of repl.\nlispstr(exp: object) -> str\nThe inverse function of parse: given a Python object representing an expression,\nparse returns the Scheme source code for it. For example, given ['+', 2, 3],\nthe result is '(+ 2 3)'.\nThe Evaluator\nNow we can appreciate the beauty of Norvig’s expression evaluator—made a little\nprettier with match/case. The evaluate function in Example 18-17 takes an Expres\nsion built by parse and an Environment.\nThe body of evaluate is a single match statement with an expression exp as the sub‐\nject. The case patterns express the syntax and semantics of Scheme with amazing\nclarity.\n676 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 707,
      "chapter": null,
      "content": "Example 18-17. evaluate takes an expression and computes its value\nKEYWORDS = ['quote', 'if', 'lambda', 'define', 'set!']\ndef evaluate(exp: Expression, env: Environment) -> Any:\n    \"Evaluate an expression in an environment.\"\n    match exp:\n        case int(x) | float(x):\n            return x\n        case Symbol(var):\n            return env[var]\n        case ['quote', x]:\n            return x\n        case ['if', test, consequence, alternative]:\n            if evaluate(test, env):\n                return evaluate(consequence, env)\n            else:\n                return evaluate(alternative, env)\n        case ['lambda', [*parms], *body] if body:\n            return Procedure(parms, body, env)\n        case ['define', Symbol(name), value_exp]:\n            env[name] = evaluate(value_exp, env)\n        case ['define', [Symbol(name), *parms], *body] if body:\n            env[name] = Procedure(parms, body, env)\n        case ['set!', Symbol(name), value_exp]:\n            env.change(name, evaluate(value_exp, env))\n        case [func_exp, *args] if func_exp not in KEYWORDS:\n            proc = evaluate(func_exp, env)\n            values = [evaluate(arg, env) for arg in args]\n            return proc(*values)\n        case _:\n            raise SyntaxError(lispstr(exp))\nLet’s study each case clause and what it does. In some cases I added comments show‐\ning an S-expression that would match the pattern when parsed into a Python list.\nDoctests extracted from examples_test.py demonstrate each case.\nEvaluating numbers\n    case int(x) | float(x):\n        return x\nSubject:\nInstance of int or float.\nAction:\nReturn value as is.\nPattern Matching in lis.py: A Case Study \n| \n677",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 708,
      "chapter": null,
      "content": "Example:\n>>> from lis import parse, evaluate, standard_env\n>>> evaluate(parse('1.5'), {})\n1.5\nEvaluating symbols\n    case Symbol(var):\n        return env[var]\nSubject:\nInstance of Symbol, i.e., a str used as an identifier.\nAction:\nLook up var in env and return its value.\nExamples:\n>>> evaluate(parse('+'), standard_env())\n<built-in function add>\n>>> evaluate(parse('ni!'), standard_env())\nTraceback (most recent call last):\n    ...\nKeyError: 'ni!'\n(quote …)\nThe quote special form treats atoms and lists as data instead of expressions to be\nevaluated.\n    # (quote (99 bottles of beer))\n    case ['quote', x]:\n        return x\nSubject:\nList starting with the symbol 'quote', followed by one expression x.\nAction:\nReturn x without evaluating it.\nExamples:\n>>> evaluate(parse('(quote no-such-name)'), standard_env())\n'no-such-name'\n>>> evaluate(parse('(quote (99 bottles of beer))'), standard_env())\n[99, 'bottles', 'of', 'beer']\n>>> evaluate(parse('(quote (/ 10 0))'), standard_env())\n['/', 10, 0]\nWithout quote, each expression in the test would raise an error:\n678 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 709,
      "chapter": null,
      "content": "• no-such-name would be looked up in the environment, raising KeyError\n• (99 bottles of beer) cannot be evaluated because the number 99 is not a\nSymbol naming a special form, operator, or function\n• (/ 10 0) would raise ZeroDivisionError\nWhy Languages Have Reserved Keywords\nAlthough simple, quote cannot be implemented as a function in Scheme. Its special\npower is to prevent the interpreter from evaluating (f 10) in the expression (quote\n(f 10)): the result is simply a list with a Symbol and an int. In contrast, in a function\ncall like (abs (f 10)), the interpreter evaluates (f 10) before invoking abs. That’s\nwhy quote is a reserved keyword: it must be handled as a special form.\nIn general, reserved keywords are needed:\n• To introduce specialized evaluation rules, as in quote and lambda—which don’t\nevaluate any of their subexpressions\n• To change the control flow, as in if and function calls—which also have special\nevaluation rules\n• To manage the environment, as in define and set\nThis is also why Python, and programming languages in general, need reserved key‐\nwords. Think about Python’s def, if, yield, import, del, and what they do.\n(if …)\n    # (if (< x 0) 0 x)\n    case ['if', test, consequence, alternative]:\n        if evaluate(test, env):\n            return evaluate(consequence, env)\n        else:\n            return evaluate(alternative, env)\nSubject:\nList starting with 'if' followed by three expressions: test, consequence, and\nalternative.\nAction:\nEvaluate test:\n• If true, evaluate consequence and return its value.\n• Otherwise, evaluate alternative and return its value.\nPattern Matching in lis.py: A Case Study \n| \n679",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 710,
      "chapter": null,
      "content": "Examples:\n>>> evaluate(parse('(if (= 3 3) 1 0))'), standard_env())\n1\n>>> evaluate(parse('(if (= 3 4) 1 0))'), standard_env())\n0\nThe consequence and alternative branches must be single expressions. If more\nthan one expression is needed in a branch, you can combine them with (begin exp1\nexp2…), provided as a function in lis.py—see Example 18-15.\n(lambda …)\nScheme’s lambda form defines anonymous functions. It doesn’t suffer from the limi‐\ntations of Python’s lambda: any function that can be written in Scheme can be written\nusing the (lambda …) syntax.\n    # (lambda (a b) (/ (+ a b) 2))\n    case ['lambda' [*parms], *body] if body:\n        return Procedure(parms, body, env)\nSubject:\nList starting with 'lambda', followed by:\n• List of zero or more parameter names.\n• One or more expressions collected in body (the guard ensures that body is\nnot empty).\nAction:\nCreate and return a new Procedure instance with the parameter names, the list\nof expressions as the body, and the current environment.\nExample:\n>>> expr = '(lambda (a b) (* (/ a b) 100))'\n>>> f = evaluate(parse(expr), standard_env())\n>>> f  # doctest: +ELLIPSIS\n<lis.Procedure object at 0x...>\n>>> f(15, 20)\n75.0\nThe Procedure class implements the concept of a closure: a callable object holding\nparameter names, a function body, and a reference to the environment in which the\nfunction is defined. We’ll study the code for Procedure in a moment.\n680 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 711,
      "chapter": null,
      "content": "(define …)\nThe define keyword is used in two different syntactic forms. The simplest is:\n    # (define half (/ 1 2))\n    case ['define', Symbol(name), value_exp]:\n        env[name] = evaluate(value_exp, env)\nSubject:\nList starting with 'define', followed by a Symbol and an expression.\nAction:\nEvaluate the expression and put its value into env, using name as key.\nExample:\n>>> global_env = standard_env()\n>>> evaluate(parse('(define answer (* 7 6))'), global_env)\n>>> global_env['answer']\n42\nThe doctest for this case creates a global_env so that we can verify that evaluate\nputs answer into that Environment.\nWe can use that simple define form to create variables or to bind names to anony‐\nmous functions, using (lambda …) as the value_exp.\nStandard Scheme provides a shortcut for defining named functions. That’s the sec‐\nond define form:\n    # (define (average a b) (/ (+ a b) 2))\n    case ['define', [Symbol(name), *parms], *body] if body:\n        env[name] = Procedure(parms, body, env)\nSubject:\nList starting with 'define', followed by:\n• A list starting with a Symbol(name), followed by zero or more items collected\ninto a list named parms.\n• One or more expressions collected in body (the guard ensures that body is\nnot empty).\nAction:\n• Create a new Procedure instance with the parameter names, the list of expres‐\nsions as the body, and the current environment.\n• Put the Procedure into env, using name as key.\nThe doctest in Example 18-18 defines a function named % that computes a percentage\nand adds it to the global_env.\nPattern Matching in lis.py: A Case Study \n| \n681",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 712,
      "chapter": null,
      "content": "11 Assignment is one of the first features taught in many programming tutorials, but set! only appears on page\n220 of the best known Scheme book, Structure and Interpretation of Computer Programs, 2nd ed., by Abelson\net al. (MIT Press), a.k.a. SICP or the “Wizard Book.” Coding in a functional style can take us very far without\nthe state changes that are typical of imperative and object-oriented programming.\nExample 18-18. Defining a function named % that computes a percentage\n>>> global_env = standard_env()\n>>> percent = '(define (% a b) (* (/ a b) 100))'\n>>> evaluate(parse(percent), global_env)\n>>> global_env['%']  # doctest: +ELLIPSIS\n<lis.Procedure object at 0x...>\n>>> global_env['%'](170, 200)\n85.0\nAfter calling evaluate, we check that % is bound to a Procedure that takes two\nnumeric arguments and returns a percentage.\nThe pattern for the second define case does not enforce that the items in parms are\nall Symbol instances. I’d have to check that before building the Procedure, but I\ndidn’t—to keep the code as easy to follow as Norvig’s.\n(set! …)\nThe set! form changes the value of a previously defined variable.11\n    # (set! n (+ n 1))\n    case ['set!', Symbol(name), value_exp]:\n        env.change(name, evaluate(value_exp, env))\nSubject:\nList starting with 'set!', followed by a Symbol and an expression.\nAction:\nUpdate the value of name in env with the result of evaluating the expression.\nThe Environment.change method traverses the chained environments from local to\nglobal, and updates the first occurrence of name with the new value. If we were not\nimplementing the 'set!' keyword, we could use Python’s ChainMap as the Environ\nment type everywhere in this interpreter.\n682 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 713,
      "chapter": null,
      "content": "Python’s nonlocal and Scheme’s set! Address the Same Issue\nThe use of the set! form is related to the use of the nonlocal keyword in Python:\ndeclaring nonlocal x allows x = 10 to update a previously defined x variable outside\nof the local scope. Without a nonlocal x declaration, x = 10 will always create a\nlocal variable in Python, as we saw in “The nonlocal Declaration” on page 315.\nSimilarly, (set! x 10) updates a previously defined x that may be outside of the local\nenvironment of the function. In contrast, the variable x in (define x 10) is always a\nlocal variable, created or updated in the local environment.\nBoth nonlocal and (set! …) are needed to update program state held in variables\nwithin a closure. Example 9-13 demonstrated the use of nonlocal to implement a\nfunction to compute a running average, holding an item count and total in a clo‐\nsure. Here is that same idea, written in the Scheme subset of lis.py:\n(define (make-averager)\n    (define count 0)\n    (define total 0)\n    (lambda (new-value)\n        (set! count (+ count 1))\n        (set! total (+ total new-value))\n        (/ total count)\n    )\n)\n(define avg (make-averager))  \n(avg 10)  \n(avg 11)  \n(avg 15)  \nCreates a new closure with the inner function defined by lambda, and the vari‐\nables count and total initialized to 0; binds the closure to avg.\nReturns 10.0.\nReturns 10.5.\nReturns 12.0.\nThe preceding code is one of the tests in lispy/py3.10/examples_test.py.\nNow we get to a function call.\nPattern Matching in lis.py: A Case Study \n| \n683",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 714,
      "chapter": null,
      "content": "Function call\n    # (gcd (* 2 105) 84)\n    case [func_exp, *args] if func_exp not in KEYWORDS:\n        proc = evaluate(func_exp, env)\n        values = [evaluate(arg, env) for arg in args]\n        return proc(*values)\nSubject:\nList with one or more items.\nThe guard ensures that func_exp is not one of ['quote', 'if', 'define',\n'lambda', 'set!']—listed right before evaluate in Example 18-17.\nThe pattern matches any list with one or more expressions, binding the first\nexpression to func_exp and the rest to args as a list, which may be empty.\nAction:\n• Evaluate func_exp to obtain a function proc.\n• Evaluate each item in args to build a list of argument values.\n• Call proc with the values as separate arguments, returning the result.\nExample:\n>>> evaluate(parse('(% (* 12 14) (- 500 100))'), global_env)\n42.0\nThis doctest continues from Example 18-18: it assumes global_env has a function\nnamed %. The arguments given to % are arithmetic expressions, to emphasize that the\narguments are evaluated before the function is called.\nThe guard in this case is needed because [func_exp, *args] matches any sequence\nsubject with one or more items. However, if func_exp is a keyword, and the subject\ndid not match any previous case, then it is really a syntax error.\nCatch syntax errors\nIf the subject exp does not match any of the previous cases, the catch-all case raises a\nSyntaxError:\n    case _:\n        raise SyntaxError(lispstr(exp))\nHere is an example of a malformed (lambda …) reported as a SyntaxError:\n>>> evaluate(parse('(lambda is not like this)'), standard_env())\nTraceback (most recent call last):\n    ...\nSyntaxError: (lambda is not like this)\n684 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 715,
      "chapter": null,
      "content": "If the case for function call did not have that guard rejecting keywords, the (lambda\nis not like this) expression would be handled as a function call, which would\nraise KeyError because 'lambda' is not part of the environment—just like lambda is\nnot a Python built-in function.\nProcedure: A Class Implementing a Closure\nThe Procedure class could very well be named Closure, because that’s what it repre‐\nsents: a function definition together with an environment. The function definition\nincludes the name of the parameters and the expressions that make up the body of\nthe function. The environment is used when the function is called to provide the val‐\nues of the free variables: variables that appear in the body of the function but are not\nparameters, local variables, or global variables. We saw the concepts of closure and\nfree variable in “Closures” on page 311.\nWe learned how to use closures in Python, but now we can dive deeper and see how a\nclosure is implemented in lis.py:\nclass Procedure:\n    \"A user-defined Scheme procedure.\"\n    def __init__(  \n        self, parms: list[Symbol], body: list[Expression], env: Environment\n    ):\n        self.parms = parms  \n        self.body = body\n        self.env = env\n    def __call__(self, *args: Expression) -> Any:  \n        local_env = dict(zip(self.parms, args))  \n        env = Environment(local_env, self.env)  \n        for exp in self.body:  \n            result = evaluate(exp, env)\n        return result  \nCalled when a function is defined by the lambda or define forms.\nSave the parameter names, body expressions, and environment for later use.\nCalled by proc(*values) in the last line of the case [func_exp, *args] clause.\nBuild local_env mapping self.parms as local variable names, and the given\nargs as values.\nBuild a new combined env, putting local_env first, and then self.env—the\nenvironment that was saved when the function was defined.\nPattern Matching in lis.py: A Case Study \n| \n685",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 716,
      "chapter": null,
      "content": "12 The official Unicode name for λ (U+03BB) is GREEK SMALL LETTER LAMDA. This is not a typo: the char‐\nacter is named “lamda” without the “b” in the Unicode database. According to the English Wikipedia article\n“Lambda”, the Unicode Consortium adopted that spelling because of “preferences expressed by the Greek\nNational Body.”\nIterate over each expression in self.body, evaluating it in the combined env.\nReturn the result of the last expression evaluated.\nThere are a couple of simple functions after evaluate in lis.py: run reads a complete\nScheme program and executes it, and main calls run or repl, depending on the com‐\nmand line—similar to what Python does. I will not describe those functions because\nthere’s nothing new in them. My goals were to share with you the beauty of Norvig’s\nlittle interpreter, to give more insight into how closures work, and to show how\nmatch/case is a great addition to Python.\nTo wrap up this extended section on pattern matching, let’s formalize the concept of\nan OR-pattern.\nUsing OR-patterns\nA series of patterns separated by | is an OR-pattern: it succeeds if any of the subpat‐\nterns succeed. The pattern in “Evaluating numbers” on page 677 is an OR-pattern:\n    case int(x) | float(x):\n        return x\nAll subpatterns in an OR-pattern must use the same variables. This restriction is nec‐\nessary to ensure that the variables are available to the guard expression and the case\nbody, regardless of the subpattern that matched.\nIn the context of a case clause, the | operator has a special mean‐\ning. It does not trigger the __or__ special method, which handles\nexpressions like a | b in other contexts, where it is overloaded to\nperform operations such as set union or integer bitwise-or,\ndepending on the operands.\nAn OR-pattern is not restricted to appear at the top level of a pattern. You can also\nuse | in subpatterns. For example, if we wanted lis.py to accept the Greek letter λ\n(lambda)12 as well as the lambda keyword, we can rewrite the pattern like this:\n    # (λ (a b) (/ (+ a b) 2) )\n    case ['lambda' | 'λ', [*parms], *body] if body:\n        return Procedure(parms, body, env)\n686 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 2184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 717,
      "chapter": null,
      "content": "Now we can move to the third and last subject of this chapter: the unusual places\nwhere an else clause may appear in Python.\nDo This, Then That: else Blocks Beyond if\nThis is no secret, but it is an underappreciated language feature: the else clause can\nbe used not only in if statements but also in for, while, and try statements.\nThe semantics of for/else, while/else, and try/else are closely related, but very\ndifferent from if/else. Initially, the word else actually hindered my understanding\nof these features, but eventually I got used to it.\nHere are the rules:\nfor\nThe else block will run only if and when the for loop runs to completion (i.e.,\nnot if the for is aborted with a break).\nwhile\nThe else block will run only if and when the while loop exits because the condi‐\ntion became falsy (i.e., not if the while is aborted with a break).\ntry\nThe else block will run only if no exception is raised in the try block. The offi‐\ncial docs also state: “Exceptions in the else clause are not handled by the preced‐\ning except clauses.”\nIn all cases, the else clause is also skipped if an exception or a return, break, or\ncontinue statement causes control to jump out of the main block of the compound\nstatement.\nI think else is a very poor choice for the keyword in all cases\nexcept if. It implies an excluding alternative, like, “Run this loop,\notherwise do that,” but the semantics for else in loops is the oppo‐\nsite: “Run this loop, then do that.” This suggests then as a better\nkeyword—which would also make sense in the try context: “Try\nthis, then do that.” However, adding a new keyword is a breaking\nchange to the language—not an easy decision to make.\nUsing else with these statements often makes the code easier to read and saves the\ntrouble of setting up control flags or coding extra if statements.\nThe use of else in loops generally follows the pattern of this snippet:\nfor item in my_list:\n    if item.flavor == 'banana':\nDo This, Then That: else Blocks Beyond if \n| \n687",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 718,
      "chapter": null,
      "content": "break\nelse:\n    raise ValueError('No banana flavor found!')\nIn the case of try/except blocks, else may seem redundant at first. After all, the\nafter_call() in the following snippet will run only if the dangerous_call() does\nnot raise an exception, correct?\ntry:\n    dangerous_call()\n    after_call()\nexcept OSError:\n    log('OSError...')\nHowever, doing so puts the after_call() inside the try block for no good reason.\nFor clarity and correctness, the body of a try block should only have the statements\nthat may generate the expected exceptions. This is better:\ntry:\n    dangerous_call()\nexcept OSError:\n    log('OSError...')\nelse:\n    after_call()\nNow it’s clear that the try block is guarding against possible errors in\ndangerous_call() and not in after_call(). It’s also explicit that after_call() will\nonly execute if no exceptions are raised in the try block.\nIn Python, try/except is commonly used for control flow, and not just for error\nhandling. There’s even an acronym/slogan for that documented in the official Python\nglossary:\nEAFP\nEasier to ask for forgiveness than permission. This common Python coding style\nassumes the existence of valid keys or attributes and catches exceptions if the\nassumption proves false. This clean and fast style is characterized by the presence\nof many try and except statements. The technique contrasts with the LBYL style\ncommon to many other languages such as C.\nThe glossary then defines LBYL:\nLBYL\nLook before you leap. This coding style explicitly tests for pre-conditions before\nmaking calls or lookups. This style contrasts with the EAFP approach and is char‐\nacterized by the presence of many if statements. In a multi-threaded environment,\nthe LBYL approach can risk introducing a race condition between “the looking”\nand “the leaping.” For example, the code, if key in mapping: return mapping[key]\ncan fail if another thread removes key from mapping after the test, but before the\nlookup. This issue can be solved with locks or by using the EAFP approach.\n688 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 2058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 719,
      "chapter": null,
      "content": "13 Watching the discussion in the python-dev mailing list I thought one reason why else was rejected was the\nlack of consensus on how to indent it within match: should else be indented at the same level as match, or at\nthe same level as case?\n14 See slide 21 in “Python is Awesome”.\nGiven the EAFP style, it makes even more sense to know and use else blocks well in\ntry/except statements.\nWhen the match statement was discussed, some people (including\nme) thought it should also have an else clause. In the end it was\ndecided that it wasn’t needed because case _: does the same job.13\nNow let’s summarize the chapter.\nChapter Summary\nThis chapter started with context managers and the meaning of the with statement,\nquickly moving beyond its common use to automatically close opened files. We\nimplemented a custom context manager: the LookingGlass class with the\n__enter__/__exit__ methods, and saw how to handle exceptions in the __exit__\nmethod. A key point that Raymond Hettinger made in his PyCon US 2013 keynote is\nthat with is not just for resource management; it’s a tool for factoring out common\nsetup and teardown code, or any pair of operations that need to be done before and\nafter another procedure.14\nWe reviewed functions in the contextlib standard library module. One of them, the\n@contextmanager decorator, makes it possible to implement a context manager using\na simple generator with one yield—a leaner solution than coding a class with at least\ntwo methods. We reimplemented the LookingGlass as a looking_glass generator\nfunction, and discussed how to do exception handling when using @contextmanager.\nThen we studied Peter Norvig’s elegant lis.py, a Scheme interpreter written in\nidiomatic Python, refactored to use match/case in evaluate—the function at the\ncore of any interpreter. Understanding how evaluate works required reviewing a lit‐\ntle bit of Scheme, a parser for S-expressions, a simple REPL, and the construction of\nnested scopes through an Environment subclass of collection.ChainMap. In the end,\nlis.py became a vehicle to explore much more than pattern matching. It shows how\nthe different parts of an interpreter work together, illuminating core features of\nPython itself: why reserved keywords are necessary, how scoping rules work, and how\nclosures are built and used.\nChapter Summary \n| \n689",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 720,
      "chapter": null,
      "content": "Further Reading\nChapter 8, “Compound Statements,” in The Python Language Reference says pretty\nmuch everything there is to say about else clauses in if, for, while, and try state‐\nments. Regarding Pythonic usage of try/except, with or without else, Raymond\nHettinger has a brilliant answer to the question “Is it a good practice to use try-\nexcept-else in Python?” in StackOverflow. Python in a Nutshell, 3rd ed., by Martelli et\nal., has a chapter about exceptions with an excellent discussion of the EAFP style,\ncrediting computing pioneer Grace Hopper for coining the phrase, “It’s easier to ask\nforgiveness than permission.”\nThe Python Standard Library, Chapter 4, “Built-in Types,” has a section devoted to\n“Context Manager Types”. The __enter__/__exit__ special methods are also docu‐\nmented in The Python Language Reference in “With Statement Context Managers”.\nContext managers were introduced in PEP 343—The “with” Statement.\nRaymond Hettinger highlighted the with statement as a “winning language feature”\nin his PyCon US 2013 keynote. He also showed some interesting applications of con‐\ntext managers in his talk, “Transforming Code into Beautiful, Idiomatic Python”, at\nthe same conference.\nJeff Preshing’s blog post “The Python with Statement by Example” is interesting for\nthe examples using context managers with the pycairo graphics library.\nThe contextlib.ExitStack class is based on an original idea by Nikolaus Rath, who\nwrote a short post explaining why its useful: “On the Beauty of Python’s ExitStack”.\nIn that text, Rath submits that ExitStack is similar but more flexible than the defer\nstatement in Go—which I think is one of the best ideas in that language.\nBeazley and Jones devised context managers for very different purposes in their\nPython Cookbook, 3rd ed. “Recipe 8.3. Making Objects Support the Context-\nManagement Protocol” implements a LazyConnection class whose instances are con‐\ntext managers that open and close network connections automatically in with blocks.\n“Recipe 9.22. Defining Context Managers the Easy Way” introduces a context man‐\nager for timing code, and another for making transactional changes to a list object:\nwithin the with block, a working copy of the list instance is made, and all changes\nare applied to that working copy. Only when the with block completes without an\nexception, the working copy replaces the original list. Simple and ingenious.\nPeter Norvig describes his small Scheme interpreters in the posts “(How to Write a\n(Lisp) Interpreter (in Python))” and “(An ((Even Better) Lisp) Interpreter (in\nPython))”. The code for lis.py and lispy.py is the norvig/pytudes repository. My repos‐\nitory fluentpython/lispy includes the mylis forks of lis.py, updated to Python 3.10, with\na nicer REPL, command-line integration, examples, more tests, and references for\n690 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 2878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 721,
      "chapter": null,
      "content": "learning more about Scheme. The best Scheme dialect and environment to learn and\nexperiment is Racket.\nSoapbox\nFactoring Out the Bread\nIn his PyCon US 2013 keynote, “What Makes Python Awesome”, Raymond Het‐\ntinger says when he first saw the with statement proposal he thought it was “a little\nbit arcane.” Initially, I had a similar reaction. PEPs are often hard to read, and PEP\n343 is typical in that regard.\nThen—Hettinger told us—he had an insight: subroutines are the most important\ninvention in the history of computer languages. If you have sequences of operations\nlike A;B;C and P;B;Q, you can factor out B in a subroutine. It’s like factoring out the\nfilling in a sandwich: using tuna with different breads. But what if you want to factor\nout the bread, to make sandwiches with wheat bread, using a different filling each\ntime? That’s what the with statement offers. It’s the complement of the subroutine.\nHettinger went on to say:\nThe with statement is a very big deal. I encourage you to go out and take this tip of\nthe iceberg and drill deeper. You can probably do profound things with the with\nstatement. The best uses of it have not been discovered yet. I expect that if you make\ngood use of it, it will be copied into other languages and all future languages will have\nit. You can be part of discovering something almost as profound as the invention of\nthe subroutine itself.\nHettinger admits he is overselling the with statement. Nevertheless, it is a very useful\nfeature. When he used the sandwich analogy to explain how with is the complement\nto the subroutine, many possibilities opened up in my mind.\nIf you need to convince anyone that Python is awesome, you should watch Het‐\ntinger’s keynote. The bit about context managers is from 23:00 to 26:15. But the\nentire keynote is excellent.\nEfficient Recursion with Proper Tail Calls\nStandard Scheme implementations are required to provide proper tail calls (PTC), to\nmake iteration through recursion a practical alternative to while loops in imperative\nlanguages. Some writers refer to PTC as tail call optimization (TCO); for others, TCO\nis something different. For more details, see “Tail call” on Wikipedia and “Tail call\noptimization in ECMAScript 6”.\nA tail call is when a function returns the result of a function call, which may be the\nsame function or not. The gcd examples in Example 18-10 and Example 18-11 make\n(recursive) tail calls in the falsy branch of the if.\nOn the other hand, this factorial does not make a tail call:\nFurther Reading \n| \n691",
      "content_length": 2528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 722,
      "chapter": null,
      "content": "def factorial(n):\n    if n < 2:\n       return 1\n    return n * factorial(n - 1)\nThe call to factorial in the last line is not a tail call because the return value is not\nthe result of the recursive call: the result is multiplied by n before it is returned.\nHere is an alternative that uses a tail call, and is therefore tail recursive:\ndef factorial_tc(n, product=1):\n    if n < 1:\n        return product\n    return factorial_tc(n - 1, product * n)\nPython does not have PTC, so there’s no advantage in writing tail recursive functions.\nIn this case, the first version is shorter and more readable in my opinion. For real-life\nuses, don’t forget that Python has math.factorial, written in C without recursion.\nThe point is that, even in languages that implement PTC, it does not benefit every\nrecursive function, only those that are carefully written to make tail calls.\nIf PTC is supported by the language, when the interpreter sees a tail call, it jumps into\nthe body of the called function without creating a new stack frame, saving memory.\nThere are also compiled languages that implement PTC, sometimes as an optimiza‐\ntion that can be toggled.\nThere is no universal consensus about the definition of TCO or the value of PTC in\nlanguages that were not designed as functional languages from the ground up, like\nPython or JavaScript. In functional languages, PTC is an expected feature, not merely\nan optimization that is nice to have. If a language has no iteration mechanism other\nthan recursion, then PTC is necessary for practical usage. Norvig’s lis.py does not\nimplement PTC, but his more elaborate lispy.py interpreter does.\nThe Case Against Proper Tail Calls in Python and JavaScript\nCPython does not implement PTC, and probably never will. Guido van Rossum\nwrote “Final Words on Tail Calls” to explain why. To summarize, here is a key pas‐\nsage from his post:\nPersonally, I think it is a fine feature for some languages, but I don’t think it fits\nPython: the elimination of stack traces for some calls but not others would certainly\nconfuse many users, who have not been raised with tail call religion but might have\nlearned about call semantics by tracing through a few calls in a debugger.\nIn 2015, PTC was included in the ECMAScript 6 standard for JavaScript. As of Octo‐\nber 2021, the interpreter in WebKit implements it. WebKit is used by Safari. The JS\ninterpreters in every other major browser don’t have PTC, and neither does Node.js,\nas it relies on the V8 engine that Google maintains for Chrome. Transpilers and poly‐\nfills targeting JS, like TypeScript, ClojureScript, and Babel, don’t support PTC either,\naccording to this “ECMAScript 6 compatibility table”.\n692 \n| \nChapter 18: with, match, and else Blocks",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 723,
      "chapter": null,
      "content": "I’ve seen several explanations for the rejection of PTC by the implementers, but the\nmost common is the same that Guido van Rossum mentioned: PTC makes debugging\nharder for everyone, while benefiting only a minority of people who’d rather use\nrecursion for iteration. For details, see “What happened to proper tail calls in Java‐\nScript?” by Graham Marlow.\nThere are cases when recursion is the best solution, even in Python without PTC. In a\nprevious post on the subject, Guido wrote:\n[…] a typical Python implementation allows 1000 recursions, which is plenty for\nnon-recursively written code and for code that recourses to traverse, for example, a\ntypical parse tree, but not enough for a recursively written loop over a large list.\nI agree with Guido and the majority of JS implementers: PTC is not a good fit for\nPython or JavaScript. The lack of PTC is the main restriction for writing Python pro‐\ngrams in a functional style—more than the limited lambda syntax.\nIf you are curious to see how PTC works in an interpreter with less features (and less\ncode) than Norvig’s lispy.py, check out mylis_2. The trick starts with the infinite loop\nin evaluate and the code in the case for function calls: that combination makes the\nintepreter jump into the body of the next Procedure without calling evaluate recur‐\nsively during a tail call. Those little interpreters demonstrate the power of abstraction:\neven though Python does not implement PTC, it’s possible and not very hard to write\nan interpreter, in Python, that does implement PTC. I learned how to do it reading\nPeter Norvig’s code. Thanks for sharing it, professor!\nNorvig’s Take on evaluate() with Pattern Matching\nI shared the code for the Python 3.10 version of lis.py with Peter Norvig. He liked the\nexample using pattern matching, but suggested a different solution: instead of the\nguards I wrote, he would have exactly one case per keyword, and have tests within\neach case, to provide more specific SyntaxError messages—for example, when a\nbody is empty. This would also make the guard in case [func_exp, *args] if\nfunc_exp not in KEYWORDS: unnecessary, as every keyword would be handled\nbefore the case for function calls.\nI’ll probably follow Norvig’s advice when I add more functionality to mylis. But the\nway I structured evaluate in Example 18-17 has some didactic advantages for this\nbook: the example parallels the implementation with if/elif/… (Example 2-11), the\ncase clauses demonstrate more features of pattern matching, and the code is more\nconcise.\nFurther Reading \n| \n693",
      "content_length": 2551,
      "extraction_method": "Direct"
    },
    {
      "page_number": 724,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 725,
      "chapter": null,
      "content": "1 Slide 8 of the talk “Concurrency Is Not Parallelism”.\n2 I studied and worked with Prof. Imre Simon, who liked to say there are two major sins in science: using dif‐\nferent words to mean the same thing and using one word to mean different things. Imre Simon (1943–2009)\nwas a pioneer of computer science in Brazil who made seminal contributions to Automata Theory and started\nthe field of Tropical Mathematics. He was also an advocate of free software and free culture.\nCHAPTER 19\nConcurrency Models in Python\nConcurrency is about dealing with lots of things at once.\nParallelism is about doing lots of things at once.\nNot the same, but related.\nOne is about structure, one is about execution.\nConcurrency provides a way to structure a solution to solve a problem that may (but\nnot necessarily) be parallelizable.\n—Rob Pike, co-inventor of the Go language1\nThis chapter is about how to make Python deal with “lots of things at once.” This\nmay involve concurrent or parallel programming—even academics who are keen on\njargon disagree on how to use those terms. I will adopt Rob Pike’s informal defini‐\ntions in this chapter’s epigraph, but note that I’ve found papers and books that claim\nto be about parallel computing but are mostly about concurrency.2\nParallelism is a special case of concurrency, in Pike’s view. All parallel systems are\nconcurrent, but not all concurrent systems are parallel. In the early 2000s we used\nsingle-core machines that handled 100 processes concurrently on GNU Linux. A\nmodern laptop with 4 CPU cores is routinely running more than 200 processes at any\ngiven time under normal, casual use. To execute 200 tasks in parallel, you’d need\n200 cores. So, in practice, most computing is concurrent and not parallel. The OS\n695",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 726,
      "chapter": null,
      "content": "3 This section was suggested by my friend Bruce Eckel—author of books about Kotlin, Scala, Java, and C++.\nmanages hundreds of processes, making sure each has an opportunity to make pro‐\ngress, even if the CPU itself can’t do more than four things at once.\nThis chapter assumes no prior knowledge of concurrent or parallel programming.\nAfter a brief conceptual introduction, we will study simple examples to introduce and\ncompare Python’s core packages for concurrent programming: threading, multi\nprocessing, and asyncio.\nThe last 30% of the chapter is a high-level overview of third-party tools, libraries,\napplication servers, and distributed task queues—all of which can enhance the per‐\nformance and scalability of Python applications. These are all important topics, but\nbeyond the scope of a book focused on core Python language features. Nevertheless, I\nfelt it was important to address these themes in this second edition of Fluent Python,\nbecause Python’s fitness for concurrent and parallel computing is not limited to what\nthe standard library provides. That’s why YouTube, DropBox, Instagram, Reddit, and\nothers were able to achieve web scale when they started, using Python as their pri‐\nmary language—despite persistent claims that “Python doesn’t scale.”\nWhat’s New in This Chapter\nThis chapter is new in the second edition of Fluent Python. The spinner examples in\n“A Concurrent Hello World” on page 701 previously were in the chapter about asyncio.\nHere they are improved, and provide the first illustration of Python’s three\napproaches to concurrency: threads, processes, and native coroutines.\nThe remaining content is new, except for a few paragraphs that originally appeared in\nthe chapters on concurrent.futures and asyncio.\n“Python in the Multicore World” on page 725 is different from the rest of the book:\nthere are no code examples. The goal is to mention important tools that you may\nwant to study to achieve high-performance concurrency and parallelism beyond\nwhat’s possible with Python’s standard library.\nThe Big Picture\nThere are many factors that make concurrent programming hard, but I want to touch\non the most basic factor: starting threads or processes is easy enough, but how do you\nkeep track of them?3\nWhen you call a function, the calling code is blocked until the function returns. So\nyou know when the function is done, and you can easily get the value it returned. If\n696 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 727,
      "chapter": null,
      "content": "the function raises an exception, the calling code can surround the call site with try/\nexcept to catch the error.\nThose familiar options are not available when you start a thread or process: you don’t\nautomatically know when it’s done, and getting back results or errors requires setting\nup some communication channel, such as a message queue.\nAdditionally, starting a thread or a process is not cheap, so you don’t want to start\none of them just to perform a single computation and quit. Often you want to amor‐\ntize the startup cost by making each thread or process into a “worker” that enters a\nloop and stands by for inputs to work on. This further complicates communications\nand introduces more questions. How do you make a worker quit when you don’t\nneed it anymore? And how do you make it quit without interrupting a job partway,\nleaving half-baked data and unreleased resources—like open files? Again the usual\nanswers involve messages and queues.\nA coroutine is cheap to start. If you start a coroutine using the await keyword, it’s\neasy to get a value returned by it, it can be safely cancelled, and you have a clear site\nto catch exceptions. But coroutines are often started by the asynchronous framework,\nand that can make them as hard to monitor as threads or processes.\nFinally, Python coroutines and threads are not suitable for CPU-intensive tasks, as\nwe’ll see.\nThat’s why concurrent programming requires learning new concepts and coding pat‐\nterns. Let’s first make sure we are on the same page regarding some core concepts.\nA Bit of Jargon\nHere are some terms I will use for the rest of this chapter and the next two:\nConcurrency\nThe ability to handle multiple pending tasks, making progress one at a time or in\nparallel (if possible) so that each of them eventually succeeds or fails. A single-\ncore CPU is capable of concurrency if it runs an OS scheduler that interleaves the\nexecution of the pending tasks. Also known as multitasking.\nParallelism\nThe ability to execute multiple computations at the same time. This requires a\nmulticore CPU, multiple CPUs, a GPU, or multiple computers in a cluster.\nExecution unit\nGeneral term for objects that execute code concurrently, each with independent\nstate and call stack. Python natively supports three kinds of execution units: pro‐\ncesses, threads, and coroutines.\nA Bit of Jargon \n| \n697",
      "content_length": 2357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 728,
      "chapter": null,
      "content": "Process\nAn instance of a computer program while it is running, using memory and a slice\nof the CPU time. Modern desktop operating systems routinely manage hundreds\nof processes concurrently, with each process isolated in its own private memory\nspace. Processes communicate via pipes, sockets, or memory mapped files—all of\nwhich can only carry raw bytes. Python objects must be serialized (converted)\ninto raw bytes to pass from one process to another. This is costly, and not all\nPython objects are serializable. A process can spawn subprocesses, each called a\nchild process. These are also isolated from each other and from the parent. Pro‐\ncesses allow preemptive multitasking: the OS scheduler preempts—i.e., suspends\n—each running process periodically to allow other processes to run. This means\nthat a frozen process can’t freeze the whole system—in theory.\nThread\nAn execution unit within a single process. When a process starts, it uses a single\nthread: the main thread. A process can create more threads to operate concur‐\nrently by calling operating system APIs. Threads within a process share the same\nmemory space, which holds live Python objects. This allows easy data sharing\nbetween threads, but can also lead to corrupted data when more than one thread\nupdates the same object concurrently. Like processes, threads also enable pre‐\nemptive multitasking under the supervision of the OS scheduler. A thread con‐\nsumes less resources than a process doing the same job.\nCoroutine\nA function that can suspend itself and resume later. In Python, classic coroutines\nare built from generator functions, and native coroutines are defined with async\ndef. “Classic Coroutines” on page 641 introduced the concept, and Chapter 21\ncovers the use of native coroutines. Python coroutines usually run within a single\nthread under the supervision of an event loop, also in the same thread. Asynchro‐\nnous programming frameworks such as asyncio, Curio, or Trio provide an event\nloop and supporting libraries for nonblocking, coroutine-based I/O. Coroutines\nsupport cooperative multitasking: each coroutine must explicitly cede control\nwith the yield or await keyword, so that another may proceed concurrently (but\nnot in parallel). This means that any blocking code in a coroutine blocks the exe‐\ncution of the event loop and all other coroutines—in contrast with the preemp‐\ntive multitasking supported by processes and threads. On the other hand, each\ncoroutine consumes less resources than a thread or process doing the same job.\nQueue\nA data structure that lets us put and get items, usually in FIFO order: first in, first\nout. Queues allow separate execution units to exchange application data and con‐\ntrol messages, such as error codes and signals to terminate. The implementation\nof a queue varies according to the underlying concurrency model: the queue\n698 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 729,
      "chapter": null,
      "content": "4 Call sys.getswitchinterval() to get the interval; change it with sys.setswitchinterval(s).\npackage in Python’s standard library provides queue classes to support threads,\nwhile the multiprocessing and asyncio packages implement their own queue\nclasses. The queue and asyncio packages also include queues that are not FIFO:\nLifoQueue and PriorityQueue.\nLock\nAn object that execution units can use to synchronize their actions and avoid\ncorrupting data. While updating a shared data structure, the running code\nshould hold an associated lock. This signals other parts of the program to wait\nuntil the lock is released before accessing the same data structure. The simplest\ntype of lock is also known as a mutex (for mutual exclusion). The implementa‐\ntion of a lock depends on the underlying concurrency model.\nContention\nDispute over a limited asset. Resource contention happens when multiple execu‐\ntion units try to access a shared resource—such as a lock or storage. There’s also\nCPU contention, when compute-intensive processes or threads must wait for the\nOS scheduler to give them a share of the CPU time.\nNow let’s use some of that jargon to understand concurrency support in Python.\nProcesses, Threads, and Python’s Infamous GIL\nHere is how the concepts we just saw apply to Python programming, in 10 points:\n1. Each instance of the Python interpreter is a process. You can start additional\nPython processes using the multiprocessing or concurrent.futures libraries.\nPython’s subprocess library is designed to launch processes to run external pro‐\ngrams, regardless of the languages used to write them.\n2. The Python interpreter uses a single thread to run the user’s program and the\nmemory garbage collector. You can start additional Python threads using the\nthreading or concurrent.futures libraries.\n3. Access to object reference counts and other internal interpreter state is con‐\ntrolled by a lock, the Global Interpreter Lock (GIL). Only one Python thread can\nhold the GIL at any time. This means that only one thread can execute Python\ncode at any time, regardless of the number of CPU cores.\n4. To prevent a Python thread from holding the GIL indefinitely, Python’s bytecode\ninterpreter pauses the current Python thread every 5ms by default,4 releasing the\nGIL. The thread can then try to reacquire the GIL, but if there are other threads\nwaiting for it, the OS scheduler may pick one of them to proceed.\nA Bit of Jargon \n| \n699",
      "content_length": 2445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 730,
      "chapter": null,
      "content": "5 A syscall is a call from user code to a function of the operating system kernel. I/O, timers, and locks are some\nof the kernel services available through syscalls. To learn more, read the Wikipedia “System call” article.\n6 The zlib and bz2 modules are specifically mentioned in a python-dev message by Antoine Pitrou, who con‐\ntributed the time-slicing GIL logic to Python 3.2.\n7 Source: slide 106 of Beazley’s “Generators: The Final Frontier” tutorial.\n8 Source: last paragraph of the “Thread objects” section.\n5. When we write Python code, we have no control over the GIL. But a built-in\nfunction or an extension written in C—or any language that interfaces at the\nPython/C API level—can release the GIL while running time-consuming tasks.\n6. Every Python standard library function that makes a syscall5 releases the GIL.\nThis includes all functions that perform disk I/O, network I/O, and\ntime.sleep(). Many CPU-intensive functions in the NumPy/SciPy libraries, as\nwell as the compressing/decompressing functions from the zlib and bz2 mod‐\nules, also release the GIL.6\n7. Extensions that integrate at the Python/C API level can also launch other non-\nPython threads that are not affected by the GIL. Such GIL-free threads generally\ncannot change Python objects, but they can read from and write to the memory\nunderlying objects that support the buffer protocol, such as bytearray,\narray.array, and NumPy arrays.\n8. The effect of the GIL on network programming with Python threads is relatively\nsmall, because the I/O functions release the GIL, and reading or writing to the\nnetwork always implies high latency—compared to reading and writing to mem‐\nory. Consequently, each individual thread spends a lot of time waiting anyway,\nso their execution can be interleaved without major impact on the overall\nthroughput. That’s why David Beazley says: “Python threads are great at doing\nnothing.”7\n9. Contention over the GIL slows down compute-intensive Python threads.\nSequential, single-threaded code is simpler and faster for such tasks.\n10. To run CPU-intensive Python code on multiple cores, you must use multiple\nPython processes.\nHere is a good summary from the threading module documentation:8\nCPython implementation detail: In CPython, due to the Global Interpreter Lock,\nonly one thread can execute Python code at once (even though certain performance-\noriented libraries might overcome this limitation). If you want your application to\nmake better use of the computational resources of multicore machines, you are advised\nto use multiprocessing or concurrent.futures.ProcessPoolExecutor. However,\n700 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 731,
      "chapter": null,
      "content": "9 Unicode has lots of characters useful for simple animations, like the Braille patterns for example. I used the\nASCII characters \"\\|/-\" to keep the examples simple.\nthreading is still an appropriate model if you want to run multiple I/O-bound tasks\nsimultaneously.\nThe previous paragraph starts with “CPython implementation detail” because the\nGIL is not part of the Python language definition. The Jython and IronPython imple‐\nmentations don’t have a GIL. Unfortunately, both are lagging behind—still tracking\nPython 2.7. The highly performant PyPy interpreter also has a GIL in its 2.7 and 3.7\nversions—the latest as of June 2021.\nThis section did not mention coroutines, because by default they\nshare the same Python thread among themselves and with the\nsupervising event loop provided by an asynchronous framework,\ntherefore the GIL does not affect them. It is possible to use multiple\nthreads in an asynchronous program, but the best practice is that\none thread runs the event loop and all coroutines, while additional\nthreads carry out specific tasks. This will be explained in “Delegat‐\ning Tasks to Executors” on page 797.\nEnough concepts for now. Let’s see some code.\nA Concurrent Hello World\nDuring a discussion about threads and how to avoid the GIL, Python contributor\nMichele Simionato posted an example that is like a concurrent “Hello World”: the\nsimplest program to show how Python can “walk and chew gum.”\nSimionato’s program uses multiprocessing, but I adapted it to introduce threading\nand asyncio as well. Let’s start with the threading version, which may look familiar\nif you’ve studied threads in Java or C.\nSpinner with Threads\nThe idea of the next few examples is simple: start a function that blocks for 3 seconds\nwhile animating characters in the terminal to let the user know that the program is\n“thinking” and not stalled.\nThe script makes an animated spinner displaying each character in the string \"\\|/-\"\nin the same screen position.9 When the slow computation finishes, the line with the\nspinner is cleared and the result is shown: Answer: 42.\nA Concurrent Hello World \n| \n701",
      "content_length": 2108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 732,
      "chapter": null,
      "content": "Figure 19-1 shows the output of two versions of the spinning example: first with\nthreads, then with coroutines. If you’re away from the computer, imagine the \\ in the\nlast line is spinning.\nFigure 19-1. The scripts spinner_thread.py and spinner_async.py produce similar out‐\nput: the repr of a spinner object and the text “Answer: 42”. In the screenshot, spin‐\nner_async.py is still running, and the animated message “/ thinking!” is shown; that\nline will be replaced by “Answer: 42” after 3 seconds.\nLet’s review the spinner_thread.py script first. Example 19-1 lists the first two func‐\ntions in the script, and Example 19-2 shows the rest.\nExample 19-1. spinner_thread.py: the spin and slow functions\nimport itertools\nimport time\nfrom threading import Thread, Event\ndef spin(msg: str, done: Event) -> None:  \n    for char in itertools.cycle(r'\\|/-'):  \n        status = f'\\r{char} {msg}'  \n        print(status, end='', flush=True)\n        if done.wait(.1):  \n            break  \n    blanks = ' ' * len(status)\n    print(f'\\r{blanks}\\r', end='')  \ndef slow() -> int:\n    time.sleep(3)  \n    return 42\nThis function will run in a separate thread. The done argument is an instance of\nthreading.Event, a simple object to synchronize threads.\nThis is an infinite loop because itertools.cycle yields one character at a time,\ncycling through the string forever.\nThe trick for text-mode animation: move the cursor back to the start of the line\nwith the carriage return ASCII control character ('\\r').\n702 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 733,
      "chapter": null,
      "content": "The Event.wait(timeout=None) method returns True when the event is set by\nanother thread; if the timeout elapses, it returns False. The .1s timeout sets the\n“frame rate” of the animation to 10 FPS. If you want the spinner to go faster, use\na smaller timeout.\nExit the infinite loop.\nClear the status line by overwriting with spaces and moving the cursor back to\nthe beginning.\nslow() will be called by the main thread. Imagine this is a slow API call over the\nnetwork. Calling sleep blocks the main thread, but the GIL is released so the\nspinner thread can proceed.\nThe first important insight of this example is that time.sleep()\nblocks the calling thread but releases the GIL, allowing other\nPython threads to run.\nThe spin and slow functions will execute concurrently. The main thread—the only\nthread when the program starts—will start a new thread to run spin and then call\nslow. By design, there is no API for terminating a thread in Python. You must send it\na message to shut down.\nThe threading.Event class is Python’s simplest signalling mechanism to coordinate\nthreads. An Event instance has an internal boolean flag that starts as False. Calling\nEvent.set() sets the flag to True. While the flag is false, if a thread calls\nEvent.wait(), it is blocked until another thread calls Event.set(), at which time\nEvent.wait() returns True. If a timeout in seconds is given to Event.wait(s), this\ncall returns False when the timeout elapses, or returns True as soon as Event.set()\nis called by another thread.\nThe supervisor function, listed in Example 19-2, uses an Event to signal the spin\nfunction to exit.\nExample 19-2. spinner_thread.py: the supervisor and main functions\ndef supervisor() -> int:  \n    done = Event()  \n    spinner = Thread(target=spin, args=('thinking!', done))  \n    print(f'spinner object: {spinner}')  \n    spinner.start()  \n    result = slow()  \n    done.set()  \nA Concurrent Hello World \n| \n703",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 734,
      "chapter": null,
      "content": "spinner.join()  \n    return result\ndef main() -> None:\n    result = supervisor()  \n    print(f'Answer: {result}')\nif __name__ == '__main__':\n    main()\nsupervisor will return the result of slow.\nThe threading.Event instance is the key to coordinate the activities of the main\nthread and the spinner thread, as explained further down.\nTo create a new Thread, provide a function as the target keyword argument,\nand positional arguments to the target as a tuple passed via args.\nDisplay the spinner object. The output is <Thread(Thread-1, initial)>, where\ninitial is the state of the thread—meaning it has not started.\nStart the spinner thread.\nCall slow, which blocks the main thread. Meanwhile, the secondary thread is run‐\nning the spinner animation.\nSet the Event flag to True; this will terminate the for loop inside the spin\nfunction.\nWait until the spinner thread finishes.\nRun the supervisor function. I wrote separate main and supervisor functions to\nmake this example look more like the asyncio version in Example 19-4.\nWhen the main thread sets the done event, the spinner thread will eventually notice\nand exit cleanly.\nNow let’s take a look at a similar example using the multiprocessing package.\nSpinner with Processes\nThe multiprocessing package supports running concurrent tasks in separate Python\nprocesses instead of threads. When you create a multiprocessing.Process instance,\na whole new Python interpreter is started as a child process in the background. Since\neach Python process has its own GIL, this allows your program to use all available\n704 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 735,
      "chapter": null,
      "content": "CPU cores—but that ultimately depends on the operating system scheduler. We’ll see\npractical effects in “A Homegrown Process Pool” on page 716, but for this simple pro‐\ngram it makes no real difference.\nThe point of this section is to introduce multiprocessing and show that its API\nemulates the threading API, making it easy to convert simple programs from\nthreads to processes, as shown in spinner_proc.py (Example 19-3).\nExample 19-3. spinner_proc.py: only the changed parts are shown; everything else is the\nsame as spinner_thread.py\nimport itertools\nimport time\nfrom multiprocessing import Process, Event  \nfrom multiprocessing import synchronize     \ndef spin(msg: str, done: synchronize.Event) -> None:  \n# [snip] the rest of spin and slow functions are unchanged from spinner_thread.py\ndef supervisor() -> int:\n    done = Event()\n    spinner = Process(target=spin,               \n                      args=('thinking!', done))\n    print(f'spinner object: {spinner}')          \n    spinner.start()\n    result = slow()\n    done.set()\n    spinner.join()\n    return result\n# [snip] main function is unchanged as well\nThe basic multiprocessing API imitates the threading API, but type hints and\nMypy expose this difference: multiprocessing.Event is a function (not a class\nlike threading.Event) which returns a synchronize.Event instance…\n…forcing us to import multiprocessing.synchronize…\n…to write this type hint.\nBasic usage of the Process class is similar to Thread.\nThe spinner object is displayed as <Process name='Process-1' parent=14868\ninitial>, where 14868 is the process ID of the Python instance running\nspinner_proc.py.\nA Concurrent Hello World \n| \n705",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 736,
      "chapter": null,
      "content": "10 The semaphore is a fundamental building block that can be used to implement other synchronization mecha‐\nnisms. Python provides different semaphore classes for use with threads, processes, and coroutines. We’ll see\nasyncio.Semaphore in “Using asyncio.as_completed and a Thread” on page 788 (Chapter 21).\nThe basic API of threading and multiprocessing are similar, but their implementa‐\ntion is very different, and multiprocessing has a much larger API to handle the\nadded complexity of multiprocess programming. For example, one challenge when\nconverting from threads to processes is how to communicate between processes that\nare isolated by the operating system and can’t share Python objects. This means that\nobjects crossing process boundaries have to be serialized and deserialized, which cre‐\nates overhead. In Example 19-3, the only data that crosses the process boundary is the\nEvent state, which is implemented with a low-level OS semaphore in the C code\nunderlying the multiprocessing module.10\nSince Python 3.8, there’s a multiprocessing.shared_memory pack‐\nage in the standard library, but it does not support instances of\nuser-defined classes. Besides raw bytes, the package allows pro‐\ncesses to share a ShareableList, a mutable sequence type that can\nhold a fixed number of items of types int, float, bool, and None,\nas well as str and bytes up to 10 MB per item. See the ShareableL\nist documentation for more.\nNow let’s see how the same behavior can be achieved with coroutines instead of\nthreads or processes.\nSpinner with Coroutines\nChapter 21 is entirely devoted to asynchronous programming with\ncoroutines. This is just a high-level introduction to contrast this\napproach with the threads and processes concurrency models. As\nsuch, we will overlook many details.\nIt is the job of OS schedulers to allocate CPU time to drive threads and processes. In\ncontrast, coroutines are driven by an application-level event loop that manages a\nqueue of pending coroutines, drives them one by one, monitors events triggered by\nI/O operations initiated by coroutines, and passes control back to the corresponding\ncoroutine when each event happens. The event loop and the library coroutines and\nthe user coroutines all execute in a single thread. Therefore, any time spent in a\ncoroutine slows down the event loop—and all other coroutines.\n706 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 737,
      "chapter": null,
      "content": "The coroutine version of the spinner program is easier to understand if we start from\nthe main function, then study the supervisor. That’s what Example 19-4 shows.\nExample 19-4. spinner_async.py: the main function and supervisor coroutine\ndef main() -> None:  \n    result = asyncio.run(supervisor())  \n    print(f'Answer: {result}')\nasync def supervisor() -> int:  \n    spinner = asyncio.create_task(spin('thinking!'))  \n    print(f'spinner object: {spinner}')  \n    result = await slow()  \n    spinner.cancel()  \n    return result\nif __name__ == '__main__':\n    main()\nmain is the only regular function defined in this program—the others are\ncoroutines.\nThe asyncio.run function starts the event loop to drive the coroutine that will\neventually set the other coroutines in motion. The main function will stay\nblocked until supervisor returns. The return value of supervisor will be the\nreturn value of asyncio.run.\nNative coroutines are defined with async def.\nasyncio.create_task schedules the eventual execution of spin, immediately\nreturning an instance of asyncio.Task.\nThe repr of the spinner object looks like <Task pending name='Task-2'\ncoro=<spin() running at /path/to/spinner_async.py:11>>.\nThe await keyword calls slow, blocking supervisor until slow returns. The\nreturn value of slow will be assigned to result.\nThe Task.cancel method raises a CancelledError exception inside the spin\ncoroutine, as we’ll see in Example 19-5.\nExample 19-4 demonstrates the three main ways of running a coroutine:\nA Concurrent Hello World \n| \n707",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 738,
      "chapter": null,
      "content": "asyncio.run(coro())\nCalled from a regular function to drive a coroutine object that usually is the entry\npoint for all the asynchronous code in the program, like the supervisor in this\nexample. This call blocks until the body of coro returns. The return value of the\nrun() call is whatever the body of coro returns.\nasyncio.create_task(coro())\nCalled from a coroutine to schedule another coroutine to execute eventually.\nThis call does not suspend the current coroutine. It returns a Task instance, an\nobject that wraps the coroutine object and provides methods to control and\nquery its state.\nawait coro()\nCalled from a coroutine to transfer control to the coroutine object returned by\ncoro(). This suspends the current coroutine until the body of coro returns. The\nvalue of the await expression is whatever the body of coro returns.\nRemember: invoking a coroutine as coro() immediately returns a\ncoroutine object, but does not run the body of the coro function.\nDriving the body of coroutines is the job of the event loop.\nNow let’s study the spin and slow coroutines in Example 19-5.\nExample 19-5. spinner_async.py: the spin and slow coroutines\nimport asyncio\nimport itertools\nasync def spin(msg: str) -> None:  \n    for char in itertools.cycle(r'\\|/-'):\n        status = f'\\r{char} {msg}'\n        print(status, flush=True, end='')\n        try:\n            await asyncio.sleep(.1)  \n        except asyncio.CancelledError:  \n            break\n    blanks = ' ' * len(status)\n    print(f'\\r{blanks}\\r', end='')\nasync def slow() -> int:\n    await asyncio.sleep(3)  \n    return 42\n708 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 739,
      "chapter": null,
      "content": "We don’t need the Event argument that was used to signal that slow had comple‐\nted its job in spinner_thread.py (Example 19-1).\nUse await asyncio.sleep(.1) instead of time.sleep(.1), to pause without\nblocking other coroutines. See the experiment after this example.\nasyncio.CancelledError is raised when the cancel method is called on the\nTask controlling this coroutine. Time to exit the loop.\nThe slow coroutine also uses await asyncio.sleep instead of time.sleep.\nExperiment: Break the spinner for an insight\nHere is an experiment I recommend to understand how spinner_async.py works.\nImport the time module, then go to the slow coroutine and replace the line await\nasyncio.sleep(3) with a call to time.sleep(3), like in Example 19-6.\nExample 19-6. spinner_async.py: replacing await asyncio.sleep(3) with\ntime.sleep(3)\nasync def slow() -> int:\n    time.sleep(3)\n    return 42\nWatching the behavior is more memorable than reading about it. Go ahead, I’ll wait.\nWhen you run the experiment, this is what you see:\n1. The spinner object is shown, similar to this: <Task pending name='Task-2'\ncoro=<spin() running at /path/to/spinner_async.py:12>>.\n2. The spinner never appears. The program hangs for 3 seconds.\n3. Answer: 42 is displayed and the program ends.\nTo understand what is happening, recall that Python code using asyncio has only\none flow of execution, unless you’ve explicitly started additional threads or processes.\nThat means only one coroutine executes at any point in time. Concurrency is\nachieved by control passing from one coroutine to another. In Example 19-7, let’s\nfocus on what happens in the supervisor and slow coroutines during the proposed\nexperiment.\nExample 19-7. spinner_async_experiment.py: the supervisor and slow coroutines\nasync def slow() -> int:\n    time.sleep(3)  \nA Concurrent Hello World \n| \n709",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 740,
      "chapter": null,
      "content": "11 Thanks to tech reviewers Caleb Hattingh and Jürgen Gmach who did not let me overlook greenlet and gevent.\n    return 42\nasync def supervisor() -> int:\n    spinner = asyncio.create_task(spin('thinking!'))  \n    print(f'spinner object: {spinner}')  \n    result = await slow()  \n    spinner.cancel()  \n    return result\nThe spinner task is created, to eventually drive the execution of spin.\nThe display shows the Task is “pending.”\nThe await expression transfers control to the slow coroutine.\ntime.sleep(3) blocks for 3 seconds; nothing else can happen in the program,\nbecause the main thread is blocked—and it is the only thread. The operating sys‐\ntem will continue with other activities. After 3 seconds, sleep unblocks, and\nslow returns.\nRight after slow returns, the spinner task is cancelled. The flow of control never\nreached the body of the spin coroutine.\nThe spinner_async_experiment.py teaches an important lesson, as explained in the\nfollowing warning.\nNever use time.sleep(…) in asyncio coroutines unless you want\nto pause your whole program. If a coroutine needs to spend some\ntime doing nothing, it should await asyncio.sleep(DELAY). This\nyields control back to the asyncio event loop, which can drive\nother pending coroutines.\nGreenlet and gevent\nAs we discuss concurrency with coroutines, it’s important to mention the greenlet\npackage, which has been around for many years and is used at scale.11 The package\nsupports cooperative multitasking through lightweight coroutines—named greenlets\n—that don’t require any special syntax such as yield or await, therefore are easier to\nintegrate into existing, sequential codebases. SQL Alchemy 1.4 ORM uses greenlets\ninternally to implement its new asynchronous API compatible with asyncio.\n710 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 741,
      "chapter": null,
      "content": "The gevent networking library monkey patches Python’s standard socket module\nmaking it nonblocking by replacing some of its code with greenlets. To a large extent,\ngevent is transparent to the surrounding code, making it easier to adapt sequential\napplications and libraries—such as database drivers—to perform concurrent network\nI/O. Numerous open source projects use gevent, including the widely deployed Guni‐\ncorn—mentioned in “WSGI Application Servers” on page 730.\nSupervisors Side-by-Side\nThe line count of spinner_thread.py and spinner_async.py is nearly the same. The\nsupervisor functions are the heart of these examples. Let’s compare them in detail.\nExample 19-8 lists only the supervisor from Example 19-2.\nExample 19-8. spinner_thread.py: the threaded supervisor function\ndef supervisor() -> int:\n    done = Event()\n    spinner = Thread(target=spin,\n                     args=('thinking!', done))\n    print('spinner object:', spinner)\n    spinner.start()\n    result = slow()\n    done.set()\n    spinner.join()\n    return result\nFor comparison, Example 19-9 shows the supervisor coroutine from Example 19-4.\nExample 19-9. spinner_async.py: the asynchronous supervisor coroutine\nasync def supervisor() -> int:\n    spinner = asyncio.create_task(spin('thinking!'))\n    print('spinner object:', spinner)\n    result = await slow()\n    spinner.cancel()\n    return result\nHere is a summary of the differences and similarities to note between the two super\nvisor implementations:\n• An asyncio.Task is roughly the equivalent of a threading.Thread.\n• A Task drives a coroutine object, and a Thread invokes a callable.\n• A coroutine yields control explicitly with the await keyword.\nA Concurrent Hello World \n| \n711",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 742,
      "chapter": null,
      "content": "• You don’t instantiate Task objects yourself, you get them by passing a coroutine\nto asyncio.create_task(…).\n• When asyncio.create_task(…) returns a Task object, it is already scheduled to\nrun, but a Thread instance must be explicitly told to run by calling its start\nmethod.\n• In the threaded supervisor, slow is a plain function and is directly invoked by\nthe main thread. In the asynchronous supervisor, slow is a coroutine driven by\nawait.\n• There’s no API to terminate a thread from the outside; instead, you must send a\nsignal—like setting the done Event object. For tasks, there is the Task.cancel()\ninstance method, which raises CancelledError at the await expression where\nthe coroutine body is currently suspended.\n• The supervisor coroutine must be started with asyncio.run in the main\nfunction.\nThis comparison should help you understand how concurrent jobs are orchestrated\nwith asyncio, in contrast to how it’s done with the Threading module, which may be\nmore familiar to you.\nOne final point related to threads versus coroutines: if you’ve done any nontrivial\nprogramming with threads, you know how challenging it is to reason about the pro‐\ngram because the scheduler can interrupt a thread at any time. You must remember\nto hold locks to protect the critical sections of your program, to avoid getting inter‐\nrupted in the middle of a multistep operation—which could leave data in an invalid\nstate.\nWith coroutines, your code is protected against interruption by default. You must\nexplicitly await to let the rest of the program run. Instead of holding locks to syn‐\nchronize the operations of multiple threads, coroutines are “synchronized” by defini‐\ntion: only one of them is running at any time. When you want to give up control, you\nuse await to yield control back to the scheduler. That’s why it is possible to safely\ncancel a coroutine: by definition, a coroutine can only be cancelled when it’s sus‐\npended at an await expression, so you can perform cleanup by handling the Cancel\nledError exception.\nThe time.sleep() call blocks but does nothing. Now we’ll experiment with a CPU-\nintensive call to get a better understanding of the GIL, as well as the effect of CPU-\nintensive functions in asynchronous code.\n712 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 743,
      "chapter": null,
      "content": "12 It’s a 15” MacBook Pro 2018 with a 6-core, 2.2 GHz Intel Core i7 CPU.\nThe Real Impact of the GIL\nIn the threading code (Example 19-1), you can replace the time.sleep(3) call in the\nslow function with an HTTP client request from your favorite library, and the spin‐\nner will keep spinning. That’s because a well-designed network library will release the\nGIL while waiting for the network.\nYou can also replace the asyncio.sleep(3) expression in the slow coroutine to\nawait for a response from a well-designed asynchronous network library, because\nsuch libraries provide coroutines that yield control back to the event loop while wait‐\ning for the network. Meanwhile, the spinner will keep spinning.\nWith CPU-intensive code, the story is different. Consider the function is_prime in\nExample 19-10, which returns True if the argument is a prime number, False if it’s\nnot.\nExample 19-10. primes.py: an easy to read primality check, from Python’s ProcessPool\nExecutor example\ndef is_prime(n: int) -> bool:\n    if n < 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    root = math.isqrt(n)\n    for i in range(3, root + 1, 2):\n        if n % i == 0:\n            return False\n    return True\nThe call is_prime(5_000_111_000_222_021) takes about 3.3s on the company laptop\nI am using now.12\nQuick Quiz\nGiven what we’ve seen so far, please take the time to consider the following three-\npart question. One part of the answer is tricky (at least it was for me).\nWhat would happen to the spinner animation if you made the following changes,\nassuming that n = 5_000_111_000_222_021—that prime which my machine takes 3.3s\nto verify:\nThe Real Impact of the GIL \n| \n713",
      "content_length": 1708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 744,
      "chapter": null,
      "content": "13 This is true today because you are probably using a modern OS with preemptive multitasking. Windows\nbefore the NT era and macOS before the OSX era were not “preemptive,” therefore any process could take\nover 100% of the CPU and freeze the whole system. We are not completely free of this kind of problem today\nbut trust this graybeard: this troubled every user in the 1990s, and a hard reset was the only cure.\n1. In spinner_proc.py, replace time.sleep(3) with a call to is_prime(n)?\n2. In spinner_thread.py, replace time.sleep(3) with a call to is_prime(n)?\n3. In spinner_async.py, replace await asyncio.sleep(3) with a call to\nis_prime(n)?\nBefore you run the code or read on, I recommend figuring out the answers on your\nown. Then, you may want to copy and modify the spinner_*.py examples as\nsuggested.\nNow the answers, from easier to hardest.\n1. Answer for multiprocessing\nThe spinner is controlled by a child process, so it continues spinning while the pri‐\nmality test is computed by the parent process.13\n2. Answer for threading\nThe spinner is controlled by a secondary thread, so it continues spinning while the\nprimality test is computed by the main thread.\nI did not get this answer right at first: I was expecting the spinner to freeze because I\noverestimated the impact of the GIL.\nIn this particular example, the spinner keeps spinning because Python suspends the\nrunning thread every 5ms (by default), making the GIL available to other pending\nthreads. Therefore, the main thread running is_prime is interrupted every 5ms,\nallowing the secondary thread to wake up and iterate once through the for loop, until\nit calls the wait method of the done event, at which time it will release the GIL. The\nmain thread will then grab the GIL, and the is_prime computation will proceed for\nanother 5ms.\nThis does not have a visible impact on the running time of this specific example,\nbecause the spin function quickly iterates once and releases the GIL as it waits for the\ndone event, so there is not much contention for the GIL. The main thread running\nis_prime will have the GIL most of the time.\nWe got away with a compute-intensive task using threading in this simple experi‐\nment because there are only two threads: one hogging the CPU, and the other waking\nup only 10 times per second to update the spinner.\n714 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 745,
      "chapter": null,
      "content": "But if you have two or more threads vying for a lot of CPU time, your program will\nbe slower than sequential code.\n3. Answer for asyncio\nIf you call is_prime(5_000_111_000_222_021) in the slow coroutine of the spin‐\nner_async.py example, the spinner will never appear. The effect would be the same\nwe had in Example 19-6, when we replaced await asyncio.sleep(3) with\ntime.sleep(3): no spinning at all. The flow of control will pass from supervisor to\nslow, and then to is_prime. When is_prime returns, slow returns as well, and super\nvisor resumes, cancelling the spinner task before it is executed even once. The pro‐\ngram appears frozen for about 3s, then shows the answer.\nPower Napping with sleep(0)\nOne way to keep the spinner alive is to rewrite is_prime as a coroutine, and periodi‐\ncally call asyncio.sleep(0) in an await expression to yield control back to the event\nloop, like in Example 19-11.\nExample 19-11. spinner_async_nap.py: is_prime is now a coroutine\nasync def is_prime(n):\n    if n < 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    root = math.isqrt(n)\n    for i in range(3, root + 1, 2):\n        if n % i == 0:\n            return False\n        if i % 100_000 == 1:\n            await asyncio.sleep(0)  \n    return True\nSleep once every 50,000 iterations (because the step in the range is 2).\nIssue #284 in the asyncio repository has an informative discussion about the use of\nasyncio.sleep(0).\nHowever, be aware this will slow down is_prime, and—more importantly—will still\nslow down the event loop and your whole program with it. When I used await asyn\ncio.sleep(0) every 100,000 iterations, the spinner was smooth but the program ran\nin 4.9s on my machine, almost 50% longer than the original primes.is_prime func‐\ntion by itself with the same argument (5_000_111_000_222_021).\nThe Real Impact of the GIL \n| \n715",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 746,
      "chapter": null,
      "content": "Using await asyncio.sleep(0) should be considered a stopgap measure before you\nrefactor your asynchronous code to delegate CPU-intensive computations to another\nprocess. We’ll see one way of doing that with asyncio.loop.run_in_executor, cov‐\nered in Chapter 21. Another option would be a task queue, which we’ll briefly discuss\nin “Distributed Task Queues” on page 732.\nSo far, we’ve only experimented with a single call to a CPU-intensive function. The\nnext section presents concurrent execution of multiple CPU-intensive calls.\nA Homegrown Process Pool\nI wrote this section to show the use of multiple processes for CPU-\nintensive tasks, and the common pattern of using queues to distrib‐\nute tasks and collect results. Chapter 20 will show a simpler way of\ndistributing tasks to processes: a ProcessPoolExecutor from the\nconcurrent.futures package, which uses queues internally.\nIn this section we’ll write programs to compute the primality of a sample of 20 inte‐\ngers, from 2 to 9,999,999,999,999,999—i.e., 1016 – 1, or more than 253. The sample\nincludes small and large primes, as well as composite numbers with small and large\nprime factors.\nThe sequential.py program provides the performance baseline. Here is a sample run:\n$ python3 sequential.py\n               2  P  0.000001s\n 142702110479723  P  0.568328s\n 299593572317531  P  0.796773s\n3333333333333301  P  2.648625s\n3333333333333333     0.000007s\n3333335652092209     2.672323s\n4444444444444423  P  3.052667s\n4444444444444444     0.000001s\n4444444488888889     3.061083s\n5555553133149889     3.451833s\n5555555555555503  P  3.556867s\n5555555555555555     0.000007s\n6666666666666666     0.000001s\n6666666666666719  P  3.781064s\n6666667141414921     3.778166s\n7777777536340681     4.120069s\n7777777777777753  P  4.141530s\n7777777777777777     0.000007s\n9999999999999917  P  4.678164s\n9999999999999999     0.000007s\nTotal time: 40.31\n716 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 747,
      "chapter": null,
      "content": "The results are shown in three columns:\n• The number to be checked.\n• P if it’s a prime number, blank if not.\n• Elapsed time for checking the primality for that specific number.\nIn this example, the total time is approximately the sum of the times for each check,\nbut it is computed separately, as you can see in Example 19-12.\nExample 19-12. sequential.py: sequential primality check for a small dataset\n#!/usr/bin/env python3\n\"\"\"\nsequential.py: baseline for comparing sequential, multiprocessing,\nand threading code for CPU-intensive work.\n\"\"\"\nfrom time import perf_counter\nfrom typing import NamedTuple\nfrom primes import is_prime, NUMBERS\nclass Result(NamedTuple):  \n    prime: bool\n    elapsed: float\ndef check(n: int) -> Result:  \n    t0 = perf_counter()\n    prime = is_prime(n)\n    return Result(prime, perf_counter() - t0)\ndef main() -> None:\n    print(f'Checking {len(NUMBERS)} numbers sequentially:')\n    t0 = perf_counter()\n    for n in NUMBERS:  \n        prime, elapsed = check(n)\n        label = 'P' if prime else ' '\n        print(f'{n:16}  {label} {elapsed:9.6f}s')\n    elapsed = perf_counter() - t0  \n    print(f'Total time: {elapsed:.2f}s')\nif __name__ == '__main__':\n    main()\nA Homegrown Process Pool \n| \n717",
      "content_length": 1228,
      "extraction_method": "Direct"
    },
    {
      "page_number": 748,
      "chapter": null,
      "content": "The check function (in the next callout) returns a Result tuple with the boolean\nvalue of the is_prime call and the elapsed time.\ncheck(n) calls is_prime(n) and computes the elapsed time to return a Result.\nFor each number in the sample, we call check and display the result.\nCompute and display the total elapsed time.\nProcess-Based Solution\nThe next example, procs.py, shows the use of multiple processes to distribute the pri‐\nmality checks across multiple CPU cores. These are the times I get with procs.py:\n$ python3 procs.py\nChecking 20 numbers with 12 processes:\n               2  P  0.000002s\n3333333333333333     0.000021s\n4444444444444444     0.000002s\n5555555555555555     0.000018s\n6666666666666666     0.000002s\n 142702110479723  P  1.350982s\n7777777777777777     0.000009s\n 299593572317531  P  1.981411s\n9999999999999999     0.000008s\n3333333333333301  P  6.328173s\n3333335652092209     6.419249s\n4444444488888889     7.051267s\n4444444444444423  P  7.122004s\n5555553133149889     7.412735s\n5555555555555503  P  7.603327s\n6666666666666719  P  7.934670s\n6666667141414921     8.017599s\n7777777536340681     8.339623s\n7777777777777753  P  8.388859s\n9999999999999917  P  8.117313s\n20 checks in 9.58s\nThe last line of the output shows that procs.py was 4.2 times faster than sequential.py.\nUnderstanding the Elapsed Times\nNote that the elapsed time in the first column is for checking that specific number.\nFor example, is_prime(7777777777777753) took almost 8.4s to return True. Mean‐\nwhile, other processes were checking other numbers in parallel.\nThere were 20 numbers to check. I wrote procs.py to start a number of worker pro‐\ncesses equal to the number of CPU cores, as determined by multiprocess\ning.cpu_count().\n718 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 749,
      "chapter": null,
      "content": "The total time in this case is much less than the sum of the elapsed time for the indi‐\nvidual checks. There is some overhead in spinning up processes and in inter-process\ncommunication, so the end result is that the multiprocess version is only about 4.2\ntimes faster than the sequential. That’s good, but a little disappointing considering\nthe code launches 12 processes to use all cores on this laptop.\nThe multiprocessing.cpu_count() function returns 12 on the\nMacBook Pro I’m using to write this chapter. It’s actually a 6-CPU\nCore-i7, but the OS reports 12 CPUs because of hyperthreading, an\nIntel technology which executes 2 threads per core. However,\nhyperthreading works better when one of the threads is not work‐\ning as hard as the other thread in the same core—perhaps the first\nis stalled waiting for data after a cache miss, and the other is\ncrunching numbers. Anyway, there’s no free lunch: this laptop per‐\nforms like a 6-CPU machine for compute-intensive work that\ndoesn’t use a lot of memory—like that simple primality test.\nCode for the Multicore Prime Checker\nWhen we delegate computing to threads or processes, our code does not call the\nworker function directly, so we can’t simply get a return value. Instead, the worker is\ndriven by the thread or process library, and it eventually produces a result that needs\nto be stored somewhere. Coordinating workers and collecting results are common\nuses of queues in concurrent programming—and also in distributed systems.\nMuch of the new code in procs.py has to do with setting up and using queues. The top\nof the file is in Example 19-13.\nSimpleQueue was added to multiprocessing in Python 3.9. If\nyou’re using an earlier version of Python, you can replace Simple\nQueue with Queue in Example 19-13.\nExample 19-13. procs.py: multiprocess primality check; imports, types, and functions\nimport sys\nfrom time import perf_counter\nfrom typing import NamedTuple\nfrom multiprocessing import Process, SimpleQueue, cpu_count  \nfrom multiprocessing import queues  \nfrom primes import is_prime, NUMBERS\nclass PrimeResult(NamedTuple):  \n    n: int\nA Homegrown Process Pool \n| \n719",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 750,
      "chapter": null,
      "content": "prime: bool\n    elapsed: float\nJobQueue = queues.SimpleQueue[int]  \nResultQueue = queues.SimpleQueue[PrimeResult]  \ndef check(n: int) -> PrimeResult:  \n    t0 = perf_counter()\n    res = is_prime(n)\n    return PrimeResult(n, res, perf_counter() - t0)\ndef worker(jobs: JobQueue, results: ResultQueue) -> None:  \n    while n := jobs.get():  \n        results.put(check(n))  \n    results.put(PrimeResult(0, False, 0.0))  \ndef start_jobs(\n    procs: int, jobs: JobQueue, results: ResultQueue  \n) -> None:\n    for n in NUMBERS:\n        jobs.put(n)  \n    for _ in range(procs):\n        proc = Process(target=worker, args=(jobs, results))  \n        proc.start()  \n        jobs.put(0)  \nTrying to emulate threading, multiprocessing provides multiprocessing.Sim\npleQueue, but this is a method bound to a predefined instance of a lower-level\nBaseContext class. We must call this SimpleQueue to build a queue, we can’t use\nit in type hints.\nmultiprocessing.queues has the SimpleQueue class we need for type hints.\nPrimeResult includes the number checked for primality. Keeping n together\nwith the other result fields simplifies displaying results later.\nThis is a type alias for a SimpleQueue that the main function (Example 19-14) will\nuse to send numbers to the processes that will do the work.\nType alias for a second SimpleQueue that will collect the results in main. The val‐\nues in the queue will be tuples made of the number to be tested for primality, and\na Result tuple.\nThis is similar to sequential.py.\nworker gets a queue with the numbers to be checked, and another to put results.\n720 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 751,
      "chapter": null,
      "content": "14 In this example, 0 is a convenient sentinel. None is also commonly used for that. Using 0 simplifies the type\nhint for PrimeResult and the code for worker.\n15 Surviving serialization without losing our identity is a pretty good life goal.\nIn this code, I use the number 0 as a poison pill: a signal for the worker to finish.\nIf n is not 0, proceed with the loop.14\nInvoke the primality check and enqueue PrimeResult.\nSend back a PrimeResult(0, False, 0.0) to let the main loop know that this\nworker is done.\nprocs is the number of processes that will compute the prime checks in parallel.\nEnqueue the numbers to be checked in jobs.\nFork a child process for each worker. Each child will run the loop inside its own\ninstance of the worker function, until it fetches a 0 from the jobs queue.\nStart each child process.\nEnqueue one 0 for each process, to terminate them.\nLoops, Sentinels, and Poison Pills\nThe worker function in Example 19-13 follows a common pattern in concurrent pro‐\ngramming: looping indefinitely while taking items from a queue and processing each\nwith a function that does the actual work. The loop ends when the queue produces a\nsentinel value. In this pattern, the sentinel that shuts down the worker is often called a\n“poison pill.”\nNone is often used as a sentinel value, but it may be unsuitable if it can occur in the\ndata stream. Calling object() is a common way to get a unique value to use as senti‐\nnel. However, that does not work across processes because Python objects must be\nserialized for inter-process communication, and when you pickle.dump and\npickle.load an instance of object, the unpickled instance is distinct from the origi‐\nnal: it doesn’t compare equal. A good alternative to None is the Ellipsis built-in\nobject (a.k.a. ...), which survives serialization without losing its identity.15\nPython’s standard library uses lots of different values as sentinels. PEP 661—Sentinel\nValues proposes a standard sentinel type. As of September 2021, it’s only a draft.\nA Homegrown Process Pool \n| \n721",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 752,
      "chapter": null,
      "content": "Now let’s study the main function of procs.py in Example 19-14.\nExample 19-14. procs.py: multiprocess primality check; main function\ndef main() -> None:\n    if len(sys.argv) < 2:  \n        procs = cpu_count()\n    else:\n        procs = int(sys.argv[1])\n    print(f'Checking {len(NUMBERS)} numbers with {procs} processes:')\n    t0 = perf_counter()\n    jobs: JobQueue = SimpleQueue()  \n    results: ResultQueue = SimpleQueue()\n    start_jobs(procs, jobs, results)  \n    checked = report(procs, results)  \n    elapsed = perf_counter() - t0\n    print(f'{checked} checks in {elapsed:.2f}s')  \ndef report(procs: int, results: ResultQueue) -> int: \n    checked = 0\n    procs_done = 0\n    while procs_done < procs:  \n        n, prime, elapsed = results.get()  \n        if n == 0:  \n            procs_done += 1\n        else:\n            checked += 1  \n            label = 'P' if prime else ' '\n            print(f'{n:16}  {label} {elapsed:9.6f}s')\n    return checked\nif __name__ == '__main__':\n    main()\nIf no command-line argument is given, set the number of processes to the num‐\nber of CPU cores; otherwise, create as many processes as given in the first\nargument.\njobs and results are the queues described in Example 19-13.\nStart proc processes to consume jobs and post results.\nRetrieve the results and display them; report is defined in .\nDisplay how many numbers were checked and the total elapsed time.\nThe arguments are the number of procs and the queue to post the results.\n722 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 753,
      "chapter": null,
      "content": "Loop until all processes are done.\nGet one PrimeResult. Calling .get() on a queue block until there is an item in\nthe queue. It’s also possible to make this nonblocking, or set a timeout. See the\nSimpleQueue.get documentation for details.\nIf n is zero, then one process exited; increment the procs_done count.\nOtherwise, increment the checked count (to keep track of the numbers checked)\nand display the results.\nThe results will not come back in the same order the jobs were submitted. That’s why\nI had to put n in each PrimeResult tuple. Otherwise, I’d have no way to know which\nresult belonged to each number.\nIf the main process exits before all subprocesses are done, you may see confusing\ntracebacks on FileNotFoundError exceptions caused by an internal lock in multi\nprocessing. Debugging concurrent code is always hard, and debugging multiproc\nessing is even harder because of all the complexity behind the thread-like façade.\nFortunately, the ProcessPoolExecutor we’ll meet in Chapter 20 is easier to use and\nmore robust.\nThanks to reader Michael Albert who noticed the code I published\nduring the early release had a race condition in Example 19-14. A\nrace condition is a bug that may or may not occur depending on\nthe order of actions performed by concurrent execution units. If\n“A” happens before “B,” all is fine; but it “B” happens first, some‐\nthing goes wrong. That’s the race.\nIf you are curious, this diff shows the bug and how I fixed it:\nexample-code-2e/commit/2c123057—but note that I later refactored\nthe example to delegate parts of main to the start_jobs and\nreport functions. There’s a README.md file in the same directory\nexplaining the problem and the solution.\nExperimenting with More or Fewer Processes\nYou may want try running procs.py, passing arguments to set the number of worker\nprocesses. For example, this command…\n$ python3 procs.py 2\n…will launch two worker processes, producing results almost twice as fast as sequen‐\ntial.py—if your machine has at least two cores and is not too busy running other\nprograms.\nA Homegrown Process Pool \n| \n723",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 754,
      "chapter": null,
      "content": "16 See 19-concurrency/primes/threads.py in the Fluent Python code repository.\nI ran procs.py 12 times with 1 to 20 processes, totaling 240 runs. Then I computed the\nmedian time for all runs with the same number of processes, and plotted Figure 19-2.\nFigure 19-2. Median run times for each number of processes from 1 to 20. Highest\nmedian time was 40.81s, with 1 process. Lowest median time was 10.39s, with 6 pro‐\ncesses, indicated by the dotted line.\nIn this 6-core laptop, the lowest median time was with 6 processes: 10.39s—marked\nby the dotted line in Figure 19-2. I expected the run time to increase after 6 processes\ndue to CPU contention, and it reached a local maximum of 12.51s at 10 processes. I\ndid not expect and I can’t explain why the performance improved at 11 processes and\nremained almost flat from 13 to 20 processes, with median times only slightly higher\nthan the lowest median time at 6 processes.\nThread-Based Nonsolution\nI also wrote threads.py, a version of procs.py using threading instead of multiproc\nessing. The code is very similar—as is usually the case when converting simple\nexamples between these two APIs.16 Due to the GIL and the compute-intensive\nnature of is_prime, the threaded version is slower than the sequential code in\nExample 19-12, and it gets slower as the number of threads increase, because of CPU\ncontention and the cost of context switching. To switch to a new thread, the OS\nneeds to save CPU registers and update the program counter and stack pointer,\n724 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 755,
      "chapter": null,
      "content": "17 To learn more, see “Context switch” in the English Wikipedia.\n18 These are probably the same reasons that prompted the creator of the Ruby language, Yukihiro Matsumoto,\nto use a GIL in his interpreter as well.\ntriggering expensive side effects like invalidating CPU caches and possibly even\nswapping memory pages.17\nThe next two chapters will cover more about concurrent programming in Python,\nusing the high-level concurrent.futures library to manage threads and processes\n(Chapter 20) and the asyncio library for asynchronous programming (Chapter 21).\nThe remaining sections in this chapter aim to answer the question:\nGiven the limitations discussed so far, how is Python thriving in a multicore world?\nPython in the Multicore World\nConsider this citation from the widely quoted article “The Free Lunch Is Over: A\nFundamental Turn Toward Concurrency in Software” by Herb Sutter:\nThe major processor manufacturers and architectures, from Intel and AMD to Sparc\nand PowerPC, have run out of room with most of their traditional approaches to\nboosting CPU performance. Instead of driving clock speeds and straight-line instruc‐\ntion throughput ever higher, they are instead turning en masse to hyper-threading and\nmulticore architectures. March 2005. [Available online].\nWhat Sutter calls the “free lunch” was the trend of software getting faster with no\nadditional developer effort because CPUs were executing sequential code faster, year\nafter year. Since 2004, that is no longer true: clock speeds and execution optimiza‐\ntions reached a plateau, and now any significant increase in performance must come\nfrom leveraging multiple cores or hyperthreading, advances that only benefit code\nthat is written for concurrent execution.\nPython’s story started in the early 1990s, when CPUs were still getting exponentially\nfaster at sequential code execution. There was no talk about multicore CPUs except\nin supercomputers back then. At the time, the decision to have a GIL was a no-\nbrainer. The GIL makes the interpreter faster when running on a single core, and its\nimplementation simpler.18 The GIL also makes it easier to write simple extensions\nthrough the Python/C API.\nPython in the Multicore World \n| \n725",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 756,
      "chapter": null,
      "content": "19 As an exercise in college, I had to implement the LZW compression algorithm in C. But first I wrote it in\nPython, to check my understanding of the spec. The C version was about 900× faster.\nI just wrote “simple extensions” because an extension does not\nneed to deal with the GIL at all. A function written in C or Fortran\nmay be hundreds of times faster than the equivalent in Python.19\nTherefore the added complexity of releasing the GIL to leverage\nmulticore CPUs may not be needed in many cases. So we can\nthank the GIL for many extensions available for Python—and that\nis certainly one of the key reasons why the language is so popular\ntoday.\nDespite the GIL, Python is thriving in applications that require concurrent or parallel\nexecution, thanks to libraries and software architectures that work around the limita‐\ntions of CPython.\nNow let’s discuss how Python is used in system administration, data science, and\nserver-side application development in the multicore, distributed computing world\nof 2021.\nSystem Administration\nPython is widely used to manage large fleets of servers, routers, load balancers, and\nnetwork-attached storage (NAS). It’s also a leading option in software-defined net‐\nworking (SDN) and ethical hacking. Major cloud service providers support Python\nthrough libraries and tutorials authored by the providers themselves or by their large\ncommunities of Python users.\nIn this domain, Python scripts automate configuration tasks by issuing commands to\nbe carried out by the remote machines, so rarely there are CPU-bound operations to\nbe done. Threads or coroutines are well suited for such jobs. In particular, the concur\nrent.futures package we’ll see in Chapter 20 can be used to perform the same oper‐\nations on many remote machines at the same time without a lot of complexity.\nBeyond the standard library, there are popular Python-based projects to manage\nserver clusters: tools like Ansible and Salt, as well as libraries like Fabric.\nThere is also a growing number of libraries for system administration supporting\ncoroutines and asyncio. In 2016, Facebook’s Production Engineering team reported:\n“We are increasingly relying on AsyncIO, which was introduced in Python 3.4, and\nseeing huge performance gains as we move codebases away from Python 2.”\n726 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 757,
      "chapter": null,
      "content": "Data Science\nData science—including artificial intelligence—and scientific computing are very\nwell served by Python. Applications in these fields are compute-intensive, but Python\nusers benefit from a vast ecosystem of numeric computing libraries written in C,\nC++, Fortran, Cython, etc.—many of which are able to leverage multicore machines,\nGPUs, and/or distributed parallel computing in heterogeneous clusters.\nAs of 2021, Python’s data science ecosystem includes impressive tools such as:\nProject Jupyter\nTwo browser-based interfaces—Jupyter Notebook and JupyterLab—that allow\nusers to run and document analytics code potentially running across the network\non remote machines. Both are hybrid Python/JavaScript applications, supporting\ncomputing kernels written in different languages, all integrated via ZeroMQ—an\nasynchronous messaging library for distributed applications. The name Jupyter\nactually comes from Julia, Python, and R, the first three languages supported by\nthe Notebook. The rich ecosystem built on top of the Jupyter tools include\nBokeh, a powerful interactive visualization library that lets users navigate and\ninteract with large datasets or continuously updated streaming data, thanks to\nthe performance of modern JavaScript engines and browsers.\nTensorFlow and PyTorch\nThese are the top two deep learning frameworks, according to O’Reilly’s January\n2021 report on usage of their learning resources during 2020. Both projects are\nwritten in C++, and are able to leverage multiple cores, GPUs, and clusters. They\nsupport other languages as well, but Python is their main focus and is used by the\nmajority of their users. TensorFlow was created and is used internally by Google;\nPyTorch by Facebook.\nDask\nA parallel computing library that can farm out work to local processes or clusters\nof machines, “tested on some of the largest supercomputers in the world”—as\ntheir home page states. Dask offers APIs that closely emulate NumPy, pandas,\nand scikit-learn—the most popular libraries in data science and machine learn‐\ning today. Dask can be used from JupyterLab or Jupyter Notebook, and leverages\nBokeh not only for data visualization but also for an interactive dashboard show‐\ning the flow of data and computations across the processes/machines in near real\ntime. Dask is so impressive that I recommend watching a video such as this 15-\nminute demo in which Matthew Rocklin—a maintainer of the project—shows\nDask crunching data on 64 cores distributed in 8 EC2 machines on AWS.\nThese are only some examples to illustrate how the data science community is creat‐\ning solutions that leverage the best of Python and overcome the limitations of the\nCPython runtime.\nPython in the Multicore World \n| \n727",
      "content_length": 2726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 758,
      "chapter": null,
      "content": "20 Source: Thoughtworks Technology Advisory Board, Technology Radar—November 2015.\n21 Contrast application caches—used directly by your application code—with HTTP caches, which would be\nplaced on the top edge of Figure 19-3 to serve static assets like images, CSS, and JS files. Content Delivery\nNetworks (CDNs) offer another type of HTTP cache, deployed in data centers closer to the end users of your\napplication.\nServer-Side Web/Mobile Development\nPython is widely used in web applications and for the backend APIs supporting\nmobile applications. How is it that Google, YouTube, Dropbox, Instagram, Quora,\nand Reddit—among others—managed to build Python server-side applications serv‐\ning hundreds of millions of users 24x7? Again, the answer goes way beyond what\nPython provides “out of the box.”\nBefore we discuss tools to support Python at scale, I must quote an admonition from\nthe Thoughtworks Technology Radar:\nHigh performance envy/web scale envy\nWe see many teams run into trouble because they have chosen complex tools, frame‐\nworks or architectures because they “might need to scale.” Companies such as Twitter\nand Netflix need to support extreme loads and so need these architectures, but they\nalso have extremely skilled development teams able to handle the complexity. Most sit‐\nuations do not require these kinds of engineering feats; teams should keep their web\nscale envy in check in favor of simpler solutions that still get the job done.20\nAt web scale, the key is an architecture that allows horizontal scaling. At that point, all\nsystems are distributed systems, and no single programming language is likely to be\nthe right choice for every part of the solution.\nDistributed systems is a field of academic research, but fortunately some practitioners\nhave written accessible books anchored on solid research and practical experience.\nOne of them is Martin Kleppmann, the author of Designing Data-Intensive Applica‐\ntions (O’Reilly).\nConsider Figure 19-3, the first of many architecture diagrams in Kleppmann’s book.\nHere are some components I’ve seen in Python engagements that I worked on or\nhave firsthand knowledge of:\n• Application caches:21 memcached, Redis, Varnish\n• Relational databases: PostgreSQL, MySQL\n• Document databases: Apache CouchDB, MongoDB\n• Full-text indexes: Elasticsearch, Apache Solr\n• Message queues: RabbitMQ, Redis\n728 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 759,
      "chapter": null,
      "content": "22 Diagram adapted from Figure 1-1, Designing Data-Intensive Applications by Martin Kleppmann (O’Reilly).\nFigure 19-3. One possible architecture for a system combining several components.22\nThere are other industrial-strength open source products in each of those categories.\nMajor cloud providers also offer their own proprietary alternatives.\nKleppmann’s diagram is general and language independent—as is his book. For\nPython server-side applications, two specific components are often deployed:\n• An application server to distribute the load among several instances of the\nPython application. The application server would appear near the top in\nFigure 19-3, handling client requests before they reached the application code.\n• A task queue built around the message queue on the righthand side of\nFigure 19-3, providing a higher-level, easier-to-use API to distribute tasks to pro‐\ncesses running on other machines.\nPython in the Multicore World \n| \n729",
      "content_length": 955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 760,
      "chapter": null,
      "content": "23 Some speakers spell out the WSGI acronym, while others pronounce it as one word rhyming with “whisky.”\n24 uWSGI is spelled with a lowercase “u,” but that is pronounced as the Greek letter “µ,” so the whole name\nsounds like “micro-whisky” with a “g” instead of the “k.”\nThe next two sections explore these components that are recommended best practi‐\nces in Python server-side deployments.\nWSGI Application Servers\nWSGI—the Web Server Gateway Interface—is a standard API for a Python frame‐\nwork or application to receive requests from an HTTP server and send responses to\nit.23 WSGI application servers manage one or more processes running your applica‐\ntion, maximizing the use of the available CPUs.\nFigure 19-4 illustrates a typical WSGI deployment.\nIf we wanted to merge the previous pair of diagrams, the content of\nthe dashed rectangle in Figure 19-4 would replace the solid “Appli‐\ncation code” rectangle at the top of Figure 19-3.\nThe best-known application servers in Python web projects are:\n• mod_wsgi\n• uWSGI24\n• Gunicorn\n• NGINX Unit\nFor users of the Apache HTTP server, mod_wsgi is the best option. It’s as old as\nWSGI itself, but is actively maintained, and now provides a command-line launcher\ncalled mod_wsgi-express that makes it easier to configure and more suitable for use\nin Docker containers.\n730 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 1367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 761,
      "chapter": null,
      "content": "25 Bloomberg engineers Peter Sperl and Ben Green wrote “Configuring uWSGI for Production Deployment”,\nexplaining how many of the default settings in uWSGI are not suitable for many common deployment sce‐\nnarios. Sperl presented a summary of their recommendations at EuroPython 2019. Highly recommended for\nusers of uWSGI.\nFigure 19-4. Clients connect to an HTTP server that delivers static files and routes\nother requests to the application server, which forks child processes to run the applica‐\ntion code, leveraging multiple CPU cores. The WSGI API is the glue between the appli‐\ncation server and the Python application code.\nuWSGI and Gunicorn are the top choices in recent projects I know about. Both are\noften used with the NGINX HTTP server. uWSGI offers a lot of extra functionality,\nincluding an application cache, a task queue, cron-like periodic tasks, and many\nother features. On the flip side, uWSGI is much harder to configure properly than\nGunicorn.25\nReleased in 2018, NGINX Unit is a new product from the makers of the well-known\nNGINX HTTP server and reverse proxy.\nPython in the Multicore World \n| \n731",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 762,
      "chapter": null,
      "content": "mod_wsgi and Gunicorn support Python web apps only, while uWSGI and NGINX\nUnit work with other languages as well. Please browse their docs to learn more.\nThe main point: all of these application servers can potentially use all CPU cores on\nthe server by forking multiple Python processes to run traditional web apps written\nin good old sequential code in Django, Flask, Pyramid, etc. This explains why it’s\nbeen possible to earn a living as a Python web developer without ever studying the\nthreading, multiprocessing, or asyncio modules: the application server handles\nconcurrency transparently.\nASGI—Asynchronous Server Gateway Interface\nWSGI is a synchronous API. It doesn’t support coroutines with\nasync/await—the most efficient way to implement WebSockets or\nHTTP long polling in Python. The ASGI specification is a succes‐\nsor to WSGI, designed for asynchronous Python web frameworks\nsuch as aiohttp, Sanic, FastAPI, etc., as well as Django and Flask,\nwhich are gradually adding asynchronous functionality.\nNow let’s turn to another way of bypassing the GIL to achieve higher performance\nwith server-side Python applications.\nDistributed Task Queues\nWhen the application server delivers a request to one of the Python processes run‐\nning your code, your app needs to respond quickly: you want the process to be avail‐\nable to handle the next request as soon as possible. However, some requests demand\nactions that may take longer—for example, sending email or generating a PDF. That’s\nthe problem that distributed task queues are designed to solve.\nCelery and RQ are the best known open source task queues with Python APIs. Cloud\nproviders also offer their own proprietary task queues.\nThese products wrap a message queue and offer a high-level API for delegating tasks\nto workers, possibly running on different machines.\nIn the context of task queues, the words producer and consumer are\nused instead of traditional client/server terminology. For example,\na Django view handler produces job requests, which are put in the\nqueue to be consumed by one or more PDF rendering processes.\nQuoting directly from Celery’s FAQ, here are some typical use cases:\n• Running something in the background. For example, to finish the web request as\nsoon as possible, then update the users page incrementally. This gives the user the\n732 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 763,
      "chapter": null,
      "content": "impression of good performance and “snappiness,” even though the real work\nmight actually take some time.\n• Running something after the web request has finished.\n• Making sure something is done, by executing it asynchronously and using retries.\n• Scheduling periodic work.\nBesides solving these immediate problems, task queues support horizontal scalability.\nProducers and consumers are decoupled: a producer doesn’t call a consumer, it puts\na request in a queue. Consumers don’t need to know anything about the producers\n(but the request may include information about the producer, if an acknowledgment\nis required). Crucially, you can easily add more workers to consume tasks as demand\ngrows. That’s why Celery and RQ are called distributed task queues.\nRecall that our simple procs.py (Example 19-13) used two queues: one for job\nrequests, the other for collecting results. The distributed architecture of Celery and\nRQ uses a similar pattern. Both support using the Redis NoSQL database as a message\nqueue and result storage. Celery also supports other message queues like RabbitMQ\nor Amazon SQS, as well other databases for result storage.\nThis wraps up our introduction to concurrency in Python. The next two chapters will\ncontinue this theme, focusing on the concurrent.futures and asyncio packages of\nthe standard library.\nChapter Summary\nAfter a bit of theory, this chapter presented the spinner scripts implemented in each\nof Python’s three native concurrency programming models:\n• Threads, with the threading package\n• Processes, with multiprocessing\n• Asynchronous coroutines with asyncio\nWe then explored the real impact of the GIL with an experiment: changing the spin‐\nner examples to compute the primality of a large integer and observe the resulting\nbehavior. This demonstrated graphically that CPU-intensive functions must be avoi‐\nded in asyncio, as they block the event loop. The threaded version of the experiment\nworked—despite the GIL—because Python periodically interrupts threads, and the\nexample used only two threads: one doing compute-intensive work, and the other\ndriving the animation only 10 times per second. The multiprocessing variant\nworked around the GIL, starting a new process just for the animation, while the main\nprocess did the primality check.\nChapter Summary \n| \n733",
      "content_length": 2310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 764,
      "chapter": null,
      "content": "The next example, computing several primes, highlighted the difference between mul\ntiprocessing and threading, proving that only processes allow Python to benefit\nfrom multicore CPUs. Python’s GIL makes threads worse than sequential code for\nheavy computations.\nThe GIL dominates discussions about concurrent and parallel computing in Python,\nbut we should not overestimate its impact. That was the point of “Python in the Mul‐\nticore World” on page 725. For example, the GIL doesn’t affect many use cases of\nPython in system administration. On the other hand, the data science and server-side\ndevelopment communities have worked around the GIL with industrial-strength sol‐\nutions tailored to their specific needs. The last two sections mentioned two common\nelements to support Python server-side applications at scale: WSGI application\nservers and distributed task queues.\nFurther Reading\nThis chapter has an extensive reading list, so I split it into subsections.\nConcurrency with Threads and Processes\nThe concurrent.futures library covered in Chapter 20 uses threads, processes, locks,\nand queues under the hood, but you won’t see individual instances of them; they’re\nbundled and managed by the higher-level abstractions of a ThreadPoolExecutor and\na ProcessPoolExecutor. If you want to learn more about the practice of concurrent\nprogramming with those low-level objects, “An Intro to Threading in Python” by Jim\nAnderson is a good first read. Doug Hellmann has a chapter titled “Concurrency with\nProcesses, Threads, and Coroutines” on his website and book, The Python 3 Standard\nLibrary by Example (Addison-Wesley).\nBrett Slatkin’s Effective Python, 2nd ed. (Addison-Wesley), David Beazley’s Python\nEssential Reference, 4th ed. (Addison-Wesley), and Martelli et al., Python in a Nut‐\nshell, 3rd ed. (O’Reilly) are other general Python references with significant coverage\nof threading and multiprocessing. The vast multiprocessing official documenta‐\ntion includes useful advice in its “Programming guidelines” section.\nJesse Noller and Richard Oudkerk contributed the multiprocessing package, intro‐\nduced in PEP 371—Addition of the multiprocessing package to the standard library.\nThe official documentation for the package is a 93 KB .rst file—that’s about 63 pages\n—making it one of the longest chapters in the Python standard library.\nIn High Performance Python, 2nd ed., (O’Reilly), authors Micha Gorelick and Ian\nOzsvald include a chapter about multiprocessing with an example about checking\nfor primes with a different strategy than our procs.py example. For each number, they\nsplit the range of possible factors—from 2 to sqrt(n)—into subranges, and make\n734 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 765,
      "chapter": null,
      "content": "26 Caleb is one of the tech reviewers for this edition of Fluent Python.\neach worker iterate over one of the subranges. Their divide-and-conquer approach is\ntypical of scientific computing applications where the datasets are huge, and worksta‐\ntions (or clusters) have more CPU cores than users. On a server-side system handling\nrequests from many users, it is simpler and more efficient to let each process work on\none computation from start to finish—reducing the overhead of communication and\ncoordination among processes. Besides multiprocessing, Gorelick and Ozsvald\npresent many other ways of developing and deploying high-performance data science\napplications leveraging multiple cores, GPUs, clusters, profilers, and compilers like\nCython and Numba. Their last chapter, “Lessons from the Field,” is a valuable collec‐\ntion of short case studies contributed by other practitioners of high-performance\ncomputing in Python.\nAdvanced Python Development by Matthew Wilkes (Apress), is a rare book that\nincludes short examples to explain concepts, while also showing how to build a realis‐\ntic application ready for production: a data aggregator, similar to DevOps monitoring\nsystems or IoT data collectors for distributed sensors. Two chapters in Advanced\nPython Development cover concurrent programming with threading and asyncio.\nJan Palach’s Parallel Programming with Python (Packt, 2014) explains the core con‐\ncepts behind concurrency and parallelism, covering Python’s standard library as well\nas Celery.\n“The Truth About Threads” is the title of Chapter 2 in Using Asyncio in Python by\nCaleb Hattingh (O’Reilly).26 The chapter covers the benefits and drawbacks of thread‐\ning—with compelling quotes from several authoritative sources—making it clear that\nthe fundamental challenges of threads have nothing to do with Python or the GIL.\nQuoting verbatim from page 14 of Using Asyncio in Python:\nThese themes repeat throughout:\n• Threading makes code hard to reason about.\n• Threading is an inefficient model for large-scale concurrency (thousands of con‐\ncurrent tasks).\nIf you want to learn the hard way how difficult it is to reason about threads and locks\n—without risking your job—try the exercises in Allen Downey’s workbook, The Little\nBook of Semaphores (Green Tea Press). The exercises in Downey’s book range from\neasy to very hard to unsolvable, but even the easy ones are eye-opening.\nFurther Reading \n| \n735",
      "content_length": 2427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 766,
      "chapter": null,
      "content": "27 Thanks to Lucas Brunialti for sending me a link to this talk.\nThe GIL\nIf you are intrigued about the GIL, remember we have no control over it from Python\ncode, so the canonical reference is in the C-API documentation: Thread State and the\nGlobal Interpreter Lock. The Python Library and Extension FAQ answers: “Can’t we\nget rid of the Global Interpreter Lock?”. Also worth reading are posts by Guido van\nRossum and Jesse Noller (contributor of the multiprocessing package), respectively:\n“It isn’t Easy to Remove the GIL” and “Python Threads and the Global Interpreter\nLock”.\nCPython Internals by Anthony Shaw (Real Python) explains the implementation of\nthe CPython 3 interpreter at the C programming level. Shaw’s longest chapter is\n“Parallelism and Concurrency”: a deep dive into Python’s native support for threads\nand processes, including managing the GIL from extensions using the C/Python API.\nFinally, David Beazley presented a detailed exploration in “Understanding the\nPython GIL”.27 In slide #54 of the presentation, Beazley reports an increase in pro‐\ncessing time for a particular benchmark with the new GIL algorithm introduced in\nPython 3.2. The issue is not significant with real workloads, according to a comment\nby Antoine Pitrou—who implemented the new GIL algorithm—in the bug report\nsubmitted by Beazley: Python issue #7946.\nConcurrency Beyond the Standard Library\nFluent Python focuses on core language features and core parts of the standard\nlibrary. Full Stack Python is a great complement to this book: it’s about Python’s eco‐\nsystem, with sections titled “Development Environments,” “Data,” “Web Develop‐\nment,” and “DevOps,” among others.\nI’ve already mentioned two books that cover concurrency using the Python standard\nlibrary that also include significant content on third-party libraries and tools:\nHigh Performance Python, 2nd ed. and Parallel Programming with Python. Francesco\nPierfederici’s Distributed Computing with Python (Packt) covers the standard library\nand also the use of cloud providers and HPC (High-Performance Computing) clus‐\nters.\n“Python, Performance, and GPUs” by Matthew Rocklin is “a status update for using\nGPU accelerators from Python,” posted in June 2019.\n“Instagram currently features the world’s largest deployment of the Django web\nframework, which is written entirely in Python.” That’s the opening sentence of the\nblog post, “Web Service Efficiency at Instagram with Python”, written by Min Ni—a\nsoftware engineer at Instagram. The post describes metrics and tools Instagram uses\n736 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2595,
      "extraction_method": "Direct"
    },
    {
      "page_number": 767,
      "chapter": null,
      "content": "to optimize the efficiency of its Python codebase, as well as detect and diagnose per‐\nformance regressions as it deploys its back end “30-50 times a day.”\nArchitecture Patterns with Python: Enabling Test-Driven Development, Domain-\nDriven Design, and Event-Driven Microservices by Harry Percival and Bob Gregory\n(O’Reilly) presents architectural patterns for Python server-side applications. The\nauthors also made the book freely available online at cosmicpython.com.\nTwo elegant and easy-to-use libraries for parallelizing tasks over processes are lelo by\nJoão S. O. Bueno and python-parallelize by Nat Pryce. The lelo package defines a @par\nallel decorator that you can apply to any function to magically make it unblocking:\nwhen you call the decorated function, its execution is started in another process. Nat\nPryce’s python-parallelize package provides a parallelize generator that distributes\nthe execution of a for loop over multiple CPUs. Both packages are built on the multi‐\nprocessing library.\nPython core developer Eric Snow maintains a Multicore Python wiki, with notes\nabout his and other people’s efforts to improve Python’s support for parallel execu‐\ntion. Snow is the author of PEP 554—Multiple Interpreters in the Stdlib. If approved\nand implemented, PEP 554 lays the groundwork for future enhancements that may\neventually allow Python to use multiple cores without the overheads of multiprocess‐\ning. One of the biggest blockers is the complex interaction between multiple active\nsubinterpreters and extensions that assume a single interpreter.\nMark Shannon—also a Python maintainer—created a useful table comparing concur‐\nrent models in Python, referenced in a discussion about subinterpreters between him,\nEric Snow, and other developers on the python-dev mailing list. In Shannon’s table,\nthe “Ideal CSP” column refers to the theoretical Communicating sequential processes\nmodel proposed by Tony Hoare in 1978. Go also allows shared objects, violating an\nessential constraint of CSP: execution units should communicate through message\npassing through channels.\nStackless Python (a.k.a. Stackless) is a fork of CPython implementing microthreads,\nwhich are application-level lightweight threads—as opposed to OS threads. The mas‐\nsively multiplayer online game EVE Online was built on Stackless, and engineers\nemployed by the game company CCP were maintainers of Stackless for a while. Some\nfeatures of Stackless were reimplemented in the Pypy interpreter and the greenlet\npackage, the core technology of the gevent networking library, which in turn is the\nfoundation of the Gunicorn application server.\nThe actor model of concurrent programming is at the core of the highly scalable\nErlang and Elixir languages, and is also the model of the Akka framework for Scala\nand Java. If you want to try out the actor model in Python, check out the Thespian\nand Pykka libraries.\nFurther Reading \n| \n737",
      "content_length": 2917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 768,
      "chapter": null,
      "content": "28 Python’s threading and concurrent.futures APIs are heavily influenced by the Java standard library.\nMy remaining recommendations have few or zero mentions of Python, but are nev‐\nertheless relevant to readers interested in the theme of this chapter.\nConcurrency and Scalability Beyond Python\nRabbitMQ in Action by Alvaro Videla and Jason J. W. Williams (Manning) is a very\nwell-written introduction to RabbitMQ and the Advanced Message Queuing Protocol\n(AMQP) standard, with examples in Python, PHP, and Ruby. Regardless of the rest\nof your tech stack, and even if you plan to use Celery with RabbitMQ under the hood,\nI recommend this book for its coverage of concepts, motivation, and patterns for dis‐\ntributed message queues, as well as operating and tuning RabbitMQ at scale.\nI learned a lot reading Seven Concurrency Models in Seven Weeks, by Paul Butcher\n(Pragmatic Bookshelf), with the eloquent subtitle When Threads Unravel. Chapter 1\nof the book presents the core concepts and challenges of programming with threads\nand locks in Java.28 The remaining six chapters of the book are devoted to what the\nauthor considers better alternatives for concurrent and parallel programming, as sup‐\nported by different languages, tools, and libraries. The examples use Java, Clojure,\nElixir, and C (for the chapter about parallel programming with the OpenCL frame‐\nwork). The CSP model is exemplified with Clojure code, although the Go language\ndeserves credit for popularizing that approach. Elixir is the language of the examples\nillustrating the actor model. A freely available, alternative bonus chapter about actors\nuses Scala and the Akka framework. Unless you already know Scala, Elixir is a more\naccessible language to learn and experiment with the actor model and the\nErlang/OTP distributed systems platform.\nUnmesh Joshi of Thoughtworks has contributed several pages documenting “Pat‐\nterns of Distributed Systems” to Martin Fowler’s blog. The opening page is a great\nintroduction the topic, with links to individual patterns. Joshi is adding patterns\nincrementally, but what’s already there distills years of hard-earned experience in\nmission-critical systems.\nMartin Kleppmann’s Designing Data-Intensive Applications (O’Reilly) is a rare book\nwritten by a practitioner with deep industry experience and advanced academic back‐\nground. The author worked with large-scale data infrastructure at LinkedIn and two\nstartups, before becoming a researcher of distributed systems at the University of\nCambridge. Each chapter in Kleppmann’s book ends with an extensive list of refer‐\nences, including recent research results. The book also includes numerous illuminat‐\ning diagrams and beautiful concept maps.\nI was fortunate to be in the audience for Francesco Cesarini’s outstanding workshop\non the architecture of reliable distributed systems at OSCON 2016: “Designing and\n738 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 769,
      "chapter": null,
      "content": "architecting for scalability with Erlang/OTP” (video at the O’Reilly Learning Plat‐\nform). Despite the title, 9:35 into the video, Cesarini explains:\nVery little of what I am going to say will be Erlang-specific […]. The fact remains that\nErlang will remove a lot of accidental difficulties to making systems which are resilient\nand which never fail, and are scalable. So it will be much easier if you do use Erlang, or\na language running on the Erlang virtual machine.\nThat workshop was based on the last four chapters of Designing for Scalability with\nErlang/OTP by Francesco Cesarini and Steve Vinoski (O’Reilly).\nProgramming distributed systems is challenging and exciting, but beware of web-\nscale envy. The KISS principle remains solid engineering advice.\nCheck out the paper “Scalability! But at what COST?” by Frank McSherry, Michael\nIsard, and Derek G. Murray. The authors identified parallel graph-processing systems\npresented in academic symposia that require hundreds of cores to outperform a\n“competent single-threaded implementation.” They also found systems that “under‐\nperform one thread for all of their reported configurations.”\nThose findings remind me of a classic hacker quip:\nMy Perl script is faster than your Hadoop cluster.\nSoapbox\nTo Manage Complexity, We Need Constraints\nI learned to program on a TI-58 calculator. Its “language” was similar to assembly. At\nthat level, all “variables” are globals, and you don’t have the luxury of structured con‐\ntrol flow statements. You have conditional jumps: instructions that take the execution\ndirectly to an arbitrary location—ahead or behind the current spot—depending on\nthe value of a CPU register or flag.\nBasically you can do anything in assembly, and that’s the challenge: there are very few\nconstraints to keep you from making mistakes, and to help maintainers understand\nthe code when changes are needed.\nThe second language I learned was the unstructured BASIC that came with 8-bit\ncomputers—nothing like Visual Basic, which appeared much later. There were FOR,\nGOSUB, and RETURN statements, but still no concept of local variables. GOSUB did not\nsupport parameter passing: it was just a fancy GOTO that put a return line number in a\nstack so that RETURN had a target to jump to. Subroutines could help themselves to the\nglobal data, and put results there too. We had to improvise other forms of control\nflow with combinations of IF and GOTO—which, again, allowed you to jump to any\nline of the program.\nFurther Reading \n| \n739",
      "content_length": 2507,
      "extraction_method": "Direct"
    },
    {
      "page_number": 770,
      "chapter": null,
      "content": "29 The Erlang community uses the term “process” for actors. In Erlang, each process is a function in its own\nloop, so they are very lightweight and it’s feasible to have millions of them active at once in a single machine\n—no relation to the heavyweight OS processes we’ve been talking about elsewhere in this chapter. So here we\nhave examples of the two sins described by Prof. Simon: using different words to mean the same thing, and\nusing one word to mean different things.\nAfter a few years of programming with jumps and global variables, I remember the\nstruggle to rewire my brain for “structured programming” when I learned Pascal.\nNow I had to use control flow statements around blocks of code that have a single\nentry point. I couldn’t jump to any instruction I liked. Global variables were unavoid‐\nable in BASIC, but now they were taboo. I needed to rethink the flow of data and\nexplicitly pass arguments to functions.\nThe next challenge for me was learning object-oriented programming. At its core,\nobject-oriented programming is structured programming with more constraints and\npolymorphism. Information hiding forces yet another rethink of where data lives. I\nremember being frustrated more than once because I had to refactor my code so that\na method I was writing could get information that was encapsulated in an object that\nmy method could not reach.\nFunctional programming languages add other constraints, but immutability is the\nhardest to swallow after decades of imperative programming and object-oriented\nprogramming. After we get used to these constraints, we see them as blessings. They\nmake reasoning about the code much easier.\nLack of constraints is the main problem with the threads-and-locks model of concur‐\nrent programming. When summarizing Chapter 1 of Seven Concurrency Models in\nSeven Weeks, Paul Butcher wrote:\nThe greatest weakness of the approach, however, is that threads-and-locks program‐\nming is hard. It may be easy for a language designer to add them to a language, but\nthey provide us, the poor programmers, with very little help.\nSome examples of unconstrained behavior in that model:\n• Threads can share access to arbitrary, mutable data structures.\n• The scheduler can interrupt a thread at almost any point, including in the middle\nof a simple operation like a += 1. Very few operations are atomic at the level of\nsource code expressions.\n• Locks are usually advisory. That’s a technical term meaning that you must\nremember to explicitly hold a lock before updating a shared data structure. If you\nforget to get the lock, nothing prevents your code from messing up the data\nwhile another thread dutifully holds the lock and is updating the same data.\nIn contrast, consider some constraints enforced by the actor model, in which the exe‐\ncution unit is called an actor:29\n740 \n| \nChapter 19: Concurrency Models in Python",
      "content_length": 2868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 771,
      "chapter": null,
      "content": "• An actor can have internal state, but cannot share state with other actors.\n• Actors can only communicate by sending and receiving messages.\n• Messages only hold copies of data, not references to mutable data.\n• An actor only handles one message at a time. There is no concurrent execution\ninside a single actor.\nOf course, you can adopt an actor style of coding in any language by following these\nrules. You can also use object-oriented programming idioms in C, and even struc‐\ntured programming patterns in assembly. But doing any of that requires a lot of\nagreement and discipline among everyone who touches the code.\nManaging locks is unnecessary in the actor model, as implemented by Erlang and\nElixir, where all data types are immutable.\nThreads-and-locks are not going away. I just don’t think dealing with such low-level\nentities is a good use of my time as I write applications—as opposed to kernel mod‐\nules or databases.\nI reserve the right to change my mind, always. But right now, I am convinced that the\nactor model is the most sensible, general-purpose concurrent programming model\navailable. CSP (Communicating Sequential Processes) is also sensible, but its\nimplementation in Go leaves out some constraints. The idea in CSP is that coroutines\n(or goroutines in Go) exchange data and synchronize using queues (called channels in\nGo). But Go also supports memory sharing and locks. I’ve seen a book about Go\nadvocate the use of shared memory and locks instead of channels—in the name of\nperformance. Old habits die hard.\nFurther Reading \n| \n741",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 772,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 773,
      "chapter": null,
      "content": "1 From Michele Simionato’s post, “Threads, processes and concurrency in Python: some thoughts”, summar‐\nized as “Removing the hype around the multicore (non) revolution and some (hopefully) sensible comment\nabout threads and other forms of concurrency.”\nCHAPTER 20\nConcurrent Executors\nThe people bashing threads are typically system programmers which have in mind use\ncases that the typical application programmer will never encounter in her life. [...] In\n99% of the use cases an application programmer is likely to run into, the simple pat‐\ntern of spawning a bunch of independent threads and collecting the results in a queue\nis everything one needs to know.\n—Michele Simionato, Python deep thinker1\nThis chapter focuses on the concurrent.futures.Executor classes that encapsulate\nthe pattern of “spawning a bunch of independent threads and collecting the results in\na queue,” described by Michele Simionato. The concurrent executors make this pat‐\ntern almost trivial to use, not only with threads but also with processes—useful for\ncompute-intensive tasks.\nHere I also introduce the concept of futures—objects representing the asynchronous\nexecution of an operation, similar to JavaScript promises. This primitive idea is the\nfoundation not only of concurrent.futures but also of the asyncio package, the\nsubject of Chapter 21.\nWhat’s New in This Chapter\nI renamed the chapter from “Concurrency with Futures” to “Concurrent Executors”\nbecause the executors are the most important high-level feature covered here. Futures\nare low-level objects, focused on in “Where Are the Futures?” on page 751, but mostly\ninvisible in the rest of the chapter.\n743",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 774,
      "chapter": null,
      "content": "2 Particularly if your cloud provider rents machines by the second, regardless of how busy the CPUs are.\nAll the HTTP client examples now use the new HTTPX library, which provides syn‐\nchronous and asynchronous APIs.\nThe setup for the experiments in “Downloads with Progress Display and Error Han‐\ndling” on page 762 is now simpler, thanks to the multithreaded server added to the\nhttp.server package in Python 3.7. Previously, the standard library only had the\nsingle-threaded BaseHttpServer, which was no good for experimenting with concur‐\nrent clients, so I had to resort to external tools in the first edition of this book.\n“Launching Processes with concurrent.futures” on page 754 now demonstrates how an\nexecutor simplifies the code we saw in “Code for the Multicore Prime Checker” on\npage 719.\nFinally, I moved most of the theory to the new Chapter 19, “Concurrency Models in\nPython”.\nConcurrent Web Downloads\nConcurrency is essential for efficient network I/O: instead of idly waiting for remote\nmachines, the application should do something else until a response comes back.2\nTo demonstrate with code, I wrote three simple programs to download images of 20\ncountry flags from the web. The first one, flags.py, runs sequentially: it only requests\nthe next image when the previous one is downloaded and saved locally. The other\ntwo scripts make concurrent downloads: they request several images practically at the\nsame time, and save them as they arrive. The flags_threadpool.py script uses the con\ncurrent.futures package, while flags_asyncio.py uses asyncio.\nExample 20-1 shows the result of running the three scripts, three times each. I also\nposted a 73s video on YouTube so you can watch them running while a macOS\nFinder window displays the flags as they are saved. The scripts are downloading\nimages from fluentpython.com, which is behind a CDN, so you may see slower results\nin the first runs. The results in Example 20-1 were obtained after several runs, so the\nCDN cache was warm.\nExample 20-1. Three typical runs of the scripts flags.py, flags_threadpool.py, and\nflags_asyncio.py\n$ python3 flags.py\nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN  \n20 flags downloaded in 7.26s  \n$ python3 flags.py\nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN\n744 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 775,
      "chapter": null,
      "content": "20 flags downloaded in 7.20s\n$ python3 flags.py\nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN\n20 flags downloaded in 7.09s\n$ python3 flags_threadpool.py\nDE BD CN JP ID EG NG BR RU CD IR MX US PH FR PK VN IN ET TR\n20 flags downloaded in 1.37s  \n$ python3 flags_threadpool.py\nEG BR FR IN BD JP DE RU PK PH CD MX ID US NG TR CN VN ET IR\n20 flags downloaded in 1.60s\n$ python3 flags_threadpool.py\nBD DE EG CN ID RU IN VN ET MX FR CD NG US JP TR PK BR IR PH\n20 flags downloaded in 1.22s\n$ python3 flags_asyncio.py  \nBD BR IN ID TR DE CN US IR PK PH FR RU NG VN ET MX EG JP CD\n20 flags downloaded in 1.36s\n$ python3 flags_asyncio.py\nRU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US\n20 flags downloaded in 1.27s\n$ python3 flags_asyncio.py\nRU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH CD TR  \n20 flags downloaded in 1.42s\nThe output for each run starts with the country codes of the flags as they are\ndownloaded, and ends with a message stating the elapsed time.\nIt took flags.py an average 7.18s to download 20 images.\nThe average for flags_threadpool.py was 1.40s.\nFor flags_asyncio.py, 1.35s was the average time.\nNote the order of the country codes: the downloads happened in a different\norder every time with the concurrent scripts.\nThe difference in performance between the concurrent scripts is not significant, but\nthey are both more than five times faster than the sequential script—and this is just\nfor the small task of downloading 20 files of a few kilobytes each. If you scale the task\nto hundreds of downloads, the concurrent scripts can outpace the sequential code by\na factor or 20 or more.\nWhile testing concurrent HTTP clients against public web servers,\nyou may inadvertently launch a denial-of-service (DoS) attack, or\nbe suspected of doing so. In the case of Example 20-1, it’s OK to do\nit because those scripts are hardcoded to make only 20 requests.\nWe’ll use Python’s http.server package to run tests later in this\nchapter.\nConcurrent Web Downloads \n| \n745",
      "content_length": 2006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 776,
      "chapter": null,
      "content": "3 For servers that may be hit by many clients, there is a difference: coroutines scale better because they use\nmuch less memory than threads, and also reduce the cost of context switching, which I mentioned in\n“Thread-Based Nonsolution” on page 724.\nNow let’s study the implementations of two of the scripts tested in Example 20-1:\nflags.py and flags_threadpool.py. I will leave the third script, flags_asyncio.py, for\nChapter 21, but I wanted to demonstrate all three together to make two points:\n1. Regardless of the concurrency constructs you use—threads or coroutines—you’ll\nsee vastly improved throughput over sequential code in network I/O operations,\nif you code it properly.\n2. For HTTP clients that can control how many requests they make, there is no sig‐\nnificant difference in performance between threads and coroutines.3\nOn to the code.\nA Sequential Download Script\nExample 20-2 contains the implementation of flags.py, the first script we ran in\nExample 20-1. It’s not very interesting, but we’ll reuse most of its code and settings to\nimplement the concurrent scripts, so it deserves some attention.\nFor clarity, there is no error handling in Example 20-2. We will\ndeal with exceptions later, but here I want to focus on the basic\nstructure of the code, to make it easier to contrast this script with\nthe concurrent ones.\nExample 20-2. flags.py: sequential download script; some functions will be reused by\nthe other scripts\nimport time\nfrom pathlib import Path\nfrom typing import Callable\nimport httpx  \nPOP20_CC = ('CN IN US ID BR PK NG BD RU JP '\n            'MX PH VN ET EG DE IR TR CD FR').split()  \nBASE_URL = 'https://www.fluentpython.com/data/flags'  \nDEST_DIR = Path('downloaded')                         \ndef save_flag(img: bytes, filename: str) -> None:     \n    (DEST_DIR / filename).write_bytes(img)\n746 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 777,
      "chapter": null,
      "content": "4 The images are originally from the CIA World Factbook, a public-domain, US government publication. I\ncopied them to my site to avoid the risk of launching a DOS attack on cia.gov.\ndef get_flag(cc: str) -> bytes:  \n    url = f'{BASE_URL}/{cc}/{cc}.gif'.lower()\n    resp = httpx.get(url, timeout=6.1,       \n                     follow_redirects=True)  \n    resp.raise_for_status()  \n    return resp.content\ndef download_many(cc_list: list[str]) -> int:  \n    for cc in sorted(cc_list):                 \n        image = get_flag(cc)\n        save_flag(image, f'{cc}.gif')\n        print(cc, end=' ', flush=True)         \n    return len(cc_list)\ndef main(downloader: Callable[[list[str]], int]) -> None:  \n    DEST_DIR.mkdir(exist_ok=True)                          \n    t0 = time.perf_counter()                               \n    count = downloader(POP20_CC)\n    elapsed = time.perf_counter() - t0\n    print(f'\\n{count} downloads in {elapsed:.2f}s')\nif __name__ == '__main__':\n    main(download_many)     \nImport the httpx library. It’s not part of the standard library, so by convention\nthe import goes after the standard library modules and a blank line.\nList of the ISO 3166 country codes for the 20 most populous countries in order of\ndecreasing population.\nThe directory with the flag images.4\nLocal directory where the images are saved.\nSave the img bytes to filename in the DEST_DIR.\nGiven a country code, build the URL and download the image, returning the\nbinary contents of the response.\nIt’s good practice to add a sensible timeout to network operations, to avoid\nblocking for several minutes for no good reason.\nConcurrent Web Downloads \n| \n747",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 778,
      "chapter": null,
      "content": "5 Setting follow_redirects=True is not needed for this example, but I wanted to highlight this important dif‐\nference between HTTPX and requests. Also, setting follow_redirects=True in this example gives me flexibil‐\nity to host the image files elsewhere in the future. I think the HTTPX default setting of follow_redirects\n=False is sensible because unexpected redirects can mask needless requests and complicate error diagnostics.\nBy default, HTTPX does not follow redirects.5\nThere’s no error handling in this script, but this method raises an exception if the\nHTTP status is not in the 2XX range—highly recommended to avoid silent\nfailures.\ndownload_many is the key function to compare with the concurrent\nimplementations.\nLoop over the list of country codes in alphabetical order, to make it easy to see\nthat the ordering is preserved in the output; return the number of country codes\ndownloaded.\nDisplay one country code at a time in the same line so we can see progress as\neach download happens. The end=' ' argument replaces the usual line break at\nthe end of each line printed with a space character, so all country codes are dis‐\nplayed progressively in the same line. The flush=True argument is needed\nbecause, by default, Python output is line buffered, meaning that Python only\ndisplays printed characters after a line break.\nmain must be called with the function that will make the downloads; that way, we\ncan use main as a library function with other implementations of download_many\nin the threadpool and ascyncio examples.\nCreate DEST_DIR if needed; don’t raise an error if the directory exists.\nRecord and report the elapsed time after running the downloader function.\nCall main with the download_many function.\nThe HTTPX library is inspired by the Pythonic requests package,\nbut is built on a more modern foundation. Crucially, HTTPX pro‐\nvides synchronous and asynchronous APIs, so we can use it in all\nHTTP client examples in this chapter and the next. Python’s stan‐\ndard library provides the urllib.request module, but its API is\nsynchronous only, and is not user friendly.\n748 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 779,
      "chapter": null,
      "content": "There’s really nothing new to flags.py. It serves as a baseline for comparing the other\nscripts, and I used it as a library to avoid redundant code when implementing them.\nNow let’s see a reimplementation using concurrent.futures.\nDownloading with concurrent.futures\nThe main features of the concurrent.futures package are the ThreadPoolExecutor\nand ProcessPoolExecutor classes, which implement an API to submit callables for\nexecution in different threads or processes, respectively. The classes transparently\nmanage a pool of worker threads or processes, and queues to distribute jobs and col‐\nlect results. But the interface is very high-level, and we don’t need to know about any\nof those details for a simple use case like our flag downloads.\nExample 20-3 shows the easiest way to implement the downloads concurrently, using\nthe ThreadPoolExecutor.map method.\nExample 20-3. flags_threadpool.py: threaded download script using futures.Thread\nPoolExecutor\nfrom concurrent import futures\nfrom flags import save_flag, get_flag, main  \ndef download_one(cc: str):  \n    image = get_flag(cc)\n    save_flag(image, f'{cc}.gif')\n    print(cc, end=' ', flush=True)\n    return cc\ndef download_many(cc_list: list[str]) -> int:\n    with futures.ThreadPoolExecutor() as executor:         \n        res = executor.map(download_one, sorted(cc_list))  \n    return len(list(res))                                  \nif __name__ == '__main__':\n    main(download_many)  \nReuse some functions from the flags module (Example 20-2).\nFunction to download a single image; this is what each worker will execute.\nInstantiate the ThreadPoolExecutor as a context manager; the executor\n.__exit__ method will call executor.shutdown(wait=True), which will block\nuntil all threads are done.\nConcurrent Web Downloads \n| \n749",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 780,
      "chapter": null,
      "content": "The map method is similar to the map built-in, except that the download_one func‐\ntion will be called concurrently from multiple threads; it returns a generator that\nyou can iterate to retrieve the value returned by each function call—in this case,\neach call to download_one will return a country code.\nReturn the number of results obtained. If any of the threaded calls raises an\nexception, that exception is raised here when the implicit next() call inside the\nlist constructor tries to retrieve the corresponding return value from the itera‐\ntor returned by executor.map.\nCall the main function from the flags module, passing the concurrent version of\ndownload_many.\nNote that the download_one function from Example 20-3 is essentially the body of the\nfor loop in the download_many function from Example 20-2. This is a common refac‐\ntoring when writing concurrent code: turning the body of a sequential for loop into a\nfunction to be called concurrently.\nExample 20-3 is very short because I was able to reuse most func‐\ntions from the sequential flags.py script. One of the best features of\nconcurrent.futures is to make it simple to add concurrent execu‐\ntion on top of legacy sequential code.\nThe ThreadPoolExecutor constructor takes several arguments not shown, but the\nfirst and most important one is max_workers, setting the maximum number of\nworker threads to be executed. When max_workers is None (the default), ThreadPool\nExecutor decides its value using the following expression—since Python 3.8:\nmax_workers = min(32, os.cpu_count() + 4)\nThe rationale is explained in the ThreadPoolExecutor documentation:\nThis default value preserves at least 5 workers for I/O bound tasks. It utilizes at most\n32 CPU cores for CPU bound tasks which release the GIL. And it avoids using very\nlarge resources implicitly on many-core machines.\nThreadPoolExecutor now reuses idle worker threads before starting max_workers\nworker threads too.\nTo conclude: the computed default for max_workers is sensible, and ThreadPoolExe\ncutor avoids starting new workers unnecessarily. Understanding the logic behind\nmax_workers may help you decide when and how to set it yourself.\nThe library is called concurrency.futures, yet there are no futures to be seen in\nExample 20-3, so you may be wondering where they are. The next section explains.\n750 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 781,
      "chapter": null,
      "content": "Where Are the Futures?\nFutures are core components of concurrent.futures and of asyncio, but as users of\nthese libraries we sometimes don’t see them. Example 20-3 depends on futures\nbehind the scenes, but the code I wrote does not touch them directly. This section is\nan overview of futures, with an example that shows them in action.\nSince Python 3.4, there are two classes named Future in the standard library: concur\nrent.futures.Future and asyncio.Future. They serve the same purpose: an\ninstance of either Future class represents a deferred computation that may or may\nnot have completed. This is somewhat similar to the Deferred class in Twisted, the\nFuture class in Tornado, and Promise in modern JavaScript.\nFutures encapsulate pending operations so that we can put them in queues, check\nwhether they are done, and retrieve results (or exceptions) when they become\navailable.\nAn important thing to know about futures is that you and I should not create them:\nthey are meant to be instantiated exclusively by the concurrency framework, be it\nconcurrent.futures or asyncio. Here is why: a Future represents something that\nwill eventually run, therefore it must be scheduled to run, and that’s the job of the\nframework. In particular, concurrent.futures.Future instances are created only as\nthe result of submitting a callable for execution with a concurrent.futures.Execu\ntor subclass. For example, the Executor.submit() method takes a callable, schedules\nit to run, and returns a Future.\nApplication code is not supposed to change the state of a future: the concurrency\nframework changes the state of a future when the computation it represents is done,\nand we can’t control when that happens.\nBoth types of Future have a .done() method that is nonblocking and returns a\nBoolean that tells you whether the callable wrapped by that future has executed or\nnot. However, instead of repeatedly asking whether a future is done, client code usu‐\nally asks to be notified. That’s why both Future classes have an .add_done_call\nback() method: you give it a callable, and the callable will be invoked with the future\nas the single argument when the future is done. Be aware that the callback callable\nwill run in the same worker thread or process that ran the function wrapped in the\nfuture.\nThere is also a .result() method, which works the same in both classes when the\nfuture is done: it returns the result of the callable, or re-raises whatever exception\nmight have been thrown when the callable was executed. However, when the future is\nnot done, the behavior of the result method is very different between the two flavors\nof Future. In a concurrency.futures.Future instance, invoking f.result() will\nblock the caller’s thread until the result is ready. An optional timeout argument can\nConcurrent Web Downloads \n| \n751",
      "content_length": 2822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 782,
      "chapter": null,
      "content": "be passed, and if the future is not done in the specified time, the result method\nraises TimeoutError. The asyncio.Future.result method does not support time‐\nout, and await is the preferred way to get the result of futures in asyncio—but await\ndoesn’t work with concurrency.futures.Future instances.\nSeveral functions in both libraries return futures; others use them in their implemen‐\ntation in a way that is transparent to the user. An example of the latter is the Execu\ntor.map we saw in Example 20-3: it returns an iterator in which __next__ calls the\nresult method of each future, so we get the results of the futures, and not the futures\nthemselves.\nTo get a practical look at futures, we can rewrite Example 20-3 to use the concur\nrent.futures.as_completed function, which takes an iterable of futures and returns\nan iterator that yields futures as they are done.\nUsing futures.as_completed requires changes to the download_many function only.\nThe higher-level executor.map call is replaced by two for loops: one to create and\nschedule the futures, the other to retrieve their results. While we are at it, we’ll add a\nfew print calls to display each future before and after it’s done. Example 20-4 shows\nthe code for a new download_many function. The code for download_many grew from\n5 to 17 lines, but now we get to inspect the mysterious futures. The remaining func‐\ntions are the same as in Example 20-3.\nExample 20-4. flags_threadpool_futures.py: replacing executor.map with execu\ntor.submit and futures.as_completed in the download_many function\ndef download_many(cc_list: list[str]) -> int:\n    cc_list = cc_list[:5]  \n    with futures.ThreadPoolExecutor(max_workers=3) as executor:  \n        to_do: list[futures.Future] = []\n        for cc in sorted(cc_list):  \n            future = executor.submit(download_one, cc)  \n            to_do.append(future)  \n            print(f'Scheduled for {cc}: {future}')  \n        for count, future in enumerate(futures.as_completed(to_do), 1):  \n            res: str = future.result()  \n            print(f'{future} result: {res!r}')  \n    return count\n752 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 783,
      "chapter": null,
      "content": "For this demonstration, use only the top five most populous countries.\nSet max_workers to 3 so we can see pending futures in the output.\nIterate over country codes alphabetically, to make it clear that results will arrive\nout of order.\nexecutor.submit schedules the callable to be executed, and returns a future\nrepresenting this pending operation.\nStore each future so we can later retrieve them with as_completed.\nDisplay a message with the country code and the respective future.\nas_completed yields futures as they are completed.\nGet the result of this future.\nDisplay the future and its result.\nNote that the future.result() call will never block in this example because the\nfuture is coming out of as_completed. Example 20-5 shows the output of one run of\nExample 20-4.\nExample 20-5. Output of flags_threadpool_futures.py\n$ python3 flags_threadpool_futures.py\nScheduled for BR: <Future at 0x100791518 state=running>  \nScheduled for CN: <Future at 0x100791710 state=running>\nScheduled for ID: <Future at 0x100791a90 state=running>\nScheduled for IN: <Future at 0x101807080 state=pending>  \nScheduled for US: <Future at 0x101807128 state=pending>\nCN <Future at 0x100791710 state=finished returned str> result: 'CN'  \nBR ID <Future at 0x100791518 state=finished returned str> result: 'BR'  \n<Future at 0x100791a90 state=finished returned str> result: 'ID'\nIN <Future at 0x101807080 state=finished returned str> result: 'IN'\nUS <Future at 0x101807128 state=finished returned str> result: 'US'\n5 downloads in 0.70s\nThe futures are scheduled in alphabetical order; the repr() of a future shows its\nstate: the first three are running, because there are three worker threads.\nThe last two futures are pending, waiting for worker threads.\nConcurrent Web Downloads \n| \n753",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 784,
      "chapter": null,
      "content": "The first CN here is the output of download_one in a worker thread; the rest of the\nline is the output of download_many.\nHere, two threads output codes before download_many in the main thread can\ndisplay the result of the first thread.\nI recommend experimenting with flags_threadpool_futures.py. If\nyou run it several times, you’ll see the order of the results varying.\nIncreasing max_workers to 5 will increase the variation in the order\nof the results. Decreasing it to 1 will make this script run sequen‐\ntially, and the order of the results will always be the order of the\nsubmit calls.\nWe saw two variants of the download script using concurrent.futures: one in\nExample 20-3 with ThreadPoolExecutor.map and one in Example 20-4 with\nfutures.as_completed. If you are curious about the code for flags_asyncio.py, you\nmay peek at Example 21-3 in Chapter 21, where it is explained.\nNow let’s take a brief look at a simple way to work around the GIL for CPU-bound\njobs using concurrent.futures.\nLaunching Processes with concurrent.futures\nThe concurrent.futures documentation page is subtitled “Launching parallel\ntasks.” The package enables parallel computation on multicore machines because it\nsupports distributing work among multiple Python processes using the ProcessPool\nExecutor class.\nBoth ProcessPoolExecutor and ThreadPoolExecutor implement the Executor\ninterface, so it’s easy to switch from a thread-based to a process-based solution using\nconcurrent.futures.\nThere is no advantage in using a ProcessPoolExecutor for the flags download exam‐\nple or any I/O-bound job. It’s easy to verify this; just change these lines in\nExample 20-3:\ndef download_many(cc_list: list[str]) -> int:\n    with futures.ThreadPoolExecutor() as executor:\nTo this:\ndef download_many(cc_list: list[str]) -> int:\n    with futures.ProcessPoolExecutor() as executor:\n754 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 785,
      "chapter": null,
      "content": "The constructor for ProcessPoolExecutor also has a max_workers parameter, which\ndefaults to None. In that case, the executor limits the number of workers to the num‐\nber returned by os.cpu_count().\nProcesses use more memory and take longer to start than threads, so the real value\nof ProcessPoolExecutor is in CPU-intensive jobs. Let’s go back to the primality\ntest example of “A Homegrown Process Pool” on page 716, rewriting it with\nconcurrent.futures.\nMulticore Prime Checker Redux\nIn “Code for the Multicore Prime Checker” on page 719 we studied procs.py, a script\nthat checked the primality of some large numbers using multiprocessing. In\nExample 20-6 we solve the same problem in the proc_pool.py program using a Proc\nessPoolExecutor. From the first import to the main() call at the end, procs.py has 43\nnonblank lines of code, and proc_pool.py has 31—28% shorter.\nExample 20-6. proc_pool.py: procs.py rewritten with ProcessPoolExecutor\nimport sys\nfrom concurrent import futures  \nfrom time import perf_counter\nfrom typing import NamedTuple\nfrom primes import is_prime, NUMBERS\nclass PrimeResult(NamedTuple):  \n    n: int\n    flag: bool\n    elapsed: float\ndef check(n: int) -> PrimeResult:\n    t0 = perf_counter()\n    res = is_prime(n)\n    return PrimeResult(n, res, perf_counter() - t0)\ndef main() -> None:\n    if len(sys.argv) < 2:\n        workers = None      \n    else:\n        workers = int(sys.argv[1])\n    executor = futures.ProcessPoolExecutor(workers)  \n    actual_workers = executor._max_workers  # type: ignore  \n    print(f'Checking {len(NUMBERS)} numbers with {actual_workers} processes:')\n    t0 = perf_counter()\nLaunching Processes with concurrent.futures \n| \n755",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 786,
      "chapter": null,
      "content": "numbers = sorted(NUMBERS, reverse=True)  \n    with executor:  \n        for n, prime, elapsed in executor.map(check, numbers):  \n            label = 'P' if prime else ' '\n            print(f'{n:16}  {label} {elapsed:9.6f}s')\n    time = perf_counter() - t0\n    print(f'Total time: {time:.2f}s')\nif __name__ == '__main__':\n    main()\nNo need to import multiprocessing, SimpleQueue etc.; concurrent.futures\nhides all that.\nThe PrimeResult tuple and the check function are the same as we saw in\nprocs.py, but we don’t need the queues and the worker function anymore.\nInstead of deciding ourselves how many workers to use if no command-line\nargument was given, we set workers to None and let the ProcessPoolExecutor\ndecide.\nHere I build the ProcessPoolExecutor before the with block in ➐ so that I can\ndisplay the actual number of workers in the next line.\n_max_workers is an undocumented instance attribute of a ProcessPoolExecutor.\nI decided to use it to show the number of workers when the workers variable is\nNone. Mypy correctly complains when I access it, so I put the type: ignore com‐\nment to silence it.\nSort the numbers to be checked in descending order. This will expose a differ‐\nence in the behavior of proc_pool.py when compared with procs.py. See the\nexplanation after this example.\nUse the executor as a context manager.\nThe executor.map call returns the PrimeResult instances returned by check in\nthe same order as the numbers arguments.\nIf you run Example 20-6, you’ll see the results appearing in strict descending order, as\nshown in Example 20-7. In contrast, the ordering of the output of procs.py (shown in\n“Process-Based Solution” on page 718) is heavily influenced by the difficulty in\nchecking whether each number is a prime. For example, procs.py shows the result for\n756 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 787,
      "chapter": null,
      "content": "7777777777777777 near the top, because it has a low divisor, 7, so is_prime quickly\ndetermines it’s not a prime.\nIn contrast, 7777777536340681 is 881917092, so is_prime will take much longer to\ndetermine that it’s a composite number, and even longer to find out that\n7777777777777753 is prime—therefore both of these numbers appear near the end of\nthe output of procs.py.\nRunning proc_pool.py, you’ll observe not only the descending order of the results, but\nalso that the program will appear to be stuck after showing the result for\n9999999999999999.\nExample 20-7. Output of proc_pool.py\n$ ./proc_pool.py\nChecking 20 numbers with 12 processes:\n9999999999999999     0.000024s  \n9999999999999917  P  9.500677s  \n7777777777777777     0.000022s  \n7777777777777753  P  8.976933s\n7777777536340681     8.896149s\n6666667141414921     8.537621s\n6666666666666719  P  8.548641s\n6666666666666666     0.000002s\n5555555555555555     0.000017s\n5555555555555503  P  8.214086s\n5555553133149889     8.067247s\n4444444488888889     7.546234s\n4444444444444444     0.000002s\n4444444444444423  P  7.622370s\n3333335652092209     6.724649s\n3333333333333333     0.000018s\n3333333333333301  P  6.655039s\n 299593572317531  P  2.072723s\n 142702110479723  P  1.461840s\n               2  P  0.000001s\nTotal time: 9.65s\nThis line appears very quickly.\nThis line takes more than 9.5s to show up.\nAll the remaining lines appear almost immediately.\nLaunching Processes with concurrent.futures \n| \n757",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 788,
      "chapter": null,
      "content": "Here is why proc_pool.py behaves in that way:\n• As mentioned before, executor.map(check, numbers) returns the result in the\nsame order as the numbers are given.\n• By default, proc_pool.py uses as many workers as there are CPUs—it’s what\nProcessPoolExecutor does when max_workers is None. That’s 12 processes in\nthis laptop.\n• Because we are submitting numbers in descending order, the first is\n9999999999999999; with 9 as a divisor, it returns quickly.\n• The second number is 9999999999999917, the largest prime in the sample. This\nwill take longer than all the others to check.\n• Meanwhile, the remaining 11 processes will be checking other numbers, which\nare either primes or composites with large factors, or composites with very small\nfactors.\n• When the worker in charge of 9999999999999917 finally determines that’s a\nprime, all the other processes have completed their last jobs, so the results appear\nimmediately after.\nAlthough the progress of proc_pool.py is not as visible as that of\nprocs.py, the overall execution time is practically the same as depic‐\nted in Figure 19-2, for the same number of workers and CPU cores.\nUnderstanding how concurrent programs behave is not straightforward, so here’s is a\nsecond experiment that may help you visualize the operation of Executor.map.\nExperimenting with Executor.map\nLet’s investigate Executor.map, now using a ThreadPoolExecutor with three workers\nrunning five callables that output timestamped messages. The code is in\nExample 20-8, the output in Example 20-9.\nExample 20-8. demo_executor_map.py: Simple demonstration of the map method of\nThreadPoolExecutor\nfrom time import sleep, strftime\nfrom concurrent import futures\ndef display(*args):  \n    print(strftime('[%H:%M:%S]'), end=' ')\n    print(*args)\n758 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 789,
      "chapter": null,
      "content": "def loiter(n):  \n    msg = '{}loiter({}): doing nothing for {}s...'\n    display(msg.format('\\t'*n, n, n))\n    sleep(n)\n    msg = '{}loiter({}): done.'\n    display(msg.format('\\t'*n, n))\n    return n * 10  \ndef main():\n    display('Script starting.')\n    executor = futures.ThreadPoolExecutor(max_workers=3)  \n    results = executor.map(loiter, range(5))  \n    display('results:', results)  \n    display('Waiting for individual results:')\n    for i, result in enumerate(results):  \n        display(f'result {i}: {result}')\nif __name__ == '__main__':\n    main()\nThis function simply prints whatever arguments it gets, preceded by a timestamp\nin the format [HH:MM:SS].\nloiter does nothing except display a message when it starts, sleep for n seconds,\nthen display a message when it ends; tabs are used to indent the messages accord‐\ning to the value of n.\nloiter returns n * 10 so we can see how to collect results.\nCreate a ThreadPoolExecutor with three threads.\nSubmit five tasks to the executor. Since there are only three threads, only three\nof those tasks will start immediately: the calls loiter(0), loiter(1), and loi\nter(2); this is a nonblocking call.\nImmediately display the results of invoking executor.map: it’s a generator, as\nthe output in Example 20-9 shows.\nThe enumerate call in the for loop will implicitly invoke next(results), which\nin turn will invoke _f.result() on the (internal) _f future representing the first\ncall, loiter(0). The result method will block until the future is done, therefore\neach iteration in this loop will have to wait for the next result to be ready.\nExperimenting with Executor.map \n| \n759",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 790,
      "chapter": null,
      "content": "6 Your mileage may vary: with threads, you never know the exact sequencing of events that should happen\nnearly at the same time; it’s possible that, in another machine, you see loiter(1) starting before loiter(0)\nfinishes, particularly because sleep always releases the GIL, so Python may switch to another thread even if\nyou sleep for 0s.\nI encourage you to run Example 20-8 and see the display being updated incremen‐\ntally. While you’re at it, play with the max_workers argument for the ThreadPoolExec\nutor and with the range function that produces the arguments for the executor.map\ncall—or replace it with lists of handpicked values to create different delays.\nExample 20-9 shows a sample run of Example 20-8.\nExample 20-9. Sample run of demo_executor_map.py from Example 20-8\n$ python3 demo_executor_map.py\n[15:56:50] Script starting.  \n[15:56:50] loiter(0): doing nothing for 0s...  \n[15:56:50] loiter(0): done.\n[15:56:50]      loiter(1): doing nothing for 1s...  \n[15:56:50]              loiter(2): doing nothing for 2s...\n[15:56:50] results: <generator object result_iterator at 0x106517168>  \n[15:56:50]                      loiter(3): doing nothing for 3s...  \n[15:56:50] Waiting for individual results:\n[15:56:50] result 0: 0  \n[15:56:51]      loiter(1): done. \n[15:56:51]                              loiter(4): doing nothing for 4s...\n[15:56:51] result 1: 10  \n[15:56:52]              loiter(2): done.  \n[15:56:52] result 2: 20\n[15:56:53]                      loiter(3): done.\n[15:56:53] result 3: 30\n[15:56:55]                              loiter(4): done.  \n[15:56:55] result 4: 40\nThis run started at 15:56:50.\nThe first thread executes loiter(0), so it will sleep for 0s and return even before\nthe second thread has a chance to start, but YMMV.6\nloiter(1) and loiter(2) start immediately (because the thread pool has three\nworkers, it can run three functions concurrently).\nThis shows that the results returned by executor.map is a generator; nothing so\nfar would block, regardless of the number of tasks and the max_workers setting.\n760 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 791,
      "chapter": null,
      "content": "Because loiter(0) is done, the first worker is now available to start the fourth\nthread for loiter(3).\nThis is where execution may block, depending on the parameters given to the\nloiter calls: the __next__ method of the results generator must wait until the\nfirst future is complete. In this case, it won’t block because the call to loiter(0)\nfinished before this loop started. Note that everything up to this point happened\nwithin the same second: 15:56:50.\nloiter(1) is done one second later, at 15:56:51. The thread is freed to start\nloiter(4).\nThe result of loiter(1) is shown: 10. Now the for loop will block waiting for the\nresult of loiter(2).\nThe pattern repeats: loiter(2) is done, its result is shown; same with loiter(3).\nThere is a 2s delay until loiter(4) is done, because it started at 15:56:51 and did\nnothing for 4s.\nThe Executor.map function is easy to use, but often it’s preferable to get the results as\nthey are ready, regardless of the order they were submitted. To do that, we need a\ncombination of the Executor.submit method and the futures.as_completed func‐\ntion, as we saw in Example 20-4. We’ll come back to this technique in “Using\nfutures.as_completed” on page 769.\nThe combination of executor.submit and futures.as_completed\nis more flexible than executor.map because you can submit\ndifferent callables and arguments, while executor.map is designed\nto run the same callable on the different arguments. In addition,\nthe set of futures you pass to futures.as_completed may come\nfrom more than one executor—perhaps some were created by\na ThreadPoolExecutor instance, while others are from a\nProcessPoolExecutor.\nIn the next section, we will resume the flag download examples with new require‐\nments that will force us to iterate over the results of futures.as_completed instead\nof using executor.map.\nExperimenting with Executor.map \n| \n761",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 792,
      "chapter": null,
      "content": "Downloads with Progress Display and Error Handling\nAs mentioned, the scripts in “Concurrent Web Downloads” on page 744 have no\nerror handling to make them easier to read and to contrast the structure of the three\napproaches: sequential, threaded, and asynchronous.\nIn order to test the handling of a variety of error conditions, I created the flags2\nexamples:\nflags2_common.py\nThis module contains common functions and settings used by all flags2 exam‐\nples, including a main function, which takes care of command-line parsing, tim‐\ning, and reporting results. That is really support code, not directly relevant to\nthe subject of this chapter, so I will not list the source code here, but you can\nread it in the fluentpython/example-code-2e repository: 20-executors/getflags/\nflags2_common.py.\nflags2_sequential.py\nA sequential HTTP client with proper error handling and progress bar display.\nIts download_one function is also used by flags2_threadpool.py.\nflags2_threadpool.py\nConcurrent HTTP client based on futures.ThreadPoolExecutor to demon‐\nstrate error handling and integration of the progress bar.\nflags2_asyncio.py\nSame functionality as the previous example, but implemented with asyncio and\nhttpx. This will be covered in “Enhancing the asyncio Downloader” on page 787,\nin Chapter 21.\nBe Careful when Testing Concurrent Clients\nWhen testing concurrent HTTP clients on public web servers, you\nmay generate many requests per second, and that’s how denial-of-\nservice (DoS) attacks are made. Carefully throttle your clients when\nhitting public servers. For testing, set up a local HTTP server. See\n“Setting Up Test Servers” on page 765 for instructions.\nThe most visible feature of the flags2 examples is that they have an animated, text-\nmode progress bar implemented with the tqdm package. I posted a 108s video on\nYouTube to show the progress bar and contrast the speed of the three flags2 scripts.\nIn the video, I start with the sequential download, but I interrupt it after 32s because\nit was going to take more than 5 minutes to hit on 676 URLs and get 194 flags. I then\nrun the threaded and asyncio scripts three times each, and every time they complete\n762 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 793,
      "chapter": null,
      "content": "the job in 6s or less (i.e., more than 60 times faster). Figure 20-1 shows two screen‐\nshots: during and after running flags2_threadpool.py.\nFigure 20-1. Top-left: flags2_threadpool.py running with live progress bar generated by\ntqdm; bottom-right: same terminal window after the script is finished.\nThe simplest tqdm example appears in an animated .gif in the project’s README.md.\nIf you type the following code in the Python console after installing the tqdm pack‐\nage, you’ll see an animated progress bar where the comment is:\n>>> import time\n>>> from tqdm import tqdm\n>>> for i in tqdm(range(1000)):\n...     time.sleep(.01)\n...\n>>> # -> progress bar will appear here <-\nBesides the neat effect, the tqdm function is also interesting conceptually: it consumes\nany iterable and produces an iterator which, while it’s consumed, displays the pro‐\ngress bar and estimates the remaining time to complete all iterations. To compute\nthat estimate, tqdm needs to get an iterable that has a len, or additionally receive the\ntotal= argument with the expected number of items. Integrating tqdm with our\nflags2 examples provides an opportunity to look deeper into how the concurrent\nscripts actually work, by forcing us to use the futures.as_completed and the asyn\ncio.as_completed functions so that tqdm can display progress as each future is\ncompleted.\nThe other feature of the flags2 example is a command-line interface. All three\nscripts accept the same options, and you can see them by running any of the scripts\nwith the -h option. Example 20-10 shows the help text.\nDownloads with Progress Display and Error Handling \n| \n763",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 794,
      "chapter": null,
      "content": "Example 20-10. Help screen for the scripts in the flags2 series\n$ python3 flags2_threadpool.py -h\nusage: flags2_threadpool.py [-h] [-a] [-e] [-l N] [-m CONCURRENT] [-s LABEL]\n                            [-v]\n                            [CC [CC ...]]\nDownload flags for country codes. Default: top 20 countries by population.\npositional arguments:\n  CC                    country code or 1st letter (eg. B for BA...BZ)\noptional arguments:\n  -h, --help            show this help message and exit\n  -a, --all             get all available flags (AD to ZW)\n  -e, --every           get flags for every possible code (AA...ZZ)\n  -l N, --limit N       limit to N first codes\n  -m CONCURRENT, --max_req CONCURRENT\n                        maximum concurrent requests (default=30)\n  -s LABEL, --server LABEL\n                        Server to hit; one of DELAY, ERROR, LOCAL, REMOTE\n                        (default=LOCAL)\n  -v, --verbose         output detailed progress info\nAll arguments are optional. But the -s/--server is essential for testing: it lets you\nchoose which HTTP server and port will be used in the test. Pass one of these case-\ninsensitive labels to determine where the script will look for the flags:\nLOCAL\nUse http://localhost:8000/flags; this is the default. You should configure a\nlocal HTTP server to answer at port 8000. See the following note for instructions.\nREMOTE\nUse http://fluentpython.com/data/flags; that is a public website owned by\nme, hosted on a shared server. Please do not pound it with too many concurrent\nrequests. The fluentpython.com domain is handled by the Cloudflare CDN (Con‐\ntent Delivery Network) so you may notice that the first downloads are slower,\nbut they get faster when the CDN cache warms up.\nDELAY\nUse http://localhost:8001/flags; a server delaying HTTP responses should\nbe listening to port 8001. I wrote slow_server.py to make it easier to experiment.\nYou’ll find it in the 20-futures/getflags/ directory of the Fluent Python code repos‐\nitory. See the following note for instructions.\nERROR\nUse http://localhost:8002/flags; a server returning some HTTP errors\nshould be listening on port 8002. Instructions are next.\n764 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 795,
      "chapter": null,
      "content": "Setting Up Test Servers\nIf you don’t have a local HTTP server for testing, I wrote setup\ninstructions using only Python ≥ 3.9 (no external libraries) in 20-\nexecutors/getflags/README.adoc in the fluentpython/example-\ncode-2e repository. In short, README.adoc describes how to use:\npython3 -m http.server\nThe LOCAL server on port 8000\npython3 slow_server.py\nThe DELAY server on port 8001, which adds a random delay\nof .5s to 5s before each response\npython3 slow_server.py 8002 --error-rate .25\nThe ERROR server on port 8002, which in addition to the ran‐\ndom delay, has a 25% chance of returning a “418 I’m a teapot”\nerror response\nBy default, each flags2*.py script will fetch the flags of the 20 most populous countries\nfrom the LOCAL server (http://localhost:8000/flags) using a default number of\nconcurrent connections, which varies from script to script. Example 20-11 shows\na sample run of the flags2_sequential.py script using all defaults. To run it, you\nneed a local server, as explained in “Be Careful when Testing Concurrent Clients” on\npage 762.\nExample 20-11. Running flags2_sequential.py with all defaults: LOCAL site, top 20\nflags, 1 concurrent connection\n$ python3 flags2_sequential.py\nLOCAL site: http://localhost:8000/flags\nSearching for 20 flags: from BD to VN\n1 concurrent connection will be used.\n--------------------\n20 flags downloaded.\nElapsed time: 0.10s\nYou can select which flags will be downloaded in several ways. Example 20-12 shows\nhow to download all flags with country codes starting with the letters A, B, or C.\nExample 20-12. Run flags2_threadpool.py to fetch all flags with country codes prefixes\nA, B, or C from the DELAY server\n$ python3 flags2_threadpool.py -s DELAY a b c\nDELAY site: http://localhost:8001/flags\nSearching for 78 flags: from AA to CZ\n30 concurrent connections will be used.\nDownloads with Progress Display and Error Handling \n| \n765",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 796,
      "chapter": null,
      "content": "--------------------\n43 flags downloaded.\n35 not found.\nElapsed time: 1.72s\nRegardless of how the country codes are selected, the number of flags to fetch can be\nlimited with the -l/--limit option. Example 20-13 demonstrates how to run exactly\n100 requests, combining the -a option to get all flags with -l 100.\nExample 20-13. Run flags2_asyncio.py to get 100 flags (-al 100) from the ERROR\nserver, using 100 concurrent requests (-m 100)\n$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100\nERROR site: http://localhost:8002/flags\nSearching for 100 flags: from AD to LK\n100 concurrent connections will be used.\n--------------------\n73 flags downloaded.\n27 errors.\nElapsed time: 0.64s\nThat’s the user interface of the flags2 examples. Let’s see how they are implemented.\nError Handling in the flags2 Examples\nThe common strategy in all three examples to deal with HTTP errors is that 404\nerrors (not found) are handled by the function in charge of downloading a single file\n(download_one). Any other exception propagates to be handled by the down\nload_many function or the supervisor coroutine—in the asyncio example.\nOnce more, we’ll start by studying the sequential code, which is easier to follow—and\nmostly reused by the thread pool script. Example 20-14 shows the functions that per‐\nform the actual downloads in the flags2_sequential.py and flags2_threadpool.py\nscripts.\nExample 20-14. flags2_sequential.py: basic functions in charge of downloading; both\nare reused in flags2_threadpool.py\nfrom collections import Counter\nfrom http import HTTPStatus\nimport httpx\nimport tqdm  # type: ignore  \nfrom flags2_common import main, save_flag, DownloadStatus  \nDEFAULT_CONCUR_REQ = 1\nMAX_CONCUR_REQ = 1\n766 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 797,
      "chapter": null,
      "content": "7 As of September 2021, there are no type hints in the current release of tdqm. That’s OK. The world will not\nend because of that. Thank Guido for optional typing!\ndef get_flag(base_url: str, cc: str) -> bytes:\n    url = f'{base_url}/{cc}/{cc}.gif'.lower()\n    resp = httpx.get(url, timeout=3.1, follow_redirects=True)\n    resp.raise_for_status()  \n    return resp.content\ndef download_one(cc: str, base_url: str, verbose: bool = False) -> DownloadStatus:\n    try:\n        image = get_flag(base_url, cc)\n    except httpx.HTTPStatusError as exc:  \n        res = exc.response\n        if res.status_code == HTTPStatus.NOT_FOUND:\n            status = DownloadStatus.NOT_FOUND  \n            msg = f'not found: {res.url}'\n        else:\n            raise  \n    else:\n        save_flag(image, f'{cc}.gif')\n        status = DownloadStatus.OK\n        msg = 'OK'\n    if verbose:  \n        print(cc, msg)\n    return status\nImport the tqdm progress-bar display library, and tell Mypy to skip checking it.7\nImport a couple of functions and an Enum from the flags2_common module.\nRaises HTTPStetusError if the HTTP status code is not in range(200, 300).\ndownload_one catches HTTPStatusError to handle HTTP code 404 specifically…\n…by setting its local status to DownloadStatus.NOT_FOUND; DownloadStatus is\nan Enum imported from flags2_common.py.\nAny other HTTPStatusError exception is re-raised to propagate to the caller.\nIf the -v/--verbose command-line option is set, the country code and status\nmessage are displayed; this is how you’ll see progress in verbose mode.\nDownloads with Progress Display and Error Handling \n| \n767",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 798,
      "chapter": null,
      "content": "Example 20-15 lists the sequential version of the download_many function. This code\nis straightforward, but it’s worth studying to contrast with the concurrent versions\ncoming up. Focus on how it reports progress, handles errors, and tallies downloads.\nExample 20-15. flags2_sequential.py: the sequential implementation of download_many\ndef download_many(cc_list: list[str],\n                  base_url: str,\n                  verbose: bool,\n                  _unused_concur_req: int) -> Counter[DownloadStatus]:\n    counter: Counter[DownloadStatus] = Counter()  \n    cc_iter = sorted(cc_list)  \n    if not verbose:\n        cc_iter = tqdm.tqdm(cc_iter)  \n    for cc in cc_iter:\n        try:\n            status = download_one(cc, base_url, verbose)  \n        except httpx.HTTPStatusError as exc:  \n            error_msg = 'HTTP error {resp.status_code} - {resp.reason_phrase}'\n            error_msg = error_msg.format(resp=exc.response)\n        except httpx.RequestError as exc:  \n            error_msg = f'{exc} {type(exc)}'.strip()\n        except KeyboardInterrupt:  \n            break\n        else:  \n            error_msg = ''\n        if error_msg:\n            status = DownloadStatus.ERROR  \n        counter[status] += 1           \n        if verbose and error_msg:      \n            print(f'{cc} error: {error_msg}')\n    return counter  \nThis Counter will tally the different download outcomes: DownloadStatus.OK,\nDownloadStatus.NOT_FOUND, or DownloadStatus.ERROR.\ncc_iter holds the list of the country codes received as arguments, ordered\nalphabetically.\nIf not running in verbose mode, cc_iter is passed to tqdm, which returns an iter‐\nator yielding the items in cc_iter while also animating the progress bar.\nMake successive calls to download_one.\n768 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 799,
      "chapter": null,
      "content": "HTTP status code exceptions raised by get_flag and not handled by down\nload_one are handled here.\nOther network-related exceptions are handled here. Any other exception will\nabort the script, because the flags2_common.main function that calls down\nload_many has no try/except.\nExit the loop if the user hits Ctrl-C.\nIf no exception escaped download_one, clear the error message.\nIf there was an error, set the local status accordingly.\nIncrement the counter for that status.\nIn verbose mode, display the error message for the current country code, if any.\nReturn counter so that main can display the numbers in the final report.\nWe’ll now study the refactored thread pool example, flags2_threadpool.py.\nUsing futures.as_completed\nIn order to integrate the tqdm progress bar and handle errors on each request,\nthe flags2_threadpool.py script uses futures.ThreadPoolExecutor with the\nfutures.as_completed function we’ve already seen. Example 20-16 is the full listing\nof flags2_threadpool.py. Only the download_many function is implemented; the other\nfunctions are reused from flags2_common.py and flags2_sequential.py.\nExample 20-16. flags2_threadpool.py: full listing\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport httpx\nimport tqdm  # type: ignore\nfrom flags2_common import main, DownloadStatus\nfrom flags2_sequential import download_one  \nDEFAULT_CONCUR_REQ = 30  \nMAX_CONCUR_REQ = 1000  \ndef download_many(cc_list: list[str],\n                  base_url: str,\nDownloads with Progress Display and Error Handling \n| \n769",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 800,
      "chapter": null,
      "content": "verbose: bool,\n                  concur_req: int) -> Counter[DownloadStatus]:\n    counter: Counter[DownloadStatus] = Counter()\n    with ThreadPoolExecutor(max_workers=concur_req) as executor:  \n        to_do_map = {}  \n        for cc in sorted(cc_list):  \n            future = executor.submit(download_one, cc,\n                                     base_url, verbose)  \n            to_do_map[future] = cc  \n        done_iter = as_completed(to_do_map)  \n        if not verbose:\n            done_iter = tqdm.tqdm(done_iter, total=len(cc_list))  \n        for future in done_iter:  \n            try:\n                status = future.result()  \n            except httpx.HTTPStatusError as exc:  \n                error_msg = 'HTTP error {resp.status_code} - {resp.reason_phrase}'\n                error_msg = error_msg.format(resp=exc.response)\n            except httpx.RequestError as exc:\n                error_msg = f'{exc} {type(exc)}'.strip()\n            except KeyboardInterrupt:\n                break\n            else:\n                error_msg = ''\n            if error_msg:\n                status = DownloadStatus.ERROR\n            counter[status] += 1\n            if verbose and error_msg:\n                cc = to_do_map[future]  \n                print(f'{cc} error: {error_msg}')\n    return counter\nif __name__ == '__main__':\n    main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)\nReuse download_one from flags2_sequential (Example 20-14).\nIf the -m/--max_req command-line option is not given, this will be the maxi‐\nmum number of concurrent requests, implemented as the size of the thread pool;\nthe actual number may be smaller if the number of flags to download is smaller.\nMAX_CONCUR_REQ caps the maximum number of concurrent requests regardless of\nthe number of flags to download or the -m/--max_req command-line option. It’s\na safety precaution to avoid launching too many threads with their significant\nmemory overhead.\n770 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 801,
      "chapter": null,
      "content": "Create the executor with max_workers set to concur_req, computed by the main\nfunction as the smaller of: MAX_CONCUR_REQ, the length of cc_list, or the value of\nthe -m/--max_req command-line option. This avoids creating more threads than\nnecessary.\nThis dict will map each Future instance—representing one download—with the\nrespective country code for error reporting.\nIterate over the list of country codes in alphabetical order. The order of the\nresults will depend on the timing of the HTTP responses more than anything,\nbut if the size of the thread pool (given by concur_req) is much smaller than\nlen(cc_list), you may notice the downloads batched alphabetically.\nEach call to executor.submit schedules the execution of one callable and returns\na Future instance. The first argument is the callable, the rest are the arguments it\nwill receive.\nStore the future and the country code in the dict.\nfutures.as_completed returns an iterator that yields futures as each task is\ndone.\nIf not in verbose mode, wrap the result of as_completed with the tqdm function\nto display the progress bar; because done_iter has no len, we must tell tqdm\nwhat is the expected number of items as the total= argument, so tqdm can esti‐\nmate the work remaining.\nIterate over the futures as they are completed.\nCalling the result method on a future either returns the value returned by the\ncallable, or raises whatever exception was caught when the callable was executed.\nThis method may block waiting for a resolution, but not in this example because\nas_completed only returns futures that are done.\nHandle the potential exceptions; the rest of this function is identical to the\nsequential download_many in Example 20-15), except for the next callout.\nTo provide context for the error message, retrieve the country code from the\nto_do_map using the current future as key. This was not necessary in the\nsequential version because we were iterating over the list of country codes, so we\nknew the current cc; here we are iterating over the futures.\nDownloads with Progress Display and Error Handling \n| \n771",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 802,
      "chapter": null,
      "content": "Example 20-16 uses an idiom that is very useful with\nfutures.as_completed: building a dict to map each future to\nother data that may be useful when the future is completed. Here\nthe to_do_map maps each future to the country code assigned to it.\nThis makes it easy to do follow-up processing with the result of the\nfutures, despite the fact that they are produced out of order.\nPython threads are well suited for I/O-intensive applications, and the concur\nrent.futures package makes it relatively simple to use for certain use cases. With\nProcessPoolExecutor, you can also solve CPU-intensive problems on multiple cores\n—if the computations are “embarrassingly parallel”. This concludes our basic intro‐\nduction to concurrent.futures.\nChapter Summary\nWe started the chapter by comparing two concurrent HTTP clients with a sequential\none, demonstrating that the concurrent solutions show significant performance gains\nover the sequential script.\nAfter studying the first example based on concurrent.futures, we took a closer look\nat future objects, either instances of concurrent.futures.Future or asyncio\n.Future, emphasizing what these classes have in common (their differences will be\nemphasized in Chapter 21). We saw how to create futures by calling Executor.sub\nmit, and iterate over completed futures with concurrent.futures.as_completed.\nWe then discussed the use of multiple processes with the concurrent.futures.Proc\nessPoolExecutor class, to go around the GIL and use multiple CPU cores to simplify\nthe multicore prime checker we first saw in Chapter 19.\nIn the following section, we saw how the concurrent.futures.ThreadPoolExecutor\nworks with a didactic example, launching tasks that did nothing for a few seconds,\nexcept for displaying their status with a timestamp.\nNext we went back to the flag downloading examples. Enhancing them with a pro‐\ngress bar and proper error handling prompted further exploration of the\nfuture.as_completed generator function, showing a common pattern: storing\nfutures in a dict to link further information to them when submitting, so that we can\nuse that information when the future comes out of the as_completed iterator.\nFurther Reading\nThe concurrent.futures package was contributed by Brian Quinlan, who presented\nit in a great talk titled “The Future Is Soon!” at PyCon Australia 2010. Quinlan’s talk\nhas no slides; he shows what the library does by typing code directly in the Python\n772 \n| \nChapter 20: Concurrent Executors",
      "content_length": 2474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 803,
      "chapter": null,
      "content": "8 Slide #9 from “A Curious Course on Coroutines and Concurrency” tutorial presented at PyCon 2009.\nconsole. As a motivating example, the presentation features a short video with XKCD\ncartoonist/programmer Randall Munroe making an unintended DoS attack on Goo‐\ngle Maps to build a colored map of driving times around his city. The formal intro‐\nduction to the library is PEP 3148 - futures - execute computations asynchronously.\nIn the PEP, Quinlan wrote that the concurrent.futures library was “heavily influ‐\nenced by the Java java.util.concurrent package.”\nFor additional resources covering concurrent.futures, please see Chapter 19. All\nthe references that cover Python’s threading and multiprocessing in “Concurrency\nwith Threads and Processes” on page 734 also cover concurrent.futures.\nSoapbox\nThread Avoidance\nConcurrency: one of the most difficult topics in computer science (usually best\navoided).\n—David Beazley, Python instructor and mad scientist8\nI agree with the apparently contradictory quotes by David Beazley and Michele Sim‐\nionato at the start of this chapter.\nI attended an undergraduate course about concurrency. All we did was POSIX\nthreads programming. What I learned: I don’t want to manage threads and locks\nmyself, for the same reason that I don’t want to manage memory allocation and deal‐\nlocation. Those jobs are best carried out by the systems programmers who have the\nknow-how, the inclination, and the time to get them right—hopefully. I am paid to\ndevelop applications, not operating systems. I don’t need all the fine-grained control\nof threads, locks, malloc, and free—see “C dynamic memory allocation”.\nThat’s why I think the concurrent.futures package is interesting: it treats threads,\nprocesses, and queues as infrastructure at your service, not something you have to\ndeal with directly. Of course, it’s designed with simple jobs in mind, the so-called\nembarrassingly parallel problems. But that’s a large slice of the concurrency problems\nwe face when writing applications—as opposed to operating systems or database\nservers, as Simionato points out in that quote.\nFor “nonembarrassing” concurrency problems, threads and locks are not the answer\neither. Threads will never disappear at the OS level, but every programming language\nI’ve found exciting in the last several years provides higher-level, concurrency\nabstractions that are easier to use correctly, as the excellent Seven Concurrency Models\nin Seven Weeks book by Paul Butcher demonstrates. Go, Elixir, and Clojure are\nFurther Reading \n| \n773",
      "content_length": 2542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 804,
      "chapter": null,
      "content": "among them. Erlang—the implementation language of Elixir—is a prime example of\na language designed from the ground up with concurrency in mind. Erlang doesn’t\nexcite me for a simple reason: I find its syntax ugly. Python spoiled me that way.\nJosé Valim, previously a Ruby on Rails core contributor, designed Elixir with a pleas‐\nant, modern syntax. Like Lisp and Clojure, Elixir implements syntactic macros. That’s\na double-edged sword. Syntactic macros enable powerful DSLs, but the proliferation\nof sublanguages can lead to incompatible codebases and community fragmentation.\nLisp drowned in a flood of macros, with each Lisp shop using its own arcane dialect.\nStandardizing around Common Lisp resulted in a bloated language. I hope José\nValim can inspire the Elixir community to avoid a similar outcome. So far, it’s look‐\ning good. The Ecto database wrapper and query generator is a joy to use: a great\nexample of using macros to create a flexible yet user-friendly DSL—Domain-Specific\nLanguage—for interacting with relational and nonrelational databases.\nLike Elixir, Go is a modern language with fresh ideas. But, in some regards, it’s a con‐\nservative language, compared to Elixir. Go doesn’t have macros, and its syntax is sim‐\npler than Python’s. Go doesn’t support inheritance or operator overloading, and it\noffers fewer opportunities for metaprogramming than Python. These limitations are\nconsidered features. They lead to more predictable behavior and performance. That’s\na big plus in the highly concurrent, mission-critical settings where Go aims to replace\nC++, Java, and Python.\nWhile Elixir and Go are direct competitors in the high-concurrency space, their\ndesign philosophies appeal to different crowds. Both are likely to thrive. But in the\nhistory of programming languages, the conservative ones tend to attract more coders.\n774 \n| \nChapter 20: Concurrent Executors",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 805,
      "chapter": null,
      "content": "1 Videla & Williams, RabbitMQ in Action (Manning), Chapter 4, “Solving Problems with Rabbit: coding and\npatterns,” p. 61.\nCHAPTER 21\nAsynchronous Programming\nThe problem with normal approaches to asynchronous programming is that they’re\nall-or-nothing propositions. You rewrite all your code so none of it blocks or you’re\njust wasting your time.\nAlvaro Videla and Jason J. W. Williams, RabbitMQ in Action1\nThis chapter addresses three major topics that are closely related:\n• Python’s async def, await, async with, and async for constructs\n• Objects supporting those constructs: native coroutines and asynchronous var‐\niants of context managers, iterables, generators, and comprehensions\n• asyncio and other asynchronous libraries\nThis chapter builds on the ideas of iterables and generators (Chapter 17, in particular\n“Classic Coroutines” on page 641), context managers (Chapter 18), and general con‐\ncepts of concurrent programming (Chapter 19).\nWe’ll study concurrent HTTP clients similar to the ones we saw in Chapter 20,\nrewritten with native coroutines and asynchronous context managers, using the same\nHTTPX library as before, but now through its asynchronous API. We’ll also see how\nto avoid blocking the event loop by delegating slow operations to a thread or process\nexecutor.\nAfter the HTTP client examples, we’ll see two simple asynchronous server-side appli‐\ncations, one of them using the increasingly popular FastAPI framework. Then\nwe’ll cover other language constructs enabled by the async/await keywords:\n775",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 806,
      "chapter": null,
      "content": "2 Selivanov implemented async/await in Python, and wrote the related PEPs 492, 525, and 530.\nasynchronous generator functions, asynchronous comprehensions, and asynchro‐\nnous generator expressions. To emphasize the fact that those language features are\nnot tied to asyncio, we’ll see one example rewritten to use Curio—the elegant and\ninnovative asynchronous framework invented by David Beazley.\nTo wrap up the chapter, I wrote a brief section on the advantages and pitfalls of asyn‐\nchronous programming.\nThat’s a lot of ground to cover. We only have space for basic examples, but they will\nillustrate the most important features of each idea.\nThe asyncio documentation is much better after Yury Selivanov2\nreorganized it, separating the few functions useful to application\ndevelopers from the low-level API for creators of packages like web\nframeworks and database drivers.\nFor book-length coverage of asyncio, I recommend Using Asyncio\nin Python by Caleb Hattingh (O’Reilly). Full disclosure: Caleb is\none of the tech reviewers of this book.\nWhat’s New in This Chapter\nWhen I wrote the first edition of Fluent Python, the asyncio library was provisional\nand the async/await keywords did not exist. Therefore, I had to update all examples\nin this chapter. I also created new examples: domain probing scripts, a FastAPI web\nservice, and experiments with Python’s new asynchronous console mode.\nNew sections cover language features that did not exist at the time, such as native\ncoroutines, async with, async for, and the objects that support those constructs.\nThe ideas in “How Async Works and How It Doesn’t” on page 825 reflect hard-\nearned lessons that I consider essential reading for anyone using asynchronous pro‐\ngramming. They may save you a lot of trouble—whether you’re using Python or\nNode.js.\nFinally, I removed several paragraphs about asyncio.Futures, which is now consid‐\nered part of the low-level asyncio APIs.\n776 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 807,
      "chapter": null,
      "content": "3 There is one exception to this rule: if you run Python with the -m asyncio option, you can use await directly\nat the >>> prompt to drive a native coroutine. This is explained in “Experimenting with Python’s async con‐\nsole” on page 812.\n4 Sorry, I could not resist it.\nA Few Definitions\nAt the start of “Classic Coroutines” on page 641, we saw that Python 3.5 and later\noffer three kinds of coroutines:\nNative coroutine\nA coroutine function defined with async def. You can delegate from a native\ncoroutine to another native coroutine using the await keyword, similar to how\nclassic coroutines use yield from. The async def statement always defines a\nnative coroutine, even if the await keyword is not used in its body. The await\nkeyword cannot be used outside of a native coroutine.3\nClassic coroutine\nA generator function that consumes data sent to it via my_coro.send(data) calls,\nand reads that data by using yield in an expression. Classic coroutines can dele‐\ngate to other classic coroutines using yield from. Classic coroutines cannot be\ndriven by await, and are no longer supported by asyncio.\nGenerator-based coroutine\nA generator function decorated with @types.coroutine—introduced in Python\n3.5. That decorator makes the generator compatible with the new await\nkeyword.\nIn this chapter, we focus on native coroutines as well as asynchronous generators:\nAsynchronous generator\nA generator function defined with async def and using yield in its body. It\nreturns an asynchronous generator object that provides __anext__, a coroutine\nmethod to retrieve the next item.\n@asyncio.coroutine has No Future4\nThe @asyncio.coroutine decorator for classic coroutines and\ngenerator-based coroutines was deprecated in Python 3.8 and is\nscheduled for removal in Python 3.11, according to Issue 43216. In\ncontrast, @types.coroutine should remain, per Issue 36921. It is\nno longer supported by asyncio, but is used in low-level code in the\nCurio and Trio asynchronous frameworks.\nA Few Definitions \n| \n777",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 808,
      "chapter": null,
      "content": "5 true.dev is available for USD 360/year as I write this. I see that for.dev is registered, but has no DNS config‐\nured.\nAn asyncio Example: Probing Domains\nImagine you are about to start a new blog on Python, and you plan to register a\ndomain using a Python keyword and the .DEV suffix—for example: AWAIT.DEV.\nExample 21-1 is a script using asyncio to check several domains concurrently. This is\nthe output it produces:\n$ python3 blogdom.py\n  with.dev\n+ elif.dev\n+ def.dev\n  from.dev\n  else.dev\n  or.dev\n  if.dev\n  del.dev\n+ as.dev\n  none.dev\n  pass.dev\n  true.dev\n+ in.dev\n+ for.dev\n+ is.dev\n+ and.dev\n+ try.dev\n+ not.dev\nNote that the domains appear unordered. If you run the script, you’ll see them dis‐\nplayed one after the other, with varying delays. The + sign indicates your machine\nwas able to resolve the domain via DNS. Otherwise, the domain did not resolve and\nmay be available.5\nIn blogdom.py, the DNS probing is done via native coroutine objects. Because the\nasynchronous operations are interleaved, the time needed to check the 18 domains is\nmuch less than checking them sequentially. In fact, the total time is practically the\nsame as the time for the single slowest DNS response, instead of the sum of the times\nof all responses.\nExample 21-1 shows the code for blogdom.py.\nExample 21-1. blogdom.py: search for domains for a Python blog\n#!/usr/bin/env python3\nimport asyncio\nimport socket\n778 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 809,
      "chapter": null,
      "content": "from keyword import kwlist\nMAX_KEYWORD_LEN = 4  \nasync def probe(domain: str) -> tuple[str, bool]:  \n    loop = asyncio.get_running_loop()  \n    try:\n        await loop.getaddrinfo(domain, None)  \n    except socket.gaierror:\n        return (domain, False)\n    return (domain, True)\nasync def main() -> None:  \n    names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN)  \n    domains = (f'{name}.dev'.lower() for name in names)  \n    coros = [probe(domain) for domain in domains]  \n    for coro in asyncio.as_completed(coros):  \n        domain, found = await coro  \n        mark = '+' if found else ' '\n        print(f'{mark} {domain}')\nif __name__ == '__main__':\n    asyncio.run(main())  \nSet maximum length of keyword for domains, because shorter is better.\nprobe returns a tuple with the domain name and a boolean; True means the\ndomain resolved. Returning the domain name will make it easier to display the\nresults.\nGet a reference to the asyncio event loop, so we can use it next.\nThe loop.getaddrinfo(…) coroutine-method returns a five-part tuple of param‐\neters to connect to the given address using a socket. In this example, we don’t\nneed the result. If we got it, the domain resolves; otherwise, it doesn’t.\nmain must be a coroutine, so that we can use await in it.\nGenerator to yield Python keywords with length up to MAX_KEYWORD_LEN.\nGenerator to yield domain names with the .dev suffix.\nBuild a list of coroutine objects by invoking the probe coroutine with each\ndomain argument.\nAn asyncio Example: Probing Domains \n| \n779",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 810,
      "chapter": null,
      "content": "asyncio.as_completed is a generator that yields coroutines that return the\nresults of the coroutines passed to it in the order they are completed—not the\norder they were submitted. It’s similar to futures.as_completed, which we saw\nin Chapter 20, Example 20-4.\nAt this point, we know the coroutine is done because that’s how as_completed\nworks. Therefore, the await expression will not block but we need it to get the\nresult from coro. If coro raised an unhandled exception, it would be re-raised\nhere.\nasyncio.run starts the event loop and returns only when the event loop exits.\nThis is a common pattern for scripts that use asyncio: implement main as a\ncoroutine, and drive it with asyncio.run inside the if __name__ ==\n'__main__': block.\nThe asyncio.get_running_loop function was added in Python 3.7\nfor use inside coroutines, as shown in probe. If there’s no running\nloop, asyncio.get_running_loop raises RuntimeError. Its imple‐\nmentation is simpler and faster than asyncio.get_event_loop,\nwhich may start an event loop if necessary. Since Python 3.10, asyn\ncio.get_event_loop is deprecated and will eventually become an\nalias to asyncio.get_running_loop.\nGuido’s Trick to Read Asynchronous Code\nThere are a lot of new concepts to grasp in asyncio, but the overall logic of\nExample 21-1 is easy to follow if you employ a trick suggested by Guido van Rossum\nhimself: squint and pretend the async and await keywords are not there. If you do\nthat, you’ll realize that coroutines read like plain old sequential functions.\nFor example, imagine that the body of this coroutine…\nasync def probe(domain: str) -> tuple[str, bool]:\n    loop = asyncio.get_running_loop()\n    try:\n        await loop.getaddrinfo(domain, None)\n    except socket.gaierror:\n        return (domain, False)\n    return (domain, True)\n…works like the following function, except that it magically never blocks:\ndef probe(domain: str) -> tuple[str, bool]:  # no async\n    loop = asyncio.get_running_loop()\n    try:\n        loop.getaddrinfo(domain, None)  # no await\n780 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 811,
      "chapter": null,
      "content": "except socket.gaierror:\n        return (domain, False)\n    return (domain, True)\nUsing the syntax await loop.getaddrinfo(...) avoids blocking because await sus‐\npends the current coroutine object. For example, during the execution of the\nprobe('if.dev') coroutine, a new coroutine object is created by getad\ndrinfo('if.dev', None). Awaiting it starts the low-level addrinfo query and yields\ncontrol back to the event loop, not to the probe(‘if.dev’) coroutine, which is sus‐\npended. The event loop can then drive other pending coroutine objects, such as\nprobe('or.dev').\nWhen the event loop gets a response for the getaddrinfo('if.dev', None) query,\nthat specific coroutine object resumes and returns control back to the\nprobe('if.dev')—which was suspended at await—and can now handle a possible\nexception and return the result tuple.\nSo far, we’ve only seen asyncio.as_completed and await applied to coroutines. But\nthey handle any awaitable object. That concept is explained next.\nNew Concept: Awaitable\nThe for keyword works with iterables. The await keyword works with awaitables.\nAs an end user of asyncio, these are the awaitables you will see on a daily basis:\n• A native coroutine object, which you get by calling a native coroutine function\n• An asyncio.Task, which you usually get by passing a coroutine object to asyn\ncio.create_task()\nHowever, end-user code does not always need to await on a Task. We use asyn\ncio.create_task(one_coro()) to schedule one_coro for concurrent execution,\nwithout waiting for its return. That’s what we did with the spinner coroutine in spin‐\nner_async.py (Example 19-4). If you don’t expect to cancel the task or wait for it,\nthere is no need to keep the Task object returned from create_task. Creating the\ntask is enough to schedule the coroutine to run.\nIn contrast, we use await other_coro() to run other_coro right now and wait for\nits completion because we need its result before we can proceed. In spinner_async.py,\nthe supervisor coroutine did res = await slow() to execute slow and get its result.\nWhen implementing asynchronous libraries or contributing to asyncio itself, you\nmay also deal with these lower-level awaitables:\nNew Concept: Awaitable \n| \n781",
      "content_length": 2208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 812,
      "chapter": null,
      "content": "• An object with an __await__ method that returns an iterator; for example, an\nasyncio.Future instance (asyncio.Task is a subclass of asyncio.Future)\n• Objects written in other languages using the Python/C API with a\ntp_as_async.am_await function, returning an iterator (similar to __await__\nmethod)\nExisting codebases may also have one additional kind of awaitable: generator-based\ncoroutine objects—which are in the process of being deprecated.\nPEP 492 states that the await expression “uses the yield from\nimplementation with an extra step of validating its argument” and\n“await only accepts an awaitable.” The PEP does not explain that\nimplementation in detail, but refers to PEP 380, which introduced\nyield from. I posted a detailed explanation in “Classic Corou‐\ntines”, section “The Meaning of yield from”, at fluentpython.com.\nNow let’s study the asyncio version of a script that downloads a fixed set of flag\nimages.\nDownloading with asyncio and HTTPX\nThe flags_asyncio.py script downloads a fixed set of 20 flags from fluentpython.com.\nWe first mentioned it in “Concurrent Web Downloads” on page 744, but now we’ll\nstudy it in detail, applying the concepts we just saw.\nAs of Python 3.10, asyncio only supports TCP and UDP directly, and there are no\nasynchronous HTTP client or server packages in the standard library. I am using\nHTTPX in all the HTTP client examples.\nWe’ll explore flags_asyncio.py from the bottom up—that is, looking first at the func‐\ntions that set up the action in Example 21-2.\nTo make the code easier to read, flags_asyncio.py has no error han‐\ndling. As we introduce async/await, it’s useful to focus on the\n“happy path” initially, to understand how regular functions and\ncoroutines are arranged in a program. Starting with “Enhancing\nthe asyncio Downloader” on page 787, the examples include error\nhandling and more features.\nThe flags_.py examples from this chapter and Chapter 20 share\ncode and data, so I put them together in the example-code-2e/20-\nexecutors/getflags directory.\n782 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 813,
      "chapter": null,
      "content": "Example 21-2. flags_asyncio.py: startup functions\ndef download_many(cc_list: list[str]) -> int:    \n    return asyncio.run(supervisor(cc_list))      \nasync def supervisor(cc_list: list[str]) -> int:\n    async with AsyncClient() as client:          \n        to_do = [download_one(client, cc)\n                 for cc in sorted(cc_list)]      \n        res = await asyncio.gather(*to_do)       \n    return len(res)                              \nif __name__ == '__main__':\n    main(download_many)\nThis needs to be a plain function—not a coroutine—so it can be passed to and\ncalled by the main function from the flags.py module (Example 20-2).\nExecute the event loop driving the supervisor(cc_list) coroutine object until\nit returns. This will block while the event loop runs. The result of this line is\nwhatever supervisor returns.\nAsynchronous HTTP client operations in httpx are methods of AsyncClient,\nwhich is also an asynchronous context manager: a context manager with asyn‐\nchronous setup and teardown methods (more about this in “Asynchronous Con‐\ntext Managers” on page 786).\nBuild a list of coroutine objects by calling the download_one coroutine once for\neach flag to be retrieved.\nWait for the asyncio.gather coroutine, which accepts one or more awaitable\narguments and waits for all of them to complete, returning a list of results for the\ngiven awaitables in the order they were submitted.\nsupervisor returns the length of the list returned by asyncio.gather.\nNow let’s review the top of flags_asyncio.py (Example 21-3). I reorganized the corou‐\ntines so we can read them in the order they are started by the event loop.\nExample 21-3. flags_asyncio.py: imports and download functions\nimport asyncio\nfrom httpx import AsyncClient  \nfrom flags import BASE_URL, save_flag, main  \nDownloading with asyncio and HTTPX \n| \n783",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 814,
      "chapter": null,
      "content": "async def download_one(client: AsyncClient, cc: str):  \n    image = await get_flag(client, cc)\n    save_flag(image, f'{cc}.gif')\n    print(cc, end=' ', flush=True)\n    return cc\nasync def get_flag(client: AsyncClient, cc: str) -> bytes:  \n    url = f'{BASE_URL}/{cc}/{cc}.gif'.lower()\n    resp = await client.get(url, timeout=6.1,\n                                  follow_redirects=True)  \n    return resp.read()  \nhttpx must be installed—it’s not in the standard library.\nReuse code from flags.py (Example 20-2).\ndownload_one must be a native coroutine, so it can await on get_flag—which\ndoes the HTTP request. Then it displays the code of the downloaded flag, and\nsaves the image.\nget_flag needs to receive the AsyncClient to make the request.\nThe get method of an httpx.AsyncClient instance returns a ClientResponse\nobject that is also an asynchronous context manager.\nNetwork I/O operations are implemented as coroutine methods, so they are\ndriven asynchronously by the asyncio event loop.\nFor better performance, the save_flag call inside get_flag should\nbe asynchronous, to avoid blocking the event loop. However, asyn‐\ncio does not provide an asynchronous filesystem API at this time—\nas Node.js does.\n“Using asyncio.as_completed and a Thread” on page 788 will show\nhow to delegate save_flag to a thread.\nYour code delegates to the httpx coroutines explicitly through await or implicitly\nthrough the special methods of the asynchronous context managers, such as Async\nClient and ClientResponse—as we’ll see in “Asynchronous Context Managers” on\npage 786.\nThe Secret of Native Coroutines: Humble Generators\nA key difference between the classic coroutine examples we saw in “Classic Corou‐\ntines” on page 641 and flags_asyncio.py is that there are no visible .send() calls or\n784 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 815,
      "chapter": null,
      "content": "yield expressions in the latter. Your code sits between the asyncio library and the\nasynchronous libraries you are using, such as HTTPX. This is illustrated in\nFigure 21-1.\nFigure 21-1. In an asynchronous program, a user’s function starts the event loop, sched‐\nuling an initial coroutine with asyncio.run. Each user’s coroutine drives the next with\nan await expression, forming a channel that enables communication between a library\nlike HTTPX and the event loop.\nUnder the hood, the asyncio event loop makes the .send calls that drive your corou‐\ntines, and your coroutines await on other coroutines, including library coroutines.\nAs mentioned, await borrows most of its implementation from yield from, which\nalso makes .send calls to drive coroutines.\nThe await chain eventually reaches a low-level awaitable, which returns a generator\nthat the event loop can drive in response to events such as timers or network I/O. The\nlow-level awaitables and generators at the end of these await chains are implemented\ndeep into the libraries, are not part of their APIs, and may be Python/C extensions.\nUsing functions like asyncio.gather and asyncio.create_task, you can start mul‐\ntiple concurrent await channels, enabling concurrent execution of multiple I/O oper‐\nations driven by a single event loop, in a single thread.\nThe All-or-Nothing Problem\nNote that in Example 21-3, I could not reuse the get_flag function from flags.py\n(Example 20-2). I had to rewrite it as a coroutine to use the asynchronous API of\nHTTPX. For peak performance with asyncio, we must replace every function that\ndoes I/O with an asynchronous version that is activated with await or asyncio.cre\nate_task, so that control is given back to the event loop while the function waits for\nI/O. If you can’t rewrite a blocking function as a coroutine, you should run it in a\nseparate thread or process, as we’ll see in “Delegating Tasks to Executors” on page 797.\nDownloading with asyncio and HTTPX \n| \n785",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 816,
      "chapter": null,
      "content": "That’s why I chose the epigraph for this chapter, which includes this advice: “You\nrewrite all your code so none of it blocks or you’re just wasting your time.”\nFor the same reason, I could not reuse the download_one function from flags_thread‐\npool.py (Example 20-3) either. The code in Example 21-3 drives get_flag with\nawait, so download_one must also be a coroutine. For each request, a download_one\ncoroutine object is created in supervisor, and they are all driven by the\nasyncio.gather coroutine.\nNow let’s study the async with statement that appeared in supervisor\n(Example 21-2) and get_flag (Example 21-3).\nAsynchronous Context Managers\nIn “Context Managers and with Blocks” on page 658, we saw how an object can be\nused to run code before and after the body of a with block, if its class provides the\n__enter__ and __exit__ methods.\nNow, consider Example 21-4, from the asyncpg asyncio-compatible PostgreSQL\ndriver documentation on transactions.\nExample 21-4. Sample code from the documentation of the asyncpg PostgreSQL driver\ntr = connection.transaction()\nawait tr.start()\ntry:\n    await connection.execute(\"INSERT INTO mytable VALUES (1, 2, 3)\")\nexcept:\n    await tr.rollback()\n    raise\nelse:\n    await tr.commit()\nA database transaction is a natural fit for the context manager protocol: the transac‐\ntion has to be started, data is changed with connection.execute, and then a rollback\nor commit must happen, depending on the outcome of the changes.\nIn an asynchronous driver like asyncpg, the setup and wrap-up need to be coroutines\nso that other operations can happen concurrently. However, the implementation of\nthe classic with statement doesn’t support coroutines doing the work of __enter__ or\n__exit__.\nThat’s why PEP 492—Coroutines with async and await syntax introduced the async\nwith statement, which works with asynchronous context managers: objects imple‐\nmenting the __aenter__ and __aexit__ methods as coroutines.\n786 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 817,
      "chapter": null,
      "content": "6 This tip is quoted verbatim from a comment by tech reviewer Caleb Hattingh. Thanks, Caleb!\nWith async with, Example 21-4 can be written like this other snippet from the\nasyncpg documentation:\nasync with connection.transaction():\n    await connection.execute(\"INSERT INTO mytable VALUES (1, 2, 3)\")\nIn the asyncpg.Transaction class, the __aenter__ coroutine method does await\nself.start(), and the __aexit__ coroutine awaits on private __rollback or __com\nmit coroutine methods, depending on whether an exception occurred or not. Using\ncoroutines to implement Transaction as an asynchronous context manager allows\nasyncpg to handle many transactions concurrently.\nCaleb Hattingh on asyncpg\nAnother really great thing about asyncpg is that it also works\naround PostgreSQL’s lack of high-concurrency support (it uses one\nserver-side process per connection) by implementing a connection\npool for internal connections to Postgres itself.\nThis means you don’t need additional tools like pgbouncer as\nexplained in the asyncpg documentation.6\nBack to flags_asyncio.py, the AsyncClient class of httpx is an asynchronous context\nmanager, so it can use awaitables in its __aenter__ and __aexit__ special coroutine\nmethods.\n“Asynchronous generators as context managers” on page 817 shows\nhow to use Python’s contextlib to create an asynchronous context\nmanager without having to write a class. That explanation comes\nlater in this chapter because of a prerequisite topic: “Asynchronous\nGenerator Functions” on page 812.\nWe’ll now enhance the asyncio flag download example with a progress bar, which will\nlead us to explore a bit more of the asyncio API.\nEnhancing the asyncio Downloader\nRecall from “Downloads with Progress Display and Error Handling” on page 762 that\nthe flags2 set of examples share the same command-line interface, and they display\na progress bar while the downloads are happening. They also include error handling.\nEnhancing the asyncio Downloader \n| \n787",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 818,
      "chapter": null,
      "content": "I encourage you to play with the flags2 examples to develop an\nintuition of how concurrent HTTP clients perform. Use the -h\noption to see the help screen in Example 20-10. Use the -a, -e, and\n-l command-line options to control the number of downloads,\nand the -m option to set the number of concurrent downloads. Run\ntests against the LOCAL, REMOTE, DELAY, and ERROR servers. Discover\nthe optimum number of concurrent downloads to maximize\nthroughput against each server. Tweak the options for the test\nservers, as described in “Setting Up Test Servers” on page 765.\nFor instance, Example 21-5 shows an attempt to get 100 flags (-al 100) from the\nERROR server, using 100 concurrent requests (-m 100). The 48 errors in the result\nare either HTTP 418 or time-out errors—the expected (mis)behavior of the\nslow_server.py.\nExample 21-5. Running flags2_asyncio.py\n$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100\nERROR site: http://localhost:8002/flags\nSearching for 100 flags: from AD to LK\n100 concurrent connections will be used.\n100%|█████████████████████████████████████████| 100/100 [00:03<00:00, 30.48it/s]\n--------------------\n 52 flags downloaded.\n 48 errors.\nElapsed time: 3.31s\nAct Responsibly When Testing Concurrent Clients\nEven if the overall download time is not much different between\nthe threaded and asyncio HTTP clients, asyncio can send requests\nfaster, so it’s more likely that the server will suspect a DoS attack.\nTo really exercise these concurrent clients at full throttle, please use\nlocal HTTP servers for testing, as explained in “Setting Up Test\nServers” on page 765.\nNow let’s see how flags2_asyncio.py is implemented.\nUsing asyncio.as_completed and a Thread\nIn Example 21-3, we passed several coroutines to asyncio.gather, which returns a\nlist with results of the coroutines in the order they were submitted. This means that\nasyncio.gather can only return when all the awaitables are done. However, to\nupdate a progress bar, we need to get results as they are done.\n788 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 819,
      "chapter": null,
      "content": "Fortunately, there is an asyncio equivalent of the as_completed generator function\nwe used in the thread pool example with the progress bar (Example 20-16).\nExample 21-6 shows the top of the flags2_asyncio.py script where the get_flag and\ndownload_one coroutines are defined. Example 21-7 lists the rest of the source, with\nsupervisor and download_many. This script is longer than flags_asyncio.py because\nof error handling.\nExample 21-6. flags2_asyncio.py: top portion of the script; remaining code is in\nExample 21-7\nimport asyncio\nfrom collections import Counter\nfrom http import HTTPStatus\nfrom pathlib import Path\nimport httpx\nimport tqdm  # type: ignore\nfrom flags2_common import main, DownloadStatus, save_flag\n# low concurrency default to avoid errors from remote site,\n# such as 503 - Service Temporarily Unavailable\nDEFAULT_CONCUR_REQ = 5\nMAX_CONCUR_REQ = 1000\nasync def get_flag(client: httpx.AsyncClient,  \n                   base_url: str,\n                   cc: str) -> bytes:\n    url = f'{base_url}/{cc}/{cc}.gif'.lower()\n    resp = await client.get(url, timeout=3.1, follow_redirects=True)   \n    resp.raise_for_status()\n    return resp.content\nasync def download_one(client: httpx.AsyncClient,\n                       cc: str,\n                       base_url: str,\n                       semaphore: asyncio.Semaphore,\n                       verbose: bool) -> DownloadStatus:\n    try:\n        async with semaphore:  \n            image = await get_flag(client, base_url, cc)\n    except httpx.HTTPStatusError as exc:  \n        res = exc.response\n        if res.status_code == HTTPStatus.NOT_FOUND:\n            status = DownloadStatus.NOT_FOUND\n            msg = f'not found: {res.url}'\n        else:\n            raise\n    else:\nEnhancing the asyncio Downloader \n| \n789",
      "content_length": 1781,
      "extraction_method": "Direct"
    },
    {
      "page_number": 820,
      "chapter": null,
      "content": "await asyncio.to_thread(save_flag, image, f'{cc}.gif')  \n        status = DownloadStatus.OK\n        msg = 'OK'\n    if verbose and msg:\n        print(cc, msg)\n    return status\nget_flag is very similar to the sequential version in Example 20-14. First differ‐\nence: it requires the client parameter.\nSecond and third differences: .get is an AsyncClient method, and it’s a corou‐\ntine, so we need to await it.\nUse the semaphore as an asynchronous context manager so that the program as a\nwhole is not blocked; only this coroutine is suspended when the semaphore\ncounter is zero. More about this in “Python’s Semaphores” on page 791.\nThe error handling logic is the same as in download_one, from Example 20-14.\nSaving the image is an I/O operation. To avoid blocking the event loop, run\nsave_flag in a thread.\nAll network I/O is done with coroutines in asyncio, but not file I/O. However, file\nI/O is also “blocking”—in the sense that reading/writing files takes thousands of\ntimes longer than reading/writing to RAM. If you’re using Network-Attached Stor‐\nage, it may even involve network I/O under the covers.\nSince Python 3.9, the asyncio.to_thread coroutine makes it easy to delegate file I/O\nto a thread pool provided by asyncio. If you need to support Python 3.7 or 3.8, “Dele‐\ngating Tasks to Executors” on page 797 shows how to add a couple of lines to do it. But\nfirst, let’s finish our study of the HTTP client code.\nThrottling Requests with a Semaphore\nNetwork clients like the ones we are studying should be throttled (i.e., limited) to\navoid pounding the server with too many concurrent requests.\nA semaphore is a synchronization primitive, more flexible than a lock. A semaphore\ncan be held by multiple coroutines, with a configurable maximum number. This\nmakes it ideal to throttle the number of active concurrent coroutines. “Python’s Sem‐\naphores” on page 791 has more information.\nIn flags2_threadpool.py (Example 20-16), the throttling was done by instantiating the\nThreadPoolExecutor with the required max_workers argument set to concur_req in\nthe download_many function. In flags2_asyncio.py, an asyncio.Semaphore is created\n790 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 821,
      "chapter": null,
      "content": "7 Thanks to Guto Maia who noted that the concept of a semaphore was not explained when he read the first\nedition draft for this chapter.\nby the supervisor function (shown in Example 21-7) and passed as the semaphore\nargument to download_one in Example 21-6.\nPython’s Semaphores\nComputer scientist Edsger W. Dijkstra invented the semaphore in the early 1960s. It’s\na simple idea, but it’s so flexible that most other synchronization objects—such as\nlocks and barriers—can be built on top of semaphores. There are three Semaphore\nclasses in Python’s standard library: one in threading, another in multiprocessing,\nand a third one in asyncio. Here we’ll describe the latter.\nAn asyncio.Semaphore has an internal counter that is decremented whenever we\nawait on the .acquire() coroutine method, and incremented when we call\nthe .release() method—which is not a coroutine because it never blocks. The initial\nvalue of the counter is set when the Semaphore is instantiated:\n    semaphore = asyncio.Semaphore(concur_req)\nAwaiting on .acquire() causes no delay when the counter is greater than zero, but if\nthe counter is zero, .acquire() suspends the awaiting coroutine until some other\ncoroutine calls .release() on the same Semaphore, thus incrementing the counter.\nInstead of using those methods directly, it’s safer to use the semaphore as an asyn‐\nchronous context manager, as I did in Example 21-6, function download_one:\n        async with semaphore:\n            image = await get_flag(client, base_url, cc)\nThe Semaphore.__aenter__ coroutine method awaits for .acquire(), and its\n__aexit__ coroutine method calls .release(). That snippet guarantees that no more\nthan concur_req instances of get_flags coroutines will be active at any time.\nEach of the Semaphore classes in the standard library has a BoundedSemaphore\nsubclass that enforces an additional constraint: the internal counter can never\nbecome larger than the initial value when there are more .release()\nthan .acquire() operations.7\nNow let’s take a look at the rest of the script in Example 21-7.\nExample 21-7. flags2_asyncio.py: script continued from Example 21-6\nasync def supervisor(cc_list: list[str],\n                     base_url: str,\nEnhancing the asyncio Downloader \n| \n791",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 822,
      "chapter": null,
      "content": "verbose: bool,\n                     concur_req: int) -> Counter[DownloadStatus]:  \n    counter: Counter[DownloadStatus] = Counter()\n    semaphore = asyncio.Semaphore(concur_req)  \n    async with httpx.AsyncClient() as client:\n        to_do = [download_one(client, cc, base_url, semaphore, verbose)\n                 for cc in sorted(cc_list)]  \n        to_do_iter = asyncio.as_completed(to_do)  \n        if not verbose:\n            to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list))  \n        error: httpx.HTTPError | None = None  \n        for coro in to_do_iter:  \n            try:\n                status = await coro  \n            except httpx.HTTPStatusError as exc:\n                error_msg = 'HTTP error {resp.status_code} - {resp.reason_phrase}'\n                error_msg = error_msg.format(resp=exc.response)\n                error = exc  \n            except httpx.RequestError as exc:\n                error_msg = f'{exc} {type(exc)}'.strip()\n                error = exc  \n            except KeyboardInterrupt:\n                break\n            if error:\n                status = DownloadStatus.ERROR  \n                if verbose:\n                    url = str(error.request.url)  \n                    cc = Path(url).stem.upper()   \n                    print(f'{cc} error: {error_msg}')\n            counter[status] += 1\n    return counter\ndef download_many(cc_list: list[str],\n                  base_url: str,\n                  verbose: bool,\n                  concur_req: int) -> Counter[DownloadStatus]:\n    coro = supervisor(cc_list, base_url, verbose, concur_req)\n    counts = asyncio.run(coro)  \n    return counts\nif __name__ == '__main__':\n    main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)\nsupervisor takes the same arguments as the download_many function, but it can‐\nnot be invoked directly from main because it’s a coroutine and not a plain func‐\ntion like download_many.\n792 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 823,
      "chapter": null,
      "content": "Create an asyncio.Semaphore that will not allow more than concur_req active\ncoroutines among those using this semaphore. The value of concur_req is com‐\nputed by the main function from flags2_common.py, based on command-line\noptions and constants set in each example.\nCreate a list of coroutine objects, one per call to the download_one coroutine.\nGet an iterator that will return coroutine objects as they are done. I did not place\nthis call to as_completed directly in the for loop below because I may need to\nwrap it with the tqdm iterator for the progress bar, depending on the user’s choice\nfor verbosity.\nWrap the as_completed iterator with the tqdm generator function to display\nprogress.\nDeclare and initialize error with None; this variable will be used to hold an\nexception beyond the try/except statement, if one is raised.\nIterate over the completed coroutine objects; this loop is similar to the one in\ndownload_many in Example 20-16.\nawait on the coroutine to get its result. This will not block because as_comple\nted only produces coroutines that are done.\nThis assignment is necessary because the exc variable scope is limited to this\nexcept clause, but I need to preserve its value for later.\nSame as before.\nIf there was an error, set the status.\nIn verbose mode, extract the URL from the exception that was raised…\n…and extract the name of the file to display the country code next.\ndownload_many instantiates the supervisor coroutine object and passes it to the\nevent loop with asyncio.run, collecting the counter supervisor returns when\nthe event loop ends.\nIn Example 21-7, we could not use the mapping of futures to country codes we saw in\nExample 20-16, because the awaitables returned by asyncio.as_completed are\nthe same awaitables we pass into the as_completed call. Internally, the asyncio\nEnhancing the asyncio Downloader \n| \n793",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 824,
      "chapter": null,
      "content": "8 A detailed discussion about this can be found in a thread I started in the python-tulip group, titled “Which\nother futures may come out of asyncio.as_completed?”. Guido responds, and gives insight on the implemen‐\ntation of as_completed, as well as the close relationship between futures and coroutines in asyncio.\nmachinery may replace the awaitables we provide with others that will, in the end,\nproduce the same results.8\nBecause I could not use the awaitables as keys to retrieve the coun‐\ntry code from a dict in case of failure, I had to extract the country\ncode from the exception. To do that, I kept the exception in the\nerror variable to retrieve outside of the try/except statement.\nPython is not a block-scoped language: statements such as loops\nand try/except don’t create a local scope in the blocks they man‐\nage. But if an except clause binds an exception to a variable, like\nthe exc variables we just saw—that binding only exists within the\nblock inside that particular except clause.\nThis wraps up the discussion of an asyncio example functionally equivalent to the\nflags2_threadpool.py we saw earlier.\nThe next example demonstrates the simple pattern of executing one asynchronous\ntask after another using coroutines. This deserves our attention because anyone with\nprevious experience with JavaScript knows that running one asynchronous function\nafter the other was the reason for the nested coding pattern known as pyramid of\ndoom. The await keyword makes that curse go away. That’s why await is now part of\nPython and JavaScript.\nMaking Multiple Requests for Each Download\nSuppose you want to save each country flag with the name of the country and the\ncountry code, instead of just the country code. Now you need to make two HTTP\nrequests per flag: one to get the flag image itself, the other to get the metadata.json file\nin the same directory as the image—that’s where the name of the country is recorded.\nCoordinating multiple requests in the same task is easy in the threaded script: just\nmake one request then the other, blocking the thread twice, and keeping both pieces\nof data (country code and name) in local variables, ready to use when saving the files.\nIf you needed to do the same in an asynchronous script with callbacks, you needed\nnested functions so that the country code and name were available in their closures\nuntil you could save the file, because each callback runs in a different local scope. The\nawait keyword provides relief from that, allowing you to drive the asynchronous\nrequests one after the other, sharing the local scope of the driving coroutine.\n794 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 825,
      "chapter": null,
      "content": "If you are doing asynchronous application programming in\nmodern Python with lots of callbacks, you are probably applying\nold patterns that don’t make sense in modern Python. That is justi‐\nfied if you are writing a library that interfaces with legacy or low-\nlevel \ncode \nthat \ndoes \nnot \nsupport \ncoroutines. \nAnyway,\nthe \nStackOverflow \nQ&A, \n“What \nis \nthe \nuse \ncase \nfor\nfuture.add_done_callback()?” explains why callbacks are needed in\nlow-level code, but are not very useful in Python application-level\ncode these days.\nThe third variation of the asyncio flag downloading script has a few changes:\nget_country\nThis new coroutine fetches the metadata.json file for the country code, and gets\nthe name of the country from it.\ndownload_one\nThis coroutine now uses await to delegate to get_flag and the new get_country\ncoroutine, using the result of the latter to build the name of the file to save.\nLet’s start with the code for get_country (Example 21-8). Note that it is very similar\nto get_flag from Example 21-6.\nExample 21-8. flags3_asyncio.py: get_country coroutine\nasync def get_country(client: httpx.AsyncClient,\n                      base_url: str,\n                      cc: str) -> str:    \n    url = f'{base_url}/{cc}/metadata.json'.lower()\n    resp = await client.get(url, timeout=3.1, follow_redirects=True)\n    resp.raise_for_status()\n    metadata = resp.json()  \n    return metadata['country']  \nThis coroutine returns a string with the country name—if all goes well.\nmetadata will get a Python dict built from the JSON contents of the response.\nReturn the country name.\nNow let’s see the modified download_one in Example 21-9, which has only a few lines\nchanged from the same coroutine in Example 21-6.\nEnhancing the asyncio Downloader \n| \n795",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 826,
      "chapter": null,
      "content": "Example 21-9. flags3_asyncio.py: download_one coroutine\nasync def download_one(client: httpx.AsyncClient,\n                       cc: str,\n                       base_url: str,\n                       semaphore: asyncio.Semaphore,\n                       verbose: bool) -> DownloadStatus:\n    try:\n        async with semaphore:  \n            image = await get_flag(client, base_url, cc)\n        async with semaphore:  \n            country = await get_country(client, base_url, cc)\n    except httpx.HTTPStatusError as exc:\n        res = exc.response\n        if res.status_code == HTTPStatus.NOT_FOUND:\n            status = DownloadStatus.NOT_FOUND\n            msg = f'not found: {res.url}'\n        else:\n            raise\n    else:\n        filename = country.replace(' ', '_')  \n        await asyncio.to_thread(save_flag, image, f'{filename}.gif')\n        status = DownloadStatus.OK\n        msg = 'OK'\n    if verbose and msg:\n        print(cc, msg)\n    return status\nHold the semaphore to await for get_flag…\n…and again for get_country.\nUse the country name to create a filename. As a command-line user, I don’t like\nto see spaces in filenames.\nMuch better than nested callbacks!\nI put the calls to get_flag and get_country in separate with blocks controlled by the\nsemaphore because it’s good practice to hold semaphores and locks for the shortest\npossible time.\nI could schedule both get_flag and get_country in parallel using asyncio.gather,\nbut if get_flag raises an exception, there is no image to save, so it’s pointless to run\nget_country. But there are cases where it makes sense to use asyncio.gather to hit\nseveral APIs at the same time instead of waiting for one response before making the\nnext request.\nIn flags3_asyncio.py, the await syntax appears six times, and async with three times.\nHopefully, you should be getting the hang of asynchronous programming in Python.\n796 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 827,
      "chapter": null,
      "content": "One challenge is to know when you have to use await and when you can’t use it. The\nanswer in principle is easy: you await coroutines and other awaitables, such as asyn\ncio.Task instances. But some APIs are tricky, mixing coroutines and plain functions\nin seemingly arbitrary ways, like the StreamWriter class we’ll use in Example 21-14.\nExample 21-9 wrapped up the flags set of examples. Now let’s discuss the use of\nthread or process executors in asynchronous programming.\nDelegating Tasks to Executors\nOne important advantage of Node.js over Python for asynchronous programming is\nthe Node.js standard library, which provides async APIs for all I/O—not just for net‐\nwork I/O. In Python, if you’re not careful, file I/O can seriously degrade the perfor‐\nmance of asynchronous applications, because reading and writing to storage in the\nmain thread blocks the event loop.\nIn the download_one coroutine of Example 21-6, I used this line to save the downloa‐\nded image to disk:\n        await asyncio.to_thread(save_flag, image, f'{cc}.gif')\nAs mentioned before, the asyncio.to_thread was added in Python 3.9. If you need\nto support 3.7 or 3.8, then replace that single line with the lines in Example 21-10.\nExample 21-10. Lines to use instead of await asyncio.to_thread\n        loop = asyncio.get_running_loop()         \n        loop.run_in_executor(None, save_flag,     \n                             image, f'{cc}.gif')  \nGet a reference to the event loop.\nThe first argument is the executor to use; passing None selects the default Thread\nPoolExecutor that is always available in the asyncio event loop.\nYou can pass positional arguments to the function to run, but if you need to pass\nkeyword arguments, then you need to resort to functool.partial, as described\nin the run_in_executor documentation.\nThe newer asyncio.to_thread function is easier to use and more flexible, as it also\naccepts keyword arguments.\nThe implementation of asyncio itself uses run_in_executor under the hood in a few\nplaces. For example, the loop.getaddrinfo(…) coroutine we saw in Example 21-1\nis implemented by calling the getaddrinfo function from the socket module—\nDelegating Tasks to Executors \n| \n797",
      "content_length": 2185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 828,
      "chapter": null,
      "content": "which is a blocking function that may take seconds to return, as it depends on DNS\nresolution.\nA common pattern in asynchronous APIs is to wrap blocking calls that are imple‐\nmentation details in coroutines using run_in_executor internally. That way, you\nprovide a consistent interface of coroutines to be driven with await, and hide the\nthreads you need to use for pragmatic reasons. The Motor asynchronous driver for\nMongoDB has an API compatible with async/await that is really a façade around a\nthreaded core that talks to the database server. A. Jesse Jiryu Davis, the lead developer\nof Motor, explains his reasoning in “Response to ‘Asynchronous Python and Data‐\nbases’”. Spoiler: Davis discovered that a thread pool was more performant in the par‐\nticular use case of a database driver—despite the myth that asynchronous approaches\nare always faster than threads for network I/O.\nThe main reason to pass an explict Executor to loop.run_in_executor is to employ\na ProcessPoolExecutor if the function to execute is CPU intensive, so that it runs in\na different Python process, avoiding contention for the GIL. Because of the high\nstart-up cost, it would be better to start the ProcessPoolExecutor in the supervisor,\nand pass it to the coroutines that need to use it.\nCaleb Hattingh—the author of Using Asyncio in Python (O’ Reilly)—is one of the\ntech reviewers of this book and suggested I add the following warning about execu‐\ntors and asyncio.\nCaleb’s Warning about run_in_executors\nUsing run_in_executor can produce hard-to-debug problems\nsince cancellation doesn’t work the way one might expect. Corou‐\ntines that use executors give merely the pretense of cancellation:\nthe underlying thread (if it’s a ThreadPoolExecutor) has no cancel‐\nlation mechanism. For example, a long-lived thread that is created\ninside a run_in_executor call may prevent your asyncio program\nfrom shutting down cleanly: asyncio.run will wait for the executor\nto fully shut down before returning, and it will wait forever\nif the executor jobs don’t stop somehow on their own. My\ngreybeard inclination is to want that function to be named\nrun_in_executor_uncancellable.\nWe’ll now go from client scripts to writing servers with asyncio.\n798 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 829,
      "chapter": null,
      "content": "Writing asyncio Servers\nThe classic toy example of a TCP server is an echo server. We’ll build slightly more\ninteresting toys: server-side Unicode character search utilities, first using HTTP with\nFastAPI, then using plain TCP with asyncio only.\nThese servers let users query for Unicode characters based on words in their standard\nnames from the unicodedata module we discussed in “The Unicode Database” on\npage 150. Figure 21-2 shows a session with web_mojifinder.py, the first server we’ll\nbuild.\nFigure 21-2. Browser window displaying search results for “mountain” from the\nweb_mojifinder.py service.\nThe Unicode search logic in these examples is in the InvertedIndex class in the char‐\nindex.py module in the Fluent Python code repository. There’s nothing concurrent in\nthat small module, so I’ll only give a brief overview in the optional box that follows.\nYou can skip to the HTTP server implementation in “A FastAPI Web Service” on\npage 800.\nMeet the Inverted Index\nAn inverted index usually maps words to documents in which they occur. In the\nmojifinder examples, each “document” is one Unicode character. The charin\ndex.InvertedIndex class indexes each word that appears in each character name in\nthe Unicode database, and creates an inverted index stored in a defaultdict. For\nexample, to index character U+0037—DIGIT SEVEN—the InvertedIndex initializer\nappends the character '7' to the entries under the keys 'DIGIT' and 'SEVEN'. After\nindexing the Unicode 13.0.0 data bundled with Python 3.9.1, 'DIGIT' maps to 868\ncharacters, and 'SEVEN' maps to 143, including U+1F556—CLOCK FACE SEVEN\nWriting asyncio Servers \n| \n799",
      "content_length": 1631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 830,
      "chapter": null,
      "content": "9 The boxed question mark in the screen shot is not a defect of the book or ebook you are reading. It’s the U\n+101EC—PHAISTOS DISC SIGN CAT character, which is missing from the font in the terminal I used. The\nPhaistos disc is an ancient artifact inscribed with pictograms, discovered in the island of Crete.\nOCLOCK and U+2790—DINGBAT NEGATIVE CIRCLED SANS-SERIF DIGIT\nSEVEN (which appears in many code listings in this book).\nSee Figure 21-3 for a demonstration using the entries for 'CAT' and 'FACE'.9\nFigure 21-3. Python console exploring InvertedIndex attribute entries and search\nmethod.\nThe InvertedIndex.search method breaks the query into words, and returns the\nintersection of the entries for each word. That’s why searching for “face” finds 171\nresults, “cat” finds 14, but “cat face” only 10.\nThat’s the beautiful idea behind an inverted index: a fundamental building block in\ninformation retrieval—the theory behind search engines. See the English Wikipedia\narticle “Inverted Index” to learn more.\nA FastAPI Web Service\nI wrote the next example—web_mojifinder.py—using FastAPI: one of the Python\nASGI Web frameworks mentioned in “ASGI—Asynchronous Server Gateway Inter‐\nface” on page 732. Figure 21-2 is a screenshot of the frontend. It’s a super simple SPA\n(Single Page Application): after the initial HTML download, the UI is updated by\nclient-side JavaScript communicating with the server.\nFastAPI is designed to implement backends for SPA and mobile apps, which mostly\nconsist of web API end points returning JSON responses instead of server-rendered\nHTML. FastAPI leverages decorators, type hints, and code introspection to eliminate\na lot of the boilerplate code for web APIs, and also automatically publishes interactive\nOpenAPI—a.k.a. Swagger—documentation for the APIs we create. Figure 21-4 shows\nthe autogenerated /docs page for web_mojifinder.py.\n800 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 831,
      "chapter": null,
      "content": "10 Instead of uvicorn, you may use another ASGI server, such as hypercorn or Daphne. See the official ASGI doc‐\numentation page about implementations for more information.\nFigure 21-4. Autogenerated OpenAPI schema for the /search endpoint.\nExample 21-11 is the code for web_mojifinder.py, but that’s just the backend code.\nWhen you hit the root URL /, the server sends the form.html file, which has 81 lines\nof code, including 54 lines of JavaScript to communicate with the server and fill a\ntable with the results. If you’re interested in reading plain framework-less JavaScript,\nplease find 21-async/mojifinder/static/form.html in the Fluent Python code repository.\nTo run web_mojifinder.py, you need to install two packages and their dependencies:\nFastAPI and uvicorn.10 This is the command to run Example 21-11 with uvicorn in\ndevelopment mode:\n$ uvicorn web_mojifinder:app --reload\nThe parameters are:\nweb_mojifinder:app\nThe package name, a colon, and the name of the ASGI application defined in it—\napp is the conventional name.\n--reload\nMake uvicorn monitor changes to application source files and automatically\nreload them. Useful only during development.\nNow let’s study the source code for web_mojifinder.py.\nWriting asyncio Servers \n| \n801",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 832,
      "chapter": null,
      "content": "11 Thanks to tech reviewer Miroslav Šedivý for highlighting good places to use pathlib in code examples.\nExample 21-11. web_mojifinder.py: complete source\nfrom pathlib import Path\nfrom unicodedata import name\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom pydantic import BaseModel\nfrom charindex import InvertedIndex\nSTATIC_PATH = Path(__file__).parent.absolute() / 'static'  \napp = FastAPI(  \n    title='Mojifinder Web',\n    description='Search for Unicode characters by name.',\n)\nclass CharName(BaseModel):  \n    char: str\n    name: str\ndef init(app):  \n    app.state.index = InvertedIndex()\n    app.state.form = (STATIC_PATH / 'form.html').read_text()\ninit(app)  \n@app.get('/search', response_model=list[CharName])  \nasync def search(q: str):  \n    chars = sorted(app.state.index.search(q))\n    return ({'char': c, 'name': name(c)} for c in chars)  \n@app.get('/', response_class=HTMLResponse, include_in_schema=False)\ndef form():  \n    return app.state.form\n# no main funcion  \nUnrelated to the theme of this chapter, but worth noting: the elegant use of the\noverloaded / operator by pathlib.11\nThis line defines the ASGI app. It could be as simple as app = FastAPI(). The\nparameters shown are metadata for the autogenerated documentation.\n802 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 833,
      "chapter": null,
      "content": "12 As mentioned in Chapter 8, pydantic enforces type hints at runtime, for data validation.\nA pydantic schema for a JSON response with char and name fields.12\nBuild the index and load the static HTML form, attaching both to the app.state\nfor later use.\nRun init when this module is loaded by the ASGI server.\nRoute for the /search endpoint; response_model uses that CharName pydantic\nmodel to describe the response format.\nFastAPI assumes that any parameters that appear in the function or coroutine\nsignature that are not in the route path will be passed in the HTTP query string,\ne.g., /search?q=cat. Since q has no default, FastAPI will return a 422 (Unpro‐\ncessable Entity) status if q is missing from the query string.\nReturning an iterable of dicts compatible with the response_model schema\nallows FastAPI to build the JSON response according to the response_model in\nthe @app.get decorator.\nRegular functions (i.e., non-async) can also be used to produce responses.\nThis module has no main function. It is loaded and driven by the ASGI server—\nuvicorn in this example.\nExample 21-11 has no direct calls to asyncio. FastAPI is built on the Starlette ASGI\ntoolkit, which in turn uses asyncio.\nAlso note that the body of search doesn’t use await, async with, or async for,\ntherefore it could be a plain function. I defined search as a coroutine just to show\nthat FastAPI knows how to handle it. In a real app, most endpoints will query data‐\nbases or hit other remote servers, so it is a critical advantage of FastAPI—and ASGI\nframeworks in general—to support coroutines that can take advantage of asynchro‐\nnous libraries for network I/O.\nWriting asyncio Servers \n| \n803",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 834,
      "chapter": null,
      "content": "The init and form functions I wrote to load and serve the static\nHTML form are a hack to make the example short and easy to run.\nThe recommended best practice is to have a proxy/load-balancer in\nfront of the ASGI server to handle all static assets, and also use a\nCDN (Content Delivery Network) when possible. One such proxy/\nload-balancer is Traefik, a self-described “edge router” that\n“receives requests on behalf of your system and finds out which\ncomponents are responsible for handling them.” FastAPI has\nproject generation scripts that prepare your code to do that.\nThe typing enthusiast may have noticed that there are no return type hints in search\nand form. Instead, FastAPI relies on the response_model= keyword argument in the\nroute decorators. The “Response Model” page in the FastAPI documentation\nexplains:\nThe response model is declared in this parameter instead of as a function return type\nannotation, because the path function may not actually return that response model but\nrather return a dict, database object or some other model, and then use the\nresponse_model to perform the field limiting and serialization.\nFor example, in search, I returned a generator of dict items, not a list of CharName\nobjects, but that’s good enough for FastAPI and pydantic to validate my data and\nbuild the appropriate JSON response compatible with response_model=list[Char\nName].\nWe’ll now focus on the tcp_mojifinder.py script that is answering the queries in\nFigure 21-5.\nAn asyncio TCP Server\nThe tcp_mojifinder.py program uses plain TCP to communicate with a client like Tel‐\nnet or Netcat, so I could write it using asyncio without external dependencies—and\nwithout reinventing HTTP. Figure 21-5 shows text-based UI.\n804 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 835,
      "chapter": null,
      "content": "Figure 21-5. Telnet session with the tcp_mojifinder.py server: querying for “fire.”\nThis program is twice as long as web_mojifinder.py, so I split the presentation into\nthree parts: Example 21-12, Example 21-14, and Example 21-15. The top of tcp_moji‐\nfinder.py—including the import statements—is in Example 21-14, but I will start by\ndescribing the supervisor coroutine and the main function that drives the program.\nExample 21-12. tcp_mojifinder.py: a simple TCP server; continues in Example 21-14\nasync def supervisor(index: InvertedIndex, host: str, port: int) -> None:\n    server = await asyncio.start_server(    \n        functools.partial(finder, index),   \n        host, port)                         \n    socket_list = cast(tuple[TransportSocket, ...], server.sockets)  \n    addr = socket_list[0].getsockname()\n    print(f'Serving on {addr}. Hit CTRL-C to stop.')  \n    await server.serve_forever()  \ndef main(host: str = '127.0.0.1', port_arg: str = '2323'):\n    port = int(port_arg)\n    print('Building index.')\n    index = InvertedIndex()                         \n    try:\n        asyncio.run(supervisor(index, host, port))  \n    except KeyboardInterrupt:                       \n        print('\\nServer shut down.')\nif __name__ == '__main__':\n    main(*sys.argv[1:])\nWriting asyncio Servers \n| \n805",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 836,
      "chapter": null,
      "content": "13 Issue #5535 is closed as of October 2021, but Mypy did not have a new release since then, so the error persists.\n14 Tech reviewer Leonardo Rochael pointed out that building the index could be delegated to another thread\nusing loop.run_with_executor() in the supervisor coroutine, so the server would be ready to take requests\nimmediately while the index is built. That’s true, but querying the index is the only thing this server does, so it\nwould not be a big win in this example.\nThis await quickly gets an instance of asyncio.Server, a TCP socket server. By\ndefault, start_server creates and starts the server, so it’s ready to receive\nconnections.\nThe first argument to start_server is client_connected_cb, a callback to run\nwhen a new client connection starts. The callback can be a function or a corou‐\ntine, but it must accept exactly two arguments: an asyncio.StreamReader and an\nasyncio.StreamWriter. However, my finder coroutine also needs to get an\nindex, so I used functools.partial to bind that parameter and obtain a callable\nthat takes the reader and writer. Adapting user functions to callback APIs is the\nmost common use case for functools.partial.\nhost and port are the second and third arguments to start_server. See the full\nsignature in the asyncio documentation.\nThis cast is needed because typeshed has an outdated type hint for the sockets\nproperty of the Server class—as of May 2021. See Issue #5535 on typeshed.13\nDisplay the address and port of the first socket of the server.\nAlthough start_server already started the server as a concurrent task, I need to\nawait on the server_forever method so that my supervisor is suspended here.\nWithout this line, supervisor would return immediately, ending the loop started\nwith asyncio.run(supervisor(…)), and exiting the program. The documenta‐\ntion for Server.serve_forever says: “This method can be called if the server is\nalready accepting connections.”\nBuild the inverted index.14\nStart the event loop running supervisor.\nCatch the KeyboardInterrupt to avoid a distracting traceback when I stop the\nserver with Ctrl-C on the terminal running it.\nYou may find it easier to understand how control flows in tcp_mojifinder.py if you\nstudy the output it generates on the server console, listed in Example 21-13.\n806 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 837,
      "chapter": null,
      "content": "Example 21-13. tcp_mojifinder.py: this is the server side of the session depicted in\nFigure 21-5\n$ python3 tcp_mojifinder.py\nBuilding index.  \nServing on ('127.0.0.1', 2323). Hit Ctrl-C to stop.  \n From ('127.0.0.1', 58192): 'cat face'   \n   To ('127.0.0.1', 58192): 10 results.\n From ('127.0.0.1', 58192): 'fire'       \n   To ('127.0.0.1', 58192): 11 results.\n From ('127.0.0.1', 58192): '\\x00'       \nClose ('127.0.0.1', 58192).              \n^C  \nServer shut down.  \n$\nOutput by main. Before the next line appears, I see a 0.6s delay on my machine\nwhile the index is built.\nOutput by supervisor.\nFirst iteration of a while loop in finder. The TCP/IP stack assigned port 58192\nto my Telnet client. If you connect several clients to the server, you’ll see their\nvarious ports in the output.\nSecond iteration of the while loop in finder.\nI hit Ctrl-C on the client terminal; the while loop in finder exits.\nThe finder coroutine displays this message then exits. Meanwhile the server is\nstill running, ready to service another client.\nI hit Ctrl-C on the server terminal; server.serve_forever is cancelled, ending\nsupervisor and the event loop.\nOutput by main.\nAfter main builds the index and starts the event loop, supervisor quickly displays the\nServing on… message and is suspended at the await server.serve_forever() line.\nAt that point, control flows into the event loop and stays there, occasionally coming\nback to the finder coroutine, which yields control back to the event loop whenever it\nneeds to wait for the network to send or receive data.\nWhile the event loop is alive, a new instance of the finder coroutine will be started\nfor each client that connects to the server. In this way, many clients can be handled\nWriting asyncio Servers \n| \n807",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 838,
      "chapter": null,
      "content": "concurrently by this simple server. This continues until a KeyboardInterrupt occurs\non the server or its process is killed by the OS.\nNow let’s see the top of tcp_mojifinder.py, with the finder coroutine.\nExample 21-14. tcp_mojifinder.py: continued from Example 21-12\nimport asyncio\nimport functools\nimport sys\nfrom asyncio.trsock import TransportSocket\nfrom typing import cast\nfrom charindex import InvertedIndex, format_results  \nCRLF = b'\\r\\n'\nPROMPT = b'?> '\nasync def finder(index: InvertedIndex,          \n                 reader: asyncio.StreamReader,\n                 writer: asyncio.StreamWriter) -> None:\n    client = writer.get_extra_info('peername')  \n    while True:  \n        writer.write(PROMPT)  # can't await!  \n        await writer.drain()  # must await!  \n        data = await reader.readline()  \n        if not data:  \n            break\n        try:\n            query = data.decode().strip()  \n        except UnicodeDecodeError:  \n            query = '\\x00'\n        print(f' From {client}: {query!r}')  \n        if query:\n            if ord(query[:1]) < 32:  \n                break\n            results = await search(query, index, writer)  \n            print(f'   To {client}: {results} results.')  \n    writer.close()  \n    await writer.wait_closed()  \n    print(f'Close {client}.')  \n808 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 839,
      "chapter": null,
      "content": "format_results is useful to display the results of InvertedIndex.search in a\ntext-based UI such as the command line or a Telnet session.\nTo pass finder to asyncio.start_server, I wrapped it with functools.par\ntial, because the server expects a coroutine or function that takes only the\nreader and writer arguments.\nGet the remote client address to which the socket is connected.\nThis loop handles a dialog that lasts until a control character is received from the\nclient.\nThe StreamWriter.write method is not a coroutine, just a plain function; this\nline sends the ?> prompt.\nStreamWriter.drain flushes the writer buffer; it is a coroutine, so it must be\ndriven with await.\nStreamWriter.readline is a coroutine that returns bytes.\nIf no bytes were received, the client closed the connection, so exit the loop.\nDecode the bytes to str, using the default UTF-8 encoding.\nA UnicodeDecodeError may happen when the user hits Ctrl-C and the Telnet cli‐\nent sends control bytes; if that happens, replace the query with a null character,\nfor simplicity.\nLog the query to the server console.\nExit the loop if a control or null character was received.\nDo the actual search; code is presented next.\nLog the response to the server console.\nClose the StreamWriter.\nWait for the StreamWriter to close. This is recommended in the .close()\nmethod documentation.\nLog the end of this client’s session to the server console.\nThe last piece of this example is the search coroutine, shown in Example 21-15.\nWriting asyncio Servers \n| \n809",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 840,
      "chapter": null,
      "content": "Example 21-15. tcp_mojifinder.py: search coroutine\nasync def search(query: str,  \n                 index: InvertedIndex,\n                 writer: asyncio.StreamWriter) -> int:\n    chars = index.search(query)  \n    lines = (line.encode() + CRLF for line  \n                in format_results(chars))\n    writer.writelines(lines)  \n    await writer.drain()      \n    status_line = f'{\"─\" * 66} {len(chars)} found'  \n    writer.write(status_line.encode() + CRLF)\n    await writer.drain()\n    return len(chars)\nsearch must be a coroutine because it writes to a StreamWriter and must use\nits .drain() coroutine method.\nQuery the inverted index.\nThis generator expression will yield byte strings encoded in UTF-8 with the Uni‐\ncode codepoint, the actual character, its name, and a CRLF sequence—e.g.,\nb'U+0039\\t9\\tDIGIT NINE\\r\\n').\nSend the lines. Surprisingly, writer.writelines is not a coroutine.\nBut writer.drain() is a coroutine. Don’t forget the await!\nBuild a status line, then send it.\nNote that all network I/O in tcp_mojifinder.py is in bytes; we need to decode the\nbytes received from the network, and encode strings before sending them out. In\nPython 3, the default encoding is UTF-8, and that’s what I used implicitly in all\nencode and decode calls in this example.\nNote that some of the I/O methods are coroutines and must be\ndriven with await, while others are simple functions. For example,\nStreamWriter.write is a plain function, because it writes to a\nbuffer. On the other hand, StreamWriter.drain—which flushes\nthe buffer and performs the network I/O—is a coroutine, as\nis StreamReader.readline—but not StreamWriter.writelines!\nWhile I was writing the first edition of this book, the asyncio API\ndocs were improved by clearly labeling coroutines as such.\n810 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 841,
      "chapter": null,
      "content": "The tcp_mojifinder.py code leverages the high-level asyncio Streams API that pro‐\nvides a ready-to-use server so you only need to implement a handler function, which\ncan be a plain callback or a coroutine. There is also a lower-level Transports and Pro‐\ntocols API, inspired by the transport and protocols abstractions in the Twisted frame‐\nwork. Refer to the asyncio documentation for more information, including TCP and\nUDP echo servers and clients implemented with that lower-level API.\nOur next topic is async for and the objects that make it work.\nAsynchronous Iteration and Asynchronous Iterables\nWe saw in “Asynchronous Context Managers” on page 786 how async with works\nwith objects implementing the __aenter__ and __aexit__ methods returning await‐\nables—usually in the form of coroutine objects.\nSimilarly, async for works with asynchronous iterables: objects that implement\n__aiter__. However, __aiter__ must be a regular method—not a coroutine method\n—and it must return an asynchronous iterator.\nAn asynchronous iterator provides an __anext__ coroutine method that returns an\nawaitable—often a coroutine object. They are also expected to implement __aiter__,\nwhich usually returns self. This mirrors the important distinction of iterables and\niterators we discussed in “Don’t Make the Iterable an Iterator for Itself” on page 605.\nThe aiopg asynchronous PostgreSQL driver documentation has an example that illus‐\ntrates the use of async for to iterate over the rows of a database cursor:\nasync def go():\n    pool = await aiopg.create_pool(dsn)\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cur:\n            await cur.execute(\"SELECT 1\")\n            ret = []\n            async for row in cur:\n                ret.append(row)\n            assert ret == [(1,)]\nIn this example the query will return a single row, but in a realistic scenario you may\nhave thousands of rows in response to a SELECT query. For large responses, the cursor\nwill not be loaded with all the rows in a single batch. Therefore it is important that\nasync for row in cur: does not block the event loop while the cursor may be wait‐\ning for additional rows. By implementing the cursor as an asynchronous iterator,\naiopg may yield to the event loop at each __anext__ call, and resume later when more\nrows arrive from PostgreSQL.\nAsynchronous Iteration and Asynchronous Iterables \n| \n811",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 842,
      "chapter": null,
      "content": "15 This is great for experimentation, like the Node.js console. Thanks to Yury Selivanov for yet another excellent\ncontribution to asynchronous Python.\nAsynchronous Generator Functions\nYou can implement an asynchronous iterator by writing a class with __anext__ and\n__aiter__, but there is a simpler way: write a function declared with async def and\nuse yield in its body. This parallels how generator functions simplify the classic Iter‐\nator pattern.\nLet’s study a simple example using async for and implementing an asynchronous\ngenerator. In Example 21-1 we saw blogdom.py, a script that probed domain names.\nNow suppose we find other uses for the probe coroutine we defined there, and decide\nto put it into a new module—domainlib.py—together with a new multi_probe asyn‐\nchronous generator that takes a list of domain names and yields results as they are\nprobed.\nWe’ll look at the implementation of domainlib.py soon, but first let’s see how it is\nused with Python’s new asynchronous console.\nExperimenting with Python’s async console\nSince Python 3.8, you can run the interpreter with the -m asyncio command-line\noption to get an “async REPL”: a Python console that imports asyncio, provides a\nrunning event loop, and accepts await, async for, and async with at the top-level\nprompt—which otherwise are syntax errors when used outside of native coroutines.15\nTo experiment with domainlib.py, go to the 21-async/domains/asyncio/ directory in\nyour local copy of the Fluent Python code repository. Then run:\n$ python -m asyncio\nYou’ll see the console start, similar to this:\nasyncio REPL 3.9.1 (v3.9.1:1e5d33e9b9, Dec  7 2020, 12:10:52)\n[Clang 6.0 (clang-600.0.57)] on darwin\nUse \"await\" directly instead of \"asyncio.run()\".\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import asyncio\n>>>\nNote how the header says you can use await instead of asyncio.run()—to drive\ncoroutines and other awaitables. Also: I did not type import asyncio. The asyncio\nmodule is automatically imported and that line makes that fact clear to the user.\n812 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 843,
      "chapter": null,
      "content": "16 See RFC 6761—Special-Use Domain Names.\nNow let’s import domainlib.py and play with its two coroutines: probe and\nmulti_probe (Example 21-16).\nExample 21-16. Experimenting with domainlib.py after running python3 -m asyncio\n>>> await asyncio.sleep(3, 'Rise and shine!')  \n'Rise and shine!'\n>>> from domainlib import *\n>>> await probe('python.org')  \nResult(domain='python.org', found=True)  \n>>> names = 'python.org rust-lang.org golang.org no-lang.invalid'.split()  \n>>> async for result in multi_probe(names):  \n...      print(*result, sep='\\t')\n...\ngolang.org      True    \nno-lang.invalid False\npython.org      True\nrust-lang.org   True\n>>>\nTry a simple await to see the asynchronous console in action. Tip: asyn\ncio.sleep() takes an optional second argument that is returned when you\nawait it.\nDrive the probe coroutine.\nThe domainlib version of probe returns a Result named tuple.\nMake a list of domains. The .invalid top-level domain is reserved for testing.\nDNS queries for such domains always get an NXDOMAIN response from DNS\nservers, meaning “that domain does not exist.”16\nIterate with async for over the multi_probe asynchronous generator to display\nthe results.\nNote that the results are not in the order the domains were given to multiprobe.\nThey appear as each DNS response comes back.\nExample 21-16 shows that multi_probe is an asynchronous generator because it is\ncompatible with async for. Now let’s do a few more experiments, continuing from\nthat example with Example 21-17.\nAsynchronous Iteration and Asynchronous Iterables \n| \n813",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 844,
      "chapter": null,
      "content": "Example 21-17. More experiments, continuing from Example 21-16\n>>> probe('python.org')  \n<coroutine object probe at 0x10e313740>\n>>> multi_probe(names)  \n<async_generator object multi_probe at 0x10e246b80>\n>>> for r in multi_probe(names):  \n...    print(r)\n...\nTraceback (most recent call last):\n   ...\nTypeError: 'async_generator' object is not iterable\nCalling a native coroutine gives you a coroutine object.\nCalling an asynchronous generator gives you an async_generator object.\nWe can’t use a regular for loop with asynchronous generators because they\nimplement __aiter__ instead of __iter__.\nAsynchronous generators are driven by async for, which can be a block statement\n(as seen in Example 21-16), and it also appears in asynchronous comprehensions,\nwhich we’ll cover soon.\nImplementing an asynchronous generator\nNow let’s study the code for domainlib.py, with the multi_probe asynchronous gen‐\nerator (Example 21-18).\nExample 21-18. domainlib.py: functions for probing domains\nimport asyncio\nimport socket\nfrom collections.abc import Iterable, AsyncIterator\nfrom typing import NamedTuple, Optional\nclass Result(NamedTuple):  \n    domain: str\n    found: bool\nOptionalLoop = Optional[asyncio.AbstractEventLoop]  \nasync def probe(domain: str, loop: OptionalLoop = None) -> Result:  \n    if loop is None:\n        loop = asyncio.get_running_loop()\n    try:\n814 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 845,
      "chapter": null,
      "content": "await loop.getaddrinfo(domain, None)\n    except socket.gaierror:\n        return Result(domain, False)\n    return Result(domain, True)\nasync def multi_probe(domains: Iterable[str]) -> AsyncIterator[Result]:  \n    loop = asyncio.get_running_loop()\n    coros = [probe(domain, loop) for domain in domains]  \n    for coro in asyncio.as_completed(coros):  \n        result = await coro  \n        yield result  \nNamedTuple makes the result from probe easier to read and debug.\nThis type alias is to avoid making the next line too long for a book listing.\nprobe now gets an optional loop argument, to avoid repeated calls to get_run\nning_loop when this coroutine is driven by multi_probe.\nAn asynchronous generator function produces an asynchronous generator\nobject, which can be annotated as AsyncIterator[SomeType].\nBuild list of probe coroutine objects, each with a different domain.\nThis is not async for because asyncio.as_completed is a classic generator.\nAwait on the coroutine object to retrieve the result.\nYield result. This line makes multi_probe an asynchronous generator.\nThe for loop in Example 21-18 could be more concise:\n    for coro in asyncio.as_completed(coros):\n        yield await coro\nPython parses that as yield (await coro), so it works.\nI thought it could be confusing to use that shortcut in the first\nasynchronous generator example in the book, so I split it into two\nlines.\nGiven domainlib.py, we can demonstrate the use of the multi_probe asynchronous\ngenerator in domaincheck.py: a script that takes a domain suffix and searches for\ndomains made from short Python keywords.\nHere is a sample output of domaincheck.py:\nAsynchronous Iteration and Asynchronous Iterables \n| \n815",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 846,
      "chapter": null,
      "content": "$ ./domaincheck.py net\nFOUND           NOT FOUND\n=====           =========\nin.net\ndel.net\ntrue.net\nfor.net\nis.net\n                none.net\ntry.net\n                from.net\nand.net\nor.net\nelse.net\nwith.net\nif.net\nas.net\n                elif.net\n                pass.net\n                not.net\n                def.net\nThanks to domainlib, the code for domaincheck.py is straightforward, as seen in\nExample 21-19.\nExample 21-19. domaincheck.py: utility for probing domains using domainlib\n#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom keyword import kwlist\nfrom domainlib import multi_probe\nasync def main(tld: str) -> None:\n    tld = tld.strip('.')\n    names = (kw for kw in kwlist if len(kw) <= 4)  \n    domains = (f'{name}.{tld}'.lower() for name in names)  \n    print('FOUND\\t\\tNOT FOUND')  \n    print('=====\\t\\t=========')\n    async for domain, found in multi_probe(domains):  \n        indent = '' if found else '\\t\\t'  \n        print(f'{indent}{domain}')\nif __name__ == '__main__':\n    if len(sys.argv) == 2:\n        asyncio.run(main(sys.argv[1]))  \n    else:\n        print('Please provide a TLD.', f'Example: {sys.argv[0]} COM.BR')\n816 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1189,
      "extraction_method": "Direct"
    },
    {
      "page_number": 847,
      "chapter": null,
      "content": "Generate keywords with length up to 4.\nGenerate domain names with the given suffix as TLD.\nFormat a header for the tabular output.\nAsynchronously iterate over multi_probe(domains).\nSet indent to zero or two tabs to put the result in the proper column.\nRun the main coroutine with the given command-line argument.\nGenerators have one extra use unrelated to iteration: they can be made into context\nmanagers. This also applies to asynchronous generators.\nAsynchronous generators as context managers\nWriting our own asynchronous context managers is not a frequent programming\ntask, but if you need to write one, consider using the @asynccontextmanager decora‐\ntor added to the contextlib module in Python 3.7. That’s very similar to the @con\ntextmanager decorator we studied in “Using @contextmanager” on page 664.\nAn interesting example combining @asynccontextmanager with loop.run_in_execu\ntor appears in Caleb Hattingh’s book Using Asyncio in Python. Example 21-20 is\nCaleb’s code—with a single change and added callouts.\nExample 21-20. Example using @asynccontextmanager and loop.run_in_executor\nfrom contextlib import asynccontextmanager\n@asynccontextmanager\nasync def web_page(url):  \n    loop = asyncio.get_running_loop()   \n    data = await loop.run_in_executor(  \n        None, download_webpage, url)\n    yield data                          \n    await loop.run_in_executor(None, update_stats, url)  \nasync with web_page('google.com') as data:  \n    process(data)\nThe decorated function must be an asynchronous generator.\nMinor update to Caleb’s code: use the lightweight get_running_loop instead of\nget_event_loop.\nAsynchronous Iteration and Asynchronous Iterables \n| \n817",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 848,
      "chapter": null,
      "content": "Suppose download_webpage is a blocking function using the requests library; we\nrun it in a separate thread to avoid blocking the event loop.\nAll lines before this yield expression will become the __aenter__ coroutine-\nmethod of the asynchronous context manager built by the decorator. The value\nof data will be bound to the data variable after the as clause in the async with\nstatement below.\nLines after the yield will become the __aexit__ coroutine method. Here,\nanother blocking call is delegated to the thread executor.\nUse web_page with async with.\nThis is very similar to the sequential @contextmanager decorator. Please see “Using\n@contextmanager” on page 664 for more details, including error handling at the\nyield line. For another example of @asynccontextmanager, see the contextlib doc‐\numentation.\nNow let’s wrap up our coverage of asynchronous generator functions by contrasting\nthem with native coroutines.\nAsynchronous generators versus native coroutines\nHere are some key similarities and differences between a native coroutine and an\nasynchronous generator function:\n• Both are declared with async def.\n• An asynchronous generator always has a yield expression in its body—that’s\nwhat makes it a generator. A native coroutine never contains yield.\n• A native coroutine may return some value other than None. An asynchronous\ngenerator can only use empty return statements.\n• Native coroutines are awaitable: they can be driven by await expressions or\npassed to one of the many asyncio functions that take awaitable arguments, such\nas create_task. Asynchronous generators are not awaitable. They are asynchro‐\nnous iterables, driven by async for or by asynchronous comprehensions.\nTime to talk about asynchronous comprehensions.\nAsync Comprehensions and Async Generator Expressions\nPEP 530—Asynchronous Comprehensions introduced the use of async for and\nawait in the syntax of comprehensions and generator expressions, starting with\nPython 3.6.\n818 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 849,
      "chapter": null,
      "content": "The only construct defined by PEP 530 that can appear outside an async def body is\nan asynchronous generator expression.\nDefining and using an asynchronous generator expression\nGiven the multi_probe asynchronous generator from Example 21-18, we could write\nanother asynchronous generator returning only the names of the domains found.\nHere is how—again using the asynchronous console launched with -m asyncio:\n>>> from domainlib import multi_probe\n>>> names = 'python.org rust-lang.org golang.org no-lang.invalid'.split()\n>>> gen_found = (name async for name, found in multi_probe(names) if found)  \n>>> gen_found\n<async_generator object <genexpr> at 0x10a8f9700>  \n>>> async for name in gen_found:  \n...     print(name)\n...\ngolang.org\npython.org\nrust-lang.org\nThe use of async for makes this an asynchronous generator expression. It can\nbe defined anywhere in a Python module.\nThe asynchronous generator expression builds an async_generator object—\nexactly the same type of object returned by an asynchronous generator function\nlike multi_probe.\nThe asynchronous generator object is driven by the async for statement, which\nin turn can only appear inside an async def body or in the magic asynchronous\nconsole I used in this example.\nTo summarize: an asynchronous generator expression can be defined anywhere in\nyour program, but it can only be consumed inside a native coroutine or asynchro‐\nnous generator function.\nThe remaining constructs introduced by PEP 530 can only be defined and used inside\nnative coroutines or asynchronous generator functions.\nAsynchronous comprehensions\nYury Selivanov—the author of PEP 530—justifies the need for asynchronous com‐\nprehensions with three short code snippets reproduced next.\nWe can all agree that we should be able to rewrite this code:\nresult = []\nasync for i in aiter():\nAsynchronous Iteration and Asynchronous Iterables \n| \n819",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 850,
      "chapter": null,
      "content": "if i % 2:\n        result.append(i)\nlike this:\nresult = [i async for i in aiter() if i % 2]\nIn addition, given a native coroutine fun, we should be able to write this:\nresult = [await fun() for fun in funcs]\nUsing await in a list comprehension is similar to using asyn\ncio.gather. But gather gives you more control over exception\nhandling, thanks to its optional return_exceptions argument.\nCaleb Hattingh recommends always setting return_excep\ntions=True (the default is False). Please see the asyncio.gather\ndocumentation for more.\nBack to the magic asynchronous console:\n>>> names = 'python.org rust-lang.org golang.org no-lang.invalid'.split()\n>>> names = sorted(names)\n>>> coros = [probe(name) for name in names]\n>>> await asyncio.gather(*coros)\n[Result(domain='golang.org', found=True),\nResult(domain='no-lang.invalid', found=False),\nResult(domain='python.org', found=True),\nResult(domain='rust-lang.org', found=True)]\n>>> [await probe(name) for name in names]\n[Result(domain='golang.org', found=True),\nResult(domain='no-lang.invalid', found=False),\nResult(domain='python.org', found=True),\nResult(domain='rust-lang.org', found=True)]\n>>>\nNote that I sorted the list of names to show that the results come out in the order they\nwere submitted, in both cases.\nPEP 530 allows the use of async for and await in list comprehensions as well as in\ndict and set comprehensions. For example, here is a dict comprehension to store\nthe results of multi_probe in the asynchronous console:\n>>> {name: found async for name, found in multi_probe(names)}\n{'golang.org': True, 'python.org': True, 'no-lang.invalid': False,\n'rust-lang.org': True}\nWe can use the await keyword in the expression before the for or async for clause,\nand also in the expression after the if clause. Here is a set comprehension in the\nasynchronous console, collecting only the domains that were found:\n>>> {name for name in names if (await probe(name)).found}\n{'rust-lang.org', 'python.org', 'golang.org'}\n820 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 851,
      "chapter": null,
      "content": "17 That’s in contrast with JavaScript, where async/await is hardwired to the built-in event loop and runtime\nenvironment, i.e., a browser, Node.js, or Deno.\nI had to put extra parentheses around the await expression due to the higher prece‐\ndence of the __getattr__ operator . (dot).\nAgain, all of these comprehensions can only appear inside an async def body or in\nthe enchanted asynchronous console.\nNow let’s talk about a very important feature of the async statements, async expres‐\nsions, and the objects they create. Those constructs are often used with asyncio but,\nthey are actually library independent.\nasync Beyond asyncio: Curio\nPython’s async/await language constructs are not tied to any specific event loop or\nlibrary.17 Thanks to the extensible API provided by special methods, anyone suffi‐\nciently motivated can write their own asynchronous runtime environment and\nframework to drive native coroutines, asynchronous generators, etc.\nThat’s what David Beazley did in his Curio project. He was interested in rethinking\nhow these new language features could be used in a framework built from scratch.\nRecall that asyncio was released in Python 3.4, and it used yield from instead of\nawait, so its API could not leverage asynchronous context managers, asynchronous\niterators, and everything else that the async/await keywords made possible. As a\nresult, Curio has a cleaner API and a simpler implementation, compared to asyncio.\nExample 21-21 shows the blogdom.py script (Example 21-1) rewritten to use Curio.\nExample 21-21. blogdom.py: Example 21-1, now using Curio\n#!/usr/bin/env python3\nfrom curio import run, TaskGroup\nimport curio.socket as socket\nfrom keyword import kwlist\nMAX_KEYWORD_LEN = 4\nasync def probe(domain: str) -> tuple[str, bool]:  \n    try:\n        await socket.getaddrinfo(domain, None)  \n    except socket.gaierror:\n        return (domain, False)\n    return (domain, True)\nasync Beyond asyncio: Curio \n| \n821",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 852,
      "chapter": null,
      "content": "async def main() -> None:\n    names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN)\n    domains = (f'{name}.dev'.lower() for name in names)\n    async with TaskGroup() as group:  \n        for domain in domains:\n            await group.spawn(probe, domain)  \n        async for task in group:  \n            domain, found = task.result\n            mark = '+' if found else ' '\n            print(f'{mark} {domain}')\nif __name__ == '__main__':\n    run(main())  \nprobe doesn’t need to get the event loop, because…\n…getaddrinfo is a top-level function of curio.socket, not a method of a loop\nobject—as it is in asyncio.\nA TaskGroup is a core concept in Curio, to monitor and control several corou‐\ntines, and to make sure they are all executed and cleaned up.\nTaskGroup.spawn is how you start a coroutine, managed by a specific TaskGroup\ninstance. The coroutine is wrapped by a Task.\nIterating with async for over a TaskGroup yields Task instances as each is\ncompleted. \nThis \ncorresponds \nto \nthe \nline \nin \nExample \n21-1 \nusing\nfor … as_completed(…):.\nCurio pioneered this sensible way to start an asynchronous program in Python.\nTo expand on the last point: if you look at the asyncio code examples for the first\nedition of Fluent Python, you’ll see lines like these, repeated over and over:\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n    loop.close()\nA Curio TaskGroup is an asynchronous context manager that replaces several ad hoc\nAPIs and coding patterns in asyncio. We just saw how iterating over a TaskGroup\nmakes the asyncio.as_completed(…) function unnecessary. Another example:\ninstead of a special gather function, this snippet from the “Task Groups” docs col‐\nlects the results of all tasks in the group:\nasync with TaskGroup(wait=all) as g:\n    await g.spawn(coro1)\n    await g.spawn(coro2)\n822 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 853,
      "chapter": null,
      "content": "await g.spawn(coro3)\nprint('Results:', g.results)\nTask groups support structured concurrency: a form of concurrent programming that\nconstrains all the activity of a group of asynchronous tasks to a single entry and exit\npoint. This is analogous to structured programming, which eschewed the GOTO com‐\nmand and introduced block statements to limit the entry and exit points of loops and\nsubroutines. When used as an asynchronous context manager, a TaskGroup ensures\nthat all tasks spawned inside are completed or cancelled, and any exceptions raised,\nupon exiting the enclosed block.\nStructured concurrency will probably be adopted by asyncio in\nupcoming Python releases. A strong indication appears in PEP\n654–Exception Groups and except*, which was approved for\nPython 3.11. The “Motivation” section mentions Trio’s “nurseries,”\ntheir name for task groups: “Implementing a better task spawning\nAPI in asyncio, inspired by Trio nurseries, was the main motiva‐\ntion for this PEP.”\nAnother important feature of Curio is better support for programming with corou‐\ntines and threads in the same codebase—a necessity in most nontrivial asynchronous\nprograms. Starting a thread with await spawn_thread(func, …) returns an Asyn\ncThread object with a Task-like interface. Threads can call coroutines thanks to a\nspecial AWAIT(coro) function—named in all caps because await is now a keyword.\nCurio also provides a UniversalQueue that can be used to coordinate the work\namong threads, Curio coroutines, and asyncio coroutines. That’s right, Curio has\nfeatures that allow it to run in a thread along with asyncio in another thread, in the\nsame process, communicating via UniversalQueue and UniversalEvent. The API for\nthese “universal” classes is the same inside and outside of coroutines, but in a corou‐\ntine, you need to prefix calls with await.\nAs I write this in October 2021, HTTPX is the first HTTP client library compatible\nwith Curio, but I don’t know of any asynchronous database libraries that support it\nyet. In the Curio repository there is an impressive set of network programming exam‐\nples, including one using WebSocket, and another implementing the RFC 8305—\nHappy Eyeballs concurrent algorithm for connecting to IPv6 endpoints with fast fall‐\nback to IPv4 if needed.\nThe design of Curio has been influential. The Trio framework started by Nathaniel J.\nSmith was heavily inspired by Curio. Curio may also have prompted Python contrib‐\nutors to improve the usability of the asyncio API. For example, in its earliest releases,\nasyncio users very often had to get and pass around a loop object because some\nessential functions were either loop methods or required a loop argument. In recent\nasync Beyond asyncio: Curio \n| \n823",
      "content_length": 2730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 854,
      "chapter": null,
      "content": "18 This differs from the annotations of classic coroutines, as discussed in “Generic Type Hints for Classic\nCoroutines” on page 650.\nversions of Python, direct access to the loop is not needed as often, and in fact several\nfunctions that accepted an optional loop are now deprecating that argument.\nType annotations for asynchronous types are our next topic.\nType Hinting Asynchronous Objects\nThe return type of a native coroutine describes what you get when you await on that\ncoroutine, which is the type of the object that appears in the return statements in the\nbody of the native coroutine function.18\nThis chapter provided many examples of annotated native coroutines, including\nprobe from Example 21-21:\nasync def probe(domain: str) -> tuple[str, bool]:\n    try:\n        await socket.getaddrinfo(domain, None)\n    except socket.gaierror:\n        return (domain, False)\n    return (domain, True)\nIf you need to annotate a parameter that takes a coroutine object, then the generic\ntype is:\nclass typing.Coroutine(Awaitable[V_co], Generic[T_co, T_contra, V_co]):\n    ...\nThat type, and the following types were introduced in Python 3.5 and 3.6 to annotate\nasynchronous objects:\nclass typing.AsyncContextManager(Generic[T_co]):\n    ...\nclass typing.AsyncIterable(Generic[T_co]):\n    ...\nclass typing.AsyncIterator(AsyncIterable[T_co]):\n    ...\nclass typing.AsyncGenerator(AsyncIterator[T_co], Generic[T_co, T_contra]):\n    ...\nclass typing.Awaitable(Generic[T_co]):\n    ...\nWith Python ≥ 3.9, use the collections.abc equivalents of these.\nI want to highlight three aspects of those generic types.\nFirst: they are all covariant on the first type parameter, which is the type of the items\nyielded from these objects. Recall rule #1 of “Variance rules of thumb” on page 551:\n824 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 855,
      "chapter": null,
      "content": "19 Video: “Introduction to Node.js” at 4:55.\nIf a formal type parameter defines a type for data that comes out of the object, it can be\ncovariant.\nSecond: AsyncGenerator and Coroutine are contravariant on the second to last\nparameter. That’s the type of the argument of the low-level .send() method that the\nevent loop calls to drive asynchronous generators and coroutines. As such, it is an\n“input” type. Therefore, it can be contravariant, per Variance Rule of Thumb #2:\nIf a formal type parameter defines a type for data that goes into the object after its ini‐\ntial construction, it can be contravariant.\nThird: AsyncGenerator has no return type, in contrast with typing.Generator,\nwhich we saw in “Generic Type Hints for Classic Coroutines” on page 650. Returning\na value by raising StopIteration(value) was one of the hacks that enabled genera‐\ntors to operate as coroutines and support yield from, as we saw in “Classic Corou‐\ntines” on page 641. There is no such overlap among the asynchronous objects:\nAsyncGenerator objects don’t return values, and are completely separate from native\ncoroutine objects, which are annotated with typing.Coroutine.\nFinally, let’s briefly discuss the advantages and challenges of asynchronous\nprogramming.\nHow Async Works and How It Doesn’t\nThe sections closing this chapter discuss high-level ideas around asynchronous pro‐\ngramming, regardless of the language or library you are using.\nLet’s begin by explaining the #1 reason why asynchronous programming is appealing,\nfollowed by a popular myth, and how to deal with it.\nRunning Circles Around Blocking Calls\nRyan Dahl, the inventor of Node.js, introduces the philosophy of his project by say‐\ning “We’re doing I/O completely wrong.”19 He defines a blocking function as one that\ndoes file or network I/O, and argues that we can’t treat them as we treat nonblocking\nfunctions. To explain why, he presents the numbers in the second column of\nTable 21-1.\nHow Async Works and How It Doesn’t \n| \n825",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 856,
      "chapter": null,
      "content": "Table 21-1. Modern computer latency for reading data from different devices; third column\nshows proportional times in a scale easier to understand for us slow humans\nDevice\nCPU cycles\nProportional “human” scale\nL1 cache\n3\n3 seconds\nL2 cache\n14\n14 seconds\nRAM\n250\n250 seconds\ndisk\n41,000,000\n1.3 years\nnetwork\n240,000,000 7.6 years\nTo make sense of Table 21-1, bear in mind that modern CPUs with GHz clocks run\nbillions of cycles per second. Let’s say that a CPU runs exactly 1 billion cycles per sec‐\nond. That CPU can make more than 333 million L1 cache reads in 1 second, or 4\n(four!) network reads in the same time. The third column of Table 21-1 puts those\nnumbers in perspective by multiplying the second column by a constant factor. So, in\nan alternate universe, if one read from L1 cache took 3 seconds, then a network read\nwould take 7.6 years!\nTable 21-1 explains why a disciplined approach to asynchronous programming can\nlead to high-performance servers. The challenge is achieving that discipline. The first\nstep is to recognize that “I/O bound system” is a fantasy.\nThe Myth of I/O-Bound Systems\nA commonly repeated meme is that asynchronous programming is good for “I/O\nbound systems.” I learned the hard way that there are no “I/O-bound systems.” You\nmay have I/O-bound functions. Perhaps the vast majority of the functions in your\nsystem are I/O bound; i.e., they spend more time waiting for I/O than crunching\ndata. While waiting, they cede control to the event loop, which can then drive some\nother pending task. But inevitably, any nontrivial system will have some parts that are\nCPU bound. Even trivial systems reveal that, under stress. In “Soapbox” on page 829,\nI tell the story of two asynchronous programs that struggled with CPU-bound func‐\ntions slowing down the event loop with severe impact on performance.\nGiven that any nontrivial system will have CPU-bound functions, dealing with them\nis the key to success in asynchronous programming.\nAvoiding CPU-Bound Traps\nIf you’re using Python at scale, you should have some automated tests designed\nspecifically to detect performance regressions as soon as they appear. This is critically\nimportant with asynchronous code, but also relevant to threaded Python code—\nbecause of the GIL. If you wait until the slowdown starts bothering the development\nteam, it’s too late. The fix will probably require some major makeover.\n826 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 857,
      "chapter": null,
      "content": "Here are some options for when you identify a CPU-hogging bottleneck:\n• Delegate the task to a Python process pool.\n• Delegate the task to an external task queue.\n• Rewrite the relevant code in Cython, C, Rust, or some other language that com‐\npiles to machine code and interfaces with the Python/C API, preferably releasing\nthe GIL.\n• Decide that you can afford the performance hit and do nothing—but record the\ndecision to make it easier to revert to it later.\nThe external task queue should be chosen and integrated as soon as possible at the\nstart of the project, so that nobody in the team hesitates to use it when needed.\nThe last option—do nothing—falls in the category of technical debt.\nConcurrent programming is a fascinating topic, and I would like to write a lot more\nabout it. But it is not the main focus of this book, and this is already one of the\nlongest chapters, so let’s wrap it up.\nChapter Summary\nThe problem with normal approaches to asynchronous programming is that they’re\nall-or-nothing propositions. You rewrite all your code so none of it blocks or you’re\njust wasting your time.\n—Alvaro Videla and Jason J. W. Williams, RabbitMQ in Action\nI chose that epigraph for this chapter for two reasons. At a high level, it reminds us to\navoid blocking the event loop by delegating slow tasks to a different processing unit,\nfrom a simple thread all the way to a distributed task queue. At a lower level, it is also\na warning: once you write your first async def, your program is inevitably going to\nhave more and more async def, await, async with, and async for. And using non-\nasynchronous libraries suddenly becomes a challenge.\nAfter the simple spinner examples in Chapter 19, here our main focus was asynchro‐\nnous programming with native coroutines, starting with the blogdom.py DNS prob‐\ning example, followed by the concept of awaitables. While reading the source code of\nflags_asyncio.py, we found the first example of an asynchronous context manager.\nThe more advanced variations of the flag downloading program introduced two\npowerful functions: the asyncio.as_completed generator and the loop.run_in_exec\nutor coroutine. We also saw the concept and application of a semaphore to limit the\nnumber of concurrent downloads—as expected from well-behaved HTTP clients.\nChapter Summary \n| \n827",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 858,
      "chapter": null,
      "content": "Server-side asynchronous programming was presented through the mojifinder exam‐\nples: a FastAPI web service and tcp_mojifinder.py—the latter using just asyncio and\nthe TCP protocol.\nAsynchronous iteration and asynchronous iterables were the next major topic, with\nsections on async for, Python’s async console, asynchronous generators, asynchro‐\nnous generator expressions, and asynchronous comprehensions.\nThe last example in the chapter was blogdom.py rewritten with the Curio framework,\nto demonstrate how Python’s asynchronous features are not tied to the asyncio\npackage. Curio also showcases the concept of structured concurrency, which may have\nan industry-wide impact, bringing more clarity to concurrent code.\nFinally, the sections under “How Async Works and How It Doesn’t” on page 825\ndiscuss the main appeal of asynchronous programming, the misconception of “I/O-\nbound systems,” and dealing with the inevitable CPU-bound parts of your program.\nFurther Reading\nDavid Beazley’s PyOhio 2016 keynote “Fear and Awaiting in Async” is a fantastic,\nlive-coded introduction to the potential of the language features made possible by\nYury Selivanov’s contribution of the async/await keywords in Python 3.5. At one\npoint, Beazley complains that await can’t be used in list comprehensions, but that\nwas fixed by Selivanov in PEP 530—Asynchronous Comprehensions, implemented\nin Python 3.6 later in that same year. Apart from that, everything else in Beazley’s\nkeynote is timeless, as he demonstrates how the asynchronous objects we saw in this\nchapter work, without the help of any framework—just a simple run function\nusing .send(None) to drive coroutines. Only at the very end Beazley shows Curio,\nwhich he started that year as an experiment to see how far can you go doing asyn‐\nchronous programming without a foundation of callbacks or futures, just coroutines.\nAs it turns out, you can go very far—as demonstrated by the evolution of Curio and\nthe later creation of Trio by Nathaniel J. Smith. Curio’s documentation has links to\nmore talks by Beazley on the subject.\nBesides starting Trio, Nathaniel J. Smith wrote two deep blog posts that I highly rec‐\nommend: “Some thoughts on asynchronous API design in a post-async/await world”,\ncontrasting the design of Curio with that of asyncio,and “Notes on structured concur‐\nrency, or: Go statement considered harmful”, about structured concurrency. Smith\nalso gave a long and informative answer to the question: “What is the core difference\nbetween asyncio and trio?” on StackOverflow.\nTo learn more about the asyncio package, I’ve mentioned the best written resources I\nknow at the start of this chapter: the official documentation after the outstanding\noverhaul started by Yury Selivanov in 2018, and Caleb Hattingh’s book Using Asyncio\nin Python (O’Reilly). In the official documentation, make sure to read “Developing\n828 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 859,
      "chapter": null,
      "content": "with asyncio”: documenting the asyncio debug mode, and also discussing common\nmistakes and traps and how to avoid them.\nFor a very accessible, 30-minute introduction to asynchronous programming in gen‐\neral and also asyncio, watch Miguel Grinberg’s “Asynchronous Python for the Com‐\nplete Beginner”, presented at PyCon 2017. Another great introduction is\n“Demystifying Python’s Async and Await Keywords”, presented by Michael Ken‐\nnedy, where among other things I learned about the unsync library that provides a\ndecorator to delegate the execution of coroutines, I/O-bound functions, and CPU-\nbound functions to asyncio, threading, or multiprocessing as needed.\nAt EuroPython 2019, Lynn Root—a global leader of PyLadies—presented the excel‐\nlent “Advanced asyncio: Solving Real-world Production Problems”, informed by her\nexperience using Python as a staff engineer at Spotify.\nIn 2020, Łukasz Langa recorded a series of great videos about asyncio, starting with\n“Learn Python’s AsyncIO #1—The Async Ecosystem”. Langa also made the super\ncool video “AsyncIO + Music” for PyCon 2020 that not only shows asyncio applied in\na very concrete event-oriented domain, but also explains it from the ground up.\nAnother area dominated by event-oriented programming is embedded systems.\nThat’s why Damien George added support for async/await in his MicroPython inter‐\npreter for microcontrollers. At PyCon Australia 2018, Matt Trentini demonstrated\nthe uasyncio library, a subset of asyncio that is part of MicroPython’s standard\nlibrary.\nFor higher-level thinking about async programming in Python, read the blog post\n“Python async frameworks—Beyond developer tribalism” by Tom Christie.\nFinally, I recommend “What Color Is Your Function?” by Bob Nystrom, discussing\nthe incompatible execution models of plain functions versus async functions—a.k.a.\ncoroutines—in JavaScript, Python, C#, and other languages. Spoiler alert: Nystrom’s\nconclusion is that the language that got this right is Go, where all functions are the\nsame color. I like that about Go. But I also think Nathaniel J. Smith has a point when\nhe wrote “Go statement considered harmful”. Nothing is perfect, and concurrent\nprogramming is always hard.\nSoapbox\nHow a Slow Function Almost Spoiled the uvloop Benchmarks\nIn 2016, Yury Selivanov released uvloop, “a fast, drop-in replacement of the built-in\nasyncio event loop.” The benchmarks presented in Selivanov’s blog post announcing\nthe library in 2016 are very impressive. He wrote: “it is at least 2x faster than nodejs,\ngevent, as well as any other Python asynchronous framework. The performance of\nuvloop-based asyncio is close to that of Go programs.”\nFurther Reading \n| \n829",
      "content_length": 2685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 860,
      "chapter": null,
      "content": "20 Using a single thread was the default setting until Go 1.5 was released. Years before, Go had already earned a\nwell-deserved reputation for enabling highly concurrent networked systems. One more evidence that concur‐\nrency doesn’t require multiple threads or CPU cores.\nHowever, the post reveals that uvloop is able to match the performance of Go under\ntwo conditions:\n1. Go is configured to use a single thread. That makes the Go runtime behave simi‐\nlarly to asyncio: concurrency is achieved via multiple coroutines driven by an\nevent loop, all in the same thread.20\n2. The Python 3.5 code uses httptools in addition to uvloop itself.\nSelivanov explains that he wrote httptools after benchmarking uvloop with aiohttp—\none of the first full-featured HTTP libraries built on asyncio:\nHowever, the performance bottleneck in aiohttp turned out to be its HTTP parser,\nwhich is so slow, that it matters very little how fast the underlying I/O library is. To\nmake things more interesting, we created a Python binding for http-parser (Node.js\nHTTP parser C library, originally developed for NGINX). The library is called\nhttptools, and is available on Github and PyPI.\nNow think about that: Selivanov’s HTTP performance tests consisted of a simple\necho server written in the different languages/libraries, pounded by the wrk bench‐\nmarking tool. Most developers would consider a simple echo server an “I/O-bound\nsystem,” right? But it turned out that parsing HTTP headers is CPU bound, and it\nhad a slow Python implementation in aiohttp in when Selivanov did the benchmarks\nin 2016. Whenever a function written in Python was parsing headers, the event loop\nwas blocked. The impact was so significant that Selivanov went to the extra trouble of\nwriting httptools. Without optimizing the CPU-bound code, the performance gains of\na faster event loop were lost.\nDeath by a Thousand Cuts\nInstead of a simple echo server, imagine a complex and evolving Python system with\ntens of thousands of lines of asynchronous code, interfacing with many external libra‐\nries. Years ago I was asked to help diagnose performance problems in a system like\nthat. It was written in Python 2.7 with the Twisted framework—a solid library and in\nmany ways a precursor to asyncio itself.\nPython was used to build a façade for the web UI, integrating functionality provided\nby preexisting libraries and command-line tools written in other languages—but not\ndesigned for concurrent execution.\n830 \n| \nChapter 21: Asynchronous Programming",
      "content_length": 2506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 861,
      "chapter": null,
      "content": "21 Regardless of technical choices, this was probably the biggest mistake in this project: the stakeholders did not\ngo for an MVP approach—delivering a Minimum Viable Product as soon as possible, and then adding fea‐\ntures at a steady pace.\nThe project was ambitious; it had been in development for more than a year already,\nbut it was not in production yet.21 Over time, the developers noticed that the\nperformance of the whole system was decreasing, and they were having a hard time\nfinding the bottlenecks.\nWhat was happening: with each added feature, more CPU-bound code was slowing\ndown Twisted’s event loop. Python’s role as a glue language meant there was a lot of\ndata parsing and conversion between formats. There wasn’t a single bottleneck: the\nproblem was spread over countless little functions added over months of develop‐\nment. Fixing that would require rethinking the architecture of the system, rewriting a\nlot of code, probably leveraging a task queue, and perhaps using microservices or cus‐\ntom libraries written in languages better suited for CPU-intensive concurrent pro‐\ncessing. The stakeholders were not prepared to make that additional investment, and\nthe project was cancelled shortly afterwards.\nWhen I told this story to Glyph Lefkowitz—founder the Twisted project—he said that\none of his priorities at the start of an asynchronous programming project is to decide\nwhich tools he will use to farm out the CPU-intensive tasks. This conversation with\nGlyph was the inspiration for “Avoiding CPU-Bound Traps” on page 826.\nFurther Reading \n| \n831",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 862,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 863,
      "chapter": null,
      "content": "PARTV\n\nMetaprogramming",
      "content_length": 22,
      "extraction_method": "OCR"
    },
    {
      "page_number": 864,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 865,
      "chapter": null,
      "content": "1 Alex Martelli, Anna Ravenscroft, and Steve Holden, Python in a Nutshell, 3rd ed. (O’Reilly), p. 123.\n2 Bertrand Meyer, Object-Oriented Software Construction, 2nd ed. (Pearson), p. 57.\nCHAPTER 22\nDynamic Attributes and Properties\nThe crucial importance of properties is that their existence makes it perfectly safe and\nindeed advisable for you to expose public data attributes as part of your class’s public\ninterface.\n— Martelli, Ravenscroft, and Holden, “Why properties are important”1\nData attributes and methods are collectively known as attributes in Python. A\nmethod is an attribute that is callable. Dynamic attributes present the same interface\nas data attributes—i.e., obj.attr—but are computed on demand. This follows Ber‐\ntrand Meyer’s Uniform Access Principle:\nAll services offered by a module should be available through a uniform notation,\nwhich does not betray whether they are implemented through storage or through\ncomputation.2\nThere are several ways to implement dynamic attributes in Python. This chapter cov‐\ners the simplest ways: the @property decorator and the __getattr__ special method.\nA user-defined class implementing __getattr__ can implement a variation of\ndynamic attributes that I call virtual attributes: attributes that are not explicitly\ndeclared anywhere in the source code of the class, and are not present in the instance\n__dict__, but may be retrieved elsewhere or computed on the fly whenever a user\ntries to read a nonexistent attribute like obj.no_such_attr.\nCoding dynamic and virtual attributes is the kind of metaprogramming that frame‐\nwork authors do. However, in Python the basic techniques are straightforward, so we\ncan use them in everyday data wrangling tasks. That’s how we’ll start this chapter.\n835",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 866,
      "chapter": null,
      "content": "3 OSCON—O’Reilly Open Source Conference—was a casualty of the COVID-19 pandemic. The original 744\nKB JSON file I used for these examples is no longer online as of January 10, 2021. You’ll find a copy of oscon‐\nfeed.json in the example code repository.\nWhat’s New in This Chapter\nMost of the updates to this chapter were motivated by a discussion of @func\ntools.cached_property (introduced in Python 3.8), as well as the combined use of\n@property with @functools.cache (new in 3.9). This affected the code for the\nRecord and Event classes that appear in “Computed Properties” on page 845. I also\nadded a refactoring to leverage the PEP 412—Key-Sharing Dictionary optimization.\nTo highlight more relevant features while keeping the examples readable, I removed\nsome nonessential code—merging the old DbRecord class into Record, replacing\nshelve.Shelve with a dict, and deleting the logic to download the OSCON dataset\n—which the examples now read from a local file included in the Fluent Python code\nrepository.\nData Wrangling with Dynamic Attributes\nIn the next few examples, we’ll leverage dynamic attributes to work with a JSON\ndataset published by O’Reilly for the OSCON 2014 conference. Example 22-1 shows\nfour records from that dataset.3\nExample 22-1. Sample records from osconfeed.json; some field contents abbreviated\n{ \"Schedule\":\n  { \"conferences\": [{\"serial\": 115 }],\n    \"events\": [\n      { \"serial\": 34505,\n        \"name\": \"Why Schools Don´t Use Open Source to Teach Programming\",\n        \"event_type\": \"40-minute conference session\",\n        \"time_start\": \"2014-07-23 11:30:00\",\n        \"time_stop\": \"2014-07-23 12:10:00\",\n        \"venue_serial\": 1462,\n        \"description\": \"Aside from the fact that high school programming...\",\n        \"website_url\": \"http://oscon.com/oscon2014/public/schedule/detail/34505\",\n        \"speakers\": [157509],\n        \"categories\": [\"Education\"] }\n    ],\n    \"speakers\": [\n      { \"serial\": 157509,\n        \"name\": \"Robert Lefkowitz\",\n        \"photo\": null,\n        \"url\": \"http://sharewave.com/\",\n        \"position\": \"CTO\",\n836 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 867,
      "chapter": null,
      "content": "\"affiliation\": \"Sharewave\",\n        \"twitter\": \"sharewaveteam\",\n        \"bio\": \"Robert ´r0ml´ Lefkowitz is the CTO at Sharewave, a startup...\" }\n    ],\n    \"venues\": [\n      { \"serial\": 1462,\n        \"name\": \"F151\",\n        \"category\": \"Conference Venues\" }\n    ]\n  }\n}\nExample 22-1 shows 4 of the 895 records in the JSON file. The entire dataset is a sin‐\ngle JSON object with the key \"Schedule\", and its value is another mapping with four\nkeys: \"conferences\", \"events\", \"speakers\", and \"venues\". Each of those four keys\nmaps to a list of records. In the full dataset, the \"events\", \"speakers\", and \"venues\"\nlists have dozens or hundreds of records, while \"conferences\" has only that one\nrecord shown in Example 22-1. Every record has a \"serial\" field, which is a unique\nidentifier for the record within the list.\nI used Python’s console to explore the dataset, as shown in Example 22-2.\nExample 22-2. Interactive exploration of osconfeed.json\n>>> import json\n>>> with open('data/osconfeed.json') as fp:\n...     feed = json.load(fp)  \n>>> sorted(feed['Schedule'].keys())  \n['conferences', 'events', 'speakers', 'venues']\n>>> for key, value in sorted(feed['Schedule'].items()):\n...     print(f'{len(value):3} {key}')  \n...\n  1 conferences\n484 events\n357 speakers\n 53 venues\n>>> feed['Schedule']['speakers'][-1]['name']  \n'Carina C. Zona'\n>>> feed['Schedule']['speakers'][-1]['serial']  \n141590\n>>> feed['Schedule']['events'][40]['name']\n'There *Will* Be Bugs'\n>>> feed['Schedule']['events'][40]['speakers']  \n[3471, 5199]\nfeed is a dict holding nested dicts and lists, with string and integer values.\nList the four record collections inside \"Schedule\".\nData Wrangling with Dynamic Attributes \n| \n837",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 868,
      "chapter": null,
      "content": "4 Two examples are AttrDict and addict.\nDisplay record counts for each collection.\nNavigate through the nested dicts and lists to get the name of the last speaker.\nGet the serial number of that same speaker.\nEach event has a 'speakers' list with zero or more speaker serial numbers.\nExploring JSON-Like Data with Dynamic Attributes\nExample 22-2 is simple enough, but the syntax feed['Schedule']['events'][40]\n['name'] is cumbersome. In JavaScript, you can get the same value by writing\nfeed.Schedule.events[40].name. It’s easy to implement a dict-like class that does\nthe same in Python—there are plenty of implementations on the web.4 I wrote\nFrozenJSON, which is simpler than most recipes because it supports reading only: it’s\njust for exploring the data. FrozenJSON is also recursive, dealing automatically with\nnested mappings and lists.\nExample 22-3 is a demonstration of FrozenJSON, and the source code is shown in\nExample 22-4.\nExample 22-3. FrozenJSON from Example 22-4 allows reading attributes like name, and\ncalling methods like .keys() and .items()\n    >>> import json\n    >>> raw_feed = json.load(open('data/osconfeed.json'))\n    >>> feed = FrozenJSON(raw_feed)  \n    >>> len(feed.Schedule.speakers)  \n    357\n    >>> feed.keys()\n    dict_keys(['Schedule'])\n    >>> sorted(feed.Schedule.keys())  \n    ['conferences', 'events', 'speakers', 'venues']\n    >>> for key, value in sorted(feed.Schedule.items()): \n    ...     print(f'{len(value):3} {key}')\n    ...\n      1 conferences\n    484 events\n    357 speakers\n     53 venues\n    >>> feed.Schedule.speakers[-1].name  \n    'Carina C. Zona'\n    >>> talk = feed.Schedule.events[40]\n    >>> type(talk)  \n838 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 869,
      "chapter": null,
      "content": "<class 'explore0.FrozenJSON'>\n    >>> talk.name\n    'There *Will* Be Bugs'\n    >>> talk.speakers  \n    [3471, 5199]\n    >>> talk.flavor  \n    Traceback (most recent call last):\n      ...\n    KeyError: 'flavor'\nBuild a FrozenJSON instance from the raw_feed made of nested dicts and lists.\nFrozenJSON allows traversing nested dicts by using attribute notation; here we\nshow the length of the list of speakers.\nMethods of the underlying dicts can also be accessed, like .keys(), to retrieve the\nrecord collection names.\nUsing items(), we can retrieve the record collection names and their contents, to\ndisplay the len() of each of them.\nA list, such as feed.Schedule.speakers, remains a list, but the items inside are\nconverted to FrozenJSON if they are mappings.\nItem 40 in the events list was a JSON object; now it’s a FrozenJSON instance.\nEvent records have a speakers list with speaker serial numbers.\nTrying to read a missing attribute raises KeyError, instead of the usual\nAttributeError.\nThe keystone of the FrozenJSON class is the __getattr__ method, which we already\nused in the Vector example in “Vector Take #3: Dynamic Attribute Access” on page\n407, to retrieve Vector components by letter: v.x, v.y, v.z, etc. It’s essential to recall\nthat the __getattr__ special method is only invoked by the interpreter when the\nusual process fails to retrieve an attribute (i.e., when the named attribute cannot be\nfound in the instance, nor in the class or in its superclasses).\nThe last line of Example 22-3 exposes a minor issue with my code: trying to read a\nmissing attribute should raise AttributeError, and not KeyError as shown. When I\nimplemented the error handling to do that, the __getattr__ method became twice as\nlong, distracting from the most important logic I wanted to show. Given that users\nwould know that a FrozenJSON is built from mappings and lists, I think the KeyError\nis not too confusing.\nData Wrangling with Dynamic Attributes \n| \n839",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 870,
      "chapter": null,
      "content": "Example 22-4. explore0.py: turn a JSON dataset into a FrozenJSON holding nested\nFrozenJSON objects, lists, and simple types\nfrom collections import abc\nclass FrozenJSON:\n    \"\"\"A read-only façade for navigating a JSON-like object\n       using attribute notation\n    \"\"\"\n    def __init__(self, mapping):\n        self.__data = dict(mapping)  \n    def __getattr__(self, name):  \n        try:\n            return getattr(self.__data, name)  \n        except AttributeError:\n            return FrozenJSON.build(self.__data[name])  \n    def __dir__(self):  \n        return self.__data.keys()\n    @classmethod\n    def build(cls, obj):  \n        if isinstance(obj, abc.Mapping):  \n            return cls(obj)\n        elif isinstance(obj, abc.MutableSequence):  \n            return [cls.build(item) for item in obj]\n        else:  \n            return obj\nBuild a dict from the mapping argument. This ensures we get a mapping or\nsomething that can be converted to one. The double-underscore prefix on\n__data makes it a private attribute.\n__getattr__ is called only when there’s no attribute with that name.\nIf name matches an attribute of the instance __data dict, return that. This is\nhow calls like feed.keys() are handled: the keys method is an attribute of the\n__data dict.\n840 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 871,
      "chapter": null,
      "content": "5 The expression self.__data[name] is where a KeyError exception may occur. Ideally, it should be handled\nand an AttributeError raised instead, because that’s what is expected from __getattr__. The diligent reader\nis invited to code the error handling as an exercise.\n6 The source of the data is JSON, and the only collection types in JSON data are dict and list.\nOtherwise, fetch the item with the key name from self.__data, and return the\nresult of calling FrozenJSON.build() on that.5\nImplementing __dir__ suports the dir() built-in, which in turns supports auto-\ncompletion in the standard Python console as well as IPython, Jupyter Notebook,\netc. This simple code will enable recursive auto-completion based on the keys in\nself.__data, because __getattr__ builds FrozenJSON instances on the fly—use‐\nful for interactive exploration of the data.\nThis is an alternate constructor, a common use for the @classmethod decorator.\nIf obj is a mapping, build a FrozenJSON with it. This is an example of goose typ‐\ning—see “Goose Typing” on page 442 if you need a refresher.\nIf it is a MutableSequence, it must be a list,6 so we build a list by passing each\nitem in obj recursively to .build().\nIf it’s not a dict or a list, return the item as it is.\nA FrozenJSON instance has the __data private instance attribute stored under the\nname _FrozenJSON__data, as explained in “Private and ‘Protected’ Attributes in\nPython” on page 382. Attempts to retrieve attributes by other names will trigger\n__getattr__. This method will first look if the self.__data dict has an attribute\n(not a key!) by that name; this allows FrozenJSON instances to handle dict methods\nsuch as items, by delegating to self.__data.items(). If self.__data doesn’t have\nan attribute with the given name, __getattr__ uses name as a key to retrieve an item\nfrom self.__data, and passes that item to FrozenJSON.build. This allows navigating\nthrough nested structures in the JSON data, as each nested mapping is converted to\nanother FrozenJSON instance by the build class method.\nNote that FrozenJSON does not transform or cache the original dataset. As we tra‐\nverse the data, __getattr__ creates FrozenJSON instances again and again. That’s OK\nfor a dataset of this size, and for a script that will only be used to explore or convert\nthe data.\nData Wrangling with Dynamic Attributes \n| \n841",
      "content_length": 2352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 872,
      "chapter": null,
      "content": "Any script that generates or emulates dynamic attribute names from arbitrary sour‐\nces must deal with one issue: the keys in the original data may not be suitable\nattribute names. The next section addresses this.\nThe Invalid Attribute Name Problem\nThe FrozenJSON code doesn’t handle attribute names that are Python keywords. For\nexample, if you build an object like this:\n>>> student = FrozenJSON({'name': 'Jim Bo', 'class': 1982})\nYou won’t be able to read student.class because class is a reserved keyword in\nPython:\n>>> student.class\n  File \"<stdin>\", line 1\n    student.class\n         ^\nSyntaxError: invalid syntax\nYou can always do this, of course:\n>>> getattr(student, 'class')\n1982\nBut the idea of FrozenJSON is to provide convenient access to the data, so a better\nsolution is checking whether a key in the mapping given to FrozenJSON.__init__ is\na keyword, and if so, append an _ to it, so the attribute can be read like this:\n>>> student.class_\n1982\nThis can be achieved by replacing the one-liner __init__ from Example 22-4 with\nthe version in Example 22-5.\nExample 22-5. explore1.py: append an _ to attribute names that are Python keywords\n    def __init__(self, mapping):\n        self.__data = {}\n        for key, value in mapping.items():\n            if keyword.iskeyword(key):  \n                key += '_'\n            self.__data[key] = value\nThe keyword.iskeyword(…) function is exactly what we need; to use it, the key\nword module must be imported, which is not shown in this snippet.\nA similar problem may arise if a key in a JSON record is not a valid Python identifier:\n>>> x = FrozenJSON({'2be':'or not'})\n>>> x.2be\n842 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 873,
      "chapter": null,
      "content": "File \"<stdin>\", line 1\n    x.2be\n      ^\nSyntaxError: invalid syntax\nSuch problematic keys are easy to detect in Python 3 because the str class provides\nthe s.isidentifier() method, which tells you whether s is a valid Python identifier\naccording to the language grammar. But turning a key that is not a valid identifier\ninto a valid attribute name is not trivial. One solution would be to implement __geti\ntem__ to allow attribute access using notation like x['2be']. For the sake of simplic‐\nity, I will not worry about this issue.\nAfter giving some thought to the dynamic attribute names, let’s turn to another\nessential feature of FrozenJSON: the logic of the build class method. Fro\nzen.JSON.build is used by __getattr__ to return a different type of object depend‐\ning on the value of the attribute being accessed: nested structures are converted to\nFrozenJSON instances or lists of FrozenJSON instances.\nInstead of a class method, the same logic could be implemented as the __new__ spe‐\ncial method, as we’ll see next.\nFlexible Object Creation with __new__\nWe often refer to __init__ as the constructor method, but that’s because we adopted\njargon from other languages. In Python, __init__ gets self as the first argument,\ntherefore the object already exists when __init__ is called by the interpreter. Also,\n__init__ cannot return anything. So it’s really an initializer, not a constructor.\nWhen a class is called to create an instance, the special method that Python calls on\nthat class to construct an instance is __new__. It’s a class method, but gets special\ntreatment, so the @classmethod decorator is not applied to it. Python takes the\ninstance returned by __new__ and then passes it as the first argument self of\n__init__. We rarely need to code __new__, because the implementation inherited\nfrom object suffices for the vast majority of use cases.\nIf necessary, the __new__ method can also return an instance of a different class.\nWhen that happens, the interpreter does not call __init__. In other words, Python’s\nlogic for building an object is similar to this pseudocode:\n# pseudocode for object construction\ndef make(the_class, some_arg):\n    new_object = the_class.__new__(some_arg)\n    if isinstance(new_object, the_class):\n        the_class.__init__(new_object, some_arg)\n    return new_object\n# the following statements are roughly equivalent\nData Wrangling with Dynamic Attributes \n| \n843",
      "content_length": 2414,
      "extraction_method": "Direct"
    },
    {
      "page_number": 874,
      "chapter": null,
      "content": "x = Foo('bar')\nx = make(Foo, 'bar')\nExample 22-6 shows a variation of FrozenJSON where the logic of the former build\nclass method was moved to __new__.\nExample 22-6. explore2.py: using __new__ instead of build to construct new objects\nthat may or may not be instances of FrozenJSON\nfrom collections import abc\nimport keyword\nclass FrozenJSON:\n    \"\"\"A read-only façade for navigating a JSON-like object\n       using attribute notation\n    \"\"\"\n    def __new__(cls, arg):  \n        if isinstance(arg, abc.Mapping):\n            return super().__new__(cls)  \n        elif isinstance(arg, abc.MutableSequence):  \n            return [cls(item) for item in arg]\n        else:\n            return arg\n    def __init__(self, mapping):\n        self.__data = {}\n        for key, value in mapping.items():\n            if keyword.iskeyword(key):\n                key += '_'\n            self.__data[key] = value\n    def __getattr__(self, name):\n        try:\n            return getattr(self.__data, name)\n        except AttributeError:\n            return FrozenJSON(self.__data[name])  \n    def __dir__(self):\n        return self.__data.keys()\nAs a class method, the first argument __new__ gets is the class itself, and the\nremaining arguments are the same that __init__ gets, except for self.\nThe default behavior is to delegate to the __new__ of a superclass. In this case, we\nare calling __new__ from the object base class, passing FrozenJSON as the only\nargument.\nThe remaining lines of __new__ are exactly as in the old build method.\n844 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 875,
      "chapter": null,
      "content": "This was where FrozenJSON.build was called before; now we just call the\nFrozenJSON class, which Python handles by calling FrozenJSON.__new__.\nThe __new__ method gets the class as the first argument because, usually, the created\nobject will be an instance of that class. So, in FrozenJSON.__new__, when the\nexpression super().__new__(cls) effectively calls object.__new__(FrozenJSON),\nthe instance built by the object class is actually an instance of FrozenJSON. The\n__class__ attribute of the new instance will hold a reference to FrozenJSON, even\nthough the actual construction is performed by object.__new__, implemented in C,\nin the guts of the interpreter.\nThe OSCON JSON dataset is structured in a way that is not helpful for interactive\nexploration. For example, the event at index 40, titled 'There *Will* Be Bugs' has\ntwo speakers, 3471 and 5199. Finding the names of the speakers is awkward, because\nthose are serial numbers and the Schedule.speakers list is not indexed by them. To\nget each speaker, we must iterate over that list until we find a record with a matching\nserial number. Our next task is restructuring the data to prepare for automatic\nretrieval of linked records.\nComputed Properties\nWe first saw the @property decorator in Chapter 11, in the section, “A Hashable Vec‐\ntor2d” on page 374. In Example 11-7, I used two properties in Vector2d just to make\nthe x and y attributes read-only. Here we will see properties that compute values,\nleading to a discussion of how to cache such values.\nThe records in the 'events' list of the OSCON JSON data contain integer serial\nnumbers pointing to records in the 'speakers' and 'venues' lists. For example, this\nis the record for a conference talk (with an elided description):\n{ \"serial\": 33950,\n  \"name\": \"There *Will* Be Bugs\",\n  \"event_type\": \"40-minute conference session\",\n  \"time_start\": \"2014-07-23 14:30:00\",\n  \"time_stop\": \"2014-07-23 15:10:00\",\n  \"venue_serial\": 1449,\n  \"description\": \"If you're pushing the envelope of programming...\",\n  \"website_url\": \"http://oscon.com/oscon2014/public/schedule/detail/33950\",\n  \"speakers\": [3471, 5199],\n  \"categories\": [\"Python\"] }\nWe will implement an Event class with venue and speakers properties to return the\nlinked data automatically—in other words, “dereferencing” the serial number. Given\nan Event instance, Example 22-7 shows the desired behavior.\nComputed Properties \n| \n845",
      "content_length": 2399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 876,
      "chapter": null,
      "content": "Example 22-7. Reading venue and speakers returns Record objects\n    >>> event  \n    <Event 'There *Will* Be Bugs'>\n    >>> event.venue  \n    <Record serial=1449>\n    >>> event.venue.name  \n    'Portland 251'\n    >>> for spkr in event.speakers:  \n    ...     print(f'{spkr.serial}: {spkr.name}')\n    ...\n    3471: Anna Martelli Ravenscroft\n    5199: Alex Martelli\nGiven an Event instance…\n…reading event.venue returns a Record object instead of a serial number.\nNow it’s easy to get the name of the venue.\nThe event.speakers property returns a list of Record instances.\nAs usual, we will build the code step-by-step, starting with the Record class and a\nfunction to read the JSON data and return a dict with Record instances.\nStep 1: Data-Driven Attribute Creation\nExample 22-8 shows the doctest to guide this first step.\nExample 22-8. Test-driving schedule_v1.py (from Example 22-9)\n    >>> records = load(JSON_PATH)  \n    >>> speaker = records['speaker.3471']  \n    >>> speaker  \n    <Record serial=3471>\n    >>> speaker.name, speaker.twitter  \n    ('Anna Martelli Ravenscroft', 'annaraven')\nload a dict with the JSON data.\nThe keys in records are strings built from the record type and serial number.\nspeaker is an instance of the Record class defined in Example 22-9.\nFields from the original JSON can be retrieved as Record instance attributes.\nThe code for schedule_v1.py is in Example 22-9.\n846 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 877,
      "chapter": null,
      "content": "Example 22-9. schedule_v1.py: reorganizing the OSCON schedule data\nimport json\nJSON_PATH = 'data/osconfeed.json'\nclass Record:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)  \n    def __repr__(self):\n        return f'<{self.__class__.__name__} serial={self.serial!r}>'  \ndef load(path=JSON_PATH):\n    records = {}  \n    with open(path) as fp:\n        raw_data = json.load(fp)  \n    for collection, raw_records in raw_data['Schedule'].items():  \n        record_type = collection[:-1]  \n        for raw_record in raw_records:\n            key = f'{record_type}.{raw_record[\"serial\"]}' \n            records[key] = Record(**raw_record)  \n    return records\nThis is a common shortcut to build an instance with attributes created from key‐\nword arguments (detailed explanation follows).\nUse the serial field to build the custom Record representation shown in\nExample 22-8.\nload will ultimately return a dict of Record instances.\nParse the JSON, returning native Python objects: lists, dicts, strings, numbers,\netc.\nIterate over the four top-level lists named 'conferences', 'events', 'speak\ners', and 'venues'.\nrecord_type is the list name without the last character, so speakers becomes\nspeaker. In Python ≥ 3.9 we can do this more explicitly with collection.remove\nsuffix('s')—see PEP 616—String methods to remove prefixes and suffixes.\nBuild the key in the format 'speaker.3471'.\nCreate a Record instance and save it in records with the key.\nComputed Properties \n| \n847",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 878,
      "chapter": null,
      "content": "7 By the way, Bunch is the name of the class used by Alex Martelli to share this tip in a recipe from 2001 titled\n“The simple but handy ‘collector of a bunch of named stuff’ class”.\nThe Record.__init__ method illustrates an old Python hack. Recall that the\n__dict__ of an object is where its attributes are kept—unless __slots__ is declared\nin the class, as we saw in “Saving Memory with __slots__” on page 384. So, updating\nan instance __dict__ with a mapping is a quick way to create a bunch of attributes in\nthat instance.7\nDepending on the application, the Record class may need to deal\nwith keys that are not valid attribute names, as we saw in “The\nInvalid Attribute Name Problem” on page 842. Dealing with that\nissue would distract from the key idea of this example, and is not a\nproblem in the dataset we are reading.\nThe definition of Record in Example 22-9 is so simple that you may be wondering\nwhy I did not use it before, instead of the more complicated FrozenJSON. There are\ntwo reasons. First, FrozenJSON works by recursively converting the nested mappings\nand lists; Record doesn’t need that because our converted dataset doesn’t have map‐\npings nested in mappings or lists. The records contain only strings, integers, lists of\nstrings, and lists of integers. Second reason: FrozenJSON provides access to the\nembedded __data dict attributes—which we used to invoke methods like .keys()—\nand now we don’t need that functionality either.\nThe Python standard library provides classes similar to Record,\nwhere each instance has an arbitrary set of attributes built from\nkeyword arguments given to __init__: types.SimpleNamespace,\nargparse.Namespace, \nand \nmultiprocessing.managers.Name\nspace. I wrote the simpler Record class to highlight the essential\nidea: __init__ updating the instance __dict__.\nAfter reorganizing the schedule dataset, we can enhance the Record class to automat‐\nically retrieve venue and speaker records referenced in an event record. We’ll use\nproperties to do that in the next examples.\nStep 2: Property to Retrieve a Linked Record\nThe goal of this next version is: given an event record, reading its venue property will\nreturn a Record. This is similar to what the Django ORM does when you access a\nForeignKey field: instead of the key, you get the linked model object.\n848 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 879,
      "chapter": null,
      "content": "We’ll start with the venue property. See the partial interaction in Example 22-10 as an\nexample.\nExample 22-10. Extract from the doctests of schedule_v2.py\n    >>> event = Record.fetch('event.33950')  \n    >>> event  \n    <Event 'There *Will* Be Bugs'>\n    >>> event.venue  \n    <Record serial=1449>\n    >>> event.venue.name  \n    'Portland 251'\n    >>> event.venue_serial  \n    1449\nThe Record.fetch static method gets a Record or an Event from the dataset.\nNote that event is an instance of the Event class.\nAccessing event.venue returns a Record instance.\nNow it’s easy to find out the name of an event.venue.\nThe Event instance also has a venue_serial attribute, from the JSON data.\nEvent is a subclass of Record adding a venue to retrieve linked records, and a special‐\nized __repr__ method.\nThe code for this section is in the schedule_v2.py module in the Fluent Python code\nrepository. The example has nearly 60 lines, so I’ll present it in parts, starting with the\nenhanced Record class.\nExample 22-11. schedule_v2.py: Record class with a new fetch method\nimport inspect  \nimport json\nJSON_PATH = 'data/osconfeed.json'\nclass Record:\n    __index = None  \n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n    def __repr__(self):\n        return f'<{self.__class__.__name__} serial={self.serial!r}>'\nComputed Properties \n| \n849",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 880,
      "chapter": null,
      "content": "@staticmethod  \n    def fetch(key):\n        if Record.__index is None:  \n            Record.__index = load()\n        return Record.__index[key]  \ninspect will be used in load, listed in Example 22-13.\nThe __index private class attribute will eventually hold a reference to the dict\nreturned by load.\nfetch is a staticmethod to make it explicit that its effect is not influenced by the\ninstance or class on which it is called.\nPopulate the Record.__index, if needed.\nUse it to retrieve the record with the given key.\nThis is one example where the use of staticmethod makes sense.\nThe fetch method always acts on the Record.__index class\nattribute, even if invoked from a subclass, like Event.fetch()—\nwhich we’ll soon explore. It would be misleading to code it as a\nclass method because the cls first argument would not be used.\nNow we get to the use of a property in the Event class, listed in Example 22-12.\nExample 22-12. schedule_v2.py: the Event class\nclass Event(Record):  \n    def __repr__(self):\n        try:\n            return f'<{self.__class__.__name__} {self.name!r}>'  \n        except AttributeError:\n            return super().__repr__()\n    @property\n    def venue(self):\n        key = f'venue.{self.venue_serial}'\n        return self.__class__.fetch(key)  \n850 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 881,
      "chapter": null,
      "content": "Event extends Record.\nIf the instance has a name attribute, it is used to produce a custom representation.\nOtherwise, delegate to the __repr__ from Record.\nThe venue property builds a key from the venue_serial attribute, and passes it\nto the fetch class method, inherited from Record (the reason for using\nself.__class__ is explained shortly).\nThe second line of the venue method of Example 22-12 returns self\n.__class__.fetch(key). Why not simply call self.fetch(key)? The simpler form\nworks with the specific OSCON dataset because there is no event record with a\n'fetch' key. But, if an event record had a key named 'fetch', then within that spe‐\ncific Event instance, the reference self.fetch would retrieve the value of that field,\ninstead of the fetch class method that Event inherits from Record. This is a subtle\nbug, and it could easily sneak through testing because it depends on the dataset.\nWhen creating instance attribute names from data, there is always\nthe risk of bugs due to shadowing of class attributes—such as\nmethods—or data loss through accidental overwriting of existing\ninstance attributes. These problems may explain why Python dicts\nare not like JavaScript objects in the first place.\nIf the Record class behaved more like a mapping, implementing a dynamic __geti\ntem__ instead of a dynamic __getattr__, there would be no risk of bugs from over‐\nwriting or shadowing. A custom mapping is probably the Pythonic way to implement\nRecord. But if I took that road, we’d not be studying the tricks and traps of dynamic\nattribute programming.\nThe final piece of this example is the revised load function in Example 22-13.\nExample 22-13. schedule_v2.py: the load function\ndef load(path=JSON_PATH):\n    records = {}\n    with open(path) as fp:\n        raw_data = json.load(fp)\n    for collection, raw_records in raw_data['Schedule'].items():\n        record_type = collection[:-1]  \n        cls_name = record_type.capitalize()  \n        cls = globals().get(cls_name, Record)  \n        if inspect.isclass(cls) and issubclass(cls, Record):  \n            factory = cls  \n        else:\n            factory = Record  \nComputed Properties \n| \n851",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 882,
      "chapter": null,
      "content": "for raw_record in raw_records:  \n            key = f'{record_type}.{raw_record[\"serial\"]}'\n            records[key] = factory(**raw_record)  \n    return records\nSo far, no changes from the load in schedule_v1.py (Example 22-9).\nCapitalize the record_type to get a possible class name; e.g., 'event' becomes\n'Event'.\nGet an object by that name from the module global scope; get the Record class if\nthere’s no such object.\nIf the object just retrieved is a class, and is a subclass of Record…\n…bind the factory name to it. This means factory may be any subclass of\nRecord, depending on the record_type.\nOtherwise, bind the factory name to Record.\nThe for loop that creates the key and saves the records is the same as before,\nexcept that…\n…the object stored in records is constructed by factory, which may be Record\nor a subclass like Event, selected according to the record_type.\nNote that the only record_type that has a custom class is Event, but if classes named\nSpeaker or Venue are coded, load will automatically use those classes when building\nand saving records, instead of the default Record class.\nWe’ll now apply the same idea to a new speakers property in the Events class.\nStep 3: Property Overriding an Existing Attribute\nThe name of the venue property in Example 22-12 does not match a field name in\nrecords of the \"events\" collection. Its data comes from a venue_serial field name.\nIn contrast, each record in the events collection has a speakers field with a list of\nserial numbers. We want to expose that information as a speakers property in Event\ninstances, which returns a list of Record instances. This name clash requires some\nspecial attention, as Example 22-14 reveals.\nExample 22-14. schedule_v3.py: the speakers property\n    @property\n    def speakers(self):\n852 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 883,
      "chapter": null,
      "content": "8 This is actually a downside of Meyer’s Uniform Access Principle, which I mentioned in the opening of this\nchapter. Read the optional “Soapbox” on page 875 if you’re interested in this discussion.\n        spkr_serials = self.__dict__['speakers']  \n        fetch = self.__class__.fetch\n        return [fetch(f'speaker.{key}')\n                for key in spkr_serials]  \nThe data we want is in a speakers attribute, but we must retrieve it directly from\nthe instance __dict__ to avoid a recursive call to the speakers property.\nReturn a list of all records with keys corresponding to the numbers in\nspkr_serials.\nInside the speakers method, trying to read self.speakers will invoke the property\nitself, quickly raising a RecursionError. However, if we read the same data via\nself.__dict__['speakers'], Python’s usual algorithm for retrieving attributes is\nbypassed, the property is not called, and the recursion is avoided. For this reason,\nreading or writing data directly to an object’s __dict__ is a common Python meta‐\nprogramming trick.\nThe interpreter evaluates obj.my_attr by first looking at the class\nof obj. If the class has a property with the my_attr name, that\nproperty shadows an instance attribute by the same name. Exam‐\nples in “Properties Override Instance Attributes” on page 861 will\ndemonstrate this, and Chapter 23 will reveal that a property is\nimplemented as a descriptor—a more powerful and general\nabstraction.\nAs I coded the list comprehension in Example 22-14, my programmer’s lizard brain\nthought: “This may be expensive.” Not really, because events in the OSCON dataset\nhave few speakers, so coding anything more complicated would be premature opti‐\nmization. However, caching a property is a common need—and there are caveats. So\nlet’s see how to do that in the next examples.\nStep 4: Bespoke Property Cache\nCaching properties is a common need because there is an expectation that an expres‐\nsion like event.venue should be inexpensive.8 Some form of caching could become\nnecessary if the Record.fetch method behind the Event properties needed to query a\ndatabase or a web API.\nComputed Properties \n| \n853",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 884,
      "chapter": null,
      "content": "In the first edition Fluent Python, I coded the custom caching logic for the speakers\nmethod, as shown in Example 22-15.\nExample 22-15. Custom caching logic using hasattr disables key-sharing optimization\n    @property\n    def speakers(self):\n        if not hasattr(self, '__speaker_objs'):  \n            spkr_serials = self.__dict__['speakers']\n            fetch = self.__class__.fetch\n            self.__speaker_objs = [fetch(f'speaker.{key}')\n                    for key in spkr_serials]\n        return self.__speaker_objs  \nIf the instance doesn’t have an attribute named __speaker_objs, fetch the\nspeaker objects and store them there.\nReturn self.__speaker_objs.\nThe handmade caching in Example 22-15 is straightforward, but creating an attribute\nafter the instance is initialized defeats the PEP 412—Key-Sharing Dictionary optimi‐\nzation, as explained in “Practical Consequences of How dict Works” on page 102.\nDepending on the size of the dataset, the difference in memory usage may be impor‐\ntant.\nA similar hand-rolled solution that works well with the key-sharing optimization\nrequires coding an __init__ for the Event class, to create the necessary\n__speaker_objs initialized to None, and then checking for that in the speakers\nmethod. See Example 22-16.\nExample 22-16. Storage defined in __init__ to leverage key-sharing optimization\nclass Event(Record):\n    def __init__(self, **kwargs):\n        self.__speaker_objs = None\n        super().__init__(**kwargs)\n# 15 lines omitted...\n    @property\n    def speakers(self):\n        if self.__speaker_objs is None:\n            spkr_serials = self.__dict__['speakers']\n            fetch = self.__class__.fetch\n            self.__speaker_objs = [fetch(f'speaker.{key}')\n                    for key in spkr_serials]\n        return self.__speaker_objs\n854 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 885,
      "chapter": null,
      "content": "Examples 22-15 and 22-16 illustrate simple caching techniques that are fairly com‐\nmon in legacy Python codebases. However, in multithreaded programs, handmade\ncaches like those introduce race conditions that may lead to corrupted data. If two\nthreads are reading a property that was not previously cached, the first thread will\nneed to compute the data for the cache attribute (__speaker_objs in the examples)\nand the second thread may read a cached value that is not yet complete.\nFortunately, Python 3.8 introduced the @functools.cached_property decorator,\nwhich is thread safe. Unfortunately, it comes with a couple of caveats, explained next.\nStep 5: Caching Properties with functools\nThe functools module provides three decorators for caching. We saw @cache and\n@lru_cache in “Memoization with functools.cache” on page 320 (Chapter 9). Python\n3.8 introduced @cached_property.\nThe functools.cached_property decorator caches the result of the method in an\ninstance attribute with the same name. For example, in Example 22-17, the value\ncomputed by the venue method is stored in a venue attribute in self. After that,\nwhen client code tries to read venue, the newly created venue instance attribute is\nused instead of the method.\nExample 22-17. Simple use of a @cached_property\n    @cached_property\n    def venue(self):\n        key = f'venue.{self.venue_serial}'\n        return self.__class__.fetch(key)\nIn “Step 3: Property Overriding an Existing Attribute” on page 852, we saw that a\nproperty shadows an instance attribute by the same name. If that is true, how can\n@cached_property work? If the property overrides the instance attribute, the venue\nattribute will be ignored and the venue method will always be called, computing the\nkey and running fetch every time!\nThe answer is a bit sad: cached_property is a misnomer. The @cached_property\ndecorator does not create a full-fledged property, it creates a nonoverriding descriptor.\nA descriptor is an object that manages the access to an attribute in another class. We\nwill dive into descriptors in Chapter 23. The property decorator is a high-level API\nto create an overriding descriptor. Chapter 23 will include a through explanation\nabout overriding versus nonoverriding descriptors.\nFor now, let us set aside the underlying implementation and focus on the differences\nbetween cached_property and property from a user’s point of view. Raymond Het‐\ntinger explains them very well in the Python docs:\nComputed Properties \n| \n855",
      "content_length": 2486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 886,
      "chapter": null,
      "content": "9 Source: @functools.cached_property documentation. I know Raymond Hettinger authored this explanation\nbecause he wrote it as a response to an issue I filed: bpo42781—functools.cached_property docs should\nexplain that it is non-overriding. Hettinger is a major contributor to the official Python docs and standard\nlibrary. He also wrote the excellent “Descriptor HowTo Guide”, a key resource for Chapter 23.\nThe mechanics of cached_property() are somewhat different from property(). A\nregular property blocks attribute writes unless a setter is defined. In contrast, a\ncached_property allows writes.\nThe cached_property decorator only runs on lookups and only when an attribute of\nthe same name doesn’t exist. When it does run, the cached_property writes to the\nattribute with the same name. Subsequent attribute reads and writes take precedence\nover the cached_property method and it works like a normal attribute.\nThe cached value can be cleared by deleting the attribute. This allows the cached_prop\nerty method to run again.9\nBack to our Event class: the specific behavior of @cached_property makes it unsuita‐\nble to decorate speakers, because that method relies on an existing attribute also\nnamed speakers, containing the serial numbers of the event speakers.\n@cached_property has some important limitations:\n• It cannot be used as a drop-in replacement to @property if the\ndecorated method already depends on an instance attribute\nwith the same name.\n• It cannot be used in a class that defines __slots__.\n• It defeats the key-sharing optimization of the instance\n__dict__, because it creates an instance attribute after\n__init__.\nDespite these limitations, @cached_property addresses a common need in a simple\nway, and it is thread safe. Its Python code is an example of using a reentrant lock.\nThe @cached_property documentation recommends an alternative solution that we\ncan use with speakers: stacking @property and @cache decorators, as shown in\nExample 22-18.\nExample 22-18. Stacking @property on @cache\n    @property  \n    @cache  \n    def speakers(self):\n        spkr_serials = self.__dict__['speakers']\n        fetch = self.__class__.fetch\n856 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 887,
      "chapter": null,
      "content": "return [fetch(f'speaker.{key}')\n                for key in spkr_serials]\nThe order is important: @property goes on top…\n…of @cache.\nRecall from “Stacked Decorators” on page 322 the meaning of that syntax. The top\nthree lines of Example 22-18 are similar to:\nspeakers = property(cache(speakers))\nThe @cache is applied to speakers, returning a new function. That function then is\ndecorated by @property, which replaces it with a newly constructed property.\nThis wraps up our discussion of read-only properties and caching decorators, explor‐\ning the OSCON dataset. In the next section, we start a new series of examples creat‐\ning read/write properties.\nUsing a Property for Attribute Validation\nBesides computing attribute values, properties are also used to enforce business rules\nby changing a public attribute into an attribute protected by a getter and setter\nwithout affecting client code. Let’s work through an extended example.\nLineItem Take #1: Class for an Item in an Order\nImagine an app for a store that sells organic food in bulk, where customers can order\nnuts, dried fruit, or cereals by weight. In that system, each order would hold a\nsequence of line items, and each line item could be represented by an instance of a\nclass, as in Example 22-19.\nExample 22-19. bulkfood_v1.py: the simplest LineItem class\nclass LineItem:\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\nThat’s nice and simple. Perhaps too simple. Example 22-20 shows a problem.\nUsing a Property for Attribute Validation \n| \n857",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 888,
      "chapter": null,
      "content": "10 Direct quote by Jeff Bezos in the Wall Street Journal story, “Birth of a Salesman” (October 15, 2011). Note that\nas of 2021, you need a subscription to read the article.\nExample 22-20. A negative weight results in a negative subtotal\n    >>> raisins = LineItem('Golden raisins', 10, 6.95)\n    >>> raisins.subtotal()\n    69.5\n    >>> raisins.weight = -20  # garbage in...\n    >>> raisins.subtotal()    # garbage out...\n    -139.0\nThis is a toy example, but not as fanciful as you may think. Here is a story from the\nearly days of Amazon.com:\nWe found that customers could order a negative quantity of books! And we would\ncredit their credit card with the price and, I assume, wait around for them to ship the\nbooks.\n— Jeff Bezos, founder and CEO of Amazon.com10\nHow do we fix this? We could change the interface of LineItem to use a getter and a\nsetter for the weight attribute. That would be the Java way, and it’s not wrong.\nOn the other hand, it’s natural to be able to set the weight of an item by just assign‐\ning to it; and perhaps the system is in production with other parts already accessing\nitem.weight directly. In this case, the Python way would be to replace the data\nattribute with a property.\nLineItem Take #2: A Validating Property\nImplementing a property will allow us to use a getter and a setter, but the interface of\nLineItem will not change (i.e., setting the weight of a LineItem will still be written as\nraisins.weight = 12).\nExample 22-21 lists the code for a read/write weight property.\nExample 22-21. bulkfood_v2.py: a LineItem with a weight property\nclass LineItem:\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight  \n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\n858 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 889,
      "chapter": null,
      "content": "@property  \n    def weight(self):  \n        return self.__weight  \n    @weight.setter  \n    def weight(self, value):\n        if value > 0:\n            self.__weight = value  \n        else:\n            raise ValueError('value must be > 0')  \nHere the property setter is already in use, making sure that no instances with\nnegative weight can be created.\n@property decorates the getter method.\nAll the methods that implement a property share the name of the public\nattribute: weight.\nThe actual value is stored in a private attribute __weight.\nThe decorated getter has a .setter attribute, which is also a decorator; this ties\nthe getter and setter together.\nIf the value is greater than zero, we set the private __weight.\nOtherwise, ValueError is raised.\nNote how a LineItem with an invalid weight cannot be created now:\n>>> walnuts = LineItem('walnuts', 0, 10.00)\nTraceback (most recent call last):\n    ...\nValueError: value must be > 0\nNow we have protected weight from users providing negative values. Although buy‐\ners usually can’t set the price of an item, a clerical error or a bug may create a LineI\ntem with a negative price. To prevent that, we could also turn price into a property,\nbut this would entail some repetition in our code.\nRemember the Paul Graham quote from Chapter 17: “When I see patterns in my pro‐\ngrams, I consider it a sign of trouble.” The cure for repetition is abstraction. There are\ntwo ways to abstract away property definitions: using a property factory or a descrip‐\ntor class. The descriptor class approach is more flexible, and we’ll devote Chapter 23\nto a full discussion of it. Properties are in fact implemented as descriptor classes\nUsing a Property for Attribute Validation \n| \n859",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 890,
      "chapter": null,
      "content": "themselves. But here we will continue our exploration of properties by implementing\na property factory as a function.\nBut before we can implement a property factory, we need to have a deeper under‐\nstanding of properties.\nA Proper Look at Properties\nAlthough often used as a decorator, the property built-in is actually a class. In\nPython, functions and classes are often interchangeable, because both are callable and\nthere is no new operator for object instantiation, so invoking a constructor is no dif‐\nferent from invoking a factory function. And both can be used as decorators, as long\nas they return a new callable that is a suitable replacement of the decorated callable.\nThis is the full signature of the property constructor:\nproperty(fget=None, fset=None, fdel=None, doc=None)\nAll arguments are optional, and if a function is not provided for one of them, the cor‐\nresponding operation is not allowed by the resulting property object.\nThe property type was added in Python 2.2, but the @ decorator syntax appeared\nonly in Python 2.4, so for a few years, properties were defined by passing the accessor\nfunctions as the first two arguments.\nThe “classic” syntax for defining properties without decorators is illustrated in\nExample 22-22.\nExample 22-22. bulkfood_v2b.py: same as Example 22-21, but without using\ndecorators\nclass LineItem:\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\n    def get_weight(self):  \n        return self.__weight\n    def set_weight(self, value):  \n        if value > 0:\n            self.__weight = value\n        else:\n            raise ValueError('value must be > 0')\n860 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 891,
      "chapter": null,
      "content": "weight = property(get_weight, set_weight)  \nA plain getter.\nA plain setter.\nBuild the property and assign it to a public class attribute.\nThe classic form is better than the decorator syntax in some situations; the code of\nthe property factory we’ll discuss shortly is one example. On the other hand, in a class\nbody with many methods, the decorators make it explicit which are the getters and\nsetters, without depending on the convention of using get and set prefixes in their\nnames.\nThe presence of a property in a class affects how attributes in instances of that class\ncan be found in a way that may be surprising at first. The next section explains.\nProperties Override Instance Attributes\nProperties are always class attributes, but they actually manage attribute access in the\ninstances of the class.\nIn “Overriding Class Attributes” on page 389 we saw that when an instance and its\nclass both have a data attribute by the same name, the instance attribute overrides, or\nshadows, the class attribute—at least when read through that instance.\nExample 22-23 illustrates this point.\nExample 22-23. Instance attribute shadows the class data attribute\n>>> class Class:  \n...     data = 'the class data attr'\n...     @property\n...     def prop(self):\n...         return 'the prop value'\n...\n>>> obj = Class()\n>>> vars(obj)  \n{}\n>>> obj.data  \n'the class data attr'\n>>> obj.data = 'bar' \n>>> vars(obj)  \n{'data': 'bar'}\n>>> obj.data  \n'bar'\n>>> Class.data  \n'the class data attr'\nA Proper Look at Properties \n| \n861",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 892,
      "chapter": null,
      "content": "Define Class with two class attributes: the data attribute and the prop property.\nvars returns the __dict__ of obj, showing it has no instance attributes.\nReading from obj.data retrieves the value of Class.data.\nWriting to obj.data creates an instance attribute.\nInspect the instance to see the instance attribute.\nNow reading from obj.data retrieves the value of the instance attribute. When\nread from the obj instance, the instance data shadows the class data.\nThe Class.data attribute is intact.\nNow, let’s try to override the prop attribute on the obj instance. Resuming the previ‐\nous console session, we have Example 22-24.\nExample 22-24. Instance attribute does not shadow the class property (continued from\nExample 22-23)\n>>> Class.prop  \n<property object at 0x1072b7408>\n>>> obj.prop  \n'the prop value'\n>>> obj.prop = 'foo'  \nTraceback (most recent call last):\n  ...\nAttributeError: can't set attribute\n>>> obj.__dict__['prop'] = 'foo'  \n>>> vars(obj)  \n{'data': 'bar', 'prop': 'foo'}\n>>> obj.prop  \n'the prop value'\n>>> Class.prop = 'baz'  \n>>> obj.prop  \n'foo'\nReading prop directly from Class retrieves the property object itself, without\nrunning its getter method.\nReading obj.prop executes the property getter.\nTrying to set an instance prop attribute fails.\nPutting 'prop' directly in the obj.__dict__ works.\n862 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 893,
      "chapter": null,
      "content": "We can see that obj now has two instance attributes: data and prop.\nHowever, reading obj.prop still runs the property getter. The property is not\nshadowed by an instance attribute.\nOverwriting Class.prop destroys the property object.\nNow obj.prop retrieves the instance attribute. Class.prop is not a property any‐\nmore, so it no longer overrides obj.prop.\nAs a final demonstration, we’ll add a new property to Class, and see it overriding an\ninstance attribute. Example 22-25 picks up where Example 22-24 left off.\nExample 22-25. New class property shadows the existing instance attribute (continued\nfrom Example 22-24)\n>>> obj.data  \n'bar'\n>>> Class.data  \n'the class data attr'\n>>> Class.data = property(lambda self: 'the \"data\" prop value')  \n>>> obj.data  \n'the \"data\" prop value'\n>>> del Class.data  \n>>> obj.data  \n'bar'\nobj.data retrieves the instance data attribute.\nClass.data retrieves the class data attribute.\nOverwrite Class.data with a new property.\nobj.data is now shadowed by the Class.data property.\nDelete the property.\nobj.data now reads the instance data attribute again.\nThe main point of this section is that an expression like obj.data does not start the\nsearch for data in obj. The search actually starts at obj.__class__, and only if there\nis no property named data in the class, Python looks in the obj instance itself. This\napplies to overriding descriptors in general, of which properties are just one example.\nFurther treatment of descriptors must wait for Chapter 23.\nA Proper Look at Properties \n| \n863",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 894,
      "chapter": null,
      "content": "Now back to properties. Every Python code unit—modules, functions, classes, meth‐\nods—can have a docstring. The next topic is how to attach documentation to\nproperties.\nProperty Documentation\nWhen tools such as the console help() function or IDEs need to display the docu‐\nmentation of a property, they extract the information from the __doc__ attribute of\nthe property.\nIf used with the classic call syntax, property can get the documentation string as the\ndoc argument:\n    weight = property(get_weight, set_weight, doc='weight in kilograms')\nThe docstring of the getter method—the one with the @property decorator itself—is\nused as the documentation of the property as a whole. Figure 22-1 shows the help\nscreens generated from the code in Example 22-26.\nFigure 22-1. Screenshots of the Python console when issuing the commands\nhelp(Foo.bar) and help(Foo). Source code is in Example 22-26.\nExample 22-26. Documentation for a property\nclass Foo:\n    @property\n    def bar(self):\n        \"\"\"The bar attribute\"\"\"\n        return self.__dict__['bar']\n    @bar.setter\n    def bar(self, value):\n        self.__dict__['bar'] = value\n864 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 895,
      "chapter": null,
      "content": "Now that we have these property essentials covered, let’s go back to the issue of pro‐\ntecting both the weight and price attributes of LineItem so they only accept values\ngreater than zero—but without implementing two nearly identical pairs of getters/\nsetters by hand.\nCoding a Property Factory\nWe’ll create a factory to create quantity properties—so named because the managed\nattributes represent quantities that can’t be negative or zero in the application.\nExample 22-27 shows the clean look of the LineItem class using two instances of\nquantity properties: one for managing the weight attribute, the other for price.\nExample 22-27. bulkfood_v2prop.py: the quantity property factory in use\nclass LineItem:\n    weight = quantity('weight')  \n    price = quantity('price')  \n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight  \n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price  \nUse the factory to define the first custom property, weight, as a class attribute.\nThis second call builds another custom property, price.\nHere the property is already active, making sure a negative or 0 weight is\nrejected.\nThe properties are also in use here, retrieving the values stored in the instance.\nRecall that properties are class attributes. When building each quantity property, we\nneed to pass the name of the LineItem attribute that will be managed by that specific\nproperty. Having to type the word weight twice in this line is unfortunate:\n    weight = quantity('weight')\nBut avoiding that repetition is complicated because the property has no way of know‐\ning which class attribute name will be bound to it. Remember: the righthand side of\nan assignment is evaluated first, so when quantity() is invoked, the weight class\nattribute doesn’t even exist.\nCoding a Property Factory \n| \n865",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 896,
      "chapter": null,
      "content": "11 This code is adapted from “Recipe 9.21. Avoiding Repetitive Property Methods” from Python Cookbook, 3rd\ned., by David Beazley and Brian K. Jones (O’Reilly).\nImproving the quantity property so that the user doesn’t need to\nretype the attribute name is a nontrivial metaprogramming prob‐\nlem. We’ll solve that problem in Chapter 23.\nExample 22-28 lists the implementation of the quantity property factory.11\nExample 22-28. bulkfood_v2prop.py: the quantity property factory\ndef quantity(storage_name):  \n    def qty_getter(instance):  \n        return instance.__dict__[storage_name]  \n    def qty_setter(instance, value):  \n        if value > 0:\n            instance.__dict__[storage_name] = value  \n        else:\n            raise ValueError('value must be > 0')\n    return property(qty_getter, qty_setter)  \nThe storage_name argument determines where the data for each property is\nstored; for the weight, the storage name will be 'weight'.\nThe first argument of the qty_getter could be named self, but that would be\nstrange because this is not a class body; instance refers to the LineItem instance\nwhere the attribute will be stored.\nqty_getter references storage_name, so it will be preserved in the closure of this\nfunction; the value is retrieved directly from the instance.__dict__ to bypass\nthe property and avoid an infinite recursion.\nqty_setter is defined, also taking instance as first argument.\nThe value is stored directly in the instance.__dict__, again bypassing the\nproperty.\nBuild a custom property object and return it.\n866 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 897,
      "chapter": null,
      "content": "The bits of Example 22-28 that deserve careful study revolve around the stor\nage_name variable. When you code each property in the traditional way, the name of\nthe attribute where you will store a value is hardcoded in the getter and setter meth‐\nods. But here, the qty_getter and qty_setter functions are generic, and they\ndepend on the storage_name variable to know where to get/set the managed attribute\nin the instance __dict__. Each time the quantity factory is called to build a property,\nthe storage_name must be set to a unique value.\nThe functions qty_getter and qty_setter will be wrapped by the property object\ncreated in the last line of the factory function. Later, when called to perform their\nduties, these functions will read the storage_name from their closures to determine\nwhere to retrieve/store the managed attribute values.\nIn Example 22-29, I create and inspect a LineItem instance, exposing the storage\nattributes.\nExample 22-29. bulkfood_v2prop.py: exploring properties and storage attributes\n    >>> nutmeg = LineItem('Moluccan nutmeg', 8, 13.95)\n    >>> nutmeg.weight, nutmeg.price  \n    (8, 13.95)\n    >>> nutmeg.__dict__  \n    {'description': 'Moluccan nutmeg', 'weight': 8, 'price': 13.95}\nReading the weight and price through the properties shadowing the namesake\ninstance attributes.\nUsing vars to inspect the nutmeg instance: here we see the actual instance\nattributes used to store the values.\nNote how the properties built by our factory leverage the behavior described in\n“Properties Override Instance Attributes” on page 861: the weight property overrides\nthe weight instance attribute so that every reference to self.weight or nut\nmeg.weight is handled by the property functions, and the only way to bypass the\nproperty logic is to access the instance __dict__ directly.\nThe code in Example 22-28 may be a bit tricky, but it’s concise: it’s identical in length\nto the decorated getter/setter pair defining just the weight property in\nExample 22-21. The LineItem definition in Example 22-27 looks much better\nwithout the noise of the getter/setters.\nIn a real system, that same kind of validation may appear in many fields, across sev‐\neral classes, and the quantity factory would be placed in a utility module to be used\nover and over again. Eventually that simple factory could be refactored into a more\nCoding a Property Factory \n| \n867",
      "content_length": 2376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 898,
      "chapter": null,
      "content": "12 The bloody scene is available on Youtube as I review this in October 2021.\nextensible descriptor class, with specialized subclasses performing different valida‐\ntions. We’ll do that in Chapter 23.\nNow let us wrap up the discussion of properties with the issue of attribute deletion.\nHandling Attribute Deletion\nWe can use the del statement to delete not only variables, but also attributes:\n>>> class Demo:\n...    pass\n...\n>>> d = Demo()\n>>> d.color = 'green'\n>>> d.color\n'green'\n>>> del d.color\n>>> d.color\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'Demo' object has no attribute 'color'\nIn practice, deleting attributes is not something we do every day in Python, and the\nrequirement to handle it with a property is even more unusual. But it is supported,\nand I can think of a silly example to demonstrate it.\nIn a property definition, the @my_property.deleter decorator wraps the method in\ncharge of deleting the attribute managed by the property. As promised, silly\nExample 22-30 is inspired by the scene with the Black Knight from Monty Python\nand the Holy Grail.12\nExample 22-30. blackknight.py\nclass BlackKnight:\n    def __init__(self):\n        self.phrases = [\n            ('an arm', \"'Tis but a scratch.\"),\n            ('another arm', \"It's just a flesh wound.\"),\n            ('a leg', \"I'm invincible!\"),\n            ('another leg', \"All right, we'll call it a draw.\")\n        ]\n    @property\n    def member(self):\n        print('next member is:')\n868 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 899,
      "chapter": null,
      "content": "return self.phrases[0][0]\n    @member.deleter\n    def member(self):\n        member, text = self.phrases.pop(0)\n        print(f'BLACK KNIGHT (loses {member}) -- {text}')\nThe doctests in blackknight.py are in Example 22-31.\nExample 22-31. blackknight.py: doctests for Example 22-30 (the Black Knight never\nconcedes defeat)\n    >>> knight = BlackKnight()\n    >>> knight.member\n    next member is:\n    'an arm'\n    >>> del knight.member\n    BLACK KNIGHT (loses an arm) -- 'Tis but a scratch.\n    >>> del knight.member\n    BLACK KNIGHT (loses another arm) -- It's just a flesh wound.\n    >>> del knight.member\n    BLACK KNIGHT (loses a leg) -- I'm invincible!\n    >>> del knight.member\n    BLACK KNIGHT (loses another leg) -- All right, we'll call it a draw.\nUsing the classic call syntax instead of decorators, the fdel argument configures the\ndeleter function. For example, the member property would be coded like this in the\nbody of the BlackKnight class:\n    member = property(member_getter, fdel=member_deleter)\nIf you are not using a property, attribute deletion can also be handled by implement‐\ning the lower-level __delattr__ special method, presented in “Special Methods for\nAttribute Handling” on page 871. Coding a silly class with __delattr__ is left as an\nexercise to the procrastinating reader.\nProperties are a powerful feature, but sometimes simpler or lower-level alternatives\nare preferable. In the final section of this chapter, we’ll review some of the core APIs\nthat Python offers for dynamic attribute programming.\nEssential Attributes and Functions for Attribute Handling\nThroughout this chapter, and even before in the book, we’ve used some of the built-\nin functions and special methods Python provides for dealing with dynamic\nattributes. This section gives an overview of them in one place, because their docu‐\nmentation is scattered in the official docs.\nEssential Attributes and Functions for Attribute Handling \n| \n869",
      "content_length": 1944,
      "extraction_method": "Direct"
    },
    {
      "page_number": 900,
      "chapter": null,
      "content": "13 Alex Martelli points out that, although __slots__ can be coded as a list, it’s better to be explicit and always\nuse a tuple, because changing the list in the __slots__ after the class body is processed has no effect, so it\nwould be misleading to use a mutable sequence there.\nSpecial Attributes that Affect Attribute Handling\nThe behavior of many of the functions and special methods listed in the following\nsections depend on three special attributes:\n__class__\nA reference to the object’s class (i.e., obj.__class__ is the same as type(obj)).\nPython looks for special methods such as __getattr__ only in an object’s class,\nand not in the instances themselves.\n__dict__\nA mapping that stores the writable attributes of an object or class. An object that\nhas a __dict__ can have arbitrary new attributes set at any time. If a class has a\n__slots__ attribute, then its instances may not have a __dict__. See __slots__\n(next).\n__slots__\nAn attribute that may be defined in a class to save memory. __slots__ is a tuple\nof strings naming the allowed attributes.13 If the '__dict__' name is not in\n__slots__, then the instances of that class will not have a __dict__ of their own,\nand only the attributes listed in __slots__ will be allowed in those instances.\nRecall “Saving Memory with __slots__” on page 384 for more.\nBuilt-In Functions for Attribute Handling\nThese five built-in functions perform object attribute reading, writing, and\nintrospection:\ndir([object])\nLists most attributes of the object. The official docs say dir is intended for inter‐\nactive use so it does not provide a comprehensive list of attributes, but an “inter‐\nesting” set of names. dir can inspect objects implemented with or without a\n__dict__. The __dict__ attribute itself is not listed by dir, but the __dict__\nkeys are listed. Several special attributes of classes, such as __mro__, __bases__,\nand __name__, are not listed by dir either. You can customize the output of dir\nby implementing the __dir__ special method, as we saw in Example 22-4. If the\noptional object argument is not given, dir lists the names in the current scope.\n870 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 901,
      "chapter": null,
      "content": "getattr(object, name[, default])\nGets the attribute identified by the name string from the object. The main use\ncase is to retrieve attributes (or methods) whose names we don’t know before‐\nhand. This may fetch an attribute from the object’s class or from a superclass. If\nno such attribute exists, getattr raises AttributeError or returns the default\nvalue, if given. One great example of using gettatr is in the Cmd.onecmd method\nin the cmd package of the standard library, where it is used to get and execute a\nuser-defined command.\nhasattr(object, name)\nReturns True if the named attribute exists in the object, or can be somehow\nfetched through it (by inheritance, for example). The documentation explains:\n“This is implemented by calling getattr(object, name) and seeing whether it raises\nan AttributeError or not.”\nsetattr(object, name, value)\nAssigns the value to the named attribute of object, if the object allows it. This\nmay create a new attribute or overwrite an existing one.\nvars([object])\nReturns the __dict__ of object; vars can’t deal with instances of classes that\ndefine __slots__ and don’t have a __dict__ (contrast with dir, which handles\nsuch instances). Without an argument, vars() does the same as locals():\nreturns a dict representing the local scope.\nSpecial Methods for Attribute Handling\nWhen implemented in a user-defined class, the special methods listed here handle\nattribute retrieval, setting, deletion, and listing.\nAttribute access using either dot notation or the built-in functions getattr, hasattr,\nand setattr triggers the appropriate special methods listed here. Reading and writ‐\ning attributes directly in the instance __dict__ does not trigger these special methods\n—and that’s the usual way to bypass them if needed.\nSection “3.3.11. Special method lookup” of the “Data model” chapter warns:\nFor custom classes, implicit invocations of special methods are only guaranteed to\nwork correctly if defined on an object’s type, not in the object’s instance dictionary.\nIn other words, assume that the special methods will be retrieved on the class itself,\neven when the target of the action is an instance. For this reason, special methods are\nnot shadowed by instance attributes with the same name.\nIn the following examples, assume there is a class named Class, obj is an instance of\nClass, and attr is an attribute of obj.\nEssential Attributes and Functions for Attribute Handling \n| \n871",
      "content_length": 2430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 902,
      "chapter": null,
      "content": "For every one of these special methods, it doesn’t matter if the attribute access is done\nusing dot notation or one of the built-in functions listed in “Built-In Functions for\nAttribute Handling” on page 870. For example, both obj.attr and getattr(obj,\n'attr', 42) trigger Class.__getattribute__(obj, 'attr').\n__delattr__(self, name)\nAlways called when there is an attempt to delete an attribute using the del state‐\nment; e.g., del obj.attr triggers Class.__delattr__(obj, 'attr'). If attr is a\nproperty, its deleter method is never called if the class implements __delattr__.\n__dir__(self)\nCalled when dir is invoked on the object, to provide a listing of attributes; e.g.,\ndir(obj) triggers Class.__dir__(obj). Also used by tab-completion in all\nmodern Python consoles.\n__getattr__(self, name)\nCalled only when an attempt to retrieve the named attribute fails, after the obj,\nClass, and its superclasses are searched. The expressions obj.no_such_attr, get\nattr(obj, 'no_such_attr'), and hasattr(obj, 'no_such_attr') may trigger\nClass.__getattr__(obj, 'no_such_attr'), but only if an attribute by that\nname cannot be found in obj or in Class and its superclasses.\n__getattribute__(self, name)\nAlways called when there is an attempt to retrieve the named attribute directly\nfrom Python code (the interpreter may bypass this in some cases, for example, to\nget the __repr__ method). Dot notation and the getattr and hasattr built-ins\ntrigger this method. __getattr__ is only invoked after __getattribute__, and\nonly when __getattribute__ raises AttributeError. To retrieve attributes of\nthe instance obj without triggering an infinite recursion, implementations of\n__getattribute__ should use super().__getattribute__(obj, name).\n__setattr__(self, name, value)\nAlways called when there is an attempt to set the named attribute. Dot notation\nand the setattr built-in trigger this method; e.g., both obj.attr = 42 and\nsetattr(obj, 'attr', 42) trigger Class.__setattr__(obj, 'attr', 42).\nIn practice, because they are unconditionally called and affect prac‐\ntically every attribute access, the \n__getattribute__ and\n__setattr__ special methods are harder to use correctly than\n__getattr__, which only handles nonexisting attribute names.\nUsing properties or descriptors is less error prone than defining\nthese special methods.\n872 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 903,
      "chapter": null,
      "content": "This concludes our dive into properties, special methods, and other techniques for\ncoding dynamic attributes.\nChapter Summary\nWe started our coverage of dynamic attributes by showing practical examples of sim‐\nple classes to make it easier to deal with a JSON dataset. The first example was the\nFrozenJSON class that converted nested dicts and lists into nested FrozenJSON instan‐\nces and lists of them. The FrozenJSON code demonstrated the use of the __getattr__\nspecial method to convert data structures on the fly, whenever their attributes were\nread. The last version of FrozenJSON showcased the use of the __new__ constructor\nmethod to transform a class into a flexible factory of objects, not limited to instances\nof itself.\nWe then converted the JSON dataset to a dict storing instances of a Record class.\nThe first rendition of Record was a few lines long and introduced the “bunch” idiom:\nusing self.__dict__.update(**kwargs) to build arbitrary attributes from keyword\narguments passed to __init__. The second iteration added the Event class, imple‐\nmenting automatic retrieval of linked records through properties. Computed prop‐\nerty values sometimes require caching, and we covered a few ways of doing that.\nAfter realizing that @functools.cached_property is not always applicable, we\nlearned about an alternative: combining @property on top of @functools.cache, in\nthat order.\nCoverage of properties continued with the LineItem class, where a property was\ndeployed to protect a weight attribute from negative or zero values that make no\nbusiness sense. After a deeper look at property syntax and semantics, we created a\nproperty factory to enforce the same validation on weight and price, without coding\nmultiple getters and setters. The property factory leveraged subtle concepts—such as\nclosures, and instance attribute overriding by properties—to provide an elegant\ngeneric solution using the same number of lines as a single hand-coded property\ndefinition.\nFinally, we had a brief look at handling attribute deletion with properties, followed by\nan overview of the key special attributes, built-in functions, and special methods that\nsupport attribute metaprogramming in the core Python language.\nFurther Reading\nThe official documentation for the attribute handling and introspection built-in\nfunctions is Chapter 2, “Built-in Functions” of The Python Standard Library. The\nrelated special methods and the __slots__ special attribute are documented in The\nPython Language Reference in “3.3.2. Customizing attribute access”. The semantics of\nChapter Summary \n| \n873",
      "content_length": 2582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 904,
      "chapter": null,
      "content": "how special methods are invoked bypassing instances is explained in “3.3.9. Special\nmethod lookup”. In Chapter 4, “Built-in Types,” of The Python Standard Library,\n“4.13. Special Attributes” covers __class__ and __dict__ attributes.\nPython Cookbook, 3rd ed., by David Beazley and Brian K. Jones (O’Reilly) has several\nrecipes covering the topics of this chapter, but I will highlight three that are outstand‐\ning: “Recipe 8.8. Extending a Property in a Subclass” addresses the thorny issue of\noverriding the methods inside a property inherited from a superclass; “Recipe 8.15.\nDelegating Attribute Access” implements a proxy class showcasing most special\nmethods from “Special Methods for Attribute Handling” on page 871 in this book;\nand the awesome “Recipe 9.21. Avoiding Repetitive Property Methods,” which was\nthe basis for the property factory function presented in Example 22-28.\nPython in a Nutshell, 3rd ed., by Alex Martelli, Anna Ravenscroft, and Steve Holden\n(O’Reilly) is rigorous and objective. They devote only three pages to properties, but\nthat’s because the book follows an axiomatic presentation style: the preceding 15\npages or so provide a thorough description of the semantics of Python classes from\nthe ground up, including descriptors, which are how properties are actually imple‐\nmented under the hood. So by the time Martelli et al., get to properties, they pack a\nlot of insights in those three pages—including what I selected to open this chapter.\nBertrand Meyer—quoted in the Uniform Access Principle definition in this chapter\nopening—pioneered the Design by Contract methodology, designed the Eiffel lan‐\nguage, and wrote the excellent Object-Oriented Software Construction, 2nd ed. (Pear‐\nson). The first six chapters provide one of the best conceptual introductions to OO\nanalysis and design I’ve seen. Chapter 11 presents Design by Contract, and Chapter\n35 offers Meyer’s assessments of some influential object-oriented languages: Simula,\nSmalltalk, CLOS (the Common Lisp Object System), Objective-C, C++, and Java,\nwith brief comments on some others. Only in the last page of the book does he reveal\nthat the highly readable “notation” he uses as pseudocode is Eiffel.\n874 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 905,
      "chapter": null,
      "content": "Soapbox\nMeyer’s Uniform Access Principle is aesthetically appealing. As a programmer using\nan API, I shouldn’t have to care whether product.price simply fetches a data\nattribute or performs a computation. As a consumer and a citizen, I do care: in e-\ncommerce today the value of product.price often depends on who is asking, so it’s\ncertainly not a mere data attribute. In fact, it’s common practice that the price is\nlower if the query comes from outside the store—say, from a price-comparison\nengine. This effectively punishes loyal customers who like to browse within a particu‐\nlar store. But I digress.\nThe previous digression does raise a relevant point for programming: although the\nUniform Access Principle makes perfect sense in an ideal world, in reality, users of an\nAPI may need to know whether reading product.price is potentially too expensive\nor time-consuming. That’s a problem with programming abstractions in general: they\nmake it hard to reason about the runtime cost of evaluating an expression. On the\nother hand, abstractions let users accomplish more with less code. It’s a trade-off. As\nusual in matters of software engineering, Ward Cunningham’s original wiki hosts\ninsightful arguments about the merits of the Uniform Access Principle.\nIn object-oriented programming languages, application or violations of the Uniform\nAccess Principle often revolve around the syntax of reading public data attributes ver‐\nsus invoking getter/setter methods.\nSmalltalk and Ruby address this issue in a simple and elegant way: they don’t support\npublic data attributes at all. Every instance attribute in these languages is private, so\nevery access to them must be through methods. But their syntax makes this painless:\nin Ruby, product.price invokes the price getter; in Smalltalk, it’s simply product\nprice.\nAt the other end of the spectrum, the Java language allows the programmer to choose\namong four access-level modifiers—including the no-name default that the Java\nTutorial calls “package-private.”\nThe general practice does not agree with the syntax established by the Java designers,\nthough. Everybody in Java-land agrees that attributes should be private, and you\nmust spell it out every time, because it’s not the default. When all attributes are pri‐\nvate, all access to them from outside the class must go through accessors. Java IDEs\ninclude shortcuts for generating accessor methods automatically. Unfortunately, the\nIDE is not so helpful when you must read the code six months later. It’s up to you to\nwade through a sea of do-nothing accessors to find those that add value by imple‐\nmenting some business logic.\nFurther Reading \n| \n875",
      "content_length": 2661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 906,
      "chapter": null,
      "content": "14 Alex Martelli, Python in a Nutshell, 2nd ed. (O’Reilly), p. 101.\nAlex Martelli speaks for the majority of the Python community when he calls acces‐\nsors “goofy idioms” and then provides these examples that look very different but do\nthe same thing:14\nsomeInstance.widgetCounter += 1\n# rather than...\nsomeInstance.setWidgetCounter(someInstance.getWidgetCounter() + 1)\nSometimes when designing APIs, I’ve wondered whether every method that does not\ntake an argument (besides self), returns a value (other than None), and is a pure\nfunction (i.e., has no side effects) should be replaced by a read-only property. In this\nchapter, the LineItem.subtotal method (as in Example 22-27) would be a good can‐\ndidate to become a read-only property. Of course, this excludes methods that are\ndesigned to change the object, such as my_list.clear(). It would be a terrible idea to\nturn that into a property, so that merely accessing my_list.clear would delete the\ncontents of the list!\nIn the Pingo GPIO library, which I coauthored (mentioned in “The __missing__\nMethod” on page 91), much of the user-level API is based on properties. For example,\nto read the current value of an analog pin, the user writes pin.value, and setting a\ndigital pin mode is written as pin.mode = OUT. Behind the scenes, reading an analog\npin value or setting a digital pin mode may involve a lot of code, depending on the\nspecific board driver. We decided to use properties in Pingo because we want the API\nto be comfortable to use even in interactive environments like a Jupyter Notebook,\nand we feel pin.mode = OUT is easier on the eyes and on the fingers than\npin.set_mode(OUT).\nAlthough I find the Smalltalk and Ruby solution cleaner, I think the Python approach\nmakes more sense than the Java one. We are allowed to start simple, coding data\nmembers as public attributes, because we know they can always be wrapped by prop‐\nerties (or descriptors, which we’ll talk about in the next chapter).\n__new__ Is Better than new\nAnother example of the Uniform Access Principle (or a variation of it) is the fact that\nfunction calls and object instantiation use the same syntax in Python: my_obj =\nfoo(), where foo may be a class or any other callable.\nOther languages influenced by C++ syntax have a new operator that makes instantia‐\ntion look different than a call. Most of the time, the user of an API doesn’t care\nwhether foo is a function or a class. For years I was under the impression that\nproperty was a function. In normal usage, it makes no difference.\n876 \n| \nChapter 22: Dynamic Attributes and Properties",
      "content_length": 2581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 907,
      "chapter": null,
      "content": "15 The reasons I am about to mention are given in the Dr. Dobbs Journal article titled “Java’s new Considered\nHarmful”, by Jonathan Amsterdam and in “Consider static factory methods instead of constructors,” which is\nItem 1 of the award-winning book Effective Java, 3rd ed., by Joshua Bloch (Addison-Wesley).\nThere are many good reasons for replacing constructors with factories.15 A popular\nmotive is limiting the number of instances by returning previously built ones (as in\nthe Singleton pattern). A related use is caching expensive object construction. Also,\nsometimes it’s convenient to return objects of different types, depending on the argu‐\nments given.\nCoding a constructor is simpler; providing a factory adds flexibility at the expense of\nmore code. In languages that have a new operator, the designer of an API must decide\nin advance whether to stick with a simple constructor or invest in a factory. If the ini‐\ntial choice is wrong, the correction may be costly—all because new is an operator.\nSometimes it may also be convenient to go the other way, and replace a simple func‐\ntion with a class.\nIn Python, classes and functions are interchangeable in many situations. Not only\nbecause there’s no new operator, but also because there is the __new__ special method,\nwhich can turn a class into a factory producing objects of different kinds (as we saw\nin “Flexible Object Creation with __new__” on page 843) or returning prebuilt\ninstances instead of creating a new one every time.\nThis function-class duality would be easier to leverage if PEP 8 — Style Guide for\nPython Code did not recommend CamelCase for class names. On the other hand,\ndozens of classes in the standard library have lowercase names (e.g., property, str,\ndefaultdict, etc.). So maybe the use of lowercase class names is a feature, and not a\nbug. But however we look at it, the inconsistent capitalization of classes in the Python\nstandard library poses a usability problem.\nAlthough calling a function is not different from calling a class, it’s good to know\nwhich is which because of another thing we can do with a class: subclassing. So I per‐\nsonally use CamelCase in every class that I code, and I wish all classes in the Python\nstandard library used the same convention. I am looking at you, collections.Order\nedDict and collections.defaultdict.\nFurther Reading \n| \n877",
      "content_length": 2360,
      "extraction_method": "Direct"
    },
    {
      "page_number": 908,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 909,
      "chapter": null,
      "content": "1 Raymond Hettinger, Descriptor HowTo Guide.\nCHAPTER 23\nAttribute Descriptors\nLearning about descriptors not only provides access to a larger toolset, it creates a\ndeeper understanding of how Python works and an appreciation for the elegance of its\ndesign.\n— Raymond Hettinger, Python core developer and guru1\nDescriptors are a way of reusing the same access logic in multiple attributes. For\nexample, field types in ORMs, such as the Django ORM and SQLAlchemy, are\ndescriptors, managing the flow of data from the fields in a database record to Python\nobject attributes and vice versa.\nA descriptor is a class that implements a dynamic protocol consisting of the __get__,\n__set__, and __delete__ methods. The property class implements the full descrip‐\ntor protocol. As usual with dynamic protocols, partial implementations are OK. In\nfact, most descriptors we see in real code implement only __get__ and __set__, and\nmany implement only one of these methods.\nDescriptors are a distinguishing feature of Python, deployed not only at the applica‐\ntion level but also in the language infrastructure. User-defined functions are descrip‐\ntors. We’ll see how the descriptor protocol allows methods to operate as bound or\nunbound methods, depending on how they are called.\nUnderstanding descriptors is key to Python mastery. This is what this chapter is\nabout.\nIn this chapter we’ll refactor the bulk food example we first saw in “Using a Property\nfor Attribute Validation” on page 857, replacing properties with descriptors. This will\nmake it easier to reuse the attribute validation logic across different classes. We’ll\n879",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 910,
      "chapter": null,
      "content": "tackle the concepts of overriding and nonoverriding descriptors, and realize that\nPython functions are descriptors. Finally we’ll see some tips about implementing\ndescriptors.\nWhat’s New in This Chapter\nThe Quantity descriptor example in “LineItem Take #4: Automatic Naming of Stor‐\nage Attributes” on page 887 was dramatically simplified thanks to the __set_name__\nspecial method added to the descriptor protocol in Python 3.6.\nI removed the property factory example formerly in “LineItem Take #4: Automatic\nNaming of Storage Attributes” on page 887 because it became irrelevant: the point was\nto show an alternative way of solving the Quantity problem, but with the addition of\n__set_name__, the descriptor solution becomes much simpler.\nThe AutoStorage class that used to appear in “LineItem Take #5: A New Descriptor\nType” on page 889 is also gone because __set_name__ made it obsolete.\nDescriptor Example: Attribute Validation\nAs we saw in “Coding a Property Factory” on page 865, a property factory is a way to\navoid repetitive coding of getters and setters by applying functional programming\npatterns. A property factory is a higher-order function that creates a parameterized\nset of accessor functions and builds a custom property instance from them, with clo‐\nsures to hold settings like the storage_name. The object-oriented way of solving the\nsame problem is a descriptor class.\nWe’ll continue the series of LineItem examples where we left off, in “Coding a Prop‐\nerty Factory” on page 865, by refactoring the quantity property factory into a Quan\ntity descriptor class. This will make it easier to use.\nLineItem Take #3: A Simple Descriptor\nAs we said in the introduction, a class implementing a __get__, a __set__, or a\n__delete__ method is a descriptor. You use a descriptor by declaring instances of it\nas class attributes of another class.\nWe’ll create a Quantity descriptor, and the LineItem class will use two instances of\nQuantity: one for managing the weight attribute, the other for price. A diagram\nhelps, so take a look at Figure 23-1.\n880 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 911,
      "chapter": null,
      "content": "Figure 23-1. UML class diagram for LineItem using a descriptor class named Quan\ntity. Underlined attributes in UML are class attributes. Note that weight and price are\ninstances of Quantity attached to the LineItem class, but LineItem instances also have\ntheir own weight and price attributes where those values are stored.\nNote that the word weight appears twice in Figure 23-1, because there are really two\ndistinct attributes named weight: one is a class attribute of LineItem, the other is an\ninstance attribute that will exist in each LineItem object. This also applies to price.\nTerms to understand descriptors\nImplementing and using descriptors involves several components, and it is useful to\nbe precise when naming those components. I will use the following terms and defini‐\ntions as I describe the examples in this chapter. They will be easier to understand\nonce you see the code, but I wanted to put the definitions up front so you can refer\nback to them when needed.\nDescriptor class\nA class implementing the descriptor protocol. That’s Quantity in Figure 23-1.\nManaged class\nThe class where the descriptor instances are declared as class attributes. In\nFigure 23-1, LineItem is the managed class.\nDescriptor instance\nEach instance of a descriptor class, declared as a class attribute of the managed\nclass. In Figure 23-1, each descriptor instance is represented by a composition\narrow with an underlined name (the underline means class attribute in UML).\nThe black diamonds touch the LineItem class, which contains the descriptor\ninstances.\nDescriptor Example: Attribute Validation \n| \n881",
      "content_length": 1603,
      "extraction_method": "Direct"
    },
    {
      "page_number": 912,
      "chapter": null,
      "content": "Managed instance\nOne instance of the managed class. In this example, LineItem instances are the\nmanaged instances (they are not shown in the class diagram).\nStorage attribute\nAn attribute of the managed instance that holds the value of a managed attribute\nfor that particular instance. In Figure 23-1, the LineItem instance attributes\nweight and price are the storage attributes. They are distinct from the descriptor\ninstances, which are always class attributes.\nManaged attribute\nA public attribute in the managed class that is handled by a descriptor instance,\nwith values stored in storage attributes. In other words, a descriptor instance and\na storage attribute provide the infrastructure for a managed attribute.\nIt’s important to realize that Quantity instances are class attributes of LineItem. This\ncrucial point is highlighted by the mills and gizmos in Figure 23-2.\nFigure 23-2. UML class diagram annotated with MGN (Mills & Gizmos Notation):\nclasses are mills that produce gizmos—the instances. The Quantity mill produces two\ngizmos with round heads, which are attached to the LineItem mill: weight and price.\nThe LineItem mill produces rectangular gizmos that have their own weight and price\nattributes where those values are stored.\n882 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 913,
      "chapter": null,
      "content": "2 Classes and instances are drawn as rectangles in UML class diagrams. There are visual differences, but instan‐\nces are rarely shown in class diagrams, so developers may not recognize them as such.\nIntroducing Mills & Gizmos Notation\nAfter explaining descriptors many times, I realized UML is not very good at showing\nrelationships involving classes and instances, like the relationship between a managed\nclass and the descriptor instances.2 So I invented my own “language,” the Mills & Giz‐\nmos Notation (MGN), which I use to annotate UML diagrams.\nMGN is designed to make very clear the distinction between classes and instances.\nSee Figure 23-3. In MGN, a class is drawn as a “mill,” a complicated machine that\nproduces gizmos. Classes/mills are always machines with levers and dials. The gizmos\nare the instances, and they look much simpler. When this book is rendered in color,\ngizmos have the same color as the mill that made it.\nFigure 23-3. MGN sketch showing the LineItem class making three instances, and Quan\ntity making two. One instance of Quantity is retrieving a value stored in a LineItem\ninstance.\nFor this example, I drew LineItem instances as rows in a tabular invoice, with three\ncells representing the three attributes (description, weight, and price). Because\nQuantity instances are descriptors, they have a magnifying glass to __get__ values,\nand a claw to __set__ values. When we get to metaclasses, you’ll thank me for these\ndoodles.\nEnough doodling for now. Here is the code: Example 23-1 shows the Quantity\ndescriptor class, and Example 23-2 lists a new LineItem class using two instances of\nQuantity.\nDescriptor Example: Attribute Validation \n| \n883",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 914,
      "chapter": null,
      "content": "Example 23-1. bulkfood_v3.py: Quantity descriptor does not accept negative values\nclass Quantity:  \n    def __init__(self, storage_name):\n        self.storage_name = storage_name  \n    def __set__(self, instance, value):  \n        if value > 0:\n            instance.__dict__[self.storage_name] = value  \n        else:\n            msg = f'{self.storage_name} must be > 0'\n            raise ValueError(msg)\n    def __get__(self, instance, owner):  \n        return instance.__dict__[self.storage_name]\nDescriptor is a protocol-based feature; no subclassing is needed to implement\none.\nEach Quantity instance will have a storage_name attribute: that’s the name of\nthe storage attribute to hold the value in the managed instances.\n__set__ is called when there is an attempt to assign to the managed attribute.\nHere, self is the descriptor instance (i.e., LineItem.weight or LineItem.price),\ninstance is the managed instance (a LineItem instance), and value is the value\nbeing assigned.\nWe must store the attribute value directly into __dict__; calling set attr\n(instance, self.storage_name) would trigger the __set__ method again,\nleading to infinite recursion.\nWe need to implement __get__ because the name of the managed attribute may\nnot be the same as the storage_name. The owner argument will be explained\nshortly.\nImplementing __get__ is necessary because a user could write something like this:\nclass House:\n    rooms = Quantity('number_of_rooms')\nIn the House class, the managed attribute is rooms, but the storage attribute is\nnumber_of_rooms. Given a House instance named chaos_manor, reading and writing\nchaos_manor.rooms goes through the Quantity descriptor instance attached to\nrooms, but reading and writing chaos_manor.number_of_rooms bypasses the\ndescriptor.\n884 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 915,
      "chapter": null,
      "content": "Note that __get__ receives three arguments: self, instance, and owner. The owner\nargument is a reference to the managed class (e.g., LineItem), and it’s useful if you\nwant the descriptor to support retrieving a class attribute—perhaps to emulate\nPython’s default behavior of retrieving a class attribute when the name is not found\nin the instance.\nIf a managed attribute, such as weight, is retrieved via the class like Line\nItem.weight, the descriptor __get__ method receives None as the value for the\ninstance argument.\nTo support introspection and other metaprogramming tricks by the user, it’s a good\npractice to make __get__ return the descriptor instance when the managed attribute\nis accessed through the class. To do that, we’d code __get__ like this:\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        else:\n            return instance.__dict__[self.storage_name]\nExample 23-2 demonstrates the use of Quantity in LineItem.\nExample 23-2. bulkfood_v3.py: Quantity descriptors manage attributes in LineItem\nclass LineItem:\n    weight = Quantity('weight')  \n    price = Quantity('price')  \n    def __init__(self, description, weight, price):  \n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\nThe first descriptor instance will manage the weight attribute.\nThe second descriptor instance will manage the price attribute.\nThe rest of the class body is as simple and clean as the original code in bulk‐\nfood_v1.py (Example 22-19).\nDescriptor Example: Attribute Validation \n| \n885",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 916,
      "chapter": null,
      "content": "3 White truffles cost thousands of dollars per pound. Disallowing the sale of truffles for $0.01 is left as an exer‐\ncise for the enterprising reader. I know a person who actually bought an $1,800 encyclopedia of statistics for\n$18 because of an error in an online store (not Amazon.com in this case).\nThe code in Example 23-2 works as intended, preventing the sale of truffles for $0:3\n>>> truffle = LineItem('White truffle', 100, 0)\nTraceback (most recent call last):\n    ...\nValueError: value must be > 0\nWhen coding descriptor __get__ and __set__ methods, keep in\nmind what the self and instance arguments mean: self is the\ndescriptor instance, and instance is the managed instance.\nDescriptors managing instance attributes should store values in the\nmanaged instances. That’s why Python provides the instance\nargument to the descriptor methods.\nIt may be tempting, but wrong, to store the value of each managed attribute in the\ndescriptor instance itself. In other words, in the __set__ method, instead of coding:\n    instance.__dict__[self.storage_name] = value\nthe tempting, but bad, alternative would be:\n    self.__dict__[self.storage_name] = value\nTo understand why this would be wrong, think about the meaning of the first two\narguments to __set__: self and instance. Here, self is the descriptor instance,\nwhich is actually a class attribute of the managed class. You may have thousands of\nLineItem instances in memory at one time, but you’ll only have two instances of the\ndescriptors: the class attributes LineItem.weight and LineItem.price. So anything\nyou store in the descriptor instances themselves is actually part of a LineItem class\nattribute, and therefore is shared among all LineItem instances.\nA drawback of Example 23-2 is the need to repeat the names of the attributes when\nthe descriptors are instantiated in the managed class body. It would be nice if the\nLineItem class could be declared like this:\nclass LineItem:\n    weight = Quantity()\n    price = Quantity()\n    # remaining methods as before\nAs it stands, Example 23-2 requires naming each Quantity explicitly, which is not\nonly inconvenient but dangerous. If a programmer copying and pasting code forgets\nto edit both names and writes something like price = Quantity('weight'), the\n886 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 917,
      "chapter": null,
      "content": "4 More precisely, __set_name__ is called by type.__new__—the constructor of objects representing classes. The\ntype built-in is actually a metaclass, the default class of user-defined classes. This is hard to grasp at first, but\nrest assured: Chapter 24 is devoted to the dynamic configuration of classes, including the concept of\nmetaclasses.\nprogram will misbehave badly, clobbering the value of weight whenever the price is\nset.\nThe problem is that—as we saw in Chapter 6—the righthand side of an assignment is\nexecuted before the variable exists. The expression Quantity() is evaluated to create\na descriptor instance, and there is no way the code in the Quantity class can guess the\nname of the variable to which the descriptor will be bound (e.g., weight or price).\nThankfully, the descriptor protocol now supports the aptly named __set_name__\nspecial method. We’ll see how to use it next.\nAutomatic naming of a descriptor storage attribute used to be a\nthorny issue. In the first edition of Fluent Python, I devoted several\npages and lines of code in this chapter and the next to presenting\ndifferent solutions, including the use of a class decorator, and then\nmetaclasses in Chapter 24. This was greatly simplified in Python\n3.6.\nLineItem Take #4: Automatic Naming of Storage Attributes\nTo avoid retyping the attribute name in the descriptor instances, we’ll implement\n__set_name__ to set the storage_name of each Quantity instance. The __set_name__\nspecial method was added to the descriptor protocol in Python 3.6. The interpreter\ncalls __set_name__ on each descriptor it finds in a class body—if the descriptor\nimplements it.4\nIn Example 23-3, the LineItem descriptor class doesn’t need an __init__. Instead,\n__set_item__ saves the name of the storage attribute.\nExample 23-3. bulkfood_v4.py: __set_name__ sets the name for each Quantity\ndescriptor instance\nclass Quantity:\n    def __set_name__(self, owner, name):  \n        self.storage_name = name          \n    def __set__(self, instance, value):   \n        if value > 0:\n            instance.__dict__[self.storage_name] = value\nDescriptor Example: Attribute Validation \n| \n887",
      "content_length": 2140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 918,
      "chapter": null,
      "content": "else:\n            msg = f'{self.storage_name} must be > 0'\n            raise ValueError(msg)\n    # no __get__ needed  \nclass LineItem:\n    weight = Quantity()  \n    price = Quantity()\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\nself is the descriptor instance (not the managed instance), owner is the managed\nclass, and name is the name of the attribute of owner to which this descriptor\ninstance was assigned in the class body of owner.\nThis is what the __init__ did in Example 23-1.\nThe __set__ method here is exactly the same as in Example 23-1.\nImplementing __get__ is not necessary because the name of the storage attribute\nmatches the name of the managed attribute. The expression product.price gets\nthe price attribute directly from the LineItem instance.\nNow we don’t need to pass the managed attribute name to the Quantity con‐\nstructor. That was the goal for this version.\nLooking at Example 23-3, you may think that’s a lot of code just for managing a cou‐\nple of attributes, but it’s important to realize that the descriptor logic is now abstrac‐\nted into a separate code unit: the Quantity class. Usually we do not define a\ndescriptor in the same module where it’s used, but in a separate utility module\ndesigned to be used across the application—even in many applications, if you are\ndeveloping a library or framework.\nWith this in mind, Example 23-4 better represents the typical usage of a descriptor.\nExample 23-4. bulkfood_v4c.py: LineItem definition uncluttered; the Quantity\ndescriptor class now resides in the imported model_v4c module\nimport model_v4c as model  \n888 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 919,
      "chapter": null,
      "content": "5 Gamma et al., Design Patterns: Elements of Reusable Object-Oriented Software, p. 326.\nclass LineItem:\n    weight = model.Quantity()  \n    price = model.Quantity()\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\nImport the model_v4c module where Quantity is implemented.\nPut model.Quantity to use.\nDjango users will notice that Example 23-4 looks a lot like a model definition. It’s no\ncoincidence: Django model fields are descriptors.\nBecause descriptors are implemented as classes, we can leverage inheritance to reuse\nsome of the code we have for new descriptors. That’s what we’ll do in the following\nsection.\nLineItem Take #5: A New Descriptor Type\nThe imaginary organic food store hits a snag: somehow a line item instance was cre‐\nated with a blank description, and the order could not be fulfilled. To prevent that,\nwe’ll create a new descriptor, NonBlank. As we design NonBlank, we realize it will be\nvery much like the Quantity descriptor, except for the validation logic.\nThis prompts a refactoring, producing Validated, an abstract class that overrides the\n__set__ method, calling a validate method that must be implemented by sub‐\nclasses.\nWe’ll then rewrite Quantity, and implement NonBlank by inheriting from Validated\nand just coding the validate methods.\nThe relationship among Validated, Quantity, and NonBlank is an application of the\ntemplate method as described in the Design Patterns classic:\nA template method defines an algorithm in terms of abstract operations that subclasses\noverride to provide concrete behavior.5\nDescriptor Example: Attribute Validation \n| \n889",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 920,
      "chapter": null,
      "content": "6 Slide #50 of Alex Martelli’s “Python Design Patterns” talk. Highly recommended.\nIn Example 23-5, Validated.__set__ is the template method and self.validate is\nthe abstract operation.\nExample 23-5. model_v5.py: the Validated ABC\nimport abc\nclass Validated(abc.ABC):\n    def __set_name__(self, owner, name):\n        self.storage_name = name\n    def __set__(self, instance, value):\n        value = self.validate(self.storage_name, value)  \n        instance.__dict__[self.storage_name] = value  \n    @abc.abstractmethod\n    def validate(self, name, value):  \n        \"\"\"return validated value or raise ValueError\"\"\"\n__set__ delegates validation to the validate method…\n…then uses the returned value to update the stored value.\nvalidate is an abstract method; this is the template method.\nAlex Martelli prefers to call this design pattern Self-Delegation, and I agree it’s a more\ndescriptive name: the first line of __set__ self-delegates to validate.6\nThe concrete Validated subclasses in this example are Quantity and NonBlank,\nshown in Example 23-6.\nExample 23-6. model_v5.py: Quantity and NonBlank, concrete Validated subclasses\nclass Quantity(Validated):\n    \"\"\"a number greater than zero\"\"\"\n    def validate(self, name, value):  \n        if value <= 0:\n            raise ValueError(f'{name} must be > 0')\n        return value\nclass NonBlank(Validated):\n    \"\"\"a string with at least one non-space character\"\"\"\n890 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 921,
      "chapter": null,
      "content": "def validate(self, name, value):\n        value = value.strip()\n        if not value:  \n            raise ValueError(f'{name} cannot be blank')\n        return value  \nImplementation of the template method required by the Validated.validate\nabstract method.\nIf nothing is left after leading and trailing blanks are stripped, reject the value.\nRequiring the concrete validate methods to return the validated value gives\nthem an opportunity to clean up, convert, or normalize the data received. In this\ncase, value is returned without leading or trailing blanks.\nUsers of model_v5.py don’t need to know all these details. What matters is that they\nget to use Quantity and NonBlank to automate the validation of instance attributes.\nSee the latest LineItem class in Example 23-7.\nExample 23-7. bulkfood_v5.py: LineItem using Quantity and NonBlank descriptors\nimport model_v5 as model  \nclass LineItem:\n    description = model.NonBlank()  \n    weight = model.Quantity()\n    price = model.Quantity()\n    def __init__(self, description, weight, price):\n        self.description = description\n        self.weight = weight\n        self.price = price\n    def subtotal(self):\n        return self.weight * self.price\nImport the model_v5 module, giving it a friendlier name.\nPut model.NonBlank to use. The rest of the code is unchanged.\nThe LineItem examples we’ve seen in this chapter demonstrate a typical use of\ndescriptors to manage data attributes. Descriptors like Quantity are called overriding\ndescriptors because its __set__ method overrides (i.e., intercepts and overrules) the\nsetting of an instance attribute by the same name in the managed instance. However,\nthere are also nonoverriding descriptors. We’ll explore this distinction in detail in the\nnext section.\nDescriptor Example: Attribute Validation \n| \n891",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 922,
      "chapter": null,
      "content": "Overriding Versus Nonoverriding Descriptors\nRecall that there is an important asymmetry in the way Python handles attributes.\nReading an attribute through an instance normally returns the attribute defined in\nthe instance, but if there is no such attribute in the instance, a class attribute will be\nretrieved. On the other hand, assigning to an attribute in an instance normally creates\nthe attribute in the instance, without affecting the class at all.\nThis asymmetry also affects descriptors, in effect creating two broad categories of\ndescriptors, depending on whether the __set__ method is implemented. If __set__\nis present, the class is an overriding descriptor; otherwise, it is a nonoverriding\ndescriptor. These terms will make sense as we study descriptor behaviors in the next\nexamples.\nObserving the different descriptor categories requires a few classes, so we’ll use the\ncode in Example 23-8 as our test bed for the following sections.\nEvery __get__ and __set__ method in Example 23-8 calls\nprint_args so their invocations are displayed in a readable way.\nUnderstanding print_args and the auxiliary functions cls_name\nand display is not important, so don’t get distracted by them.\nExample 23-8. descriptorkinds.py: simple classes for studying descriptor overriding\nbehaviors\n### auxiliary functions for display only ###\ndef cls_name(obj_or_cls):\n    cls = type(obj_or_cls)\n    if cls is type:\n        cls = obj_or_cls\n    return cls.__name__.split('.')[-1]\ndef display(obj):\n    cls = type(obj)\n    if cls is type:\n        return f'<class {obj.__name__}>'\n    elif cls in [type(None), int]:\n        return repr(obj)\n    else:\n        return f'<{cls_name(obj)} object>'\ndef print_args(name, *args):\n    pseudo_args = ', '.join(display(x) for x in args)\n    print(f'-> {cls_name(args[0])}.__{name}__({pseudo_args})')\n### essential classes for this example ###\n892 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 923,
      "chapter": null,
      "content": "class Overriding:  \n    \"\"\"a.k.a. data descriptor or enforced descriptor\"\"\"\n    def __get__(self, instance, owner):\n        print_args('get', self, instance, owner)  \n    def __set__(self, instance, value):\n        print_args('set', self, instance, value)\nclass OverridingNoGet:  \n    \"\"\"an overriding descriptor without ``__get__``\"\"\"\n    def __set__(self, instance, value):\n        print_args('set', self, instance, value)\nclass NonOverriding:  \n    \"\"\"a.k.a. non-data or shadowable descriptor\"\"\"\n    def __get__(self, instance, owner):\n        print_args('get', self, instance, owner)\nclass Managed:  \n    over = Overriding()\n    over_no_get = OverridingNoGet()\n    non_over = NonOverriding()\n    def spam(self):  \n        print(f'-> Managed.spam({display(self)})')\nAn overriding descriptor class with __get__ and __set__.\nThe print_args function is called by every descriptor method in this example.\nAn overriding descriptor without a __get__ method.\nNo __set__ method here, so this is a nonoverriding descriptor.\nThe managed class, using one instance of each of the descriptor classes.\nThe spam method is here for comparison, because methods are also descriptors.\nIn the following sections, we will examine the behavior of attribute reads and writes\non the Managed class, and one instance of it, going through each of the different\ndescriptors defined.\nOverriding Versus Nonoverriding Descriptors \n| \n893",
      "content_length": 1409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 924,
      "chapter": null,
      "content": "Overriding Descriptors\nA descriptor that implements the __set__ method is an overriding descriptor, because\nalthough it is a class attribute, a descriptor implementing __set__ will override\nattempts to assign to instance attributes. This is how Example 23-3 was implemented.\nProperties are also overriding descriptors: if you don’t provide a setter function, the\ndefault __set__ from the property class will raise AttributeError to signal that the\nattribute is read-only. Given the code in Example 23-8, experiments with an overrid‐\ning descriptor can be seen in Example 23-9.\nPython contributors and authors use different terms when discus‐\nsing these concepts. I adopted “overriding descriptor” from the\nbook Python in a Nutshell. The official Python documentation uses\n“data descriptor,” but “overriding descriptor” highlights the special\nbehavior. Overriding descriptors are also called “enforced descrip‐\ntors.” Synonyms for nonoverriding descriptors include “nondata\ndescriptors” or “shadowable descriptors.”\nExample 23-9. Behavior of an overriding descriptor\n>>> obj = Managed()  \n>>> obj.over  \n-> Overriding.__get__(<Overriding object>, <Managed object>, <class Managed>)\n>>> Managed.over  \n-> Overriding.__get__(<Overriding object>, None, <class Managed>)\n>>> obj.over = 7  \n-> Overriding.__set__(<Overriding object>, <Managed object>, 7)\n>>> obj.over  \n-> Overriding.__get__(<Overriding object>, <Managed object>, <class Managed>)\n>>> obj.__dict__['over'] = 8  \n>>> vars(obj)  \n{'over': 8}\n>>> obj.over  \n-> Overriding.__get__(<Overriding object>, <Managed object>, <class Managed>)\nCreate Managed object for testing.\nobj.over triggers the descriptor __get__ method, passing the managed instance\nobj as the second argument.\nManaged.over triggers the descriptor __get__ method, passing None as the sec‐\nond argument (instance).\nAssigning to obj.over triggers the descriptor __set__ method, passing the value\n7 as the last argument.\n894 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 925,
      "chapter": null,
      "content": "Reading obj.over still invokes the descriptor __get__ method.\nBypassing the descriptor, setting a value directly to the obj.__dict__.\nVerify that the value is in the obj.__dict__, under the over key.\nHowever, even with an instance attribute named over, the Managed.over\ndescriptor still overrides attempts to read obj.over.\nOverriding Descriptor Without __get__\nProperties and other overriding descriptors, such as Django model fields, implement\nboth __set__ and __get__, but it’s also possible to implement only __set__, as we\nsaw in Example 23-2. In this case, only writing is handled by the descriptor. Reading\nthe descriptor through an instance will return the descriptor object itself because\nthere is no __get__ to handle that access. If a namesake instance attribute is created\nwith a new value via direct access to the instance __dict__, the __set__ method will\nstill override further attempts to set that attribute, but reading that attribute will sim‐\nply return the new value from the instance, instead of returning the descriptor object.\nIn other words, the instance attribute will shadow the descriptor, but only when read‐\ning. See Example 23-10.\nExample 23-10. Overriding descriptor without __get__\n>>> obj.over_no_get  \n<__main__.OverridingNoGet object at 0x665bcc>\n>>> Managed.over_no_get  \n<__main__.OverridingNoGet object at 0x665bcc>\n>>> obj.over_no_get = 7  \n-> OverridingNoGet.__set__(<OverridingNoGet object>, <Managed object>, 7)\n>>> obj.over_no_get  \n<__main__.OverridingNoGet object at 0x665bcc>\n>>> obj.__dict__['over_no_get'] = 9  \n>>> obj.over_no_get  \n9\n>>> obj.over_no_get = 7  \n-> OverridingNoGet.__set__(<OverridingNoGet object>, <Managed object>, 7)\n>>> obj.over_no_get  \n9\nThis overriding descriptor doesn’t have a __get__ method, so reading\nobj.over_no_get retrieves the descriptor instance from the class.\nThe same thing happens if we retrieve the descriptor instance directly from the\nmanaged class.\nOverriding Versus Nonoverriding Descriptors \n| \n895",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 926,
      "chapter": null,
      "content": "Trying to set a value to obj.over_no_get invokes the __set__ descriptor\nmethod.\nBecause our __set__ doesn’t make changes, reading obj.over_no_get again\nretrieves the descriptor instance from the managed class.\nGoing through the instance __dict__ to set an instance attribute named\nover_no_get.\nNow that over_no_get instance attribute shadows the descriptor, but only for\nreading.\nTrying to assign a value to obj.over_no_get still goes through the descriptor set.\nBut for reading, that descriptor is shadowed as long as there is a namesake\ninstance attribute.\nNonoverriding Descriptor\nA descriptor that does not implement __set__ is a nonoverriding descriptor. Setting\nan instance attribute with the same name will shadow the descriptor, rendering\nit ineffective for handling that attribute in that specific instance. Methods and @func\ntools.cached_property \nare \nimplemented \nas \nnonoverriding \ndescriptors.\nExample 23-11 shows the operation of a nonoverriding descriptor.\nExample 23-11. Behavior of a nonoverriding descriptor\n>>> obj = Managed()\n>>> obj.non_over  \n-> NonOverriding.__get__(<NonOverriding object>, <Managed object>, <class Managed>)\n>>> obj.non_over = 7  \n>>> obj.non_over  \n7\n>>> Managed.non_over  \n-> NonOverriding.__get__(<NonOverriding object>, None, <class Managed>)\n>>> del obj.non_over  \n>>> obj.non_over  \n-> NonOverriding.__get__(<NonOverriding object>, <Managed object>, <class Managed>)\nobj.non_over triggers the descriptor __get__ method, passing obj as the second\nargument.\nManaged.non_over is a nonoverriding descriptor, so there is no __set__ to inter‐\nfere with this assignment.\n896 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 927,
      "chapter": null,
      "content": "The obj now has an instance attribute named non_over, which shadows the\nnamesake descriptor attribute in the Managed class.\nThe Managed.non_over descriptor is still there, and catches this access via the\nclass.\nIf the non_over instance attribute is deleted…\n…then reading obj.non_over hits the __get__ method of the descriptor in the\nclass, but note that the second argument is the managed instance.\nIn the previous examples, we saw several assignments to an instance attribute with\nthe same name as a descriptor, and different results according to the presence of a\n__set__ method in the descriptor.\nThe setting of attributes in the class cannot be controlled by descriptors attached to\nthe same class. In particular, this means that the descriptor attributes themselves can\nbe clobbered by assigning to the class, as the next section explains.\nOverwriting a Descriptor in the Class\nRegardless of whether a descriptor is overriding or not, it can be overwritten by\nassignment to the class. This is a monkey-patching technique, but in Example 23-12\nthe descriptors are replaced by integers, which would effectively break any class that\ndepended on the descriptors for proper operation.\nExample 23-12. Any descriptor can be overwritten on the class itself\n>>> obj = Managed()  \n>>> Managed.over = 1  \n>>> Managed.over_no_get = 2\n>>> Managed.non_over = 3\n>>> obj.over, obj.over_no_get, obj.non_over  \n(1, 2, 3)\nCreate a new instance for later testing.\nOverwrite the descriptor attributes in the class.\nThe descriptors are really gone.\nExample 23-12 reveals another asymmetry regarding reading and writing attributes:\nalthough the reading of a class attribute can be controlled by a descriptor with\n__get__ attached to the managed class, the writing of a class attribute cannot be han‐\ndled by a descriptor with __set__ attached to the same class.\nOverriding Versus Nonoverriding Descriptors \n| \n897",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 928,
      "chapter": null,
      "content": "In order to control the setting of attributes in a class, you have to\nattach descriptors to the class of the class—in other words, the met‐\naclass. By default, the metaclass of user-defined classes is type, and\nyou cannot add attributes to type. But in Chapter 24, we’ll create\nour own metaclasses.\nLet’s now focus on how descriptors are used to implement methods in Python.\nMethods Are Descriptors\nA function within a class becomes a bound method when invoked on an instance\nbecause all user-defined functions have a __get__ method, therefore they operate as\ndescriptors when attached to a class. Example 23-13 demonstrates reading the spam\nmethod from the Managed class introduced in Example 23-8.\nExample 23-13. A method is a nonoverriding descriptor\n>>> obj = Managed()\n>>> obj.spam  \n<bound method Managed.spam of <descriptorkinds.Managed object at 0x74c80c>>\n>>> Managed.spam  \n<function Managed.spam at 0x734734>\n>>> obj.spam = 7  \n>>> obj.spam\n7\nReading from obj.spam retrieves a bound method object.\nBut reading from Managed.spam retrieves a function.\nAssigning a value to obj.spam shadows the class attribute, rendering the spam\nmethod inaccessible from the obj instance.\nFunctions do not implement __set__, therefore they are nonoverriding descriptors,\nas the last line of Example 23-13 shows.\nThe other key takeaway from Example 23-13 is that obj.spam and Managed.spam\nretrieve different objects. As usual with descriptors, the __get__ of a function returns\na reference to itself when the access happens through the managed class. But when\nthe access goes through an instance, the __get__ of the function returns a bound\nmethod object: a callable that wraps the function and binds the managed instance\n(e.g., obj) to the first argument of the function (i.e., self), like the functools.par\ntial function does (as seen in “Freezing Arguments with functools.partial” on page\n247). For a deeper understanding of this mechanism, take a look at Example 23-14.\n898 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 929,
      "chapter": null,
      "content": "Example 23-14. method_is_descriptor.py: a Text class, derived from UserString\nimport collections\nclass Text(collections.UserString):\n    def __repr__(self):\n        return 'Text({!r})'.format(self.data)\n    def reverse(self):\n        return self[::-1]\nNow let’s investigate the Text.reverse method. See Example 23-15.\nExample 23-15. Experiments with a method\n    >>> word = Text('forward')\n    >>> word  \n    Text('forward')\n    >>> word.reverse()  \n    Text('drawrof')\n    >>> Text.reverse(Text('backward'))  \n    Text('drawkcab')\n    >>> type(Text.reverse), type(word.reverse)  \n    (<class 'function'>, <class 'method'>)\n    >>> list(map(Text.reverse, ['repaid', (10, 20, 30), Text('stressed')]))  \n    ['diaper', (30, 20, 10), Text('desserts')]\n    >>> Text.reverse.__get__(word)  \n    <bound method Text.reverse of Text('forward')>\n    >>> Text.reverse.__get__(None, Text)  \n    <function Text.reverse at 0x101244e18>\n    >>> word.reverse  \n    <bound method Text.reverse of Text('forward')>\n    >>> word.reverse.__self__  \n    Text('forward')\n    >>> word.reverse.__func__ is Text.reverse  \n    True\nThe repr of a Text instance looks like a Text constructor call that would make an\nequal instance.\nThe reverse method returns the text spelled backward.\nA method called on the class works as a function.\nNote the different types: a function and a method.\nMethods Are Descriptors \n| \n899",
      "content_length": 1390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 930,
      "chapter": null,
      "content": "7 A __delete__ method is also provided by the property decorator, even if no deleter method is defined by\nyou.\nText.reverse operates as a function, even working with objects that are not\ninstances of Text.\nAny function is a nonoverriding descriptor. Calling its __get__ with an instance\nretrieves a method bound to that instance.\nCalling the function’s __get__ with None as the instance argument retrieves the\nfunction itself.\nThe expression word.reverse actually invokes Text.reverse.__get__(word),\nreturning the bound method.\nThe bound method object has a __self__ attribute holding a reference to the\ninstance on which the method was called.\nThe __func__ attribute of the bound method is a reference to the original func‐\ntion attached to the managed class.\nThe bound method object also has a __call__ method, which handles the actual\ninvocation. This method calls the original function referenced in __func__, passing\nthe __self__ attribute of the method as the first argument. That’s how the implicit\nbinding of the conventional self argument works.\nThe way functions are turned into bound methods is a prime example of how\ndescriptors are used as infrastructure in the language.\nAfter this deep dive into how descriptors and methods work, let’s go through some\npractical advice about their use.\nDescriptor Usage Tips\nThe following list addresses some practical consequences of the descriptor character‐\nistics just described:\nUse property to keep it simple\nThe property built-in creates overriding descriptors implementing __set__ and\n__get__ even if you do not define a setter method.7 The default __set__ of a\nproperty raises AttributeError: can't set attribute, so a property is the\neasiest way to create a read-only attribute, avoiding the issue described next.\n900 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 931,
      "chapter": null,
      "content": "8 Python is not consistent in such messages. Trying to change the c.real attribute of a complex number gets\nAttributeError: readonly attribute, but an attempt to change c.conjugate (a method of complex),\nresults in AttributeError: 'complex' object attribute 'conjugate' is read-only. Even the spelling\nof “read-only” is different.\n9 However, recall that creating instance attributes after the __init__ method runs defeats the key-sharing\nmemory optimization, as discussed in from “Practical Consequences of How dict Works” on page 102.\nRead-only descriptors require __set__\nIf you use a descriptor class to implement a read-only attribute, you must\nremember to code both __get__ and __set__, otherwise setting a namesake\nattribute on an instance will shadow the descriptor. The __set__ method of a\nread-only attribute should just raise AttributeError with a suitable message.8\nValidation descriptors can work with __set__ only\nIn a descriptor designed only for validation, the __set__ method should check\nthe value argument it gets, and if valid, set it directly in the instance __dict__\nusing the descriptor instance name as key. That way, reading the attribute with\nthe same name from the instance will be as fast as possible, because it will not\nrequire a __get__. See the code for Example 23-3.\nCaching can be done efficiently with __get__ only\nIf you code just the __get__ method, you have a nonoverriding descriptor. These\nare useful to make some expensive computation and then cache the result by set‐\nting an attribute by the same name on the instance.9 The namesake instance\nattribute will shadow the descriptor, so subsequent access to that attribute will\nfetch it directly from the instance __dict__ and not trigger the descriptor\n__get__ anymore. The @functools.cached_property decorator actually pro‐\nduces a nonoverriding descriptor.\nNonspecial methods can be shadowed by instance attributes\nBecause functions and methods only implement __get__, they are nonoverriding\ndescriptors. A simple assignment like my_obj.the_method = 7 means that fur‐\nther access to the_method through that instance will retrieve the number 7—\nwithout affecting the class or other instances. However, this issue does not inter‐\nfere with special methods. The interpreter only looks for special methods in the\nclass itself, in other words, repr(x) is executed as x.__class__.__repr__(x), so\na __repr__ attribute defined in x has no effect on repr(x). For the same reason,\nthe existence of an attribute named __getattr__ in an instance will not subvert\nthe usual attribute access algorithm.\nThe fact that nonspecial methods can be overridden so easily in instances may sound\nfragile and error prone, but I personally have never been bitten by this in more than\n20 years of Python coding. On the other hand, if you are doing a lot of dynamic\nDescriptor Usage Tips \n| \n901",
      "content_length": 2859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 932,
      "chapter": null,
      "content": "10 Customizing the help text for each descriptor instance is surprisingly hard. One solution requires dynamically\nbuilding a wrapper class for each descriptor instance.\nattribute creation, where the attribute names come from data you don’t control (as\nwe did in the earlier parts of this chapter), then you should be aware of this and per‐\nhaps implement some filtering or escaping of the dynamic attribute names to pre‐\nserve your sanity.\nThe FrozenJSON class in Example 22-5 is safe from instance\nattribute shadowing methods because its only methods are special\nmethods and the build class method. Class methods are safe as\nlong as they are always accessed through the class, as I did with\nFrozenJSON.build in Example 22-5—later replaced by __new__ in\nExample 22-6. The Record and Event classes presented in “Com‐\nputed Properties” on page 845 are also safe: they implement only\nspecial methods, static methods, and properties. Properties are\noverriding descriptors, so they are not shadowed by instance\nattributes.\nTo close this chapter, we’ll cover two features we saw with properties that we have\nnot addressed in the context of descriptors: documentation and handling attempts to\ndelete a managed attribute.\nDescriptor Docstring and Overriding Deletion\nThe docstring of a descriptor class is used to document every instance of the descrip‐\ntor in the managed class. Figure 23-4 shows the help displays for the LineItem class\nwith the Quantity and NonBlank descriptors from Examples 23-6 and 23-7.\nThat is somewhat unsatisfactory. In the case of LineItem, it would be good to add, for\nexample, the information that weight must be in kilograms. That would be trivial\nwith properties, because each property handles a specific managed attribute. But with\ndescriptors, the same Quantity descriptor class is used for weight and price.10\nThe second detail we discussed with properties, but have not addressed with descrip‐\ntors, is handling attempts to delete a managed attribute. That can be done by imple‐\nmenting a __delete__ method alongside or instead of the usual __get__ and/or\n__set__ in the descriptor class. I deliberately omitted coverage of __delete__\nbecause I believe real-world usage is rare. If you need this, please see the “Implement‐\ning Descriptors” section of the Python Data Model documentation. Coding a silly\ndescriptor class with __delete__ is left as an exercise to the leisurely reader.\n902 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 2454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 933,
      "chapter": null,
      "content": "Figure 23-4. Screenshots of the Python console when issuing the commands help(LineI\ntem.weight) and help(LineItem).\nChapter Summary\nThe first example of this chapter was a continuation of the LineItem examples from\nChapter 22. In Example 23-2, we replaced properties with descriptors. We saw that a\ndescriptor is a class that provides instances that are deployed as attributes in the man‐\naged class. Discussing this mechanism required special terminology, introducing\nterms such as managed instance and storage attribute.\nIn “LineItem Take #4: Automatic Naming of Storage Attributes” on page 887, we\nremoved the requirement that Quantity descriptors were declared with an explicit\nstorage_name, which was redundant and error prone. The solution was to imple‐\nment the __set_name__ special method in Quantity, to save the name of the man‐\naged property as self.storage_name.\n“LineItem Take #5: A New Descriptor Type” on page 889 showed how to subclass an\nabstract descriptor class to share code while building specialized descriptors with\nsome common functionality.\nChapter Summary \n| \n903",
      "content_length": 1089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 934,
      "chapter": null,
      "content": "We then looked at the different behaviors of descriptors providing or omitting the\n__set__ method, making the crucial distinction between overriding and nonoverrid‐\ning descriptors, a.k.a. data and nondata descriptors. Through detailed testing we\nuncovered when descriptors are in control and when they are shadowed, bypassed, or\noverwritten.\nFollowing that, we studied a particular category of nonoverriding descriptors: meth‐\nods. Console experiments revealed how a function attached to a class becomes a\nmethod when accessed through an instance, by leveraging the descriptor protocol.\nTo conclude the chapter, “Descriptor Usage Tips” on page 900 presented practical\ntips, and “Descriptor Docstring and Overriding Deletion” on page 902 provided a\nbrief look at how to document descriptors.\nAs noted in “What’s New in This Chapter” on page 880, several\nexamples in this chapter became much simpler thanks to the\n__set_name__ special method of the descriptor protocol, added in\nPython 3.6. That’s language evolution!\nFurther Reading\nBesides the obligatory reference to the “Data Model” chapter, Raymond Hettinger’s\n“Descriptor HowTo Guide” is a valuable resource—part of the HowTo collection in\nthe official Python documentation.\nAs usual with Python object model subjects, Martelli, Ravenscroft, and Holden’s\nPython in a Nutshell, 3rd ed. (O’Reilly) is authoritative and objective. Martelli also\nhas a presentation titled “Python’s Object Model,” which covers properties and\ndescriptors in depth (see the slides and video).\nBeware that any coverage of descriptors written or recorded before\nPEP 487 was adopted in 2016 is likely to contain examples that are\nneedlessly complicated today, because __set_name__ was not sup‐\nported in Python versions prior to 3.6.\nFor more practical examples, Python Cookbook, 3rd ed., by David Beazley and Brian\nK. Jones (O’Reilly), has many recipes illustrating descriptors, of which I want to\nhighlight “6.12. Reading Nested and Variable-Sized Binary Structures,” “8.10. Using\nLazily Computed Properties,” “8.13. Implementing a Data Model or Type System,”\nand “9.9. Defining Decorators As Classes.” The last recipe of which addresses deep\nissues with the interaction of function decorators, descriptors, and methods,\nexplaining how a function decorator implemented as a class with __call__ also\n904 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 2371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 935,
      "chapter": null,
      "content": "needs to implement __get__ if it wants to work with decorating methods as well as\nfunctions.\nPEP 487—Simpler customization of class creation introduced the __set_name__ spe‐\ncial method, and includes an example of a validating descriptor.\nSoapbox\nThe Design of self\nThe requirement to explicitly declare self as a first argument in methods is a contro‐\nversial design decision in Python. After 23 years using the language, I am used to it. I\nthink that decision is an example of “worse is better”: a design philosophy described\nby computer scientist Richard P. Gabriel in “The Rise of Worse is Better”. The first\npriority of this philosophy is “simplicity,” which Gabriel presents as:\nThe design must be simple, both in implementation and interface. It is more impor‐\ntant for the implementation to be simple than the interface. Simplicity is the most\nimportant consideration in a design.\nPython’s explicit self embodies that design philosophy. The implementation is sim‐\nple—elegant even—at the expense of the user interface: a method signature like def\nzfill(self, width): doesn’t visually match the invocation label.zfill(8).\nModula-3 introduced that convention with the same identifier self. But there is a\nkey difference: in Modula-3, interfaces are declared separately from their implemen‐\ntation, and in the interface declaration the self argument is omitted, so from the\nuser’s perspective, a method appears in an interface declaration with the same explicit\nparameters used to call it.\nOver time, Python’s error messages related to method arguments became clearer. For\na user-defined method with one argument besides self, if the user invokes\nobj.meth(), Python 2.7 raised:\nTypeError: meth() takes exactly 2 arguments (1 given)\nIn Python 3, the confusing argument count is not mentioned, and the missing argu‐\nment is named:\nTypeError: meth() missing 1 required positional argument: 'x'\nBesides the use of self as an explicit argument, the requirement to qualify every\naccess to instance attributes with self is also criticized. See, for example, A. M. Kuch‐\nling’s famous “Python Warts” post (archived); Kuchling himself is not so bothered by\nthe self qualifier, but he mentions it—probably echoing opinions from the\ncomp.lang.python group. I personally don’t mind typing the self qualifier: it’s good\nto distinguish local variables from attributes. My issue is with the use of self in the\ndef statement.\nFurther Reading \n| \n905",
      "content_length": 2438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 936,
      "chapter": null,
      "content": "Anyone who is unhappy about the explicit self in Python can feel a lot better by con‐\nsidering the baffling semantics of the implicit this in JavaScript. Guido had some\ngood reasons to make self work as it does, and he wrote about them in “Adding Sup‐\nport for User-Defined Classes”, a post on his blog, The History of Python.\n906 \n| \nChapter 23: Attribute Descriptors",
      "content_length": 368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 937,
      "chapter": null,
      "content": "1 Quote from Chapter 2, “Expression” of The Elements of Programming Style, 2nd ed. (McGraw-Hill), page 10.\n2 That doesn’t mean PEP 487 broke code that used those features. It just means that some code that used class\ndecorators or metaclasses prior to Python 3.6 can now be refactored to use plain classes, resulting in simpler\nand possibly more efficient code.\nCHAPTER 24\nClass Metaprogramming\nEveryone knows that debugging is twice as hard as writing a program in the first place.\nSo if you’re as clever as you can be when you write it, how will you ever debug it?\n—Brian W. Kernighan and P. J. Plauger, The Elements of Programming Style1\nClass metaprogramming is the art of creating or customizing classes at runtime.\nClasses are first-class objects in Python, so a function can be used to create a new\nclass at any time, without using the class keyword. Class decorators are also func‐\ntions, but designed to inspect, change, and even replace the decorated class with\nanother class. Finally, metaclasses are the most advanced tool for class metaprogram‐\nming: they let you create whole new categories of classes with special traits, such as\nthe abstract base classes we’ve already seen.\nMetaclasses are powerful, but hard to justify and even harder to get right. Class deco‐\nrators solve many of the same problems and are easier to understand. Furthermore,\nPython 3.6 implemented PEP 487—Simpler customization of class creation, provid‐\ning special methods supporting tasks that previously required metaclasses or class\ndecorators.2\nThis chapter presents the class metaprogramming techniques in ascending order of\ncomplexity.\n907",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 938,
      "chapter": null,
      "content": "This is an exciting topic, and it’s easy to get carried away. So I must\noffer this advice.\nFor the sake of readability and maintainability, you should proba‐\nbly avoid the techniques described in this chapter in application\ncode.\nOn the other hand, these are the tools of the trade if you want to\nwrite the next great Python framework.\nWhat’s New in This Chapter\nAll the code in the “Class Metaprogramming” chapter of the first edition of Fluent\nPython still runs correctly. However, some of the previous examples no longer repre‐\nsent the simplest solutions in light of new features added since Python 3.6.\nI replaced those examples with different ones, highlighting Python’s new metaprog‐\nramming features or adding further requirements to justify the use of the more\nadvanced techniques. Some of the new examples leverage type hints to provide class\nbuilders similar to the @dataclass decorator and typing.NamedTuple.\n“Metaclasses in the Real World” on page 947 is a new section with some high-level con‐\nsiderations about the applicability of metaclasses.\nSome of the best refactorings are removing code made redundant\nby newer and simpler ways of solving the same problems. This\napplies to production code as well as books.\nWe’ll get started by reviewing attributes and methods defined in the Python Data\nModel for all classes.\nClasses as Objects\nLike most program entities in Python, classes are also objects. Every class has a num‐\nber of attributes defined in the Python Data Model, documented in “4.13. Special\nAttributes” of the “Built-in Types” chapter in The Python Standard Library. Three of\nthose attributes appeared several times in this book already: __class__, __name__,\nand __mro__. Other class standard attributes are:\ncls.__bases__\nThe tuple of base classes of the class.\n908 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 939,
      "chapter": null,
      "content": "cls.__qualname__\nThe qualified name of a class or function, which is a dotted path from the global\nscope of the module to the class definition. This is relevant when the class is\ndefined inside another class. For example, in a Django model class such as Ox,\nthere is an inner class called Meta. The __qualname__ of Meta is Ox.Meta, but its\n__name__ is just Meta. The specification for this attribute is PEP 3155—Qualified\nname for classes and functions.\ncls.__subclasses__()\nThis method returns a list of the immediate subclasses of the class. The imple‐\nmentation uses weak references to avoid circular references between the super‐\nclass and its subclasses—which hold a strong reference to the superclasses in\ntheir __bases__ attribute. The method lists subclasses currently in memory. Sub‐\nclasses in modules not yet imported will not appear in the result.\ncls.mro()\nThe interpreter calls this method when building a class to obtain the tuple of\nsuperclasses stored in the __mro__ attribute of the class. A metaclass can override\nthis method to customize the method resolution order of the class under\nconstruction.\nNone of the attributes mentioned in this section are listed by the\ndir(…) function.\nNow, if a class is an object, what is the class of a class?\ntype: The Built-In Class Factory\nWe usually think of type as a function that returns the class of an object, because\nthat’s what type(my_object) does: it returns my_object.__class__.\nHowever, type is a class that creates a new class when invoked with three arguments.\nConsider this simple class:\nclass MyClass(MySuperClass, MyMixin):\n    x = 42\n    def x2(self):\n        return self.x * 2\nUsing the type constructor, you can create MyClass at runtime with this code:\ntype: The Built-In Class Factory \n| \n909",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 940,
      "chapter": null,
      "content": "MyClass = type('MyClass',\n               (MySuperClass, MyMixin),\n               {'x': 42, 'x2': lambda self: self.x * 2},\n          )\nThat type call is functionally equivalent to the previous class MyClass… block\nstatement.\nWhen Python reads a class statement, it calls type to build the class object with\nthese parameters:\nname\nThe identifier that appears after the class keyword, e.g., MyClass.\nbases\nThe tuple of superclasses given in parentheses after the class identifier, or\n(object,) if superclasses are not mentioned in the class statement.\ndict\nA mapping of attribute names to values. Callables become methods, as we saw in\n“Methods Are Descriptors” on page 898. Other values become class attributes.\nThe type constructor accepts optional keyword arguments, which\nare ignored by type itself, but are passed untouched into\n__init_subclass__, which must consume them. We’ll study that\nspecial method in “Introducing __init_subclass__” on page 914, but\nI won’t cover the use of keyword arguments. For more, please read\nPEP 487—Simpler customization of class creation.\nThe type class is a metaclass: a class that builds classes. In other words, instances of\nthe type class are classes. The standard library provides a few other metaclasses, but\ntype is the default:\n>>> type(7)\n<class 'int'>\n>>> type(int)\n<class 'type'>\n>>> type(OSError)\n<class 'type'>\n>>> class Whatever:\n...     pass\n...\n>>> type(Whatever)\n<class 'type'>\nWe’ll build custom metaclasses in “Metaclasses 101” on page 931.\nNext, we’ll use the type built-in to make a function that builds classes.\n910 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 941,
      "chapter": null,
      "content": "A Class Factory Function\nThe standard library has a class factory function that appears several times in this\nbook: collections.namedtuple. In Chapter 5 we also saw typing.NamedTuple and\n@dataclass. All of these class builders leverage techniques covered in this chapter.\nWe’ll start with a super simple factory for classes of mutable objects—the simplest\npossible replacement for @dataclass.\nSuppose I’m writing a pet shop application and I want to store data for dogs as simple\nrecords. But I don’t want to write boilerplate like this:\nclass Dog:\n    def __init__(self, name, weight, owner):\n        self.name = name\n        self.weight = weight\n        self.owner = owner\nBoring…each field name appears three times, and that boilerplate doesn’t even buy\nus a nice repr:\n>>> rex = Dog('Rex', 30, 'Bob')\n>>> rex\n<__main__.Dog object at 0x2865bac>\nTaking a hint from collections.namedtuple, let’s create a record_factory that cre‐\nates simple classes like Dog on the fly. Example 24-1 shows how it should work.\nExample 24-1. Testing record_factory, a simple class factory\n    >>> Dog = record_factory('Dog', 'name weight owner')  \n    >>> rex = Dog('Rex', 30, 'Bob')\n    >>> rex  \n    Dog(name='Rex', weight=30, owner='Bob')\n    >>> name, weight, _ = rex  \n    >>> name, weight\n    ('Rex', 30)\n    >>> \"{2}'s dog weighs {1}kg\".format(*rex)  \n    \"Bob's dog weighs 30kg\"\n    >>> rex.weight = 32  \n    >>> rex\n    Dog(name='Rex', weight=32, owner='Bob')\n    >>> Dog.__mro__  \n    (<class 'factories.Dog'>, <class 'object'>)\nFactory can be called like namedtuple: class name, followed by attribute names\nseparated by spaces in a single string.\nNice repr.\nA Class Factory Function \n| \n911",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 942,
      "chapter": null,
      "content": "3 Thanks to my friend J. S. O. Bueno for contributing to this example.\nInstances are iterable, so they can be conveniently unpacked on assignment…\n…or when passing to functions like format.\nA record instance is mutable.\nThe newly created class inherits from object—no relationship to our factory.\nThe code for record_factory is in Example 24-2.3\nExample 24-2. record_factory.py: a simple class factory\nfrom typing import Union, Any\nfrom collections.abc import Iterable, Iterator\nFieldNames = Union[str, Iterable[str]]  \ndef record_factory(cls_name: str, field_names: FieldNames) -> type[tuple]:  \n    slots = parse_identifiers(field_names)  \n    def __init__(self, *args, **kwargs) -> None:  \n        attrs = dict(zip(self.__slots__, args))\n        attrs.update(kwargs)\n        for name, value in attrs.items():\n            setattr(self, name, value)\n    def __iter__(self) -> Iterator[Any]:  \n        for name in self.__slots__:\n            yield getattr(self, name)\n    def __repr__(self):  \n        values = ', '.join(f'{name}={value!r}'\n            for name, value in zip(self.__slots__, self))\n        cls_name = self.__class__.__name__\n        return f'{cls_name}({values})'\n    cls_attrs = dict(  \n        __slots__=slots,\n        __init__=__init__,\n        __iter__=__iter__,\n        __repr__=__repr__,\n    )\n    return type(cls_name, (object,), cls_attrs)  \n912 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 943,
      "chapter": null,
      "content": "4 I did not add type hints to the arguments because the actual types are Any. I put the return type hint because\notherwise Mypy will not check inside the method.\ndef parse_identifiers(names: FieldNames) -> tuple[str, ...]:\n    if isinstance(names, str):\n        names = names.replace(',', ' ').split()  \n    if not all(s.isidentifier() for s in names):\n        raise ValueError('names must all be valid identifiers')\n    return tuple(names)\nUser can provide field names as a single string or an iterable of strings.\nAccept arguments like the first two of collections.namedtuple; return a type—\ni.e., a class that behaves like a tuple.\nBuild a tuple of attribute names; this will be the __slots__ attribute of the new\nclass.\nThis function will become the __init__ method in the new class. It accepts posi‐\ntional and/or keyword arguments.4\nYield the field values in the order given by __slots__.\nProduce the nice repr, iterating over __slots__ and self.\nAssemble a dictionary of class attributes.\nBuild and return the new class, calling the type constructor.\nConvert names separated by spaces or commas to list of str.\nExample 24-2 is the first time we’ve seen type in a type hint. If the annotation was\njust -> type, that would mean that record_factory returns a class—and it would be\ncorrect. But the annotation -> type[tuple] is more precise: it says the returned class\nwill be a subclass of tuple.\nThe last line of record_factory in Example 24-2 builds a class named by the value of\ncls_name, with object as its single immediate base class, and with a namespace\nloaded with __slots__, __init__, __iter__, and __repr__, of which the last three\nare instance methods.\nWe could have named the __slots__ class attribute anything else, but then we’d have\nto implement __setattr__ to validate the names of attributes being assigned,\nA Class Factory Function \n| \n913",
      "content_length": 1861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 944,
      "chapter": null,
      "content": "because for our record-like classes we want the set of attributes to be always the same\nand in the same order. However, recall that the main feature of __slots__ is saving\nmemory when you are dealing with millions of instances, and using __slots__ has\nsome drawbacks, discussed in “Saving Memory with __slots__” on page 384.\nInstances of classes created by record_factory are not serializable\n—that is, they can’t be exported with the dump function from the\npickle module. Solving this problem is beyond the scope of this\nexample, which aims to show the type class in action in a simple\nuse case. For the full solution, study the source code for\ncollections.namedtuple; search for the word “pickling.”\nNow let’s see how to emulate more modern class builders like typing.NamedTuple,\nwhich takes a user-defined class written as a class statement, and automatically\nenhances it with more functionality.\nIntroducing __init_subclass__\nBoth __init_subclass__ and __set_name__ were proposed in PEP 487—Simpler\ncustomization of class creation. We saw the __set_name__ special method for\ndescriptors for the first time in “LineItem Take #4: Automatic Naming of Storage\nAttributes” on page 887. Now let’s study __init_subclass__.\nIn Chapter 5, we saw that typing.NamedTuple and @dataclass let programmers use\nthe class statement to specify attributes for a new class, which is then enhanced by\nthe class builder with the automatic addition of essential methods like __init__,\n__repr__, __eq__, etc.\nBoth of these class builders read type hints in the user’s class statement to enhance\nthe class. Those type hints also allow static type checkers to validate code that sets or\ngets those attributes. However, NamedTuple and @dataclass do not take advantage\nof the type hints for attribute validation at runtime. The Checked class in the next\nexample does.\nIt is not possible to support every conceivable static type hint for\nruntime type checking, which is probably why typing.NamedTuple\nand @dataclass don’t even try it. However, some types that are\nalso concrete classes can be used with Checked. This includes sim‐\nple types often used for field contents, such as str, int, float, and\nbool, as well as lists of those types.\nExample 24-3 shows how to use Checked to build a Movie class.\n914 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 945,
      "chapter": null,
      "content": "5 That’s true for any object, except when its class overrides the __str__ or __repr__ methods inherited from\nobject with broken implementations.\n6 This solution avoids using None as a default. Avoiding null values is a good idea. They are hard to avoid in\ngeneral, but easy in some cases. In Python as well as SQL, I prefer to represent missing data in a text field with\nan empty string instead of None or NULL. Learning Go reinforced this idea: variables and struct fields of primi‐\ntive types in Go are initialized by default with a “zero value.” See “Zero values” in the online Tour of Go if you\nare curious.\nExample 24-3. initsub/checkedlib.py: doctest for creating a Movie subclass of Checked\n    >>> class Movie(Checked):  \n    ...     title: str  \n    ...     year: int\n    ...     box_office: float\n    ...\n    >>> movie = Movie(title='The Godfather', year=1972, box_office=137)  \n    >>> movie.title\n    'The Godfather'\n    >>> movie  \n    Movie(title='The Godfather', year=1972, box_office=137.0)\nMovie inherits from Checked—which we’ll define later in Example 24-5.\nEach attribute is annotated with a constructor. Here I used built-in types.\nMovie instances must be created using keyword arguments.\nIn return, you get a nice __repr__.\nThe constructors used as the attribute type hints may be any callable that takes zero\nor one argument and returns a value suitable for the intended field type, or rejects the\nargument by raising TypeError or ValueError.\nUsing built-in types for the annotations in Example 24-3 means the values must be\nacceptable by the constructor of the type. For int, this means any x such that int(x)\nreturns an int. For str, anything goes at runtime, because str(x) works with any x\nin Python.5\nWhen called with no arguments, the constructor should return a default value of its\ntype.6\nThis is standard behavior for Python’s built-in constructors:\n>>> int(), float(), bool(), str(), list(), dict(), set()\n(0, 0.0, False, '', [], {}, set())\nIntroducing __init_subclass__ \n| \n915",
      "content_length": 2011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 946,
      "chapter": null,
      "content": "In a Checked subclass like Movie, missing parameters create instances with default\nvalues returned by the field constructors. For example:\n    >>> Movie(title='Life of Brian')\n    Movie(title='Life of Brian', year=0, box_office=0.0)\nThe constructors are used for validation during instantiation and when an attribute is\nset directly on an instance:\n    >>> blockbuster = Movie(title='Avatar', year=2009, box_office='billions')\n    Traceback (most recent call last):\n      ...\n    TypeError: 'billions' is not compatible with box_office:float\n    >>> movie.year = 'MCMLXXII'\n    Traceback (most recent call last):\n      ...\n    TypeError: 'MCMLXXII' is not compatible with year:int\nChecked Subclasses and Static Type Checking\nIn a .py source file with a movie instance of Movie, as defined in\nExample 24-3, Mypy flags this assignment as a type error:\nmovie.year = 'MCMLXXII'\nHowever, Mypy can’t detect type errors in this constructor call:\nblockbuster = Movie(title='Avatar', year='MMIX')\nThat’s because Movie inherits Checked.__init__, and the signature\nof that method must accept any keyword arguments to support\narbitrary user-defined classes.\nOn the other hand, if you declare a Checked subclass field with the\ntype hint list[float], Mypy can flag assignments of lists with\nincompatible contents, but Checked will ignore the type parameter\nand treat that the same as list.\nNow let’s look at the implementation of checkedlib.py. The first class is the Field\ndescriptor, as shown in Example 24-4.\nExample 24-4. initsub/checkedlib.py: the Field descriptor class\nfrom collections.abc import Callable  \nfrom typing import Any, NoReturn, get_type_hints\nclass Field:\n    def __init__(self, name: str, constructor: Callable) -> None:  \n        if not callable(constructor) or constructor is type(None):  \n            raise TypeError(f'{name!r} type hint must be callable')\n        self.name = name\n916 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 947,
      "chapter": null,
      "content": "7 I believe that callable should be made suitable for type hinting. As of May 6, 2021, this is an open issue.\n        self.constructor = constructor\n    def __set__(self, instance: Any, value: Any) -> None:\n        if value is ...:  \n            value = self.constructor()\n        else:\n            try:\n                value = self.constructor(value)  \n            except (TypeError, ValueError) as e:  \n                type_name = self.constructor.__name__\n                msg = f'{value!r} is not compatible with {self.name}:{type_name}'\n                raise TypeError(msg) from e\n        instance.__dict__[self.name] = value  \nRecall that since Python 3.9, the Callable type for annotations is the ABC in\ncollections.abc, and not the deprecated typing.Callable.\nThis is a minimal Callable type hint; the parameter type and return type for\nconstructor are both implicitly Any.\nFor runtime checking, we use the callable built-in.7 The test against\ntype(None) is necessary because Python reads None in a type as NoneType, the\nclass of None (therefore callable), but a useless constructor that only returns None.\nIf Checked.__init__ sets the value as ... (the Ellipsis built-in object), we call\nthe constructor with no arguments.\nOtherwise, call the constructor with the given value.\nIf constructor raises either of these exceptions, we raise TypeError with a help‐\nful message including the names of the field and constructor; e.g., 'MMIX' is\nnot compatible with year:int.\nIf no exceptions were raised, the value is stored in the instance.__dict__.\nIn __set__, we need to catch TypeError and ValueError because built-in construc‐\ntors may raise either of them, depending on the argument. For example, float(None)\nraises TypeError, but float('A') raises ValueError. On the other hand, float('8')\nraises no error and returns 8.0. I hereby declare that this is a feature and not a bug of\nthis toy example.\nIntroducing __init_subclass__ \n| \n917",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 948,
      "chapter": null,
      "content": "In “LineItem Take #4: Automatic Naming of Storage Attributes”\non page 887, we saw the handy __set_name__ special method for\ndescriptors. We don’t need it in the Field class because the\ndescriptors are not instantiated in client source code; the user\ndeclares types that are constructors, as we saw in the Movie class\n(Example 24-3). Instead, the Field descriptor instances are created\nat runtime by the Checked.__init_subclass__ method, which\nwe’ll see in Example 24-5.\nNow let’s focus on the Checked class. I split it in two listings. Example 24-5 shows the\ntop of the class, which includes the most important methods in this example. The\nremaining methods are in Example 24-6.\nExample 24-5. initsub/checkedlib.py: the most important methods of the Checked class\nclass Checked:\n    @classmethod\n    def _fields(cls) -> dict[str, type]:  \n        return get_type_hints(cls)\n    def __init_subclass__(subclass) -> None:  \n        super().__init_subclass__()           \n        for name, constructor in subclass._fields().items():   \n            setattr(subclass, name, Field(name, constructor))  \n    def __init__(self, **kwargs: Any) -> None:\n        for name in self._fields():             \n            value = kwargs.pop(name, ...)       \n            setattr(self, name, value)          \n        if kwargs:                              \n            self.__flag_unknown_attrs(*kwargs)  \nI wrote this class method to hide the call to typing.get_type_hints from the\nrest of the class. If I need to support Python ≥ 3.10 only, I’d call\ninspect.get_annotations instead. Review “Problems with Annotations at Run‐\ntime” on page 538 for the issues with those functions.\n__init_subclass__ is called when a subclass of the current class is defined. It\ngets that new subclass as its first argument—which is why I named the argument\nsubclass instead of the usual cls. For more on this, see “__init_subclass__ Is\nNot a Typical Class Method” on page 919.\nsuper().__init_subclass__() is not strictly necessary, but should be invoked\nto play nice with other classes that might implement .__init_subclass__() in\n918 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 949,
      "chapter": null,
      "content": "8 As mentioned in “Loops, Sentinels, and Poison Pills” on page 721, the Ellipsis object is a convenient and\nsafe sentinel value. It has been around for a long time, but recently people are finding more uses for it, as we\nsee in type hints and NumPy.\nthe same inheritance graph. See “Multiple Inheritance and Method Resolution\nOrder” on page 494.\nIterate over each field name and constructor…\n…creating an attribute on subclass with that name bound to a Field descriptor\nparameterized with name and constructor.\nFor each name in the class fields…\n…get the corresponding value from kwargs and remove it from kwargs.\nUsing ... (the Ellipsis object) as default allows us to distinguish between argu‐\nments given the value None from arguments that were not given.8\nThis setattr call triggers Checked.__setattr__, shown in Example 24-6.\nIf there are remaining items in kwargs, their names do not match any of the\ndeclared fields, and __init__ will fail.\nThe error is reported by __flag_unknown_attrs, listed in Example 24-6. It takes\na *names argument with the unknown attribute names. I used a single asterisk in\n*kwargs to pass its keys as a sequence of arguments.\n__init_subclass__ Is Not a Typical Class Method\nThe @classmethod decorator is never used with __init_subclass__, but that doesn’t\nmean much, because the __new__ special method behaves as a class method even\nwithout @classmethod. The first argument that Python passes to __init_subclass__\nis a class. However, it is never the class where __init_subclass__ is implemented: it\nis a newly defined subclass of that class. That’s unlike __new__ and every other class\nmethod that I know about. Therefore, I think __init_subclass__ is not a class\nmethod in the usual sense, and it is misleading to name the first argument cls. The\n__init_suclass__ documentation names the argument cls but explains: “…called\nwhenever the containing class is subclassed. cls is then the new subclass.”\nNow let’s see the remaining methods of the Checked class, continuing from\nExample 24-5. Note that I prepended _ to the _fields and _asdict method names\nIntroducing __init_subclass__ \n| \n919",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 950,
      "chapter": null,
      "content": "9 The subtle concept of an overriding descriptor was explained in “Overriding Descriptors” on page 894.\nfor the same reason the collections.namedtuple API does: to reduce the chance of\nname clashes with user-defined field names.\nExample 24-6. initsub/checkedlib.py: remaining methods of the Checked class\n    def __setattr__(self, name: str, value: Any) -> None:  \n        if name in self._fields():              \n            cls = self.__class__\n            descriptor = getattr(cls, name)\n            descriptor.__set__(self, value)     \n        else:                                   \n            self.__flag_unknown_attrs(name)\n    def __flag_unknown_attrs(self, *names: str) -> NoReturn:  \n        plural = 's' if len(names) > 1 else ''\n        extra = ', '.join(f'{name!r}' for name in names)\n        cls_name = repr(self.__class__.__name__)\n        raise AttributeError(f'{cls_name} object has no attribute{plural} {extra}')\n    def _asdict(self) -> dict[str, Any]:  \n        return {\n            name: getattr(self, name)\n            for name, attr in self.__class__.__dict__.items()\n            if isinstance(attr, Field)\n        }\n    def __repr__(self) -> str:  \n        kwargs = ', '.join(\n            f'{key}={value!r}' for key, value in self._asdict().items()\n        )\n        return f'{self.__class__.__name__}({kwargs})'\nIntercept all attempts to set an instance attribute. This is needed to prevent set‐\nting an unknown attribute.\nIf the attribute name is known, fetch the corresponding descriptor.\nUsually we don’t need to call the descriptor __set__ explicitly. It was necessary\nin this case because __setattr__ intercepts all attempts to set an attribute on the\ninstance, including in the presence of an overriding descriptor such as Field.9\nOtherwise, the attribute name is unknown, and an exception will be raised by\n__flag_unknown_attrs.\n920 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1904,
      "extraction_method": "Direct"
    },
    {
      "page_number": 951,
      "chapter": null,
      "content": "Build a helpful error message listing all unexpected arguments, and raise Attribu\nteError. This is a rare example of the NoReturn special type, covered in “NoRe‐\nturn” on page 294.\nCreate a dict from the attributes of a Movie object. I’d call this method\n_as_dict, but I followed the convention started by the _asdict method in col\nlections.namedtuple.\nImplementing a nice __repr__ is the main reason for having _asdict in this\nexample.\nThe Checked example illustrates how to handle overriding descriptors when imple‐\nmenting __setattr__ to block arbitrary attribute setting after instantiation. It is\ndebatable whether implementing __setattr__ is worthwhile in this example.\nWithout it, setting movie.director = 'Greta Gerwig' would succeed, but the\ndirector attribute would not be checked in any way, and would not appear in the\n__repr__ nor would it be included in the dict returned by _asdict—both defined in\nExample 24-6.\nIn record_factory.py (Example 24-2) I solved this issue using the __slots__ class\nattribute. However, this simpler solution is not viable in this case, as explained next.\nWhy __init_subclass__ Cannot Configure __slots__\nThe __slots__ attribute is only effective if it is one of the entries in the class name‐\nspace passed to type.__new__. Adding __slots__ to an existing class has no effect.\nPython invokes __init_subclass__ only after the class is built—by then it’s too late\nto configure __slots__. A class decorator can’t configure __slots__ either, because\nit is applied even later than __init_subclass__. We’ll explore these timing issues in\n“What Happens When: Import Time Versus Runtime” on page 925.\nTo configure __slots__ at runtime, your own code must build the class namespace\npassed as the last argument of type.__new__. To do that, you can write a class factory\nfunction, like record_factory.py, or you can take the nuclear option and implement a\nmetaclass. We will see how to dynamically configure __slots__ in “Metaclasses 101”\non page 931.\nBefore PEP 487 simplified the customization of class creation with __init_sub\nclass__ in Python 3.7, similar functionality had to be implemented using a class dec‐\norator. That’s the focus of the next section.\nIntroducing __init_subclass__ \n| \n921",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 952,
      "chapter": null,
      "content": "10 This rationale appears in the abstract of PEP 557–Data Classes to explain why it was implemented as a class\ndecorator.\nEnhancing Classes with a Class Decorator\nA class decorator is a callable that behaves similarly to a function decorator: it gets\nthe decorated class as an argument, and should return a class to replace the decorated\nclass. Class decorators often return the decorated class itself, after injecting more\nmethods in it via attribute assignment.\nProbably the most common reason to choose a class decorator over the simpler\n__init_subclass__ is to avoid interfering with other class features, such as inheri‐\ntance and metaclasses.10\nIn this section, we’ll study checkeddeco.py, which provides the same service as check‐\nedlib.py, but using a class decorator. As usual, we’ll start by looking at a usage exam‐\nple, extracted from the doctests in checkeddeco.py (Example 24-7).\nExample 24-7. checkeddeco.py: creating a Movie class decorated with @checked\n    >>> @checked\n    ... class Movie:\n    ...     title: str\n    ...     year: int\n    ...     box_office: float\n    ...\n    >>> movie = Movie(title='The Godfather', year=1972, box_office=137)\n    >>> movie.title\n    'The Godfather'\n    >>> movie\n    Movie(title='The Godfather', year=1972, box_office=137.0)\nThe only difference between Example 24-7 and Example 24-3 is the way the Movie\nclass is declared: it is decorated with @checked instead of subclassing Checked. Other‐\nwise, the external behavior is the same, including the type validation and default\nvalue assignments shown after Example 24-3 in “Introducing __init_subclass__” on\npage 914.\nNow let’s look at the implementation of checkeddeco.py. The imports and Field class\nare the same as in checkedlib.py, listed in Example 24-4. There is no other class, only\nfunctions in checkeddeco.py.\nThe logic previously implemented in __init_subclass__ is now part of the checked\nfunction—the class decorator listed in Example 24-8.\n922 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 953,
      "chapter": null,
      "content": "Example 24-8. checkeddeco.py: the class decorator\ndef checked(cls: type) -> type:  \n    for name, constructor in _fields(cls).items():    \n        setattr(cls, name, Field(name, constructor))  \n    cls._fields = classmethod(_fields)  # type: ignore  \n    instance_methods = (  \n        __init__,\n        __repr__,\n        __setattr__,\n        _asdict,\n        __flag_unknown_attrs,\n    )\n    for method in instance_methods:  \n        setattr(cls, method.__name__, method)\n    return cls  \nRecall that classes are instances of type. These type hints strongly suggest this is a\nclass decorator: it takes a class and returns a class.\n_fields is a top-level function defined later in the module (in Example 24-9).\nReplacing each attribute returned by _fields with a Field descriptor instance is\nwhat __init_subclass__ did in Example 24-5. Here there is more work to do…\nBuild a class method from _fields, and add it to the decorated class. The type:\nignore comment is needed because Mypy complains that type has no _fields\nattribute.\nModule-level functions that will become instance methods of the decorated class.\nAdd each of the instance_methods to cls.\nReturn the decorated cls, fulfilling the essential contract of a class decorator.\nEnhancing Classes with a Class Decorator \n| \n923",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 954,
      "chapter": null,
      "content": "Every top-level function in checkeddeco.py is prefixed with an underscore, except the\nchecked decorator. This naming convention makes sense for a couple of reasons:\n• checked is part of the public interface of the checkeddeco.py module, but the\nother functions are not.\n• The functions in Example 24-9 will be injected in the decorated class, and the\nleading _ reduces the chance of naming conflicts with user-defined attributes and\nmethods of the decorated class.\nThe rest of checkeddeco.py is listed in Example 24-9. Those module-level functions\nhave the same code as the corresponding methods of the Checked class of checked‐\nlib.py. They were explained in Examples 24-5 and 24-6.\nNote that the _fields function does double duty in checkeddeco.py. It is used as a\nregular function in the first line of the checked decorator, and it will also be injected\nas a class method of the decorated class.\nExample 24-9. checkeddeco.py: the methods to be injected in the decorated class\ndef _fields(cls: type) -> dict[str, type]:\n    return get_type_hints(cls)\ndef __init__(self: Any, **kwargs: Any) -> None:\n    for name in self._fields():\n        value = kwargs.pop(name, ...)\n        setattr(self, name, value)\n    if kwargs:\n        self.__flag_unknown_attrs(*kwargs)\ndef __setattr__(self: Any, name: str, value: Any) -> None:\n    if name in self._fields():\n        cls = self.__class__\n        descriptor = getattr(cls, name)\n        descriptor.__set__(self, value)\n    else:\n        self.__flag_unknown_attrs(name)\ndef __flag_unknown_attrs(self: Any, *names: str) -> NoReturn:\n    plural = 's' if len(names) > 1 else ''\n    extra = ', '.join(f'{name!r}' for name in names)\n    cls_name = repr(self.__class__.__name__)\n    raise AttributeError(f'{cls_name} has no attribute{plural} {extra}')\ndef _asdict(self: Any) -> dict[str, Any]:\n    return {\n        name: getattr(self, name)\n        for name, attr in self.__class__.__dict__.items()\n        if isinstance(attr, Field)\n924 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 955,
      "chapter": null,
      "content": "11 Contrast with the import statement in Java, which is just a declaration to let the compiler know that certain\npackages are required.\n    }\ndef __repr__(self: Any) -> str:\n    kwargs = ', '.join(\n        f'{key}={value!r}' for key, value in self._asdict().items()\n    )\n    return f'{self.__class__.__name__}({kwargs})'\nThe checkeddeco.py module implements a simple but usable class decorator. Python’s\n@dataclass does a lot more. It supports many configuration options, adds more\nmethods to the decorated class, handles or warns about conflicts with user-defined\nmethods in the decorated class, and even traverses the __mro__ to collect user-\ndefined attributes declared in the superclasses of the decorated class. The source code\nof the dataclasses package in Python 3.9 is more than 1,200 lines long.\nFor metaprogramming classes, we must be aware of when the Python interpreter\nevaluates each block of code during the construction of a class. This is covered next.\nWhat Happens When: Import Time Versus Runtime\nPython programmers talk about “import time” versus “runtime,” but the terms are\nnot strictly defined and there is a gray area between them.\nAt import time, the interpreter:\n1. Parses the source code of a .py module in one pass from top to bottom. This is\nwhen a SyntaxError may occur.\n2. Compiles the bytecode to be executed.\n3. Executes the top-level code of the compiled module.\nIf there is an up-to-date .pyc file available in the local __pycache__, parsing and\ncompiling are skipped because the bytecode is ready to run.\nAlthough parsing and compiling are definitely “import time” activities, other things\nmay happen at that time, because almost every statement in Python is executable in\nthe sense that they can potentially run user code and may change the state of the user\nprogram.\nIn particular, the import statement is not merely a declaration,11 but it actually runs\nall the top-level code of a module when it is imported for the first time in the process.\nFurther imports of the same module will use a cache, and then the only effect will be\nbinding the imported objects to names in the client module. That top-level code may\nWhat Happens When: Import Time Versus Runtime \n| \n925",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 956,
      "chapter": null,
      "content": "12 I’m not saying opening a database connection just because a module is imported is a good idea, only pointing\nout it can be done.\ndo anything, including actions typical of “runtime,” such as writing to a log or con‐\nnecting to a database.12 That’s why the border between “import time” and “runtime”\nis fuzzy: the import statement can trigger all sorts of “runtime” behavior. Conversely,\n“import time” can also happen deep inside runtime, because the import statement\nand the __import__() built-in can be used inside any regular function.\nThis is all rather abstract and subtle, so let’s do some experiments to see what hap‐\npens when.\nEvaluation Time Experiments\nConsider an evaldemo.py script that uses a class decorator, a descriptor, and a class\nbuilder based on __init_subclass__, all defined in a builderlib.py module. The\nmodules have several print calls to show what happens under the covers. Otherwise,\nthey don’t perform anything useful. The goal of these experiments is to observe the\norder in which these print calls happen.\nApplying a class decorator and a class builder with __init_sub\nclass__ together in single class is likely a sign of overengineering\nor desperation. This unusual combination is useful in these experi‐\nments to show the timing of the changes that a class decorator and\n__init_subclass__ can apply to a class.\nLet’s start by checking out builderlib.py, split into two parts: Example 24-10 and\nExample 24-11.\nExample 24-10. builderlib.py: top of the module\nprint('@ builderlib module start')\nclass Builder:  \n    print('@ Builder body')\n    def __init_subclass__(cls):  \n        print(f'@ Builder.__init_subclass__({cls!r})')\n        def inner_0(self):  \n            print(f'@ SuperA.__init_subclass__:inner_0({self!r})')\n        cls.method_a = inner_0\n    def __init__(self):\n926 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 957,
      "chapter": null,
      "content": "super().__init__()\n        print(f'@ Builder.__init__({self!r})')\ndef deco(cls):  \n    print(f'@ deco({cls!r})')\n    def inner_1(self):  \n        print(f'@ deco:inner_1({self!r})')\n    cls.method_b = inner_1\n    return cls  \nThis is a class builder to implement…\n…an __init_subclass__ method.\nDefine a function to be added to the subclass in the assignment below.\nA class decorator.\nFunction to be added to the decorated class.\nReturn the class received as an argument.\nContinuing with builderlib.py in Example 24-11…\nExample 24-11. builderlib.py: bottom of the module\nclass Descriptor:  \n    print('@ Descriptor body')\n    def __init__(self):  \n        print(f'@ Descriptor.__init__({self!r})')\n    def __set_name__(self, owner, name):  \n        args = (self, owner, name)\n        print(f'@ Descriptor.__set_name__{args!r}')\n    def __set__(self, instance, value):  \n        args = (self, instance, value)\n        print(f'@ Descriptor.__set__{args!r}')\n    def __repr__(self):\n        return '<Descriptor instance>'\nprint('@ builderlib module end')\nWhat Happens When: Import Time Versus Runtime \n| \n927",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 958,
      "chapter": null,
      "content": "A descriptor class to demonstrate when…\n…a descriptor instance is created, and when…\n…__set_name__ will be invoked during the owner class construction.\nLike the other methods, this __set__ doesn’t do anything except display its argu‐\nments.\nIf you import builderlib.py in the Python console, this is what you get:\n>>> import builderlib\n@ builderlib module start\n@ Builder body\n@ Descriptor body\n@ builderlib module end\nNote that the lines printed by builderlib.py are prefixed with @.\nNow let’s turn to evaldemo.py, which will trigger special methods in builderlib.py\n(Example 24-12).\nExample 24-12. evaldemo.py: script to experiment with builderlib.py\n#!/usr/bin/env python3\nfrom builderlib import Builder, deco, Descriptor\nprint('# evaldemo module start')\n@deco  \nclass Klass(Builder):  \n    print('# Klass body')\n    attr = Descriptor()  \n    def __init__(self):\n        super().__init__()\n        print(f'# Klass.__init__({self!r})')\n    def __repr__(self):\n        return '<Klass instance>'\ndef main():  \n    obj = Klass()\n    obj.method_a()\n    obj.method_b()\n    obj.attr = 999\n928 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 959,
      "chapter": null,
      "content": "if __name__ == '__main__':\n    main()\nprint('# evaldemo module end')\nApply a decorator.\nSubclass Builder to trigger its __init_subclass__.\nInstantiate the descriptor.\nThis will only be called if the module is run as the main program.\nThe print calls in evaldemo.py show a # prefix. If you open the console again and\nimport evaldemo.py, Example 24-13 is the output.\nExample 24-13. Console experiment with evaldemo.py\n>>> import evaldemo\n@ builderlib module start  \n@ Builder body\n@ Descriptor body\n@ builderlib module end\n# evaldemo module start\n# Klass body  \n@ Descriptor.__init__(<Descriptor instance>)  \n@ Descriptor.__set_name__(<Descriptor instance>,\n      <class 'evaldemo.Klass'>, 'attr')                \n@ Builder.__init_subclass__(<class 'evaldemo.Klass'>)  \n@ deco(<class 'evaldemo.Klass'>)  \n# evaldemo module end\nThe top four lines are the result of from builderlib import… . They will not\nappear if you didn’t close the console after the previous experiment, because buil‐\nderlib.py is already loaded.\nThis signals that Python started reading the body of Klass. At this point, the\nclass object does not exist yet.\nThe descriptor instance is created and bound to attr in the namespace that\nPython will pass to the default class object constructor: type.__new__.\nAt this point, Python’s built-in type.__new__ has created the Klass object and\ncalls __set_name__ on each descriptor instance of descriptor classes that provide\nthat method, passing Klass as the owner argument.\nWhat Happens When: Import Time Versus Runtime \n| \n929",
      "content_length": 1538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 960,
      "chapter": null,
      "content": "type.__new__ then calls __init_subclass__ on the superclass of Klass, passing\nKlass as the single argument.\nWhen type.__new__ returns the class object, Python applies the decorator.\nIn this example, the class returned by deco is bound to Klass in the module\nnamespace.\nThe implementation of type.__new__ is written in C. The behavior I just described is\ndocumented in the “Creating the class object” section of Python’s “Data Model” ref‐\nerence.\nNote that the main() function of evaldemo.py (Example 24-12) was not executed in\nthe console session (Example 24-13), therefore no instance of Klass was created. All\nthe action we saw was triggered by “import time” operations: importing builderlib\nand defining Klass.\nIf you run evaldemo.py as a script, you will see the same output as Example 24-13\nwith extra lines right before the end. The extra lines are the result of running main()\n(Example 24-14).\nExample 24-14. Running evaldemo.py as a program\n$ ./evaldemo.py\n[... 9 lines omitted ...]\n@ deco(<class '__main__.Klass'>)  \n@ Builder.__init__(<Klass instance>)  \n# Klass.__init__(<Klass instance>)\n@ SuperA.__init_subclass__:inner_0(<Klass instance>)  \n@ deco:inner_1(<Klass instance>)  \n@ Descriptor.__set__(<Descriptor instance>, <Klass instance>, 999)  \n# evaldemo module end\nThe top 10 lines—including this one—are the same as shown in Example 24-13.\nTriggered by super().__init__() in Klass.__init__.\nTriggered \nby \nobj.method_a() \nin \nmain; \nmethod_a \nwas \ninjected \nby\nSuperA.__init_subclass__.\nTriggered by obj.method_b() in main; method_b was injected by deco.\nTriggered by obj.attr = 999 in main.\nA base class with __init_subclass__ and a class decorator are powerful tools, but\nthey are limited to working with a class already built by type.__new__ under the\n930 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 961,
      "chapter": null,
      "content": "13 Message to comp.lang.python, subject: “Acrimony in c.l.p.”. This is another part of the same message from\nDecember 23, 2002, quoted in the Preface. The TimBot was inspired that day.\ncovers. In the rare occasions when you need to adjust the arguments passed to\ntype.__new__, you need a metaclass. That’s the final destination of this chapter—and\nthis book.\nMetaclasses 101\n[Metaclasses] are deeper magic than 99% of users should ever worry about. If you\nwonder whether you need them, you don’t (the people who actually need them know\nwith certainty that they need them, and don’t need an explanation about why).\n—Tim Peters, inventor of the Timsort algorithm and prolific Python contributor13\nA metaclass is a class factory. In contrast with record_factory from Example 24-2, a\nmetaclass is written as a class. In other words, a metaclass is a class whose instances\nare classes. Figure 24-1 depicts a metaclass using the Mills & Gizmos Notation: a mill\nproducing another mill.\nFigure 24-1. A metaclass is a class that builds classes.\nConsider the Python object model: classes are objects, therefore each class must be an\ninstance of some other class. By default, Python classes are instances of type. In other\nwords, type is the metaclass for most built-in and user-defined classes:\n>>> str.__class__\n<class 'type'>\n>>> from bulkfood_v5 import LineItem\n>>> LineItem.__class__\n<class 'type'>\nMetaclasses 101 \n| \n931",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 962,
      "chapter": null,
      "content": ">>> type.__class__\n<class 'type'>\nTo avoid infinite regress, the class of type is type, as the last line shows.\nNote that I am not saying that str or LineItem are subclasses of type. What I am\nsaying is that str and LineItem are instances of type. They all are subclasses of\nobject. Figure 24-2 may help you confront this strange reality.\nFigure 24-2. Both diagrams are true. The left one emphasizes that str, type, and LineI\ntem are subclasses of object. The right one makes it clear that str, object, and LineI\ntem are instances type, because they are all classes.\nThe classes object and type have a unique relationship: object is\nan instance of type, and type is a subclass of object. This relation‐\nship is “magic”: it cannot be expressed in Python because either\nclass would have to exist before the other could be defined. The fact\nthat type is an instance of itself is also magical.\nThe next snippet shows that the class of collections.Iterable is abc.ABCMeta.\nNote that Iterable is an abstract class, but ABCMeta is a concrete class—after all,\nIterable is an instance of ABCMeta:\n>>> from collections.abc import Iterable\n>>> Iterable.__class__\n<class 'abc.ABCMeta'>\n>>> import abc\n>>> from abc import ABCMeta\n>>> ABCMeta.__class__\n<class 'type'>\nUltimately, the class of ABCMeta is also type. Every class is an instance of type,\ndirectly or indirectly, but only metaclasses are also subclasses of type. That’s the most\nimportant relationship to understand metaclasses: a metaclass, such as ABCMeta,\n932 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1548,
      "extraction_method": "Direct"
    },
    {
      "page_number": 963,
      "chapter": null,
      "content": "inherits from type the power to construct classes. Figure 24-3 illustrates this crucial\nrelationship.\nFigure 24-3. Iterable is a subclass of object and an instance of ABCMeta. Both object\nand ABCMeta are instances of type, but the key relationship here is that ABCMeta is also\na subclass of type, because ABCMeta is a metaclass. In this diagram, Iterable is the\nonly abstract class.\nThe important takeaway here is that metaclasses are subclasses of type, and that’s\nwhat makes them work as class factories. A metaclass can customize its instances by\nimplementing special methods, as the next sections demonstrate.\nHow a Metaclass Customizes a Class\nTo use a metaclass, it’s critical to understand how __new__ works on any class. This\nwas discussed in “Flexible Object Creation with __new__” on page 843.\nThe same mechanics happen at a “meta” level when a metaclass is about to create a\nnew instance, which is a class. Consider this declaration:\nclass Klass(SuperKlass, metaclass=MetaKlass):\n    x = 42\n    def __init__(self, y):\n        self.y = y\nTo process that class statement, Python calls MetaKlass.__new__ with these\narguments:\nmeta_cls\nThe metaclass itself (MetaKlass), because __new__ works as class method.\nMetaclasses 101 \n| \n933",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 964,
      "chapter": null,
      "content": "cls_name\nThe string Klass.\nbases\nThe single-element tuple (SuperKlass,), with more elements in the case of mul‐\ntiple inheritance.\ncls_dict\nA mapping like:\n{x: 42, `__init__`: <function __init__ at 0x1009c4040>}\nWhen you implement MetaKlass.__new__, you can inspect and change those argu‐\nments before passing them to super().__new__, which will eventually call\ntype.__new__ to create the new class object.\nAfter super().__new__ returns, you can also apply further processing to the newly\ncreated class before returning it to Python. Python then calls Super\nKlass.__init_subclass__, passing the class you created, and then applies a class\ndecorator to it, if one is present. Finally, Python binds the class object to its name in\nthe surrounding namespace—usually the global namespace of a module, if the class\nstatement was a top-level statement.\nThe most common processing made in a metaclass __new__ is to add or replace items\nin the cls_dict—the mapping that represents the namespace of the class under con‐\nstruction. For instance, before calling super().__new__, you can inject methods in\nthe class under construction by adding functions to cls_dict. However, note that\nadding methods can also be done after the class is built, which is why we were able to\ndo it using __init_subclass__ or a class decorator.\nOne attribute that you must add to the cls_dict before type.__new__ runs is\n__slots__, as discussed in “Why __init_subclass__ Cannot Configure __slots__” on\npage 921. The __new__ method of a metaclass is the ideal place to configure\n__slots__. The next section shows how to do that.\nA Nice Metaclass Example\nThe MetaBunch metaclass presented here is a variation of the last example in Chapter\n4 of Python in a Nutshell, 3rd ed., by Alex Martelli, Anna Ravenscroft, and Steve\n934 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 965,
      "chapter": null,
      "content": "14 The authors kindly gave me permission to use their example. MetaBunch first appeared in a message posted by\nMartelli in the comp.lang.python group on July 7, 2002, with the subject line “a nice metaclass example (was\nRe: structs in python)”, following a discussion about record-like data structures in Python. Martelli’s original\ncode for Python 2.2 still runs after a single change: to use a metaclass in Python 3, you must use the metaclass\nkeyword argument in the class declaration, e.g., Bunch(metaclass=MetaBunch), instead of the older conven‐\ntion of adding a __metaclass__ class-level attribute.\nHolden, written to run on Python 2.7 and 3.5.14 Assuming Python 3.6 or later, I was\nable to further simplify the code.\nFirst, let’s see what the Bunch base class provides:\n    >>> class Point(Bunch):\n    ...     x = 0.0\n    ...     y = 0.0\n    ...     color = 'gray'\n    ...\n    >>> Point(x=1.2, y=3, color='green')\n    Point(x=1.2, y=3, color='green')\n    >>> p = Point()\n    >>> p.x, p.y, p.color\n    (0.0, 0.0, 'gray')\n    >>> p\n    Point()\nRemember that Checked assigns names to the Field descriptors in subclasses based\non class variable type hints, which do not actually become attributes on the class\nsince they don’t have values.\nBunch subclasses, on the other hand, use actual class attributes with values, which\nthen become the default values of the instance attributes. The generated __repr__\nomits the arguments for attributes that are equal to the defaults.\nMetaBunch—the metaclass of Bunch—generates __slots__ for the new class from the\nclass attributes declared in the user’s class. This blocks the instantiation and later\nassignment of undeclared attributes:\n    >>> Point(x=1, y=2, z=3)\n    Traceback (most recent call last):\n      ...\n    AttributeError: No slots left for: 'z'\n    >>> p = Point(x=21)\n    >>> p.y = 42\n    >>> p\n    Point(x=21, y=42)\n    >>> p.flavor = 'banana'\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'Point' object has no attribute 'flavor'\nMetaclasses 101 \n| \n935",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 966,
      "chapter": null,
      "content": "Now let’s dive into the elegant code of MetaBunch in Example 24-15.\nExample 24-15. metabunch/from3.6/bunch.py: MetaBunch metaclass and Bunch class\nclass MetaBunch(type):  \n    def __new__(meta_cls, cls_name, bases, cls_dict):  \n        defaults = {}  \n        def __init__(self, **kwargs):  \n            for name, default in defaults.items():  \n                setattr(self, name, kwargs.pop(name, default))\n            if kwargs:  \n                extra = ', '.join(kwargs)\n                raise AttributeError(f'No slots left for: {extra!r}')\n        def __repr__(self):  \n            rep = ', '.join(f'{name}={value!r}'\n                            for name, default in defaults.items()\n                            if (value := getattr(self, name)) != default)\n            return f'{cls_name}({rep})'\n        new_dict = dict(__slots__=[], __init__=__init__, __repr__=__repr__)  \n        for name, value in cls_dict.items():  \n            if name.startswith('__') and name.endswith('__'):  \n                if name in new_dict:\n                    raise AttributeError(f\"Can't set {name!r} in {cls_name!r}\")\n                new_dict[name] = value\n            else:  \n                new_dict['__slots__'].append(name)\n                defaults[name] = value\n        return super().__new__(meta_cls, cls_name, bases, new_dict)  \nclass Bunch(metaclass=MetaBunch):  \n    pass\nTo create a new metaclass, inherit from type.\n__new__ works as a class method, but the class is a metaclass, so I like to name\nthe first argument meta_cls (mcs is a common alternative). The remaining three\narguments are the same as the three-argument signature for calling type()\ndirectly to create a class.\ndefaults will hold a mapping of attribute names and their default values.\nThis will be injected into the new class.\n936 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 967,
      "chapter": null,
      "content": "Read the defaults and set the corresponding instance attribute with a value pop‐\nped from kwargs or a default.\nIf there is still any item in kwargs, it means there are no slots left where we can\nplace them. We believe in failing fast as best practice, so we don’t want to silently\nignore extra items. A quick and effective solution is to pop one item from kwargs\nand try to set it on the instance, triggering an AttributeError on purpose.\n__repr__ returns a string that looks like a constructor call—e.g., Point(x=3),\nomitting the keyword arguments with default values.\nInitialize namespace for the new class.\nIterate over the namespace of the user’s class.\nIf a dunder name is found, copy the item to the new class namespace, unless it’s\nalready there. This prevents users from overwriting __init__, __repr__, and\nother attributes set by Python, such as __qualname__ and __module__.\nIf not a dunder name, append to __slots__ and save its value in defaults.\nBuild and return the new class.\nProvide a base class, so users don’t need to see MetaBunch.\nMetaBunch works because it is able to configure __slots__ before calling\nsuper().__new__ to build the final class. As usual when metaprogramming, under‐\nstanding the sequence of actions is key. Let’s do another evaluation time experiment,\nnow with a metaclass.\nMetaclass Evaluation Time Experiment\nThis is a variation of “Evaluation Time Experiments” on page 926, adding a metaclass\nto the mix. The builderlib.py module is the same as before, but the main script is now\nevaldemo_meta.py, listed in Example 24-16.\nExample 24-16. evaldemo_meta.py: experimenting with a metaclass\n#!/usr/bin/env python3\nfrom builderlib import Builder, deco, Descriptor\nfrom metalib import MetaKlass  \nprint('# evaldemo_meta module start')\nMetaclasses 101 \n| \n937",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 968,
      "chapter": null,
      "content": "@deco\nclass Klass(Builder, metaclass=MetaKlass):  \n    print('# Klass body')\n    attr = Descriptor()\n    def __init__(self):\n        super().__init__()\n        print(f'# Klass.__init__({self!r})')\n    def __repr__(self):\n        return '<Klass instance>'\ndef main():\n    obj = Klass()\n    obj.method_a()\n    obj.method_b()\n    obj.method_c()  \n    obj.attr = 999\nif __name__ == '__main__':\n    main()\nprint('# evaldemo_meta module end')\nImport MetaKlass from metalib.py, which we’ll see in Example 24-18.\nDeclare Klass as a subclass of Builder and an instance of MetaKlass.\nThis method is injected by MetaKlass.__new__, as we’ll see.\nIn the interest of science, Example 24-16 defies all reason and\napplies three different metaprogramming techniques together on\nKlass: a decorator, a base class using __init_subclass__, and a\ncustom metaclass. If you do this in production code, please don’t\nblame me. Again, the goal is to observe the order in which the\nthree techniques interfere in the class construction process.\nAs in the previous evaluation time experiment, this example does nothing but print\nmessages revealing the flow of execution. Example 24-17 shows the code for the top\npart of metalib.py—the rest is in Example 24-18.\nExample 24-17. metalib.py: the NosyDict class\nprint('% metalib module start')\n938 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 969,
      "chapter": null,
      "content": "import collections\nclass NosyDict(collections.UserDict):\n    def __setitem__(self, key, value):\n        args = (self, key, value)\n        print(f'% NosyDict.__setitem__{args!r}')\n        super().__setitem__(key, value)\n    def __repr__(self):\n        return '<NosyDict instance>'\nI wrote the NosyDict class to override __setitem__ to display each key and value as\nthey are set. The metaclass will use a NosyDict instance to hold the namespace of the\nclass under construction, revealing more of Python’s inner workings.\nThe main attraction of metalib.py is the metaclass in Example 24-18. It implements\nthe __prepare__ special method, a class method that Python only invokes on meta‐\nclasses. The __prepare__ method provides the earliest opportunity to influence the\nprocess of creating a new class.\nWhen coding a metaclass, I find it useful to adopt this naming con‐\nvention for special method arguments:\n• Use cls instead of self for instance methods, because the\ninstance is a class.\n• Use meta_cls instead of cls for class methods, because the\nclass is a metaclass. Recall that __new__ behaves as a class\nmethod even without the @classmethod decorator.\nExample 24-18. metalib.py: the MetaKlass\nclass MetaKlass(type):\n    print('% MetaKlass body')\n    @classmethod  \n    def __prepare__(meta_cls, cls_name, bases):  \n        args = (meta_cls, cls_name, bases)\n        print(f'% MetaKlass.__prepare__{args!r}')\n        return NosyDict()  \n    def __new__(meta_cls, cls_name, bases, cls_dict):  \n        args = (meta_cls, cls_name, bases, cls_dict)\n        print(f'% MetaKlass.__new__{args!r}')\n        def inner_2(self):\n            print(f'% MetaKlass.__new__:inner_2({self!r})')\n        cls = super().__new__(meta_cls, cls_name, bases, cls_dict.data)  \nMetaclasses 101 \n| \n939",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 970,
      "chapter": null,
      "content": "cls.method_c = inner_2  \n        return cls  \n    def __repr__(cls):  \n        cls_name = cls.__name__\n        return f\"<class {cls_name!r} built by MetaKlass>\"\nprint('% metalib module end')\n__prepare__ should be declared as a class method. It is not an instance method\nbecause the class under construction does not exist yet when Python calls\n__prepare__.\nPython calls __prepare__ on a metaclass to obtain a mapping to hold the name‐\nspace of the class under construction.\nReturn NosyDict instance to be used as the namespace.\ncls_dict is a NosyDict instance returned by __prepare__.\ntype.__new__ requires a real dict as the last argument, so I give it the data\nattribute of NosyDict, inherited from UserDict.\nInject a method in the newly created class.\nAs usual, __new__ must return the object just created—in this case, the new class.\nDefining __repr__ on a metaclass allows customizing the repr() of class objects.\nThe main use case for __prepare__ before Python 3.6 was to provide an OrderedDict\nto hold the attributes of the class under construction, so that the metaclass __new__\ncould process those attributes in the order in which they appear in the source code of\nthe user’s class definition. Now that dict preserves the insertion order, __prepare__\nis rarely needed. You will see a creative use for it in “A Metaclass Hack with __pre‐\npare__” on page 950.\nImporting metalib.py in the Python console is not very exciting. Note the use of % to\nprefix the lines output by this module:\n>>> import metalib\n% metalib module start\n% MetaKlass body\n% metalib module end\nLots of things happen if you import evaldemo_meta.py, as you can see in\nExample 24-19.\n940 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 971,
      "chapter": null,
      "content": "Example 24-19. Console experiment with evaldemo_meta.py\n>>> import evaldemo_meta\n@ builderlib module start\n@ Builder body\n@ Descriptor body\n@ builderlib module end\n% metalib module start\n% MetaKlass body\n% metalib module end\n# evaldemo_meta module start  \n% MetaKlass.__prepare__(<class 'metalib.MetaKlass'>, 'Klass',  \n                        (<class 'builderlib.Builder'>,))\n% NosyDict.__setitem__(<NosyDict instance>, '__module__', 'evaldemo_meta')  \n% NosyDict.__setitem__(<NosyDict instance>, '__qualname__', 'Klass')\n# Klass body\n@ Descriptor.__init__(<Descriptor instance>)  \n% NosyDict.__setitem__(<NosyDict instance>, 'attr', <Descriptor instance>)  \n% NosyDict.__setitem__(<NosyDict instance>, '__init__',\n                       <function Klass.__init__ at …>)  \n% NosyDict.__setitem__(<NosyDict instance>, '__repr__',\n                       <function Klass.__repr__ at …>)\n% NosyDict.__setitem__(<NosyDict instance>, '__classcell__', <cell at …: empty>)\n% MetaKlass.__new__(<class 'metalib.MetaKlass'>, 'Klass',\n                    (<class 'builderlib.Builder'>,), <NosyDict instance>)  \n@ Descriptor.__set_name__(<Descriptor instance>,\n                          <class 'Klass' built by MetaKlass>, 'attr')  \n@ Builder.__init_subclass__(<class 'Klass' built by MetaKlass>)\n@ deco(<class 'Klass' built by MetaKlass>)\n# evaldemo_meta module end\nThe lines before this are the result of importing builderlib.py and metalib.py.\nPython invokes __prepare__ to start processing a class statement.\nBefore parsing the class body, Python adds the __module__ and __qualname__\nentries to the namespace of the class under construction.\nThe descriptor instance is created…\n…and bound to attr in the class namespace.\n__init__ and __repr__ methods are defined and added to the namespace.\nOnce Python finishes processing the class body, it calls MetaKlass.__new__.\n__set_name__, __init_subclass__, and the decorator are invoked in this order,\nafter the __new__ method of the metaclass returns the newly constructed class.\nMetaclasses 101 \n| \n941",
      "content_length": 2038,
      "extraction_method": "Direct"
    },
    {
      "page_number": 972,
      "chapter": null,
      "content": "If you run evaldemo_meta.py as script, main() is called, and a few more things\nhappen (Example 24-20).\nExample 24-20. Running evaldemo_meta.py as a program\n$ ./evaldemo_meta.py\n[... 20 lines omitted ...]\n@ deco(<class 'Klass' built by MetaKlass>)  \n@ Builder.__init__(<Klass instance>)\n# Klass.__init__(<Klass instance>)\n@ SuperA.__init_subclass__:inner_0(<Klass instance>)\n@ deco:inner_1(<Klass instance>)\n% MetaKlass.__new__:inner_2(<Klass instance>)  \n@ Descriptor.__set__(<Descriptor instance>, <Klass instance>, 999)\n# evaldemo_meta module end\nThe top 21 lines—including this one—are the same shown in Example 24-19.\nTriggered by obj.method_c() in main; method_c was injected by Meta\nKlass.__new__.\nLet’s now go back to the idea of the Checked class with the Field descriptors imple‐\nmenting runtime type validation, and see how it can be done with a metaclass.\nA Metaclass Solution for Checked\nI don’t want to encourage premature optimization and overengineering, so here is a\nmake-believe scenario to justify rewriting checkedlib.py with __slots__, which\nrequires the application of a metaclass. Feel free to skip it.\nA Bit of Storytelling\nOur checkedlib.py using __init_subclass__ is a company-wide success, and our pro‐\nduction servers have millions of instances of Checked subclasses in memory at any\none time.\nProfiling a proof-of-concept, we discover that using __slots__ will reduce the cloud\nhosting bill for two reasons:\n• Lower memory usage, as Checked instances don’t need their own __dict__\n• Higher performance, by removing __setattr__, which was created just to block\nunexpected attributes, but is triggered at instantiation and for all attribute setting\nbefore Field.__set__ is called to do its job\n942 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 973,
      "chapter": null,
      "content": "The metaclass/checkedlib.py module we’ll study next is a drop-in replacement for init‐\nsub/checkedlib.py. The doctests embedded in them are identical, as well as the check‐\nedlib_test.py files for pytest.\nThe complexity in checkedlib.py is abstracted away from the user. Here is the source\ncode of a script using the package:\nfrom checkedlib import Checked\nclass Movie(Checked):\n    title: str\n    year: int\n    box_office: float\nif __name__ == '__main__':\n    movie = Movie(title='The Godfather', year=1972, box_office=137)\n    print(movie)\n    print(movie.title)\nThat concise Movie class definition leverages three instances of the Field validating\ndescriptor, a __slots__ configuration, five methods inherited from Checked, and a\nmetaclass to put it all together. The only visible part of checkedlib is the Checked\nbase class.\nConsider Figure 24-4. The Mills & Gizmos Notation complements the UML class\ndiagram by making the relationship between classes and instances more visible.\nFor example, a Movie class using the new checkedlib.py is an instance of CheckedMeta,\nand a subclass of Checked. Also, the title, year, and box_office class attributes of\nMovie are three separate instances of Field. Each Movie instance has its own _title,\n_year, and _box_office attributes, to store the values of the corresponding fields.\nNow let’s study the code, starting with the Field class, shown in Example 24-21.\nThe Field descriptor class is now a bit different. In the previous examples, each\nField descriptor instance stored its value in the managed instance using an attribute\nof the same name. For example, in the Movie class, the title descriptor stored the\nfield value in a title attribute in the managed instance. This made it unnecessary for\nField to provide a __get__ method.\nHowever, when a class like Movie uses __slots__, it cannot have class attributes and\ninstance attributes with the same name. Each descriptor instance is a class attribute,\nand now we need separate per-instance storage attributes. The code uses the descrip‐\ntor name prefixed with a single _. Therefore Field instances have separate name and\nstorage_name attributes, and we implement Field.__get__.\nA Metaclass Solution for Checked \n| \n943",
      "content_length": 2217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 974,
      "chapter": null,
      "content": "Figure 24-4. UML class diagram annotated with MGN: the CheckedMeta meta-mill\nbuilds the Movie mill. The Field mill builds the title, year, and box_office descrip‐\ntors, which are class attributes of Movie. The per-instance data for the fields is stored in\nthe _title, _year, and _box_office instance attributes of Movie. Note the package\nboundary of checkedlib. The developer of Movie doesn’t need to grok all the machi‐\nnery inside checkedlib.py.\nExample 24-21 shows the source code for Field, with callouts describing only the\nchanges in this version.\nExample 24-21. metaclass/checkedlib.py: the Field descriptor with storage_name and\n__get__\nclass Field:\n    def __init__(self, name: str, constructor: Callable) -> None:\n        if not callable(constructor) or constructor is type(None):\n            raise TypeError(f'{name!r} type hint must be callable')\n        self.name = name\n        self.storage_name = '_' + name  \n        self.constructor = constructor\n    def __get__(self, instance, owner=None):\n        if instance is None:  \n            return self\n        return getattr(instance, self.storage_name)  \n    def __set__(self, instance: Any, value: Any) -> None:\n        if value is ...:\n            value = self.constructor()\n        else:\n944 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 975,
      "chapter": null,
      "content": "try:\n                value = self.constructor(value)\n            except (TypeError, ValueError) as e:\n                type_name = self.constructor.__name__\n                msg = f'{value!r} is not compatible with {self.name}:{type_name}'\n                raise TypeError(msg) from e\n        setattr(instance, self.storage_name, value)  \nCompute storage_name from the name argument.\nIf __get__ gets None as the instance argument, the descriptor is being read from\nthe managed class itself, not a managed instance. So we return the descriptor.\nOtherwise, return the value stored in the attribute named storage_name.\n__set__ now uses setattr to set or update the managed attribute.\nExample 24-22 shows the code for the metaclass that drives this example.\nExample 24-22. metaclass/checkedlib.py: the CheckedMeta metaclass\nclass CheckedMeta(type):\n    def __new__(meta_cls, cls_name, bases, cls_dict):  \n        if '__slots__' not in cls_dict:  \n            slots = []\n            type_hints = cls_dict.get('__annotations__', {})  \n            for name, constructor in type_hints.items():   \n                field = Field(name, constructor)  \n                cls_dict[name] = field  \n                slots.append(field.storage_name)  \n            cls_dict['__slots__'] = slots  \n        return super().__new__(\n                meta_cls, cls_name, bases, cls_dict)  \n__new__ is the only method implemented in CheckedMeta.\nOnly enhance the class if its cls_dict doesn’t include __slots__. If __slots__ is\nalready present, assume it is the Checked base class and not a user-defined sub‐\nclass, and build the class as is.\nTo get the type hints in prior examples, we used typing.get_type_hints, but\nthat requires an existing class as the first argument. At this point, the class we are\nconfiguring does not exist yet, so we need to retrieve the __annotations__\nA Metaclass Solution for Checked \n| \n945",
      "content_length": 1890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 976,
      "chapter": null,
      "content": "directly from the cls_dict—the namespace of the class under construction,\nwhich Python passes as the last argument to the metaclass __new__.\nIterate over type_hints to…\n…build a Field for each annotated attribute…\n…overwrite the corresponding entry in cls_dict with the Field instance…\n…and append the storage_name of the field in the list we’ll use to…\n…populate the __slots__ entry in cls_dict—the namespace of the class under\nconstruction.\nFinally, we call super().__new__.\nThe last part of metaclass/checkedlib.py is the Checked base class that users of this\nlibrary will subclass to enhance their classes, like Movie.\nThe code for this version of Checked is the same as Checked in initsub/checkedlib.py\n(listed in Example 24-5 and Example 24-6), with three changes:\n1. Added an empty __slots__ to signal to CheckedMeta.__new__ that this class\ndoesn’t require special processing.\n2. Removed __init_subclass__. Its job is now done by CheckedMeta.__new__.\n3. Removed __setattr__. It became redundant because adding __slots__ to the\nuser-defined class prevents setting undeclared attributes.\nExample 24-23 is a complete listing of the final version of Checked.\nExample 24-23. metaclass/checkedlib.py: the Checked base class\nclass Checked(metaclass=CheckedMeta):\n    __slots__ = ()  # skip CheckedMeta.__new__ processing\n    @classmethod\n    def _fields(cls) -> dict[str, type]:\n        return get_type_hints(cls)\n    def __init__(self, **kwargs: Any) -> None:\n        for name in self._fields():\n            value = kwargs.pop(name, ...)\n            setattr(self, name, value)\n        if kwargs:\n            self.__flag_unknown_attrs(*kwargs)\n946 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 977,
      "chapter": null,
      "content": "15 In the first edition of Fluent Python, the more advanced versions of the LineItem class used a metaclass just to\nset the storage name of the attributes. See the code in the metaclasses of bulkfood in the first edition code\nrepository.\n    def __flag_unknown_attrs(self, *names: str) -> NoReturn:\n        plural = 's' if len(names) > 1 else ''\n        extra = ', '.join(f'{name!r}' for name in names)\n        cls_name = repr(self.__class__.__name__)\n        raise AttributeError(f'{cls_name} object has no attribute{plural} {extra}')\n    def _asdict(self) -> dict[str, Any]:\n        return {\n            name: getattr(self, name)\n            for name, attr in self.__class__.__dict__.items()\n            if isinstance(attr, Field)\n        }\n    def __repr__(self) -> str:\n        kwargs = ', '.join(\n            f'{key}={value!r}' for key, value in self._asdict().items()\n        )\n        return f'{self.__class__.__name__}({kwargs})'\nThis concludes the third rendering of a class builder with validated descriptors.\nThe next section covers some general issues related to metaclasses.\nMetaclasses in the Real World\nMetaclasses are powerful, but tricky. Before deciding to implement a metaclass, con‐\nsider the following points.\nModern Features Simplify or Replace Metaclasses\nOver time, several common use cases of metaclasses were made redundant by new\nlanguage features:\nClass decorators\nSimpler to understand than metaclasses, and less likely to cause conflicts with\nbase classes and metaclasses.\n__set_name__\nAvoids the need for custom metaclass logic to automatically set the name of a\ndescriptor.15\nMetaclasses in the Real World \n| \n947",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 978,
      "chapter": null,
      "content": "__init_subclass__\nProvides a way to customize class creation that is transparent to the end user and\neven simpler than a decorator—but may introduce conflicts in a complex class\nhierarchy.\nBuilt-in dict preserving key insertion order\nEliminated the #1 reason to use __prepare__: to provide an OrderedDict to store\nthe namespace of the class under construction. Python only calls __prepare__ on\nmetaclasses, so if you needed to process the class namespace in the order it\nappears in the source code, you had to use a metaclass before Python 3.6.\nAs of 2021, every actively maintained version of CPython supports all the features\njust listed.\nI keep advocating these features because I see too much unnecessary complexity in\nour profession, and metaclasses are a gateway to complexity.\nMetaclasses Are Stable Language Features\nMetaclasses were introduced in Python 2.2 in 2002, together with so-called “new-\nstyle classes,” descriptors, and properties.\nIt is remarkable that the MetaBunch example, first posted by Alex Martelli in July\n2002, still works in Python 3.9—the only change being the way to specify the meta‐\nclass to use, which in Python 3 is done with the syntax class Bunch(metaclass=Meta\nBunch):.\nNone of the additions I mentioned in “Modern Features Simplify or Replace Meta‐\nclasses” on page 947 broke existing code using metaclasses. But legacy code using\nmetaclasses can often be simplified by leveraging those features, especially if you can\ndrop support to Python versions before 3.6—which are no longer maintained.\nA Class Can Only Have One Metaclass\nIf your class declaration involves two or more metaclasses, you will see this puzzling\nerror message:\nTypeError: metaclass conflict: the metaclass of a derived class\nmust be a (non-strict) subclass of the metaclasses of all its bases\nThis may happen even without multiple inheritance. For example, a declaration like\nthis could trigger that TypeError:\nclass Record(abc.ABC, metaclass=PersistentMeta):\n    pass\nWe saw that abc.ABC is an instance of the abc.ABCMeta metaclass. If that Persistent\nmetaclass is not itself a subclass of abc.ABCMeta, you get a metaclass conflict.\n948 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 979,
      "chapter": null,
      "content": "16 If you just got dizzy considering the implications of multiple inheritance with metaclasses, good for you. I’d\nstay way from this solution as well.\n17 I made a living writing Django code for a few years before I decided to study how Django’s model fields were\nimplemented. Only then I learned about descriptors and metaclasses.\nThere are two ways of dealing with that error:\n• Find some other way of doing what you need to do, while avoiding at least one of\nthe metaclasses involved.\n• Write your own PersistentABCMeta metaclass as a subclass of both abc.ABCMeta\nand PersistentMeta, using multiple inheritance, and use that as the only meta‐\nclass for Record.16\nI can imagine the solution of the metaclass with two base meta‐\nclasses implemented to meet a deadline. In my experience, meta‐\nclass programming always takes longer than anticipated, which\nmakes this approach risky before a hard deadline. If you do it and\nmake the deadline, the code may contain subtle bugs. Even in the\nabsence of known bugs, you should consider this approach as tech‐\nnical debt simply because it is hard to understand and maintain.\nMetaclasses Should Be Implementation Details\nBesides type, there are only six metaclasses in the entire Python 3.9 standard library.\nThe better known metaclasses are probably abc.ABCMeta, typing.NamedTupleMeta,\nand enum.EnumMeta. None of them are intended to appear explicitly in user code. We\nmay consider them implementation details.\nAlthough you can do some really wacky metaprogramming with metaclasses, it’s best\nto heed the principle of least astonishment so that most users can indeed regard met‐\naclasses as implementation details.17\nIn recent years, some metaclasses in the Python standard library were replaced by\nother mechanisms, without breaking the public API of their packages. The simplest\nway to future-proof such APIs is to offer a regular class that users subclass to access\nthe functionality provided by the metaclass, as we’ve done in our examples.\nTo wrap up our coverage of class metaprogramming, I will share with you the cool‐\nest, small example of metaclass I found as I researched this chapter.\nMetaclasses in the Real World \n| \n949",
      "content_length": 2177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 980,
      "chapter": null,
      "content": "A Metaclass Hack with __prepare__\nWhen I updated this chapter for the second edition, I needed to find simple but illu‐\nminating examples to replace the bulkfood LineItem code that no longer require met‐\naclasses since Python 3.6.\nThe simplest and most interesting metaclass idea was given to me by João S. O.\nBueno—better known as JS in the Brazilian Python community. One application of\nhis idea is to create a class that autogenerates numeric constants:\n    >>> class Flavor(AutoConst):\n    ...     banana\n    ...     coconut\n    ...     vanilla\n    ...\n    >>> Flavor.vanilla\n    2\n    >>> Flavor.banana, Flavor.coconut\n    (0, 1)\nYes, that code works as shown! That’s actually a doctest in autoconst_demo.py.\nHere is the user-friendly AutoConst base class and the metaclass behind it, imple‐\nmented in autoconst.py:\nclass AutoConstMeta(type):\n    def __prepare__(name, bases, **kwargs):\n        return WilyDict()\nclass AutoConst(metaclass=AutoConstMeta):\n    pass\nThat’s it.\nClearly the trick is in WilyDict.\nWhen Python processes the namespace of the user’s class and reads banana, it looks\nup that name in the mapping provided by __prepare__: an instance of WilyDict.\nWilyDict implements __missing__, covered in “The __missing__ Method” on page\n91. The WilyDict instance initially has no 'banana' key, so the __missing__ method\nis triggered. It makes an item on the fly with the key 'banana' and the value 0,\nreturning that value. Python is happy with that, then tries to retrieve 'coconut'. Wily\nDict promptly adds that entry with the value 1, returning it. The same happens with\n'vanilla', which is then mapped to 2.\n950 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 981,
      "chapter": null,
      "content": "We’ve seen __prepare__ and __missing__ before. The real innovation is how JS put\nthem together.\nHere is the source code for WilyDict, also from autoconst.py:\nclass WilyDict(dict):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__next_value = 0\n    def __missing__(self, key):\n        if key.startswith('__') and key.endswith('__'):\n            raise KeyError(key)\n        self[key] = value = self.__next_value\n        self.__next_value += 1\n        return value\nWhile experimenting, I found that Python looked up __name__ in the namespace of\nthe class under construction, causing WilyDict to add a __name__ entry, and incre‐\nment __next_value. So I added that if statement in __missing__ to raise KeyError\nfor keys that look like dunder attributes.\nThe autoconst.py package both requires and illustrates mastery of Python’s dynamic\nclass building machinery.\nI had a great time adding more functionality to AutoConstMeta and AutoConst, but\ninstead of sharing my experiments, I will let you have fun playing with JS’s ingenious\nhack.\nHere are some ideas:\n• Make it possible to retrieve the constant name if you have the value. For example,\nFlavor[2] could return 'vanilla'. You can to this by implementing __geti\ntem__ in AutoConstMeta. Since Python 3.9, you can implement __class_geti\ntem__ in AutoConst itself.\n• Support iteration over the class, by implementing __iter__ on the metaclass. I\nwould make the __iter__ yield the constants as (name, value) pairs.\n• Implement a new Enum variant. This would be a major undertaking, because the\nenum package is full of tricks, including the EnumMeta metaclass with hundreds of\nlines of code and a nontrivial __prepare__ method.\nEnjoy!\nA Metaclass Hack with __prepare__ \n| \n951",
      "content_length": 1769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 982,
      "chapter": null,
      "content": "The __class_getitem__ special method was added in Python 3.9\nto support generic types, as part of PEP 585—Type Hinting Gener‐\nics In Standard Collections. Thanks to __class_getitem__,\nPython’s core developers did not have to write a new metaclass for\nthe built-in types to implement __getitem__ so that we could write\ngeneric type hints like list[int]. This is a narrow feature, but rep‐\nresentative of a wider use case for metaclasses: implementing oper‐\nators and other special methods to work at the class level, such as\nmaking the class itself iterable, just like Enum subclasses.\nWrapping Up\nMetaclasses, as well as class decorators and __init_subclass__ are useful for:\n• Subclass registration\n• Subclass structural validation\n• Applying decorators to many methods at once\n• Object serialization\n• Object-relational mapping\n• Object-based persistence\n• Implementing special methods at the class level\n• Implementing class features found in other languages, such as traits and aspect-\noriented programming\nClass metaprogramming can also help with performance issues in some cases, by per‐\nforming tasks at import time that otherwise would execute repeatedly at runtime.\nTo wrap up, let’s recall Alex Martelli’s final advice from his essay “Waterfowl and\nABCs” on page 443:\nAnd, don’t define custom ABCs (or metaclasses) in production code… if you feel the\nurge to do so, I’d bet it’s likely to be a case of “all problems look like a nail”-syndrome\nfor somebody who just got a shiny new hammer—you (and future maintainers of your\ncode) will be much happier sticking with straightforward and simple code, eschewing\nsuch depths.\nI believe Martelli’s advice applies not only to ABCs and metaclasses, but also to class\nhierarchies, operator overloading, function decorators, descriptors, class decorators,\nand class builders using __init_subclass__.\nThose powerful tools exist primarily to support library and framework development.\nApplications naturally should use those tools, as provided by the Python standard\n952 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 983,
      "chapter": null,
      "content": "18 The phrase is widely quoted. I found an early direct quote in a post in DHH’s blog from 2005.\nlibrary or external packages. But implementing them in application code is often pre‐\nmature abstraction.\nGood frameworks are extracted, not invented.18\n—David Heinemeier Hansson, creator of Ruby on Rails\nChapter Summary\nThis chapter started with an overview of attributes found in class objects, such as\n__qualname__ and the __subclasses__() method. Next, we saw how the type built-\nin can be used to construct classes at runtime.\nThe __init_subclass__ special method was introduced, with the first iteration of a\nChecked base class designed to replace attribute type hints in user-defined subclasses\nwith Field instances that apply constructors to enforce the type of those attributes at\nruntime.\nThe same idea was implemented with a @checked class decorator that adds features to\nuser-defined classes, similar to what __init_subclass__ allows. We saw that neither\n__init_subclass__ nor a class decorator can dynamically configure __slots__,\nbecause they operate only after a class is created.\nThe concepts of “import time” and “runtime” were clarified with experiments show‐\ning the order in which Python code is executed when modules, descriptors, class dec‐\norators, and __init_subclass__ is involved.\nOur coverage of metaclasses began with an overall explanation of type as a metaclass,\nand how user-defined metaclasses can implement __new__ to customize the classes it\nbuilds. We then saw our first custom metaclass, the classic MetaBunch example using\n__slots__. Next, another evaluation time experiment demonstrated how the __pre\npare__ and __new__ methods of a metaclass are invoked earlier than __init_sub\nclass__ and class decorators, providing opportunities for deeper class customization.\nThe third iteration of a Checked class builder with Field descriptors and custom\n__slots__ configuration was presented, followed by some general considerations\nabout metaclass usage in practice.\nFinally, we saw the AutoConst hack invented by João S. O. Bueno, based on the cun‐\nning idea of a metaclass with __prepare__ returning a mapping that implements\n__missing__. In less than 20 lines of code, autoconst.py showcases the power of com‐\nbining Python metaprogramming techniques\nChapter Summary \n| \n953",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 984,
      "chapter": null,
      "content": "I haven’t yet found a language that manages to be easy for beginners, practical for\nprofessionals, and exciting for hackers in the way that Python is. Thanks, Guido van\nRossum and everybody else who makes it so.\nFurther Reading\nCaleb Hattingh—a technical reviewer of this book—wrote the autoslot package, pro‐\nviding a metaclass to automatically create a __slots__ attribute in a user-defined\nclass by inspecting the bytecode of __init__ and finding all assignments to attributes\nof self. It’s useful and also an excellent example to study: only 74 lines of code in\nautoslot.py, including 20 lines of comments explaining the most difficult parts.\nThe essential references for this chapter in the Python documentation are “3.3.3. Cus‐\ntomizing class creation” in the “Data Model” chapter of The Python Language Refer‐\nence, which covers __init_subclass__ and metaclasses. The type class\ndocumentation in the “Built-in Functions” page, and “4.13. Special Attributes” of the\n“Built-in Types” chapter in the The Python Standard Library are also essential read‐\ning.\nIn the The Python Standard Library, the types module documentation covers two\nfunctions \nadded \nin \nPython \n3.3 \nthat \nsimplify \nclass \nmetaprogramming:\ntypes.new_class and types.prepare_class.\nClass decorators were formalized in PEP 3129—Class Decorators, written by Collin\nWinter, with the reference implementation authored by Jack Diederich. The PyCon\n2009 talk “Class Decorators: Radically Simple” (video), also by Jack Diederich, is a\nquick introduction to the feature. Besides @dataclass, an interesting—and much\nsimpler—example of a class decorator in Python’s standard library is func\ntools.total_ordering that generates special methods for object comparison.\nFor metaclasses, the main reference in Python’s documentation is PEP 3115—Meta‐\nclasses in Python 3000, in which the __prepare__ special method was introduced.\nPython in a Nutshell, 3rd ed., by Alex Martelli, Anna Ravenscroft, and Steve Holden,\nis authoritative, but was written before PEP 487—Simpler customization of class cre‐\nation came out. The main metaclass example in that book—MetaBunch—is still valid,\nbecause it can’t be written with simpler mechanisms. Brett Slatkin’s Effective Python,\n2nd ed. (Addison-Wesley) has several up-to-date examples of class building techni‐\nques, including metaclasses.\n954 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 985,
      "chapter": null,
      "content": "19 I bought a used copy and found it a very challenging read.\nTo learn about the origins of class metaprogramming in Python, I recommend Guido\nvan Rossum’s paper from 2003, “Unifying types and classes in Python 2.2”. The text\napplies to modern Python as well, as it covers what were then called the “new-style”\nclass semantics—the default semantics in Python 3—including descriptors and meta‐\nclasses. One of the references cited by Guido is Putting Metaclasses to Work: a New\nDimension in Object-Oriented Programming, by Ira R. Forman and Scott H. Danforth\n(Addison-Wesley), a book to which he gave five stars on Amazon.com, adding the\nfollowing review:\nThis book contributed to the design for metaclasses in Python 2.2\nToo bad this is out of print; I keep referring to it as the best tutorial I know for the\ndifficult subject of cooperative multiple inheritance, supported by Python via the\nsuper() function.19\nIf you are keen on metaprogramming, you may wish Python had the ultimate meta‐\nprogramming feature: syntactic macros, as offered in the Lisp family of languages and\n—more recently—by Elixir and Rust. Syntactic macros are more powerful and less\nerror prone than the primitive code substitution macros in the C language. They are\nspecial functions that rewrite source code using custom syntax into standard code\nbefore the compilation step, enabling developers to introduce new language con‐\nstructs without changing the compiler. Like operator overloading, syntactic macros\ncan be abused. But as long as the community understands and manages the down‐\nsides, they support powerful and user-friendly abstractions, like DSLs (Domain-\nSpecific Languages). In September 2020, Python core developer Mark Shannon\nposted PEP 638—Syntactic Macros, advocating just that. A year after it was initially\npublished, PEP 638 was still in draft and there were no ongoing discussions about it.\nClearly it’s not a top priority for the Python core developers. I would like to see PEP\n638 further discussed and eventually approved. Syntactic macros would allow the\nPython community to experiment with controversial new features, such as the walrus\noperator (PEP 572), pattern matching (PEP 634), and alternative rules for evaluating\ntype hints (PEPs 563 and 649) before making permanent changes to the core lan‐\nguage. Meanwhile, you can get a taste of syntactic macros with the MacroPy package.\nFurther Reading \n| \n955",
      "content_length": 2412,
      "extraction_method": "Direct"
    },
    {
      "page_number": 986,
      "chapter": null,
      "content": "20 See p. xvii. Full text available at Berkeley.edu.\nSoapbox\nI will start the last soapbox in the book with a long quote from Brian Harvey and\nMatthew Wright, two computer science professors from the University of California\n(Berkeley and Santa Barbara). In their book, Simply Scheme: Introducing Computer\nScience (MIT Press), Harvey and Wright wrote:\nThere are two schools of thought about teaching computer science. We might carica‐\nture the two views this way:\n1. The conservative view: Computer programs have become too large and com‐\nplex to encompass in a human mind. Therefore, the job of computer science\neducation is to teach people how to discipline their work in such a way that 500\nmediocre programmers can join together and produce a program that correctly\nmeets its specification.\n2. The radical view: Computer programs have become too large and complex to\nencompass in a human mind. Therefore, the job of computer science education\nis to teach people how to expand their minds so that the programs can fit, by\nlearning to think in a vocabulary of larger, more powerful, more flexible ideas\nthan the obvious ones. Each unit of programming thought must have a big pay‐\noff in the capabilities of the program.\n—Brian Harvey and Matthew Wright, preface to Simply Scheme20\nHarvey and Wright’s exaggerated descriptions are about teaching computer science,\nbut they also apply to programming language design. By now, you should have\nguessed that I subscribe to the “radical” view, and I believe Python was designed in\nthat spirit.\nThe property idea is a great step forward compared to the accessors-from-the-start\napproach practically demanded by Java and supported by Java IDEs generating get‐\nters/setters with a keyboard shortcut. The main advantage of properties is to let us\nstart our programs simply exposing attributes as public—in the spirit of KISS—know‐\ning a public attribute can become a property at any time without much pain. But the\ndescriptor idea goes way beyond that, providing a framework for abstracting away\nrepetitive accessor logic. That framework is so effective that essential Python con‐\nstructs use it behind the scenes.\nAnother powerful idea is functions as first-class objects, paving the way to higher-\norder functions. Turns out the combination of descriptors and higher-order func‐\ntions enable the unification of functions and methods. A function’s __get__ produces\n956 \n| \nChapter 24: Class Metaprogramming",
      "content_length": 2448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 987,
      "chapter": null,
      "content": "21 Machine Beauty: Elegance and the Heart of Technology by David Gelernter (Basic Books) opens with an intri‐\nguing discussion of elegance and aesthetics in works of engineering, from bridges to software. The later chap‐\nters are not great, but the opening is worth the price.\na method object on the fly by binding the instance to the self argument.\nThis is elegant.21\nFinally, we have the idea of classes as first-class objects. It’s an outstanding feat of\ndesign that a beginner-friendly language provides powerful abstractions such as class\nbuilders, class decorators, and full-fledged, user-defined metaclasses. Best of all, the\nadvanced features are integrated in a way that does not complicate Python’s suitabil‐\nity for casual programming (they actually help it, under the covers). The convenience\nand success of frameworks such as Django and SQLAlchemy owe much to meta‐\nclasses. Over the years, class metaprogramming in Python is becoming simpler and\nsimpler, at least for common use cases. The best language features are those that ben‐\nefit everyone, even if some Python users are not aware of them. But they can always\nlearn and create the next great library.\nI look forward to learning about your contributions to the Python community and\necosystem!\nFurther Reading \n| \n957",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 988,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 989,
      "chapter": null,
      "content": "Afterword\nPython is a language for consenting adults.\n—Alan Runyan, cofounder of Plone\nAlan’s pithy definition expresses one of the best qualities of Python: it gets out of the\nway and lets you do what you must. This also means it doesn’t give you tools to\nrestrict what others can do with your code and the objects it builds.\nAt age 30, Python is still growing in popularity. But of course, it is not perfect.\nAmong the top irritants to me is the inconsistent use of CamelCase, snake_case, and\njoinedwords in the standard library. But the language definition and the standard\nlibrary are only part of an ecosystem. The community of users and contributors is the\nbest part of the Python ecosystem.\nHere is one example of the community at its best: while writing about asyncio in the\nfirst edition, I was frustrated because the API has many functions, dozens of which\nare coroutines, and you had to call the coroutines with yield from—now with await\n—but you can’t do that with regular functions. This was documented in the asyncio\npages, but sometimes you had to read a few paragraphs to find out whether a particu‐\nlar function was a coroutine. So I sent a message to python-tulip titled “Proposal:\nmake coroutines stand out in the asyncio docs”. Victor Stinner, an asyncio core devel‐\noper; Andrew Svetlov, main author of aiohttp; Ben Darnell, lead developer of Tor‐\nnado; and Glyph Lefkowitz, inventor of Twisted, joined the conversation. Darnell\nsuggested a solution, Alexander Shorin explained how to implement it in Sphinx, and\nStinner added the necessary configuration and markup. Less than 12 hours after I\nraised the issue, the entire asyncio documentation set online was updated with the\ncoroutine tags you can see today.\nThat story did not happen in an exclusive club. Anybody can join the python-tulip\nlist, and I had posted only a few times when I wrote the proposal. The story illustrates\na community that is really open to new ideas and new members. Guido van Rossum\nused to hang out in python-tulip and often answered basic questions.\n959",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 990,
      "chapter": null,
      "content": "Another example of openness: the Python Software Foundation (PSF) has been\nworking to increase diversity in the Python community. Some encouraging results\nare already in. The 2013–2014 PSF board saw the first women elected directors: Jes‐\nsica McKellar and Lynn Root. In 2015, Diana Clarke chaired PyCon North America\nin Montréal, where about one-third of the speakers were women. PyLadies became a\ntruly global movement, and I am proud that we have so many PyLadies chapters in\nBrazil.\nIf you are a Pythonista but you have not engaged with the community, I encourage\nyou to do so. Seek the PyLadies or Python Users Group (PUG) in your area. If there\nisn’t one, create it. Python is everywhere, so you will not be alone. Travel to events if\nyou can. Join live events too. During the Covid-19 pandemic I learned a lot in the\n“hallway tracks” of online conferences. Come to a PythonBrasil conference—we’ve\nhad international speakers regularly for many years now. Hanging out with fellow\nPythonistas brings real benefits besides all the knowledge sharing. Like real jobs and\nreal friendships.\nI know I could not have written this book without the help of many friends I made\nover the years in the Python community.\nMy father, Jairo Ramalho, used to say “Só erra quem trabalha,” Portuguese for “Only\nthose who work make mistakes,” great advice to avoid being paralyzed by the fear of\nmaking errors. I certainly made my share of mistakes while writing this book. The\nreviewers, editors, and early release readers caught many of them. Within hours of\nthe first edition early release, a reader was reporting typos in the errata page for the\nbook. Other readers contributed more reports, and friends contacted me directly to\noffer suggestions and corrections. The O’Reilly copyeditors will catch other errors\nduring the production process, which will start as soon as I manage to stop writing. I\ntake responsibility and apologize for any errors and suboptimal prose that remains.\nI am very happy to bring this second edition to conclusion, mistakes and all, and I am\nvery grateful to everybody who helped along the way.\nI hope to see you soon at some live event. Please come say hi if you see me around!\nFurther Reading\nI will wrap up the book with references regarding what it its to be “Pythonic”—the\nmain question this book tried to address.\nBrandon Rhodes is an awesome Python teacher, and his talk “A Python Æsthetic:\nBeauty and Why I Python” is beautiful, starting with the use of Unicode U+00C6\n(LATIN CAPITAL LETTER AE) in the title. Another awesome teacher, Raymond Het‐\ntinger, spoke of beauty in Python at PyCon US 2013: “Transforming Code into Beau‐\ntiful, Idiomatic Python”.\n960 \n| \nAfterword",
      "content_length": 2697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 991,
      "chapter": null,
      "content": "The “Evolution of Style Guides” thread that Ian Lee started on Python-ideas is worth\nreading. Lee is the maintainer of the pep8 package that checks Python source code for\nPEP 8 compliance. To check the code in this book, I used flake8, which wraps pep8,\npyflakes, and Ned Batchelder’s McCabe complexity plug-in.\nBesides PEP 8, other influential style guides are the Google Python Style Guide and the\nPocoo Styleguide, from the team that brought us Flake, Sphinx, Jinja 2, and other\ngreat Python libraries.\nThe Hitchhiker’s Guide to Python! is a collective work about writing Pythonic code.\nIts most prolific contributor is Kenneth Reitz, a community hero thanks to his beau‐\ntifully Pythonic requests package. David Goodger presented a tutorial at PyCon US\n2008 titled “Code Like a Pythonista: Idiomatic Python”. If printed, the tutorial notes\nare 30 pages long. Goodger created both reStructuredText and docutils—the foun‐\ndations of Sphinx, Python’s excellent documentation system (which, by the way, is\nalso the official documentation system for MongoDB and many other projects).\nMartijn Faassen tackles the question head-on in “What is Pythonic?” In the python-\nlist, there is a thread with that same title. Martijn’s post is from 2005, and the thread\nfrom 2003, but the Pythonic ideal hasn’t changed much—neither has the language,\nfor that matter. A great thread with “Pythonic” in the title is “Pythonic way to sum n-\nth list element?”, from which I quoted extensively in the “Soapbox” on page 427.\nPEP 3099 — Things that will Not Change in Python 3000 explains why many things\nare the way they are, even after the major overhaul that was Python 3. For a long\ntime, Python 3 was nicknamed Python 3000, but it arrived a few centuries sooner—to\nthe dismay of some. PEP 3099 was written by Georg Brandl, compiling many opin‐\nions expressed by the BDFL, Guido van Rossum. The “Python Essays” page lists sev‐\neral texts by Guido himself.\nAfterword \n| \n961",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 992,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 993,
      "chapter": null,
      "content": "Index\nSymbols\n!= (not equal to) operator, 577\n!r conversion field, 12\n% (modulo) operator, 5, 12\n%r placeholder, 12\n* (star) operator, 10, 36-37, 50-56, 240, 572-574\n** (double star) operator, 80, 240\n*= (star equals) operator, 53-56, 580-585\n*_ symbol, 42\n*_new*_, 843-845\n+ operator, 10, 50-56, 566-572\n+= (addition assignment) operator, 53-56,\n580-585\n+ELLIPSIS directive, 7\n:= (Walrus operator), 26\n< (less than) operator, 577\n<= (less than or equal to) operator, 577\n== (equality) operator, 206, 225, 577\n> (greater than) operator, 577\n>= (greater than or equal to) operator, 577\n@ sign, 574-576\n@asyncio.coroutine decorator, 777\n@cached_property, 856\n@contextmanager decorator, 664-669\n@dataclass\ndefault settings, 180\nexample using, 187-189\nfield options, 180-183\ninit-only variables, 186\nkeyword parameters accepted by, 179\npost-init processing, 183-185\ntyped class attributes, 185\n__hash__ method, 180\n@typing.overload decorator, 520-526\n[] (square brackets), 6, 26, 35, 49\n\\ (backslash), 26\n\\ line continuation escape, 26\n\\N{} (Unicode literals escape notation), 136\n_ symbol, 41\n__ (double underscore), 3\n__abs__, 11\n__add__, 11\n__bool__, 13\n__bytes__, 365\n__call__, 360\n__class__, 870, 908\n__contains__, 7, 93\n__delattr__, 872\n__delete__, 879\n__del__, 219\n__dict__, 870\n__dir__, 872\n__enter__, 658\n__eq__, 411-416\n__exit__, 658\n__format__, 365, 370, 418-425, 428\n__getattribute__, 872\n__getattr__, 407-411, 872\n__getitem__, 5-8, 49, 403-407\n__get__, 879\n__hash__, 180, 411-416\n__iadd__, 53\n__init_subclass__, 914-921\n__init__, 9, 183, 399\n__invert__, 563\n__iter__, 603-610\n__len__, 5-8, 17, 403-407\n__missing__, 91-95\n963",
      "content_length": 1633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 994,
      "chapter": null,
      "content": "__mro__, 908\n__mul__, 11\n__name__, 908\n__neg__, 563\n__post_init__, 183\n__pos__, 563\n__prepare__, 950-952\n__repr__, 11-13, 364, 399\n__setattr__, 872\n__setitem__, 49\n__set__, 879\n__slots__, 384-388, 411, 870, 921\n__str__, 13, 364\n{} (curly brackets), 26\nǀ (pipe) operator, 80, 686\nǀ= (pipe equals) operator, 80\n… (ellipsis), 7, 49\nA\nabc.ABC class, 449\nabc.Iterable, 281\nabc.Sequence, 281\nABCs (abstract base classes)\nABC syntax details, 457\ndefining and using ABCs, 451-457\nfurther reading on, 482\ngoose typing and, 442-445\noverview of, 481\nin Python standard library, 449-451\nSoapbox discussion, 484-486\nstructural typing with, 464-466\nsubclassing ABCs, 447-449, 458-460\ntype hints (type annotations), 278-280\nUML class diagrams, 14-15\nusage of register, 463\nvirtual subclasses of ABCs, 460-463\nabs built-in function, 10\nactual type parameters, 544\naddition assignment (+=) operator, 53-56,\n580-585\naliasing, 204-208\nanonymous functions, 236, 251, 311\nAny type, 266-269\nAnyStr, 286\n.append method, 67\napply function, 234-236\narguments\nfreezing with functools.partial, 247\nkey argument, 74\nkeyword-only arguments, 242\narrays, 23, 59-62\nas keyword, 41\nassignment expression (:=), 26\nasynchronous generators, 238, 777\nasynchronous programming\nasynchronous context managers, 786-787\nasyncio script example, 778-781\navoiding CPU-bound traps, 826\nawaitables, 781\nbenefits of, 825\nCurio project, 821-824\ndelegating tasks to executors, 797-798\nenhancing asyncio downloader, 787-797\nfurther reading on, 828\niteration and iterables, 811-821\nmyth of I/O-bound systems, 826\noverview of, 827\nrelevant terminology, 777\nsignificant changes to, 776\nSoapbox discussion, 829\ntopics covered, 775\ntype hinting asynchronous objects, 824\nwriting asyncio servers, 799-811\nAsynchronous Server Gateway Interface\n(ASGI), 732\nasyncio package\nachieving peak performance with, 785\ndocumentation, 776\ndownloading with, 782-786\nenhancing asyncio downloader, 787-797\nexample script, 778-781\nqueue implementation by, 69\nwriting asyncio servers, 799-811\nasyncpg, 786\nattribute descriptors (see also attributes;\ndynamic attributes and properties)\nattribute validation, 880-891\ndescriptor docstring and overriding dele‐\ntion, 902\ndescriptor usage tips, 900-902\nfurther reading on, 904\nmethods as descriptors, 898-900\noverriding versus nonoverriding, 855,\n892-898\noverview of, 903\npurpose of, 879\nrelevant terminology, 881\nsignificant changes to, 880\n964 \n| \nIndex",
      "content_length": 2426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 995,
      "chapter": null,
      "content": "Soapbox discussion, 905\ntopics covered, 879\nattributes (see also attribute descriptors;\ndynamic attributes and properties)\ndynamic attribute access, 407-411\nhandling attribute deletion, 868\noverriding class attributes, 389-391\nprivate and protected, 382-384\nproperties and up-front costs, 394\nsafety versus security in private, 395\nusing attribute descriptors for validation,\n880-891\nusing properties for attribute validation,\n857-860\nvirtual attributes, 835\naugmented assignment operators, 53-56,\n580-585\naverages, computing, 643-645\nawait keyword, 781\nB\nbackslash (\\), 26\nbehavioral subtyping, 268\nbinary records, parsing with struct, 118\nbinary sequences, 120 (see also Unicode text\nversus bytes)\nbisect module, 58\nblue tool, 259\nBOMs (byte-order marks), 129\nbool type, 13\nBoolean values, custom types and, 13\nbuilt-in functions, 238\nbyte sequences, 128 (see also Unicode text ver‐\nsus bytes)\nbytecode, disassembling, 310\nC\ncall by sharing, 213\ncallable objects\nnine types of, 237-239\nuser-defined, 239\nusing iter() with, 598-599\nCallable type, 291\ncard deck example, 5-8\nCartesian products, 27-29\ncase folding, 142\nchain generator, 633\nChainMap, 95\ncharacters\nfinding Unicode by name, 151-153\nnumeric meaning of, 153-155\nChardet library, 129\ncladistics, 444\nclass metaprogramming\nbenefits and drawbacks of, 907\nbuilt-in class factory, 909\nclass factory function, 911-914\nclasses as objects, 908\nenhancing classes with class decorators,\n922-925\nfurther reading on, 954\nimport time versus runtime, 925-931\n__init_subclass__, 914-921\nmetaclass basics, 931-942\nmetaclass issues, 947-949\nmetaclass solution for checkedlib.py,\n942-947\noverview of, 953\n__prepare__ method, 950-952\nsignificant changes to, 908\nSoapbox discussion, 956\nuseful applications of metaclasses, 952\nclasses (see also protocols)\nas callable objects, 238\nimplementing generic classes, 541-544\ntopics covered, xxi\nundocumented classes, 557\nclassic refactoring strategy, 342-346\nclassmethod decorator, 369\nclient codes, 632\nclock decorators\nclass-based, 335\nparameterized, 332-334\nclosures (see decorators and closures)\ncls.mro(), 909\ncls.__bases__, 908\ncls.__qualname__, 909\ncls.__subclasses__(), 909\ncode examples, obtaining and using, xxiv\ncode points, 119, 161\ncode smells, 163, 190-192, 446\ncodecs, 123\nCollection API, 14-15\ncollections.abc module\nabstract base classes defined in, 449\nChainMap, 95\nCounter, 96\ndefaultdict and OrderedDict, 85, 95\nIndex \n| \n965",
      "content_length": 2432,
      "extraction_method": "Direct"
    },
    {
      "page_number": 996,
      "chapter": null,
      "content": "Mapping and MutableMapping ABCs, 83\nmultiple inheritance in, 502\nUserDict, 97\ncollections.deque class, 67-70\ncollections.namedtuple, 5, 169-172\nCommand pattern, 355-357\ncomments and questions, xxv\ncomparison operators, 206, 577-580\ncomputed properties\ncaching properties with functools, 855-857\ndata-driven attribute creation, 846-848\nproperties that compute values, 845\nproperty caching, 853-855\nproperty overriding existing attributes, 852\nproperty to retrieve linked records, 848-852\nconcatenation, 50-56\nconcurrency models\nbasics of concurrency, 696\nbenefits of concurrency, 695\nfurther reading on, 734-739\nGlobal Interpreter Lock impact, 713-715\nHello World example, 701-712\nindefinite loops and sentinels, 721\nmulticore processors and, 725-733\noverview of, 733\nprocess pools, 716-725\nPython programming concepts, 699-701\nrelevant terminology, 697-699\nsignificant changes to, 696\nSoapbox discussion, 739-741\nstructured concurrency, 823\ntopics covered, 696\nconcurrent executors\nconcurrent web downloads, 744-754\ndownloads with progress display and error\nhandling, 762-772\nExecutor.map, 758-761\nfurther reading on, 772\nlaunching processes with concur‐\nrent.futures, 754-758\noverview of, 772\npurpose of, 743\nsignificant changes to, 743\nSoapbox discussion, 773\nconcurrent.futures\ndownloading with, 749-750\nlaunching processes with, 754-758\nconsistent-with relationships, 268\nContainer interface, 15\ncontainer sequences, 22, 73\ncontention, 699\ncontext managers\n@contextmanager decorator, 664-669\nasynchronous, 786-787\nasynchronous generators as, 817\ncontextlib utilities, 663\ncreative uses for, 658\ndemonstrations of, 659-662\nmethods included in interface, 658\nparenthesized in Python 3.10, 663\npurpose of, 658\ncontravariance (see variance)\ncontrol flow, xxi (see also asynchronous pro‐\ngramming; concurrent executors; concur‐\nrency models; iterators; generators; with,\nmatch, and else blocks)\ncooperative multiple inheritance, 497\ncopies\ndeep, 211-213\nshallow, 208-211\ncoroutine objects, 642\ncoroutines\ncomputing running averages, 643-645\ndefinition of term, 698\nfurther reading on, 652\ngenerator-based, 777\ngeneric type hints for, 650\nGlobal Interpreter Lock impact, 715\noverview of, 652\nreturning values from, 646-650\nsignificant changes to, 594\nSoapbox discussion, 654\nspinners (loading indicators) using, 706-710\ntopics covered, 594\ntypes of, 777\nunderstanding classic, 641\nCounter, 96\ncovariance (see variance)\nCPU-bound systems, 826\nCurio project, 821-824\ncurly brackets ({}), 26\nD\nDask, 727\ndata attributes (see attributes)\ndata class builders\n@dataclass, 179-189\nclassic named tuples, 169-172\n966 \n| \nIndex",
      "content_length": 2615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 997,
      "chapter": null,
      "content": "data class as code smell, 190-192\nfurther reading on, 196\nmain features, 167-169\noverview of, 164-167, 195\npattern matching class instances, 192-195\nsignificant changes to, 164\nSoapbox discussion, 197\ntopics covered, 163\ntype hints, 173-179\ntyped named tuples, 172\ndata descriptors (see overriding descriptors)\ndata model (see Python Data Model)\ndata science, 727\ndata structures, xxi (see also data class builders;\ndictionaries and sets; object references;\nsequences; Unicode text versus bytes)\ndata wrangling\nwith dynamic attributes, 836-838\nflexible object creation, 843-845\ninvalid attribute name problem, 842\nJSON-like data, 838-842\ndecimal.Decimal class, 565\ndecoding (see also Unicode text versus bytes)\nbasics of, 123-124\ndefinition of, 119\nunderstanding encode/decode problems,\n125-131\ndecorator-enhanced strategy pattern, 353-355\ndecorators and closures\nclassmethod versus staticmethod, 369\nclosure basics, 311-314\nclosures in lis.py, 685\ndecorator basics, 304-306\ndecorator execution, 306\ndecorator implementation, 317-320\ndecorators in Python standard library,\n320-323\nenhancing classes with class decorators,\n922-925\nfurther reading on, 336\nnonlocal declarations, 315-317\noverview of, 336\nparameterized decorators, 329-335\npurpose of, 303\nregistration decorators, 308\nsignificant changes to, 304\nSoapbox discussion, 338-340\ntopics covered, 303\nvariable scope rules, 308-310\ndeep copies, 211-213\ndefaultdict, 85, 90\ndefensive programming, 87, 440-442\ndel statement, 219-221, 868\ndelegating generators, 632\ndeprecated collection types, 272\ndeque (double-ended queue), 59, 67-70\ndescriptor classes, 881\ndescriptor instances, 881\ndescriptors, 177, 879 (see also attribute descrip‐\ntors)\ndesign patterns (see functions, design patterns\nwith first-class)\ndestructuring, 40\ndiacritics, normalization and, 144-148\ndictcomps (dict comprehensions), 79\ndictionaries and sets\nautomatic handling of missing keys, 90-95\nconsequences of how dict works, 102\nconsequences of how set works, 107\ndictionary views, 101-102\nfurther reading on, 113\nimmutable mappings, 99\ninternals of, 78\nmodern dict syntax, 78-81\noverview of, 112\npattern matching with mappings, 81-83\nset operations, 107-110\nset operations on dict views, 110\nset theory, 103-106\nsignificant changes to, 78\nSoapbox discussion, 114\nstandard API of mapping types, 83-90\ntopics covered, 77\nvariations of dict, 95-99\ndir([object]) function, 870\ndis module, 310\ndisplays, formatting, 370-374\ndistributed task queues, 732\nDjango generic views mixins, 504-507\ndoctest package\ndocumentation, xxii\nellipsis in, 7\ndouble star (**) operator, 80, 240\ndouble underscore (__), 3\ndouble() function, 466-468\nDublin Core Schema, 187\nduck typing, 87, 261, 302, 402, 427, 431, 470\ndunder methods, 4\nIndex \n| \n967",
      "content_length": 2752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 998,
      "chapter": null,
      "content": "dynamic attributes and properties\ncoding property factories, 865-868\ncomputed properties, 845-857\ndata wrangling with dynamic attributes,\n836-845\ndynamic versus virtual attributes, 835\nessential attributes and functions for\nattribute handling, 869-873\nfurther reading on, 873\nhandling attribute deletion, 868\noverview of, 873\nproperty class, 860-865\nsignificant changes to, 836\nSoapbox discussion, 875-877\nusing properties for attribute validation,\n857-860\ndynamic protocols, 403, 435\ndynamic type, 266-269\nE\nellipsis (…), 7, 49\nelse blocks, 687-689\nemojis\nbuilding, 118\nconsole font and, 136\nfinding characters by name, 151-153\nin the Museum of Modern Art, 160\nincreasing issues with, 162\nUCS-2 versus UTF-16 encoding, 124\nvaried support for, 152\nencoding (see also Unicode text versus bytes)\nbasics of, 123-124\ndefinition of, 119\nencoding defaults, 134-139\nunderstanding encode/decode problems,\n125-131\nequality (==) operator, 206, 225, 577\nerror handling, in network I/O, 762-772\nexecution units, 697\nExecutor.map, 758-761\nexecutors, delegating tasks to, 797-798 (see also\nconcurrent executors)\nexplicit self argument, 905\nF\nf-string syntax\nbenefits of, 5\ndelegation of formatting by, 370\nstring representation using special methods,\n12\nfail-fast philosophy, 87, 417, 440-442\nFastAPI framework, 800-804\nFIFO (first in, first out), 59, 67\nfilter function, 27, 234-236\nfiltering generator functions, 619\nfirst-class functions (see functions, as first-class\nobjects)\nfirst-class objects, 231\nflake8 tool, 259\nflat sequences, 22, 73\nflawed typing, 296\nfluentpython.com, xxiii\nForkingMixIn, 503\nformal type parameters, 544\nformat() function, 370\nforward reference problem, 538\nfree variables, 313, 685\nfrozenset, 103 (see also dictionaries and sets)\nfunction decorators (see decorators and clo‐\nsures)\nfunction parameters, introspection of, 232\nfunction-class duality, 876\nfunction-oriented refactoring strategy, 347-350\nfunctional programming\npackages for, 243-249\nwith Python, 250\nfunctions\nabs built-in function, 10\ndir([object]) function, 870\ndisassembling bytecode of, 310\ndouble() function, 466\nfilter, map, and reduce functions, 27, 234\nformat() function, 370\ngetattr function, 871\ngetattr(object, name[, default]) function,\n871\nglobals() function, 351\nhasattr function, 871\nhigher-order functions, 234-236\nid() function, 206\niter() function, 596\nlen() function, 6\nmap function, 27\nmax() function, 521\nrepr() function, 364\nsetattr funcion, 871\n968 \n| \nIndex",
      "content_length": 2462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 999,
      "chapter": null,
      "content": "setattr(object, name, value) function, 871,\n871\nsingle dispatch generic functions, 324-329\nstr() function, 13, 364\nsuper() function, 411, 488\nzip() function, 416\nfunctions, as first-class objects (see also decora‐\ntors and closures)\nanonymous functions, 236\ncallable objects, 237-239\ndefinition of term, 231\nflexible parameter handling and, 240-243\nfurther reading on, 250\nhigher-order functions, 234-236\noverview of, 249\npackages for functional programming,\n243-249\nsignificant changes to, 232\nSoapbox discussion, 250\ntopics covered, xxi\ntreating functions like objects, 232-234\nuser-defined callable types, 239\nfunctions, design patterns with first-class\nCommand pattern, 355-357\ndecorator-enhanced strategy pattern,\n353-355\ndynamic languages and, 341\nfurther reading on, 358\noverview of, 357\nrefactoring strategies, 342-353\nsignificant changes to, 342\nSoapbox discussion, 359\nfunctions, type hints in\nannotating positional only and variadic\nparameters, 295\nbenefits and drawbacks of, 253\nflawed typing and strong testing, 296\nfurther reading on, 298\ngradual typing, 254-260\noverview of, 297\nsignificant changes to, 254\nSoapbox discussion, 299-302\nsupported operations and, 261-265\ntopics covered, 254\ntypes usable in annotations, 266-295\nfunctools module\ncaching properties with, 855-857\nfreezing arguments with, 247-249\nfunctools.cache decorator, 320-323\nfunctools.lru_cache function, 323\nfunctools.singledispatch decorator, 326-329\nfutures\nbasics of, 751-754\ndefinition of term, 743\nfutures.as_completed, 769-772\nG\ngarbage collection, 219-221, 226\ngenerator expressions (genexps), 26, 29, 235,\n613, 818-821\ngenerators\narithmetic progression generators, 615-619\nasynchronous generator functions, 812-818\nexamples of, 607-610\nfurther reading on, 652\ngenerator functions in Python standard\nlibrary, 238, 619-630\ngenerator-based coroutines, 777\ngeneric iterable types, 639\nhumble generators, 784\niterable reducing functions, 630-631\nversus iterators, 614\nlazy generators, 610-613\noverview of, 652\nSentence classes with, 606\nsignificant changes to, 594\nSoapbox discussion, 654\nsubgenerators with yield from expression,\n632-639\ntopics covered, 594\nwhen to use generator expressions, 613\nyield keyword, 607\ngeneric classes, implementing, 541-544\ngeneric collections\nparameterized generics and TypeVar,\n282-286\nSoapbox discussion, 302\ntype annotations and, 271-272\ngeneric functions, single dispatch, 324-329\ngeneric mapping types, 276\ngeneric static protocols, 552-554\ngetattr function, 871\ngetattr(object, name[, default]) function, 871\ngevent library, 711\nGlobal Interpreter Lock (GIL), 66, 699,\n713-715, 725, 736\nglobals() function, 351\ngoose typing\nIndex \n| \n969",
      "content_length": 2664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1000,
      "chapter": null,
      "content": "ABC syntax details, 457\nABCs in Python standard library, 449-451\nabstract base classes (ABCs), 442-445\ndefining and using ABCs, 451-457\ndefinition of term, 431\noverview of, 446\nstructural typing with ABCs, 464-466\nsubclassing ABCs, 447-449, 458-460\nusage of register, 463\nvirtual subclasses of ABCs, 460-463\ngradual type system (see also type hints (type\nannotations))\nabstract base classes, 278-280\nAny type, 266-267\nbasics of, 254\nCallable type, 291\nfurther reading on, 555\ngeneric collections, 271-272\ngeneric mappings, 276\nimplementing generic classes, 541-544\nimplementing generic static protocols,\n552-554\nin practice, 255-260\nIterable, 280-282\nlegacy support and deprecated collection\ntypes, 272\nNoReturn type, 294\nOptional and Union types, 270\noverloaded signatures, 520-526\noverview of, 554\nparameterized generics and TypeVar,\n282-286\nreading hints at runtime, 537-541\nsignificant changes to, 519\nsimple types and classes, 269\nSoapbox discussion, 557\nstatic protocols, 286-291\nsubtype-of versus consistent-with relation‐\nships, 267-269\ntopics covered, 519\ntuple types, 274-276\ntype casting, 534-537\nTypedDict, 526\nvariance and, 544-551\ngreater than (>) operator, 577\ngreater than or equal to (>=) operator, 577\ngreenlet package, 710\nH\nhasattr function, 871\nhash code, versus hash value, 84\nhash tables, 77\nhashable, definition of, 84\nheapq package, 69\nhigher-order functions, 234-236\nHTTPServer class, 503\nHTTPX library, 782-786\nhumble generators, 784\nI\nI/O (input/output) (see network I/O)\nid() function, 206\nimmutable mappings, 99\nimmutable sequences, 24\nimplicit conversion, 117\nimport time versus runtime, 306, 925\nindefinite loops, 721\ninfix operators, 561, 574-576\ninheritance and subclassing\nbest practices, 510-514\nfurther reading on, 515\nmixin classes, 500-502\nmultiple inheritance and method resolution\norder, 494-499\noverview of, 514\nreal-world examples of, 502-509\nsignificant changes to, 488\nSoapbox discussion, 517-518\nsubclassing ABCs, 447-449, 458-460\nsubclassing built-in types, 490-494\nsuper() function, 488-490\ntopics covered, 487\nvirtual subclasses of ABCs, 460-463\ninput expanding generator functions, 625\ninterfaces (see also goose typing; protocols)\nContainer interface, 15\nfurther reading on, 482\nIterable interface, 15, 280, 435\noverview of, 481\nprotocols as informal, 427\nrole in object-oriented programming, 431\nsignificant changes to, 433\nSized interface, 15\nSoapbox discussion, 484-486\ntopics covered, 433\ntyping map, 432\nways of defining and using, 431\n970 \n| \nIndex",
      "content_length": 2505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1001,
      "chapter": null,
      "content": "interning, 222\ninvalid attribute name problem, 842\ninverted indexes, 799\nis operator, 206, 221\n.items method, 110\niter() function, 596-599\nIterable interface, 15, 280-282, 435-437\niterables\nasynchronous, 811-821\niterable reducing functions, 630-631\nversus iterators, 599-603\nunpacking, 35-38\niterators\nasynchronous, 811-821\nfurther reading on, 652\nversus generators, 614\ngeneric iterable types, 639\niter() function, 596-599\nversus iterables, 599-603\nlazy sentences, 610-613\noverview of, 652\nrole of, 593\nSentence classes with __iter__, 603-610\nsequence protocol, 594-596\nsignificant changes to, 594\nSoapbox discussion, 654\ntopics covered, 594\nitertools module, 618\nJ\nJSON-like data, 838-842\nK\nkey argument, 74\nkeys\nautomatic handling of missing, 90-95\nconverting nonstring keys to str, 98\nhashability, 84\npersistent storage for mapping, 97\npractical consequences of using dict, 102\npreserving key insertion order, 15, 948\nsorting multiple, 244\n.keys method, 110\nkeyword class patterns, 193\nkeyword-only arguments, 242\nkeywords\nas keyword, 41\nawait keyword, 781\nlambda keyword, 236\nnonlocal keyword, 315-317, 683\nreserved keywords, 679\nyield keyword, 238, 605-610, 614, 642, 645,\n669\nKISS principle, 427, 517, 739\nL\nlambda keyword, 236\nlazy sentences, 610-613\nLeast Recently Used (LRU), 323\nlen() function, 6\nless than (<) operator, 577\nless than or equal to (<=) operator, 577\nline breaks, 26\nlis.py interpreter\nEnvironment class, 673\nevaluate function, 676-685\nimports and types, 671\nOR-patterns, 686\nparser, 671-673\npattern matching in, 43-47\nProcedure class, 685-686\nREPL (read-eval-print-loop), 675\nScheme syntax, 669-671\ntopics covered, 669\nlist comprehensions (listcomps)\nasynchronous, 818-821\nbuilding lists from cartesian products, 27\nbuilding sequences with, 25\nversus generator expressions, 29\nlocal scope within, 26\nreadability and, 25\nsyntax tip, 26\nversus map and filter functions, 27, 235\nlists\nalternatives to, 59-70\nbuilding lists of lists, 51\nlist.sort versus sorted built-in, 56-58\nmixed-bag, 74\nmultiline, 26\nshallow copies of, 208-211\nversus tuples, 34\nusing tuples as immutable, 32-34\nlocks, definition of term, 699\nLRU (see Least Recently Used)\nM\nmagic methods, 4, 19\nmanaged attributes, 882\nIndex \n| \n971",
      "content_length": 2227,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1002,
      "chapter": null,
      "content": "managed classes, 881\nmanaged instances, 882\nmap function, 27, 234-236\nmappings\nautomatic handling of missing keys, 90-95\ncase-insensitive, 500-502\nimmutable mappings, 99\nmapping generator functions, 620\nmerging, 80\npattern matching with, 81-83\nstandard API of mapping types, 83-90\nunpacking, 80\nmatch blocks (see with, match, and else blocks)\nmatch/case statement, 38, 81\nmathematical vector operations, 566\nmax() function, 521-526\nmemoization, 320-323\nmemory, saving with __slots__, 384-388\nmemoryview class, 62-64\nmetaclasses\nbasics of, 931-933\nconsiderations for use, 947-949\ncustomizing classes, 933\ndefinition of term, 910\nexample metaclass, 934-937\nmetaclass evaluation time experiment,\n937-942\nmetaclass solution for checkedlib.py,\n942-947\nuseful applications of, 952\nmetaobjects, 19\nmetaprogramming, xxii (see also attribute\ndescriptors; class metaprogramming;\ndynamic attributes and properties)\nmethod resolution order (MRO), 494-499\nmethods, as callable objects, 238 (see also\nsequences, special methods for; special\nmethods)\nMeyer's Uniform Access Principle, 875-876\nMills & Gizmos Notation (MGN), 883\nmixin classes, 500-502\nmixin methods, 502\nmodulo (%) operator, 5, 12\nmonkey-patching, 438-440, 486, 897\nMRO (see method resolution order)\nmulticore processing\ndata science, 727\ndistributed task queues, 732\nincreased availability of, 725\nserver-side web/mobile development, 728\nsystem administration, 726\nWSGI application servers, 730\nmultiline lists, 26\nmultiple inheritance (see also inheritance and\nsubclassing)\nmethod resolution order and, 494-499\nreal-world examples of, 502-509\nmultiplication, scalar, 10, 572-574\nmultiprocessing package, 69, 704\nmutable objects, 225 (see also object references)\nmutable parameters, 214-218\nmutable sequences, 24\nmutable values, inserting or updating, 87-90\nMutableMapping ABC, 83\nMypy type checker, 255-260\nmy_fmt.format() method, 5\nN\nname mangling, 382\nnamedtuple, 169-172\nnative coroutines\nversus asynchronous generators, 818\ndefinition of term, 777\nfunctions defined with async def, 238\nhumble generators and, 784\nnetwork I/O\ndownloading with asyncio, 782-786\ndownloading with concurrent.futures,\n749-750\ndownloads with progress display and error\nhandling, 762-772\nenhancing asyncio downloader, 787-797\nessential role of concurrency in, 744-746\nmyth of I/O-bound systems, 826\nrole of futures, 751-754\nsequential download script, 746-749\nNFC (see Normalization Form C)\nnominal typing, 262\nnonlocal keyword, 315-317, 683\nnonoverriding descriptors, 855, 892-898\nNoReturn type, 294\nNormalization Form C (NFC), 140\nnormalized text matching, 143-148\nnot equal to (!=) operator, 577\nnumbers ABCs, 478-481\nnumbers package, 279\nnumeric protocols, 478-481\nnumeric tower, 279\n972 \n| \nIndex",
      "content_length": 2734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1003,
      "chapter": null,
      "content": "numeric types\nchecking for convertibility, 468, 480\nemulating using special methods, 9-12\nhashability of, 84\nsupport for, 479\nNumPy, 64-67\nO\nobject references\naliasing, 204-208\ndeep copies, 211-213\ndel and garbage collection, 219-221\ndistinction between objects and their\nnames, 201\nfunction parameters as references, 213-218\nfurther reading on, 224\nimmutability and, 221\noverview of, 223\nshallow copies, 208-211\nSoapbox discussion, 225\nvariables as labels versus boxes, 202-204\nobjects\ncallable objects, 237-239, 598-599\nfirst-class, 231\nflexible object creation, 843-845\nmutable, 225\ntreating functions like, 232-234\nuser-defined callable objects, 239\noperator module, 243-247\noperator overloading\naugmented assignment operators, 580-585\nbasics of, 562\nfurther reading on, 587\ninfix operator method names, 576\ninfix operators, 561\noverloading * for scalar multiplication,\n572-574\noverloading + for vector addition, 566-572\noverview of, 585\nrich comparison operators, 577-580\nsignificant changes to, 562\nSoapbox discussion, 588\ntopics covered, 562\nunary operators, 563-566\nusing @ as infix operator, 574-576\nOptional type, 270\nOR-patterns, 686\nOrderedDict, 85, 95\nos functions, str versus bytes in, 156\noverloaded signatures, 520-526\noverriding descriptors, 855, 863, 892-898\nP\nparallelism, 695, 697\nparameterized decorators, 329-335\nparameterized types, 544\nparameters\nannotating positional only and variadic\nparameters, 295\nintrospection of function parameters, 232\nkeyword-only, 242\nmutable, 214-218\nparameter passing, 213\npositional, 240-243\npattern matching\n*_ symbol, 42\ndestructuring, 40\nin lis.py interpreter, 43-47, 669-687\nwith mappings, 81-83\nmatch/case statement, 38\npattern matching class instances, 192-195\ntuples and lists, 41\ntype information, 42\n_ symbol, 41\npatterns (see functions, design patterns with\nfirst-class; pattern matching)\npickle module, 97\nPingo library, 99\npipe (ǀ) operator, 80, 686\npipe equals (ǀ=) operator, 80\nplain text, 129, 161\n.pop method, 67\npositional class patterns, 194\npositional parameters, 240-243\npositional patterns, 377\nprocess pools\ncode for multicore prime checker, 719-723\nexample problem, 716\nprocess-based solution, 718\nthread-based nonsolution, 724\nunderstanding elapsed times, 718\nvarying process numbers, 723\nprocesses\ndefinition of term, 698\nlaunching with concurrent.futures, 754-758\nprogress displays, 762-772\nProject Jupyter, 727\nproper tail calls (PTC), 691-693\nIndex \n| \n973",
      "content_length": 2439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1004,
      "chapter": null,
      "content": "properties (see computed properties; dynamic\nattributes and properties)\nproperty class, 860-865\nprotocol classes, 403\nProtocol type, 286-291\nprotocols (see also interfaces)\ndefensive programming, 440-442\nduck typing and, 402\nfurther reading on, 482\nimplementing at runtime, 438-440\nimplementing generic static protocols,\n552-554\nas informal interfaces, 427\nmeanings of protocol, 434\nnumeric, 478-481\noverview of, 481\nsequence and iterable protocols, 435-437\nsignificant changes to, 433\nSoapbox discussion, 484-486\nstatic protocols, 403, 466-481\ntopics covered, 433\nPSF (see Python Software Foundation)\nPUG (see Python Users Group)\nPyICU, 150\nPyLadies, 960\npytest package, xxii\nPython\nappreciating language-specific features, xix\napproach to learning, xx-xxii\ncommunity support for, 959\nfluentpython.com, xxiii\nfunctional programming with, 250\nfunctioning with multicore processors,\n725-733\nfurther reading on, 960\nprerequisites to learning, xx\ntarget audience, xx\nversions featured, xx\nPython Data Model\nfurther reading on, 18\n__getitem__ and __len__, 5-8\nmaking len work with custom objects, 17\noverview of, 3, 18\nsignificant changes to, 4\nSoapbox discussion, 19\nspecial methods overview, 15-17\nusing special methods, 8-15\nPython Software Foundation (PSF), 960\nPython type checkers, 255\nPython Users Group (PUG), 960\npython-tulip list, 959\nPythonic Card Deck example, 5-8\nPythonic objects (see also objects)\nalternative constructor for, 368\nbuilding user-defined classes, 363\nclassmethod versus staticmethod, 369\nformatted displays, 370-374\nfurther reading on, 392\nhashable Vector2d, 374-377\nobject representations, 364\noverriding class attributes, 389-391\noverview of, 391\nprivate and protected attributes, 382-384\nsaving memory with __slots__, 384-388\nsignificant changes to, 364\nSoapbox discussion, 394-396\nsupporting positional patterns, 377\ntopics covered, 363\nVector2d class example, 365-368\nVector2d full listing, 378-381\nPythonic sums, 428-430\npyuca library, 150, 158\nQ\nquantity properties, 865-868\nquestions and comments, xxv\nqueues\ndefinition of term, 698\ndeque (double-ended queue), 59, 67\ndistributed task queues, 732\nimplementing, 69\nR\nrace conditions, 723\nrandom.choice function, 6\nrecycling (see garbage collection)\nreduce function, 234-236\nreducing functions, 412, 630-631\nrefactoring strategies\nchoosing the best, 350\nclassic, 342-346\nCommand pattern, 355-357\ndecorator-enhanced pattern, 353-355\nfinding strategies in modules, 351-353\nfunction-oriented, 347-350\nreference counting, 219\nregistration decorators, 308, 329-332\nregular expressions, str versus bytes in, 155\n974 \n| \nIndex",
      "content_length": 2601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1005,
      "chapter": null,
      "content": "repr() function, 364\nreserved keywords, 679\nrich comparison operators, 577-580\nrunning averages, computing, 643-645\nS\nS-expression, 669\nsalts, 85\nScheme language, 43-47, 669-671\nSciPy, 64-67\nscope\ndynamic scope versus lexical scope, 338-339\nfunction local scope, 310\nmodule global scope, 310\nvariable scope rules, 308-310\nwithin comprehensions and generator\nexpressions, 26\nsemaphores, 790-794\nSentence classes, 603-610\nsentinels, 721\nsequence protocol, 435-437, 594-596\nsequences\nalternatives to lists, 59-70\nfurther reading on, 71\nlist comprehensions and generator expres‐\nsions, 25-30\nlist.sort versus sorted built-in, 56-58\noverview of, 70\noverview of built-in, 22-24\npattern matching with, 38-47\nsignificant changes to, 22\nslicing, 47-50\nSoapbox discussion, 73-75\ntopics covered, 22\ntuples, 30-35\nuniform handling of, 21\nunpacking sequences and iterables, 35-38\nusing + and * with, 50-56\nsequences, special methods for\napplications beyond three dimensions, 398\ndynamic attribute access, 407-411\n__format__, 418-425\nfurther reading on, 426\n__hash__ and __eq__, 411-416\noverview of, 425\nprotocols and duck typing, 402\nsignificant changes to, 398\nsliceable sequences, 403-407\nSoapbox discussion, 427-430\ntopics covered, 397\nVector implementation strategy, 398\nVector2d compatibility, 399-401\nsequential.py program, 716\nserver-side web/mobile development, 728\nservers\nAsynchronous Server Gateway Interface\n(ASGI), 732\nHTTPServer class, 503\nTCP servers, 804-811\ntest servers, 765\nThreadingHTTPServer class, 503\nWeb Server Gateway Interface (WSGI), 730\nwriting asyncio servers, 799-811\nsetattr function, 871\nsets (see also dictionaries and sets)\nconsequences of how set works, 107\nset comprehensions, 106\nset literals, 105\nset operations, 107-110\nset operations on dict views, 110\nset theory, 103-105\nshallow copies, 208-211\nshelve module, 97\nsimple class patterns, 192\nsingle dispatch generic functions, 324-329\nSized interface, 15\nslicing\nassigning to slices, 50\nexcluding last item in, 47\nmultidimensional slicing and ellipses, 49\nslice objects, 48\nsliceable sequences, 403-407\nSoapbox sidebars\n@dataclass, 197\nanonymous functions, 251\n__call__, 360\ncode points, 161\ndata model versus object model, 19\ndesign patterns, 359\nduck typing, 302, 427\ndynamic scope versus lexical scope, 338-339\nequality (==) operator, 225\nexplicit self argument, 905\nflat versus container sequences, 73\n__format__, 428\nfunction-class duality, 876\nfunctional programming with Python, 250\ngeneric collections, 302\nIndex \n| \n975",
      "content_length": 2505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1006,
      "chapter": null,
      "content": "inheritance across languages, 518\ninterfaces, 486\nkey argument, 74\nlis.py and evaluate function, 693\nmagic methods, 19\nmetaobjects, 19\nminimalistic iterator interface, 654\nmixed-bag lists, 74\nmonkey-patching, 486\nmultilevel class hierarchies, 517\nmutability, 225\nnon-ASCII names in source code, 160\nobject destruction and garbage collection,\n226\noperator overloading, 588\nOracle, Google, and the Timbot, 74\nplain text, 161\npluggable generators, 654-656\nprogramming language design, 956\nproper tail calls (PTC), 691-693\nproperties and up-front costs, 394\nprotocols as informal interfaces, 427\nPython decorators and decorator design\npattern, 339\nPythonic sums, 428-430\nsafety versus security in private attributes,\n395\nstatic typing, 484\nsyntactic sugar, 114\nthread avoidance, 773\nthreads-and-locks versus actor-style pro‐\ngramming, 739-741\ntrade-offs of built-ins, 517\ntuples, 73\nTwisted library, 830\ntype hints (type annotations), 299-302\ntyping map, 485\nundocumented classes, 557\nUniform Access Principle, 875-876\nuvloop, 829\nvariance notation in other classes, 558\nwith statements, 691\nsorted function, 56-58\nspecial methods (see also sequences, special\nmethods for)\nadvantages of using, 6\nBoolean values of custom types, 13\ncalling, 8\nCollection API, 14-15\nemulating numeric types, 9-12\n__getitem__ and __len__, 5-8\nnaming conventions, 3\npurpose of, 3\nspecial method names (operators excluded),\n15\nspecial method names and symbols for\noperators, 16\nstring representation, 12\nspinners (loading indicators) (see also network\nI/O)\ncomparing supervisor functions, 711\ncreated using coroutines, 706-710\ncreated with multiprocessing package, 704\ncreated with threading, 701-704\nGlobal Interpreter Lock impact, 713-715\nkeeping alive, 715\nsquare brackets ([]), 6, 26, 35, 49\nstacked decorators, 322\nstar (*) operator, 10, 36-37, 50-56, 240, 572-574\nstar equals (*=) operator, 580-585\nstatic duck typing, 291, 432, 515\nstatic protocols\nbest practices for protocol design, 476\ndefinition of, 435\ndesigning, 474-476\nversus dynamic protocols, 403\nextending, 477\nimplementing generic static protocols,\n552-554\nlimitations of runtime protocol checks, 471\nnumbers ABCS and numeric protocols,\n478-481\nruntime checkable, 468-471\nSoapbox discussion, 484\nsupporting, 472-474\ntype hints (type annotations), 286-291\ntyped double function, 466-468\nstatic typing, 431\nstaticmethod decorator, 369\nstorage attributes, 882\nstr() function, 13, 364\nstr.format() method, 5, 370\nStrategy pattern, 342-346\nstrings\ndefault sorting of, 58\ndual-mode str and bytes APIs, 155-157\nnormalizing Unicode for reliable compari‐\nsons, 140-148\n976 \n| \nIndex",
      "content_length": 2616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1007,
      "chapter": null,
      "content": "representation using special methods, 12\nstrong testing, 296\nstruct module, 118\nstructural typing, 464-466\nstructured concurrency, 823\nsubclassing (see inheritance and subclassing)\nsubgenerators, 632\nsubtype-of relationships, 267\nsuper() function, 411, 488-490\nsyntactic sugar, 114\nSyntaxError, 128\nsystem administration, 726\nT\ntail call optimization (TCO), 691-693\nTCP servers, 804-811\nTensorFlow, 727\ntest servers, 765\ntext files, handling, 131-139 (see also Unicode\ntext versus bytes)\nThreadingHTTPServer class, 503\nThreadingMixIn class, 503\nthreads\ndefinition of term, 698\nenhancing asyncio downloader, 788-797\nfurther reading on, 734\nGlobal Interpreter Lock impact, 714\nspinners (loading indicators) using, 701-704\nthread avoidance, 773\nthread-based process pools, 724\nthrottling, 790-794\nTimsort algorithm, 74\nTkinter GUI toolkit\nbenefits and drawbacks of, 513\nmultiple inheritance in, 507-509\ntree structures, traversing, 634-639\ntuples\nclassic named tuples, 169-172\nimmutability and, 221\nas immutable lists, 32-34\nversus lists, 34\nnature of, 73\nas records, 30-32\nrelative immutability of, 207\nsimplified memory diagram for, 23\ntuple unpacking, 32\ntype hints (type annotations), 274-276\ntyping.NamedTuple, 172\nTwisted library, 830\ntype casting, 534-537\ntype hints (type annotations)\nannotating positional only and variadic\nparameters, 295\nfor asynchronous objects, 824\nbasics of, 173-179\nbenefits and drawbacks of, 253\nflawed typing and strong testing, 296\nfurther reading on, 298\ngeneric type hints for coroutines, 650\ngradual typing, 254-260 (see also gradual\ntype system)\noverview of, 297\nsignificant changes to, 254\nSoapbox discussion, 299-302\nsupported operations and, 261-265\ntopics covered, 254\ntypes usable in, 266-295\ntyped double function, 466-468\nTypedDict, 164, 526-534\nTypeshed project, 280\nTypeVar, 282-286\ntyping map, 432, 485 (see also type hints (type\nannotations))\ntyping module, 266\ntyping.NamedTuple, 172\nU\nUCA (see Unicode Collation Algorithm)\nUCS-2 encoding, 124\nUML class diagrams\nABCs in collections.abc, 449\nannotated with MGN, 882, 943\nCommand design pattern, 355\ndjango.views.generic.base module, 504\ndjango.views.generic.list module, 506\nfundamental collection types, 14\nmanaged and descriptor classes, 880\nMutableSequence ABC and superclasses,\n448\nSequence ABC and abstract classes, 436\nsimplified for collections.abc, 24\nsimplified for MutableMapping and super‐\nclasses, 83\nsimplified for MutableSet and superclasses,\n107\nStrategy design pattern, 342\nTkinter Text widget class and superclasses,\n498\nIndex \n| \n977",
      "content_length": 2549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1008,
      "chapter": null,
      "content": "TomboList, 461\nunary operators, 563-566\nundocumented classes, 557\nUnicode Collation Algorithm (UCA), 150\nUnicode literals escape notation (\\N{}), 136\nUnicode sandwich, 131\nUnicode text versus bytes\nbasic encoders/decoders, 123-124\nbyte essentials, 120-123\ncharacters and Unicode standard, 118-120\ndual-mode str and bytes APIs, 155-157\nfurther reading on, 158\nhandling text files, 131-139\nnormalizing Unicode for reliable compari‐\nsons, 140-148\noverview of, 157\nsignificant changes to, 118\nSoapbox discussion, 160\nsorting Unicode text, 148-150\ntopics covered, 117\nunderstanding encode/decode problems,\n125-131\nUnicode database, 150-155\nUnicodeDecodeError, 126\nUnicodeEncodeError, 125\nUniform Access Principle, 875-876\nUnion type, 270\nunittest module, xxii\nunpacking\niterables and mappings, 240\nmapping unpackings, 80\nnested, 37\nsequences and iterables, 35\nusing * to grab excess items, 36\nwith * in function calls and sequence liter‐\nals, 37\nuser-defined functions, 238\nUserDict, 97-99\nUTF-8 decoding, 129\nUTF-8-SIG encoding, 130\nuvloop, 829\nV\nvariable annotations\nmeaning of, 175-179\nsyntax of, 174\nvariable scope rules, 308-310\nvariables\nfree, 313\ninit-only variables, 186\nas labels versus boxes, 202-204\nlookup logic, 316\nvariadic parameters, 295\nvariance\ncontravariant types, 547\ncovariant types, 546\nin callable types, 292\ninvariant types, 545\noverview of, 549\nrelevance of, 544\nrules of thumb, 551\nvariance notation in other classes, 558\nvars([object]) function, 871\nVector class, multidimensional\napplications beyond three dimensions, 398\ndynamic attribute access, 407-411\n__format__, 418-425\nfurther reading on, 426\n__hash__ and __eq__, 411-416\nimplementation strategy, 398\noverview of, 425\nprotocols and duck typing, 402\nsliceable sequences, 403-407\ntopics covered, 397\nVector2d compatibility, 399-401\nVector2d\nclass example, 365-368\nfull listing, 378-381\nhashable, 374\nvectors\noverloading + for vector addition, 566-572\nrepresenting two-dimensional, 9-12\nvirtual attributes, 835\nvirtual subclasses, 460-463\nW\nWalrus operator (:=), 26\nweak references, 221\nWeb Server Gateway Interface (WSGI), 730\nweb/mobile development, 728\nwith, match, and else blocks\ncontext managers and with blocks, 658-669\nelse clause, 687-689\nfurther reading on, 690\noverview of, 689\npattern matching in lis.py, 669-687\npurpose of with statements, 658\nsignificant changes to, 658\n978 \n| \nIndex",
      "content_length": 2375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1009,
      "chapter": null,
      "content": "Soapbox discussion, 691-693\ntopics covered, 657\nY\nyield from expression, 632-639\nyield keyword, 238, 605-610, 614, 642, 645, 669\nZ\nzero-based indexing, 47\nzip() function, 416\nIndex \n| \n979",
      "content_length": 188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1010,
      "chapter": null,
      "content": "About the Author\nLuciano Ramalho was a web developer before the Netscape IPO in 1995, and\nswitched from Perl to Java to Python in 1998. He joined Thoughtworks in 2015,\nwhere he is a Principal Consultant in the São Paulo office. He has delivered keynotes,\ntalks, and tutorials at Python events in the Americas, Europe, and Asia, and also pre‐\nsented at Go and Elixir conferences, focusing on language design topics. Ramalho is a\nfellow of the Python Software Foundation and cofounder of Garoa Hacker Clube, the\nfirst hackerspace in Brazil.\nColophon\nThe animal on the cover of Fluent Python is a Namaqua sand lizard (Pedioplanis\nnamaquensis), found throughout Namibia in arid savannah and semi-desert regions.\nThe Namaqua sand lizard has a black body with four white stripes running down its\nback, brown legs with white spots, a white belly, and a long, pinkish-brown tail. It is\none of the fastest of the lizards active during the day and feeds on small insects. It\ninhabits sparsely vegetated sand gravel flats. Female Namaqua sand lizards lay\nbetween three to five eggs in November, and these lizards spends the rest of winter\ndormant in burrows that they dig near the base of bushes.\nThe current conservation status of the Namaqua sand lizard is of “Least Concern.”\nMany of the animals on O’Reilly covers are endangered; all of them are important to\nthe world.\nThe cover illustration is by Karen Montgomery, based on a black and white engrav‐\ning from Wood’s Natural History. The cover fonts are Gilroy Semibold and Guardian\nSans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐\ndensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 1011,
      "chapter": null,
      "content": "Learn from experts. \nBecome one yourself.\nBooks | Live online courses  \nInstant Answers | Virtual events\nVideos | Interactive learning\nGet started at oreilly.com. \n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",
      "content_length": 255,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}