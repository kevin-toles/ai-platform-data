{
  "metadata": {
    "title": "Machine Learning Engineering",
    "source_file": "Machine Learning Engineering_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 2-11)",
      "start_page": 2,
      "end_page": 11,
      "summary": "In other words, if you’re looking for opportunities to create innovative ML-based solutions to business problems, you want the discipline called Applied Machine Learning, not Machine Learning Research, so most books won’t suit your needs.\nYou’re looking at one of the few true Applied Machine Learning books out there.\nWhen I created Making Friends with Machine Learning in 2016, Google’s Applied Machine Learning course loved by more than ten thousand of our engineers and leaders, I gave it a very similar structure to the one in this book.\nMachine Learning Engineering - Draft\nOne of my favorite things about this book is how fully it embraces the most important thing you need to know about machine learning: mistakes are possible...and sometimes they hurt.\nThese are hugely important topics in practical machine learning, but they’re so often neglected in other books.\nIf you intend to use machine learning to solve business problems at scale, I’m delighted you got your hands on this book.\nMachine Learning Engineering - Draft\nThere are plenty of good books on machine learning, both theoretical and hands-on.\nFrom a typical machine learning book, you can learn the types of machine learning, major families of algorithms, how they work, and how to build models from data using those algorithms.\nA typical machine learning book is less concerned with the engineering aspects of implementing machine learning projects.\nSuch questions as data collection, storage, preprocessing, feature engineering, as well as testing and debugging of models, their deployment to and retirement from production, runtime and post-production maintenance, are often left outside the scope of machine learning books.\nI assume that the reader of this book understands machine learning basics and is capable of building a model, given a properly formatted dataset using a favorite programming language or a machine learning library.\nIf you don’t feel comfortable applying machine learning algorithms to data and don’t clearly see the diﬀerence between logistic regression, support vector machine, and random forest, I recommend starting your journey with The Hundred-Page Machine Learning Book, and then move to this book.\nThe target audience of this book is data analysts who lean towards a machine learning engineering role, machine learning engineers who want to bring more structure to their work, machine learning engineering students, as well as software architects who happen to deal with models provided by data analysts and machine learning engineers.\nMachine Learning Engineering - Draft\nThis book is a comprehensive review of machine learning engineering best practices and design patterns.\nLike its companion and precursor The Hundred-Page Machine Learning Book, this book is distributed on the “read-ﬁrst, buy-later” principle.\nMachine Learning Engineering - Draft\nThough the reader of this book should have a basic understanding of machine learning, it is still important to start with deﬁnitions, so that we are sure that we have a common understanding of the terms used throughout the book.\nBelow, I repeat some of the deﬁnitions from Chapter 2 of The Hundred-Page Machine Learning Book and also give several new ones.\nMachine Learning Engineering - Draft",
      "keywords": [
        "Machine Learning",
        "Machine Learning Engineering",
        "machine learning book",
        "Applied Machine Learning",
        "Burkov Machine Learning",
        "Learning",
        "Machine",
        "Machine Learning Research",
        "machine learning engineers",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "learning book",
        "book",
        "Hundred-Page Machine Learning",
        "Applied Machine"
      ],
      "concepts": [
        "machine",
        "learning",
        "engineer",
        "engineering",
        "data",
        "books",
        "applied",
        "apply",
        "applying",
        "liked"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 5,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "book",
          "learning book",
          "books",
          "applied machine",
          "page machine"
        ],
        "semantic": [],
        "merged": [
          "book",
          "learning book",
          "books",
          "applied machine",
          "page machine"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30033381023815386,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009607+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 12-19)",
      "start_page": 12,
      "end_page": 19,
      "summary": "For example, the set [0,1] includes such values as 0, 0.0001, 0.25, 0.784, 0.9995, and 1.0.\nMachine learning is a subﬁeld of computer science that is concerned with building al- gorithms that, to be useful, rely on a collection of examples of some phenomenon.\nIn supervised learning, the data analyst works with a collection of labeled examples {(x1,y1),(x2,y2),...,(xN,yN)}.\nA feature vector is a vector in which each dimension j from 1 to D contains a value that describes the example.\nFor instance, if each example x in our collection represents a person, then the ﬁrst feature, x(1), could contain height in cm, the second feature, x(2), could contain weight in kg, x(3) could contain gender, and so on.\nFor all examples in the dataset, the feature at position j in the feature vector always contains the same kind of information.\nIn classiﬁcation, the learning algorithm looks for a line (or, more generally, a hypersurface) that separates examples of diﬀerent classes from one another.\nIn regression, on the other hand, the learning algorithm looks to ﬁnd a line or a hypersurface that closely follows the training examples.\nThe goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector.\nThat output value is a number or a class “the most similar” to the labels seen in the past in the examples with similar values of features.\nIn unsupervised learning, the dataset is a collection of unlabeled examples {x1,x2,...,xN}.\nAgain, x is a feature vector, and the goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practical problem.\nFor example, in clustering, the model returns the ID of the cluster for each feature vector in the dataset.\nFor example, the scientist has a feature vector that is too complex to visualize (it has more than three dimensions).\nIn outlier detection, the output is a real number that indicates how the input feature vector is diﬀerent from a “typical” example in the dataset.\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples.\nThe hope here is that, by using many unlabeled examples, a learning algorithm can ﬁnd (we might say “produce” or “compute”) a better model.\nReinforcement learning is a subﬁeld of machine learning where the machine (called an agent) “lives’’ in an environment and is capable of perceiving the state of that environment as a vector of features.\nAn optimal policy is a function (similar to the model in supervised learning) that takes the feature vector of a state as input and outputs an optimal action to execute in that state.\nNow let’s introduce the common data terminology (such as data used directly and indirectly, raw and tidy data, training and holdout data) and the terminology related to machine learning (such as baseline, hyperparameter, pipeline, and others).\nThe data you will work with in your machine learning project can be used to form the examples x directly or indirectly.\nTo make the data readable by a machine learning algorithm, we have to transform each natural language word into a machine-readable array of attributes, which we call a feature vector.4 Some features in the feature vector may contain the information that distinguishes that speciﬁc word from other words in the dictionary.\nTo create these latter binary features, we may decide to use some dictionaries, lookup tables, gazetteers, or other machine learning models making predictions about words.\nYou could already have noticed that the collection of word sequences is the data used to form training examples directly, while the data contained in dictionaries, lookup tables, and gazetteers is used indirectly: we can use it to extend feature vectors with additional features, but we cannot use it to create new feature vectors.\nRaw data is a collection of entities in their natural form; they cannot always be directly employable for machine learning.\nFor instance, a Word document or a JPEG ﬁle are pieces of raw data; they cannot be directly used by a machine learning algorithm.5\nIn this book, I use the term “attribute” to describe a speciﬁc property of an example, while the term “feature” refers to value x(j) at position j in the feature vector x used by a machine learning algorithm.\nTidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example, as shown in Figure 3.\nHowever, in practice, to obtain tidy data from raw data, data analysts often resort to the procedure called feature engineering, which is applied to the direct and, optionally, indirect data with the goal to transform each raw example into a feature vector x.\nFigure 3: Tidy data: examples are rows and attributes are columns.\nIt’s important to note here that for some tasks, an example used by a learning algorithm can have a form of a sequence of vectors, a matrix, or a sequence of matrices.\nAs I mentioned at the beginning of this subsection, data can be tidy, but still not usable by a particular machine learning algorithm.\nMost machine learning algorithms, in fact, only accept training data in the form of a collection of numerical feature vectors.\nNote that in the academic machine learning literature, the word “example” typically refers to a tidy data example with an optionally assigned label.\nHowever, during the stage of data collection and labeling, which we consider in the next chapter, examples can still be in the raw form: images, texts, or rows with categorical attributes in a spreadsheet.\nOtherwise, assume that examples have the form of feature vectors.\nIn practice, data analysts work with three distinct sets of examples:\nOnce you have got the data in the form of a collection of examples, the ﬁrst thing you do in your machine learning project is shuﬄe the examples and split the dataset into three distinct sets: training, validation, and test.\nThe training set is usually the biggest one; the learning algorithm uses the training set to produce the model.\nThe learning algorithm is not allowed to use examples from the validation or test sets to train the model.\nThe reason to have three sets, and not one, is simple: when we train a model, we don’t want the model to only do well at predicting labels of examples the learning algorithm has already seen.\nWhat we really want is a model that is good at predicting examples that the learning algorithm didn’t see.\nWe need two holdout sets and not one because we use the validation set to 1) choose the learning algorithm, and 2) ﬁnd the best conﬁguration values for that learning algorithm (known as hyperparameters).\nThat is why it’s important to make sure that no information from the validation or test sets is exposed to the learning algorithm.\n7In some literature, the validation set can also be called “development set.” Sometimes, when the labeled examples are scarce, analysts can decide to work without a validation set, as we will see in Chapter 5 in the section on cross-validation.",
      "keywords": [
        "Machine Learning",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Learning",
        "learning algorithm",
        "feature vector",
        "machine learning algorithm",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "data",
        "Machine",
        "feature",
        "Burkov Machine",
        "vector",
        "Supervised Learning"
      ],
      "concepts": [
        "examples",
        "data",
        "model",
        "features",
        "learning",
        "sets",
        "algorithm",
        "capital",
        "capitalized",
        "contains"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 15,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.57,
          "base_score": 0.57,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "vector",
          "feature vector",
          "feature",
          "learning algorithm",
          "examples"
        ],
        "semantic": [],
        "merged": [
          "vector",
          "feature vector",
          "feature",
          "learning algorithm",
          "examples"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3517077509945539,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009677+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 20-27)",
      "start_page": 20,
      "end_page": 27,
      "summary": "In machine learning, a baseline is a simple algorithm for solving a problem, usually based on a heuristic, simple summary statistics, randomization, or very basic machine learning algorithm.\nHyperparameters are inputs of machine learning algorithms or pipelines that inﬂuence the performance of the model.\nFor example, the maximum depth of the tree in the decision tree learning algorithm, the misclassiﬁcation penalty in support vector machines, k in the k-nearest neighbors algorithm, the target dimensionality in dimensionality reduction, and the choice of the missing data imputation technique are all examples of hyperparameters.\nParameters are directly modiﬁed by the learning algorithm based on the training data.\nIn machine learning, the classiﬁcation problem is solved by a classiﬁcation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a\nMachine Learning Engineering - Draft\nThe regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target.\nModel-based learning algorithms use the training data to create a model with parameters learned from the training data.\nA shallow learning algorithm learns the parameters of the model directly from the features of the training examples.\nMost machine learning algorithms are shallow.\ncontrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers.\nWhen we apply a machine learning algorithm to a dataset in order to obtain a model, we talk about model training or simply training.\n1.4 When to Use Machine Learning\nMachine learning is a powerful tool for solving practical problems.\nTrying to solve all problems using machine learning would be a mistake.\nToday, it’s hard to imagine someone trying to solve perceptive problems such as speech, image, and video recognition without using machine learning.\nToday, perceptive problems are eﬀectively solved using machine learning models, such as neural networks.\nIf we need to be able to make predictions of some phenomenon that is not well-studied scientiﬁcally, but examples of it are observable, then machine learning might be an appropriate\nFor example, machine learning can be used to generate personalized mental health medication options based on the patient’s genetic and sensory data.\nIf the number of examples of historical log records is high enough (which is often the case), the machine can learn patterns hidden in logs and be able to make predictions without knowing anything about each process.\nBased on those expressions alone, a machine learning model deployed in a social network can recommend the content or other people to connect with.\nMachine learning is especially suitable for solving problems that you can formulate as a problem with a simple objective: such as yes/no decisions or a single number.\nIn contrast, you cannot use machine learning to build a model that works as a general video game, like Mario, or a word processing software, like Word.\n1.5 When Not to Use Machine Learning\nThere are plenty of problems that cannot be solved using machine learning; it’s hard to characterize all of them.\n1.6 What is Machine Learning Engineering\nA machine learning engineer, in turn, is concerned with sourcing the data from various systems and locations and preprocessing it, programming features, training an eﬀective model that will run in the production environment, coexist well with other production processes, be stable, maintainable, and easily accessible by diﬀerent types of users with diﬀerent use cases.\ndata analyst’s code from rather slow R and Python11 into more eﬃcient Java or C++, scaling this code and making it more robust, packaging the code into an easy-to-deploy versioned package, optimizing the machine learning algorithm to make sure that it generates a model compatible with, and running correctly in, the organization’s production environment.\nOn the other hand, machine learning engineers often execute some of the data analysis tasks, including learning algorithm selection, hyperparameter tuning, and model evaluation.\nWorking on a machine learning project is diﬀerent from working on a typical software engineering project.\nThe engineering project may or may not have a In this book, we, of course, consider engineering projects that machine learning part.\nThe goal of machine learning is a speciﬁcation of what a statistical model receives as input, what it generates as output, and the criteria of acceptable (or unacceptable) behavior of the model.\nGoogle might create multiple machine learning engineering projects to achieve that business objective.\nOverall, a machine learning project life cycle, illustrated in Figure 4, consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\nA model-based machine learning algorithm takes a collection of training examples as input and outputs a model.\nAn instance-based machine learning algorithm uses the entire training",
      "keywords": [
        "machine learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning algorithm",
        "machine learning project",
        "learning",
        "machine",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "learning algorithm",
        "Machine Learning Pipeline",
        "Burkov Machine",
        "model",
        "machine learning models",
        "machine learning engineer"
      ],
      "concepts": [
        "data",
        "model",
        "machine",
        "learning",
        "engineering",
        "engineer",
        "algorithm",
        "problems",
        "feature",
        "training"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 17,
          "title": "",
          "score": 0.858,
          "base_score": 0.708,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.588,
          "base_score": 0.588,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.57,
          "base_score": 0.57,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 9,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "algorithm",
          "learning algorithm",
          "using machine",
          "solved",
          "training"
        ],
        "semantic": [],
        "merged": [
          "algorithm",
          "learning algorithm",
          "using machine",
          "solved",
          "training"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4038477088455015,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009715+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 28-36)",
      "start_page": 28,
      "end_page": 36,
      "summary": "The training data is exposed to the machine learning algorithm, while holdout data isn’t.\nAn unsupervised learning algorithm builds a model that takes a feature vector as input and transforms it into something useful.\nThe data for machine learning must be tidy.\nIn addition to being tidy, most machine learning algorithms require numerical data, as opposed to categorical.\nFeature engineering is the process of transforming data into a form that machine learning algorithms can use.\nIn practice, machine learning is implemented as a pipeline that contains chained stages of data transformation, from data partitioning to missing-data imputation, to class imbalance and dimensionality reduction, to model training.\nParameters of the model are optimized by the learning algorithm based on the training data.\nA shallow learning algorithm trains a model that makes predictions directly from the input features.\nYou should consider using machine learning to solve a business problem when the problem is too complex for coding, the problem is constantly changing, it is a perceptive problem, it is an unstudied phenomenon, the problem has a simple objective, and it is cost-eﬀective.\nThere are many situations when machine learning should, probably, not be used: when explainability is needed, when errors are intolerable, when traditional software engineering is a less expensive option, when all inputs and outputs can be enumerated and saved in a database, and when data is hard to get or too expensive.\nA machine learning project life cycle consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\nBefore a machine learning project starts, it must be prioritized.\nWith machine learning, accurate complexity estimation is rarely possible because of major unknowns, such as whether the required model quality is attainable in practice, how much data is needed, and what, and how many features are necessary.\nFurthermore, a machine learning project must have a well-deﬁned goal.\nIn this chapter, we consider these and related activities that must be taken care of before a machine learning project starts.\n2.1 Prioritization of Machine Learning Projects\nThe key considerations in the prioritization of a machine learning project, are impact and cost.\nThe impact of using machine learning in a broader engineering project is high when, 1) machine learning can replace a complex part in your engineering project or 2) there’s a great beneﬁt in getting inexpensive (but probably imperfect) predictions.\nIf yes, such a machine learning project would have a high impact and low cost.\nThree factors highly inﬂuence the cost of a machine learning project:\ncan data be generated automatically (if yes, the problem is greatly simpliﬁed), • what is the cost of manual annotation of the data (i.e., assigning labels to unlabeled examples),\nThe machine learning project’s cost grows superlinearly with the accuracy requirement, as illus- trated in Figure 1.\n2.2 Estimating Complexity of a Machine Learning Project\nwhether the required quality is attainable in practice, • how much data you will need to reach the required quality, • what features and how many features are necessary so that the model can learn and generalize suﬃciently,\nThe progress of a machine learning project is nonlinear.\n2.3 Deﬁning the Goal of a Machine Learning Project\nThe goal of a machine learning project is to build a model that solves, or helps solve, a business problem.\nAlmost any business problem solvable with machine learning can be deﬁned in a form similar to one from the above list.",
      "keywords": [
        "machine learning project",
        "machine learning",
        "Machine learning engineering",
        "Burkov Machine Learning",
        "learning project",
        "learning",
        "Andriy Burkov Machine",
        "machine",
        "machine learning algorithms",
        "learning engineering",
        "Burkov Machine",
        "model",
        "learning algorithm",
        "data",
        "problem"
      ],
      "concepts": [
        "data",
        "model",
        "learned",
        "cost",
        "problem",
        "engineering",
        "engineers",
        "feature",
        "examples",
        "classes"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 1,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 5,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "project",
          "learning project",
          "data",
          "problem",
          "cost"
        ],
        "semantic": [],
        "merged": [
          "project",
          "learning project",
          "data",
          "problem",
          "cost"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4070461157520114,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009749+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 37-45)",
      "start_page": 37,
      "end_page": 45,
      "summary": "When deﬁning the goal of your machine learning project, you should ﬁnd the right balance between those possibly conﬂicting requirements and translate them into the choice of the model’s input and output, cost function, and performance metric.\nThere are two cultures of structuring a machine learning team, depending on the organization.\nMachine Learning Engineering - Draft\nOne culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers.\nIn such a culture, a software engineer doesn’t need to have deep expertise in machine learning, but has to understand the vocabulary of their fellow data analysts.\nAccording to other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\nA data analyst must be an expert in many machine learning techniques and have a deep understanding of the theory to come up with an eﬀective solution to most problems, fast and with minimal eﬀort.\nBesides machine learning and software engineering skills, a machine learning team may include experts in data engineering (also known as data engineers) and experts in data labeling.\nTypically, data engineers are not expected to know any machine learning.\nIn most big companies, data engineers work separately from machine learning engineers in a data engineering team.\nAgain, in big companies, data labeling experts may be organized in two or three diﬀerent teams: one or two teams of labelers (for example, one local and one outsourced) and a team of software engineers, plus a user experience (UX) specialist, responsible for building labeling tools.\nThey work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model mainte- nance.\nIn big companies, DevOps engineers employed in machine learning projects usually work in a larger DevOps team.\nSome companies introduced the MLOps role, whose responsibility is to deploy machine learning models in production, upgrade those models, and build data processing pipelines involving machine learning models.\nAs of 2020, both data science and machine learning engineering are relatively new disciplines.\nmight exist in an organization don’t have expertise in handling data and machine learning models appropriately.\nThe quality of the data is crucial for a machine learning project’s success.\nIn most machine learning projects, analysts use labeled data.\nThough, according to the same reports, companies that outsource data labeling are more likely to get their machine learning projects up to production.\nData needed for a machine learning project often resides within an organization in diﬀerent places with diﬀerent ownership, security constraints, and in diﬀerent formats.\nEven within one branch of an organization, there are often several teams involved in a machine learning project at diﬀerent stages.\nFor example, the data engineering team provides access to the data or individual features, the data science team works on modeling, ETL or DevOps work on the engineering aspects of deployment and monitoring, while the automation and internal tools teams develop tools and processes for a continuous model update.\nBecause of the high cost of many machine learning projects (due to high expertise and infrastructure cost) some organizations, to “recoup the investment,” might target very ambitious goals: to completely transform the organization or the product or provide unrealistic\nMany machine learning projects start without a clear understanding, by the technical team, of the business objective.\nBefore a machine learning project starts, it must be prioritized, and the team working on the project must be built.\nThe cost of the machine learning project is highly inﬂuenced by three factors: 1) the diﬃculty of the problem, 2) the cost of data, and 3) the needed model performance quality.\nThe goal of a machine learning project is to build a model that solves some business problem.\nThere are two cultures of structuring a machine learning team, depending on the organization.\nOne culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers.\nIn such a culture, a software engineer doesn’t need to have profound expertise in machine learning but has to understand the vocabulary of their fellow data analysts or scientists.\nAccording to the other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\nBesides having machine learning and software engineering skills, a machine learning team may include experts in data labeling and data engineering experts.\nDevOps engineers work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model maintenance.\nlack of experienced talent, • lack of support by the leadership, • missing data infrastructure, • data labeling challenge, • siloed organizations and lack of collaboration, • technically infeasible projects, and • lack of alignment between technical and business teams.",
      "keywords": [
        "machine learning",
        "machine learning project",
        "Machine Learning Team",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "learning",
        "machine",
        "machine learning models",
        "machine learning engineers",
        "data",
        "learning project",
        "Learning Team",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "Burkov Machine"
      ],
      "concepts": [
        "data",
        "team",
        "model",
        "engineering",
        "engineers",
        "project",
        "scientists",
        "labeling",
        "organization",
        "organized"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 1,
          "title": "",
          "score": 0.676,
          "base_score": 0.526,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "team",
          "learning team",
          "engineers",
          "data",
          "culture"
        ],
        "semantic": [],
        "merged": [
          "team",
          "learning team",
          "engineers",
          "data",
          "culture"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29287890484910867,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009778+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 46-54)",
      "start_page": 46,
      "end_page": 54,
      "summary": "Before any machine learning activity can start, the analyst must collect and prepare the data.\nThe data available to the analyst is not always “right” and is not always in a form that a machine learning algorithm can use.\nIn particular, we talk about the properties of good quality data, typical problems a dataset can have, and ways to prepare and store data for machine learning.\n3.1 Questions About the Data\nNow that you have a machine learning goal with well-deﬁned model input, output, and success criteria, you can start collecting the data needed to train your model.\nIs the Data Accessible?\nDoes the data you need already exist?\nDo you need to share the data along with the model?\nDo you need to anonymize data,1 for example, to remove personally identiﬁable infor- mation (PII), during analysis or in preparation for sharing?\nEven if it’s physically possible to get the data you need, don’t work with it until all the above questions are resolved.\nHowever, as we already found out, it’s usually not known how much data is needed to reach your goal, especially if the minimum model quality requirement is stringent.\nFor some projects, you can start with what’s initially available and, while you are working on feature engineering, modeling, and solving other relevant technical problems, new data might gradually come in.\nIt can come in naturally, as the result of some observable or measurable process, or progressively be provided by your data labeling experts or a third-party data provider.\nBy looking at the learning curves, you will see your model’s performance will plateau after you reach a certain number of training examples.\n2Don’t forget, in your estimates, that you need not just training but also holdout data to validate the model performance on the examples it wasn’t trained on.\nThat holdout data also has to be sizeable to provide reliable, in a statistical sense, model quality estimates.\nIf you observe the performance of the learning algorithm plateaued, it might be a sign that collecting more data will not help in training a better model.\nyou used a learning algorithm incapable of training a complex enough model using the data you have.\nHowever, deep neural networks usually require more training data compared to shallow learning algorithms.\nA smaller sample of big data can give good results in practice and accelerate the search for a better model.\nThe data quality is one of the major factors aﬀecting the model performance.\nHowever, if you use this dataset blindly, you might realize that no matter how hard you try to improve the quality of your model, its performance on new data is low.\nThe model trained using such expired data from an older printer generation might perform worse when deployed on the new generation of printers.\nIn practice, data can only become useable for modeling after preprocessing; hence the importance of visual analysis of the dataset before you start modeling.\nIf you don’t preprocess such data by removing the dates, the model can learn the date-topic correlation, and such a model will be of no practical use.\nIs the Data Understandable?\nIn Section 3.2.8, we will consider the problem of data leakage in more detail.\nIs the Data Reliable?\nTherefore delayed labels make our data less reliable.\nthe label is indirect, this also makes such data less reliable.\nA feedback loop is a property in the system design when the data used to train the model is obtained using the model itself.\nAgain, imagine that you work on a problem of predicting whether a speciﬁc user of a website will like the content, and you only have indirect labels – clicks.\nIf the model is already deployed on the website and the users click on links recommended by the model, this means that the new data indirectly reﬂects not only the interest of users to the content, but also how intensively the model recommended that content.\n3.2 Common Problems With Data\nAs we have just seen, the data you will work with can have problems.\nGetting unlabeled data can be expensive; however, labeling data is the most expensive work, especially if the work is done manually.\nNow we need labeled data: “coﬀee house,” “bank,” “grocery,” “drug store,” “gas station,” etc.\nreCAPTCHA thus solves two problems: reducing spam on the Web and providing cheap labeled data to Google.\nWell-designed labeling tools will minimize mouse use (including menus activated by mouse clicks), maximize hotkeys, and reduce costs by increasing the speed of data labeling.\nIf the labeler clicks “Not Sure,” you can save this example to analyze later or simply not use such examples for training the model.\nThen you build the ﬁrst model that works reasonably well, using this initial set of labeled examples.\nRemember that data quality is one of the major factors aﬀecting the performance of the model.\nData quality has two components: raw data quality and labeling quality.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Data",
        "machine learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "model",
        "Burkov Machine",
        "learning",
        "Data Collection",
        "Andriy Burkov",
        "machine",
        "machine learning project",
        "dataset",
        "data quality"
      ],
      "concepts": [
        "data",
        "labeling",
        "model",
        "learning",
        "dataset",
        "problems",
        "quality",
        "engineering",
        "engineers",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.631,
          "base_score": 0.481,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 5,
          "title": "",
          "score": 0.554,
          "base_score": 0.404,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 1,
          "title": "",
          "score": 0.54,
          "base_score": 0.39,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "quality",
          "data quality",
          "reliable",
          "clicks"
        ],
        "semantic": [],
        "merged": [
          "data",
          "quality",
          "data quality",
          "reliable",
          "clicks"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3080821360276387,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009807+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 55-62)",
      "start_page": 55,
      "end_page": 62,
      "summary": "Some common problems with raw data are noise, bias, low predictive power, outdated examples, outliers, and leakage.\nNoise in data is a corruption of examples.\nNoise is more a problem when the dataset is relatively small (thousands of examples or less), because the presence of noise can lead to overﬁtting: the algorithm may learn to model the noise contained in the training data, which is undesirable.\nIn the big data context, on the other hand, noise, if it’s randomly applied to each example independently of other examples in the dataset, is typically “averaged out” over multiple examples.\nA real-life example of selection bias is an image generated by the Photo Upsampling via Latent Space Exploration (PULSE) algorithm that uses a neural network model to upscale (increase the resolution) of images.\nFigure 6: An illustration of the eﬀect the selection bias can have on the trained model.\nThe above example shows that it cannot be assumed that machine learning model is correct, simply because machine learning algorithms are impartial and the trained models are based on data.\nIf the data has a bias, it will most likely be reﬂected in the model.\nSelf-selection bias is a form of selection bias where you get the data from sources that “volunteered” to provide it.\nMost poll data has this type of bias.\nFor example, you want to train a model that predicts the behavior of successful entrepreneurs.\nLet’s say, you want to train a model that predicts whether a book will be liked by the readers.\nOmitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction.\nFor example, let’s assume that you are working on a churn prediction model and you want to predict whether a customer cancels their subscription within six months.\nSampling bias (also known as distribution shift) occurs when the distribution of examples used for training doesn’t reﬂect the distribution of the inputs the model will receive in production.\nOne of the possible reasons is sampling bias: one or two frequent topics in production data might account for 80% of all input.\nUsing a photo archive to train a model that distinguishes men from women might show, for example, men more frequently in work or outdoor contexts, and women more often at home indoors.\nIf we use such biased data, our model will have more diﬃculty recognizing a woman outdoors or a man at home.\nA famous example of this type of bias is looking for associations for words using word embeddings trained with an algorithm like word2vec.\nThis results in a machine learning model making suboptimal predictions when deployed in the production environment.\nFor example, the training data is gathered using a camera with a white balance which makes white look yellowish.\nOn the other hand, if the measurements are consistently skewed in one direction, then it damages training data, and ultimately results in a poor-quality model.\nApplied to machine learning, experimenter bias often occurs when each example in the dataset is obtained from the answers to a survey given by a particular person, one example per person.\nLabeling bias happens when labels are assigned to unlabeled examples by a biased process or person.\nIf the data is a result of some research, question the research method and make sure that it doesn’t contribute to any of the biases described above.\nSelection bias can be avoided by systematically questioning the reason why a speciﬁc data source was chosen.\nTraining the model using only the data about your current customers is likely a bad idea, because your existing customers are more loyal to your brand than a random potential customer.\nAlternatively, let us suspect that a particular variable would be important for accurate predictions, and leaving it out of our model could result in an omitted variable bias.\nSampling bias can be avoided by researching the real proportion of various properties in the data that will be observed in production, and then sampling the training data by keeping similar proportions.\nWhen developing the training model to distinguish pictures of women from men, a data analyst could choose to under-sample the number of women indoors, or oversample the number of men at home.\nIn other words, prejudice or stereotype bias is reduced by exposing the learning algorithm to a more even- handed distribution of examples.\nLabeling bias can be avoided by asking several labelers to identify the same example.\nYou cannot entirely avoid bias in data.\nRecall that there is a temptation among the data analysts to assume that machine learning models are inherently fair because they make decisions based on evidence and math, as opposed to often messy or irrational human judgments.\nThis is, unfortunately, not always the case: inevitably, a model trained on biased data will produce biased results.\nLow predictive power is an issue that you often don’t consider until you have spent fruitless energy trying to train a good model.\nThe model you train with this data will be far from perfect.\nBased on these photos alone, it’s very unlikely that we will be able to train a model that accurately predicts such an event.\nIf you cannot obtain acceptable results, no matter how complex the model becomes, it may be time to consider the problem of low predictive power.\nOnce an erratic behavior is detected, new training data is added to adjust the model; the model is then retrained and redeployed.\nIn such cases, additional training examples will solidify the model.\nThe examples added to the training data in the past no longer reﬂect some user’s preferences and start hurting the model performance, rather than contributing to it.\nConsider it if you see a decreasing trend in model performance on new data.\nCorrect the model by removing the outdated examples from the training data.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "data",
        "machine learning",
        "model",
        "Andriy Burkov Machine",
        "bias",
        "machine learning models",
        "Learning Engineering",
        "Burkov Machine",
        "learning",
        "training data",
        "machine",
        "Andriy Burkov",
        "noise"
      ],
      "concepts": [
        "data",
        "biased",
        "biases",
        "models",
        "examples",
        "features",
        "quality",
        "training",
        "answers",
        "answering"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.482,
          "base_score": 0.482,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.476,
          "base_score": 0.476,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.438,
          "base_score": 0.438,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 32,
          "title": "",
          "score": 0.393,
          "base_score": 0.393,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.377,
          "base_score": 0.377,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bias",
          "data",
          "noise",
          "selection bias",
          "biased"
        ],
        "semantic": [],
        "merged": [
          "bias",
          "data",
          "noise",
          "selection bias",
          "biased"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.291956583818017,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.009835+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 63-71)",
      "start_page": 63,
      "end_page": 71,
      "summary": "Whether to exclude outliers from the training data, or to use machine learning algorithms and models robust to outliers, is debatable.\nIn the big data context, on the other hand, outliers don’t typically have a signiﬁcant inﬂuence on the model.\nFrom a practical standpoint, if excluding some training examples results in better performance of the model on the holdout data, the exclusion may be justiﬁed.\n3.2.8 Data Leakage\nData leakage, also called target leakage, is a problem aﬀecting several stages of the machine learning life cycle, from data collection to model evaluation.\nTraining on contaminated data leads to overly optimistic expectations about the model performance.\n3.3 What Is Good Data\nBut what constitutes good data for a machine learning project?\nBelow we take a look at several properties of good data.\n3.3.1 Good Data Is Informative\nGood data contains enough information that can be used for modeling.\nFor example, if you want to train a model that predicts whether the customer will buy a speciﬁc product, you will need to possess both the properties of the product in question and If you only have the the properties of the products customers purchased in the past.\nIf you have enough training examples, then the model can potentially derive the gender and ethnicity from the name and make diﬀerent predictions for men, women, locations, and ethnicities, but not to each customer individually.\nGood data has good coverage of what you want to do with the model.\nFor example, if you’re going to use the model to classify web pages by topic and you have a thousand topics of interest, then your data has to contain examples of documents on each of the thousand topics in quantity suﬃcient for the algorithm to be able to learn the diﬀerence between topics.\nIf the algorithm decides to use IDs to separate these couple examples from the rest of the dataset, then the learned model will not be able to generalize: it will not see any of those IDs ever again.\nGood data reﬂects real inputs that the model will see in production.\n3.3.4 Good Data Is Unbiased\nStill, bias can be present in both the data you use for training and the data that the model is applied to in the production environment.\nGood data is not a result of the model itself.\nFor example, you cannot train a model that predicts the gender of a person from their name, and then use the prediction to label a new training example.\n3.3.6 Good Data Has Consistent Labels\nGood data has consistent labels.\n3.3.7 Good Data Is Big Enough\nNo matter how much data you throw on the learning algorithm: the information contained in the data has low predictive power for your problem.\n3.3.8 Summary of Good Data\nInteraction data is the data you can collect from user interactions with the system your model supports.\nYou are considered lucky if you can gather good data from interactions of the user with the system.\nLet’s discuss the three most frequent causes of data leakage that can happen during data collection and preparation: 1) target being a function of a feature, 2) feature hiding the target, and 3) feature coming from the future.\nAn example of such data is shown in Figure 9.\nIf you don’t do a careful analysis of each attribute and its relation to GDP, you might let a leakage happen: in the data in Figure 9, two columns, Population and GDP per capita, multiplied, equal GDP.\nThe fact that you let GDP be one of the features, though in a slightly modiﬁed form (devised by the population), constitutes contamination and, therefore, leads to data leakage.\nIn this scenario, you use customer data to predict their gender.\nIf the data about a customer’s gender and age is factual (as opposed to being guessed by another model that might be available in production), then the column Group constitutes a form of data leakage, when the value you want to predict is “hidden” in the value of a feature.\nImagine a client asked you to train a model that predicts whether a borrower will pay back the loan, based on attributes such as age, gender, education, salary, marital status, and so on.\nAn example of such data is shown in Figure 11.\nIf you don’t make an eﬀort to understand the business context in which your model will be used, you might decide to use all available attributes to predict the value in the column Will Pay Loan, including the data from the column Late Payment Reminders.\nIf in your training data, you have positional features for each news item served in the past (e.g., the x − y position of the title, and the abstract block on the webpage), such information will not be available on the serving time, because you don’t know the positions of articles on the page before you rank them.\nUnderstanding the business context in which the model will be used is, thus, crucial to avoid data leakage.\nof the ﬁrst chapter, in practical machine learning, we typically use three disjoint sets of examples: training set, validation set, and test set.\nThe training set is used by the machine learning algorithm to train the model.\nThe examples in the test set are your best representatives of the production data.\nIn older literature (pre-big data), you might ﬁnd the recommended splits of either 70%/15%/15% or 80%/10%/10% (for training, validation, and test sets, respectively, in proportion to the entire dataset).\nThat makes it wasteful only to use 70% or 80% of the available data for training.\nThe validation and test data are only used to calculate statistics reﬂecting the performance of the model.\nDeep learning models tend to signiﬁcantly improve when exposed to more training data.\nA small dataset of less than a thousand examples would do best with 90% of the data used for training.\nIt’s worth mentioning that when you split time-series data into the three datasets, you must execute the split so that the order of observations in each example is preserved during the shuﬄing.\nAs you already know, data leakage may happen at any stage, from data collection to model evaluation.\nIf you apply the partitioning technique discussed above (shuﬄe, then split), images of the same patient might appear in both the training and holdout data.\nThe model would remember that patient A’s brain has speciﬁc brain convolutions, and if they have a speciﬁc disease in the training data, the model successfully predicts this disease in the validation data by recognizing patient A from just the brain convolutions.\nremoving the examples with missing attributes from the dataset (this can be done if your dataset is big enough to safely sacriﬁce some data);",
      "keywords": [
        "Good Data",
        "data",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Data Leakage",
        "model",
        "machine learning",
        "Andriy Burkov Machine",
        "Good",
        "Learning Engineering",
        "Good Data Reﬂects",
        "Burkov Machine",
        "learning",
        "training data",
        "Good interaction data"
      ],
      "concepts": [
        "data",
        "model",
        "good",
        "examples",
        "learn",
        "time",
        "feature",
        "label",
        "prediction",
        "predicts"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 1,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 5,
          "title": "",
          "score": 0.543,
          "base_score": 0.393,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "good data",
          "good",
          "leakage",
          "data leakage"
        ],
        "semantic": [],
        "merged": [
          "data",
          "good data",
          "good",
          "leakage",
          "data leakage"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3341424265825411,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009865+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 72-81)",
      "start_page": 72,
      "end_page": 81,
      "summary": "3.7.1 Data Imputation Techniques\nTo impute the value of a missing numerical attribute, one technique consists in replacing the missing value by the average value of this attribute in the rest of the dataset.\nLet j be an attribute that is missing in some examples in the original dataset, and let S(j) be the set of size N(j) that contains only those examples from the original dataset in which the value of the attribute j is present.\nwhere N(j) < N and the summation is made only over those examples where the value of the attribute j is present.\nAn illustration of this technique is given in Figure 13, where two examples (at row 1 and 3) have the Height attribute missing.\nFor example, if the regular range is [0,1], you can set the missing value to 2 or −1; if the attribute is categorical, such as days of the week, then a missing value can be replaced by the value “Unknown.” Here, the learning algorithm learns what to do when the attribute has a value diﬀerent from regular values.\nFor example, if the range for an attribute is [−1,1], you can set the missing value to be equal to 0.\nOf course, to build training examples (ˆx, ˆy), you only use those examples from the original dataset, in which the value of attribute j is present.\nLet’s say that examples in your dataset are D-dimensional, and attribute at position j = 12 has missing values.\nFor each example x, you then add the attribute at position j = D + 1, which is equal to 1 if the value of the attribute at position 12 is present in x and 0 otherwise.\nAt prediction time, if your example is not complete, you should use the same data imputation technique to ﬁll the missing values as the technique you used to complete the training data.\nUsing all available examples, you contaminate the training data with information obtained from the validation and test examples.\nFor some types of data, it’s quite easy to get more labeled examples without additional labeling.\nIn Figure 14, you can see examples of operations that can be easily applied to a given image to obtain one or more new images: ﬂip, rotation, crop, color shift, noise addition, perspective change, contrast change, and information loss.\nFigure 14: Examples of data augmentation techniques.\nIn practice, the data augmentation techniques are applied to the original data on-the-ﬂy during training.\nOne can use BERT to generate k most likely predictions for a masked word and then use them as synonyms for data augmentation.\nAnother useful text data augmentation technique is back translation.\nSo, if 60% examples belong to one class and 40% belong to the other, and you use a popular machine learning algorithm in its standard formulation, it should not cause any signiﬁcant performance degradation.\nBy making multi- ple copies of minority class examples, it increases their weight, as illustrated in Figure 15a.\nYou might also create synthetic examples by sampling feature values of several examples of the minority class and combining them to obtain a new example of that class.\nTwo popular algo- rithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\nFor a given example xi of the minority class, they pick k nearest neighbors.\nLet’s denote this set of k examples as Sk. The synthetic example xnew is deﬁned as xi + λ(xzi − xi), where xzi is an example of the minority class chosen randomly from Sk. The interpolation hyperparameter λ is an arbitrary number in the range [0,1].\nIn ADASYN, the number of synthetic examples generated for each xi is proportional to the number of examples in Sk, which are not from the minority class.\nAn opposite approach, undersampling, is to remove from the training set some examples of the majority class (Figure 15b).\nThe undersampling can be done randomly; that is, the examples to remove from the majority class can be chosen at random.\nAlternatively, examples to withdraw from the majority class can be selected based on some property.\nIn Figure 17, you can see how removing examples from the majority class based on Tomek links helps to establish a clear margin between examples of two classes.\nDecide on the number of examples you want to have in the majority class resulting from undersampling.\nRun a centroid-based clustering algorithm on the majority examples only with k being the desired number of clusters.\nThen replace all examples in the majority classes with the k centroids.\n3.10 Data Sampling Strategies\nSimilarly, when you undersample the majority class to adjust for data imbalance, the smaller data sample should be representative of the entire majority class.\nprobability sampling, all examples have a chance to be selected.\nThis means that some examples don’t have a chance of being selected, no matter how many samples you build.\nThe main drawback of nonprobability sampling methods is that they include non-representative samples and might systematically exclude important examples.\nSimple random sampling is the most straightforward method, and the one I refer to when I say “sample randomly.” Here, each example from the entire dataset is chosen purely by chance; each example has an equal chance of being selected.\nOne way of obtaining a simple random sample is to assign a number to each example, and then use a random number generator to decide which examples to select.\nFor example, if your entire dataset contains 1000 examples, tagged from 0 to 999, use groups of three digits from the random number generator to select an example.\nA disadvantage of simple random sampling is that you may not select enough examples that would have a particular property of interest.\nAn advantage of the systematic sampling over the simple random sampling is that it draws examples from the whole range of values.\nHowever, if the list of examples is randomized, then systematic sampling often results in a better sample than simple random sampling.\nIf you know about the existence of several groups (e.g., gender, location, or age) in your data, you should have examples from each of those groups in your sample.\nIn stratiﬁed sampling, you ﬁrst divide your dataset into groups (called strata) and then randomly select examples from each stratum, like in simple random sampling.\nThis technique is also useful to choose the unlabeled examples to send for labeling to a human labeler.\nChoose examples carefully, so that each stratum or cluster is represented in our labeled data.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Data",
        "Machine Learning",
        "Learning Engineering",
        "Data Augmentation",
        "Burkov Machine",
        "data augmentation techniques",
        "sampling",
        "Simple Random Sampling",
        "attribute",
        "Andriy Burkov",
        "learning",
        "data imputation technique"
      ],
      "concepts": [
        "data",
        "examples",
        "sampling",
        "sample",
        "techniques",
        "randomly",
        "randomized",
        "classes",
        "dataset",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 17,
          "title": "",
          "score": 0.423,
          "base_score": 0.273,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.385,
          "base_score": 0.385,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.337,
          "base_score": 0.337,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.327,
          "base_score": 0.327,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "examples",
          "sampling",
          "attribute",
          "class",
          "majority"
        ],
        "semantic": [],
        "merged": [
          "examples",
          "sampling",
          "attribute",
          "class",
          "majority"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24423673104522492,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009891+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 82-90)",
      "start_page": 82,
      "end_page": 90,
      "summary": "3.11.1 Data Formats\nData for machine learning can be stored in various formats.\nData used indirectly, such as dictionaries or gazetteers, may be stored as a table in a relational database, a collection in a key-value store, or a structured text ﬁle.\nIn addition to general-purpose formats, certain popular machine learning packages use proprietary data formats to store tidy data.\nThe data in the LIBSVM format consists of one ﬁle containing all examples.\n3.11.2 Data Storage Levels\nBefore deciding how and where to store the data, it’s essential to choose the appropriate storage level.\nStorage can be organized in diﬀerent levels of abstraction: from the lowest level, the ﬁlesystem, to the highest level, such as data lake.\nThe fundamental unit of data on that level is a ﬁle.\nThe simplicity of ﬁlesystem-level storage and support for standard protocols allows you to store and share data with a small group of colleagues with minimal eﬀort.\nFilesystem-level storage is a cost-eﬀective option for archiving data, thanks to the availability and accessibility of scale-out NAS solutions.\nParallel access to the data on the ﬁlesystem level is fast for retrieval access but slow for storage, so it’s an appropriate storage level for smaller teams and data.\nThe fundamental unit of data in an object storage level is an object.\nThe access to the data stored on the object storage level can often be done in parallel, but the access is not as fast as on the ﬁlesystem level.\nThe database level of data storage allows persistent, fast, and scalable storage of structured data with fast parallel access for both storage and retrieval.\nThe fundamental unit of data at this level is a row.\nThey all support SQL (Structured Query Language), an interface for accessing and modifying data stored in the databases, as well as creating, modifying, and erasing databases.10\n3.11.3 Data Versioning\nVersioning the data is also needed if you frequently update the model by collecting more data, especially in an automated way.\nSometimes, after an update of the data, the new model performs worse, and you would like to investigate why by switching from one version of the data to another.\nData versioning is also critical in supervised learning when the labeling is done by multiple labelers.\nData versioning can be implemented in several levels of complexity, from the most basic to the most elaborate.\nLevel 0: data is unversioned.\nAt this level, data may reside on a local ﬁlesystem, object storage, or in a database.\nA deployed machine learning model is a mix of code and data.\nIf the code is versioned, the data must be too.\nLevel 1: data is versioned as a snapshot at training time.\nAt this level, data is versioned by storing, at training time, a snapshot of everything needed to train a model.\nLevel 2: both data and code are versioned as one asset.\nAt this level of versioning, small data assets, such as dictionaries, gazetteers, and small\nLarge ﬁles are stored in object storage, such as S3 or GCS, with unique IDs. The training data is stored as JSON, XML, or another standard format, and includes relevant metadata such as labels, the identity of the labeler, time of labeling, the tool used to label the data, and so on.\nThe version of the dataset is deﬁned by the git signatures of the code and the data ﬁle.\nLevel 3: using or building a specialized data versioning solution.\nthe details of train-validation-test splits, • details of all pre-processing steps, • an explanation of any data that were excluded, • what format is used to store the data, • types of attributes or features (which values are allowed for each attribute or feature), • number of examples,\nSome data can be stored indeﬁnitely.\nData can be stored in diﬀerent data formats and on several data storage levels.\nData versioning is a critical element in supervised learning when the labeling is done by multiple labelers.\nData versioning can be implemented with several levels of complexity, from the most basic to the most elaborate: unversioned (level 0), versioned as a snapshot at training time (level 1), versioned as one asset containing both data and code (level 2), and versioned by using or building a specialized data versioning solution (level 3).\nIt must also contain an explanation of any data that were excluded, what format is used to store the data, types of attributes or features, number of examples, and possible values for labels or the allowable range for a numerical target.",
      "keywords": [
        "data",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "Andriy Burkov Machine",
        "Data Versioning",
        "Burkov Machine",
        "level",
        "Learning Engineering",
        "Data Storage Levels",
        "learning",
        "Storage",
        "machine",
        "data asset",
        "data stored"
      ],
      "concepts": [
        "data",
        "levels",
        "formats",
        "store",
        "stored",
        "values",
        "machines",
        "labeled",
        "storage",
        "samples"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.404,
          "base_score": 0.404,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.401,
          "base_score": 0.401,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.378,
          "base_score": 0.378,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.353,
          "base_score": 0.353,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.353,
          "base_score": 0.353,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "level",
          "data",
          "storage",
          "stored",
          "versioning"
        ],
        "semantic": [],
        "merged": [
          "level",
          "data",
          "storage",
          "stored",
          "versioning"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28467691009183715,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.009918+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 91-98)",
      "start_page": 91,
      "end_page": 98,
      "summary": "4 Feature Engineering\nAfter data collection and preparation, feature engineering is the second most important activity in machine learning.\nFeature engineering is a process of ﬁrst conceptually and then programmatically transforming a raw example into a feature vector.\n4.1 Why Engineer Features\nMachine learning algorithms can only apply to feature vectors.\n4.2 How to Engineer Features\n4.2.1 Feature Engineering for Text\nWhen it comes to text, scientists and engineers often use simple feature engineering tricks.\nBag-of-words is a generalization of applying the one-hot encoding technique to text data.\nInstead of representing one attribute as a binary vector, you use this technique to represent an entire text document as a binary vector.\nA classiﬁcation learning algorithm expects inputs to be labeled feature vectors, so you have to transform the text document collection into a feature vector collection.\nLet’s use a simple tokenizer that extracts words and ignores everything else.\nWe transform our collection into a collection of binary feature vectors, as shown below:\nFigure 6: Feature vectors.\nFor instance, document 1 “Love, love is a verb” is represented by the following feature vector:\nUse the corresponding labeled feature vectors as the training data, which any classiﬁcation learning algorithm can work with.\nThere are several bag-of-words “ﬂavors.” The above binary-value model often works well.\nIf you use the counts of words, then the feature value for “love” in Document 1 “Love, love is a verb” would be 2, representing the number of times the word “love” appears in the document.\nBy mixing all n-grams, up to a certain n, with tokens in one dictionary, we obtain a bag of n-grams that we can tokenize the same way as we deal with a bag-of-words model.\nBecause sequences of words are often less common than individual words, using n-grams creates a more sparse feature vector.\nFor example, the expressions “this movie was not good and boring” and “this movie was good and not boring” have opposite meaning, but would result in the same bag-of-words vectors, based solely on words.\nFeature vectors only work when certain rules are followed.\nOne rule is that a feature at position j in a feature vector must represent the same property in all examples in the dataset.\nEach feature represents the same property of a document: whether a speciﬁc token is present or absent in a document.\nAnother rule is that similar feature vectors must represent similar entities in the dataset.\nTwo identical documents will have identical feature vectors.\nLikewise, two texts regarding the same topic will have higher chances to have similar feature vectors, because they will share more words than those of two diﬀerent topics.\nFirst, the sample mean of the label is calculated using all examples where the feature has value z.\nEach value z of the categorical feature is then replaced by that sample mean value.\nIf you work on a binary classiﬁcation problem, in addition to sample mean, you can use other useful quantities: the raw counts of the positive class for a given value of z, the odds ratio, and the log-odds ratio.\nIn application to quantifying a categorical feature, we can calculate the odds ratio between the value z of a categorical feature (event A) and the positive label (event B).\nLet’s assume that we have a labeled dataset of email messages, and we engineered a feature that contains the most frequent word in each email message.\nLet us ﬁnd the numerical value that would replace the categorical value “infected” of this feature.\nNow you can replace the value “infected” in the above categorical feature with the value of 2.2.\nYou can proceed the same way for other values of that categorical feature and convert all of them into log-odds ratio values.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "Andriy Burkov Machine",
        "Feature",
        "Feature Engineering",
        "Learning Engineering",
        "document",
        "Burkov Machine",
        "feature vectors",
        "learning",
        "Features Feature engineering",
        "machine",
        "machine learning algorithm",
        "Engineering"
      ],
      "concepts": [
        "words",
        "feature",
        "values",
        "tokenization",
        "document",
        "documents",
        "examples",
        "vector",
        "learning",
        "let"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 12,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.743,
          "base_score": 0.593,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 15,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "vectors",
          "feature vectors",
          "love",
          "words"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "vectors",
          "feature vectors",
          "love",
          "words"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3393657358481454,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009948+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 99-107)",
      "start_page": 99,
      "end_page": 107,
      "summary": "Let p denote the integer value of our cyclical feature.\nFeature hashing, or hashing trick, converts text data, or categorical attributes with many values, into a feature vector of arbitrary dimensionality.\nOne-hot encoding and bag-of-words have a drawback: many unique values will create high-dimensional feature vectors.\nFor example, if there are one million unique tokens in a collection of text documents, bag-of-words will produce feature vectors that each have a dimensionality of one million.\nThen, using a hash function, you ﬁrst convert all values of your categorical attribute (or all tokens in your collection of documents) into a number, and then you convert this number into an index of your feature vector.\nLet’s illustrate how it would work for converting a text “Love is a doing word” into a feature vector.\nTopic modeling is a family of techniques that uses unlabeled data, typically in the form of natural language text documents.\nThe model learns to represent a document as a vector of topics.\nThen, each document could be represented as a ﬁve-dimensional feature vector, one dimension per topic:\nThe above feature vector represents a document that mixes two major topics: politics (with a weight of 0.5) and ﬁnance (with a weight of 0.3).\ndef get_features(self, new_docs):\n# Get topic-based features for new documents new_vectors = self.TF_IDF.transform(new_docs) return self.LSA_model.transform(new_vectors)\n# Later, in production, instantiate LSA model docs = [\"This is a text.\", \"This another one.\"] LSA_featurizer = LSA(docs)\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LSA_features = LSA_featurizer.get_features(new_docs)\nget_features <- function(LSA_model, new_docs){\n# Later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LSA_features <- get_features(LSA_fit, new_docs)\n# Convert documents to TF-IDF vectors self.TF = CountVectorizer() self.TF.fit(docs) vectors = self.TF.transform(docs) # Build the LDA topic model self.LDA_model = LatentDirichletAllocation(n_components=50) self.LDA_model.fit(vectors) return\ndef get_features(self, new_docs):\n# Get topic-based features for new documents new_vectors = self.TF.transform(new_docs) return self.LDA_model.transform(new_vectors)\n# Later, in production, instantiate LDA model docs = [\"This is a text.\", \"This another one.\"] LDA_featurizer = LDA(docs)\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LDA_features = LDA_featurizer.get_features(new_docs)\n# Generate feature for new_docs by using LDA_model get_features <- function(LDA_mode, new_docs){\n# new_docs can be passed as tm::Corpus object or as a vector # holding character strings representing documents: if(!inherits(new_docs, \"Corpus\")) new_docs <- VCorpus(VectorSource(new_docs)) new_dtm <- DocumentTermMatrix(new_docs, control = list(weighting = weightTf)) posterior(LDA_mode, newdata = new_dtm)$topics\n# later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LDA_features <- get_features(LDA_fit, new_docs)\n4.2.6 Features for Time-Series\nTime-series data is diﬀerent from the traditional supervised learning data, which has a form of unordered collections of independent observations.\nAn example of a time-series data is given in Figure 10.\nFigure 10: An example of time-series data in the form of an event stream.\nIf observations are irregular, such time-series data is called a point process or an event stream.\nIt’s usually possible to convert an event stream into the classical time-series data by aggregating observations.\nBy applying the AVERAGE operator to the event stream data in Figure 10, we obtain the classical time-series data shown in Figure 11.\nWhile it’s possible to directly work with event streams, bringing time series to the classical form makes it simpler to apply further aggregations and generate features for machine learning.\nTo transform a time-series into training data in the form of feature vectors, two decisions must be made:\nhow to convert a sequence of observations into a ﬁxed-dimensionality feature vector.\nHowever, some recipes work for many time-series data.\nWords and characters are usually represented as embedding vectors; the latter are learned from large corpora of text documents.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Andriy Burkov Machine",
        "Machine Learning",
        "docs",
        "feature",
        "Burkov Machine",
        "feature vector",
        "Learning Engineering",
        "LSA",
        "LDA",
        "Andriy Burkov",
        "Learning",
        "data",
        "vectors"
      ],
      "concepts": [
        "features",
        "value",
        "data",
        "learning",
        "docs",
        "prediction",
        "predict",
        "modeling",
        "topic",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 16,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "new_docs",
          "time series",
          "self",
          "series",
          "get_features"
        ],
        "semantic": [],
        "merged": [
          "new_docs",
          "time series",
          "self",
          "series",
          "get_features"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23755632923845388,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009973+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 108-115)",
      "start_page": 108,
      "end_page": 115,
      "summary": "4.3 Stacking Features\nTo represent such multi-part examples, we ﬁrst transform each part into a feature vector, and then stack the three feature vectors next to one another to obtain the feature vector for the entire example.\n4.3.1 Stacking Feature Vectors\nWe then apply bag-of-words to transform each left context into a binary feature vector.\nNext, collect all extractions and, using bag-of-words, transform each extraction into a binary feature vector.\nThen we collect all the right contexts and apply bag-of-words to transform each right context into a binary feature vector.\nFinally, we concatenate each example, joining the feature vectors of the left context, the extraction, and the right context.\nWe obtain the ﬁnal feature vector that represents the entire example, as shown in Figure 13.\nNote that the three feature vectors (one from each part of the example) are created indepen- dently of one another.\nThis ensures each feature represents the same property from one example to another.\nThis is a very time-eﬃcient way of engineering features, but some problems require more to obtain feature vectors with high enough predictive power.\nWe consider the predictive power of a feature in the next section.\nYou might want to enrich the feature vectors in your movie title classiﬁcation problem with this additional information available from the classiﬁer mA.\nfeature vector ofthe extraction\nfeature vector ofthe left context\nfeature vector ofthe entire example\nfeature vector ofthe right context\nFigure 13: Creating and stacking feature vectors.\nAgain, we concatenate the three partial feature vectors, as shown in Figure 14.\nExamples of such features are:\nAll these additional features, as long as they are numerical, can be concatenated to the feature vector.\n4.4 Properties of Good Features\nFirst of all, a good feature has high predictive power.\nHowever, a feature can also have high or low predictive power.\nPredictive power is a property of the feature with respect to the problem.\nA tweet is short, and a bag-of-words-based feature vector will be sparse.\nThus, we cannot call such features reliable.\nFurthermore, some predictions can become entirely wrong if the value of an important feature is missing.\n4.5 Feature Selection\nNot all features will be equally important for your problem.\nIf the learning algorithm “sees” that some feature has a non-zero value only in a couple of training examples, it is doubtful the algorithm will learn any useful pattern from that feature.\nHowever, if the feature vector is very wide (contains thousands or millions of features), the training time can become prohibitively long.\nTypically, if a feature contains information (e.g., a non-zero value) only for a handful of examples, such a feature could be removed from the feature vector.\nHowever, removing long-tail features often results in faster learning and a better model.\nCutting the long tail is not the only way to select important features and remove less important ones.\nBoruta iteratively trains random forest models and runs statistical tests to identify features as important and unimportant.\nAt the second stage, the values of a certain feature are randomly permuted across examples, and the tests are repeated.\nThe importance of the feature for a single tree is then computed as the diﬀerence between the number of correct classiﬁcations between the original and permuted setting, divided by the number of examples.\nBuild extended training feature vectors, where each original feature is replicated.\nRandomly permute the values of the replicated features across the training examples to remove any correlation between the replicated variables and the target.\nFor each run, compute the importance (z-score) of all original and replicated features.\n– For each original feature, we count and record the number of hits.\n– The number of hits for a feature is the number of runs in which the importance",
      "keywords": [
        "feature",
        "feature vector",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Stacking Features Back",
        "Machine Learning",
        "Stacking Feature Vectors",
        "feature vector ofthe",
        "Feature selection",
        "binary feature vector",
        "Andriy Burkov Machine",
        "Features Back",
        "Learning Engineering",
        "vector",
        "Burkov Machine"
      ],
      "concepts": [
        "features",
        "important",
        "importance",
        "time",
        "examples",
        "training",
        "counts",
        "randomly",
        "randomized",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.692,
          "base_score": 0.542,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 12,
          "title": "",
          "score": 0.683,
          "base_score": 0.533,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 15,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "feature vector",
          "vector",
          "feature vectors",
          "vectors"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "feature vector",
          "vector",
          "feature vectors",
          "vectors"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.259345608080049,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.009998+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 116-123)",
      "start_page": 116,
      "end_page": 123,
      "summary": "For example, we can remove some features from bag-of-words vectors representing natural language texts by excluding the dimensions corresponding to stop words.\nTo further reduce the feature vector dimensionality obtained from the text data, sometimes it’s practical to preprocess the text by replacing infrequent words (e.g., those whose count in the corpus is below three) with the same synthetic token, for example RARE_WORD.\nBinning, also known as bucketing, is a popular technique that allows transforming a numerical feature into a categorical one by replacing numerical values in a speciﬁc range by a constant categorical value.\nIn uniform binning, once the model is deployed in production, if the value of the feature in the input feature vector is below or above the range of any bin, then the closest bin is assigned, which is either the leftmost or the rightmost bin.\nFigure 18: Synthetic features based on sample mean and standard deviation.\n4.6.2 Synthesizing Features from Relational Data\nThe table User already contains two potentially useful features: Gender and Age. We can also create synthetic features using the data from tables Order and Call.\nA typical approach is to compute various statistics from the data coming from multiple rows and use the value of each statistic as a feature.\nIf you want to increase the predictive power of your feature vectors, or when your training set is rather small, you can synthesize additional features that would help in predictions.\nThere are two typical ways to synthesize additional features: from the data, or from other features.\n4.6.3 Synthesizing Features from the Data\nApply k-means clustering to the feature vectors in your training data.\nYou can synthesize even more features by applying diﬀerent clustering algorithms, or by restarting k-means multiple times from randomly chosen starting points.\nIf you have data in abundance, you can train a deep multilayer perceptron model that will learn to cleverly combine the basic unitary features it receives as input.\nIf you don’t have an inﬁnite supply of training examples (often the case in practice), very deep neural networks lose their appeal.7 In the case of smaller to moderately large datasets (the number of training examples varies between a thousand and a hundred thousand), you might prefer to use a shallow learning algorithm and “help” your learning algorithm learn by providing a richer set of features.\nThree typical simple transformations that apply to a numerical feature j in example i are 1) discretization of the feature, 2) squaring the feature, and 3) computing the sample mean and the standard deviation of feature j from k-nearest neighbors of the example i found by using some metric like Euclidean distance or cosine similarity.\nFor example, you can obtain the value of a new feature q in example i, where q > D, by combining the values of features 2 and 6 in the following way: x(q) .\n4.7 Learning Features from Data\nSometimes, useful features can be learned from data.\nWord embeddings are feature vectors that represent words.\nSimilar words have similar feature vectors, where similarity is given by a certain measure, such as cosine similarity.\nWord embeddings are learned from large corpora of text documents.\nA shallow neural network with one hidden layer (called the embedding layer) is trained to predict a word, given its surrounding words, or to predict the surrounding words, given the word in the middle.\nOnce the neural network is trained, the parameters of the embedding layer are used as word embeddings.\nThere are many algorithms to learn word embeddings from data.\nLet’s see how word embeddings are trained by one version of the word2vec algorithm called skip-gram.\nIn word embedding learning, our goal is to build a model that we can use to convert a one-hot encoding of a word into a word embedding.\nThe neural network has to learn to predict the context words of the skip-gram, given the input’s central word.\nThe embedding for a word is given by the parameters of the embedding layer that apply when a one-hot encoded word is given as the input to the model.\nWord embeddings are an eﬀective way of representing natural language texts for use in such neural network architectures as recurrent neural networks (RNN) and convolutional neural networks (CNN) adapted for working with sequences.\nHowever, if you want to use word embeddings for representing variable-length texts for shallow learning algorithms (which require the input feature vectors of ﬁxed dimensions), you would have to apply some aggregation operation to word vectors, such as weighted sum or average.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "features",
        "machine learning",
        "Word Embeddings",
        "WORD",
        "Learning Engineering",
        "Andriy Burkov Machine",
        "learning",
        "Burkov Machine",
        "feature vectors",
        "feature selection",
        "data",
        "Embeddings",
        "machine"
      ],
      "concepts": [
        "feature",
        "words",
        "learn",
        "binning",
        "bins",
        "bin",
        "ways",
        "way",
        "documents",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 12,
          "title": "",
          "score": 0.7,
          "base_score": 0.55,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.59,
          "base_score": 0.44,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 16,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "word",
          "word embeddings",
          "embeddings",
          "features",
          "embedding"
        ],
        "semantic": [],
        "merged": [
          "word",
          "word embeddings",
          "embeddings",
          "features",
          "embedding"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.29400642534286053,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010025+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 124-132)",
      "start_page": 124,
      "end_page": 132,
      "summary": "To obtain an embedding of an image not used for training the model, we send that image (usually represented as three matrices of pixels, one per channel R, G, and B) to the input of the neural network, and then use the output of one of the fully connected layers before non-linearity.\nFor example, if you have a labeled corpus of documents, then you can optimize the embedding dimensionality by minimizing the number of prediction errors made by the classiﬁer trained on that labeled data, where the words in the documents are represented by the embeddings.\nWhen we apply a dimensionality reduction technique to a dataset, we replace all features in the original feature vector with a new vector, of lower dimensionality, and of synthetic features.\nBoth can be speciﬁcally programmed to produce 2D or 3D feature vectors, while in PCA, the algorithm produces D so-called principal components (where D is the dimensionality of your data), and the analyst must pick the ﬁrst two or three principal components as features for visualization.\n4.9 Scaling Features\nFeature scaling is bringing all your features to the same, or very similar, ranges of values or distributions.\nMultiple experiments demonstrated that a learning algorithm applied to scaled features might produce a better model.\nwhere x(j) is an original value of feature j in some example; min(j) and max(j) are, respectively, the minimum and the maximum value of the feature j in the training data.\nA drawback of normalization is that the values max(j) and min(j) are usually outliers, so normalization will “squeeze” the normal feature values into a very small range.\nBefore calculating the scaled value by using one of the above two formulas, the value x(j) of the feature is set (“clipped”) to a if x(j) is below a, or to b if it’s above b.\nwhere µ(j) is the sample mean of the values of feature j.\nStandardization (or z-score normalization) is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution, with µ = 0 and σ = 1, where µ is the sample mean (the average value of the feature, averaged over all examples in the training data) and σ is the standard deviation from the sample mean.\nFeature scaling is usually beneﬁcial to most learning algorithms.\n4.10 Data Leakage in Feature Engineering\nIf you proceed like that, the values of features in the training data will, in part, be obtained by using the examples that belong to the holdout sets.\nAgain, the model will display artiﬁcially better performance than had you divided your data before feature engineering.\nA solution, as you might have guessed, is ﬁrst to split the entire dataset into training and holdout sets, and only do feature engineering on the training data.\nThis also applies when you use mean encoding to transform a categorical feature to a number: split the data ﬁrst and then compute the sample mean of the label, based on the training data only.\n4.11 Storing and Documenting Features\nEven if you plan to train the model right after you ﬁnish engineering features, it’s advised to design a schema ﬁle that provides a description of the features’ expected properties.\nfor each feature:\nfeature {\nfeature {\nfeature {\nfeature {\n4.11.2 Feature Store\nLarge and distributed organizations may use a feature store that allows keeping, document- ing, reusing, and sharing features across multiple data science teams and projects.\nSome real-time machine learning models aren’t based on informative but computationally intensive features.\nThe values of some features might depend on the entire historical dataset unavailable at the service time.\nFor the model to work correctly, each feature must have the same value for the same input data entity, both in the oﬄine (development) and online (production) mode.\nWhen a new input example comes into the production environment, there is no way to know exactly which features need to be recomputed; rather the entire pipeline needs to be run to compute the values of all features needed for prediction.\nIn addition to those attributes in the schema ﬁle, the feature metadata may supply: why the feature was added to the model, how it contributes to generalization, the person’s name in the organization responsible for maintaining the feature’s data source,8 the input type (e.g., numerical, string, image), the output type (e.g., numerical scalar, categorical, numerical vector), whether the feature store must cache the value of the feature, and if yes, for how long.\nFeatures available for online processing must be implemented in such a way that their value can be either: 1) read fast from a cache or a value store or 2) computed in real-time.\nFeatures that can be computed in real-time include, for example, squaring the input number, determining the shape of the word, or doing a search in the organization’s intranet.\nIt will be executed in a runtime environment and applied to the input to compute the feature value.\nA feature store allows data engineers to insert features.\nIn turn, data analysts and machine learning engineers use an API to get feature values which they deem relevant.\nOr, the analyst working on a model oﬄine may want to convert the training data into a collection of feature vectors, and will send to the feature store a batch of inputs.\nWith feature value versioning, the data analyst is able to rebuild the model with the same feature values as those used to train the previous model version.\nIt contains two feature stores, online and oﬄine, whose data is in sync.\nAt Uber, the online feature store is updated frequently, in near real-time, by using the real-time data.\nIn contrast, the oﬄine feature store is updated in batch-mode by using values of some features computed online, as well as with historical data from logs and oﬄine databases.",
      "keywords": [
        "Machine Learning Engineering",
        "feature",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Machine Learning",
        "Feature Store",
        "Learning Engineering",
        "data",
        "Feature Engineering",
        "Burkov Machine",
        "learning",
        "Andriy Burkov",
        "Feature Engineering Data",
        "dimensionality",
        "Embedding Dimensionality"
      ],
      "concepts": [
        "feature",
        "data",
        "values",
        "documents",
        "model",
        "trained",
        "learning",
        "embedding",
        "dimensionality",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.652,
          "base_score": 0.502,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.619,
          "base_score": 0.469,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.558,
          "base_score": 0.408,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.516,
          "base_score": 0.366,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 12,
          "title": "",
          "score": 0.49,
          "base_score": 0.34,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "feature store",
          "features",
          "store",
          "data"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "feature store",
          "features",
          "store",
          "data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2853751620607626,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010053+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 133-142)",
      "start_page": 133,
      "end_page": 142,
      "summary": "At the beginning of the modeling process, try to engineer as many “simple” features as possible.\nA feature is simple when it doesn’t take signiﬁcant time to code.\nWhen replacing an old, non-machine-learning-based algorithm with a statistical model, use the output of the old algorithm as a feature for the new model.\nIf the old algorithm is too slow to be a feature, use the old algorithm’s inputs as features for the new model.\nBy using location ID as a feature, you can add the training examples for one general location, and train the model to behave diﬀerently in other speciﬁc locations.\nUse categorical features with many values (more than a dozen) only when you want the model to have diﬀerent “modes” of behavior that depend on that categorical feature.\nYou might consider using the categorical feature “Country” if you want the model to behave diﬀerently in Russia versus the United States, for otherwise similar inputs.9\nIf you have a categorical feature with many values, but you do not need a model that has several modes depending on that feature, try to reduce the cardinality (i.e., the number of distinct values) of that feature.\nEven if you think that the model must make similar predictions independently of the country, in reality, you might get poor model performance because the distribution of labels in the training data is diﬀerent for diﬀerent countries.\nIf we decide to group or remove some values in the state feature, we might inadvertently destroy the information that would allow the model to distinguish one “Springﬁeld” from another.\nIt is considered a best practice to re-evaluate the model and the features from time to time.\nFeature extractors are the ﬁrst place to look for a problem if the model’s behavior is strange.\nOnce the model is deployed in the production environment, and each time it is loaded, you must rerun feature extractor tests.\nIt is also recommended to perform regular runs of feature extractors on a ﬁxed test data to make sure that the feature value distribution remains the same.\nThe version of the feature extraction code must be in sync with the model’s version and the data used to build it.\nThe feature extraction code must be independent of the remaining code that supports the model.\nIt should be possible to update the code responsible for each feature without aﬀecting other features, the data processing pipeline, or the way the model is called.\n4.12.10 Serialize Together Model and Feature Extractor\nWhen possible, jointly serialize (pickle in Python, RDS in R) the model and the feature extractor object that was used when the model was built.\nIf your production environment doesn’t let you deserialize both the model and the feature extraction code, use the same feature extraction code when you train the model and serve it.\nEven a tiny diﬀerence between the code a data scientist used to train the model, and the optimized code the IT team might have written for the production environment, may result in signiﬁcant prediction error.\nOnce the production code for feature extraction is ready, use it to retrain the model.\nAlways completely retrain the model after any change in the feature extraction code.\n4.12.11 Log the Values of Features\nLog the feature values extracted in production for a random sample of online examples.\nWhen you work on a new version of the model, these values will be useful to control the quality of the training data.\nThey will allow you to compare and ensure that the feature values logged in the production environment are the same as those you observed in the training data.\nFeatures are values extracted from the data entities your model is designed to work with.\nFeatures are organized in feature vectors, and the model learns to perform mathematical operations on those feature vectors to generate the desired output.\nFeature hashing is a way to convert text data, or categorical attributes with many values, into a feature vector of an arbitrary dimensionality.\nIt is important that the distribution of feature values in the training set is similar to the distribution of values the production model will receive.\nFor text, features can be learned from unlabeled data in the form of word and document embeddings.\nWise use of feature selection techniques remove features that don’t contribute to a model’s quality.\nIt is considered best practices to scale features before training the model, store and document features in schema ﬁles or feature stores, and keep code, model, and training data in sync.\nFeature extraction code is one of the most important parts of a machine learning system.\nif a human can label examples without too much eﬀort, math, or complex logic derivations, then you can hope to achieve human-level performance with your model; • if the information needed to make a labeling decision is fully contained in the features, you can expect to have near-zero error;\nOften the machine learning model performance can improve as more labeled data comes in; and,",
      "keywords": [
        "feature",
        "model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "feature extraction code",
        "Andriy Burkov Machine",
        "machine learning model",
        "feature extraction",
        "Burkov Machine",
        "model performance",
        "data",
        "Learning Engineering",
        "learning",
        "learning model performance"
      ],
      "concepts": [
        "feature",
        "modeling",
        "data",
        "values",
        "useful",
        "code",
        "learn",
        "prediction",
        "predictions",
        "predictive"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 14,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 12,
          "title": "",
          "score": 0.542,
          "base_score": 0.392,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 11,
          "title": "",
          "score": 0.526,
          "base_score": 0.376,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 2,
          "title": "",
          "score": 0.496,
          "base_score": 0.346,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 13,
          "title": "",
          "score": 0.479,
          "base_score": 0.329,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feature",
          "code",
          "feature extraction",
          "extraction",
          "extraction code"
        ],
        "semantic": [],
        "merged": [
          "feature",
          "code",
          "feature extraction",
          "extraction",
          "extraction code"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2950900565531319,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010082+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 143-150)",
      "start_page": 143,
      "end_page": 150,
      "summary": "Our model (training)\nOur model (training)\nFigure 2: A model performance compared to a human-performance baseline: (a) the model looks good, so we can decide to regularize it or add more training examples; (b) the model isn’t performing well, so we need to add more features, or increase the model complexity.\nBefore you start working on a predictive model, it is important to establish baseline perfor- mance on your problem.\nA baseline is a model or an algorithm that provides a reference point for comparison.\nIf the value of the performance metric for the machine learning model is better than the value obtained using the baseline, then machine learning provides value.\nIn Figure 2a, the model looks good, so we can decide to regularize it or add more training examples.\nThe baseline is a model or an algorithm that gets an input, and outputs a prediction.\nA baseline doesn’t have to be the result of any learning algorithm.\nIt can be a rule-based or heuristic algorithm, a simple statistic applied to the training data, or something else.\nThe random prediction algorithm makes a prediction by randomly choosing a label from the collection of labels assigned to the training examples.\nThe zero rule algorithm yields a tighter baseline than the random prediction algorithm.\nTo make predictions, the zero rule algorithm uses more information about the problem.\nIn classiﬁcation, the zero rule algorithm strategy is to always predict the class most common in the training set, independently of the input value.\nLet the training data for your classiﬁcation problem contain 800 examples of the positive class, and 200 examples of the negative class.\nThe zero rule algorithm will predict the positive class all the time, and the accuracy (one of the popular performance metrics that we will consider in Section 5.5.2) of the baseline will be 800/1000 = 0.8 or 80%, which is not bad for such a simple classiﬁer.\nAccording to the zero rule algorithm, the strategy for regression is to predict the sample average of the target values observed in the training data.\nIf you work on a standard, so-called classical, prediction problem, you can use a state-of-the-art algorithm found in a popular library such as Python’s scikit-learn.\nFor text classiﬁcation, for example, represent the text as bag-of-words, and then train a support vector machine model with a linear kernel.\nFor example, if the problem is to build a model that predicts whether a given website visitor will like a recommended article, a simple rule-based system could work as follows.\nIt is the data the machine learning algorithm “sees.” The second and third are the holdout sets.\nThe validation set is not seen by the machine learning algorithm.\nThe data analyst uses it to estimate the performance of diﬀerent machine learning algorithms (or the same algorithm conﬁgured with diﬀerent values of hyperparameters) or models when applied to new data.\nThe remaining test set, which is also not seen by the learning algorithm, is used at the end of the project to evaluate and report the performance of the model the best performing on the validation data.\n2. Draw validation and test data from a distribution that looks much like the data you expect to observe once the model is deployed in production.\nThis often happens when the model is frequently updated after production deployment, and new examples are added to the training set.\nThe properties of the data used to train the model, and that of the data used to validate and test it, can diverge over time.\nSome machine learning algorithms, like those you ﬁnd in scikit-learn, accept labels in their natural form: strings.\nIn the case of multiclass classiﬁcation (that is, when the model predicts only one label given an input feature vector), one-hot encoding is typically used to convert labels to\nHowever, some algorithms, like decision tree learning, can naturally work with categorical features.\nIn multi-label classiﬁcation, the model may predict several labels for one input at the same time (for example, an image can contain both a dog and a cat).\nBelow are several questions and answers which may guide you in choosing a machine learning algorithm or model.\nThe most accurate machine learning algorithms and models are so-called “black boxes.” They make very few prediction errors, but it may be diﬃcult to understand, and even harder to explain, why a model or an algorithm made a speciﬁc prediction.\nOtherwise, you would prefer incremental learning algorithms that can improve the model by reading data gradually.\nExamples of such algorithms are Naïve Bayes and the algorithms for training neural networks.\nSome algorithms, including those used for training neural networks and random forests, can handle a huge number of examples and millions of features.\nOthers, like the algorithms for training support vector machines (SVM), can be relatively modest in their capacity.\nHow much time is a learning algorithm allowed to use to build a model, and how often you will need to retrain the model on updated data?\nSome algorithms, such as random forest learning, beneﬁt from multiple CPU cores, so their training time can be signiﬁcantly reduced on a machine with dozens of cores.\nModels like SVMs and linear and logistic regression models, and not-very-deep feedforward neural networks, are extremely fast at prediction time.\nIf you don’t want to guess the best algorithm for your data, a popular way to choose one is by testing several candidate algorithms on the validation set as a hyperparameter.\nselect algorithms based on diﬀerent principles (sometimes called orthogonal), such as instance-based algorithms, kernel-based, shallow learning, deep learning, ensembles; • try each algorithm with 3 − 5 diﬀerent values of the most sensitive hyperparameters (such as the number of neighbors k in k-nearest neighbors, penalty C in support vector machines, or decision threshold in logistic regression);",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "machine learning algorithm",
        "learning algorithm",
        "Andriy Burkov Machine",
        "learning",
        "model",
        "Learning Engineering",
        "algorithm",
        "Burkov Machine",
        "machine",
        "data",
        "image",
        "Training"
      ],
      "concepts": [
        "algorithm",
        "model",
        "learning",
        "data",
        "predictive",
        "prediction",
        "predictions",
        "training",
        "baseline",
        "label"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.858,
          "base_score": 0.708,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.519,
          "base_score": 0.519,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 24,
          "title": "",
          "score": 0.483,
          "base_score": 0.483,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.439,
          "base_score": 0.439,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.437,
          "base_score": 0.437,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "algorithm",
          "baseline",
          "rule",
          "algorithms",
          "zero rule"
        ],
        "semantic": [],
        "merged": [
          "algorithm",
          "baseline",
          "rule",
          "algorithms",
          "zero rule"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3521908337108086,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010114+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 151-158)",
      "start_page": 151,
      "end_page": 158,
      "summary": "A pipeline is a sequence of transformations the training data goes through, before it becomes a model.\nAn example of a pipeline used to train a document classiﬁcation model out of a collection of labeled text documents is shown below:\nModel\n# Define a pipeline pipe = Pipeline([('dim_reduction', PCA()), ('model_training', SVC())])\nWhen the command pipe.predict(new_example) is executed, the input example is ﬁrst transformed into a reduced dimensionality vector using the PCA model.\n5.5 Assessing Model Performance\nThe most common way to get a good model is to compare diﬀerent models by calculating a performance metric on the holdout data.\nRegression and classiﬁcation models are assessed using diﬀerent metrics.\nLet’s ﬁrst consider performance metrics for regression: mean squared error (MSE), median absolute error (MAE), and almost correct predictions error rate (ACPER).\nThe metric most often used to quantify the performance of a regression model is the same as the cost function: mean squared error (MSE), deﬁned as,\nwhere f is the model that takes a feature vector x as input and outputs a prediction, and i, ranging from 1 to N, denotes the index of an example from a dataset.\nA well-ﬁtting regression model predicts values close to the observed data values.\nThe mean model, which always predicts the average of the training data labels, generally would be used if there were no informative features.\nwhere {|f(xi) − yi|}N i=1 to N, on which the evaluation of the model is performed.\nThis will give the value of the ACPER metric for your model.\nThe most widely used metrics to assess a classiﬁcation model are:\nA confusion matrix is a table that summarizes how successful the classiﬁcation model is at predicting examples belonging to various classes.\nOne axis of the confusion matrix is the class that the model predicted; the other axis is the actual label.\nLet’s say, our model predicts classes “spam” and “not_spam”:\nThe above matrix shows that out of 24 actual spam examples, the model correctly classiﬁed 23.\nThe model incorrectly classiﬁed 1 spam example as not_spam.\nSimilarly, out of 568 actual not_spam examples, the model classiﬁed correctly 556 and incorrectly 12 examples (556 true negatives, TN = 556, and 12 false positives, FP = 12).\nFor example, a confusion matrix could reveal that a model trained to recognize diﬀerent species of animals tends to mistakenly predict “cat” instead of “panther,” or “mouse” instead of “rat.” In this case, you can add more labeled examples of these species to help the learning algorithm “see” the diﬀerence between those animals.\nThe confusion matrix is used to calculate three performance metrics: precision, recall, and accuracy.\nPrecision and recall are most frequently used to assess a binary model.\nRecall is the ratio of true positive predictions to the overall number of positive examples:\nTo understand the meaning and the importance of precision and recall for model assessment, it’s useful to think about the prediction problem as the problem of research of documents in a database using a query.\nby tuning hyperparameters to maximize either precision or recall on the validation set; • by varying the decision threshold for algorithms that return prediction scores.\nTo increase precision (at the cost of a lower recall), we can decide that the prediction will be positive only if the score returned by the model is higher than 0.9 (instead of the default value of 0.5).\nEven if precision and recall are deﬁned for binary classiﬁcation, you can also use them to assess a multiclass classiﬁcation model.\nFor example, you would like to avoid situations where the ﬁrst model has a higher precision, when the second model has a higher recall: if it’s the case, which model is better?\nOne way to compare models based on one number is to threshold the minimum acceptable value for one metric, say recall, and then only compare models based on the value of another metric.\nFor example, say you will accept any model whose recall is above 90%.\nThen you will give preference to the model whose precision is the highest (assuming that its recall is above 90%).\nMore generally, F-measure is parametrized with a positive real β, chosen such that recall is considered β times as important as precision:\nAccuracy is a useful metric when errors in predicting all classes are judged to be equally important.\nAccuracy measures the performance of the model for all classes at once, and it conveniently returns a single number.\nFirst, calculate the accuracy of prediction for each class {1,...,C}, and then take an average of C individual accuracy measures.\nPer-class accuracy will not be an appropriate model quality measure for a multiclass clas- siﬁcation problem where many classes have very few examples (roughly, less than a dozen examples per class).\nIn that case, the accuracy values obtained for the binary classiﬁcation problems corresponding to these minority classes will not be statistically reliable.\nThe advantage of this metric over accuracy is that Cohen’s kappa tells you how much better your classiﬁcation model is performing, compared to a classiﬁer that randomly guesses a class according to the frequency of each class.\nValues of 0 or less indicate that the model has a problem.\nROC curves use a combination of the true positive rate (deﬁned exactly as recall) and false positive rate (the proportion of negative examples predicted incorrectly) to build up a summary picture of the classiﬁcation performance.\nFor example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves.\nThen, use each discrete value as the prediction threshold for your model.\nFor example, if you want to calculate TPR and FPR for the threshold equal to 0.7, you apply the model to each example and get the score.\nIf the score is greater than or equal to 0.7, you predict the positive class.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "model",
        "Andriy Burkov Machine",
        "machine learning",
        "Learning Engineering",
        "Burkov Machine",
        "regression model",
        "recall",
        "precision",
        "learning",
        "classiﬁcation model",
        "Pipeline",
        "Andriy Burkov",
        "spam"
      ],
      "concepts": [
        "model",
        "examples",
        "prediction",
        "predictions",
        "predicted",
        "classes",
        "recall",
        "data",
        "metric",
        "precision"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.549,
          "base_score": 0.549,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.509,
          "base_score": 0.359,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "recall",
          "precision",
          "classiﬁcation",
          "accuracy",
          "classiﬁcation model"
        ],
        "semantic": [],
        "merged": [
          "recall",
          "precision",
          "classiﬁcation",
          "accuracy",
          "classiﬁcation model"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3596577113006852,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010146+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 159-167)",
      "start_page": 159,
      "end_page": 167,
      "summary": "DCG measures the usefulness, or gain, of a document based on its position in the result list.\nCumulative gain (CG) is the sum of the graded relevance values of all results in a search result list.\nTo use it in the above formula, reli must be numeric, for example, ranging from 0 (the document at position i is entirely irrelevant to the query) to 1 (the document at position i is maximally relevant to the query).\nFor a given search result, DCG accumulated at a particular rank position p is often deﬁned as:\nDCGp IDCGp\nand RELp represents the list of the documents relevant to the query in the corpus up to position p (ordered by their relevance).\nSo, RELp is the ideal ranking, up to position p, that the search engine ranking algorithm (or model) should have returned for the query.\nThe nDCG values for all queries are usually averaged to obtain a performance measure for a search engine ranking algorithm or model.\nThe cumulative gain of this search result, up to position p = 5, is,\nNow we will calculate the discounted cumulative gain designed, with the presence of the logarithmic discounting, to have a higher value if highly relevant documents appear early in the for each i: result list.\nTo obtain nDCG for a collection of test queries and the corresponding lists of search results, we average the values of nDCGp obtained for each individual query.\nNow that we have a performance metric, we can use it to compare models in the process known as hyperparameter tuning.\nHyperparameters play an important role in the model training process.\nEach machine learning model and each learning algorithm have a unique set of hyperparameters.\nFurthermore, every step in your entire machine learning pipeline, data pre-processing, feature extraction, model training, and making predictions, can have its own hyperparameters.\nFor example, in data pre-processing, the hyperparameters could specify whether to use data-augmentation or using which technique to ﬁll missing values.\nFigure 6: Grid search for two hyperparameters: each green circle represents a pair of hyperparameter values.\nGrid search is the simplest hyperparameter tuning technique.\nThe technique consists of discretizing each of the two hyperparameters, and then evaluating each pair of discrete values, as shown in Figure 6.\n1) conﬁguring a pipeline with a pair of hyperparameter values, 2) applying the pipeline to the training data and training a model, and 3) computing the performance metric for the model on the validation data.\nThe pair of hyperparameter values that results in the best performing model is then selected\nThe Python code below uses grid search with cross-validation.2 It shows how to optimize the hyperparameters of the simple two-stage scikit-learn pipeline considered above:\n# Define hyperparamer values to try param_grid = dict(dim_reduction__n_components=[2, 5, 10], \\ model_training__C=[0.1, 10, 100])\nIn the above example, we use grid search to try the values [2,5,10] of the hyperparameter n_components of PCA, and the values [0.1,10,100] of the hyperparameter C of SVM.\nRandom search diﬀers from grid search in that you do not provide a discrete set of values to explore for each hyperparameter.\nFigure 7: Random search for two hyperparameters and 16 pairs to test.\nFigure 8: Coarse-to-ﬁne search for two hyperparameters: 16 coarse random search pairs to test and one grid search in the region of the highest value found using the random search.\nThen, using a ﬁne grid search in these regions, one ﬁnds the best values for hyperparameters, as shown in Figure 8.\nBayesian techniques diﬀer from random and grid searches in that they use past evaluation results to choose the next values to evaluate.\nGrid search and other techniques of hyperparameter tuning discussed above are used when you have a good-sized validation set.3 When you don’t, a common technique of model evaluation is cross-validation.\nFirst, you ﬁx the values of the hyperparameters to evaluate.\nTo train the ﬁrst model, f1, you use all examples from folds F2, F3, F4, and F5 as the training set, and the examples from F1 as the validation set.\nYou continue training models fk iteratively4 for all remaining folds, and compute the value of the metric of interest on each\nMore generally, in n-fold cross-validation, you train model fn on all folds, except for the n-th fold Fn. You can use grid search, random search, or any other such technique with cross-validation to ﬁnd the best values of hyperparameters.\nOnce you have found those values, you typically use the entire training set to train the ﬁnal model by using the best values of hyperparameters found via cross-validation.\nDeploy a “good enough” model to production, then continue to run the search of the ideal values for hyperparameters (for weeks if it is what it takes).\nA typical model training strategy for shallow learning algorithms looks as follows:\n5. Pick a combination H of hyperparameter values for algorithm A using strategy T.\n6. Use the training set and train a model M using algorithm A parametrized with hyperparameter values H.\n7. Use the validation set and calculate the value of metric P for model M.\nhyperparameter values using strategy T and go back to step 6.\n9. Return the model for which the value of metric P is maximized.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Machine Learning",
        "search",
        "Learning Engineering",
        "Burkov Machine",
        "hyperparameter",
        "Grid search",
        "model",
        "Learning",
        "Andriy Burkov",
        "relevant documents",
        "random search",
        "relevant"
      ],
      "concepts": [
        "models",
        "value",
        "searches",
        "technique",
        "ranking",
        "learning",
        "training",
        "important",
        "results",
        "validation"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.468,
          "base_score": 0.318,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.446,
          "base_score": 0.296,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.412,
          "base_score": 0.262,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 27,
          "title": "",
          "score": 0.405,
          "base_score": 0.255,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "search",
          "grid",
          "grid search",
          "hyperparameter",
          "values"
        ],
        "semantic": [],
        "merged": [
          "search",
          "grid",
          "grid search",
          "hyperparameter",
          "values"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.15828662713709543,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010170+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 168-176)",
      "start_page": 168,
      "end_page": 176,
      "summary": "Once you trained a model or a pipeline, you must save it to a ﬁle so that it can be deployed to production and then used for scoring.\n# Train the model model.fit(X, y)\n# Train the model model <- svm(X,y)\n# Make a prediction prediction <- predict(restored_model, new_example) Now, let’s talk about the particularities of the model training process that analysts must take care of in practice to produce an optimal model.\nDeveloping a model includes both searching for an optimal algorithm, as well as ﬁnding the best performing hyperparameters.\nThe model is said to have a low bias if it ablely predicts the training data labels.\nIf the model makes too many mistakes on the training data, we say that it has a high bias, or that the model underﬁts the training data.\nthe model is too simple for the data (for example linear models often underﬁt); • the features are not informative enough; • you regularize too much (we talk about regularization in the next section).\nThe model oversimpliﬁes the data.\ntrying a more complex model, • engineering features with higher predictive power, • adding more training data, when possible, and • reducing regularization.\nFigure 9: Examples of underﬁtting (linear model), good ﬁt (quadratic model), and overﬁtting (polynomial of degree 15).\nThe model that overﬁts usually predicts the training data labels very well, but works poorly on the holdout data.\nIf you sampled the training data diﬀerently, the result would be a signiﬁcantly diﬀerent model.\nThese overﬁtting models perform poorly on the holdout data, since holdout and training data are sampled from the dataset independently of one another.\nthe model is too complex for the data.\nreduce the dimensionality of examples in the dataset; • add more training data, if possible; and, • regularize the model.\nThis is called the bias-variance tradeoﬀ: by trying too hard to build a model that performs perfectly on the training data, you end up with a model that performs poorly on the holdout data.\nWhile many factors determine whether the model performs well on the training data, the most important factor is the complexity of the model.\nA suﬃciently complex model will learn to memorize all training examples and their labels and, thus, will not make prediction errors when applied to the training data.\nAs the model complexity grows, the typical evolution of the average prediction error of a model when applied to the training and holdout data is shown in Figure 10.\nOnce in this zone, you can ﬁne-tune the hyperparameters to reach the needed precision-recall ratio, or optimize another model performance metric appropriate for your problem.\nmove to the right by increasing the complexity of the model, and, by so doing, reducing its bias, or\nmove to the left by regularizing the model to reduce variance by making the model simpler (we talk about regularization in the next section).\nIf you work with shallow models, like linear regression, you can increase the complexity by switching to higher-order polynomial regression.\nEnsemble learning algorithms, based on the idea of boosting, allow bias reduction by combining several (usually, hundreds of) high-bias “weak” models.\nTraining a neural network model longer (i.e., for more epochs) also usually results in lower bias.\nIf, by increasing the complexity of your model, you ﬁnd yourself in the right-hand side of the graph in Figure 10, you have to reduce the variance of the model.\nRegularization is an umbrella term for methods that force a learning algorithm to train a less complex model.\nThis is the expression optimized by the learning algorithm when training the model.\nRegularization adds a penalizing term whose value is higher when the model is more complex.\nFor simplicity, we will illustrate regularization using linear regression, but same principle can be applied to a wide variety of models.\nThe learning algorithm will deduce the values of parameters w(1), w(2), and b from the training data by minimizing the objective.\nIf we set C to zero, the model becomes a standard non-regularized linear regression model.\nThis property of L1 regularization is useful when we want to increase model explainability.\nHowever, if our goal is to maximize the model performance on the holdout data, then L2 usually gives better results.\nIn addition to being widely used with linear models, L1 and L2 are often used with neural networks and many other types of models that directly minimize an objective function.\nHow much time is a learning algorithm allowed to use to train a model?\nModels like SVM, linear and logistic regression, as well as not very deep feedforward neural networks, are extremely fast at the prediction time.\nA typical way to know how good is the model, is to calculate the value of a performance metric on the holdout data.\nBy varying the complexity of the model, we can reach the so-called “zone of solutions,” a situation in which both bias and variance of the model are relatively low.\nRegularization is an umbrella term for methods that force the learning algorithm to build a less complex model.\nA pipeline is a sequence of transformations the training data undergoes before it becomes a model.\nIt consists of discretizing the values of hyperparameters, and trying all combinations of values by 1) training a model for each combination of hyperparameters, and 2) computing the performance metric by applying each trained model to the validation set.",
      "keywords": [
        "Model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Andriy Burkov Machine",
        "Machine Learning",
        "training data",
        "data",
        "training",
        "Learning Engineering",
        "Burkov Machine",
        "Learning",
        "regularization",
        "learning algorithm",
        "machine learning models",
        "Andriy Burkov"
      ],
      "concepts": [
        "model",
        "data",
        "regularize",
        "regularization",
        "regularized",
        "prediction",
        "predict",
        "predictions",
        "trained",
        "bias"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 24,
          "title": "",
          "score": 0.599,
          "base_score": 0.599,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.549,
          "base_score": 0.549,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.543,
          "base_score": 0.543,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.539,
          "base_score": 0.539,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.524,
          "base_score": 0.524,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "regularization",
          "data",
          "linear",
          "bias",
          "training"
        ],
        "semantic": [],
        "merged": [
          "regularization",
          "data",
          "linear",
          "bias",
          "training"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39958522065199176,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010204+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 177-184)",
      "start_page": 177,
      "end_page": 184,
      "summary": "Compared to shallow models, the model training strategy for deep neural networks has more moving parts.\n6.1 Deep Model Training Strategy\n1) use its learned parameters to initialize your own model, or 2) use the pre-trained model as a feature extractor for your model.\n6.1.1 Neural Network Training Strategy\n2. Deﬁne the cost function C.\n7. Train model M, using algorithm A, parametrized with hyperparameters H, to optimize cost function C.\nStep 1 is similar to step 1 of the shallow model training strategy (Section ??): we deﬁne a metric that would allow comparing the performance of two models on the holdout data, and select the better of the two.\nIn step 2, we deﬁne what our learning algorithm will optimize in order to train a model.\nIf our neural network is a regression model, then, in most cases, the cost function is the mean squared error (MSE) deﬁned in Equation ??\nFor classiﬁcation, a typical choice for the cost function is either categorical cross-entropy (for multiclass classiﬁcation) or binary cross-entropy (for binary and multi-label classiﬁca- tion).\nRecall that when we train a neural network for multiclass classiﬁcation, we should represent labels using the one-hot encoding.\nLet yi,j denote the value in position j (where j spans from 1 to C) in example i.\nwhere ˆyi is the C-dimensional vector of prediction issued by the neural network for the input xi.\nThe cost function is typically deﬁned as the sum of losses of individual examples:\nIn binary classiﬁcation, the output of the neural network for the input feature vector xi, is a single value ˆyi, while the label of the example is a single value yi, just like in logistic regression.\nSimilarly, the cost function for classiﬁcation of the training set is typically deﬁned as the sum of losses of individual examples:\nThe labels are now C- dimensional bag-of-words vectors yi, while the predictions are C-dimensional vectors ˆyi, whose values ˆyi,j in each dimension j range between 0 and 1.\nThe cost function for the classiﬁcation of the entire training set is typically deﬁned as the sum of losses of individual examples,\nIn multi-label classiﬁcation, the output layer contains C logistic units whose values also lie in the range (0,1), but their sum lies in the range (0,C).\nLet the output value of the output unit before non-linearity for the input example i be denoted as zi.\nLet the output value of the output unit before non-linearity for the input example i be denoted as zi.\nThe only diﬀerence is that in multi-label classiﬁcation, the output layer contains C logistic units, one per class.\nIf ˆyi,j denotes the output, after nonlinearity, of the logistic unit for class j, when input example is i, then the sum of ˆyi,j, for all j = 1,...,C, lies between 0 and C.\nHowever, in this case, the output of each unit of the output layer is controlled by the softmax function.\nBefore the training starts, the parameter values in all units are unknown.\nTraining algorithms for neural networks, such as gradient descent and its stochastic variants that we consider in a few moments, are iterative in nature and require the analyst to specify some initial point from which to begin the iterations.\nIf you work with a neural network training module such as TensorFlow, Keras, or PyTorch, they provide some parameter initializers, and also recommend default choices.\nIn step 4, we select a cost-function optimization algorithm.\nIf the function is called f, this relation is denoted y = f(x), read “y equals f of x.” The element x is the argument, or input of the\nfunction, and y is the value of the function, or the output.\nWe often say that f is a function of the variable x.\nA derivative f0 of a function f is a function or a value that describes how fast f increases or decreases.\nIf the derivative is a constant value, like 5 or −3, then the function increases or decreases constantly, at any point x of its domain.\nIf the derivative f0 is positive at some point x, then the function f increases at this point.\nIf the derivative of f is negative at some x, then the function decreases at this point.\nThe derivative of any function f(x) = c, where c is a constant value, is zero.\nFinding a partial derivative of a function is the process of ﬁnding the derivative by focusing on one of the function’s inputs and considering all other inputs as constant values.\nFor example, if our function is deﬁned as f([x(1),x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as\nSimilarly, the partial derivative of function f with respect to x(2),\nThe gradient of function f, denoted as ∇f is given by the vector [ ∂f",
      "keywords": [
        "Supervised Model Training",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "function",
        "Model",
        "Andriy Burkov Machine",
        "neural network",
        "model training strategy",
        "Supervised Model",
        "Model Training",
        "output",
        "cost function",
        "Learning Engineering",
        "Machine Learning",
        "Burkov Machine"
      ],
      "concepts": [
        "model",
        "function",
        "functions",
        "training",
        "values",
        "layer",
        "draft",
        "units",
        "strategy",
        "strategies"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.543,
          "base_score": 0.543,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "function",
          "derivative",
          "cost function",
          "output",
          "ˆyi"
        ],
        "semantic": [],
        "merged": [
          "function",
          "derivative",
          "cost function",
          "output",
          "ˆyi"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36106519756878247,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010247+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 185-194)",
      "start_page": 185,
      "end_page": 194,
      "summary": "Gradient descent is sensitive to the choice of the learning rate α.\nIn Figure 2, you can see an illustration of gradient descent for one parameter of a neural network and three values of the learning rate.\nIf the learning rate is too large, the value of the parameter will oscillate away from the minimum (Figure 2b).\nThe problem of choosing a value for the learning rate α is still present in the “vanilla” minibatch SGD.\nThe beneﬁts of using a learning rate decay schedule include faster gradient descent convergence (faster learning) and higher model quality.\nLearning rate decay consists of gradually reducing the value of the learning rate α as the epochs progress.\nwhere αn is the new value of the learning rate, αn−1 is the value of the learning at the previous epoch n − 1, and d is the decay rate, a hyperparameter.\nFor example, if the initial value of the learning rate α0 = 0.3, then the values of the learning rate at the ﬁrst ﬁve epochs are shown in the table below:\nThe mathematical formula for the learning rate update, according to a popular step-based learning rate decay schedule, is:\nwhere αn is the learning rate at epoch n, α0 is the initial value of the learning rate, d is the decay rate that reﬂects how much the learning rate should change at each drop step (0.5 corresponds to halving), and r is the so-called drop rate deﬁning the length of drop steps (10 corresponds to a drop every 10 epochs).\nThese algorithms update the learning rate automatically based on the performance of the learning process.\nYou don’t have to worry about choosing the initial learning rate value, the decay schedule and rate, or other related hyperparameters.\nThese algorithms have demonstrated good performance in practice, and practitioners often use them instead of manually tuning the learning rate.\nIt’s recommended to start training the model with Adam.\nEarly stopping trains a neural network by saving the preliminary model after every epoch.\nAfter some epoch, the model can start overﬁtting, and the model’s performance on the validation data can deteriorate.\nBy keeping a version of the model after each epoch, you can stop the training once you start observing a decreased performance on the validation set.\nStep 5 of the deep model training strategy is similar to that in the shallow model training strategy — choose a hyperparameter tuning strategy T.\nTypical parameters include the size of the minibatch, the value of the learning rate (if you use the vanilla minibatch SGD), or an algorithm that automatically updates the learning rate, such\nFigure 3: The neural network model training ﬂowchart.\nStep 7 reads, “Build the training model M, using algorithm A, parametrized with hyper- parameters H, to optimize the cost function C.” This is the main diﬀerence with shallow learning.\nWhen you work with a shallow learning algorithm or a model, you can only tweak some built-in hyperparameters.\nWith neural networks, you have all the control, and training a model is more a process than a single action.\nObserve that you start with some model, and then increase its size until it ﬁts the training data well.\nThe most eﬀective is dropout, where you randomly remove some units from the network and make it simpler and “dumber.” A simpler model would work better on the holdout data, and this is your goal.\nContinue until the model ﬁts the training data again.\nIf the performance of the best model is still not satisfactory, try a diﬀerent network architecture, add more labeled data, or try transfer learning.\nThe properties of a trained neural network depend a lot on the choice of the values of hyperparameters.\nBut before you choose speciﬁc values of hyperparameters, train a model, and validate its properties on the validation data, you must decide which hyperparameters are important enough for you to spend the time on.\nThe libraries for training neural networks often come with default values for hyperparameters: stochastic gradient descent version (often, Adam), the parameter initialization strategy (often, random normal or random uniform), minibatch size (often, 32), and so on.\nLearning rate Learning rate schedule Loss function Units per layer Parameter initialization strategy Medium Medium Number of layers Medium Layer properties Medium Degree of regularization Low Choice of optimizer Low Optimizer properties Low Size of minibatch Low Choice of non-linearity\nIn addition, the model has to return a tag describing the object, such as “person,” “cat,” or “hamster.” Your training example will be a feature vector representing an image and a label.\nRecall, transfer learning consists of using a pre-trained model to build a new model.\nThe parameters learned by the pre-trained models can be useful for your task.\nA pre-trained model can be used in two ways:\n1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\nUsing Pre-Trained Model as Initializer\nAs discussed, the choice of parameter initialization strategy aﬀects the properties of the learned model.\nPre-trained models, whether available on the Internet, or trained by you, usually perform well for solving the original learning problem.\nIf your current problem is similar to the one solved by the pre-trained model, chances are high that the optimal parameters for your current problem will not be too diﬀerent from the pre- trained parameters, especially in the initial neural network layers (those closest to the input).\nFigure 4: An illustration of transfer learning: (a) a pre-trained model and (b) your model, where you used the left part of the pre-trained model, and added new layers, including a diﬀerent output layer tailored for your problem.\nThe learning might go faster for your problem because gradient descent will search for the optimal parameter values in a smaller region of potentially good values.\nIf the pre-trained model was built using a training set much bigger than yours, searching in a region of potentially good values might also lead to a better generalization.\nUsing Pre-Trained Model as Feature Extractor If you use a pre-trained model as an initializer for your model, it gives you more ﬂexibility.\nSome pre-trained models contain hundreds of layers and millions of parameters.\nIf you have a limited amount of computational resources, you might prefer using some layers of the pre-trained model as feature extractors for your model.\nIn practice, it means that you only keep several initial layers of the pre-trained model, those closest to and including the input layer.\nOnly the parameters of the new layers will be updated by gradient descent during training on your data.\nThe blue neural network is a pre-trained model.\nHow many layers of the pre-trained model to use in the new model?\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model.",
      "keywords": [
        "learning rate",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "learning rate decay",
        "learning",
        "machine learning",
        "model",
        "Learning Engineering",
        "Andriy Burkov Machine",
        "rate",
        "learning rate update",
        "Burkov Machine",
        "pre-trained model",
        "Machine Learning Book",
        "rate decay"
      ],
      "concepts": [
        "model",
        "learned",
        "layer",
        "epochs",
        "networks",
        "data",
        "training",
        "parameter",
        "performance",
        "performs"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.399,
          "base_score": 0.399,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.343,
          "base_score": 0.343,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.318,
          "base_score": 0.318,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "learning rate",
          "rate",
          "pre trained",
          "pre",
          "trained"
        ],
        "semantic": [],
        "merged": [
          "learning rate",
          "rate",
          "pre trained",
          "pre",
          "trained"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2245173543348718,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010271+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 195-202)",
      "start_page": 195,
      "end_page": 202,
      "summary": "It consists of applying all your base models to the input x, and then returning the majority class among all predictions.\nModel stacking is an ensemble learning method that trains a strong model by inputting the outputs of other strong models.\nTo create a synthetic training example (ˆxi, ˆyi) for the stacked model from the original training example (xi,yi), set ˆxi ← [f1(x),f2(x),f3(x)], and ˆyi ← yi.\nIf some of your base models return a class plus a class score, you can use those scores as additional input features for the stacked model.\nTo train the stacked model, use synthetic examples, and tune the hyperparameters of the stacked model using cross-validation.\nMake sure your stacked model performs better on the validation set than each of the stacked base models.\nIn addition to using diﬀerent machine learning algorithms and models, some base models, to be weakly correlated, can be trained by randomly sampling the examples and features of the original training set.\nFurthermore, the same learning algorithm, trained with very diﬀerent hyperparameter values, could produce suﬃciently uncorrelated models.\nTo avoid data leakage, be careful when training a stacked model.\nTo create the synthetic training set for the stacked model, follow a process similar to cross-validation.\nTemporarily exclude one block from the training data, and train the base models on the remaining blocks.\nThen apply the base models to the examples in the excluded block.\nObtain the predictions, and build the synthetic training examples for the excluded block by using the predictions from the base models.\nRepeat the same process for each of the remaining blocks, and you will end up with the training set for the stacking model.\nYou might not have enough labeled Instagram photos for training, so you hope to train the model by using the Web crawl data, and then be able to use that model to classify the Instagram photos.\nThen you would train the model as usual.\nTo each example in Training Set 1, add the original label as an additional feature, then assign the new label “Training” to that example.\nMerge the Modiﬁed Training Set 1 and the Modiﬁed Test Set to obtain a new Synthetic Training Set. You will use it for solving a binary classiﬁcation problem of distinguishing the “Training” examples from the “Test” examples.\nObserve that the binary classiﬁer we have trained will predict, for a given original example, whether it’s a training or a test example.\nApply that binary classiﬁer to the examples from Training Set 2.\nIdentify the examples predicted as “Test,” which the binary model is most certain about.\nRemove the examples from Training Set 1 which the binary model predicted “Training” with the highest certainty.\nUse the remaining examples in Training Set 1 as the training data for your original problem.\nSome algorithms and models, such as support vector machine (SVM), decision trees, and random forests, allow the data analyst to provide weights for each class.\nIf you set higher the loss of minority misclassiﬁcation, then the model will try harder to avoid misclassifying those examples.\nHere, we transformed our imbalanced binary learning problem into four balanced problems by chunking the examples of the majority class into four subsets.\nSecond, you can make several consecutive updates of the model parameters each time you encounter an example of a minority class.\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic that we considered in Section ??\nSome models return a score along with the predicted class.\nWe say that the model is well-calibrated when, for input example x and predicted label ˆy, it returns the score that can be interpreted as the true probability for x to belong to class ˆy.\nMost machine learning algorithms train models that are not well-calibrated, as shown4 by the calibration plots in Figure 8.\nFigure 8: Calibration plots for models trained by several machine learning algorithms applied to a random binary dataset.\nBecause a logistic regression model returns the true probabilities of the positive class, its calibration plot is closest to the diagonal.\nThen, we apply the model f to each example i = 1,...,M and obtain, for each example i, the prediction fi.\nIt’s diﬃcult to diﬀerentiate whether the model performs poorly because your code contains a bug, or if there are problems with your training data, learning algorithm, or the way you designed your pipeline.",
      "keywords": [
        "training set",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "training",
        "synthetic training set",
        "original training set",
        "model",
        "machine learning",
        "Modiﬁed Training Set",
        "learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "base models",
        "Burkov Machine",
        "synthetic training"
      ],
      "concepts": [
        "model",
        "examples",
        "learning",
        "classes",
        "data",
        "sets",
        "trains",
        "predictions",
        "predicting",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.454,
          "base_score": 0.454,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 17,
          "title": "",
          "score": 0.432,
          "base_score": 0.432,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 24,
          "title": "",
          "score": 0.429,
          "base_score": 0.429,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.412,
          "base_score": 0.412,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stacked",
          "training",
          "stacked model",
          "set",
          "training set"
        ],
        "semantic": [],
        "merged": [
          "stacked",
          "training",
          "stacked model",
          "set",
          "training set"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31637916447689346,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010333+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 203-210)",
      "start_page": 203,
      "end_page": 210,
      "summary": "If your model does poorly on the training data (underﬁts it), common reasons are:\nyou don’t have enough data for the model to generalize (try to get more data, use data augmentation, or transfer learning); or\nIf your model does well on the training data, but poorly on the holdout data (overﬁts the training data), common reasons are:\nyou don’t have enough data for generalization (add more data or use data augmentation); • your model is under-regularized (add regularization or, for neural networks, both regularization and batch normalization);\nIf you have access to new labeled data (for example, you can label examples yourself, or easily request the help of a labeler) then, you can reﬁne the model using a simple iterative process:\n2. Test the model by applying it to a small subset of the validation set (100−300 examples).\nRemove those examples from the validation set, because your model will now overﬁt to them.\n4. Generate new features, or add more training data to ﬁx the observed error patterns.\nIterative model reﬁnement is a simpliﬁed version of error analysis.\nFocused errors, or error trends, usually happen when some use cases aren’t well-represented in the training data.\nUniform errors cannot be entirely avoided, but important focused errors should be discovered before the model is deployed in production.\nThe distribution of the production (online) data can be signiﬁcantly diﬀerent from the oﬄine data distribution used for model training/pre-deployment tests.\nTo identify error trends on a scatter plot, use diﬀerent markers depending on whether a model’s prediction was correct or not.\nWhether you are satisﬁed or dissatisﬁed by the model’s performance on the holdout data, you can always improve the model by analyzing individual errors.\nLet your model have an accuracy of 80%, which corresponds to an error rate of 20%.\nIf you ﬁx all error patterns, you can improve the model’s performance by at most 20 percentage points.\nIf your small error-analysis batch was of 300 examples, your model made 0.2×300 = 60 errors.\nIf you address the blurry-image problem (for example, by adding more labeled blurry images to your training data), you can hope to decrease your error by (40/60) × 20 = 13 percentage points.\nSo, in the best-case scenario, your model will make 20 − 1.7 = 18.3 percent errors, which might be signiﬁcant for some problems, or insigniﬁcant for others.\npreprocessing the input (e.g. image background removal, text spelling correction); • data augmentation (e.g., blurring or cropping of images); • labeling more training examples; and • engineering new features that would allow the learning algorithm to distinguish between “hard” cases.\nFor example, if the language predictor accuracy was 95%, the machine translation model accuracy6 was 90%, and the classiﬁer accuracy was 85%, then, in the case of independence of the three models, the overall accuracy of the entire three-stage system would be 0.95 × 0.90 × 0.85 = 0.73, or 73 percent.\nHowever, in practice, some errors made by a given model might not signiﬁcantly aﬀect the overall performance of the system.\nFor example, if the language predictor often confuses Spanish and Portuguese, the machine translation model could still be capable of generating an adequate translation for the third-stage classiﬁcation model.\nReaching the human-level performance for a machine translation model can turn out to be a daunting task, not worth the eﬀort, especially when what we can achieve in the end is one percentage point gain for the entire system.\nIf you see that the value of the performance metric changes between segments or classes, you can try to ﬁx the problem by adding more labeled data to the segments or classes, where the performance of the model is unsatisfactory, or engineer additional features.\nThis can cause poor model performance on the model on both training and holdout data.\nApply the model to the training data from which it was built, and analyze the examples for which it made a diﬀerent prediction as compared to the labels provided by humans.\nIf wrong labels in the training data is a serious issue, you can avoid it by asking several individuals to provide labels for the same training example.\nAs discussed above, error analysis can reveal that more labeled data is needed from speciﬁc regions of feature space.\nHow should you decide which examples to label so as to maximize the positive impact on the model?\nIf your model returns a prediction score, an eﬀective way is to use your best model to score the unlabeled examples.\nTo avoid problems when training a deep model, follow a workﬂow shown below:\nLearning rate too high Learning rate too low Gradient not ﬂowing through the whole model Too much regularization Incorrect input to loss function Data or labels corrupted\nOnce your model overﬁts one minibatch, get back to the entire dataset, and train, evaluate, then tune hyperparameters until no improvements on the validation data are possible.\nIf the performance of the model is still unsatisfactory, update the model (e.g., by increasing its depth or width), or the training data (e.g., by changing the pre-processing, or adding features).\nWhile you are searching for the best architecture for your model, it’s convenient not just to use a smaller training set, but also to simplify the problem by either,\nAt the evaluation step of the deep learning troubleshooting workﬂow shown in Figure 10, verify if the poor model performance could be caused by one of the reasons listed in Section 6.6.1.\nChoose the next step depending on whether the performance can be improved by tuning hyperparameters, updating the model, features, or the training data.\nIn this section, I gathered practical advice on training machine learning models.\nLearning algorithms try to reduce training data error.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Model",
        "Machine Learning",
        "Andriy Burkov Machine",
        "data",
        "training data",
        "error",
        "Learning Engineering",
        "learning",
        "Poor Model Behavior",
        "Burkov Machine",
        "machine translation model",
        "machine learning models",
        "training"
      ],
      "concepts": [
        "errors",
        "model",
        "trains",
        "labeling",
        "learning",
        "performance",
        "perform",
        "images",
        "predictive",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.599,
          "base_score": 0.599,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 32,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 17,
          "title": "",
          "score": 0.483,
          "base_score": 0.483,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.449,
          "base_score": 0.449,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 23,
          "title": "",
          "score": 0.429,
          "base_score": 0.429,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "error",
          "training",
          "translation",
          "translation model"
        ],
        "semantic": [],
        "merged": [
          "data",
          "error",
          "training",
          "translation",
          "translation model"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34494708767603405,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010364+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 211-219)",
      "start_page": 211,
      "end_page": 219,
      "summary": "It consists of iteratively upgrading the existing model by using only new training examples and running additional training iterations.\nIt’s a situation in which the model that was once capable of something, “forgets” that capability because of learning something new.\nNote that upgrading the model is not the same as transfer learning.\nAnalysts use transfer learning when the data used to build the pre-trained model, or adequate computing resources, are not available.\nThe negative eﬀect on mB of the change in model mA may go unnoticed for a long time.\nInstead of building a correction cascade, it is recommended to update model mA to include the use cases for solving problem B.\nIt would be wise to add features allowing the model to distinguish between the examples of problem B.\nOne might also use transfer learning, or build an entirely independent model for solving problem B.\n6.7.6 Use Model Cascading With Caution\nModel mA’s updated output must be reﬂected in the training data for model mB.\nSuch libraries as PyPy and Numba for Python, or pqR for R, would compile the code into the OS (operating system) native binary code, which can signiﬁcantly increase the speed of data processing and model training.\nIf you work with modern libraries and modules, you can ﬁnd learning algorithms that exploit multicore CPUs. Some allow GPUs to speed up the training of neural networks and many other models.\nTraining of some models, such as SVM, cannot be eﬀectively parallelized.\nBy reducing to a minimum the time needed to train a model, you can spend more time tweaking your model, testing data pre-processing ideas, feature engineering, neural network architectures, and other creative activities.\nIf you used a data dump from some time ago to create training, validation, and test sets, observe how your model behaves with data collected before and after this period.\nWhen confronted to insuﬃcient model performance, to improve the performance of the model, analysts are often tempted into crafting a more sophisticated learning algorithm or a pipeline.\nIf, despite adding more training examples and designing clever features, the performance of your model plateaus, think about diﬀerent information sources.\nFor example, when we train a neural network, we initialize model parameters randomly; the minibatch stochastic gradient descent generates minibatches randomly; the decision trees in a random forest are built randomly; when we shuﬄe examples before splitting the data into three sets, we do it randomly; and so on.\nThis means that when you train a model on the same data twice, you might end up having two diﬀerent models.\nIf your random seed remains the same, then, if your data doesn’t change, you will obtain exactly the same model each time you train.\nEven if a machine learning framework allows us to set the value of the random seed, there’s no guarantee that the code of the framework that uses randomization doesn’t change between versions of the framework.\nInstead of training your model from scratch, it can be useful to start with a pre-trained model.\nA pre-trained model can be used in two ways: 1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\nUsing a pre-trained model to build your own is called transfer learning.\nThe fact that deep models allow for transfer learning is one of the most important properties of deep learning.\nMinibatch stochastic gradient descent and its variants are the most frequently used cost function optimization algorithms for deep models.\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model.\nThere are ensemble learning algorithms, such as random forest and gradient boosting, that build an ensemble of several hundred to thousands of weak models, and obtain a strong model that has a signiﬁcantly better performance than the performance of each weak model.\nModel stacking, being the most eﬀective of the ensembling methods, consists of training a meta-model that takes the output of base models as input.\nIf you train your model using stochastic gradient descent, the class imbalance can be tackled in two additional ways: 1) by setting diﬀerent learning rates for diﬀerent classes, and 2) by making several consecutive updates of the model parameters each time you encounter an example of a minority class.\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic.\nPoor performance can be caused by a bug in your code, training data errors, learning algorithm issues, or pipeline design.\nErrors made by a machine learning model can be uniform and appear in all use cases with the same rate, or focused and appear in just certain types of use cases.\n1. Train the model using the best values of hyperparameters identiﬁed so far.",
      "keywords": [
        "Machine Learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "model",
        "learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "data",
        "machine learning model",
        "Burkov Machine",
        "Machine",
        "Machine Learning Systems",
        "training",
        "machine learning project",
        "learning algorithms"
      ],
      "concepts": [
        "models",
        "learning",
        "data",
        "train",
        "performance",
        "performing",
        "useful",
        "packages",
        "processing",
        "features"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.586,
          "base_score": 0.436,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 27,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.509,
          "base_score": 0.359,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.472,
          "base_score": 0.322,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "transfer learning",
          "training",
          "transfer",
          "ensemble",
          "pre trained"
        ],
        "semantic": [],
        "merged": [
          "transfer learning",
          "training",
          "transfer",
          "ensemble",
          "pre trained"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3197345802343862,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010411+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 220-227)",
      "start_page": 220,
      "end_page": 227,
      "summary": "7 Model Evaluation\n• Evaluate the performance of the model.\nBefore the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training.\n• Evaluate the performance of the model.\nBefore the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training.\nFirst, validation data is used to assess the chosen performance metric and compare models.\nOnce the best model is identiﬁed, the test set is used, also in oﬄine mode, to again assess the best model’s performance.\nIn this chapter, we talk, among other topics, about establishing statistical bounds on the oﬄine test performance of the model.\nA signiﬁcant part of the chapter is devoted to the online model evaluation, that is, testing and comparing models in production by using online data.\nThe diﬀerence between oﬄine and online model evaluation, as well as the placement of each type of evaluation in a machine learning system, is illustrated in Figure 2.\nFigure 2: The placement of oﬄine and online model evaluations in a machine learning system.\nThen, user queries and the model predictions are used for an online evaluation of the model.\nThe online data is then used to improve the model.\nThe oﬄine model evaluation reﬂects how well the analyst succeeded in ﬁnding the right features, learning algorithm, model, and values of hyperparameters.\nFurthermore, oﬄine evaluation doesn’t allow us to test the model in some conditions that can be observed online, such as connection and data loss, and call delays.\nTypical examples of a distribution shift include the ever-changing interests of the user of a mobile or online application, instability of ﬁnancial markets, climate change, or wear of a mechanical system whose properties the model is intended to predict.\nOne way of doing such monitoring is to compare the performance of the model on online and historical data.\nIf the performance on online data becomes signiﬁcantly worse, as compared to historical, it’s time to retrain the model.\nAnother common scenario is to monitor user behavior in response to diﬀerent versions of the model.\nThen we apply a statistical signiﬁcance test to decide whether the performance of the new model is better than the old one.\nSimilar to A/B testing, it identiﬁes the best performing models by exposing model candidates to a fraction of users.\nThen it gradually exposes the best model to more users, by keeping gathering performance statistics until it’s reliable.\n7.2 A/B Testing\nWhen applied to online model evaluation, it allows us to answer such questions as, “Does the new model mB work better in production than the existing model mA?” or, “Which of the two model candidates works better in production?”\nA/B testing is often used on websites and mobile applications to test whether a speciﬁc change in the design or wording positively aﬀects business metrics such as user engagement, click-through rate, or sales rate.\nThe live traﬃc that contains input data for the model is split into two disjoint groups: A (control) and B (experiment).\nGroup A traﬃc is routed to the old model, while group B traﬃc is routed to the new model.\nBy comparing the performance of the two models, a decision is made about whether the new model performs better than the old model.\nAn A/B test is usually formulated to answer the following question: “Does the new model lead to a statistically signiﬁcant change in this speciﬁc business metric?” The null hypothesis states that the new model doesn’t change the average value of the business metric.\nThe alternative hypothesis states that the new model changes the average value of the metric.\nThe ﬁrst formulation of A/B test is based on the G-test.\nThe users of group A are routed to the environment running the old model, while the group’s B traﬃc is routed to the new model.\nFigure 3: The counts of answers to the yes-or-no question by users from groups A and B.\nIn the above table, ˆayes is the number of users in group A, for which the answer to the question is “yes,” ˆbyes is the number of users in group B, for which the answer to the question is “yes,” ˆano is the number of users in group A, for which the answer to the question is “no,” and so on.\nStatistically speaking, under the null hypothesis (A and B are equal), G follows a chi-square distribution with one degree of freedom:\nA large value of G would make us suspicious that one of the models performs better than the other.\nIf the p-value is small enough (e.g., below 0.05) then the performances of the new and the old model are very likely diﬀerent (the null hypothesis is rejected).\nIn this case, if byes is higher than ayes, then the new model is very likely to work better than the old model; otherwise, the old model is better.\nIf the p-value corresponding to the value of G is not small enough then the observed diﬀerence of performance between the new and the old model is not statistically signiﬁcant, and you can keep the old model in production.\nNote that it is possible to test more than two models (e.g. models A, B, and C) and more than two possible answers to the question that deﬁne our metric (e.g., “yes,” “no,” “maybe”).\nIf we want to test k diﬀerent models and l diﬀerent possible answers, the G statistic would follow a chi-square distribution with (k − 1) × (l − 1) degrees of freedom.\nThe problem here is that a test with multiple models and answers will tell you whether there is something diﬀerent somewhere between your models, but it will not tell you where is the diﬀerence.\nIn practice, it is easier to compare your current model with only one new model and to formulate a question metric with a binary answer.\nThe second formulation of A/B test applies when the question for each user is, “How many?” or, “How much?” (as opposed to a yes-or-no question considered in the previous subsection).\nFor simplicity of illustration, let’s measure the time a user spends on a website where our model is deployed.\nAs usual, users are routed to versions A and B of the website, where version A serves the old model and version B serves the new model.\nTo compute the value of the Z-test, we ﬁrst compute sample mean and sample vari- ance for A and B.\nAs for the G-test, we will use the p-value to decide whether or not Z is large enough to think that the time spent on B is really greater than time spent on A.",
      "keywords": [
        "Model",
        "Machine learning engineering",
        "Model Evaluation",
        "Burkov Machine Learning",
        "online model evaluation",
        "machine learning",
        "Andriy Burkov Machine",
        "data",
        "online model",
        "learning engineering",
        "Burkov Machine",
        "Evaluation",
        "online",
        "online data",
        "oﬄine model"
      ],
      "concepts": [
        "model",
        "data",
        "statistics",
        "user",
        "business",
        "performance",
        "performing",
        "samples",
        "evaluation",
        "evaluated"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 27,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 31,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "online",
          "old model",
          "question",
          "old",
          "new model"
        ],
        "semantic": [],
        "merged": [
          "online",
          "old model",
          "question",
          "old",
          "new model"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3538918089106543,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010444+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 228-235)",
      "start_page": 228,
      "end_page": 235,
      "summary": "that says that the diﬀerence in performance of the two models is not statistically signiﬁcant.\nSimilar to the G-test, it is convenient to ﬁnd the p-value of the Z-test using a programming language.\nWhile pointing this out, Cassie Kozyrkov, the Chief Decision Scientist at Google and one of the reviewers of this chapter, emphasized that the above two tests are rarely a good idea to apply in practice because they only show that two models are diﬀerent, but they don’t show whether the diﬀerence is “of at least x.” If replacing the old model with the new one has a signiﬁcant cost or poses a risk, then just knowing that the new model is “somewhat” better is not enough to make a replacement decision.\nThe number of test results in groups A and B you need to calculate the value of the A/B test is high.\nLet’s see how the multi-armed bandit problem applies to an online evaluation of two models.\nMore statistical tests will be added over time.\nIn other words, UCB1 routes the user to the best performing model more often when its conﬁdence about the model performance is high.\nOtherwise, UCB1 might route the user to a suboptimal model so as to get a more conﬁdent estimate of that model’s performance.\nIf ca = 0 for some arm a, then this arm is played; otherwise, the arm with the greatest UCB value is played.\nThe UCB value of an arm a, denoted as ua, is deﬁned as follows:\ndef __init__(self, n_arms): self.c = [0]*n_arms self.v = [0.0]*n_arms self.M = n_arms return\n7.4 Statistical Bounds on the Model Performance\nWhen reporting the model performance, sometimes, besides the value of the metric, it is required to also provide the statistical bounds, also known as the statistical interval.\nIf you report the error ratio “err” for a classiﬁcation model (where err def= 1 − accuracy), then the following technique can be used to obtain the statistical interval for “err.”\nLet N be the size of the test set.\nThe value of zN depends on the required conﬁdence level.\nFor other conﬁdence level values, the values of zN can be found in the table below:\nfrom scipy.stats import norm def get_z_N(confidence_level): # a value in (0,100)\nget_z_N <- function(confidence_level) {# a value in (0,100)\nHowever, a more accurate rule of thumb for obtaining the minimum size N of the test set is as follows: ﬁnd the value of N such that N × err(1 − err) ≥ 5.\nIntuitively, the greater the size of the test set, the lower our uncertainty about the true performance of the model.\nA popular technique for reporting the statistical interval for any metric, and which applies to both classiﬁcation and regression, is based on the idea of bootstrapping.\nBootstrapping is a statistical procedure that consists of building B samples of a dataset, and then training a model or computing some statistic using those B samples.\nGiven the test set, we create B random samples Sb, one for each b = 1,...,B.\nOnce we have B bootstrap samples of the test set, we compute the value of the performance metric mb using each sample Sb as the test set.\nThen ﬁnd the value S of the sum of all B values of the metric: S def= PB b=1 mb.\nTo obtain a c percent statistical interval for the metric, pick the tightest interval between a minimum a and a maximum b such that the sum of the values mb that lie in that interval accounts for at least c percent of S.\nOur statistical interval is then given by [a,b].\nLet the values of the metric, computed by applying the model to B bootstrap samples, be [9.8,7.5,7.9,10.1,9.7,8.4,7.1,9.9,7.7,8.5].\nfrom numpy import percentile def get_interval(values, confidence_level):\nget_interval <- function(values, confidence_level) {\nOnce you have the boundaries a = 7.46 and b = 9.92 of the statistical interval, you can report that the value of the metric for your model lies in the interval [7.46,9.92] with conﬁdence 80%.\nUntil now, we considered the statistical interval for an entire model and a given performance metric.\nIn this section, we will use bootstrapping to compute the prediction interval for a regression model and a given feature vector x, which this model receives as input.\nGiven a regression model f and an input feature vector x, what is an interval of values [fmin(x),fmax(x)] such that the prediction f(x) lies inside that interval with conﬁdence c percent?\nThe only diﬀerence is that now we build B bootstrap samples of the training set (and not the test set).\nBy using B bootstrap samples as B training sets, we build B regression models, one per bootstrap sample.\nApply B models to x and obtain B predictions.\nNow, by using the same technique as above, ﬁnd the tightest interval between a minimum a and a maximum b such that the sum of the values of predictions that lie in the interval accounts for at least c percent of the sum of B predictions.\nThen return the prediction f(x), and state that, with conﬁdence c percent, it lies in the interval [a,b].\nThe number B of bootstrap samples is set to 100 (or as many as the time allows).\nThe same approach applies to the testing of all the code developed “around” the statistical model: the code that gets the input from the user, transforms it into features, and the code that interprets the outputs of the model, and serves the result to the user.\nThe test examples used to evaluate the model must also be designed in such a way that they allow the discovery of the model’s defective behavior before the model reaches production.\nNeuron coverage of a test set for a neural network model is deﬁned as the ratio of the units (neurons) activated by the examples from the test set, to the total number of units.\nA technique for building such a test set is to start with a set of unlabeled examples, and all units of the model uncovered.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "test set",
        "statistical interval",
        "model",
        "interval",
        "Machine Learning",
        "Learning Engineering",
        "Burkov Machine",
        "statistical",
        "Andriy Burkov",
        "level",
        "arm",
        "Conﬁdence"
      ],
      "concepts": [
        "models",
        "time",
        "statistically",
        "values",
        "let",
        "interval",
        "sets",
        "err",
        "follows",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.56,
          "base_score": 0.41,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.505,
          "base_score": 0.355,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.438,
          "base_score": 0.288,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 19,
          "title": "",
          "score": 0.405,
          "base_score": 0.255,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "interval",
          "statistical",
          "statistical interval",
          "test set",
          "samples"
        ],
        "semantic": [],
        "merged": [
          "interval",
          "statistical",
          "statistical interval",
          "test set",
          "samples"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.17969933456575263,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010490+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 236-244)",
      "start_page": 236,
      "end_page": 244,
      "summary": "The robustness of a machine learning model refers to the stability of the model performance after adding some noise to the input data.\nLet us, before applying a model f to that input example, modify the values of some features, chosen randomly, by replacing them with a zero, to obtain a modiﬁed input x0.\nIf you have several models that perform similarly according to the performance metric, you would prefer to deploy in production a model that is (cid:15)-robust, when applied to the test data, with the smallest (cid:15).\nPick the model f you want tested for robustness.\nOnce you ﬁnd the value of δ = ˆδ where (cid:15) ≥ ˆ(cid:15), note that the model f you are testing for robustness is ˆ(cid:15)-robust to ˆδ-perturbation of the input.\nOnce you have the value of ˆδ-perturbation for each model, deploy in production the model whose ˆδ is the greatest.\nThat is, as measured on the test data, the chance to predict 1 with the model f for women is the same as the chance to predict 1 for men.\nThe exclusion of the protected attributes from the feature vector in the training data doesn’t guarantee that the model will have demographic parity, as some of the remaining features\nThe above equality means that, as measured on the test data, the chance to predict 1 by the model f for women who qualify for that prediction is the same as the chance to predict 1 for men who also qualify.\nestimate legal risks of putting the model in production, • understand the main properties of the distribution of the data used to train the model, • evaluate the performance of the model prior to deployment, and • monitor the performance of the deployed model.\nThe online model evaluation consists of testing and comparing models in the production environment using online data.\nA popular technique of online model evaluation is A/B testing.\n8 Model Deployment\nOnce the model has been built and thoroughly tested, it can be deployed.\nDeploying a model means to make it available for accepting queries generated by the users of the production system.\nModel deployment is the sixth stage in the machine learning project life cycle:\nA trained model can be deployed in various ways.\nA model can be deployed following several patterns:\nstatically, as a part of an installable software package, • dynamically on the user’s device, • dynamically on a server, or • via model streaming.\nThe static deployment of a machine learning model is very similar to traditional software deployment: you prepare an installable binary of the entire software.\nthe software has direct access to the model, so the execution time is fast for the user, • the user data doesn’t have to be uploaded to the server at the time of prediction; this saves time and preserves privacy,\nThe diﬀerence is that in dynamic deployment, the model is not part of the binary code of the application.\nPushing model updates is done without updating the whole application running on the user’s device.\nMoreover, a dynamic deployment may allow the same piece of code to select the right model, based on the available compute resources.\nby deploying model parameters, • by deploying a serialized object, and • by deploying to the browser.\n8.2.1 Deployment of Model Parameters\nIn this deployment scenario, the model ﬁle only contains the learned parameters, while the user’s device has installed a runtime environment for the model.\nThe advantage of this approach is that you don’t need to have a runtime environment for your model on the user’s device.\nSome machine learning frameworks, such as TensorFlow.js, have versions that allow to train and run a model in a browser, by using JavaScript as a runtime.\nIt’s even possible to train a TensorFlow model in Python, and then deploy it to, and run it in the browser’s JavaScript runtime environment.\nThe main advantage of dynamic deployment to users’ devices is that the calls to the model will be fast for the user.\nDeploying models on the user’s device means that the model easily becomes available for third-party analyses.",
      "keywords": [
        "model",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "Andriy Burkov Machine",
        "machine learning model",
        "Burkov Machine",
        "Learning Engineering",
        "deployment",
        "machine",
        "cid",
        "learning",
        "Andriy Burkov",
        "model evaluation",
        "user"
      ],
      "concepts": [
        "model",
        "tested",
        "examples",
        "learning",
        "applicative",
        "application",
        "deploy",
        "device",
        "data",
        "statistical"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.377,
          "base_score": 0.377,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.349,
          "base_score": 0.349,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 32,
          "title": "",
          "score": 0.341,
          "base_score": 0.341,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.334,
          "base_score": 0.334,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "deployment",
          "cid",
          "cid 15",
          "user",
          "device"
        ],
        "semantic": [],
        "merged": [
          "deployment",
          "cid",
          "cid 15",
          "user",
          "device"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.22695374601178517,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010515+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 245-254)",
      "start_page": 245,
      "end_page": 254,
      "summary": "Figure 2: Deploying a machine learning model as a web service on a virtual machine.\nAs with static deployment, deploying to a user’s device makes it diﬃcult to monitor the model performance.\nBecause of the above complications, and problems with performance monitoring, the most frequent deployment pattern is to place the model on a server (or servers), and make it available as a Representational State Transfer application programming interface (REST API) in the form of a web service, or Google’s Remote Procedure Call (gRPC) service.\n8.3.1 Deployment on a Virtual Machine\nIn a typical web service architecture deployed in a cloud environment, the predictions are served in response to canonically-formatted HTTP requests.\nA web service running on a virtual machine receives a user request containing the input data, calls the machine learning system on that input data, and then transforms the output of the machine learning system into the output JavaScript Object Notation (JSON) or Extensible Markup Language (XML) string.\nEach instance, denoted as an orange square, contains all the code needed to run the feature extractor and the model.\nThe advantage of deploying on a virtual machine is that the architecture of the software system is conceptually simple: it’s a typical web or gRPC service.\nFinally, deploying on a virtual machine has a relatively higher cost, compared to deployment in a container, or a serverless deployment that we consider below.\n8.3.2 Deployment in a Container\nA more modern alternative to a virtual-machine-based deployment is a container-based deployment.\nIf your software is deployed in a cloud environment, a cluster autoscaler can launch (and add to the cluster) or terminate virtual machines, based on the usage of the cluster.\nFigure 3: Deploying a model as a web service in a container running on a cluster.\nDeployment in a container has the advantage of being more resource-eﬃcient as compared to the deployment on a virtual machine.\nThe serverless deployment consists of preparing a zip archive with all the code needed to run\nthe machine learning system (model, feature extractor, and scoring code).\nThe cloud platform takes care of deploying the code and the model on an adequate computational resource, executing the code, and routing the output back to the client.\nLikewise, the unavailability of GPU access1 can be a signiﬁcant limitation for deploying deep models.\nA deployment pattern appropriate for one model may be less optimal for another one.\nlike Google Home or Amazon Echo might have a model that recognizes the activation phrase (such as “OK, Google” or “Alexa”) deployed on the client’s device, and more complex models handle requests like “put song X on device Y” will instead run on the server.\nModel streaming is a deployment pattern that can be seen as an inverse to the REST API.\nAccording to the REST API deployment pattern, we need one REST API per model.\nInstead of having one REST API per model, all models, as well as the code needed to run them, are registered within a stream-processing engine (SPE).\nFigure 4: The diﬀerence between a REST API and streaming: (a) to process one data element, a client using a REST API sends a series of requests, one by one, and receives responses synchronously; (b) to process one data element, a client using streaming opens a connection, sends a request, and receives update events as they happen.\nAn SPE-based streaming application runs on its own cluster of virtual or physical machines, and takes care of distributing the data processing load among the available resources.\nTo deploy on a server in a cloud environment, you prepare a new virtual machine, or a container running the new version of the model.\nTo deploy on a physical server, you will upload a new model ﬁle (and the feature extraction object, if needed) on the server.\nTo deploy on the user’s device, you push the new model ﬁle to the user’s device, along with any needed feature extraction object, and restart the software.\nIf the new model or the feature extractor contains a bug, all users will be aﬀected.\nIt deploys the new model version and new feature extractor, and keeps the old ones.\nSilent deployment has the beneﬁt of providing enough time to ensure the new model works as expected, without adversely aﬀecting any users.\nRecall, canary deployment, or canarying, pushes the new model version and code to a small fraction of users, while keeping the old version running for most users.\nContrary to the silent deployment, canary deployment allows validating the new model’s performance, and its predictions’ eﬀects.\nBy opting for the canary deployment, you accept the additional complexity of having and maintaining several versions of the model deployed simultaneously.\nIf you deploy the new version to 5% of users, and a bug aﬀects 2% of users, then you have only 0.1% chance that the bug will be discovered.\nIt means that after the convergence of the MAB algorithm, most of the time, all users are routed to the software version running the best model.\nThe MAB algorithm, thus, solves two problems — online model evaluation and model deployment — simultaneously.\nOnly deploy a model in production when it’s accompanied with the following assets:\nOnce the system using the model is initially evoked on an instance of a server or client’s device, an external process must call the model on the end-to-end test data and validate that all predictions are correct.\nIf the feature extractor was not changed, its version still must be updated to be in sync with the data and the model.\nIf the feature extractor was updated, then a new model must be built using an updated feature extractor, and the versions are incremented for the feature extractor, the model, and the training data (even if the latter wasn’t changed).\nThe deployment of a new model version must be automated by a script in a transactional way.\nGiven a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment.\nthe name of the learning algorithm, and names and values of the hyperparameters, • the list of features required by the model, • the list of outputs, their types, and how the outputs should be consumed, • the version and location of the data used to train the model, • the version and location of the validation data used to tune model’s hyperparameters, • the model scoring code that runs the model on new data and outputs the prediction.\n8.6 Model Deployment Best Practices\nWe also outline several useful and practical tips for model deployment.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "model",
        "machine learning",
        "deployment",
        "machine learning model",
        "virtual machine",
        "Burkov Machine",
        "machine learning system",
        "REST API",
        "machine",
        "Learning Engineering",
        "Andriy Burkov",
        "REST API deployment"
      ],
      "concepts": [
        "deploying",
        "model",
        "data",
        "process",
        "processing",
        "version",
        "versions",
        "resource",
        "machine",
        "streaming"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 31,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 33,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.419,
          "base_score": 0.419,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 34,
          "title": "",
          "score": 0.418,
          "base_score": 0.418,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 3,
          "title": "",
          "score": 0.395,
          "base_score": 0.395,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "deployment",
          "virtual",
          "virtual machine",
          "rest api",
          "api"
        ],
        "semantic": [],
        "merged": [
          "deployment",
          "virtual",
          "virtual machine",
          "rest api",
          "api"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2961504921710805,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010543+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 255-262)",
      "start_page": 255,
      "end_page": 262,
      "summary": "8.6.2 Deployment of Deep Models\nSo only the model could be deployed in an environment with one or several GPUs optimized for fast scoring.\nIn machine learning, such resource-consuming functions are models, especially when they run on GPUs.\n@lru_cache(maxsize=500) def run_model(input_example):\nreturn model.predict(input_example)\n# Now you can call run_model # on new data The ﬁrst time the function run_model is called for some input, model.predict will be called.\nFor the subsequent calls of run_model with the same value of the input, the output will be read from cache that memorizes the result of maxsize most recent calls of model.predict.\nrun_model <- function(input_example) {\nresult <- predict(model, input_example) result\n# Now you can use run_model_memo # instead of run_model on new data\n8.6.4 Delivery Format for Model and Code\nRecall, serialization is the most straightforward way to deliver the model and the feature extractor code to the production environment.\n# Read model from file classifier2 <- readRDS(\"./model.rds\")\n# Save model to file dump(classifier, \"model.joblib\")\n# Read model from file classifier2 = load(\"model.joblib\")\nIf a data analyst has built a model using Python or R, there are three options to deploy for production:\nrewrite the code in a compiled, production-environment programming language, • use a model representation standard such as PMML or PFA, or • use a specialized execution engine such as MLeap.\nThe Predictive Model Markup Language (PMML) is an XML-based predictive model interchange format that provides a way for data analysts to save and share models between PMML-compliant applications.\nFor example, imagine you use Python to build an SVM model, and then save the model as a PMML ﬁle.\nAs long as PMML is supported by a machine learning library for JVM, and that library has an implementation of SVM, your model can be used in production directly.\nPFA allows us to easily share models and machine learning pipelines across heterogeneous systems and provides algorithmic ﬂexibility.\nEvaluators read the model or the pipeline from a ﬁle, execute it by applying it to the input data, and output the prediction.\nAlternatively, such execution engines as MLeap can execute machine learning models and pipelines fast in a JVM environment.\nNow, let us brieﬂy outline several useful and practical tips for model deployment.\nDeploying and applying the model in production can be more complex than it might seem.\nComplex models and pipelines have many dependencies and large numbers of hyperparameters to tune, and are more prone to implementation and deployment errors.\nBefore putting your model in production, test your model on outsiders, and not just on the test data.\nA model can be deployed following several patterns: statically, as a part of installable software, dynamically on the user’s device, dynamically on a server, or via model streaming.\nThe static deployment has many advantages, such as fast execution time, preserved user privacy, and the ability to call the model when the user is oﬄine.\nThe principal advantage of the dynamic deployment on the users’ devices is that the calls to the model will be fast for the user.\nAs with the static deployment, deploying the model on a user’s device makes it diﬃcult to monitor the performance of the model.\nIn single deployment, you serialize the new model into a ﬁle, and then replace the old one.\nThus, there is enough time to make sure that the new model works as expected without aﬀecting any user.\nCanary deployment allows model performance validation and evaluating the users’ experience.\nMulti-armed bandits allow us to deploy the new model while keeping the old one.\nThe deployment of a new model version must be automated by a script in a transactional way.\nGiven a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment.\nAlgorithmic eﬃciency is an important consideration in model deployment.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "model",
        "Andriy Burkov Machine",
        "Machine Learning",
        "Burkov Machine",
        "Learning Engineering",
        "Python",
        "Andriy Burkov",
        "machine learning models",
        "Machine",
        "max",
        "Deployment",
        "Learning",
        "result"
      ],
      "concepts": [
        "models",
        "deployment",
        "data",
        "algorithms",
        "algorithmic",
        "classifier",
        "product",
        "production",
        "useful",
        "engineering"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 18,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 21,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.586,
          "base_score": 0.436,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 31,
          "title": "",
          "score": 0.513,
          "base_score": 0.513,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "deployment",
          "run_model",
          "pmml",
          "input_example",
          "read"
        ],
        "semantic": [],
        "merged": [
          "deployment",
          "run_model",
          "pmml",
          "input_example",
          "read"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.34174524599585526,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010574+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 263-271)",
      "start_page": 263,
      "end_page": 271,
      "summary": "9 Model Serving, Monitoring, and Maintenance\nIn this chapter, we consider the best practices of serving, monitoring, and maintaining models in production.\nIn particular, we characterize the properties of a machine learning runtime, the environment in which the input data is applied to the model, and the modes of model serving, such as batch and on demand.\n9.1 Properties of the Model Serving Runtime\nThe model serving runtime is the environment in which the model is applied to the input data.\nwhether a speciﬁc user has authorized access to the models they want to run,\nTypically, a model streaming-based application is updated by streaming the new version of the model.\nFurthermore, it makes sure the model, the feature extractor, and other components are in sync.\nThus, the runtime should allow easy access to the feature extraction code for various needs, including model retraining, ad-hoc model calls, and production.\nHowever, users only see the messages that our model classiﬁed as not spam.\nSo, the action of the user is signiﬁcantly aﬀected by our model, which makes the data we get from the user skewed: we inﬂuence the phenomenon from which we learn.\nTo avoid the skew, mark a small percentage of examples as “held-out,” and show all of them to the user without pre-applying the model.\n9.2 Modes of Model Serving\nMachine learning models are served in either batch or on-demand mode.\nOn-demand, a model can be served to either a human client or a machine.\nA model is usually served in batch mode when it is applied to large quantities of input data.\nOne example could be when the model is used to exhaustively process the data of all users of a product or service.\nWhen served in batch mode, the model usually accepts between a hundred and a thousand feature vectors at once.\nThe six steps of serving the model on demand to a human are as follows:\n1) validate the request, 2) gather the context, 3) transform the context into model input, 4) apply the model to the input, and get the output, 5) make sure that the output makes sense, 6) present the output to the user.\nBefore running a model in production for a request coming from a user, it might be necessary to verify whether that user has the correct permissions for this model.\nIt will contain the information needed by the feature extractor to generate all the feature values the model expects.\nA feature extractor transforms the context into the model input.\nBefore serving the model to a human, it’s common to measure the prediction conﬁdence score.\nFor example, let the cost of making an error is estimated as 1000 dollars and the model outputs the conﬁdence score equal to 0.95, then the expected error cost value is (1 − 0.95) × 1000 = 50 dollars.\nYou might put a threshold on the expected cost value for diﬀerent actions recommended by the model, and prompt the user if the expected cost is above the threshold.\nIn addition to measuring the model’s conﬁdence, calculate whether the value of the prediction makes sense.\nIt is convenient to log the context in which the model was served, as well as the reaction of the user.",
      "keywords": [
        "Model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "Andriy Burkov Machine",
        "Model Serving Runtime",
        "Model Serving",
        "Machine learning models",
        "Burkov Machine",
        "user",
        "Model Serving Machine",
        "Learning Engineering",
        "machine",
        "learning",
        "Serving Machine learning"
      ],
      "concepts": [
        "models",
        "user",
        "version",
        "versions",
        "requests",
        "request",
        "streaming",
        "blood",
        "values",
        "contains"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 33,
          "title": "",
          "score": 0.767,
          "base_score": 0.617,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 29,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.513,
          "base_score": 0.513,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.481,
          "base_score": 0.481,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.449,
          "base_score": 0.449,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "serving",
          "model serving",
          "user",
          "serving machine",
          "runtime"
        ],
        "semantic": [],
        "merged": [
          "serving",
          "model serving",
          "user",
          "serving machine",
          "runtime"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3492403979889457,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010604+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 272-280)",
      "start_page": 272,
      "end_page": 280,
      "summary": "In machine-learning-based software, errors are an integral part of the solution: no model is perfect.\nAn error can be arbitrarily “crazy.” For example, a model for a self-driving car, at the speed of 120 km/h (~74 mph) with no obstacles, may predict that the best action is to stop and drive backward.\nFor example, the model that recognizes dangerous situations on the factory ﬂoor may start making errors after the lightbulb near the camera is replaced.\nIf you have a million users, one percent of prediction errors will aﬀect thousands.\nIt’s rare that ﬁxing one error in a model results in new errors.\nwill be directly visible to the user, calculate the expected cost of the error, as discussed above, and do not display the prediction to the user if the cost is above a threshold.\nAlternatively, train a second model mB that predicts, for an input, that the ﬁrst model mA is likely to make an error on that input.\nThe model can make two kinds of errors: 1) extract an entity even if the document doesn’t contain it (false positive, FP), and 2) not extract an entity that is present in the document (false negative, FN).\nWhen the former error happens, the user receives an irrelevant alert and gets frustrated.\nIf the latter, the user doesn’t receive any alert, remains unaware of the error, and avoids frustration.\nWhen you train a model, decide which kind of errors you would most like to avoid, and then optimize your hyperparameters, including the prediction threshold, accordingly.\nAnother way to avoid user’s frustration with model errors is to dose the user’s exposure to the model.\nMeasure the number of errors your model makes, and estimate how many errors per minute (day, week, or month) a user is ready to tolerate.\nThen limit the interactions the user will have with the model to keep the number of perceived errors below that level.\nFor situations when an error happened and might have been perceived, add a possibility for the user to report the error.\nExplain to the user what actions will be taken to prevent a similar error from happening in the future.\nTo reduce an error’s negative impact even further, if the system allows it, give the user an option to undo an action recommended by the system.\nRecall that machine-learning models’ errors can be arbitrarily “crazy” like in the example of a self-driving car that can suddenly decide to drive backward.\nIf the model predicts to buy or sell more stocks than the moving average plus one standard deviations, it’s a good idea to send an alert and put an otherwise “automatic” action on hold.\nThe same logic should apply if the model predicts to serve an unreasonably high dose of a drug to a patient, or change the speed of the car to a value substantially above or below usual.\nIf your system can automatically reject the model prediction, it’s best to implement some fallback strategy, in addition to informing the user about a failure (Figure 4).\nIn this case, an error message should be sent to the user.\nThere are two types of model change:\nThe notion of what is a correct prediction may change because of the users’ preferences and interests.\nYou might have added training examples, retrained the model and observed a better performance metric value.\nUsers interested in those classes’ predictions see decreased performance, and complain or even abandon your system.\nIf you expect that the user might negatively perceive the change, give them time to adapt.\nEducate the user about the changes and what to expect from the new model.\nAlternatively, you can run both the new and the old model in parallel, and let the user switch to the old model for some time before sunsetting it.\nA model’s output must be served in an intuitive way, without assuming that the user knows anything about machine learning and AI.\nIn fact, many users will assume that they work with typical software and will be surprised to see errors.\nIt means that the user perceives the model’s predictive capacity as too high.\n9.4 Model Monitoring\nA deployed model must be constantly monitored.\nMonitoring should be designed to provide early warnings about issues with the model in production.\nnew training data used to update the model made it perform worse; • the live data in production changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is being abused or under an adversarial attack.\nSometimes, the properties of the data in production gradually change, but the model doesn’t adapt.\nBut if the engineer fails to also update the production model, the performance may change in an unpredictable manner.\nIf your machine learning system learns from the user’s actions, then some may act to change the model behavior in their favor.\nMonitoring must allow us to make sure that the model generates reasonable performance metrics when applied to the conﬁdence test set.\nWhile it’s obvious that accuracy, precision, and recall are good candidates for monitoring, one metric is especially useful for measuring the change over time: prediction bias.\nIf you observe otherwise, the model is exhibiting prediction bias.\nThe deﬁnition or format of the data in some columns might change, while the unadapted models still assume the previous deﬁnitions and formats.\nmonitor the number of model servings during an hour, and compare it to the corre- sponding value calculated one day earlier.\nmonitor the daily number of model servings and compare it to the corresponding value calculated one week earlier.\n1) accumulate inputs by randomly putting some aside during a certain time period, 2) send those inputs for labeling, 3) run the model, and calculate the value of the performance metric, 4) alert the stakeholders if there is signiﬁcant performance degradation.\nThese models oﬀer recommendations to website or application users.\nIt can be useful to monitor click-though rate (CTR), that is, ratio of users who clicked on a recommendation to the number of total users who received recommendations from that model.\nFor visual model performance analysis, the monitoring tool’s user interface should provide trend charts showing how the model degradation evolves over time.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "model",
        "Andriy Burkov Machine",
        "user",
        "Machine Learning",
        "system",
        "Burkov Machine",
        "machine learning system",
        "Learning Engineering",
        "errors",
        "Machine",
        "machine learning model",
        "Andriy Burkov",
        "Learning"
      ],
      "concepts": [
        "models",
        "users",
        "errors",
        "predict",
        "prediction",
        "monitoring",
        "data",
        "value",
        "alerted",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 33,
          "title": "",
          "score": 0.517,
          "base_score": 0.517,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 20,
          "title": "",
          "score": 0.507,
          "base_score": 0.507,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 24,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.485,
          "base_score": 0.485,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 25,
          "title": "",
          "score": 0.431,
          "base_score": 0.431,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "user",
          "errors",
          "error",
          "change",
          "alert"
        ],
        "semantic": [],
        "merged": [
          "user",
          "errors",
          "error",
          "change",
          "alert"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3406225620171525,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:06:30.010635+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 281-289)",
      "start_page": 281,
      "end_page": 289,
      "summary": "As discussed in Section 9.2, the context might include: the content of the webpage (or the state of the application), the user’s position on the web page, time of the day, where the user came from, and what they clicked before the model prediction was served.\nthe model’s output, and time it took to generate it, • the new context of the user, once they observed the model’s output, • the user’s reaction to the output.\nThe user’s reaction is the immediate action that followed the observation of the model output: what was clicked, and how much time after the output was served.\nIn large systems with thousands of users, where the model is served to each user hundreds of times a day, it can be prohibitive to log every event.\nAlternatively, some users might want to reverse-engineer the training data, or learn how to make the model produce a desired output.\nMost production models must be regularly updated.\nhow often it makes errors and how critical they are, • how “fresh” the model should be, so as to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to deploy the model, and • how much a model update contributes to the product and the achievement of user goals.\nIn this section, we talk about model maintenance: when and how to update the model after it’s deployed in production.\nSome of them could be critical, so the model needs an update.\nIf the user utilizes a model to get recommended content on a news website, the model might need to be updated weekly.\nThe speed of availability of new training data also aﬀects the rate of model updates.\nHere, we have three repositories: data, code, and model; all three repositories are versioned.\nWe also have two runtimes: model training and production.\nWhen an update of the model is needed, the model training runtime pulls the training data, as well as the model training code, from the data and code repositories, respectively.\nIt then trains the new model and saves it in the model repository.\nFigure 6: On-demand model serving and update with a message broker.\nOnce a new version of the model is placed in the repository, the production runtime pulls,\nModel update in on-demand model serving with a message broker architecture is similar to that of model streaming (see Section 9.2.3 and Figure 3).\nFigure 6 illustrates a message-broker-based architecture that allows not just serving the model and updating it, but also contains a human labeler in the loop.\nWhen their quantity is suﬃcient to signiﬁcantly update the model, it trains a new model, saves it in the model repository, and sends the “model ready” message to the broker.\nMany companies use a continuous integration workﬂow in which the models are trained automatically as soon as new training data becomes available.\nIt is recommended to retrain the model from scratch, by using the entire training data, instead of ﬁne-tuning an existing model on the new examples only.\nOnce a new model version is deployed, validate it doesn’t make signiﬁcantly more costly errors than the previous model.\nIt’s undesirable if the new model negatively aﬀects more users from a minority or speciﬁc location.\nIf any of the above validations fail, it is not recommended to deploy the new model.\nAs discussed in Section 9.1, rolling back to the previous model must be as easy as deploying the new model.\nadditional training data made the model perform worse; • the properties of the production data changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is abused or is under an adversarial attack.\nIf the model is served to a front-end user, it’s important to log the user’s context at the moment of the model serving.\nThe log could also include the outputs obtained from the model, and time it took to generate it, the new context of the user once they observed the output of the model, and the reaction of the user to the output.\nSome users can utilize your model as a basis for their own business.\nThey might reverse- engineer the training data, or learn how to “trick” your model.\nMost machine learning models must be regularly or occasionally updated.\nhow often it makes errors and how critical they are, • how “fresh” the model should be to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to train and deploy the model, and • how much a model update contributes to the achievement of user goals.\nIt’s also important to validate that the new model doesn’t make signiﬁcantly more costly errors than the previous model.\nIt’s undesirable if the new model negatively aﬀects users from a minority or speciﬁc location.",
      "keywords": [
        "model",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Machine Learning",
        "Machine learning models",
        "Andriy Burkov Machine",
        "user",
        "data",
        "Burkov Machine",
        "Learning Engineering",
        "model update",
        "Machine",
        "Learning",
        "training data",
        "model training"
      ],
      "concepts": [
        "model",
        "user",
        "data",
        "make",
        "making",
        "training",
        "update",
        "updated",
        "business",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 31,
          "title": "",
          "score": 0.767,
          "base_score": 0.617,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 29,
          "title": "",
          "score": 0.584,
          "base_score": 0.434,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 32,
          "title": "",
          "score": 0.517,
          "base_score": 0.517,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 26,
          "title": "",
          "score": 0.458,
          "base_score": 0.458,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 30,
          "title": "",
          "score": 0.407,
          "base_score": 0.407,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "user",
          "new",
          "update",
          "new model",
          "training"
        ],
        "semantic": [],
        "merged": [
          "user",
          "new",
          "update",
          "new model",
          "training"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.28637727160416104,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010661+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 290-296)",
      "start_page": 290,
      "end_page": 296,
      "summary": "Thanks to open-source code, crowdsourcing, easily accessible books, online courses, and publicly available datasets, many scientists, engineers, and even home enthusiasts may now train machine learning models.\nMachine learning texts, online tutorials, and courses are devoted to explaining how machine learning algorithms work and how to apply them to a dataset.\nWhat data you can get and whether you can get enough of it, how you prepare it for learning, what features you engineer, whether your solution is scalable, maintainable, cannot be manipulated by attackers, and doesn’t make costly errors — these factors are much more important for an applied machine learning project.\nMost of the time, the greatest challenges must be solved before you type from sklearn.linear_model import LogisticRegression: you must deﬁne your goal, select a baseline, gather relevant data, get it labeled with quality labels, and transform labeled data into training, validation, and test sets.\nThe seasoned analyst or machine learning engineer understands that not all problems, business or otherwise, will be solved with machine learning.\nWith rare exceptions, machine learning models are blackboxes.\nMachine Learning Engineering - Draft\nSometimes the data needed to train and maintain a model is too hard or even impossible to get.\nThe machine learning project life cycle consists of the following stages: goal deﬁnition, data collection and preparation, feature engineering, model training, evaluation, deployment, serving, monitoring, and maintenance.\nFeature extraction code is one of the most important parts of a machine learning system.\nBest practices are to scale features, store and document them in schema ﬁles or feature stores, and keep code, model, and training data in sync.\nBefore starting to work on a model, make sure that data conforms to the schema, then split it into three sets: training, validation, and test.\nMost machine learning algorithms, models, and pipelines have hyperparameters.\nHowever, these are not learned from data.\nMachine Learning Engineering - Draft\nUsing a pre-trained model to build your own is called transfer learning.\nThe fact that deep models allow for transfer learning is one of its most important properties.\nYour machine learning system’s performance may beneﬁt from model stacking.\nMachine learning model errors can be either uniform, and apply to all use cases with the same rate, or focused, and apply to certain use cases more frequently.\nPerform an oﬄine model evaluation when the model is initially trained, based on the historical data.\nAn online model evaluation consists of testing and comparing models in the production environment, using online data.\nMachine Learning Engineering - Draft\nIf a data analyst has built a model in Python or R, there are several options for production deployment: rewrite the code in a compiled programming language of the production environment, use a model representation standard such as PMML or PFA, or use a specialized execution engine such as MLeap.\nMachine learning models are served in either batch or on-demand mode.\nRegularly update your model by analyzing users’ behavior and input data, to make it more robust.\nThanks to online publications and open source, I’m sure that new best practices, libraries, and frameworks simplifying or solidifying the stages of data preparation, model evaluation, deployment, serving, and monitoring, will appear during the upcoming years.\nMachine Learning Engineering - Draft\nThere are many great books on machine learning and artiﬁcial intelligence.\nIf you would like to get hands-on experience with practical machine learning in Python, there are two books:\nMachine Learning Engineering - Draft\nMachine Learning Engineering - Draft",
      "keywords": [
        "machine learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning models",
        "learning",
        "machine",
        "model",
        "machine learning project",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "machine learning algorithms",
        "machine learning system",
        "Burkov Machine",
        "data",
        "learning models"
      ],
      "concepts": [
        "models",
        "data",
        "train",
        "machines",
        "learned",
        "features",
        "errors",
        "user",
        "book",
        "deployment"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Engineering",
          "chapter": 1,
          "title": "",
          "score": 0.819,
          "base_score": 0.669,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 4,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 5,
          "title": "",
          "score": 0.723,
          "base_score": 0.573,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 6,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Machine Learning Engineering",
          "chapter": 8,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering draft",
          "draft",
          "data",
          "online",
          "books"
        ],
        "semantic": [],
        "merged": [
          "engineering draft",
          "draft",
          "data",
          "online",
          "books"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3869251697861401,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:06:30.010695+00:00"
      }
    }
  ],
  "total_chapters": 34,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Machine Learning Engineering_metadata.json",
    "enrichment_date": "2025-12-17T23:06:30.017785+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 3931.9818769999983,
    "total_similar_chapters": 166
  }
}