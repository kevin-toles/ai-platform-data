{
  "metadata": {
    "title": "Machine Learning Engineering",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 296,
    "conversion_date": "2025-11-28T12:58:04.390575",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Machine Learning Engineering.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-11)",
      "start_page": 2,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "Foreword\n\nForeword by Cassie Kozyrkov, Chief Decision Scientist at Google, author of the course Making Friends with Machine Learning on Google Cloud Platform\n\nI’d like to let you in on a secret: when people say “machine learning” it sounds like there’s only one discipline here. Surprise! There are actually two machine learnings, and they are as diﬀerent as innovating in food recipes and inventing new kitchen appliances. Both are noble callings, as long as you don’t get them confused; imagine hiring a pastry chef to build you an oven or an electrical engineer to bake bread for you!\n\nThe bad news is that almost everyone does mix these two machine learnings up. No wonder so many businesses fail at machine learning as a result. What no one seems to tell beginners is that most machine learning courses and textbooks are about Machine Learning Research — how to build ovens (and microwaves, blenders, toasters, kettles...the kitchen sink!) from scratch, not how to cook things and innovate with recipes at enormous scale. In other words, if you’re looking for opportunities to create innovative ML-based solutions to business problems, you want the discipline called Applied Machine Learning, not Machine Learning Research, so most books won’t suit your needs.\n\nAnd now for the good news! You’re looking at one of the few true Applied Machine Learning books out there. That’s right, you found one! A real applied needle in the haystack of research-oriented stuﬀ. Excellent job, dear reader...unless what you were actually looking for is a book to help you learn the skills to design general purpose algorithms, in which case I hope the author won’t be too upset with me for telling you to ﬂee now and go pick up pretty much any other machine learning book. This one is diﬀerent.\n\nWhen I created Making Friends with Machine Learning in 2016, Google’s Applied Machine Learning course loved by more than ten thousand of our engineers and leaders, I gave it a very similar structure to the one in this book. That’s because doing things in the right order is crucial in the applied space. As you use your newfound data powers, tackling certain steps before you’ve completed others can lead to anything from wasted eﬀort to a project-demolishing kablooie. In fact, the similarity in table of contents between this book and my course is what originally convinced me to give this book a read. In a clear case of convergent evolution, I saw in the author a fellow thinker kept up at night by the lack of available resources on Applied Machine Learning, one of the most potentially-useful yet horribly-misunderstood areas of engineering, enough to want to do something about it. So, if you’re about to close this book, how about you do me a quick favor and at least ponder why the Table of Contents is arranged the way it is. You’ll learn something good just from that, I promise.\n\nSo, what’s in the rest of the book? The machine learning equivalent of a bumper guide to innovating in recipes to make food at scale. Since you haven’t read the book yet, I’ll put\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nit in culinary terms: you’ll need to ﬁgure out what’s worth cooking / what the objectives are (decision-making and product management), understand the suppliers and the customers (domain expertise and business acumen), how to process ingredients at scale (data engineering and analysis), how to try many diﬀerent ingredient-appliance combinations quickly to generate potential recipes (prototype phase ML engineering), how to check that the quality of the recipe is good enough to serve (statistics), how to turn a potential recipe into millions of dishes served eﬃciently (production phase ML engineering), and how to ensure that your dishes stay top notch even if the delivery truck brings you a ton of potatoes instead of the rice you ordered (reliability engineering). This book is one of the few to oﬀer perspectives on each step of the end-to-end process.\n\nNow would be a good moment for me to be blunt with you, dear reader. This book is pretty good. It is. Really. But it’s not perfect. It cuts corners on occasion — just like a professional machine learning engineer is wont to do — though on the whole it gets its message right. And, since it covers an area with rapidly-evolving best practices, it doesn’t pretend to oﬀer the last word on the subject. But even if it were terribly sloppy, it would still be worth reading. Given how few comprehensive guides to Applied Machine Learning are out there, a coherent introduction to these topics is worth its weight in gold. I’m so glad this one is here!\n\nOne of my favorite things about this book is how fully it embraces the most important thing you need to know about machine learning: mistakes are possible...and sometimes they hurt. As my colleagues in site reliability engineering love to say, “Hope is not a strategy.” Hoping that there will be no mistakes is the worst approach you can take. This book does so much better. It promptly shatters any false sense of security you were tempted to have about building an AI system that is more “intelligent” than you are. (Um, no. Just no.) Then it diligently takes you through a survey of all kinds of things that can go wrong in practice and how to prevent/detect/handle them. This book does a great job of outlining the importance of monitoring, how to approach model maintenance, what to do when things go wrong, how to think about fallback strategies for the kinds of mistakes you can’t anticipate, how to deal with adversaries who try to exploit your system, and how to manage the expectations of your human users (there’s also a section on what to do when your, er, users are machines). These are hugely important topics in practical machine learning, but they’re so often neglected in other books. Not here.\n\nIf you intend to use machine learning to solve business problems at scale, I’m delighted you got your hands on this book. Enjoy!\n\nCassie Kozyrkov\n\nSeptember 2020\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\nPreface\n\nDuring the past several years, machine learning (ML), for many, has become a synonym for artiﬁcial intelligence. Even though machine learning, as a ﬁeld of science, has existed for several decades, only a handful of organizations in the world have fully harnessed its potential. Despite the availability of modern open-source machine learning libraries, packages and frameworks supported by the leading organizations and broad communities of scientists and software engineers, most organizations are still struggling to apply machine learning for solving practical business problems.\n\nOne diﬃculty lies in the scarcity of talent. However, even when they have access to talented machine learning engineers and data analysts, in 2020, most organizations1 still spend between 31 and 90 days deploying one model, while 18 percent of companies are taking longer than 90 days — some spending more than a year productionizing. The main challenges organizations face when developing ML capabilities, such as model version control, reproducibility, and scaling, are rather engineering than scientiﬁc.\n\nThere are plenty of good books on machine learning, both theoretical and hands-on. From a typical machine learning book, you can learn the types of machine learning, major families of algorithms, how they work, and how to build models from data using those algorithms.\n\nA typical machine learning book is less concerned with the engineering aspects of implementing machine learning projects. Such questions as data collection, storage, preprocessing, feature engineering, as well as testing and debugging of models, their deployment to and retirement from production, runtime and post-production maintenance, are often left outside the scope of machine learning books.\n\nThis book intends to ﬁll that gap.\n\nWho This Book is For\n\nI assume that the reader of this book understands machine learning basics and is capable of building a model, given a properly formatted dataset using a favorite programming language or a machine learning library. If you don’t feel comfortable applying machine learning algorithms to data and don’t clearly see the diﬀerence between logistic regression, support vector machine, and random forest, I recommend starting your journey with The Hundred-Page Machine Learning Book, and then move to this book.\n\nThe target audience of this book is data analysts who lean towards a machine learning engineering role, machine learning engineers who want to bring more structure to their work, machine learning engineering students, as well as software architects who happen to deal with models provided by data analysts and machine learning engineers.\n\n1“2020 state of enterprise machine learning”, Algorithmia, 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nHow to Use This Book\n\nThis book is a comprehensive review of machine learning engineering best practices and design patterns. I recommend reading it from beginning to end. However, you can read chapters in any order as they cover distinct aspects of the machine learning project lifecycle and do not have direct dependencies.\n\nShould You Buy This Book?\n\nLike its companion and precursor The Hundred-Page Machine Learning Book, this book is distributed on the “read-ﬁrst, buy-later” principle. I ﬁrmly believe that readers must be able to read a book before paying for it; otherwise, they buy a pig in a poke.\n\nThe “read-ﬁrst, buy-later” principle implies that you can freely download the book, read it, and share it with your friends and colleagues. If you read and liked the book, or found it helpful or useful in your work, business, or studies, then buy it.\n\nNow you are all set. Enjoy your reading!\n\nAndriy Burkov\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n1 Introduction\n\nThough the reader of this book should have a basic understanding of machine learning, it is still important to start with deﬁnitions, so that we are sure that we have a common understanding of the terms used throughout the book.\n\nBelow, I repeat some of the deﬁnitions from Chapter 2 of The Hundred-Page Machine Learning Book and also give several new ones. If you read my ﬁrst book, some parts of this chapter might sound familiar.\n\nAfter reading this chapter, we will understand the same way such concepts as supervised and unsupervised learning. We will agree on the data terminology, such as data used directly and indirectly, raw and tidy data, training and holdout data.\n\nWe will know when to use machine learning, when not to use it, and various forms of machine learning such as model- and instance-based, deep and shallow, classiﬁcation and regression, and others.\n\nFinally, we will deﬁne the scope of machine learning engineering and introduce the machine learning project lifecycle.\n\n1.1 Notation and Deﬁnitions\n\nLet’s start by stating the basic mathematical notation and deﬁne the terms and notions, to which we will often have recourse in this book.\n\n1.1.1 Data Structures\n\nA scalar1 is a simple numerical value, like 15 or −3.25. Variables or constants that take scalar values are denoted by an italic letter, like x or a.\n\nA vector is an ordered list of scalar values, called attributes. We denote a vector as a bold character, for example, x or w. Vectors can be visualized as arrows that point to some directions as well as points in a multi-dimensional space. Illustrations of three two-dimensional vectors, a = [2,3], b = [−2,5], and c = [1,0] are given in Figure 1. We denote an attribute of a vector as an italic value with an index, like this: w(j) or x(j). The index j denotes a speciﬁc dimension of the vector, the position of an attribute in the list. For instance, in the vector a shown in red in Figure 1, a(1) = 2 and a(2) = 3.\n\n1If a term is in bold, that means that the term can be found in the index at the end of the book.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nFigure 1: Three vectors visualized as directions and as points.\n\nThe notation x(j) should not be confused with the power operator, such as the 2 in x2 (squared) or 3 in x3 (cubed). If we want to apply a power operator, say squared, to an indexed attribute of a vector, we write like this: (x(j))2.\n\nA variable can have two or more indices, like this: x(j) neural networks, we denote as x(j) l,u A matrix is a rectangular array of numbers arranged in rows and columns. Below is an example of a matrix with two rows and three columns,\n\nor like this x(k) i,j i the input feature j of unit u in layer l.\n\n. For example, in\n\nA =\n\n(cid:20)2 −2 5 3\n\n(cid:21) 1 0\n\n.\n\nMatrices are denoted with bold capital letters, such as A or W. You can notice from the above example of matrix A that matrices can be seen as regular structures composed of vectors. Indeed, the columns of matrix A above are vectors a, b, and c illustrated in Figure 1.\n\nA set is an unordered collection of unique elements. We denote a set as a calligraphic capital character, for example, S. A set of numbers can be ﬁnite (include a ﬁxed amount of values). In this case, it is denoted using accolades, for example, {1,3,18,23,235} or {x1,x2,x3,x4,...,xn}. Alternatively, a set can be inﬁnite and include all values in some inter- val. If a set includes all values between a and b, including a and b, it is denoted using brackets as [a,b]. If the set doesn’t include the values a and b, such a set is denoted using parentheses like this: (a,b). For example, the set [0,1] includes such values as 0, 0.0001, 0.25, 0.784, 0.9995, and 1.0. A special set denoted R includes all numbers from minus inﬁnity to plus inﬁnity.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nWhen an element x belongs to a set S, we write x ∈ S. We can obtain a new set S3 as an intersection of two sets S1 and S2. In this case, we write S3 ← S1 ∩ S2. For example {1,3,5,8} ∩ {1,8,4} gives the new set {1,8}. We can obtain a new set S3 as a union of two sets S1 and S2. S3 ← S1 ∪ S2. For example {1,3,5,8} ∪ {1,8,4} gives the new set {1,3,5,8,4}. The notation |S| means the size of set S, that is, the number of elements it contains.\n\nIn this case, we write\n\n1.1.2 Capital Sigma Notation\n\nThe summation over a collection X = {x1,x2,...,xn−1,xn} or over the attributes of a vector x = [x(1),x(2),...,x(m−1),x(m)] is denoted like this:\n\nn X\n\nxi\n\ndef= x1 + x2 + ... + xn−1 + xn, or else:\n\nm X\n\nx(j) def= x(1) + x(2) + ... + x(m−1) + x(m).\n\ni=1\n\nj=1\n\nThe notation def= means “is deﬁned as”.\n\nThe Euclidean norm of a vector x, denoted by kxk, characterizes the “size” or the “length” of the vector. It’s given by\n\nqPD\n\n(cid:0)x(j)(cid:1)2.\n\nj=1\n\nThe distance between two vectors a and b is given by the Euclidean distance:\n\nka − bk def=\n\nv u u t\n\nN X\n\n(cid:0)a(i) − b(i)(cid:1)2\n\n.\n\ni=1\n\n1.2 What is Machine Learning\n\nMachine learning is a subﬁeld of computer science that is concerned with building al- gorithms that, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans, or generated by another algo- rithm.\n\nMachine learning can also be deﬁned as the process of solving a practical problem by,\n\n1) collecting a dataset, and 2) algorithmically training a statistical model based on that dataset.\n\nThat statistical model is assumed to be used somehow to solve the practical problem. To save keystrokes, I use the terms “learning” and “machine learning” interchangeably. For the same reason, I often say “model” referring to a statistical model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nLearning can be supervised, semi-supervised, unsupervised, and reinforcement.\n\n1.2.1 Supervised Learning\n\nIn supervised learning, the data analyst works with a collection of labeled examples {(x1,y1),(x2,y2),...,(xN,yN)}. Each element xi among N is called a feature vector. In computer science, a vector is a one-dimensional array. A one-dimensional array, in turn, is an ordered and indexed sequence of values. The length of that sequence of values, D, is called the vector’s dimensionality.\n\nA feature vector is a vector in which each dimension j from 1 to D contains a value that describes the example. Each such value is called a feature and is denoted as x(j). For instance, if each example x in our collection represents a person, then the ﬁrst feature, x(1), could contain height in cm, the second feature, x(2), could contain weight in kg, x(3) could contain gender, and so on. For all examples in the dataset, the feature at position j in the feature vector always contains the same kind of information. It means that if x(2) contains weight in kg in some example xi, then x(2) will also contain weight in kg in every example k xk, for all k from 1 to N. The label yi can be either an element belonging to a ﬁnite set of classes {1,2,...,C}, or a real number, or a more complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated, in this book yi is either one of a ﬁnite set of classes or a real number.2 You can think of a class as a category to which an example belongs.\n\ni\n\nFor instance, if your examples are email messages and your problem is spam detection, then you have two classes: spam and not_spam. In supervised learning, the problem of predicting a class is called classiﬁcation, while the problem of predicting a real number is called regression. The value that has to be predicted by a supervised model is called a target. An example of regression is a problem of predicting the salary of an employee given their work experience and knowledge. An example of classiﬁcation is when a doctor enters the charac- teristics of a patient into a software application, and the application returns the diagnosis.\n\nThe diﬀerence between classiﬁcation and regression is shown in Figure 2. In classiﬁcation, the learning algorithm looks for a line (or, more generally, a hypersurface) that separates examples of diﬀerent classes from one another. In regression, on the other hand, the learning algorithm looks to ﬁnd a line or a hypersurface that closely follows the training examples.\n\n2A real number is a quantity that can represent a distance along a line. Examples: 0, −256.34, 1000,\n\n1000.2.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "page_number": 2,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 2-11). Key topics include machine, learning, and engineer.",
      "keywords": [
        "Machine Learning",
        "Machine Learning Engineering",
        "machine learning book",
        "Applied Machine Learning",
        "Burkov Machine Learning",
        "Learning",
        "Machine",
        "Machine Learning Research",
        "machine learning engineers",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "learning book",
        "book",
        "Hundred-Page Machine Learning",
        "Applied Machine"
      ],
      "concepts": [
        "machine",
        "learning",
        "engineer",
        "engineering",
        "data",
        "books",
        "applied",
        "apply",
        "applying",
        "liked"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-19)",
      "start_page": 12,
      "end_page": 19,
      "detection_method": "topic_boundary",
      "content": "classiﬁcationregression\n\nFigure 2: Diﬀerence between classiﬁcation and regression.\n\nThe goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector. For instance, a model created using a dataset of patients could take as input a feature vector describing a patient and output a probability that the patient has cancer.\n\nEven if the model is typically a mathematical function, when thinking about what the model does with the input, it is convenient to think that the model “looks” at the values of some features in the input and, based on experience with similar examples, outputs a value. That output value is a number or a class “the most similar” to the labels seen in the past in the examples with similar values of features. It looks simplistic, but the decision tree model and the k-nearest neighbors algorithm work almost like that.\n\n1.2.2 Unsupervised Learning\n\nIn unsupervised learning, the dataset is a collection of unlabeled examples {x1,x2,...,xN}. Again, x is a feature vector, and the goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practical problem. For example, in clustering, the model returns the ID of the cluster for each feature vector in the dataset. Clustering is useful for ﬁnding groups of similar objects in a large collection of objects, such as images or text documents. By using clustering, for example, the analyst can sample a suﬃciently representative yet small subset of unlabeled examples from a large\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\ncollection of examples for manual labeling: a few examples are sampled from each cluster instead of sampling directly from the large collection and risking only sampling examples very similar to one another.\n\nIn dimensionality reduction, the model’s output is a feature vector with fewer dimensions than the input. For example, the scientist has a feature vector that is too complex to visualize (it has more than three dimensions). The dimensionality reduction model can transform that feature vector into a new feature vector (by preserving the information up to some extent) with only two or three dimensions. This new feature vector can be plotted on a graph.\n\nIn outlier detection, the output is a real number that indicates how the input feature vector is diﬀerent from a “typical” example in the dataset. Outlier detection is useful for solving a network intrusion problem (by detecting abnormal network packets that are diﬀerent from a typical packet in “normal” traﬃc) or detecting novelty (such as a document diﬀerent from the existing documents in a collection).\n\n1.2.3 Semi-Supervised Learning\n\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples. Usually, the quantity of unlabeled examples is much higher than the number of labeled examples. The goal of a semi-supervised learning algorithm is the same as the goal of the supervised learning algorithm. The hope here is that, by using many unlabeled examples, a learning algorithm can ﬁnd (we might say “produce” or “compute”) a better model.\n\n1.2.4 Reinforcement Learning\n\nReinforcement learning is a subﬁeld of machine learning where the machine (called an agent) “lives’’ in an environment and is capable of perceiving the state of that environment as a vector of features. The machine can execute actions in non-terminal states. Diﬀerent actions bring diﬀerent rewards and could also move the machine to another state of the environment. A common goal of a reinforcement learning algorithm is to learn an optimal policy.\n\nAn optimal policy is a function (similar to the model in supervised learning) that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average long-term reward.\n\nReinforcement learning solves a particular problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics.\n\nIn this book, for simplicity, most explanations are limited to supervised learning. However, all the material presented in the book is applicable to other types of machine learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\n1.3 Data and Machine Learning Terminology\n\nNow let’s introduce the common data terminology (such as data used directly and indirectly, raw and tidy data, training and holdout data) and the terminology related to machine learning (such as baseline, hyperparameter, pipeline, and others).\n\n1.3.1 Data Used Directly and Indirectly\n\nThe data you will work with in your machine learning project can be used to form the examples x directly or indirectly.\n\nImagine that we build a named entity recognition system. The input of the model is a sequence of words; the output is the sequence of labels3 of the same length as the input. To make the data readable by a machine learning algorithm, we have to transform each natural language word into a machine-readable array of attributes, which we call a feature vector.4 Some features in the feature vector may contain the information that distinguishes that speciﬁc word from other words in the dictionary. Other features can contain additional attributes of the word in that speciﬁc sequence, such as its shape (lowercase, uppercase, capitalized, and so on). Or it can be binary attributes indicating whether this word is the ﬁrst word of some human name or the last word of the name of some location or organization. To create these latter binary features, we may decide to use some dictionaries, lookup tables, gazetteers, or other machine learning models making predictions about words.\n\nYou could already have noticed that the collection of word sequences is the data used to form training examples directly, while the data contained in dictionaries, lookup tables, and gazetteers is used indirectly: we can use it to extend feature vectors with additional features, but we cannot use it to create new feature vectors.\n\n1.3.2 Raw and Tidy Data\n\nAs we just discussed, directly used data is a collection of entities that constitute the basis of a dataset. Each entity in that collection can be transformed into a training example. Raw data is a collection of entities in their natural form; they cannot always be directly employable for machine learning. For instance, a Word document or a JPEG ﬁle are pieces of raw data; they cannot be directly used by a machine learning algorithm.5\n\n3Labels can be, for example, values from the set {“Location”, “Organization”, “Person”, “Other”}. 4The terms “attribute” and “feature” are often used interchangeably. In this book, I use the term “attribute” to describe a speciﬁc property of an example, while the term “feature” refers to value x(j) at position j in the feature vector x used by a machine learning algorithm.\n\n5The term “unstructured data” is often used to designate a data element that contains information whose type was not formally deﬁned. Examples of unstructured data are photos, images, videos, text messages, social media posts, PDFs, text documents, and emails. The term “semi-structured data” refers to data elements whose structure helps deriving types of some information encoded in those data elements. Examples of semi-structured data include log ﬁles, comma- and tab-delimited text ﬁles, as well as documents in JSON and XML formats.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nTo be employable in machine learning, a necessary (but not suﬃcient) condition for the data is to be tidy. Tidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example, as shown in Figure 3. Sometimes raw data can be tidy, e.g., provided to you in the form of a spreadsheet. However, in practice, to obtain tidy data from raw data, data analysts often resort to the procedure called feature engineering, which is applied to the direct and, optionally, indirect data with the goal to transform each raw example into a feature vector x. Chapter 4 is devoted entirely to feature engineering.\n\nGermany\n\nGermany\n\nChina\n\n83M\n\n67M\n\nAisa\n\n3.7T\n\n...\n\n...\n\n1386M\n\n12.2T\n\nRegion\n\nPopulation\n\nEurope\n\nEurope\n\n...\n\nGDP\n\nCountry\n\nFrance\n\n2.6T\n\n...\n\nCountry\n\nFrance\n\n...\n\nChina\n\n83M\n\n67M\n\n2.6T\n\nPopulation\n\n...\n\n...\n\n3.7T\n\n12.2T\n\nRegion\n\nEurope\n\nEurope\n\n...\n\nAsia\n\nattributesexamples\n\n1386M\n\nGDP\n\nFigure 3: Tidy data: examples are rows and attributes are columns.\n\nIt’s important to note here that for some tasks, an example used by a learning algorithm can have a form of a sequence of vectors, a matrix, or a sequence of matrices. The notion of data tidiness for such algorithms is deﬁned similarly: you only replace “row of ﬁxed width in a spreadsheet” by a matrix of ﬁxed width and height, or a generalization of matrices to a higher dimension called a tensor.\n\nThe term “tidy data” was coined by Hadley Wickham in his paper with the same title.6\n\nAs I mentioned at the beginning of this subsection, data can be tidy, but still not usable by a particular machine learning algorithm. Most machine learning algorithms, in fact, only accept training data in the form of a collection of numerical feature vectors. Consider the data shown in Figure 3. The attribute “Region” is categorical and not numerical. The decision tree learning algorithm can work with categorical values of attributes, but most learning algorithms cannot. In Section ?? of Chapter 4, we will see how to transform a categorical attribute into a numerical feature.\n\nNote that in the academic machine learning literature, the word “example” typically refers to a tidy data example with an optionally assigned label. However, during the stage of data collection and labeling, which we consider in the next chapter, examples can still be in the raw form: images, texts, or rows with categorical attributes in a spreadsheet. In this book, when it’s important to highlight the diﬀerence, I will say raw example to indicate that a\n\n6Wickham, Hadley. “Tidy data.” Journal of Statistical Software 59.10 (2014): 1-23.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\npiece of data was not transformed into a feature vector yet. Otherwise, assume that examples have the form of feature vectors.\n\n1.3.3 Training and Holdout Sets\n\nIn practice, data analysts work with three distinct sets of examples:\n\n1) training set, 2) validation set,7 and 3) test set.\n\nOnce you have got the data in the form of a collection of examples, the ﬁrst thing you do in your machine learning project is shuﬄe the examples and split the dataset into three distinct sets: training, validation, and test. The training set is usually the biggest one; the learning algorithm uses the training set to produce the model. The validation and test sets are roughly the same size, much smaller than the size of the training set. The learning algorithm is not allowed to use examples from the validation or test sets to train the model. That is why those two sets are also called holdout sets.\n\nThe reason to have three sets, and not one, is simple: when we train a model, we don’t want the model to only do well at predicting labels of examples the learning algorithm has already seen. A trivial algorithm that simply memorizes all training examples and then uses the memory to “predict” their labels will make no mistakes when asked to predict the labels of the training examples. However, such an algorithm would be useless in practice. What we really want is a model that is good at predicting examples that the learning algorithm didn’t see. In other words, we want good performance on a holdout set.8\n\nWe need two holdout sets and not one because we use the validation set to 1) choose the learning algorithm, and 2) ﬁnd the best conﬁguration values for that learning algorithm (known as hyperparameters). We use the test set to assess the model before delivering it to the client or putting it in production. That is why it’s important to make sure that no information from the validation or test sets is exposed to the learning algorithm. Otherwise, the validation and test results will most likely be too optimistic. This can indeed happen due to data leakage, an important phenomenon we consider in Section ?? of Chapter 3 and subsequent chapters.\n\n7In some literature, the validation set can also be called “development set.” Sometimes, when the labeled examples are scarce, analysts can decide to work without a validation set, as we will see in Chapter 5 in the section on cross-validation.\n\n8To be precise, we want the model to do well on most random samples from the statistical distribution to which our data belongs. We assume that if the model demonstrates good performance on a holdout set, randomly drawn from the unknown distribution of our data, there are high chances that our model will do well on other random samples of our data.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\n1.3.4 Baseline\n\nIn machine learning, a baseline is a simple algorithm for solving a problem, usually based on a heuristic, simple summary statistics, randomization, or very basic machine learning algorithm. For example, if your problem is classiﬁcation, you can pick a baseline classiﬁer and measure its performance. This baseline performance will then become what you compare any future model to (usually, built using a more sophisticated approach).\n\n1.3.5 Machine Learning Pipeline\n\nA machine learning pipeline is a sequence of operations on the dataset that goes from its initial state to the model.\n\nA pipeline can include, among others, such stages as data partitioning, missing data im- putation, feature extraction, data augmentation, class imbalance reduction, dimensionality reduction, and model training.\n\nIn practice, when we deploy a model in production, we usually deploy an entire pipeline. Furthermore, an entire pipeline is usually optimized when hyperparameters are tuned.\n\n1.3.6 Parameters vs. Hyperparameters\n\nHyperparameters are inputs of machine learning algorithms or pipelines that inﬂuence the performance of the model. They don’t belong to the training data and cannot be learned from it. For example, the maximum depth of the tree in the decision tree learning algorithm, the misclassiﬁcation penalty in support vector machines, k in the k-nearest neighbors algorithm, the target dimensionality in dimensionality reduction, and the choice of the missing data imputation technique are all examples of hyperparameters.\n\nParameters, on the other hand, are variables that deﬁne the model trained by the learning algorithm. Parameters are directly modiﬁed by the learning algorithm based on the training data. The goal of learning is to ﬁnd such values of parameters that make the model optimal in a certain sense. Examples of parameters are w and b in the equation of linear regression y = wx+b. In this equation, x is the input of the model, and y is its output (the prediction).\n\n1.3.7 Classiﬁcation vs. Regression\n\nClassiﬁcation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classiﬁcation.\n\nIn machine learning, the classiﬁcation problem is solved by a classiﬁcation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\nnumber that can be used by the analyst to deduce the label. An example of such a number is a probability of an input data element to have a speciﬁc label.\n\nIn a classiﬁcation problem, a label is a member of a ﬁnite set of classes. If the size of the set of classes is two (“sick”/“healthy”, “spam”/“not_spam”), we talk about binary classiﬁcation (also called binomial in some sources). Multiclass classiﬁcation (also called multinomial) is a classiﬁcation problem with three or more classes.9\n\nWhile some learning algorithms naturally allow for more than two classes, others are by nature binary classiﬁcation algorithms. There are strategies to turn a binary classiﬁcation learning algorithm into a multiclass one. I talk about one of them, one-versus-rest, in Section ?? of Chapter 6.\n\nRegression is a problem of predicting a real-valued quantity given an unlabeled example. Estimating house price valuation based on house features, such as area, number of bedrooms, location, and so on, is a famous example of regression.\n\nThe regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target.\n\n1.3.8 Model-Based vs. Instance-Based Learning\n\nMost supervised learning algorithms are model-based. A typical model is a support vector machine (SVM). Model-based learning algorithms use the training data to create a model with parameters learned from the training data. In SVM, the two parameters are w (a vector) and b (a real number). After the model is trained, it can be saved on disk while the training data can be discarded.\n\nInstance-based learning algorithms use the whole dataset as the model. One instance- based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classiﬁcation, to predict a label for an input example, the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw most often in this close neighborhood.\n\n1.3.9 Shallow vs. Deep Learning\n\nA shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most machine learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speciﬁcally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning),\n\n9There’s still one label per example, though.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\ncontrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers.\n\n1.3.10 Training vs. Scoring\n\nWhen we apply a machine learning algorithm to a dataset in order to obtain a model, we talk about model training or simply training.\n\nWhen we apply a trained model to an input example (or, sometimes, a sequence of examples) in order to obtain a prediction (or, predictions) or to somehow transform an input, we talk about scoring.\n\n1.4 When to Use Machine Learning\n\nMachine learning is a powerful tool for solving practical problems. However, like any tool, it should be used in the right context. Trying to solve all problems using machine learning would be a mistake.\n\nYou should consider using machine learning in one of the following situations.\n\n1.4.1 When the Problem Is Too Complex for Coding\n\nIn a situation where the problem is so complex or big that you cannot hope to write all the rules to solve it and where a partial solution is viable and interesting, you can try to solve the problem with machine learning.\n\nOne example is spam detection: it’s impossible to write the code that will implement such a logic that will eﬀectively detect spam messages and let genuine messages reach the inbox. There are just too many factors to consider. For instance, if you program your spam ﬁlter to reject all messages from people who are not in your contacts, you risk losing messages from someone who has got your business card at a conference. If you make an exception for messages containing speciﬁc keywords related to your work, you will probably miss a message from your child’s teacher, and so on.\n\nIf you still decide to directly program a solution to that complex problem, with time, you will have in your programming code so many conditions and exceptions from those conditions that maintaining that code will eventually become infeasible. In this situation, training a classiﬁer on examples “spam”/“not_spam” seems logical and the only viable choice.\n\nAnother diﬃculty for writing code to solve a problem lies in the fact that humans have a hard time with prediction problems based on input that has too many parameters; it’s especially true when those parameters are correlated in unknown ways. For example, take the problem of predicting whether a borrower will repay a loan. Hundreds of numbers represent each borrower: age, salary, account balance, frequency of past payments, married or not, number\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "page_number": 12,
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 12-19). Key topics include examples, data, and model. A variable can have two or more indices, like this: x(j) neural networks, we denote as x(j) l,u A matrix is a rectangular array of numbers arranged in rows and columns.",
      "keywords": [
        "Machine Learning",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Learning",
        "learning algorithm",
        "feature vector",
        "machine learning algorithm",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "data",
        "Machine",
        "feature",
        "Burkov Machine",
        "vector",
        "Supervised Learning"
      ],
      "concepts": [
        "examples",
        "data",
        "model",
        "features",
        "learning",
        "sets",
        "algorithm",
        "capital",
        "capitalized",
        "contains"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "Segment 30 (pages 257-264)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 20-27)",
      "start_page": 20,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "of children, make and year of the car, mortgage balance, and so on. Some of those numbers may be important to make the decision, some may be less important alone, but become more important if considered in combination with some other numbers.\n\nWriting code that will make such decisions is hard because, even for an expert, it’s not clear how to combine, in an optimal way, all the attributes describing a person into a prediction.\n\n1.4.2 When the Problem Is Constantly Changing\n\nSome problems may continuously change with time so that the programming code must be regularly updated. That results in the frustration of software engineers working on the problem, an increased chance of introducing errors, diﬃculties of combining “previous” and “new” logic, and signiﬁcant overhead of testing and deploying updated solutions.\n\nFor example, you can have a task of scraping speciﬁc data elements from a collection of webpages. Let’s say that for each webpage in that collection, you write a set of ﬁxed data extraction rules in the following form: “pick the third <p> element from <body> and then pick the data from the second <div> inside that <p>.” If a website owner changes the design of a webpage, the data you scrape may end up in the second or the fourth <p> element, making your extraction rule wrong. If the collection of webpages you scrape is large (thousands of URLs), every day you will have rules that become wrong; you will end up endlessly ﬁxing those rules. Needless to say that very few software engineers would love to do such work on a daily basis.\n\n1.4.3 When It Is a Perceptive Problem\n\nToday, it’s hard to imagine someone trying to solve perceptive problems such as speech, image, and video recognition without using machine learning. Consider an image. It’s represented by millions of pixels. Each pixel is given by three numbers: the intensity of red, green, and blue channels. In the past, engineers tried to solve the problem of image recognition (detecting what’s on the picture) by applying handcrafted “ﬁlters” to square patches of pixels. If one ﬁlter, for example, the one that was designed to “detect” grass, generates a high value when applied to many pixel patches, while another ﬁlter, designed to detect brown fur, also returns high values for many patches, then we can say that there are high chances that the image represents a cow in a ﬁeld (I’m simplifying a bit).\n\nToday, perceptive problems are eﬀectively solved using machine learning models, such as neural networks. We consider the problem of training neural networks in Chapter 6.\n\n1.4.4 When It Is an Unstudied Phenomenon\n\nIf we need to be able to make predictions of some phenomenon that is not well-studied scientiﬁcally, but examples of it are observable, then machine learning might be an appropriate\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\n(and, in some cases, the only available) option. For example, machine learning can be used to generate personalized mental health medication options based on the patient’s genetic and sensory data. Doctors might not necessarily be able to interpret such data to make an optimal recommendation, while a machine can discover patterns in data by analyzing thousands of patients and predicting which molecule has the highest chance to help a given patient.\n\nAnother example of observable but unstudied phenomena are logs of a complex computing system or a network. Such logs are generated by multiple independent or interdependent processes. For a human, it’s hard to make predictions about the future state of the system based on logs alone without having a model of each process and their interdependency. If the number of examples of historical log records is high enough (which is often the case), the machine can learn patterns hidden in logs and be able to make predictions without knowing anything about each process.\n\nFinally, making predictions about people based on their observed behavior is hard. In this problem, we obviously cannot have a model of a person’s brain, but we have readily available examples of expressions of the person’s ideas (in the form of online posts, comments, and other activities). Based on those expressions alone, a machine learning model deployed in a social network can recommend the content or other people to connect with.\n\n1.4.5 When the Problem Has a Simple Objective\n\nMachine learning is especially suitable for solving problems that you can formulate as a problem with a simple objective: such as yes/no decisions or a single number. In contrast, you cannot use machine learning to build a model that works as a general video game, like Mario, or a word processing software, like Word. This is due to too many diﬀerent decisions to make: what to display, where and when, what should happen as a reaction to the user’s input, what to write to or read from the hard drive, and so on; getting examples that illustrate all (or even most) of those decisions is practically infeasible.\n\n1.4.6 When It Is Cost-Eﬀective\n\nThree major sources of cost in machine learning are:\n\ncollecting, preparing, and cleaning the data, • training the model, • building and running the infrastructure to serve and and monitor the model, as well as labor resources to maintain it.\n\nThe cost of training the model includes human labor and, in some cases, the expensive hardware needed to train deep models. Model maintenance includes continuously monitoring the model and collecting additional data to keep the model up to date.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\n1.5 When Not to Use Machine Learning\n\nThere are plenty of problems that cannot be solved using machine learning; it’s hard to characterize all of them. Here we only consider several hints.\n\nYou probably should not use machine learning when:\n\nevery action of the system or a decision made by it must be explainable, • every change in the system’s behavior compared to its past behavior in a similar situation must be explainable,\n\nthe cost of an error made by the system is too high, • you want to get to the market as fast as possible, • getting the right data is too hard or impossible, • you can solve the problem using traditional software development at a lower cost, • a simple heuristic would work reasonably well, • the phenomenon has too many outcomes while you cannot get a suﬃcient amount of examples to represent them (like in video games or word processing software),\n\nyou build a system that will not have to be improved frequently over time, • you can manually ﬁll an exhaustive lookup table by providing the expected output for any input (that is, the number of possible input values is not too large, or getting outputs is fast and cheap).\n\n1.6 What is Machine Learning Engineering\n\nMachine learning engineering (MLE) is the use of scientiﬁc principles, tools, and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE encompasses all stages from data collection, to model training, to making the model available for use by the product or the customers.\n\nTypically, a data analyst10 is concerned with understanding the business problem, building a model to solve it, and evaluating the model in a restricted development environment. A machine learning engineer, in turn, is concerned with sourcing the data from various systems and locations and preprocessing it, programming features, training an eﬀective model that will run in the production environment, coexist well with other production processes, be stable, maintainable, and easily accessible by diﬀerent types of users with diﬀerent use cases.\n\nIn other words, MLE includes any activity that lets machine learning algorithms be imple- mented as a part of an eﬀective production system.\n\nIn practice, machine learning engineers might be employed in such activities as rewriting a\n\n10Since circa 2013, data scientist has become a popular job title. Unfortunately, companies and experts don’t have an agreement on the deﬁnition of the term. Instead, I use the term “data analyst” by referring to a person capable of applying numerical or statistical analysis to data ready for analysis.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\ndata analyst’s code from rather slow R and Python11 into more eﬃcient Java or C++, scaling this code and making it more robust, packaging the code into an easy-to-deploy versioned package, optimizing the machine learning algorithm to make sure that it generates a model compatible with, and running correctly in, the organization’s production environment.\n\nIn many organizations, data analysts execute some of the MLE tasks, such as data collection, transformation, and feature engineering. On the other hand, machine learning engineers often execute some of the data analysis tasks, including learning algorithm selection, hyperparameter tuning, and model evaluation.\n\nWorking on a machine learning project is diﬀerent from working on a typical software engineering project. Unlike traditional software, where a program’s behavior usually is deterministic, machine learning applications incorporate models whose behavior may naturally degrade over time, or they can start behaving abnormally. Such abnormal behavior of the model might be explained by various reasons, including a fundamental change in the input data or an updated feature extractor that now returns a diﬀerent distribution of values or values of a diﬀerent type. They often say that machine learning systems “fail silently.” A machine learning engineer must be capable of preventing such failures or, when it’s impossible to prevent them entirely, know how to detect and handle them when they happen.\n\n1.7 Machine Learning Project Life Cycle\n\nA machine learning project starts with understanding the business objective. Usually, a business analyst works with the client12 and the data analyst to transform a business problem into an engineering project. The engineering project may or may not have a In this book, we, of course, consider engineering projects that machine learning part. have some machine learning involved.\n\nOnce an engineering project is deﬁned, this is where the scope of the machine learning engineering starts. In the scope of a broader engineering project, machine learning must ﬁrst have a well-deﬁned goal. The goal of machine learning is a speciﬁcation of what a statistical model receives as input, what it generates as output, and the criteria of acceptable (or unacceptable) behavior of the model.\n\nThe goal of machine learning is not necessarily the same as the business objective. The business objective is what the organization wants to achieve. For example, the business objective of Google with Gmail can be to make Gmail the most-used email service in the world. Google might create multiple machine learning engineering projects to achieve that business objective. The goal of one of those machine learning projects can be to distinguish Primary emails from Promotions with accuracy above 90%.\n\n11Many scientiﬁc modules in Python are indeed implemented in fast C/C++; however, data analyst’s own\n\nPython code can still be slow.\n\n12If the machine learning project supports a product developed and sold by the organization, then the\n\nbusiness analyst works with the product owner.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\nFigure 4: Machine learning project life cycle.\n\nOverall, a machine learning project life cycle, illustrated in Figure 4, consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\n\nIn Figure 4, the scope of machine learning engineering (and the scope of this book) is limited by the blue zone. The solid arrows show a typical ﬂow of the project stages. The dashed arrows indicate that at some stages, a decision can be made to go back in the process and either collect more data or collect diﬀerent data, and revise features (by decommissioning some of them and engineering new ones).\n\nEvery stage mentioned above will be considered in one of the book’s chapters. But ﬁrst, let’s discuss how to prioritize machine learning projects, deﬁne the project’s goal, and structure a machine learning team. The next chapter is devoted to these three questions.\n\n1.8 Summary\n\nA model-based machine learning algorithm takes a collection of training examples as input and outputs a model. An instance-based machine learning algorithm uses the entire training\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19\n\ndataset as a model. The training data is exposed to the machine learning algorithm, while holdout data isn’t.\n\nA supervised learning algorithm builds a model that takes a feature vector and outputs a prediction about that feature vector. An unsupervised learning algorithm builds a model that takes a feature vector as input and transforms it into something useful.\n\nClassiﬁcation is the problem of predicting, for an input example, one of a ﬁnite set of classes. Regression, in turn, is a problem of predicting a numerical target.\n\nData can be used directly or indirectly. Directly-used data is a basis for forming a dataset of examples. Indirectly-used data is used to enrich those examples.\n\nThe data for machine learning must be tidy. A tidy dataset can be seen as a spreadsheet where each row is an example, and each column is one of the properties of an example. In addition to being tidy, most machine learning algorithms require numerical data, as opposed to categorical. Feature engineering is the process of transforming data into a form that machine learning algorithms can use.\n\nA baseline is essential to make sure that the model works better than a simple heuristic.\n\nIn practice, machine learning is implemented as a pipeline that contains chained stages of data transformation, from data partitioning to missing-data imputation, to class imbalance and dimensionality reduction, to model training. The hyperparameters of the entire pipeline are usually optimized; the entire pipeline can be deployed and used for predictions.\n\nParameters of the model are optimized by the learning algorithm based on the training data. The values of hyperparameters cannot be learned by the learning algorithm and are, in turn, tuned by using the validation dataset. The test set is only used to assess the model’s performance and report it to the client or product owner.\n\nA shallow learning algorithm trains a model that makes predictions directly from the input features. A deep learning algorithm trains a layered model, in which each layer generates outputs by taking the outputs of the preceding layer as inputs.\n\nYou should consider using machine learning to solve a business problem when the problem is too complex for coding, the problem is constantly changing, it is a perceptive problem, it is an unstudied phenomenon, the problem has a simple objective, and it is cost-eﬀective.\n\nThere are many situations when machine learning should, probably, not be used: when explainability is needed, when errors are intolerable, when traditional software engineering is a less expensive option, when all inputs and outputs can be enumerated and saved in a database, and when data is hard to get or too expensive.\n\nMachine learning engineering (MLE) is the use of scientiﬁc principles, tools, and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE encompasses all stages from data collection, to model training, to making the model available for use by the product or the consumers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\nA machine learning project life cycle consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\n\nEvery stage will be considered in one of the book’s chapters.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "page_number": 20,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 20-27). Key topics include data, model, and machine. For example, if your problem is classiﬁcation, you can pick a baseline classiﬁer and measure its performance.",
      "keywords": [
        "machine learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning algorithm",
        "machine learning project",
        "learning",
        "machine",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "learning algorithm",
        "Machine Learning Pipeline",
        "Burkov Machine",
        "model",
        "machine learning models",
        "machine learning engineer"
      ],
      "concepts": [
        "data",
        "model",
        "machine",
        "learning",
        "engineering",
        "engineer",
        "algorithm",
        "problems",
        "feature",
        "training"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 281-289)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.69,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 28-36)",
      "start_page": 28,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "2 Before the Project Starts\n\nBefore a machine learning project starts, it must be prioritized. Prioritization is inevitable: the team and equipment capacity is limited, while the organization’s backlog of projects could be very long.\n\nTo prioritize a project, one has to estimate its complexity. With machine learning, accurate complexity estimation is rarely possible because of major unknowns, such as whether the required model quality is attainable in practice, how much data is needed, and what, and how many features are necessary.\n\nFurthermore, a machine learning project must have a well-deﬁned goal. Based on the goal of the project, the team could be adequately adjusted and resources provisioned.\n\nIn this chapter, we consider these and related activities that must be taken care of before a machine learning project starts.\n\n2.1 Prioritization of Machine Learning Projects\n\nThe key considerations in the prioritization of a machine learning project, are impact and cost.\n\n2.1.1\n\nImpact of Machine Learning\n\nThe impact of using machine learning in a broader engineering project is high when, 1) machine learning can replace a complex part in your engineering project or 2) there’s a great beneﬁt in getting inexpensive (but probably imperfect) predictions.\n\nFor example, a complex part of an existing system can be rule-based, with many nested rules and exceptions. Building and maintaining such a system can be extremely diﬃcult, time-consuming, and error-prone. It can also be a source of signiﬁcant frustration for software engineers when they are asked to maintain that part of the system. Can the rules be learned instead of programming them? Can an existing system be used to generate labeled data easily? If yes, such a machine learning project would have a high impact and low cost.\n\nInexpensive and imperfect predictions can be valuable, for example, in a system that dispatches a large number of requests. Let’s say many such requests are “easy” and can be solved quickly using some existing automation. The remaining requests are considered “diﬃcult” and must be addressed manually.\n\nA machine learning-based system that recognizes “easy” tasks and dispatches them to the automation will save a lot of time for humans who will only concentrate their eﬀort and time on diﬃcult requests. Even if the dispatcher makes an error in prediction, the diﬃcult request will reach the automation, the automation will fail on it, and the human will eventually receive that request. If the human gets an easy request by mistake, there’s no problem either: that easy request can still be sent to the automation or processed by the human.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\n2.1.2 Cost of Machine Learning\n\nThree factors highly inﬂuence the cost of a machine learning project:\n\nthe diﬃculty of the problem, • the cost of data, and • the need for accuracy.\n\nGetting the right data in the right amount can be very costly, especially if manual labeling is involved. The need for high accuracy can translate into the requirement of getting more data or training a more complex model, such as a unique architecture of a deep neural network or a nontrivial ensembling architecture.\n\nWhen you think about the problem’s diﬃculty, the primary considerations are:\n\nwhether an implemented algorithm or a software library capable of solving the problem is available (if yes, the problem is greatly simpliﬁed),\n\nwhether signiﬁcant computation power is needed to build the model or to run it in the production environment.\n\nThe second driver of the cost is data. The following considerations have to be made:\n\ncan data be generated automatically (if yes, the problem is greatly simpliﬁed), • what is the cost of manual annotation of the data (i.e., assigning labels to unlabeled examples),\n\nhow many examples are needed (usually, that cannot be known in advance, but can be estimated from known published results or the organization’s own experience).\n\nFinally, one of the most inﬂuential cost factors is the desired accuracy of the model. The machine learning project’s cost grows superlinearly with the accuracy requirement, as illus- trated in Figure 1. Low accuracy can also be a source of signiﬁcant loss when the model is deployed in the production environment. The considerations to make:\n\nhow costly is each wrong prediction, and • what is the lowest accuracy level below which the model becomes impractical.\n\n2.2 Estimating Complexity of a Machine Learning Project\n\nThere is no standard complexity estimation method for a machine learning project, other than by comparison with other projects executed by the organization or reported in the literature.\n\n2.2.1 The Unknowns\n\nThere are several major unknowns that are almost impossible to guess with conﬁdence unless you worked on a similar project in the past or read about such a project. The unknowns are:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nAccuracyCost\n\n80%90%99%99.9%\n\nFigure 1: Superlinear growth of the cost as a function of accuracy requirement.\n\nwhether the required quality is attainable in practice, • how much data you will need to reach the required quality, • what features and how many features are necessary so that the model can learn and generalize suﬃciently,\n\nhow large the model should be (especially relevant for neural networks and ensemble architectures), and\n\nhow long will it take to train one model (in other words, how much time is needed to run one experiment) and how many experiments will be required to reach the desired level of performance.\n\nOne thing you can almost be sure of: if the required level of model accuracy (one of the popular model quality metrics we consider in Section ?? of Chapter 5) is above 99%, you can expect complications related to an insuﬃcient quantity of labeled data. In some problems, even 95% accuracy is considered very hard to reach. (Here we assume, of course, that the data is balanced, that is, there’s no class imbalance. We will discuss class imbalance in Section ?? of the next chapter.)\n\nAnother useful reference is the human performance on the task. This is typically a hard problem if you want your model to perform as well as a human.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\n2.2.2 Simplifying the Problem\n\nOne way to make a more educated guess is to simplify the problem and solve a simpler problem ﬁrst. For example, assume that the problem is that of classifying a set of documents into 1000 topics. Run a pilot project by focusing on 10 topics ﬁrst, by considering documents belonging to other 990 topics as “Other.”1 Manually label the data for these 11 classes (10 real topics, plus “Other”). The logic here is that it’s much simpler for a human to keep in mind the deﬁnitions of only 10 topics compared to memorizing the diﬀerence between 1000 topics2.\n\nOnce you have simpliﬁed your problem to 11 classes, solve it, and measure time on every stage. Once you see that the problem for 11 classes is solvable, you can reasonably hope that it will be solvable for 1000 classes as well. Your saved measurements can then be used to estimate the time required to solve the full problem, though you cannot simply multiply this time by 100 to get an accurate estimate. The quantity of data needed to learn to distinguish between more classes usually grows superlinearly with the number of classes.\n\nAn alternative way of obtaining a simpler problem from a potentially complex one is to split the problem into several simple ones by using the natural slices in the available data. For example, let an organization have customers in multiple locations. If we want to train a model that predicts something about the customers, we can try to solve that problem only for one location, or for customers in a speciﬁc age range.\n\n2.2.3 Nonlinear Progress\n\nThe progress of a machine learning project is nonlinear. The prediction error usually decreases fast in the beginning, but then the progress gradually slows down.3 Sometimes you see no progress and decide to add additional features that could potentially depend on external databases or knowledge bases. While you are working on a new feature or labeling more data (or outsourcing this task), no progress in model performance is happening.\n\nBecause of this nonlinearity of progress, you should make sure that the product owner (or the client) understands the constraints and risks. Carefully log every activity and track the time it took. This will help not only in reporting, but also in the estimation of the complexity of similar projects in the future.\n\n1Putting examples belonging to 990 classes in one class will likely create a highly imbalanced dataset. If it’s the case, you would prefer to undersample the data in the class “Other.” We consider data undersampling in Section ?? of the next chapter.\n\n2To save even more time, apply clustering to the whole collection of unlabeled documents and only\n\nmanually label documents belonging to one or a few clusters.\n\n3The 80/20 rule of thumb often applies: 80% of progress is made using the ﬁrst 20% of resources.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\n2.3 Deﬁning the Goal of a Machine Learning Project\n\nThe goal of a machine learning project is to build a model that solves, or helps solve, a business problem. Within a project, the model is often seen as a black box described by the structure of its input (or inputs) and output (or outputs), and the minimum acceptable level of performance (as measured by accuracy of prediction or another performance metric).\n\n2.3.1 What a Model Can Do\n\nThe model is typically used as a part of a system that serves some purpose. In particular, the model can be used within a broader system to:\n\nautomate (for example, by taking action on the user’s behalf or by starting or stopping a speciﬁc activity on a server),\n\nalert or prompt (for example, by asking the user if an action should be taken or by asking a system administrator if the traﬃc seems suspicious),\n\norganize, by presenting a set of items in an order that might be useful for a user (for example, by sorting pictures or documents in the order of similarity to a query or according to the user’s preferences),\n\nannotate (for instance, by adding contextual annotations to displayed information, or by highlighting, in a text, phrases relevant to the user’s task),\n\nextract (for example, by detecting smaller pieces of relevant information in a larger input, such as named entities in the text: proper names, companies, or locations), • recommend (for example, by detecting and showing to a user highly relevant items in a large collection based on item’s content or user’s reaction to the past recommendations), • classify (for example, by dispatching input examples into one, or several, of a predeﬁned set of distinctly-named groups),\n\nquantify (for example, by assigning a number, such as a price, to an object, such as a house),\n\nsynthesize (for example, by generating new text, image, sound, or another object similar to the objects in a collection),\n\nanswer an explicit question (for example, “Does this text describe that image?” or “Are these two images similar?”),\n\ntransform its input (for example, by reducing its dimensionality for visualization purposes, paraphrasing a long text as a short abstract, translating a sentence into another language, or augmenting an image by applying a ﬁlter to it),\n\ndetect a novelty or an anomaly.\n\nAlmost any business problem solvable with machine learning can be deﬁned in a form similar to one from the above list. If you cannot deﬁne your business problem in such a form, likely, machine learning is not the best solution in your case.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\n2.3.2 Properties of a Successful Model\n\nA successful model has the following four properties:\n\nit respects the input and output speciﬁcations and the performance requirement, • it beneﬁts the organization (measured via cost reduction, increased sales or proﬁt), • it helps the user (measured via productivity, engagement, and sentiment), • it is scientiﬁcally rigorous.\n\nA scientiﬁcally rigorous model is characterized by a predictable behavior (for the input examples that are similar to the examples that were used for training) and is reproducible. The former property (predictability) means that if input feature vectors come from the same distribution of values as the training data, then the model, on average, has to make the same percentage of errors as observed on the holdout data when the model was trained. The latter property (reproducibility) means that a model with similar properties can be easily built once again from the same training data using the same algorithm and values of hyperparameters. The word “easily” means that no additional analysis, labeling, or coding is necessary to rebuild the model, only the compute power.\n\nWhen deﬁning the goal of machine learning, make sure you solve the right problem. To give an example of an incorrectly deﬁned goal, imagine your client has a cat and a dog and needs a system that lets their cat in the house but keeps their dog out. You might decide to train the model to distinguish cats from dogs. However, this model will also let any cat in and not just their cat. Alternatively, you may decide that because the client only has two animals, you will train a model that distinguishes between those two. In this case, because your classiﬁcation model is binary, a raccoon will be classiﬁed as either the dog or the cat. If it’s classiﬁed as the cat, it will be let in the house.4\n\nDeﬁning a single goal for a machine learning project could be challenging. Usually, within an organization, there will be multiple stakeholders having interest towards your project. An obvious stakeholder is the product owner. Let their objective be to increase the time the user spends on an online platform by at least 15%. At the same time, the executive VP would like to increase the revenue from advertisements by 20%. Furthermore, the ﬁnance team would like to reduce the monthly cloud bill by 10%. When deﬁning the goal of your machine learning project, you should ﬁnd the right balance between those possibly conﬂicting requirements and translate them into the choice of the model’s input and output, cost function, and performance metric.\n\n2.4 Structuring a Machine Learning Team\n\nThere are two cultures of structuring a machine learning team, depending on the organization.\n\n4This is why having the class “Other” your classiﬁcation problems is almost always a good idea.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\n2.4.1 Two Cultures\n\nOne culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers. In such a culture, a software engineer doesn’t need to have deep expertise in machine learning, but has to understand the vocabulary of their fellow data analysts.\n\nAccording to other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\n\nThere are pros and cons in each culture. The proponents of the former say that each team member must be the best in what they do. A data analyst must be an expert in many machine learning techniques and have a deep understanding of the theory to come up with an eﬀective solution to most problems, fast and with minimal eﬀort. Similarly, a software engineer must have a deep understanding of various computing frameworks and be capable of writing eﬃcient and maintainable code.\n\nThe proponents of the latter say that scientists are hard to integrate with software engi- neering teams. Scientists care more about how accurate their solution is and often come up with solutions that are impractical and cannot be eﬀectively executed in the production environment. Also, because scientists don’t usually write eﬃcient, well-structured code, the latter has to be rewritten into production code by a software engineer; depending on the project, that can turn out to be a daunting task.\n\n2.4.2 Members of a Machine Learning Team\n\nBesides machine learning and software engineering skills, a machine learning team may include experts in data engineering (also known as data engineers) and experts in data labeling.\n\nData engineers are software engineers responsible for ETL (for Extract, Transform, Load). These three conceptual steps are part of a typical data pipeline. Data engineers use ETL techniques and create an automated pipeline, in which raw data is transformed into analysis- ready data. Data engineers design how to structure the data and how to integrate it from various resources. They write on-demand queries on that data, or wrap the most frequent queries into fast application programming interfaces (APIs) to make sure that the data is easily accessible by analysts and other data consumers. Typically, data engineers are not expected to know any machine learning.\n\nIn most big companies, data engineers work separately from machine learning engineers in a data engineering team.\n\nExperts in data labeling are responsible for four activities:\n\nmanually or semi-automatically assign labels to unlabeled examples according to the speciﬁcation provided by data analysts,\n\nbuild labeling tools,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nmanage outsourced labelers, and • validate labeled examples for quality.\n\nA labeler is person responsible for assigning labels to unlabeled examples. Again, in big companies, data labeling experts may be organized in two or three diﬀerent teams: one or two teams of labelers (for example, one local and one outsourced) and a team of software engineers, plus a user experience (UX) specialist, responsible for building labeling tools.\n\nWhen possible, invite domain experts to work closely with scientists and engineers. Employ domain experts in your decision making about the inputs, outputs, and features of your model. Ask them what they think your model should predict. Just the fact that the data you can get access to can allow you to predict some quantity doesn’t mean the model will be useful for the business.\n\nDiscuss with the domain experts what they look for in the data to make a speciﬁc business decision; that will help you with feature engineering. Discuss also what clients pay for and what is a deal-breaker for them; that will help you to translate a business problem into a machine learning problem.\n\nFinally, there are DevOps engineers. They work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model mainte- nance. In smaller companies and startups, a DevOps engineer may be part of the machine learning team, or a machine learning engineer could be responsible for the DevOps activities. In big companies, DevOps engineers employed in machine learning projects usually work in a larger DevOps team. Some companies introduced the MLOps role, whose responsibility is to deploy machine learning models in production, upgrade those models, and build data processing pipelines involving machine learning models.\n\n2.5 Why Machine Learning Projects Fail\n\nAccording to various estimates made between 2017 and 2020, from 74% to 87% of machine learning and advanced analytics projects fail or don’t reach production. The reasons for a failure range from organizational to engineering. In this section, we consider the most impactful of them.\n\n2.5.1 Lack of Experienced Talent\n\nAs of 2020, both data science and machine learning engineering are relatively new disciplines. There’s still no standard way to teach them. Most organizations don’t know how to hire experts in machine learning and how to compare them. Most of the available talent on the market are people who completed one or several online courses and who don’t possess signiﬁcant practical experience. A signiﬁcant fraction of the workforce has superﬁcial expertise in machine learning obtained on toy datasets in a classroom context. Many don’t have experience with the entire machine learning project life cycle. On the other hand, experienced software engineers that\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\nmight exist in an organization don’t have expertise in handling data and machine learning models appropriately.\n\n2.5.2 Lack of Support by the Leadership\n\nAs discussed in the previous section on the two cultures, scientists and software engineers often have diﬀerent goals, motivations, and success criteria. They also work very diﬀerently. In a typical Agile organization, software engineering teams work in sprints with clearly deﬁned expected deliverables and little uncertainty.\n\nScientists, on the other hand, work in high uncertainty and move ahead with multiple experiments. Most of such experiments don’t result in any deliverable and, thus, can be seen by inexperienced leaders as no progress. Sometimes, after the model is built and deployed, the entire process has to start over because the model doesn’t result in the expected increase of the metric the business cares about. Again, this can lead to the perception of the scientist’s work by the leadership as wasted time and resources.\n\nFurthermore, in many organizations, leaders responsible for data science and artiﬁcial intelligence (AI), especially at the vice-president level, have a non-scientiﬁc or even non- engineering background. They don’t know how AI works, or have a very superﬁcial or overly optimistic understanding of it drawn from popular sources. They might have such a mindset that with enough resources, technical and human, AI can solve any problem in a short amount of time. When fast progress doesn’t happen, they easily blame scientists or entirely lose interest in AI as an ineﬀective tool with hard-to-predict and uncertain results.\n\nOften, the problem lies in the inability of scientists to communicate the results and challenges to upper management. Because they don’t share the vocabulary and have very diﬀerent levels of technical expertise, even a success presented badly can be seen as a failure.\n\nThis is why, in successful organizations, data scientists are good popularizers, while top-level managers, responsible for AI and analytics, often have a technical or scientiﬁc background.\n\n2.5.3 Missing Data Infrastructure\n\nData analysts and scientists work with data. The quality of the data is crucial for a machine learning project’s success. Enterprise data infrastructure must provide the analyst with simple ways to get quality data for training models. At the same time, the infrastructure must make sure that similar quality data will be available once the model is deployed in production.\n\nHowever, in practice, this is often not the case. Scientists obtain the data for training by using various ad-hoc scripts; they also use diﬀerent scripts and tools to combine various data sources. Once the model is ready, it turns out that it’s impossible, by using the available production infrastructure, to generate input examples for the model fast enough (or at all). We extensively talk about storing data and features in Chapters 3 and 4.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "page_number": 28,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 28-36). Key topics include data, model, and learned. The training data is exposed to the machine learning algorithm, while holdout data isn’t.",
      "keywords": [
        "machine learning project",
        "machine learning",
        "Machine learning engineering",
        "Burkov Machine Learning",
        "learning project",
        "learning",
        "Andriy Burkov Machine",
        "machine",
        "machine learning algorithms",
        "learning engineering",
        "Burkov Machine",
        "model",
        "learning algorithm",
        "data",
        "problem"
      ],
      "concepts": [
        "data",
        "model",
        "learned",
        "cost",
        "problem",
        "engineering",
        "engineers",
        "feature",
        "examples",
        "classes"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "Segment 21 (pages 178-185)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.69,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-45)",
      "start_page": 37,
      "end_page": 45,
      "detection_method": "topic_boundary",
      "content": "2.5.4 Data Labeling Challenge\n\nIn most machine learning projects, analysts use labeled data. This data is usually custom, so labeling is executed speciﬁcally for each project. As of 2019, according to some reports,5 as many as 76% AI and data science teams label training data on their own, while 63% build their own labeling and annotation automation technology.\n\nThis results in a signiﬁcant time spent by skilled data scientists on data labeling and labeling tool development. This is a major challenge for the eﬀective execution of an AI project.\n\nSome companies outsource data labeling to third-party vendors. However, without proper quality validation, such labeled data can turn out to be of low quality or entirely wrong. Organizations, in order to maintain quality and consistency across datasets, have to invest in formal and standardized training of internal or third-party labelers. This, in turn, can slow down machine learning projects. Though, according to the same reports, companies that outsource data labeling are more likely to get their machine learning projects up to production.\n\n2.5.5 Siloed Organizations and Lack of Collaboration\n\nData needed for a machine learning project often resides within an organization in diﬀerent places with diﬀerent ownership, security constraints, and in diﬀerent formats. In siloed organizations, people responsible for diﬀerent data assets might not know one another. Lack of trust and collaboration results in friction when one department needs access to the data stored in a diﬀerent department. Furthermore, diﬀerent branches of one organization often have their own budgets, so collaboration becomes complicated because no side has an interest in spending their budget helping to the other side.\n\nEven within one branch of an organization, there are often several teams involved in a machine learning project at diﬀerent stages. For example, the data engineering team provides access to the data or individual features, the data science team works on modeling, ETL or DevOps work on the engineering aspects of deployment and monitoring, while the automation and internal tools teams develop tools and processes for a continuous model update. Lack of collaboration between any pair of the involved teams might result in the project being frozen for a long time. Typical reasons for mistrust between teams is the lack of understanding by the engineers of the tools and approaches used by the scientists and the lack of knowledge (or plain ignorance) by the scientists of software engineering good practices and design patterns.\n\n2.5.6 Technically Infeasible Projects\n\nBecause of the high cost of many machine learning projects (due to high expertise and infrastructure cost) some organizations, to “recoup the investment,” might target very ambitious goals: to completely transform the organization or the product or provide unrealistic\n\n5Alegion and Dimensional Research, “What data scientists tell us about AI model training today,” 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\nreturn or investment. This results in very large-scale projects, involving collaboration between multiple teams, departments, and third-parties, and pushing those teams to their limits.\n\nAs a result, such overly ambitious projects could take months or even years to complete; some key players, including leaders and key scientists, might lose interest in the project or even leave the organization. The project could eventually be deprioritized, or, even when completed, be too late to the market. It is best, at least in the beginning, to focus on achievable projects, involving simple collaboration between teams, easy to scope, and targeting a simple business objective.\n\n2.5.7 Lack of Alignment Between Technical and Business Teams\n\nMany machine learning projects start without a clear understanding, by the technical team, of the business objective. Scientists usually frame the problem as classiﬁcation or regression with a technical objective, such as high accuracy or low mean squared error. Without continuous feedback from the business team on the achievement of a business objective (such as an increased click-through rate or user retention), the scientists often reach an initial level of model performance (according to the technical objective), and then they are not sure if they are making any useful progress and whether an additional eﬀort is worth it. In such situations, the projects end up being put on the shelf because time and resources were spent but the business team didn’t accept the result.\n\n2.6 Summary\n\nBefore a machine learning project starts, it must be prioritized, and the team working on the project must be built. The key considerations in the prioritization of a machine learning project, are impact and cost.\n\nThe impact of using machine learning is high when, 1) machine learning can replace a complex part in your engineering project, or 2) there’s a great beneﬁt in getting inexpensive (but probably imperfect) predictions.\n\nThe cost of the machine learning project is highly inﬂuenced by three factors: 1) the diﬃculty of the problem, 2) the cost of data, and 3) the needed model performance quality.\n\nThere is no standard method of estimation of how complex a machine learning project is other than by comparison with other projects executed by the organization or reported in the literature. There are several major unknowns that are almost impossible to guess: whether the required level of model performance is attainable in practice, how much data you will need to reach that level of performance, what features and how many features are needed, how large the model should be, and how long will it take to run one experiment and how many experiments will be needed to reach the desired level of performance.\n\nOne way to make a more educated guess is to simplify the problem and solve a simpler one.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\nThe progress of a machine learning project is nonlinear. The error usually decreases fast in the beginning, but then the progress slows down. Because of this nonlinearity of progress, it’s better to make sure that the client understands the constraints and the risks. Carefully log every activity and track the time it took. This will help not only in reporting but also in estimating complexity for similar projects in the future.\n\nThe goal of a machine learning project is to build a model that solves some business problem. In particular, the model can be used within a broader system to automate, alert or prompt, organize, annotate, extract, recommend, classify, quantify, synthesize, answer an explicit question, transform its input, and detect novelty or an anomaly. If you cannot frame the goal of machine learning in one of these forms, likely, machine learning is not the best solution.\n\nA successful model 1) respects the input and output speciﬁcations and the minimum perfor- mance requirement, 2) beneﬁts the organization and the user, and 3) is scientiﬁcally rigorous.\n\nThere are two cultures of structuring a machine learning team, depending on the organization. One culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers. In such a culture, a software engineer doesn’t need to have profound expertise in machine learning but has to understand the vocabulary of their fellow data analysts or scientists. According to the other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\n\nBesides having machine learning and software engineering skills, a machine learning team may include experts in data labeling and data engineering experts. DevOps engineers work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model maintenance.\n\nA machine learning projects can fail for many reasons, and most actually do. Typical reasons for a failure are:\n\nlack of experienced talent, • lack of support by the leadership, • missing data infrastructure, • data labeling challenge, • siloed organizations and lack of collaboration, • technically infeasible projects, and • lack of alignment between technical and business teams.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n3 Data Collection and Preparation\n\nBefore any machine learning activity can start, the analyst must collect and prepare the data. The data available to the analyst is not always “right” and is not always in a form that a machine learning algorithm can use. This chapter focuses on the second stage in the machine learning project life cycle, as shown below:\n\nFigure 1: Machine learning project life cycle.\n\nIn particular, we talk about the properties of good quality data, typical problems a dataset can have, and ways to prepare and store data for machine learning.\n\n3.1 Questions About the Data\n\nNow that you have a machine learning goal with well-deﬁned model input, output, and success criteria, you can start collecting the data needed to train your model. However, before you start collecting the data, there are some questions to answer.\n\n3.1.1\n\nIs the Data Accessible?\n\nDoes the data you need already exist? If yes, is it accessible (physically, contractually, ethically, or from a cost perspective)? If you are purchasing or re-using someone else’s data sources, have you considered how that data might be used or shared? Do you need to negotiate a new licensing agreement with the original supplier?\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nIf the data is accessible, is it protected by copyright or other legal norms? If so, have you established who owns the copyright in your data? Might there be joint copyright?\n\nIs the data sensitive (e.g., concerning your organization’s projects, clients, or partners, or it is classiﬁed by the government), and are there any potential privacy issues? If so, have you discussed data sharing with the respondents from whom you collected the data? Can you preserve personal information for a long-term so that it can be used in the future?\n\nDo you need to share the data along with the model? If so, do you need to get written consent from owners or respondents?\n\nDo you need to anonymize data,1 for example, to remove personally identiﬁable infor- mation (PII), during analysis or in preparation for sharing?\n\nEven if it’s physically possible to get the data you need, don’t work with it until all the above questions are resolved.\n\n3.1.2\n\nIs the Data Sizeable?\n\nThe question for which you would like to have a deﬁnitive answer is whether there’s enough data. However, as we already found out, it’s usually not known how much data is needed to reach your goal, especially if the minimum model quality requirement is stringent.\n\nIf you have doubts about the immediate availability of suﬃcient data, ﬁnd out how frequently new data gets generated. For some projects, you can start with what’s initially available and, while you are working on feature engineering, modeling, and solving other relevant technical problems, new data might gradually come in. It can come in naturally, as the result of some observable or measurable process, or progressively be provided by your data labeling experts or a third-party data provider.\n\nConsider the estimated time needed to accomplish the project. Will a suﬃciently large dataset2 be gathered during this time? Base your answer on the experience working on similar projects or results reported in the literature.\n\nOne practical way to ﬁnd out if you have collected suﬃcient data is to plot learning curves. More speciﬁcally, plot the training and validation scores of your learning algorithm for varying numbers of training examples, as shown in Figure 2.\n\nBy looking at the learning curves, you will see your model’s performance will plateau after you reach a certain number of training examples. Upon reaching that number of training\n\n1An illustrative example is the content redistribution policy of Twitter. The policy restricts the sharing of tweet information other than tweet IDs and user IDs. Twitter wants the analysts always to pull fresh data using Twitter API. One possible explanation of such a restriction is that some users might want to delete a particular tweet because they changed their mind or found it too controversial. If that tweet has already been pulled and is shared on a public domain, then it might make that user vulnerable.\n\n2Don’t forget, in your estimates, that you need not just training but also holdout data to validate the model performance on the examples it wasn’t trained on. That holdout data also has to be sizeable to provide reliable, in a statistical sense, model quality estimates.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nFigure 2: Learning curves for the Naïve Bayes learning algorithm applied to the standard \"digits\" dataset of scikit-learn.\n\nexamples, you will begin to experience diminishing returns from additional examples.\n\nIf you observe the performance of the learning algorithm plateaued, it might be a sign that collecting more data will not help in training a better model. I used the expression “might be” because two other explanations are possible:\n\nyou didn’t have enough informative features that your learning algorithm can leverage to build a more performant model, or\n\nyou used a learning algorithm incapable of training a complex enough model using the data you have.\n\nIn the former case, you might think about engineering additional features by combining the existing features in some clever ways, or by using information from indirect data sources, such as lookup tables and gazetteers. We consider techniques for synthesizing features in Section ?? of Chapter 4.\n\nIn the latter case, one possible approach would be to use an ensemble learning method or train a deep neural network. However, deep neural networks usually require more training data compared to shallow learning algorithms.\n\nSome practitioners use rules of thumb to estimate the number of training examples needed for a problem. Usually, they are using scaling factors applied to either,\n\nnumber of features, or • number of classes, or\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nnumber of trainable parameters in the model.\n\nSuch rules of thumb often work, but they are diﬀerent for diﬀerent problem domains. Each analyst adjusts the numbers based on experience. While you would discover by experience those “magical” scaling factors that work for you, the most frequently cited numbers in various online sources are:\n\n10 times the amount of features (this often exaggerates the size of the training set, but works well as an upper bound),\n\n100 or 1000 times the number of classes (this often underestimates the size), or • ten times the number of trainable parameters (usually applied to neural networks).\n\nKeep in mind that just because you have big data does not mean that you should use all of it. A smaller sample of big data can give good results in practice and accelerate the search for a better model. It’s important to ensure, though, that the sample is representative of the whole big dataset. Sampling strategies such as stratiﬁed and systematic sampling can lead to better results. We consider data sampling strategies in Section 3.10.\n\n3.1.3\n\nIs the Data Useable?\n\nThe data quality is one of the major factors aﬀecting the model performance. Imagine that you want to train a model that predicts a person’s gender, given their name. You might acquire a dataset of people that contains gender information. However, if you use this dataset blindly, you might realize that no matter how hard you try to improve the quality of your model, its performance on new data is low. What is the reason for such a weak performance?\n\nThe answer could be that the gender information was not factual but obtained using a rather low-quality statistical classiﬁer. In this case, the best you can achieve with your model is the performance of that low-quality classiﬁer.\n\nIf the dataset comes in the form of a spreadsheet, the ﬁrst thing to check is if the data in the spreadsheet is tidy. As discussed in the introduction, the dataset used for machine learning must be tidy. If it’s not the case for your data, you must transform it into tidy data using, as already mentioned, feature engineering.\n\nA tidy dataset can have missing values. Consider data imputation techniques to ﬁll the missing values. We will discuss several such techniques in Section 3.7.\n\nOne frequent problem with datasets compiled by a human is that people can decide to indicate missing values with some magic number like 9999 or −1. Such situations must be spotted during the visual analysis of the data, and those magic numbers have to be replaced using an appropriate data imputation technique.\n\nAnother property to validate is whether the dataset contains duplicates. Usually, duplicates are removed, unless you added them on purpose to balance an imbalanced problem. We consider this problem and methods to alleviate it in Section 3.9.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\nData can be expired or be signiﬁcantly not up to date. For example, let your goal be to train a model that recognizes abnormality in the behavior of a complex piece of electronic appliance, such as a printer. You have measurements taken during the normal and abnormal functioning of a printer. However, these measurements have been recorded for a previous generation of printers, while the new generation has received several signiﬁcant upgrades since then. The model trained using such expired data from an older printer generation might perform worse when deployed on the new generation of printers.\n\nFinally, data can be incomplete or unrepresentative of the phenomenon. For example, a dataset of animal photos might contain pictures taken only during the summer or in a speciﬁc geography. A dataset of pedestrians for self-driving car systems might be created with engineers posing as pedestrians; in such a dataset, most situations would include only younger men, while children, women, and the elderly would be underrepresented or entirely absent.\n\nA company working on facial expression recognition might have the research and development oﬃce in a predominantly white location, so the dataset would only show faces of white men and women, while black or Asian people would be underrepresented. Engineers developing a posture recognition model for a camera might build the training dataset by taking pictures of people indoors, while the customers would typically use the camera outdoors.\n\nIn practice, data can only become useable for modeling after preprocessing; hence the importance of visual analysis of the dataset before you start modeling. Let’s say you work on a problem of predicting the topic in news articles. It’s likely you will scrape your data from news websites. It’s also likely that download dates would be saved in the same document as the news article text. Imagine also that the data engineer decides to loop over news topics mentioned on the websites and scrape one topic at a time. So, on Monday the arts-related articles were scraped, on Tuesday — sports, on Wednesday — technology, and so on.\n\nIf you don’t preprocess such data by removing the dates, the model can learn the date-topic correlation, and such a model will be of no practical use.\n\n3.1.4\n\nIs the Data Understandable?\n\nAs demonstrated in gender prediction, it’s crucial to understand from where each attribute in the dataset came. It is equally important to understand what each attribute exactly represents. One frequent problem observed in practice is when the variable that the analyst tries to predict is found among the features in the feature vector. How could that happen?\n\nImagine that you work on the problem of predicting the price of a house from its attributes such as the number of bedrooms, surface, location, year of construction, and so on. The attributes of each house were provided to you by the client, a large online real estate sales platform. The data has the form of an Excel spreadsheet. Without spending too much time analyzing each column, you remove only the transaction price from the attributes and use that value as the target you want to learn to predict. Very quickly you realize that the model is almost perfect: it predicts the transaction price with accuracy near 100%. You deliver the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "page_number": 37,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 37-45). Key topics include data, team, and model. The latter property (reproducibility) means that a model with similar properties can be easily built once again from the same training data using the same algorithm and values of hyperparameters.",
      "keywords": [
        "machine learning",
        "machine learning project",
        "Machine Learning Team",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "learning",
        "machine",
        "machine learning models",
        "machine learning engineers",
        "data",
        "learning project",
        "Learning Team",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "Burkov Machine"
      ],
      "concepts": [
        "data",
        "team",
        "model",
        "engineering",
        "engineers",
        "project",
        "scientists",
        "labeling",
        "organization",
        "organized"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 3,
          "title": "Segment 3 (pages 19-26)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 46-54)",
      "start_page": 46,
      "end_page": 54,
      "detection_method": "topic_boundary",
      "content": "model to the client, they deploy it in production, and the tests show that the model is wrong most of the time. What happened?\n\nWhat did happen is called data leakage (also known as target leakage). After a more careful examination of the dataset, you realize that one of the columns in the spreadsheet contained the real estate agent’s commission. Of course, the model easily learned to convert this attribute into the house price perfectly. However, this information is not available in the production environment before the house is sold, because the commission depends on the selling price. In Section 3.2.8, we will consider the problem of data leakage in more detail.\n\n3.1.5\n\nIs the Data Reliable?\n\nThe reliability of a dataset varies depending on the procedure used to gather that dataset. Can you trust the labels? If the data was produced by the workers on Mechanical Turk (so-called “turkers”), then the reliability of such data might be very low. In some cases, the labels assigned to feature vectors might be obtained as a majority vote (or an average) of several turkers. If that’s the case, the data can be considered more reliable. However, it’s better to do additional validation of quality on a small random sample of the dataset.\n\nOn the other hand, if the data represents measurements made by some measuring devices, you can ﬁnd the details of each measurement’s accuracy in the technical documentation of the corresponding measuring device.\n\nThe reliability of labels can also be aﬀected by the delayed or indirect nature of the label. The label is considered delayed when the feature vector to which the label was assigned represents something that happened signiﬁcantly earlier than the time of label observation.\n\nTo be more concrete, take the churn prediction problem. Here, we have a feature vector describing a customer, and we want to predict whether the customer will leave at some point in the future (typically six months to one year from now). The feature vector represents what we know about the customer now, but the label (“left” or “stayed”) will be assigned in the future. This is an important property, because between now and the future, many events, not reﬂected in our feature vector might happen which would aﬀect the customer’s decision to stay or leave. Therefore delayed labels make our data less reliable.\n\nWhether a label is direct or indirect also aﬀects reliability, depending, of course, on what we are trying to predict. For example, let’s say our goal is to predict whether the website visitor will be interested in a webpage. We might acquire a certain dataset containing information about users, webpages, and labels “interested”/“not_interested” reﬂecting whether a speciﬁc user was interested in a particular webpage. A direct label would indeed indicate interest, while an indirect label could suggest some interest. For example, if the user pressed the “Like” button, we have the direct indicator of interest. However, if the user only clicked on the link, this could be an indicator of some interest, but it’s an indirect indicator. The user could have clicked by mistake, or because the link text was a clickbait, we cannot know for sure. If\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nOriginal photo\n\nLabeled photo\n\nFigure 3: The unlabeled and labeled aerial photo. Photo credit: Tom Fisk.\n\nthe label is indirect, this also makes such data less reliable. Of course, it’s less reliable for predicting the interest, but can be perfectly reliable for predicting clicks.\n\nAnother source of unreliability in the data is feedback loops. A feedback loop is a property in the system design when the data used to train the model is obtained using the model itself. Again, imagine that you work on a problem of predicting whether a speciﬁc user of a website will like the content, and you only have indirect labels – clicks. If the model is already deployed on the website and the users click on links recommended by the model, this means that the new data indirectly reﬂects not only the interest of users to the content, but also how intensively the model recommended that content. If the model decided that a speciﬁc link is important enough to recommend to many users, more users would likely click on that link, especially if the recommendation was made repeatedly during several days or weeks.\n\n3.2 Common Problems With Data\n\nAs we have just seen, the data you will work with can have problems. In this section, we cite the most important of these problems and what you can do to alleviate them.\n\n3.2.1 High Cost\n\nGetting unlabeled data can be expensive; however, labeling data is the most expensive work, especially if the work is done manually.\n\nGetting unlabeled data becomes expensive when it nust be gathered speciﬁcally for your problem. Let’s say your goal is to know where diﬀerent types of commerce are located in a city. The best solution would be to buy this data from a government agency. However,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nfor various reasons it can be complicated or even impossible: the government database may be incomplete or outdated. To get up-to-date data, you may decide to send cars equipped with cameras on the streets of a given city. They would take pictures of all buildings on the streets.\n\nAs you might imagine, such an enterprise is not cheap. Collecting pictures of the buildings is not enough. We need the type of commerce in every building. Now we need labeled data: “coﬀee house,” “bank,” “grocery,” “drug store,” “gas station,” etc. These must be assigned manually, and paying someone to do that work is expensive. By the way, Google has a clever technique outsourcing the labeling to random people with its free reCAPTCHA service. reCAPTCHA thus solves two problems: reducing spam on the Web and providing cheap labeled data to Google.\n\nIn Figure 3, you can see the work needed to label one image. The goal here is to segment a picture by assigning labels to every pixel from the following: “heavy truck,” “car or light truck,” “boat,” “building,” “container,” “other.” Labeling image in Figure 3 took me about 30 minutes. If there were more types, for example “motorcycle,” “tree,” “road,” it would take longer, and the labeling cost would be higher.\n\nWell-designed labeling tools will minimize mouse use (including menus activated by mouse clicks), maximize hotkeys, and reduce costs by increasing the speed of data labeling.\n\nWhenever possible, reduce decision-making to a yes/no answer. Instead of asking “Find all prices in this text”, extract all numbers from the text and then display each number, one by one, asking, “Is this number a price?” as shown in 4. If the labeler clicks “Not Sure,” you can save this example to analyze later or simply not use such examples for training the model.\n\nAnother trick allowing for accelerated labeling is noisy pre-labeling consisting of pre- labeling the example using the current best model. In this scenario, you start by labeling a certain quantity of examples “from scratch” (that is, without using any support). Then you build the ﬁrst model that works reasonably well, using this initial set of labeled examples. Next, use the current model and label each new example in place of the human labeler.3 Ask whether the automatically assigned label is correct. If the labeler clicks “Yes,” save this example as usual. If they click “No,” then ask to label this example manually. See the workﬂow chart illustrating this process in Figure 5. The goal of a good labeling process design is to make the labeling as streamlined as possible. Keeping the labeler engaged is also key. Show progress in the number of labels added, as well as the quality of the current best model. This engages the labeler and adds purpose to the labeling task.\n\n3This is why it’s called a “noisy” pre-labeling: the labels assigned to examples using a sub-optimal model\n\nwould not all be accurate and require a human validation.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\nFigure 4: An example of simple labeling interface.\n\nFigure 5: An example of noisy pre-labeling workﬂow.\n\n3.2.2 Bad Quality\n\nRemember that data quality is one of the major factors aﬀecting the performance of the model. I cannot stress it strongly enough.\n\nData quality has two components: raw data quality and labeling quality.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\nSome common problems with raw data are noise, bias, low predictive power, outdated examples, outliers, and leakage.\n\n3.2.3 Noise\n\nNoise in data is a corruption of examples. Images can be blurry or incomplete. Text can lose formatting, which makes some words concatenated or split. Audio data can have noise in the background. Poll answers can be incomplete or have missing attributes, such as the responder’s age or gender. Noise is often a random process that corrupts each example independently of other examples in the collection.\n\nIf tidy data has missing attributes, data imputation techniques can help in guessing values for those attributes. We will consider data imputation techniques in Section 3.7.1.\n\nBlurred images can be deblurred using speciﬁc image deblurring algorithms, though deep machine learning models, such as neural networks, can learn to deblur if needed. The same can be said about noise in audio data: it can be algorithmically suppressed.\n\nNoise is more a problem when the dataset is relatively small (thousands of examples or less), because the presence of noise can lead to overﬁtting: the algorithm may learn to model the noise contained in the training data, which is undesirable. In the big data context, on the other hand, noise, if it’s randomly applied to each example independently of other examples in the dataset, is typically “averaged out” over multiple examples. In that latter context, noise can bring a regularization eﬀect as it prevents the learning algorithm from relying too much on a small subset of input features.4\n\n3.2.4 Bias\n\nBias in data is an inconsistency with the phenomenon that data represents. This inconsistency may occur for a number of reasons (which are not mutually exclusive).\n\nTypes of Bias\n\nSelection bias is the tendency to skew your choice of data sources to those that are easily available, convenient, and/or cost-eﬀective. For example, you might want to know the opinion of the readers on your new book. You decide to send several initial chapters to the mailing list of your previous book’s readers. It’s very likely this select group will like your new book. However, this information doesn’t tell you much about a general reader.\n\nA real-life example of selection bias is an image generated by the Photo Upsampling via Latent Space Exploration (PULSE) algorithm that uses a neural network model to upscale (increase the resolution) of images. When Internet users tested it, they discovered that an\n\n4This is, by the way, the rationale behind the increase in performance brought by the dropout regularization\n\ntechnique in deep learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\nupscaled image of a black person could, in some cases, represent a white person, as illustrated by Barack Obama’s upscaled photo shown in Figure 6.\n\nFigure 6: An illustration of the eﬀect the selection bias can have on the trained model. Image: Twitter / @Chicken3gg.\n\nThe above example shows that it cannot be assumed that machine learning model is correct, simply because machine learning algorithms are impartial and the trained models are based on data. If the data has a bias, it will most likely be reﬂected in the model.\n\nSelf-selection bias is a form of selection bias where you get the data from sources that “volunteered” to provide it. Most poll data has this type of bias. For example, you want to train a model that predicts the behavior of successful entrepreneurs. You decide to ﬁrst ask entrepreneurs whether they are successful or not. Then you only keep the data obtained from those who declared themselves successful. The problem here is that most likely, really successful entrepreneurs don’t have time to answer your questions, while those who claim themselves successful can be wrong on that matter.\n\nHere’s another example. Let’s say, you want to train a model that predicts whether a book will be liked by the readers. You can use the rating users gave to similar books in the past. However, it turns out that unhappy users tend to provide disproportionally low ratings. The data will be biased towards too many very low ratings as compared to the quantity of mid-range ratings, as shown in Figure 7. The bias is compounded by the fact that we tend to rate only when the experience was either very good or very bad.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\nFigure 7: The distribution of ratings given by the readers to a popular AI book on Amazon.\n\nOmitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction. For example, let’s assume that you are working on a churn prediction model and you want to predict whether a customer cancels their subscription within six months. You train a model, and it’s accurate enough; however, several weeks after deployment you see many unexpected false negatives. You investigate the decreased model performance and discover a new competitor now oﬀers a very similar service for a lower price. This feature wasn’t initially available to your model, therefore important information for accurate prediction was missing.\n\nSponsorship or funding bias aﬀects the data produced by a sponsored agency. For example, let a famous video game company sponsor a news agency to provide news about the video game industry. If you try to make a prediction about the video game industry, you might include in your data the story produced by this sponsored agency.\n\nHowever, sponsored news agencies tend to suppress bad news about their sponsor and exaggerate their achievements. As a result, the model’s performance will be suboptimal.\n\nSampling bias (also known as distribution shift) occurs when the distribution of examples used for training doesn’t reﬂect the distribution of the inputs the model will receive in production. This type of bias is frequently observed in practice. For example, you are working on a system that classiﬁes documents according to a taxonomy of several hundred topics. You might decide to create a collection of documents in which an equal amount of documents represents each topic. Once you ﬁnish the work on the model, you observe 5% error. Soon after deployment, you see the wrong assignment to about 30% of documents. Why did this happen?\n\nOne of the possible reasons is sampling bias: one or two frequent topics in production data might account for 80% of all input. If your model doesn’t perform well for these frequent topics, then your system will make more errors in production than you initially expected.\n\nPrejudice or stereotype bias is often observed in data obtained from historical sources,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\nsuch as books or photo archives, or from online activity such as social media, online forums, and comments to online publications.\n\nUsing a photo archive to train a model that distinguishes men from women might show, for example, men more frequently in work or outdoor contexts, and women more often at home indoors. If we use such biased data, our model will have more diﬃculty recognizing a woman outdoors or a man at home.\n\nA famous example of this type of bias is looking for associations for words using word embeddings trained with an algorithm like word2vec. The model predicts that king − man+woman ≈ queen, but at the same time, that programmer−man+woman ≈ homemaker.\n\nSystematic value distortion is bias usually occurring with the device making measurements or observations. This results in a machine learning model making suboptimal predictions when deployed in the production environment.\n\nFor example, the training data is gathered using a camera with a white balance which makes white look yellowish. In production, however, engineers decide to use a higher-quality camera which “sees” white as white. Because your model was trained on lower-quality pictures, the predictions using higher-quality input will be suboptimal.\n\nThis should not be confused with noisy data. Noise is the result of a random process that distorts the data. When you have a suﬃciently large dataset, noise becomes less of a problem because it might average out. On the other hand, if the measurements are consistently skewed in one direction, then it damages training data, and ultimately results in a poor-quality model.\n\nExperimenter bias is the tendency to search for, interpret, favor, or recall information in a way that aﬃrms one’s prior beliefs or hypotheses. Applied to machine learning, experimenter bias often occurs when each example in the dataset is obtained from the answers to a survey given by a particular person, one example per person.\n\nUsually, each survey contains multiple questions. The form of those questions can signiﬁcantly aﬀect the responses. The simplest way for a question to aﬀect the response is to provide limited response options: “Which kind of pizza do you like: pepperoni, all meats, or vegetarian?” This doesn’t leave the choice of giving a diﬀerent answer, or even “Other.”\n\nAlternatively, a survey question might be constructed with a built-in slant. Instead of asking, “Do you recycle?” an analyst with a experimenter bias might ask, “Do you dodge from recycling?” In the former case, the respondent is more likely to give an honest answer, compared to the latter.\n\nFurthermore, experimenter bias might happen when the analyst is briefed in advance to support a particular conclusion (for example, the one in favor of doing “business as usual”). In that situation, they can exclude speciﬁc variables from the analysis as unreliable or noisy.\n\nLabeling bias happens when labels are assigned to unlabeled examples by a biased process or person. For example, if you ask several labelers to assign a topic to a document by reading the document, some labelers can indeed read the document entirely and assign well-thought\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\nlabels. In contrast, others could just try to quickly “scan” the text, spot some keyphrases and choose the topic that corresponds the best to the selected keyphrases. Because each person’s brain pays more attention to keyphrases from a speciﬁc domain or domains and less to others, the labels assigned by labelers who scan the text without reading will be biased.\n\nAlternatively, some labelers would be more interested in reading documents on some topics that they personally prefer. If it’s the case, a labeler might skip uninteresting documents, and the latter will be underrepresented in your data.\n\nWays to Avoid Bias\n\nIt is usually impossible to know exactly what biases are present in a dataset. Furthermore, even knowing there are biases, avoiding them is a challenging task. First of all, be prepared.\n\nA good habit is to question everything: who created the data, what were their motivations and quality criteria, and more importantly, how and why the data was created. If the data is a result of some research, question the research method and make sure that it doesn’t contribute to any of the biases described above.\n\nSelection bias can be avoided by systematically questioning the reason why a speciﬁc data source was chosen. If the reason is simplicity or low cost, then pay careful attention. Recall the example whether a speciﬁc customer would subscribe to your new oﬀering. Training the model using only the data about your current customers is likely a bad idea, because your existing customers are more loyal to your brand than a random potential customer. Your estimates of model’s quality will be overly optimistic.\n\nSelf-selection bias cannot be completely eliminated. It usually appears in surveys; the mere consent of the responder to answer the questions represents self-selection bias. The longer the survey, the less likely the respondent will answer with a high degree of attention. Therefore, keep your survey short and provide an incentive to give quality answers.\n\nPre-select responders to reduce self-selection. Don’t ask entrepreneurs whether they consider themselves successful. Rather, build a list based on references from experts or publications, and only contact those individuals.\n\nIt’s tough to avoid the omitted variable bias completely, because, as they say, “we don’t know what we don’t know.” One approach is to use all available information, that is, to include in your feature vector as many features as possible, even those you deem unnecessary. This could make your feature vector very wide (i.e., of many dimensions) and sparse (i.e., when the values in most dimensions are zero). Still, if you use a well-tuned regularization, your model will “decide” which features are important, and which ones aren’t.\n\nAlternatively, let us suspect that a particular variable would be important for accurate predictions, and leaving it out of our model could result in an omitted variable bias. Suppose getting that data is problematic. Try using a proxy variable in lieu of the omitted variable. For instance, if we want to train a model that predicts the price of a used car, and we cannot get the car’s age, use, instead, the length of ownership by its current owner. The amount of time the current owner owned the car can be taken as a proxy for the age of the vehicle.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "page_number": 46,
      "chapter_number": 6,
      "summary": "This chapter focuses on the second stage in the machine learning project life cycle, as shown below:\n\nFigure 1: Machine learning project life cycle Key topics include data, labeling, and model.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Data",
        "machine learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "model",
        "Burkov Machine",
        "learning",
        "Data Collection",
        "Andriy Burkov",
        "machine",
        "machine learning project",
        "dataset",
        "data quality"
      ],
      "concepts": [
        "data",
        "labeling",
        "model",
        "learning",
        "dataset",
        "problems",
        "quality",
        "engineering",
        "engineers",
        "examples"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 433-440)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 55-62)",
      "start_page": 55,
      "end_page": 62,
      "detection_method": "topic_boundary",
      "content": "Sponsorship bias can be reduced by carefully investigating the data source, speciﬁcally the source owner’s incentive to provide the data. For example, it’s known that publications on tobacco and pharmaceutical drugs are very often sponsored by tobacco and pharmaceutical companies, or their opponents. The same can be said about news companies, especially those that depend on the advertisement revenue or have an undisclosed business model.\n\nSampling bias can be avoided by researching the real proportion of various properties in the data that will be observed in production, and then sampling the training data by keeping similar proportions.\n\nPrejudice or stereotype bias can be controlled. When developing the training model to distinguish pictures of women from men, a data analyst could choose to under-sample the number of women indoors, or oversample the number of men at home. In other words, prejudice or stereotype bias is reduced by exposing the learning algorithm to a more even- handed distribution of examples.\n\nSystematic value distortion bias can be alleviated by having multiple measuring devices, or hiring humans trained to compare the output of measuring or observing devices.\n\nExperimenter bias can be avoided by letting multiple people validate the questions asked in the survey. Ask yourself: “Do I feel uncomfortable or constrained answering this question?”\n\nFurthermore, despite more diﬃculties in analysis, opt for open-ended questions rather than yes/no or multiple-choice questions. If you still prefer to give responders a choice of answers, include the option “Other” and a place to write a diﬀerent answer.\n\nLabeling bias can be avoided by asking several labelers to identify the same example. Ask the labelers why they decided to assign a speciﬁc label to examples that produced diﬀerent results. If you see that some labelers refer to certain keyphrases, rather than trying to paraphrase the entire document, you can identify those who are quickly scanning instead of reading.\n\nYou can also compare the frequency of skipped documents for diﬀerent labelers. If you see that a labeler skips documents more often than the average, ask if they encountered technical problems, or simply were not interested in some topics.\n\nYou cannot entirely avoid bias in data. There’s no silver bullet. As a general rule, keep a human in the loop, especially if your model aﬀects people’s lives.\n\nRecall that there is a temptation among the data analysts to assume that machine learning models are inherently fair because they make decisions based on evidence and math, as opposed to often messy or irrational human judgments. This is, unfortunately, not always the case: inevitably, a model trained on biased data will produce biased results.\n\nIt is the duty of people training the model to ensure that the output is fair. But what’s fair, you may ask? Unfortunately, again, there is no silver bullet measurement that would always detect unfairness. Choosing an appropriate deﬁnition of model fairness is always problem-speciﬁc and requires human judgment. In Section ?? of Chapter 7, we consider several deﬁnitions of fairness in machine learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\nHuman involvement in all stages of data gathering and preparation is the best approach to make sure that the possible damage caused by machine learning is minimized.\n\n3.2.5 Low Predictive Power\n\nLow predictive power is an issue that you often don’t consider until you have spent fruitless energy trying to train a good model. Does the model underperform because it is not expressive enough? Does the data not contain enough information from which to learn? You don’t know.\n\nSuppose the goal is to predict whether a listener will like a new song on a music streaming service. Your data is the name of the artist, the song title, lyrics, and whether that song is in their playlist. The model you train with this data will be far from perfect.\n\nArtists who are not in the listener’s playlist are unlikely to receive a high score from the model. Furthermore, many users will only add some songs of a speciﬁc artist to their playlist. Their musical preferences are signiﬁcantly inﬂuenced by the song arrangement, choice of instruments, sound eﬀects, tone of voice, and subtle changes in tonality, rhythm, and beat. These are properties of songs that cannot be found in lyrics, title, or the artist’s name; they have to be extracted from the sound ﬁle.\n\nOn the other hand, extracting these relevant features from an audio ﬁle is challenging. Even with modern neural networks, recommending songs based on how they sound is considered a hard task for artiﬁcial intelligence. Typically, song recommendations are developed by comparing playlists of diﬀerent listeners and ﬁnding those with similar compositions.\n\nConsider a diﬀerent example of low predictive power. Let’s say we want to train a model that will predict where to point the telescope and observe something interesting. Our data are photos of various regions of the sky where something unusual was captured in the past. Based on these photos alone, it’s very unlikely that we will be able to train a model that accurately predicts such an event. However, if we add to this data the measurements of various sensors, such as those measuring radiofrequency signals from diﬀerent zones, or particle bursts, it is more likely we will be able to make better predictions.\n\nYour work may be especially challenging the ﬁrst time working with the dataset. If you cannot obtain acceptable results, no matter how complex the model becomes, it may be time to consider the problem of low predictive power. Engineer as many additional features as possible (apply your creativity!). Consider indirect data sources to enrich feature vectors.\n\n3.2.6 Outdated Examples\n\nOnce you build the model and deploy it in production, the model usually performs well for some time. This period depends entirely on the phenomenon you are modeling.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\nTypically, as we will discuss in Section ?? of Chapter 9, a certain model quality monitoring procedure is deployed in the production environment. Once an erratic behavior is detected, new training data is added to adjust the model; the model is then retrained and redeployed.\n\nOften, the cause of an error is explained by the ﬁniteness of the training set. In such cases, additional training examples will solidify the model. However, in many practical scenarios, the model starts to make errors because of concept drift. Concept drift is a fundamental change in the statistical relationship between the features and the label.\n\nImagine your model predicts whether a user will like certain content on a website. Over time, the preferences of some users may start to change, perhaps due to aging, or because a user discovers something new (I didn’t listen to jazz three years ago, now I do!). The examples added to the training data in the past no longer reﬂect some user’s preferences and start hurting the model performance, rather than contributing to it. This is concept drift. Consider it if you see a decreasing trend in model performance on new data.\n\nCorrect the model by removing the outdated examples from the training data. Sort your training examples, most recent ﬁrst. Deﬁne an additional hyperparameter — what percentage of the most recent examples to use to retrain the model — and tune it using grid search, or another hyperparameter tuning technique.\n\nConcept drift is an example of a broader problem known as distribution shift. We consider hyperparameter tuning and other types of distribution shift in Sections ?? and ??.\n\n3.2.7 Outliers\n\nOutliers are examples that look dissimilar to the majority of examples from the dataset. It’s up to the data analyst to deﬁne “dissimilar.” Typically, dissimilarity is measured by some distance metric, such as Euclidean distance.\n\nIn practice, however, what seems to be an outlier in the original feature vector space can be a typical example in a feature vector space transformed using tools such as a kernel function. Feature space transformation is often explicitly done by a kernel-based model, such as support vector machine (SVM), or implicitly by a deep neural network.\n\nShallow algorithms, such as linear or logistic regression, and some ensemble methods, such as AdaBoost, are particularly sensitive to outliers. SVM has one deﬁnition that is less sensitive to outliers: a special penalty hyperparameter regulates the inﬂuence of misclassiﬁed examples (which often happen to be outliers) on the decision boundary. If this penalty value is low, the SVM algorithm may completely ignore outliers from consideration when drawing the decision boundary (an imaginary hyperplane separating positive and negative examples). If it’s too low, even some regular examples can end up on the wrong side of the decision boundary. The best value for that hyperparameter should be found by the analyst using a hyperparameter tuning technique.\n\nA suﬃciently complex neural network can learn to behave diﬀerently for each outlier in the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19\n\ndataset and, at the same time, still work well for the regular examples. It’s not the desired outcome, as the model becomes unnecessarily complex for the task. More complexity results in longer training and prediction time, and poorer generalization after production deployment.\n\nWhether to exclude outliers from the training data, or to use machine learning algorithms and models robust to outliers, is debatable. Deleting examples from a dataset is not considered scientiﬁcally or methodologically sound, especially in small datasets. In the big data context, on the other hand, outliers don’t typically have a signiﬁcant inﬂuence on the model.\n\nFrom a practical standpoint, if excluding some training examples results in better performance of the model on the holdout data, the exclusion may be justiﬁed. Which examples to consider for exclusion can be decided based on a certain similarity measure. A modern approach to getting such a measure is to build an autoencoder and use the reconstruction error5 as the measure of (dis)similarity: the higher the reconstruction error for a given example, the more dissimilar it is to the dataset.\n\n3.2.8 Data Leakage\n\nData leakage, also called target leakage, is a problem aﬀecting several stages of the machine learning life cycle, from data collection to model evaluation. In this section, I will only describe how this problem manifests itself at the data collection and preparation stages. In the subsequent chapters, I will describe its other forms.\n\nInformation available at theprediction time\n\nInformation unavailable at theprediction timeTimePrediction time\n\nContamination\n\nFigure 8: Data leakage in a nutshell.\n\nData leakage in supervised learning is the unintentional introduction of information about the target that should not be made available. We call it “contamination” (Figure 8). Training on contaminated data leads to overly optimistic expectations about the model performance.\n\n5An autoencoder model is trained to reconstruct its input from an embedding vector. The hyperparame-\n\nters of the autoencoder are tuned to minimize the reconstruction error of the holdout data.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\n3.3 What Is Good Data\n\nWe already considered questions to answer about the data before to start collecting it and the common problems with the data an analyst might encounter. But what constitutes good data for a machine learning project? Below we take a look at several properties of good data.\n\n3.3.1 Good Data Is Informative\n\nGood data contains enough information that can be used for modeling. For example, if you want to train a model that predicts whether the customer will buy a speciﬁc product, you will need to possess both the properties of the product in question and If you only have the the properties of the products customers purchased in the past. properties of the product and a customer’s location and name, then the predictions will be the same for all users from the same location.\n\nIf you have enough training examples, then the model can potentially derive the gender and ethnicity from the name and make diﬀerent predictions for men, women, locations, and ethnicities, but not to each customer individually.\n\n3.3.2 Good Data Has Good Coverage\n\nGood data has good coverage of what you want to do with the model. For example, if you’re going to use the model to classify web pages by topic and you have a thousand topics of interest, then your data has to contain examples of documents on each of the thousand topics in quantity suﬃcient for the algorithm to be able to learn the diﬀerence between topics.\n\nImagine a diﬀerent situation. Let’s say that for a particular topic, you only have one or a couple of documents. Let each document contain a unique ID in the text. In such a scenario, the learning algorithm will not be sure what it must look at in each document to understand to which topic it belongs. Maybe the IDs? They look like good diﬀerentiators. If the algorithm decides to use IDs to separate these couple examples from the rest of the dataset, then the learned model will not be able to generalize: it will not see any of those IDs ever again.\n\n3.3.3 Good Data Reﬂects Real Inputs\n\nGood data reﬂects real inputs that the model will see in production. For example, if you build a system that recognizes cars on the road and all pictures you have were taken during the working hours, then it’s unlikely that you will have many examples of night pictures. Once you deploy the model in production, pictures will start coming from all times of the day, and your model will more frequently make errors on night pictures. Also, remember the problem of a cat, a dog, and a raccoon: if your model doesn’t know anything about raccoons, it will predict their pictures as either dogs or cats.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\n3.3.4 Good Data Is Unbiased\n\nGood data is as unbiased as possible. This property can look similar to the previous one. Still, bias can be present in both the data you use for training and the data that the model is applied to in the production environment.\n\nWe discussed several sources of bias in data and how to deal with it in Section 3.2. A user interface can also be a source of bias. For example, you want to predict the popularity of a news article, and use the click rate as a feature. If some news article was displayed on the top of the page, the number of clicks it got would often be higher compared to another news article displayed on the bottom, even if the latter is more engaging.\n\n3.3.5 Good Data Is Not a Result of a Feedback Loop\n\nGood data is not a result of the model itself. This echoes the problem of the feedback loop discussed above. For example, you cannot train a model that predicts the gender of a person from their name, and then use the prediction to label a new training example.\n\nAlternatively, if you use the model to decide which email messages are important to the user and highlight those important messages, you should not directly take the clicks on those emails as a signal that the email is important. The user might have clicked on them because the model highlighted them.\n\n3.3.6 Good Data Has Consistent Labels\n\nGood data has consistent labels. Inconsistency in labeling can come from several sources:\n\nDiﬀerent people do labeling according to diﬀerent criteria. Even if people believe that they use the same criteria, diﬀerent people often interpret the same criteria diﬀerently.6 • The deﬁnition of some classes evolved over time. This results in a situation when two very similar feature vectors receive two diﬀerent labels.\n\nMisinterpretation of user’s motives. For example, assume that the user ignored a recommended news article. As a consequence, this news article receives a negative label. However, the motive of the user for ignoring this recommendation might be that they already knew the story and not that they are uninterested in the topic of the story.\n\n3.3.7 Good Data Is Big Enough\n\nGood data is big enough to allow generalization. Sometimes, nothing can be done to increase the accuracy of the model. No matter how much data you throw on the learning algorithm: the information contained in the data has low predictive power for your problem. However,\n\n6Recall the example of Mechanical Turk we considered in Section 3.1. To improve the reliability of labels\n\nassigned by diﬀerent people, one can use a majority vote (or an average) of several labelers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\nmore often, you can get a very accurate model if you pass from thousands of examples to millions or hundreds of millions. You cannot know how much data you need before you start working on your problem and see the progress.\n\n3.3.8 Summary of Good Data\n\nFor the convenience of future reference, let me once again repeat the properties of good data:\n\nit contains enough information that can be used for modeling, • it has good coverage of what you want to do with the model, • it reﬂects real inputs that the model will see in production, • it is as unbiased as possible, • it is not a result of the model itself, • it has consistent labels, and • it is big enough to allow generalization.\n\n3.4 Dealing With Interaction Data\n\nInteraction data is the data you can collect from user interactions with the system your model supports. You are considered lucky if you can gather good data from interactions of the user with the system.\n\nGood interaction data contains information on three aspects:\n\ncontext of interaction, • action of the user in that context, and • outcome of interaction.\n\nAs an example, assume that you build a search engine, and your model reranks search results for each user individually. A reranking model takes as input the list of links returned by the search engine, based on keywords provided by the user and outputs another list in which the items change order. Usually, a reranked model “knows” something about the user and their preferences and can reorder the generic search results for each user individually according to that user’s learned preferences. The context here is the search query and the hundred documents presented to the user in a speciﬁc order. The action is a click of the user on a particular document link. The outcome is how much time the user spent reading the document and whether the user hit “back.” Another action is the click on the “next page” link.\n\nThe intuition is that the ranking was good if the user clicked on some link and spent signiﬁcant time reading the page. The ranking was not so good if the user clicked on a link to a result and then hit “back” quickly. The ranking was bad if the user clicked on the “next page” link. This data can be used to improve the ranking algorithm and make it more personalized.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\nAsia\n\nFrance\n\nGermany\n\n...\n\nChina\n\n83M\n\n67M\n\n38,800\n\n...\n\n...\n\n...\n\n1386M\n\n8,802\n\nRegion\n\nEurope\n\n44,578\n\nCountry\n\nGDP\n\n...\n\nPopulation\n\nEurope\n\n...\n\n12.2T\n\n3.7T\n\n...\n\n...\n\n...\n\n2.6T\n\n...\n\nGDP percapita\n\nFigure 9: An example of the target (GDP) being a simple function of two features: Population and GDP per capita.\n\n3.5 Causes of Data Leakage\n\nLet’s discuss the three most frequent causes of data leakage that can happen during data collection and preparation: 1) target being a function of a feature, 2) feature hiding the target, and 3) feature coming from the future.\n\n3.5.1 Target is a Function of a Feature\n\nGross Domestic Product (GDP) is deﬁned as the monetary measure of all ﬁnished goods and services in a country within a speciﬁc period. Let our goal be to predict a country’s GDP based on various attributes: area, population, geographic region, and so on. An example of such data is shown in Figure 9. If you don’t do a careful analysis of each attribute and its relation to GDP, you might let a leakage happen: in the data in Figure 9, two columns, Population and GDP per capita, multiplied, equal GDP. The model you will train will perfectly predict GDP by looking at these two columns only. The fact that you let GDP be one of the features, though in a slightly modiﬁed form (devised by the population), constitutes contamination and, therefore, leads to data leakage.\n\nA simpler example is when you have a copy of the target among features, just put in a diﬀerent format. Imagine you train a model to predict the yearly salary, given the attributes of an employee. The training data is a table that contains both monthly and yearly salary, among many other attributes. If you forget to remove the monthly salary from the list of features, that attribute alone will perfectly predict the yearly salary, making you believe your model is perfect. Once the model is put in production, it will likely stop receiving information about a person’s monthly salary: otherwise, the modeling would not be needed.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "page_number": 55,
      "chapter_number": 7,
      "summary": "We will consider data imputation techniques in Section 3.7.1 Key topics include data, biased, and biases. 3.2.3 Noise\n\nNoise in data is a corruption of examples.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "data",
        "machine learning",
        "model",
        "Andriy Burkov Machine",
        "bias",
        "machine learning models",
        "Learning Engineering",
        "Burkov Machine",
        "learning",
        "training data",
        "machine",
        "Andriy Burkov",
        "noise"
      ],
      "concepts": [
        "data",
        "biased",
        "biases",
        "models",
        "examples",
        "features",
        "quality",
        "training",
        "answers",
        "answering"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "Segment 4 (pages 28-35)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 40,
          "title": "Segment 40 (pages 341-348)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 37,
          "title": "Segment 37 (pages 746-766)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 63-71)",
      "start_page": 63,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "content": "8,543\n\n3653\n\n11,987\n\nM18-25\n\nF25-35\n\n18879\n\n...\n\n2\n\n1\n\nGender\n\nM\n\n...\n\nF\n\nCustomer ID\n\n...\n\nGroup\n\n...\n\n...\n\nF\n\n...\n\n...\n\nF65+\n\n6,775\n\nYearlySpendings\n\n1350\n\n2365\n\n...\n\nYearlyPageviews\n\n...\n\n...\n\nFigure 10: An example of the target being hidden in one of the features.\n\n3.5.2 Feature Hides the Target\n\nSometimes the target is not a function of one or more features, but rather is “hidden” in one of the features. Consider the dataset in Figure 10.\n\nIn this scenario, you use customer data to predict their gender. Look at the column Group. If you closely investigate the data in the column Group, you will see that it represents a demographic value to which each existing customer was related in the past. If the data about a customer’s gender and age is factual (as opposed to being guessed by another model that might be available in production), then the column Group constitutes a form of data leakage, when the value you want to predict is “hidden” in the value of a feature.\n\nOn the other hand, if the Group values are predictions provided by another, possibly less accurate model, then you can use this attribute to build a potentially stronger model. This is called model stacking, and we will consider this topic in Section ?? in Chapter 6.\n\n3.5.3 Feature From the Future\n\nFeature from the future is a kind of data leakage that is hard to catch if you don’t have a clear understanding of the business goal. Imagine a client asked you to train a model that predicts whether a borrower will pay back the loan, based on attributes such as age, gender, education, salary, marital status, and so on. An example of such data is shown in Figure 11.\n\nIf you don’t make an eﬀort to understand the business context in which your model will be used, you might decide to use all available attributes to predict the value in the column Will Pay Loan, including the data from the column Late Payment Reminders. Your model will look accurate at testing time and you send it to the client, who will later report that the model doesn’t work well in the production environment.\n\nAfter investigation, you ﬁnd out that, in the production environment, the value of Late Payment Reminders is always zero. This makes sense because the client uses your model before the borrower gets the credit, so no reminders have yet been made! However, your\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25\n\n...\n\nF25-35\n\n65723\n\n...\n\n2\n\n1\n\n...\n\nBorrower ID\n\nM35-50\n\nN\n\nWill Pay Loan\n\n1\n\nY\n\n3\n\nMaster's\n\n...\n\n0\n\nLate PaymentReminders\n\nN\n\n...\n\n...\n\n...\n\nM25-35\n\n...\n\nEducation\n\nHigh school\n\nMaster's\n\n...\n\nDemographicGroup\n\n...\n\nFigure 11: A feature unavailable at the prediction time: Late Payment Reminders.\n\nmodel most likely learned to make the “No” prediction when Late Payment Reminders is 1 or more and pays less attention to the other features.\n\nHere is another example. Let’s say you have a news website and you want to predict the ranking of news you serve to the user, so as to maximize the number of clicks on stories. If in your training data, you have positional features for each news item served in the past (e.g., the x − y position of the title, and the abstract block on the webpage), such information will not be available on the serving time, because you don’t know the positions of articles on the page before you rank them.\n\nUnderstanding the business context in which the model will be used is, thus, crucial to avoid data leakage.\n\n3.6 Data Partitioning\n\nAs discussed in Section ?? of the ﬁrst chapter, in practical machine learning, we typically use three disjoint sets of examples: training set, validation set, and test set.\n\nTraining dataTestdata\n\nValidationdata\n\nEntire dataset\n\nFigure 12: The entire dataset partitioned into a training, validation and test sets.\n\nThe training set is used by the machine learning algorithm to train the model.\n\nThe validation set is needed to ﬁnd the best values for the hyperparameters of the machine learning pipeline. The analyst tries diﬀerent combinations of hyperparameter values one by\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26\n\none, trains a model by using each combination, and notes the model performance on the validation set. The hyperparameters that maximize the model performance are then used to train the model for production. We consider techniques of hyperparameter tuning in more detail in Section ?? of Chapter 5.\n\nThe test set is used for reporting: once you have your best model, you test its performance on the test set and report the results.\n\nValidation and test sets are often referred to as holdout sets: they contain the examples that the learning algorithm is not allowed to see.\n\nTo obtain good partitions of your entire dataset into these three disjoint sets, as schematically illustrated in Figure 12, partitioning has to satisfy several conditions.\n\nCondition 1: Split was applied to raw data.\n\nOnce you have access to raw examples, and before everything else, do the split. This will allow avoiding data leakage, as we will see later.\n\nCondition 2: Data was randomized before the split.\n\nRandomly shuﬄe your examples ﬁrst, then do the split.\n\nCondition 3: Validation and test sets follow the same distribution.\n\nWhen you select the best values of hyperparameters using the validation set, you want that this selection yields a model that works well in production. The examples in the test set are your best representatives of the production data. Hence the need for the validation and test sets to follow the same distribution.\n\nCondition 4: Leakage during the split was avoided.\n\nData leakage can happen even during the data partitioning. Below, we will see what forms of leakage can happen at that stage.\n\nThere is no ideal ratio for the split. In older literature (pre-big data), you might ﬁnd the recommended splits of either 70%/15%/15% or 80%/10%/10% (for training, validation, and test sets, respectively, in proportion to the entire dataset).\n\nToday, in the era of the Internet and cheap labor (e.g., Mechanical Turk or crowdsourcing), organizations, scientists, and even enthusiasts at home can get access to millions of training examples. That makes it wasteful only to use 70% or 80% of the available data for training.\n\nThe validation and test data are only used to calculate statistics reﬂecting the performance of the model. Those two sets just need to be large enough to provide reliable statistics. How much is debatable. As a rule of thumb, having a dozen examples per class is a desirable minimum. If you can have a hundred examples per class in each of the two holdout sets, you have a solid setup and the statistics calculated based on such sets are reliable.\n\nThe percentage of the split can also be dependent on the chosen machine learning algorithm or model. Deep learning models tend to signiﬁcantly improve when exposed to more training data. This is less true for shallow algorithms and models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27\n\nYour proportions may depend on the size of the dataset. A small dataset of less than a thousand examples would do best with 90% of the data used for training. In this case, you might decide to not have a distinct validation set, and instead simulate with the cross- validation technique. We will talk more about that in Section ?? in Chapter 5.\n\nIt’s worth mentioning that when you split time-series data into the three datasets, you must execute the split so that the order of observations in each example is preserved during the shuﬄing. Otherwise, for most predictive problems, your data will be broken, and no learning will be possible. We talk more about time series in Section ?? in Chapter 4.\n\n3.6.1 Leakage During Partitioning\n\nAs you already know, data leakage may happen at any stage, from data collection to model evaluation. The data partitioning stage is no exception.\n\nGroup leakage may occur during partitioning. Imagine you have magnetic resonance images of the brains of multiple patients. Each image is labeled with certain brain disease, and the same patient may be represented by several images taken at diﬀerent times. If you apply the partitioning technique discussed above (shuﬄe, then split), images of the same patient might appear in both the training and holdout data.\n\nThe model might learn from the particularities of the patient rather than the disease. The model would remember that patient A’s brain has speciﬁc brain convolutions, and if they have a speciﬁc disease in the training data, the model successfully predicts this disease in the validation data by recognizing patient A from just the brain convolutions.\n\nThe solution to group leakage is group partitioning. It consists of keeping all patient examples together in one set: either training or holdout. Once again, you can see how important it is for the data analyst to know as much as possible about the data.\n\n3.7 Dealing with Missing Attributes\n\nSometimes, the data comes to the analyst in a tidy form, such as an Excel spreadsheet,7 but you might ﬁnd some attributes missing. This often happens when the dataset was handcrafted, and the person forgot to ﬁll some values or didn’t get them measured.\n\nThe list of typical approaches of dealing with missing values for an attribute include:\n\nremoving the examples with missing attributes from the dataset (this can be done if your dataset is big enough to safely sacriﬁce some data);\n\nusing a learning algorithm that can deal with missing attribute values (such as the decision tree learning algorithm); • using a data imputation technique.\n\n7The fact that your raw dataset is contained in an Excel spreadsheet doesn’t guarantee that the data is\n\ntidy. One property of tidiness is that each row represents one example.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28\n\n3.7.1 Data Imputation Techniques\n\nTo impute the value of a missing numerical attribute, one technique consists in replacing the missing value by the average value of this attribute in the rest of the dataset. Mathematically it looks as follows. Let j be an attribute that is missing in some examples in the original dataset, and let S(j) be the set of size N(j) that contains only those examples from the original dataset in which the value of the attribute j is present. Then the missing value ˆx(j) of the attribute j is given by,\n\nˆx(j) ←\n\n1 N(j)\n\nX\n\nx(j) i\n\n,\n\ni∈S(j)\n\nwhere N(j) < N and the summation is made only over those examples where the value of the attribute j is present. An illustration of this technique is given in Figure 13, where two examples (at row 1 and 3) have the Height attribute missing. The average value, 177, will be imputed in the empty cells.\n\nAge\n\nWeight\n\n2\n\nRow\n\n5\n\n169\n\nSalary\n\n3\n\n18\n\n175\n\n21\n\n19,000\n\n60\n\n65\n\n34\n\n187\n\n43\n\nHeight\n\n1\n\n4\n\n65\n\n70\n\n35,000\n\n66\n\n76,500\n\n87\n\n26,900\n\n94,800\n\nFigure 13: Replacing the missing value by an average value of this attribute in the dataset.\n\nAnother technique is to replace the missing value with a value outside the normal range of values. For example, if the regular range is [0,1], you can set the missing value to 2 or −1; if the attribute is categorical, such as days of the week, then a missing value can be replaced by the value “Unknown.” Here, the learning algorithm learns what to do when the attribute has a value diﬀerent from regular values. If the attribute is numerical, another technique is replacing the missing value with a value in the middle of the range. For example, if the range for an attribute is [−1,1], you can set the missing value to be equal to 0. Here, the idea is that the value in the middle of the range will not signiﬁcantly aﬀect the prediction.\n\nA more advanced technique is to use the missing value as the target variable for a regression problem. (In this case, we assume all attributes are numerical.) You can use the remaining ] to form a feature vector ˆxi, set ˆyi ← x(j) attributes [x(1) ,\n\n,x(2) i\n\n,...,x(j−1) i\n\n,x(j+1) i\n\n,...,x(D)\n\ni\n\ni\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\ni\n\n29\n\nwhere j is the attribute with a missing value. Then you build a regression model to predict ˆy from ˆx. Of course, to build training examples (ˆx, ˆy), you only use those examples from the original dataset, in which the value of attribute j is present.\n\nFinally, if you have a signiﬁcantly large dataset and just a few attributes with missing values, you can add a synthetic binary indicator attribute for each original attribute with missing values. Let’s say that examples in your dataset are D-dimensional, and attribute at position j = 12 has missing values. For each example x, you then add the attribute at position j = D + 1, which is equal to 1 if the value of the attribute at position 12 is present in x and 0 otherwise. The missing value then can be replaced by 0 or any value of your choice.\n\nAt prediction time, if your example is not complete, you should use the same data imputation technique to ﬁll the missing values as the technique you used to complete the training data.\n\nBefore you start working on the learning problem, you cannot tell which data imputation technique will work best. Try several techniques, build several models, and select the one that works best (using the validation set to compare models).\n\n3.7.2 Leakage During Imputation\n\nIf you use the imputation techniques that compute some statistic of one attribute (such as average) or several attributes (by solving the regression problem), the leakage happens if you use the whole dataset to compute this statistic. Using all available examples, you contaminate the training data with information obtained from the validation and test examples.\n\nThis type of leakage is not as signiﬁcant as other types discussed earlier. However, you still have to be aware of it and avoid it by partitioning ﬁrst, and then computing the imputation statistic only on the training set.\n\n3.8 Data Augmentation\n\nFor some types of data, it’s quite easy to get more labeled examples without additional labeling. The strategy is called data augmentation, and it’s most eﬀective when applied to images. It consists of applying simple operations, such as crop or ﬂip, to the original images to obtain new images.\n\n3.8.1 Data Augmentation for Images\n\nIn Figure 14, you can see examples of operations that can be easily applied to a given image to obtain one or more new images: ﬂip, rotation, crop, color shift, noise addition, perspective change, contrast change, and information loss.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30\n\nFigure 14: Examples of data augmentation techniques. Photo credit: Alfonso Escalante.\n\nFlipping, of course, has to be done only with respect to the axis for which the meaning of the image is preserved. If it’s a football, you can ﬂip with respect to both axes,8 but if it’s a car or a pedestrian, then you should only ﬂip with respect to the vertical axis.\n\nRotation should be applied with slight angles to simulate an incorrect horizon calibration. You can rotate an image in both directions.\n\nCrops can be randomly applied multiple times to the same image by keeping a signiﬁcant part of the object(s) of interest in the cropped images.\n\nIn color shift, nuances of red-green-blue (RGB) are slightly changed to simulate diﬀerent lighting conditions. Contrast change (both decreasing and increasing) and Gaussian noise of diﬀerent intensity can also be applied multiple times to the same image.\n\nBy randomly removing parts of an image, we can simulate situations when an object is\n\n8Unless the context, like grass, makes ﬂipping according to the horizontal axis irrelevant.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31\n\nrecognizable but not entirely visible because of an obstacle.\n\nAnother popular technique of data augmentation that seems counterintuitive, but works very well in practice, is mixup. As the name suggests, the technique consists of training the model on a mix of the images from the training set. More precisely, instead of training the model on the raw images, we take two images (that could be of the same class or not) and use for training their linear combination:\n\nmixup_image = t × image1 + (1 − t) × image2,\n\nwhere t is a real number between 0 and 1. The target of that mixup image is a combination of the original targets obtained using the same value of t:\n\nmixup_target = t × target1 + (1 − t) × target2.\n\nExperiments9 on the ImageNet-2012, CIFAR-10, and several other datasets showed that mixup improves the generalization of neural network models. The authors of the mixup also found that it increases the robustness to adversarial examples and stabilizes the training of generative adversarial networks (GANs).\n\nIn addition to the techniques shown in Figure 14, if you expect the input images in your production system will come overcompressed, you can simulate overcompression by using some frequently used lossy compression methods and ﬁle formats, such as JPEG or GIF.\n\nOnly training data undergoes augmentation. Of course, it’s impractical to generate all these additional examples in advance and store them. In practice, the data augmentation techniques are applied to the original data on-the-ﬂy during training.\n\n3.8.2 Data Augmentation for Text\n\nWhen it comes to text data augmentations, it is not as straightforward. We need to use appropriate transformation techniques to preserve the contextual and grammatical structure of natural language texts.\n\nOne technique involves replacing random words in a sentence with their close synonyms. For the sentence, “The car stopped near a shopping mall.” some equivalent sentences are:\n\n“The automobile stopped near a shopping mall.”\n\n“The car stopped near a shopping center.”\n\n“The auto stopped near a mall.”\n\n9More details on the mixup technique can be found in Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. “mixup: Beyond empirical risk minimization.” arXiv preprint arXiv:1710.09412 (2017).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32\n\nA similar technique uses hypernyms instead of synonyms. A hypernym is a word that has more general meaning. For example, “mammal” is a hypernym for “whale” and “cat”; “vehicle” is a hypernym for “car” and “bus.” From our example above, we could create the fol- lowing sentences:\n\n“The vehicle stopped near a shopping mall.”\n\n“The car stopped near a building.”\n\nIf you represent words or documents in your dataset using word or document embeddings, you can apply slight Gaussian noise to randomly chosen embedding features to make a variation of the same word or document. You can tune the number of features to modify and the noise intensity as hyperparameters by optimizing the performance on validation data.\n\nAlternatively, to replace a given word w in the sentence, you can ﬁnd k nearest neighbors to the word w in the word embedding space and generate k new sentences by replacing the word w with its respective neighbor. The nearest neighbors can be found using a measure such as cosine similarity or Euclidean distance. The choice of the measure and the value of k, can be tuned as hyperparameters.\n\nA modern alternative to the k-nearest-neighbors approach described above is to use a deep pre-trained model such as Bidirectional Encoder Representations from Transformers (BERT). Models like BERT are trained to predict a masked word given other words in a sentence. One can use BERT to generate k most likely predictions for a masked word and then use them as synonyms for data augmentation.\n\nSimilarly, if your problem is document classiﬁcation, and you have a large corpus of unlabeled documents, but only a small corpus of labeled documents, you can do as follows. First, build document embeddings for all documents in your large corpus. Use doc2vec or any other technique of document embedding. Then, for each labeled document d in your dataset, ﬁnd k closest unlabeled documents in the document embedding space and label them with the same label as d. Again, tune k on the validation data.\n\nAnother useful text data augmentation technique is back translation. To create a new example from a text written in English (it can be a sentence or a document), ﬁrst translate it into another language l using a machine translation system. Then translate it back from l into English. If the text obtained through back translation is diﬀerent from the original text, you add it to the dataset by assigning the same label as the original text.\n\nThere are also data augmentation techniques for other data types, such as audio and video: addition of noise, shifting an audio or a video clip in time, slowing it down or accelerating, changing pitch for audio and color balance for video, to name a few. Describing these techniques in detail is out of the scope of this book. You should just be aware that data augmentation can be applied to any media data, and not only images and text.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33",
      "page_number": 63,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 63-71). Key topics include data, model, and good. Whether to exclude outliers from the training data, or to use machine learning algorithms and models robust to outliers, is debatable.",
      "keywords": [
        "Good Data",
        "data",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Data Leakage",
        "model",
        "machine learning",
        "Andriy Burkov Machine",
        "Good",
        "Learning Engineering",
        "Good Data Reﬂects",
        "Burkov Machine",
        "learning",
        "training data",
        "Good interaction data"
      ],
      "concepts": [
        "data",
        "model",
        "good",
        "examples",
        "learn",
        "time",
        "feature",
        "label",
        "prediction",
        "predicts"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 433-440)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 72-81)",
      "start_page": 72,
      "end_page": 81,
      "detection_method": "topic_boundary",
      "content": "3.9 Dealing With Imbalanced Data\n\nClass imbalance is a condition in the data that can signiﬁcantly aﬀect the performance of the model, independently of the chosen learning algorithm. The problem is a very uneven distribution of labels in the training data.\n\nThis is the case, for example, when your classiﬁer has to distinguish between genuine and fraudulent e-commerce transactions: the examples of genuine transactions are much more frequent. Typically, a machine learning algorithm tries to classify most training examples correctly. The algorithm is pushed to do so because it needs to minimize a cost function that typically assigns a positive loss value to each misclassiﬁed example. If the loss is the same for the misclassiﬁcation of a minority class example as it is for the misclassiﬁcation of a majority class, then it’s very likely that the learning algorithm decides to “give up” on many minority class examples in order to make fewer mistakes in the majority class.\n\nWhile there is no formal deﬁnition of imbalanced data, consider the following rule of thumb. If there are two classes, then balanced data would mean half of the dataset representing each class. A slight class imbalance is usually not a problem. So, if 60% examples belong to one class and 40% belong to the other, and you use a popular machine learning algorithm in its standard formulation, it should not cause any signiﬁcant performance degradation. However, when the class imbalance is high, for example when 90% examples are of one class, and 10% are of the other, using the standard formulation of the learning algorithm that usually equally weights errors made in both classes may not be as eﬀective and would need modiﬁcation.\n\n3.9.1 Oversampling\n\nA technique used frequently to mitigate class imbalance is oversampling. By making multi- ple copies of minority class examples, it increases their weight, as illustrated in Figure 15a. You might also create synthetic examples by sampling feature values of several examples of the minority class and combining them to obtain a new example of that class. Two popular algo- rithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\n\nSMOTE and ADASYN work similarly in many ways. For a given example xi of the minority class, they pick k nearest neighbors. Let’s denote this set of k examples as Sk. The synthetic example xnew is deﬁned as xi + λ(xzi − xi), where xzi is an example of the minority class chosen randomly from Sk. The interpolation hyperparameter λ is an arbitrary number in the range [0,1]. (See an illustration for λ = 0.5 in Figure 16.)\n\nBoth SMOTE and ADASYN randomly pick among all possible xi in the dataset. In ADASYN, the number of synthetic examples generated for each xi is proportional to the number of examples in Sk, which are not from the minority class. Therefore, more synthetic examples are generated in the area where the minority class examples are rare.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34\n\nOriginal dataOversampled data\n\nOriginal dataUndersampled data\n\nFigure 15: Undersampling (left) and oversampling (right).\n\n3.9.2 Undersampling\n\nAn opposite approach, undersampling, is to remove from the training set some examples of the majority class (Figure 15b).\n\nThe undersampling can be done randomly; that is, the examples to remove from the majority class can be chosen at random. Alternatively, examples to withdraw from the majority class can be selected based on some property. One such property is Tomek links. A Tomek link exists between two examples xi and xj belonging to two diﬀerent classes if there’s no other example xk in the dataset closer to either xi or xj than the latter two are to each other. The closeness can be deﬁned using a metric such as cosine similarity or Euclidean distance.\n\nIn Figure 17, you can see how removing examples from the majority class based on Tomek links helps to establish a clear margin between examples of two classes.\n\nCluster-based undersampling works as follows. Decide on the number of examples you want to have in the majority class resulting from undersampling. Let that number be k. Run a centroid-based clustering algorithm on the majority examples only with k being the desired number of clusters. Then replace all examples in the majority classes with the k centroids. An example of a centroid-based clustering algorithm is k-nearest neighbors.\n\n3.9.3 Hybrid Strategies\n\nYou can develop your hybrid strategies (by combining both over- and undersampling) and possibly get better results. One such strategy consists of using ADASYN to oversample, and then Tomek links to undersample.\n\nAnother possible strategy consists of combining cluster-based undersampling with SMOTE.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35\n\nFigure 16: An illustration of a synthetic example generation for SMOTE and ADASYN. (Built using a script adapted from Guillaume Lemaitre.)\n\n3.10 Data Sampling Strategies\n\nWhen you have a large data asset, so-called big data, it’s not always practical or necessary to work with the entire data asset. Instead, you can draw a smaller data sample that contains enough information for learning.\n\nSimilarly, when you undersample the majority class to adjust for data imbalance, the smaller data sample should be representative of the entire majority class. In this section, we discuss several sampling strategies, their properties, advantages, and drawbacks.\n\nIn There are two main strategies: probability sampling and nonprobability sampling. probability sampling, all examples have a chance to be selected. These techniques involve randomness.\n\nNonprobability sampling is not random. To build a sample, it follows a ﬁxed deterministic sequence of heuristic actions. This means that some examples don’t have a chance of being selected, no matter how many samples you build.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36\n\n(a) original data\n\n(b) Tomek links\n\n(c) undersampled data\n\nFigure 17: Undersampling with Tomek links.\n\nHistorically, nonprobability methods were more manageable for a human to execute manually. Nowadays this advantage is not signiﬁcant. Data analysts use computers and software that greatly simplify sampling, even from big data. The main drawback of nonprobability sampling methods is that they include non-representative samples and might systematically exclude important examples. These drawbacks outweigh the possible advantages of nonprobability sampling methods. Therefore, in this book I will only present probability sampling methods.\n\n3.10.1 Simple Random Sampling\n\nSimple random sampling is the most straightforward method, and the one I refer to when I say “sample randomly.” Here, each example from the entire dataset is chosen purely by chance; each example has an equal chance of being selected.\n\nOne way of obtaining a simple random sample is to assign a number to each example, and then use a random number generator to decide which examples to select. For example, if your entire dataset contains 1000 examples, tagged from 0 to 999, use groups of three digits from the random number generator to select an example. So, if the ﬁrst three numbers from the random number generator were 0, 5, and 7, choose the example numbered 57, and so on.\n\nSimplicity is the great advantage of this sampling method, and it can be easily implemented as any programming language can serve as a random number generator. A disadvantage of simple random sampling is that you may not select enough examples that would have a particular property of interest. Consider the situation where you extract a sample from a large imbalanced dataset. In so doing, you accidentally fail to capture a suﬃcient number of examples from the minority class - or any at all.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37\n\n3.10.2 Systematic Sampling\n\nTo implement systematic sampling (also known as interval sampling), you create a list containing all examples. From that list, you randomly select the ﬁrst example xstart from the ﬁrst k elements on the list. Then, you select every kth item on the list starting from xstart. You choose such a value of k that will give you a sample of the desired size.\n\nAn advantage of the systematic sampling over the simple random sampling is that it draws examples from the whole range of values. However, systematic sampling is inappropriate if the list of examples has periodicity or repetitive patterns. In the latter case, the obtained sample can exhibit a bias. However, if the list of examples is randomized, then systematic sampling often results in a better sample than simple random sampling.\n\n3.10.3 Stratiﬁed Sampling\n\nIf you know about the existence of several groups (e.g., gender, location, or age) in your data, you should have examples from each of those groups in your sample. In stratiﬁed sampling, you ﬁrst divide your dataset into groups (called strata) and then randomly select examples from each stratum, like in simple random sampling. The number of examples to select from each stratum is proportional to the size of the stratum.\n\nStratiﬁed sampling often improves the representativeness of the sample by reducing its bias; in the worst of cases, the resulting sample is of no less quality than the results of simple random sampling. However, to deﬁne strata, the analyst has to understand the properties of the dataset. Furthermore, it can be diﬃcult to decide which attributes will deﬁne the strata.\n\nIf you don’t know how to best deﬁne the strata, you can use clustering. The only decision you have to make is how many clusters you need. This technique is also useful to choose the unlabeled examples to send for labeling to a human labeler. It often happens that we have millions of unlabeled examples, and few resources available for labeling. Choose examples carefully, so that each stratum or cluster is represented in our labeled data.\n\nStratiﬁed sampling is the slowest of the three methods due to the additional overhead of working with several independent strata. However, its potential beneﬁt of producing a less biased sample typically outweighs its drawbacks.\n\n3.11 Storing Data\n\nKeeping data safe is insurance for your organization’s business: if you lose a business-critical model for any reason, such as a disaster or human mistake (the ﬁle with the model was accidentally erased or overwritten), having the data will allow you to rebuild that model easily.\n\nWhen sensitive data or personally identiﬁable information (PII) is provided by customers or business partners, it must be stored in not just a safe but also a secure location. Jointly with\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38\n\nthe DBA or DevOps engineers, access to sensitive data can be restricted by username and, if needed, IP address. Access to a relational database might also be limited on the per row and per column basis.\n\nIt’s also recommended to limit access to read-only and add-only operations, by restricting write and erase operations to speciﬁc users.\n\nIf the data is collected on mobile devices, it might be necessary to store it on the mobile device until the owner connects to wiﬁ. This data might need to be encrypted so that other applications cannot access it. Once the user is connected to wiﬁ, the data has to be synchronized with a secure server by using cryptographic protocols, such as Transport Layer Security (TLS). Each data element on the mobile device has to be marked with a timestamp to allow its proper synchronization with the data on the server.\n\n3.11.1 Data Formats\n\nData for machine learning can be stored in various formats. Data used indirectly, such as dictionaries or gazetteers, may be stored as a table in a relational database, a collection in a key-value store, or a structured text ﬁle.\n\nThe tidy data is usually stored as comma-separated values (CSV) or tab-separated values (TSV) ﬁles. In this case, all examples are stored in one ﬁle. Alternatively, collection of XML (Extensible Markup Language) ﬁles or JSON (JavaScript Object Notation) ﬁles can contain one example per ﬁle.\n\nIn addition to general-purpose formats, certain popular machine learning packages use proprietary data formats to store tidy data. Other machine learning packages often provide application programming interfaces (APIs) to one or several such proprietary data formats. The most frequently supported formats are ARFF (Attribute-Relation File Format used in the Weka machine learning package) and the LIBSVM (Library for Support Vector Machines) format, which is the default format used by the LIBSVM and LIBLINEAR (Library for Large Linear Classiﬁcation) machine learning libraries.\n\nThe data in the LIBSVM format consists of one ﬁle containing all examples. Each line of that ﬁle represents a labeled feature vector using the following format:\n\nlabel index1:value1 index2:value2 ...\n\nwhere indexX:valueY speciﬁes the value Y of the feature at position (dimension) X. If the value at some position is zero, it can be omitted. This data format is especially convenient for sparse data consisting of examples in which the values of most features are zero.\n\nFurthermore, diﬀerent programming languages come with data serialization capabilities. The data for a speciﬁc machine learning package can be persisted on the hard drive using a serialization object or function provided by the programming language or library. When needed, the data can be deserialized in its original form. For example, in Python, a popular\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39\n\ngeneral-purpose serialization module is Pickle; R has built-in saveRDS and readRDS functions. Diﬀerent data analysis packages can also oﬀer their own serialization/deserialization tools.\n\nIn Java, any object that implements the java.io.Serializable interface can be serialized into a ﬁle and then deserialized when needed.\n\n3.11.2 Data Storage Levels\n\nBefore deciding how and where to store the data, it’s essential to choose the appropriate storage level. Storage can be organized in diﬀerent levels of abstraction: from the lowest level, the ﬁlesystem, to the highest level, such as data lake.\n\nFilesystem is the foundational level of storage. The fundamental unit of data on that level is a ﬁle. A ﬁle can be text or binary, is not versioned, and can be easily erased or overwritten.\n\nA ﬁlesystem can be local or networked. A networked ﬁlesystem can be simple or distributed.\n\nA local ﬁlesystem can be as simple as a locally mounted disk containing all the ﬁles needed for your machine learning project.\n\nA distributed ﬁlesystem, such as NFS (Network File System), CephFS (Ceph File System) or HDFS, can be accessed over the network by multiple physical or virtual machines. Files in a distributed ﬁlesystem are stored and accessed over multiple machines in the network.\n\nDespite its simplicity, ﬁlesystem-level storage is appropriate for a many use cases, including:\n\nFile sharing\n\nThe simplicity of ﬁlesystem-level storage and support for standard protocols allows you to store and share data with a small group of colleagues with minimal eﬀort.\n\nLocal archiving\n\nFilesystem-level storage is a cost-eﬀective option for archiving data, thanks to the availability and accessibility of scale-out NAS solutions.\n\nData protection\n\nFilesystem-level storage is a viable data protection solution thanks to built-in redundancy and replication.\n\nParallel access to the data on the ﬁlesystem level is fast for retrieval access but slow for storage, so it’s an appropriate storage level for smaller teams and data.\n\nObject storage is an application programming interface (API) deﬁned over a ﬁlesystem. Using an API, you can programmatically execute such operations on ﬁles as GET, PUT, or DELETE without worrying where the ﬁles are actually stored. The API is typically provided by an API service available on the network and accessible by HTTP or, more generally, TCP/IP or a diﬀerent communication protocol suite.\n\nThe fundamental unit of data in an object storage level is an object. Objects are usually binary: images, sound, or video ﬁles, and other data elements having a particular format.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40\n\nSuch features as versioning and redundancy can be built into the API service. The access to the data stored on the object storage level can often be done in parallel, but the access is not as fast as on the ﬁlesystem level.\n\nCanonical examples of object storage are Amazon S3 and Google Cloud Storage (GCS). Alternatively, Ceph is a storage platform that implements object storage on a single dis- tributed computer cluster and provides interfaces for both object- and ﬁlesystem-level storage. It’s often used as an alternative to S3 and GCS in on-premises computing systems.\n\nThe database level of data storage allows persistent, fast, and scalable storage of structured data with fast parallel access for both storage and retrieval.\n\nA modern database management system (DBMS) stores data in random-access memory (RAM), but software ensures that data is persisted (and operations on data are logged) to disk and never lost.\n\nThe fundamental unit of data at this level is a row. A row has a unique ID and contains values in columns. In a relational database, rows are organized in tables. Rows can have references to other rows in the same or diﬀerent tables.\n\nDatabases are not exceptionally well suited for storing binary data, though rather small binary objects can sometimes be stored in a column in the form of a blob (for Binary Large OBject). Blob is a collection of binary data stored as a single entity. More often, though, a row stores references to binary objects stored elsewhere — in a ﬁlesystem or object storage.\n\nThe four most frequently used DBMS in the industry are Oracle, MySQL, Microsoft SQL Server, and PostgresSQL. They all support SQL (Structured Query Language), an interface for accessing and modifying data stored in the databases, as well as creating, modifying, and erasing databases.10\n\nA data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or ﬁles. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data.\n\nThe data is saved in the data lake in its raw format, including the structured data. To read data from a data lake, the analyst needs to write the programming code that reads and parses the data stored in a ﬁle or a blob. Writing a script to parse the data ﬁle or a blob is an approach called schema on read, as opposed to the schema on write in DBMS. In a DBMS, the schema of data is deﬁned beforehand, and, at each write, the DBMS makes sure that the data corresponds to the schema.\n\n10The SQL Server uses its proprietary Transact SQL (T-SQL) while Oracle uses Procedural Language SQL\n\n(PL/SQL).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41\n\n3.11.3 Data Versioning\n\nIf data is held and updated in multiple places, you might need to keep track of versions. Versioning the data is also needed if you frequently update the model by collecting more data, especially in an automated way. This happens when you work on automated driving, spam detection, or personalized recommendations, for example. The new data comes from a human driving a car, or the user cleaning up their electronic mail, or recent video streaming. Sometimes, after an update of the data, the new model performs worse, and you would like to investigate why by switching from one version of the data to another.\n\nData versioning is also critical in supervised learning when the labeling is done by multiple labelers. Some labelers might assign very diﬀerent labels to similar examples, which typically hurts the performance of the model. You would like to keep the examples annotated by diﬀerent labelers separately and only merge them when you build the model. Careful analysis of the model performance may show that labelers didn’t provide quality or consistent labels. Exclude such data from the training data, or relabel it, and data versioning will allow this with minimal eﬀort.\n\nData versioning can be implemented in several levels of complexity, from the most basic to the most elaborate.\n\nLevel 0: data is unversioned.\n\nAt this level, data may reside on a local ﬁlesystem, object storage, or in a database. The advantage of having unversioned data is the speed and simplicity of dealing with the data. Still, that advantage is outweighed by potential problems you might encounter when working on your model. Most likely, your ﬁrst problem will be the inability to make versioned deployments. As we will discuss in Chapter 8, model deployments must be versioned. A deployed machine learning model is a mix of code and data. If the code is versioned, the data must be too. Otherwise, the deployment will be unversioned.\n\nIf you don’t version deployments, you will not be able to get back to the previous level of performance in case of any problem with the model. Therefore, unversioned data is not recommended.\n\nLevel 1: data is versioned as a snapshot at training time.\n\nAt this level, data is versioned by storing, at training time, a snapshot of everything needed to train a model. Such an approach allows you to version deployed models and get back to past performance. You should keep track of each version in some document, typically an Excel spreadsheet. That document should describe the location of the snapshot of both code and data, hyperparameter values, and other metadata needed to reproduce the experiment if needed. If you don’t have many models and don’t update them too frequently, this level of versioning could be a viable strategy. Otherwise, it’s not recommended.\n\nLevel 2: both data and code are versioned as one asset.\n\nAt this level of versioning, small data assets, such as dictionaries, gazetteers, and small\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42\n\ndatasets, are stored jointly with the code in a version control system, such as Git or Mercurial. Large ﬁles are stored in object storage, such as S3 or GCS, with unique IDs. The training data is stored as JSON, XML, or another standard format, and includes relevant metadata such as labels, the identity of the labeler, time of labeling, the tool used to label the data, and so on.\n\nTools like Git Large File Storage (LFS) automatically replace large ﬁles such as audio samples, videos, large datasets, and graphics with text pointers, inside Git, while storing the ﬁle contents on a remote server.\n\nThe version of the dataset is deﬁned by the git signatures of the code and the data ﬁle. It can also be helpful to add a timestamp to identify a needed version easily.\n\nLevel 3: using or building a specialized data versioning solution.\n\nData versioning software such as DVC and Pachyderm provide additional tools for data versioning. They typically interoperate with code versioning software, such as Git.\n\nLevel 2 of versioning is a recommended way of implementing versioning for most projects. If you feel like Level 2 is not suﬃcient for your needs, explore Level 3 solutions, or consider building your own. Otherwise, that approach is not recommended, as it adds complexity to what is already a complex engineering project.\n\n3.11.4 Documentation and Metadata\n\nWhile you are actively working on a machine learning project, you are often capable of remembering important details about the data. However, once the project goes to production and you switch to another project, this information will eventually become less detailed.\n\nBefore you switch to another project, you should make sure that others can understand your data and use it properly.\n\nIf the data is self-explanatory, then you might leave it undocumented. However, it’s rather rare that someone who didn’t create a dataset can easily understand it and know how to use it just by looking at it.\n\nDocumentation has to accompany any data asset that was used to train a model. This documentation has to contain the following details:\n\nwhat that data means, • how it was collected, or methods used to create it (instructions to labelers and methods for quality control),\n\nthe details of train-validation-test splits, • details of all pre-processing steps, • an explanation of any data that were excluded, • what format is used to store the data, • types of attributes or features (which values are allowed for each attribute or feature), • number of examples,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n43",
      "page_number": 72,
      "chapter_number": 9,
      "summary": "This chapter covers segment 9 (pages 72-81). Key topics include data, examples, and sampling. Let j be an attribute that is missing in some examples in the original dataset, and let S(j) be the set of size N(j) that contains only those examples from the original dataset in which the value of the attribute j is present.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Data",
        "Machine Learning",
        "Learning Engineering",
        "Data Augmentation",
        "Burkov Machine",
        "data augmentation techniques",
        "sampling",
        "Simple Random Sampling",
        "attribute",
        "Andriy Burkov",
        "learning",
        "data imputation technique"
      ],
      "concepts": [
        "data",
        "examples",
        "sampling",
        "sample",
        "techniques",
        "randomly",
        "randomized",
        "classes",
        "dataset",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 53,
          "title": "Segment 53 (pages 449-456)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 18,
          "title": "Segment 18 (pages 151-158)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 4,
          "title": "Segment 4 (pages 28-35)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 82-90)",
      "start_page": 82,
      "end_page": 90,
      "detection_method": "topic_boundary",
      "content": "possible values for labels or the allowable range for a numerical target.\n\n3.11.5 Data Lifecycle\n\nSome data can be stored indeﬁnitely. However, in some business contexts, you might be allowed to store some data for a speciﬁc time, and then you might have to erase it. If such restrictions apply to the data you work with, you have to make sure that a reliable alerting system is in place. That alerting system has to contact the person responsible for the data erasure and have a backup plan in case that person is not available. Don’t forget that the consequences for not erasing data can sometimes be very serious for the organization.\n\nFor every sensitive data asset, a data lifecycle document has to describe the asset, the circle of persons who have access to that data asset, both during and after the project development. The document has to describe how long the data asset will be stored and whether it has to be explicitly destroyed.\n\n3.12 Data Manipulation Best Practices\n\nTo conclude this chapter, we consider two remaining best practices: reproducibility and “data ﬁrst, algorithm second.”\n\n3.12.1 Reproducibility\n\nReproducibility should be an important concern in everything you do, including data collection and preparation. You should avoid transforming data manually, or using powerful tools included in text editors or command line shells, such as regular expressions, “quick and dirty” ad hoc awk or sed commands, and piped expressions.\n\nUsually, the data collection and transformation activities consist of multiple stages. These include downloading data from web APIs or databases, replacing multiword expressions by unique tokens, removing stop-words and noise, cropping and unblurring images, imputation of missing values, and so on. Each step in this multistage process has to be implemented as a software script, such as Python or R script with their inputs and outputs. If you are organized like that in your work, it will allow you to keep track of all changes in the data. If during any stage something wrong happens to the data, you can always ﬁx the script and run the entire data processing pipeline from scratch.\n\nOn the other hand, manual interventions can be hard to reproduce. These are diﬃcult to apply to updated data, or scale for much more data (once you can aﬀord getting more data or a diﬀerent dataset).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n44\n\n3.12.2 Data First, Algorithm Second\n\nRemember that in the industry, contrary to academia, “data ﬁrst, algorithm second,” so focus most of your eﬀort and time on getting more data of wide variety and high quality, instead of trying to squeeze the maximum out of a learning algorithm.\n\nData augmentation, when implemented well, will most likely contribute more to the quality of the model than the search for the best hyperparameter values or model architecture.\n\n3.13 Summary\n\nBefore you start collecting the data, there are ﬁve questions to answer: is the data you will work with accessible, sizeable, useable, understandable, and reliable.\n\nCommon problems with data are high cost, bias, low predictive power, outdated examples, outliers, and leakage.\n\nGood data contains enough information that can be used for modeling, has good coverage of what you want to do with the model, and reﬂects real inputs that the model will see in production. It is as unbiased as possible and not a result of the model itself, has consistent labels, and is big enough to allow generalization.\n\nGood interaction data contains information on three aspects: context of interaction, action of the user in that context, and outcome of the interaction.\n\nTo obtain a good partition of your entire dataset into training, validation and test sets, the process of partitioning has to satisfy several conditions: 1) data was randomized before the split, 2) split was applied to raw data, 3) validation and test sets follow the same distribution, and 4) leakage was avoided.\n\nData imputation techniques can be used to deal with missing attributes in the data.\n\nData augmentation techniques are often used to get more labeled examples without additional manual labeling. The techniques usually apply to image data, but could also be applied to text and other types of perceptive data.\n\nClass imbalance can signiﬁcantly aﬀect the performance of the model. Learning algorithms perform suboptimally when the training data suﬀers from class imbalance. Such techniques as over- and undersampling can help to overcome the class imbalance problem.\n\nWhen you work with big data, it’s not always practical and necessary to work with the entire data asset. Instead, draw a smaller sample of data that contains enough information for learning. Diﬀerent data sampling strategies can be used for that, in particular simple random sampling, systematic sampling, stratiﬁed sampling, and cluster sampling.\n\nData can be stored in diﬀerent data formats and on several data storage levels. Data versioning is a critical element in supervised learning when the labeling is done by multiple labelers. Diﬀerent labelers may provide labels of varying quality, so it’s important to keep\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n45\n\ntrack of who created which labeled example. Data versioning can be implemented with several levels of complexity, from the most basic to the most elaborate: unversioned (level 0), versioned as a snapshot at training time (level 1), versioned as one asset containing both data and code (level 2), and versioned by using or building a specialized data versioning solution (level 3).\n\nLevel 2 is recommended for most projects.\n\nDocumentation has to accompany any data asset that was used to train a model. That documentation has to contain the following details: what that data means, how it was collected, or methods used to create it (instructions to labelers and methods for quality control), the details of train-validation-test splits and of all pre-processing steps. It must also contain an explanation of any data that were excluded, what format is used to store the data, types of attributes or features, number of examples, and possible values for labels or the allowable range for a numerical target.\n\nFor every sensitive data asset, a data lifecycle document has to describe the asset, the circle of persons who have access to that data asset both during and after the project development.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n46\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n4 Feature Engineering\n\nAfter data collection and preparation, feature engineering is the second most important activity in machine learning. It’s also the third stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nFeature engineering is a process of ﬁrst conceptually and then programmatically transforming a raw example into a feature vector. It consists of conceptualizing a feature and then writing the programming code that would transform the entire raw example, with potentially the help of some indirect data, into a feature.\n\n4.1 Why Engineer Features\n\nTo be more speciﬁc, consider the problem of recognizing movie titles in tweets. Say you have a vast collection of movie titles; this is data to use indirectly. You also have a collection of tweets; this data will be used directly to create examples. First, build an index of movie titles for fast string matching.1 Then ﬁnd all movie title matches in your tweets. Now stipulate that your examples are matches, and your machine learning problem is that of binary classiﬁcation: whether a match is a movie, or is not a movie.\n\nConsider the following tweet:\n\n1To build an index for fast string matching, you can, for example, use the Aho–Corasick algorithm.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nFigure 2: A tweet from Kyle.\n\nOur movie title matching index would help us ﬁnd the following matches: “avatar,” “the terminator,” “It,” and “her”. That gives us four unlabeled examples. You can label those four examples: {(avatar,False),(the terminator,True),(It,False),(her,False)}. However, a machine learning algorithm cannot learn anything from the movie title alone (neither can a human): it needs a context. You might decide that the ﬁve words preceding the match and the ﬁve words following it are a suﬃciently informative context. In machine learning jargon, we call such a context a “ten-word window” around a match. You can tune the width of the window as a hyperparameter.\n\nNow, your examples are labeled matches in their context. However, a learning algorithm cannot be applied to such data. Machine learning algorithms can only apply to feature vectors. This is why you resort to feature engineering.\n\n4.2 How to Engineer Features\n\nFeature engineering is a creative process where the analyst applies their imagination, intuition, and domain expertise. In our illustrative problem of movie title recognition in tweets, we used our intuition to ﬁx the width of the window around the match to ten. Now, we need to be even more creative to transform string sequences into numerical vectors.\n\n4.2.1 Feature Engineering for Text\n\nWhen it comes to text, scientists and engineers often use simple feature engineering tricks. Two such tricks are one-hot encoding and bag-of-words.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nGenerally speaking, one-hot encoding transforms a categorical attribute into several binary ones. Let’s say your dataset has an attribute “Color” with possible values “red,” “yellow,” and “green.” We transform each value into a three-dimensional binary vector, as shown below:\n\nred = [1,0,0] yellow = [0,1,0] green = [0,0,1].\n\nIn a spreadsheet, instead of one column headed with the attribute “Color,” you will use three synthetic columns, with the values 1 or 0. The advantage is you now have a vast range of machine learning algorithms at your disposal, for only a handful of learning algorithms support categorical attributes.\n\nBag-of-words is a generalization of applying the one-hot encoding technique to text data. Instead of representing one attribute as a binary vector, you use this technique to represent an entire text document as a binary vector. Let’s see how it works.\n\nImagine that you have a collection of six text documents, as shown below:\n\nLove, love is a verb\n\nLove is a doing word\n\nFeathers on my breath\n\nGentle impulsion\n\nShakes me, makes me lighter\n\nFeathers on my breath\n\nDocument 1\n\nDocument 3\n\nDocument 4\n\nDocument 5\n\nDocument 6\n\nDocument 2\n\nFigure 3: A collection of six documents.\n\nLet your problem be to build a text classiﬁer by topic. A classiﬁcation learning algorithm expects inputs to be labeled feature vectors, so you have to transform the text document collection into a feature vector collection. Bag-of-words allows you to do just that.\n\nFirst, tokenize the texts. Tokenization is a procedure of splitting a text into pieces called “tokens.” A tokenizer is software that takes a string as input, and returns a sequence of tokens extracted from that string. Typically, tokens are words, but it’s not strictly necessary. It can be a punctuation mark, a word, or, in some cases, a combination of words, such as a company (e.g., McDonald’s) or a place (e.g., Red Square). Let’s use a simple tokenizer that extracts words and ignores everything else. We obtain the following collection:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nDocument 1\n\n[Love, love, is a verb]\n\nDocument 2\n\n[Love, is, a, doing, word]\n\n[Feathers, on, my, breath]\n\n[Gentle, impulsion]\n\n[Shakes, me, makes, me lighter]\n\n[Feathers, on, my, breath]\n\nDocument 6\n\nDocument 3\n\nDocument 4\n\nDocument 5\n\nFigure 4: The collection of tokenized documents.\n\nThe next step is to build a vocabulary. It contains 16 tokens:2\n\na gentle love on\n\nbreath impulsion makes shakes\n\ndoing is me verb\n\nfeathers lighter my word\n\nNow order your vocabulary in some way and assign a unique index to each token. I ordered the tokens alphabetically:\n\n10\n\nbreathdoingfeathersgentleimpulsionislighterlovemakesmemyonshakesverbword\n\n16\n\n14\n\na\n\n3\n\n6\n\n15\n\n1\n\n7\n\n2\n\n9\n\n5\n\n4\n\n11\n\n12\n\n13\n\n8\n\nFigure 5: Ordered and indexed tokens.\n\nEach token in the vocabulary has a unique index, from 1 to 16. We transform our collection into a collection of binary feature vectors, as shown below:\n\n2I decided to ignore capitalization, but you, as an analyst, might choose to treat the two tokens “Love”\n\nand “love” as two separate vocabulary entities.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\n0\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n16\n\nDocument 2\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n15\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\nDocument 5\n\n1\n\nDocument 3\n\n1\n\nDocument 6\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nDocument 1\n\n1\n\n0\n\n0\n\n0\n\nDocument 4\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0aword...\n\n0\n\n0\n\n0\n\n0\n\n1\n\nFigure 6: Feature vectors.\n\nThe 1 is in a speciﬁc position if the corresponding token is present in the text. Otherwise, the feature at that position has a 0.\n\nFor instance, document 1 “Love, love is a verb” is represented by the following feature vector:\n\n[1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0]\n\nUse the corresponding labeled feature vectors as the training data, which any classiﬁcation learning algorithm can work with.\n\nThere are several bag-of-words “ﬂavors.” The above binary-value model often works well. Alternatives to binary values include 1) counts of tokens, 2) frequencies of tokens, or 3) TF-IDF (term frequency-inverse document frequency). If you use the counts of words, then the feature value for “love” in Document 1 “Love, love is a verb” would be 2, representing the number of times the word “love” appears in the document. If applying frequencies of tokens, the value for “love” would be 2/5 = 0.4, assuming that the tokenizer extracted two “love” tokens, and ﬁve total tokens from Document 1. The TF–IDF value increases proportionally to the frequency of a word in the document and is oﬀset by the number of documents in the corpus that contain that word. This adjusts for some words, such as prepositions and pronouns, appearing more frequently in general. I will not go into further detail on TF-IDF, but would recommend the interested reader to learn more about it online.\n\nA straightforward extension of the bag-of-words technique is bag-of-n-grams. An n-gram is a sequence of n words taken from the corpus. If n = 2, and you ignore the punctuation, then all two-grams (usually called bigrams) that can be found in the text “No, I am your father.” are as follows: [“No I,” “I am,” “am your,” “your father”]. The three-grams are [“No\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "page_number": 82,
      "chapter_number": 10,
      "summary": "Each line of that ﬁle represents a labeled feature vector using the following format:\n\nlabel index1:value1 index2:value2 Key topics include data, levels, and formats.",
      "keywords": [
        "data",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "Andriy Burkov Machine",
        "Data Versioning",
        "Burkov Machine",
        "level",
        "Learning Engineering",
        "Data Storage Levels",
        "learning",
        "Storage",
        "machine",
        "data asset",
        "data stored"
      ],
      "concepts": [
        "data",
        "levels",
        "formats",
        "store",
        "stored",
        "values",
        "machines",
        "labeled",
        "storage",
        "samples"
      ],
      "similar_chapters": [
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 174-182)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 13,
          "title": "Segment 13 (pages 119-126)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 89-108)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 4,
          "title": "Segment 4 (pages 27-34)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 91-98)",
      "start_page": 91,
      "end_page": 98,
      "detection_method": "topic_boundary",
      "content": "I am,” “I am your,” “am your father”]. By mixing all n-grams, up to a certain n, with tokens in one dictionary, we obtain a bag of n-grams that we can tokenize the same way as we deal with a bag-of-words model.\n\nBecause sequences of words are often less common than individual words, using n-grams creates a more sparse feature vector. At the same time, n-grams allow the machine learning algorithm to learn a more nuanced model. For example, the expressions “this movie was not good and boring” and “this movie was good and not boring” have opposite meaning, but would result in the same bag-of-words vectors, based solely on words. If we consider bigrams of words, then bag-of-words vectors of bigrams for those two expressions would be diﬀerent.\n\n4.2.2 Why Bag-of-Words Works\n\nFeature vectors only work when certain rules are followed. One rule is that a feature at position j in a feature vector must represent the same property in all examples in the dataset. If that feature represents the height in cm of a certain person in a dataset, where each example represents a diﬀerent person, then that must hold true in all other examples. The feature at position j must always represent the height in cm, and nothing else.\n\nThe bag-of-words technique works the same way. Each feature represents the same property of a document: whether a speciﬁc token is present or absent in a document.\n\nAnother rule is that similar feature vectors must represent similar entities in the dataset. This property is also respected when using the bag-of-words technique. Two identical documents will have identical feature vectors. Likewise, two texts regarding the same topic will have higher chances to have similar feature vectors, because they will share more words than those of two diﬀerent topics.\n\n4.2.3 Converting Categorical Features to Numbers\n\nOne-hot encoding is not the only way to convert categorical features to numbers, and it’s not always the best way.\n\nMean encoding, also known as bin counting or feature calibration, is another technique. First, the sample mean of the label is calculated using all examples where the feature has value z. Each value z of the categorical feature is then replaced by that sample mean value. The advantage of this technique is that the data dimensionality doesn’t increase, and by design, the numerical value contains some information about the label.\n\nIf you work on a binary classiﬁcation problem, in addition to sample mean, you can use other useful quantities: the raw counts of the positive class for a given value of z, the odds ratio, and the log-odds ratio. The odds ratio (OR) is usually deﬁned between two random variables. In a general sense, OR is a statistic that quantiﬁes the strength of the association between two events A and B. Two events are considered independent if the OR equals 1, that is, the odds of one event are the same in either the presence or absence of the other event.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nIn application to quantifying a categorical feature, we can calculate the odds ratio between the value z of a categorical feature (event A) and the positive label (event B). Let’s illustrate that with an example. Let our problem be to predict whether an email message is spam or not spam. Let’s assume that we have a labeled dataset of email messages, and we engineered a feature that contains the most frequent word in each email message. Let us ﬁnd the numerical value that would replace the categorical value “infected” of this feature. We ﬁrst build the contingency table for “infected” and “spam”:\n\nSpam145contains\"infected\"doesn't contain\"infected\"Not Spam83462909Total1533255Total49129173408\n\nFigure 7: Contingency table for “infected” and “spam.”\n\nThe odds-ratio of “infected” and “spam” is given by:\n\nodds ratio(infected,spam) =\n\n145/8 346/2909\n\n= 152.4.\n\nAs you can see, the odds ratio, depending on the values in the contingency table, can be extremely low (near zero) or extremely high (an arbitrarily high positive value). To avoid numerical overﬂow issues, analysts often use the log-odds ratio:\n\nlog odds ratio(infected,spam) = log(145/8) − log(346/2909)\n\n= log(145) − log(8) − log(346) + log(2909) = 2.2.\n\nNow you can replace the value “infected” in the above categorical feature with the value of 2.2. You can proceed the same way for other values of that categorical feature and convert all of them into log-odds ratio values.\n\nSometimes, categorical features are ordered, but not cyclical. Examples include school marks (from “A” to “E”) and seniority levels (“junior,” “mid-level,” “senior”). Instead of using one-hot encoding, it’s convenient to represent them with meaningful numbers. Use uniform numbers in the [0,1] range, like 1/3 for “junior”, 2/3 for “mid-level” and 1 for “senior.” If some values should be farther apart, you can reﬂect that with diﬀerent ratios. If “senior” should be farther from “mid-level” than “mid-level” from “junior,” you might use 1/5, 2/5, 1 for “junior,” “mid-level,” and “senior,” respectively. This is why domain knowledge is important.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nWhen categorical features are cyclical, integer encoding does not work well. For example, try converting Monday through Sunday to the integers 1 through 7. The diﬀerence between Sunday and Saturday is 1, while the diﬀerence between Monday and Sunday is −6. However, our reasoning suggests the same diﬀerence of 1, because Monday is just one day past Sunday.\n\nInstead, use the sine-cosine transformation. It converts a cyclical feature into two synthetic features. Let p denote the integer value of our cyclical feature. Replace the value p of the cyclical feature with the following two values:\n\npsin = sin\n\n(cid:18)2 × π × p max(p)\n\n(cid:19)\n\n,pcos = cos\n\n(cid:18)2 × π × p max(p)\n\n(cid:19)\n\n.\n\nThe table below contains the values of psin and pcos for the seven days of the week:\n\np\n\npcos psin 0.78 0.62 1 0.97 −0.22 2 3 0.43 −0.9 4 −0.43 −0.9 5 −0.97 −0.22 0.62 6 −0.78 1 7\n\n0\n\nFigure 8 contains the scatter plot built using the above table. You can see the cyclical nature of the two new features.\n\nNow, in your tidy data, replace “Monday” with two values [0.78,0.62], “Tuesday” with [0.97,−0.22], and so on. The dataset has added another dimension, but the model’s predictive quality is signiﬁcantly better, compared to integer encoding.\n\n4.2.4 Feature Hashing\n\nFeature hashing, or hashing trick, converts text data, or categorical attributes with many values, into a feature vector of arbitrary dimensionality. One-hot encoding and bag-of-words have a drawback: many unique values will create high-dimensional feature vectors. For example, if there are one million unique tokens in a collection of text documents, bag-of-words will produce feature vectors that each have a dimensionality of one million. Working with such high-dimensional data might be very computationally expensive.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\nFigure 8: The sine-cosine transformed feature that represents the days of the week.\n\nValue 2\n\nHash function\n\nFeature 0\n\nValue K\n\nValue K-1\n\nValue 3...\n\nOriginal valuesFeatures\n\nFeature 5\n\nFeature 4\n\nFeature 3\n\nFeature 1\n\nFeature 0\n\nValue 1\n\nFigure 9: An illustration of the hashing trick for the desired dimensionality of 5 for the original cardinality K of values of an attribute.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\nTo keep your data manageable, you can use the hashing trick that works as follows. First you decide on the desired dimensionality of your feature vectors. Then, using a hash function, you ﬁrst convert all values of your categorical attribute (or all tokens in your collection of documents) into a number, and then you convert this number into an index of your feature vector. The process is illustrated in Figure 9.\n\nLet’s illustrate how it would work for converting a text “Love is a doing word” into a feature vector. Let us have a hash function h that takes a string as input and outputs a non-negative integer, and let the desired dimensionality be 5. By applying the hash function to each word and applying the modulo of 5 to obtain the index of the word, we get:\n\nh(love) mod 5 = 0\n\nh(is) mod 5 = 3\n\nh(a) mod 5 = 1\n\nh(doing) mod 5 = 3 h(word) mod 5 = 4.\n\nThen we build the feature vector as,\n\n[1,1,0,2,1].\n\nIndeed, h(love) mod 5 = 0 means that we have one word in dimension 0 of the feature vector; h(is) mod 5 = 3 and h(doing) mod 5 = 3 means that we have two words in dimension 3 of the feature vector, and so on. As you can see, there is a collision between words “is” and “doing”: they both are represented by dimension 3. The lower the desired dimensionality, the higher are the chances of collision. This is the trade-oﬀ between speed and quality of learning.\n\nCommonly used hash functions are MurmurHash3, Jenkins, CityHash, and MD5.\n\n4.2.5 Topic Modeling\n\nTopic modeling is a family of techniques that uses unlabeled data, typically in the form of natural language text documents. The model learns to represent a document as a vector of topics. For example, in a collection of news articles, the ﬁve major topics could be “sports,” “politics,” “entertainment,” “ﬁnance,” and “technology”. Then, each document could be represented as a ﬁve-dimensional feature vector, one dimension per topic:\n\n[0.04,0.5,0.1,0.3,0.06]\n\nThe above feature vector represents a document that mixes two major topics: politics (with a weight of 0.5) and ﬁnance (with a weight of 0.3). Topic modeling algorithms, such as Latent\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n1\n\n2\n\n3\n\n4\n\n5\n\nSemantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), learn by analyzing the unlabeled documents. These two algorithms produce similar outputs, but are based on diﬀerent mathematical models. LSA uses singular value decomposition (SVD) of the word-to-document matrix (constructed using a binary bag-of-words or TF-IDF). LDA uses a hierarchical Bayesian model, in which each document is a mixture of several topics, and each word’s presence is attributable to one of the topics.\n\nLet us illustrate how it works in Python and R. Below is a Python code for LSA:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD\n\nclass LSA():\n\ndef __init__(self, docs):\n\n# Convert documents to TF-IDF vectors self.TF_IDF = TfidfVectorizer() self.TF_IDF.fit(docs) vectors = self.TF_IDF.transform(docs)\n\n# Build the LSA topic model self.LSA_model = TruncatedSVD(n_components=50) self.LSA_model.fit(vectors) return\n\ndef get_features(self, new_docs):\n\n# Get topic-based features for new documents new_vectors = self.TF_IDF.transform(new_docs) return self.LSA_model.transform(new_vectors)\n\n# Later, in production, instantiate LSA model docs = [\"This is a text.\", \"This another one.\"] LSA_featurizer = LSA(docs)\n\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LSA_features = LSA_featurizer.get_features(new_docs)\n\nThe corresponding code3 in R is shown below:\n\nlibrary(tm) library(lsa)\n\nget_features <- function(LSA_model, new_docs){\n\n# new_docs can be passed as a tm::Corpus object or as a vector\n\n3The R code for LSA and LDA is courtesy of Julian Amon.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n# holding character strings representing documents: if(!inherits(new_docs, \"Corpus\")) new_docs <- VCorpus(VectorSource(new_docs)) tdm_test <- TermDocumentMatrix(\n\nnew_docs, control = list(\n\ndictionary = rownames(LSA_model$tk), weighting = weightTfIdf\n\n)\n\n) txt_mat <- as.textmatrix(as.matrix(tdm_test)) crossprod(t(crossprod(txt_mat, LSA_model$tk)), diag(1/LSA_model$sk))\n\n}\n\n# Train LSA model using docs docs <- c(\"This is a text.\", \"This another one.\") corpus <- VCorpus(VectorSource(docs)) tdm_train <- TermDocumentMatrix( corpus, control = list(weighting = weightTfIdf)) txt_mat <- as.textmatrix(as.matrix(tdm_train)) LSA_fit <- lsa(txt_mat, dims = 2)\n\n# Later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LSA_features <- get_features(LSA_fit, new_docs)\n\nBelow is a Python code for LDA:\n\nfrom sklearn.feature_extraction.text import CountVectorizer from sklearn.decomposition import LatentDirichletAllocation\n\nclass LDA():\n\ndef __init__(self, docs):\n\n# Convert documents to TF-IDF vectors self.TF = CountVectorizer() self.TF.fit(docs) vectors = self.TF.transform(docs) # Build the LDA topic model self.LDA_model = LatentDirichletAllocation(n_components=50) self.LDA_model.fit(vectors) return\n\ndef get_features(self, new_docs):\n\n# Get topic-based features for new documents new_vectors = self.TF.transform(new_docs) return self.LDA_model.transform(new_vectors)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n# Later, in production, instantiate LDA model docs = [\"This is a text.\", \"This another one.\"] LDA_featurizer = LDA(docs)\n\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LDA_features = LDA_featurizer.get_features(new_docs)\n\nAnd here is the corresponding code in R:\n\nlibrary(tm) library(topicmodels)\n\n# Generate feature for new_docs by using LDA_model get_features <- function(LDA_mode, new_docs){\n\n# new_docs can be passed as tm::Corpus object or as a vector # holding character strings representing documents: if(!inherits(new_docs, \"Corpus\")) new_docs <- VCorpus(VectorSource(new_docs)) new_dtm <- DocumentTermMatrix(new_docs, control = list(weighting = weightTf)) posterior(LDA_mode, newdata = new_dtm)$topics\n\n}\n\n# train LDA model using docs docs <- c(\"This is a text.\", \"This another one.\") corpus <- VCorpus(VectorSource(docs)) dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTf)) LDA_fit <- LDA(dtm, k = 5)\n\n# later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LDA_features <- get_features(LDA_fit, new_docs)\n\nIn the above listings, docs is a collection of text documents. It can, for example, be a list of strings, where each string is a document.\n\n4.2.6 Features for Time-Series\n\nTime-series data is diﬀerent from the traditional supervised learning data, which has a form of unordered collections of independent observations. A time series is an ordered sequence of observations, and each is marked with a time-related attribute, such as timestamp, date, month-year, year, and so on. An example of a time-series data is given in Figure 10.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "page_number": 91,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 91-98). Key topics include words, feature, and values. Feature engineering is a process of ﬁrst conceptually and then programmatically transforming a raw example into a feature vector.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "Andriy Burkov Machine",
        "Feature",
        "Feature Engineering",
        "Learning Engineering",
        "document",
        "Burkov Machine",
        "feature vectors",
        "learning",
        "Features Feature engineering",
        "machine",
        "machine learning algorithm",
        "Engineering"
      ],
      "concepts": [
        "words",
        "feature",
        "values",
        "tokenization",
        "document",
        "documents",
        "examples",
        "vector",
        "learning",
        "let"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 99-107)",
      "start_page": 99,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": "Stock Price...14.715.916.8DateS&P 500...3,3523,3473,52117.93,2982020-01-112020-01-122020-01-122016-01-132020-01-13Dow Jones...28,61129,00128,12728,31216.83,54017.93,6872016-01-152020-01-1427,99828,564......2016-01-16...14.53,3452020-01-1228,583\n\nFigure 10: An example of time-series data in the form of an event stream.\n\nStock Price...DateS&P 500...17.43,4102020-01-112020-01-13Dow Jones...28,22016.83,54017.93,6872016-01-152020-01-1427,99828,564......2016-01-16...15.03,3482020-01-1228,732\n\nFigure 11: Classical time series obtained by aggregating the event stream from Figure 10.\n\nIn Figure 10, each row corresponds to the cost of a certain stock at a moment in time, as well as the values of two indices: S&P 500 and Dow Jones. The observations were made irregularly: on 2020-01-12, three observations were made. On 2020-01-13, there were two observations. In the classical time-series data, observations are evenly spaced over time,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\n17.43,41028,22017.93,68728,56417.43,41028,22015.03,34828,73216.83,54017.93,68727,99828,564𝑖+1example𝑖example𝑖+2example𝑡−1𝑡−2𝑡−1𝑡−2𝑡−1𝑡−2\n\nFigure 12: Time-series chunked into segments of length w = 2.\n\nsuch as one observation per second, per minute, per day, and so on. If observations are irregular, such time-series data is called a point process or an event stream.\n\nIt’s usually possible to convert an event stream into the classical time-series data by aggregating observations. Examples of aggregation operators are COUNT and AVERAGE. By applying the AVERAGE operator to the event stream data in Figure 10, we obtain the classical time-series data shown in Figure 11.\n\nWhile it’s possible to directly work with event streams, bringing time series to the classical form makes it simpler to apply further aggregations and generate features for machine learning.\n\nAnalysts typically use time-series data to solve two kinds of prediction problems. Given a sequence of recent observations:\n\npredict something about the next observation (for example, given the stock price and the value of stock indices for the last seven days, predict the stock price for tomorrow), or • predict something about the phenomenon that generated that sequence (for example, given a user’s connection log to a software system, predict whether they are likely to cancel their subscription during the current quarter).\n\nBefore neural networks reached their modern learning capacity, analysts worked with time- series data using the shallow machine learning toolkit. To transform a time-series into training data in the form of feature vectors, two decisions must be made:\n\nhow many of the consecutive observations are needed to make an accurate prediction (so-called prediction window), and\n\nhow to convert a sequence of observations into a ﬁxed-dimensionality feature vector.\n\nThere’s no simple way to answer either question. Usually decisions are made based on the subject-matter expert’s knowledge, or by using a hyperparameter tuning technique. However, some recipes work for many time-series data. Below is one such recipe:\n\n1) chunk the entire time series into segments of length w,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\n2) create a training example e from each segment s, 3) for each e, calculate various statistics on the observations in s.\n\nWe take Figure 11’s data and chunk it into segments of length w = 2, where w the length of the prediction window. Figure 12 shows that each segment is now a separate example.\n\nIn practice, w is usually larger than 2. Let’s say our prediction window has a length of seven. The statistics calculated at step (3) of the above recipe could be:\n\naverage (e.g., the mean or median of the stock price during the last seven days), • spread (e.g., standard deviation, median absolute deviation, or interquartile range of the values of the S&P 500 index during the last seven days),\n\noutliers (e.g., the fraction of observations, in which the values of the Dow Jones index was atypically low; for example, more than two standard deviations from the mean), • growth (e.g., whether the values of the S&P 500 index have grown between the day t − 6 and t, days t − 3 and t, and between t − 1 and t.\n\nvisual (e.g., how diﬀerent the curve of the stock price values is from a known visual image, such as a hat, or head and shoulders).\n\nNow you see why converting a time series into a classical form is recommended: the above statistics are only meaningful when calculated on the comparable values.\n\nIt should be noted that in the modern neural-network era, analysts most often prefer to train deep neural networks. Long short-term memory (LSTM), convolutional neural network (CNN), and Transformer are popular choices of architecture for a time-series model. These can read arbitrary length time-series as input, and generate a prediction based on the entire sequence. Similarly, neural networks are often applied to texts by reading them word-by-word, or character-by-character. Words and characters are usually represented as embedding vectors; the latter are learned from large corpora of text documents. We will talk about embeddings in Section 4.7.1.\n\n4.2.7 Use Your Creativity\n\nAs I mentioned at the beginning of this section, feature engineering is a creative process. As an analyst, you are in the best position to determine what are good features for your prediction model. Put yourself “in the shoes” of a learning algorithm and imagine what you would look at in your data to decide which label to assign.\n\nSay you are classifying emails as important or unimportant. You might notice that a signiﬁcant number of important messages come from the government revenue agency on the ﬁrst Monday of each month. Create a feature “government ﬁrst monday.” Let it equal 1 when the email came from the government revenue agency on the ﬁrst Monday of a month, and 0 otherwise. Alternatively, you might notice that an email with more than one smiley is rarely important. Create a feature “contains smileys.” Let it equal 1 when an email contains more than one smiley, and 0 otherwise.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\n4.3 Stacking Features\n\nBack to our problem of movie title classiﬁcation in tweets. Each example has three parts:\n\n1) ﬁve words4 that precede the extracted potential movie title (the left context), 2) the extracted potential movie title (the extraction), 3) ﬁve words that follow the extracted movie title (the right context).\n\nTo represent such multi-part examples, we ﬁrst transform each part into a feature vector, and then stack the three feature vectors next to one another to obtain the feature vector for the entire example.\n\n4.3.1 Stacking Feature Vectors\n\nIn our movie title classiﬁcation problem, we ﬁrst collect all the left contexts. We then apply bag-of-words to transform each left context into a binary feature vector. Next, collect all extractions and, using bag-of-words, transform each extraction into a binary feature vector. Then we collect all the right contexts and apply bag-of-words to transform each right context into a binary feature vector. Finally, we concatenate each example, joining the feature vectors of the left context, the extraction, and the right context. We obtain the ﬁnal feature vector that represents the entire example, as shown in Figure 13.\n\nNote that the three feature vectors (one from each part of the example) are created indepen- dently of one another. This means that the vocabulary of tokens is diﬀerent for each part and, therefore, the feature vector dimensionality of each part may also be diﬀerent.\n\nThe order in which you concatenate feature vectors doesn’t matter. The left context features can be placed in the middle or right side of the ﬁnal feature vector. However, you must keep the same concatenation order in all examples. This ensures each feature represents the same property from one example to another.\n\n4.3.2 Stacking Individual Features\n\nUntil now, we engineered features in bulk. One-hot encoding and bag-of-words often generate thousands of features. This is a very time-eﬃcient way of engineering features, but some problems require more to obtain feature vectors with high enough predictive power. We consider the predictive power of a feature in the next section.\n\nImagine that you already have a classiﬁer mA that takes an entire tweet as input and predicts its topic. Let one of the topics be cinema. You might want to enrich the feature vectors in your movie title classiﬁcation problem with this additional information available from the classiﬁer mA. In this case, you will engineer one feature that can be described as “whether\n\n4In practice, the context to the left or the right of the potential movie title can for some examples be\n\nshorter than ﬁve words, because it’s either the beginning or the end of the tweet.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19\n\n010avatar is sarah connor from0...0\n\n0001...1\n\n0100...0\n\nbag ofwordsconcatenationconcatenation\n\n000the terminator1...1\n\nfeature vector ofthe extraction\n\nfeature vector ofthe left context\n\nfeature vector ofthe entire example\n\nbag ofwords\n\n001it is her favorite movie0...1\n\n0010...1\n\nbag ofwords\n\nfeature vector ofthe right context\n\nFigure 13: Creating and stacking feature vectors.\n\nthe topic of the tweet is cinema” and that feature will also be binary: 1 if the topic predicted by mA for the entire tweet is cinema, and 0 otherwise. Again, we concatenate the three partial feature vectors, as shown in Figure 14.\n\nYou might come up with many more useful features for title classiﬁcation in tweets. Examples of such features are:\n\nthe average IMDB score of the movie, • the number of votes for the movie on IMDB, • the Rotten Tomato score of the movie, • whether the movie is recent (or the number that represents the release year), • whether the tweet text contains other movie titles, and • whether the tweet text includes the names of actors or directors.\n\nAll these additional features, as long as they are numerical, can be concatenated to the feature vector. The only condition is that they are concatenated in the same order in all examples.\n\n4.4 Properties of Good Features\n\nNot all features are created equal. In this section, we consider the properties of a good feature.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\n0\n\nB-o-w 1B-o-w 2B-o-w MIs cinema?Example 1\n\n1\n\n0\n\n0\n\n...\n\n0\n\n0\n\n0\n\n0\n\n1\n\n...\n\n1\n\n0\n\n0\n\n1\n\n...\n\n0\n\n0\n\n...\n\n...\n\n...\n\n1\n\n...\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n...\n\n0Example 2\n\n...\n\n1\n\n0\n\n1\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n1\n\n...\n\n...\n\n...\n\n...\n\n0\n\n0\n\n1\n\nB-o-w M\n\n...\n\n...\n\n–1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n...\n\n0\n\n...\n\n0\n\n...\n\n1Example N\n\n1\n\n0\n\n1\n\n...\n\n1\n\nFigure 14: Single feature stacking.\n\n4.4.1 High Predictive Power\n\nFirst of all, a good feature has high predictive power. In Chapter 3, you read about predictive power as a property of data. However, a feature can also have high or low predictive power. Let’s say you want to predict whether a patient has cancer. Among other features, you know the make of the person’s car and whether the person is married. These two features are not good predictors for cancer, so our machine learning algorithm will not learn a meaningful relationship between these features and the label. Predictive power is a property of the feature with respect to the problem. The make of the person’s car and whether the person is married could have high predictive power if the problem were diﬀerent.\n\n4.4.2 Fast Computability\n\nGood features can be computed fast. Let’s say you want to predict the topic of a tweet. A tweet is short, and a bag-of-words-based feature vector will be sparse. A sparse vector is a vector whose values in most dimensions are zero. If your dataset is small and the texts are short, the learning algorithm will have a hard time seeing patterns in sparse vectors because they contain little information compared to their size. The information in one sparse vector is rarely contained in the same dimensions as the information in another sparse vector, even if they represent similar concepts.\n\nTo reduce sparsity, you might want to augment your sparse feature vectors with additional non-zero values. To do that, you might send the tweet text to Wikipedia as a search query, and then extract other words from the search results. Wikipedia’s API doesn’t give any guarantee for the speed of response, so it could take several seconds to get a response. For real-time systems, feature extraction must be fast: a less informative feature computed in a fraction of a millisecond is often preferred to a feature with a high predictive power that takes seconds to compute. If your application must be fast, the features obtained from Wikipedia might not be appropriate for your task.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\n4.4.3 Reliability\n\nA good feature must also be reliable. Again, in our Wikipedia example, we cannot have a guarantee that the website will respond at all: it can be down, on planned maintenance, or the API may be temporarily overused and is rejecting requests. Therefore, we cannot trust that Wikipedia-based features will always be available and complete. Thus, we cannot call such features reliable. One unreliable feature can reduce the quality of predictions made by your model. Furthermore, some predictions can become entirely wrong if the value of an important feature is missing.\n\n4.4.4 Uncorrelatedness\n\nCorrelation of two features means their values are related. If the growth of one feature implies the growth of the other, and the inverse is also true, then the two features are correlated.\n\nOnce the model is in production, its performance may change because the input data’s properties may change over time. When many of your features are highly correlated, even a minor change in the input data’s properties may result in signiﬁcant changes in the model’s behavior.\n\nSometimes the model was built under strict time constraints, so the developer used all possible sources of features. With time, maintaining those sources can become costly. It’s generally recommended to eliminate redundant or highly correlated features. Feature selection techniques help reduce such features.\n\n4.4.5 Other Properties\n\nAn essential property of a good feature is that the distribution of its values in the training set is similar to the distribution it will receive in production. For example, a tweet’s date might be necessary for some predictions about it. However, if you apply the model built on historical tweets to predict something about current tweets, the date of your production examples will always be out of the training distribution, which can result in a signiﬁcant error.5\n\nFinally, features that you design should be unitary, easy to understand, and maintain. Unitary means the feature represents a certain simple-to-understand and -explain quantity. For example, when classifying a car’s type given its characteristics, you may use such unitary features as weight, length, width, and color. A feature like “length divided by weight” is not unitary, as it’s composed of two unitary features.\n\n5The date information often is relevant for machine learning and can still be included in the training data. For instance, you could consider engineering cyclical features like “hour of the day,” “day of the week,” “month of the year.” For the prediction problems in which time seasonality has predictive power, having such features can be useful.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\nSome learning algorithms may beneﬁt from combining features. However, it’s preferable to do this in a dedicated stage in the model training pipeline. We will consider feature combination and generation of synthetic features later in this chapter.\n\n4.5 Feature Selection\n\nNot all features will be equally important for your problem. For instance, in the problem of detecting movies in tweets, the length of the movie might not be a very important feature. At the same time, when you use bag-of-words, the vocabulary can be very large, while most tokens will appear in the collection of texts only once. If the learning algorithm “sees” that some feature has a non-zero value only in a couple of training examples, it is doubtful the algorithm will learn any useful pattern from that feature. However, if the feature vector is very wide (contains thousands or millions of features), the training time can become prohibitively long. Furthermore, the overall size of the training data can become too large to ﬁt in the RAM of a conventional server.\n\nIf we could estimate the importance of features, we would keep only the most important ones. That would allow us to save time, ﬁt more examples in memory, and improve the model’s quality. Below, we consider some feature selection techniques.\n\n4.5.1 Cutting the Long Tail\n\nTypically, if a feature contains information (e.g., a non-zero value) only for a handful of examples, such a feature could be removed from the feature vector. In bag-of-words, you can build a graph with the distribution of token counts, and then cut oﬀ the so-called long tail, as shown in Figure 15.\n\nA long tail of a distribution is such a part of that distribution that contains elements with substantially lower counts compared to a smaller group of elements with the highest counts. This smaller group is called the head of the distribution, and their aggregated counts make for at least half of all the counts.\n\nThe decision on a threshold for deﬁning the long tail is somewhat subjective. You can set it as a hyperparameter for your problem and discover the optimal value experimentally. On the other hand, the decision can be made by looking at the distribution of counts, as shown in Figure 15a. As you can see, I cut oﬀ the long tail at a point where the distribution of the elements in the tail has become visually ﬂat (Figure 15b).\n\nWhether to cut the long tail, and where to do it, is debatable. In classiﬁcation problems with many classes, the diﬀerence between some classes can be very subtle. Even features whose values are rarely non-zero may become important. However, removing long-tail features often results in faster learning and a better model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\n(a) distribution of word counts in English\n\n(b) the long tail\n\nFigure 15: The distribution of word counts in a collection of texts in English (a) and the long tail (b, zone in blue). The highest count corresponds to “the” (a count of 615); the lowest count corresponds to “zambia” (a count of 1).\n\n4.5.2 Boruta\n\nCutting the long tail is not the only way to select important features and remove less important ones. One popular tool used in Kaggle competitions is Boruta. Boruta iteratively trains random forest models and runs statistical tests to identify features as important and unimportant. The tool exists both in the form of an R package and a Python module.\n\nBoruta works as a wrapper around the random forest learning algorithm, hence its name — Boruta is a spirit of the forests in Slavic mythology. To understand the Boruta algorithm, let’s ﬁrst recall how the random forest learning algorithm works.\n\nRandom forest is based on the idea of bagging. It makes many random samples of the training set and then trains a diﬀerent statistical model on each sample. The prediction is then made by taking the majority vote (for classiﬁcation) or an average (for regression) of all models. The only substantial diﬀerence of random forest from the vanilla bagging algorithm is that in the former, the trained statistical models are decision trees. At each split of the decision tree, a random subset of all features is considered.\n\nOne useful feature of the random forest is its built-in capability to estimate the importance of each feature. Below, I will explain how this estimation works for the case of classiﬁcation.\n\nThe algorithm works in two stages. First, it classiﬁes all training examples from the original training set. Each decision tree in the random forest model votes only on the classiﬁcation of examples that weren’t used to build that tree. After a tree is tested, the number of correct predictions is recorded for that tree.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "page_number": 99,
      "chapter_number": 12,
      "summary": "Andriy Burkov\n\nMachine Learning Engineering - Draft\n\n10 Figure 8: The sine-cosine transformed feature that represents the days of the week Key topics include features, value, and data.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Andriy Burkov Machine",
        "Machine Learning",
        "docs",
        "feature",
        "Burkov Machine",
        "feature vector",
        "Learning Engineering",
        "LSA",
        "LDA",
        "Andriy Burkov",
        "Learning",
        "data",
        "vectors"
      ],
      "concepts": [
        "features",
        "value",
        "data",
        "learning",
        "docs",
        "prediction",
        "predict",
        "modeling",
        "topic",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 20,
          "title": "Segment 20 (pages 168-178)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 10,
          "title": "Segment 10 (pages 81-92)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 35,
          "title": "Segment 35 (pages 297-305)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 108-115)",
      "start_page": 108,
      "end_page": 115,
      "detection_method": "topic_boundary",
      "content": "At the second stage, the values of a certain feature are randomly permuted across examples, and the tests are repeated. The number of correct predictions is once again recorded for each tree. The importance of the feature for a single tree is then computed as the diﬀerence between the number of correct classiﬁcations between the original and permuted setting, divided by the number of examples. To obtain the feature importance score, the feature importance measures for individual trees are averaged. While not strictly necessary, it’s convenient to use z-scores instead of the raw importance scores.\n\nTo obtain a z-score for a feature, we ﬁrst ﬁnd the average value and the standard deviation of individual feature scores for individual trees. The feature’s z-score is obtained by subtracting the average value from the score, and then dividing it by the standard deviation.\n\nYou might stop here and use the z-scores of each feature as the criterion to keep it (the higher, the better). However, in practice, the importance score alone often doesn’t reﬂect meaningful correlations between features and the target. Therefore, we need a diﬀerent tool to distinguish the truly important features from the non-important ones, and, as you could guess, Boruta provides that tool.\n\nThe underlying idea of Boruta is simple: we ﬁrst extend the list of features by adding a randomized copy of each original feature, and then build a classiﬁer based on this extended dataset. To assess the importance of an original feature, we compare it to all randomized features. Only features for which the importance is higher than that of the randomized features — and statistically signiﬁcant — are considered truly important.\n\nBelow, I outline the main steps of the Boruta algorithm in the way it was described by its authors6 with adaptations for consistency and clarity:\n\nThe Boruta Algorithm\n\nBuild extended training feature vectors, where each original feature is replicated. Randomly permute the values of the replicated features across the training examples to remove any correlation between the replicated variables and the target.\n\nPerform several random forest learning runs. The replicated features are random- ized before each run by applying the same random feature value permutation process as in the previous step.\n\nFor each run, compute the importance (z-score) of all original and replicated features.\n\n– A feature is deemed important for a single run if its importance is higher than\n\nthe maximal importance among all replicated features.\n\nPerform a statistical test for all original features.\n\n6Miron B. Kursa, Aleksander Jankowski, Witold R. Rudnicki, “Boruta - A System for Feature Selection,”\n\npublished in Fundamenta Informaticae 101 in 2010, pages 271–285.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25\n\n– The null hypothesis is that the feature’s importance is equal to the maximal\n\nimportance of the replicated features (MIRA).\n\n– The statistical test is a two-sided equality test - the hypothesis may be rejected either when the importance of the feature is signiﬁcantly higher or signiﬁcantly lower than MIRA.\n\n– For each original feature, we count and record the number of hits.\n\n– The number of hits for a feature is the number of runs in which the importance\n\nof that feature was higher than MIRA.\n\n– The expected number of hits for R runs is E(R) = 0.5R with standard\n\n√\n\ndeviation S =\n\n0.25R (binomial distribution with p = q = 0.5).\n\n– An original feature is deemed important (accepted) when the number of hits is signiﬁcantly higher than the expected number of hits and is deemed unimportant (rejected) when the number of hits is signiﬁcantly lower than the expected. (It is possible to compute limits for accepting and rejecting feature for any number of runs for the desired conﬁdence level.)\n\nRemove the features which are deemed unimportant from the feature vectors (both original and replicated).\n\nPerform the same procedure for a predeﬁned number of iterations, or until all features are either rejected or conclusively deemed important, whichever comes ﬁrst.\n\nBoruta worked well for many Kaggle competitions; therefore, you can consider it a universally applicable tool for feature selection. One thing worth noting, though, before using Boruta in production: Boruta is a heuristic. There are no theoretical guarantees for its performance. If you want to be sure that Boruta doesn’t harm, run it multiple times and make sure that the feature selection is stable (i.e., consistent across multiple Boruta applications to your data). If the feature selection is not stable, make sure that the number of trees in the random forest is large enough to generate stable results.\n\nThough Boruta is an eﬀective method of feature selection, it’s not the only one used by practitioners. You will ﬁnd the description of several other methods in the book’s companion wiki in an extended version of this chapter.\n\n4.5.3 L1-Regularization\n\nRegularization is an umbrella term for a range of techniques that improve the general- ization of the model. Generalization, in turn, is the model’s ability to correctly predict the label for unseen examples.\n\nWhile regularization doesn’t let you identify important features, some regularization tech- niques, such as L1, allow the machine learning algorithm to learn to ignore some features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26\n\nDepending on the kind of model you train, L1 may apply diﬀerently, but the main principle remains the same: L1 penalizes the model for being too complex.\n\nIn practice, L1 regularization produces a sparse model, which is a model that has most of its parameters equal to zero. Therefore, L1 implicitly performs feature selection by deciding which features are essential for prediction, and which ones are not. We will talk about regularization in more detail in the next chapter.\n\n4.5.4 Task-Speciﬁc Feature Selection\n\nFeature selection can also be task-speciﬁc. For example, we can remove some features from bag-of-words vectors representing natural language texts by excluding the dimensions corresponding to stop words. Stop words are the words that are too generic or common for the problem we are trying to solve. Frequent examples of stop words are articles, prepositions, and pronouns. Dictionaries of stop words for most languages are available online.\n\nTo further reduce the feature vector dimensionality obtained from the text data, sometimes it’s practical to preprocess the text by replacing infrequent words (e.g., those whose count in the corpus is below three) with the same synthetic token, for example RARE_WORD.\n\n4.6 Synthesizing Features\n\nThe learning algorithms implemented in the most popular machine learning package for Python, scikit-learn, only work with numerical features. But it can still be useful to convert numerical features into categorical ones.\n\n4.6.1 Feature Discretization\n\nThe reasons to discretize a real-valued numerical feature can be numerous. For example, some feature selection techniques only apply to categorical features. A successful discretization adds useful information to the learning algorithm when the training dataset is relatively small. Numerous studies show that discretization can lead to improved predictive accuracy. It is also simpler for a human to interpret a model’s prediction if it is based on discrete groups of values, such as age groups or salary ranges.\n\nBinning, also known as bucketing, is a popular technique that allows transforming a numerical feature into a categorical one by replacing numerical values in a speciﬁc range by a constant categorical value.\n\nThere are three typical approaches to binning:\n\nuniform binning, • k-means-based binning, and • quantile-based binning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27\n\nuniform: bins of the same width\n\nk-means: each bin is a clusterquantile: each bin contains 4 examplesbin 1bin 2bin 3bin 1bin 2bin 3bin 1bin 2bin 3\n\nFigure 16: Three binning approaches: uniform, k-means-based, and quantile-based.\n\nIn all three cases, you should decide how many bins you want to have. Consider an illustration in Figure 16. Here, we have a numerical feature j and 12 values of this feature, one for each of the 12 examples in our dataset. Let’s say we decided to have three bins. In uniform binning, all bins for a feature have identical widths, as illustrated in Figure 16 on the top.\n\nIn k-means-based binning, values in each bin belong to the nearest one-dimensional k-means cluster, as shown in Figure 16 in the middle.\n\nIn quantile-based binning, all bins have the same number of examples, as shown in Figure 16 at the bottom.\n\nIn uniform binning, once the model is deployed in production, if the value of the feature in the input feature vector is below or above the range of any bin, then the closest bin is assigned, which is either the leftmost or the rightmost bin.\n\nRemember, most modern machine learning algorithm implementations require numerical features. The bins must be transformed back to numerical values by using a technique like one-hot encoding.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28\n\nGenderMFFFUser ID12435Age18252821.........Date Subscribed2016-01-122017-08-232019-12-192016-11-30......UserUser ID2422Order ID1234Amount23187.58.3.........Order Date2017-09-132018-11-232019-12-192016-11-30......OrderUser ID4234Call ID1235Call Duration5523547614.........Call Date2016-01-122016-01-132016-12-172016-11-30......CallM19...2019-12-18443342019-12-19...\n\nFigure 17: Relational data for churn analysis.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29\n\nGenderFUser ID24Age25User featuresM19Mean OrderAmount12.918Std DevOrder Amount7.10Mean CallDuration235134.3Std Dev CallDuration0142.7\n\nFigure 18: Synthetic features based on sample mean and standard deviation.\n\n4.6.2 Synthesizing Features from Relational Data\n\nData analysts often work with data in a relational database. For example, a mobile phone operator wants to know whether a customer will soon abandon the subscription. This problem is known as churn analysis. We have to represent each customer as a vector of features.\n\nLet’s say the data on the users is contained in three relational tables: User, Order, and Call, as shown in Figure 17.\n\nThe table User already contains two potentially useful features: Gender and Age. We can also create synthetic features using the data from tables Order and Call. As you can see, user 2 has three rows in table Order, while user 4 has one row in table Order, but three rows in table Calls. In order to create a feature that represents one user, we have to reduce those several records into one value. A typical approach is to compute various statistics from the data coming from multiple rows and use the value of each statistic as a feature. The most commonly used statistics are sample mean and standard deviation. (Standard deviation is the square root of sample variance.)\n\nTo give a concrete example, I have calculated the values of four features for users 2 and 4. You can ﬁnd them in Figure 18.\n\nSometimes, a relational database can have a deeper structure. For example, a user can have orders, while each order can have ordered items. In such a case, we can compute a statistic of a statistic. For example, one feature can be created by ﬁrst calculating the standard deviation of item prices in each order, and then by taking the average of those standard deviations for a speciﬁc user. You can combine the statistics in arbitrary ways: the mean of the mean, the standard deviation of the mean, the standard deviation of the standard deviation, and so on. The same principle applies to the database whose table structure is deeper than two levels.\n\nOnce you have generated features based on all possible combinations of statistics, you can select the most useful ones by using one of the feature selection methods.\n\nIf you want to increase the predictive power of your feature vectors, or when your training set is rather small, you can synthesize additional features that would help in predictions. There are two typical ways to synthesize additional features: from the data, or from other features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30\n\n4.6.3 Synthesizing Features from the Data\n\nOne technique commonly used to synthesize one or more additional features is clustering. Let us use the k-means clustering. Choose a value for k. If your ultimate goal is to build a classiﬁcation model, a common way to assign a value for k is to use the number C of classes. In regression, use your intuition or apply any technique allowing to determine the right value of clusters in your data, such as prediction strength or the elbow method. Apply k-means clustering to the feature vectors in your training data. Then add k additional features to your feature vectors. The additional feature D + j, where j = 1,...,k, will be binary and equal to 1 if the corresponding feature vector belongs to cluster j.\n\nYou can synthesize even more features by applying diﬀerent clustering algorithms, or by restarting k-means multiple times from randomly chosen starting points.\n\n4.6.4 Synthesizing Features from Other Features\n\nNeural networks are notorious for their ability to learn complex features by combining simple features in unordinary ways. They combine simple features by letting their values undergo several levels of nested nonlinear transformations. If you have data in abundance, you can train a deep multilayer perceptron model that will learn to cleverly combine the basic unitary features it receives as input.\n\nIf you don’t have an inﬁnite supply of training examples (often the case in practice), very deep neural networks lose their appeal.7 In the case of smaller to moderately large datasets (the number of training examples varies between a thousand and a hundred thousand), you might prefer to use a shallow learning algorithm and “help” your learning algorithm learn by providing a richer set of features.\n\nIn practice, the most common way to obtain new features from the existing features is to apply a simple transformation to one or a pair of existing features. Three typical simple transformations that apply to a numerical feature j in example i are 1) discretization of the feature, 2) squaring the feature, and 3) computing the sample mean and the standard deviation of feature j from k-nearest neighbors of the example i found by using some metric like Euclidean distance or cosine similarity.\n\nTransformations that apply to a pair of numerical features are simple arithmetic operators: +, −, ×, and ÷ (a technique also known as feature-crossing). For example, you can obtain the value of a new feature q in example i, where q > D, by combining the values of features 2 and 6 in the following way: x(q) . I selected features 2 and 6, as well as the i transformation ÷ arbitrarily. If the number D of original features is not too large, you can generate all possible transformations (by considering all pairs of features and all arithmetic operators). Then, by using one of the feature selection methods, select those that increase the quality of the model.\n\ndef= x(2)\n\ni ÷ x(6)\n\ni\n\n7Unless you use deep pre-trained models in transfer learning, as we will discuss in the next chapter.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31\n\n4.7 Learning Features from Data\n\nSometimes, useful features can be learned from data. Learning features from data is especially eﬀective when we can get access to large collections of relevant labeled or unlabeled data, such as text corpora or collections of images from the Web.\n\n4.7.1 Word Embeddings\n\nIn Chapter 3, we used word embeddings for data augmentation. Word embeddings are feature vectors that represent words. Similar words have similar feature vectors, where similarity is given by a certain measure, such as cosine similarity. Word embeddings are learned from large corpora of text documents. A shallow neural network with one hidden layer (called the embedding layer) is trained to predict a word, given its surrounding words, or to predict the surrounding words, given the word in the middle. Once the neural network is trained, the parameters of the embedding layer are used as word embeddings. There are many algorithms to learn word embeddings from data. The most widely used algorithm, invented at Google, with the code available in open source, is word2vec. Pre-trained word2vec embeddings for many languages are available for download.\n\nOnce you have a collection of word embeddings for some language, you can use them to represent individual words in sentences or documents written in that language, instead of using one-hot encoding.\n\nLet’s see how word embeddings are trained by one version of the word2vec algorithm called skip-gram. In word embedding learning, our goal is to build a model that we can use to convert a one-hot encoding of a word into a word embedding. Let our dictionary contain 10,000 words. The one-hot vector for each word is a 10,000-dimensional vector of all zeros, except for one dimension that contains a 1. Diﬀerent words have a 1 in diﬀerent dimensions.\n\nConsider a sentence: “I am attentively reading the book on machine learning engineering.” Now, take the same sentence, but remove one word, say “book.” Our sentence becomes: “I am attentively reading the · on machine learning engineering.” Now let’s only keep the three words before the · and the three words after it: “attentively reading the · on machine learning.” Looking at this six-word window around the ·, if I ask you to guess what · stands for, you would probably say: “book,” “article,” or “paper.” That’s how the context words let you predict the word they surround. It’s also how the machine can learn that words “book,” “paper,” and “article” have a similar meaning. They share similar contexts in multiple texts.\n\nIt turns out that it works the other way around too: a word can predict the context surrounding it. The piece “attentively reading the · on machine learning” is called a skip-gram, with window size 6 (3 + 3). By using the documents available on the Web, we can easily create hundreds of millions of skip-grams.\n\nIn our Let’s denote a skip-gram in the following way: sentence, x−3 is the one-hot vector for “attentively,” x−2 corresponds to “reading,” x is the\n\n[x−3,x−2,x−1,x,x+1,x+2,x+3].\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32",
      "page_number": 108,
      "chapter_number": 13,
      "summary": "We obtain the ﬁnal feature vector that represents the entire example, as shown in Figure 13 Key topics include features, important, and importance. 4.3.1 Stacking Feature Vectors\n\nIn our movie title classiﬁcation problem, we ﬁrst collect all the left contexts.",
      "keywords": [
        "feature",
        "feature vector",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Stacking Features Back",
        "Machine Learning",
        "Stacking Feature Vectors",
        "feature vector ofthe",
        "Feature selection",
        "binary feature vector",
        "Andriy Burkov Machine",
        "Features Back",
        "Learning Engineering",
        "vector",
        "Burkov Machine"
      ],
      "concepts": [
        "features",
        "important",
        "importance",
        "time",
        "examples",
        "training",
        "counts",
        "randomly",
        "randomized",
        "model"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 16,
          "title": "Segment 16 (pages 141-148)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 46,
          "title": "Segment 46 (pages 405-421)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 20,
          "title": "Segment 20 (pages 168-178)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.45,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 116-123)",
      "start_page": 116,
      "end_page": 123,
      "detection_method": "topic_boundary",
      "content": "skipped word (·), x+1 is “on,” and so on. A skip-gram with window size 4 will look like this: [x−2,x−1,x,x+1,x+2]. It can also be schematically depicted, as shown in Figure 19. It is a fully-connected network, like the multilayer perceptron. The input word is denoted as · in the skip-gram. The neural network has to learn to predict the context words of the skip-gram, given the input’s central word.\n\nThe activation function used in the output layer is softmax. The cost function is the negative log-likelihood. The embedding for a word is given by the parameters of the embedding layer that apply when a one-hot encoded word is given as the input to the model.\n\nOne problem with word embeddings trained using word2vec is that the set of word embeddings is ﬁxed, and you cannot use the model for out-of-vocabulary words, that is, the words that weren’t present in the corpus used to train word embeddings. There are other architectures of neural networks that allow obtaining embeddings for any word, including out-of-vocabulary words. One such architecture, often used in practice, is fastText. It was invented at Facebook, and the code is available in open source.\n\nThe key diﬀerence between word2vec and fastText is that word2vec treats each word in the corpus as a unitary entity, and learns a vector for each word. Alternatively, fastText treats each word as an average of embedding vectors representing character n-grams that word is composed of. For example, the embedding for the word “mouse” is an average of the embedding vectors of the n-grams “<mo,” “mou,” “<mou,” “mous,” “<mous,” “mouse,” “<mouse,” “mouse>,” “ous,” “ouse,” “ouse>,” “use,” “use>,” “se>” (assuming that the sizes of the smallest and the largest n-gram are, respectively, 3 and 6).\n\nWord embeddings are an eﬀective way of representing natural language texts for use in such neural network architectures as recurrent neural networks (RNN) and convolutional neural networks (CNN) adapted for working with sequences. However, if you want to use word embeddings for representing variable-length texts for shallow learning algorithms (which require the input feature vectors of ﬁxed dimensions), you would have to apply some aggregation operation to word vectors, such as weighted sum or average. The representation of a text document obtained as an average of the words composing that document turns out to be not very useful in practice.\n\n4.7.2 Document Embeddings\n\nA popular way of obtaining an embedding for a sentence or an entire document is to use the doc2vec neural network architecture, also invented at Google and available in open source. The architecture of doc2vec is very similar to word2vec. The only major diﬀerence is that now there are two embedding vectors, one for the document ID and one for the word. The prediction of the surrounding words for an input word is made by, ﬁrst, averaging the two embedding vectors (the document embedding vector and the word embedding vector), and then predicting the surrounding words from that average. To average the two vectors, they\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33\n\nFigure 19: The skip-gram model with window size 4 and the embedding layer of 300 units.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34\n\nmust be of the same dimensionality. Interestingly, this makes it possible to compare not just document vectors (by ﬁnding the cosine similarity), but also a document and a word vector. The word vectors trained that way are very similar to those trained using word2vec.\n\nTo obtain an embedding for a new document, not belonging to the corpus of documents used to train document embeddings, this new document is ﬁrst added to the corpus. It gets a new document ID assigned to it. Then the existing model is additionally trained for several epochs with all trained parameters being frozen but the new ones, corresponding to the new document ID. The input document ID is provided as a one-hot encoding.\n\n4.7.3 Embeddings of Anything\n\nThe following technique is commonly used to obtain embedding vectors for any object (and not just words or documents). First, we formulate a supervised learning problem that takes our objects as input and outputs a prediction. Then we build a labeled dataset and train a neural network model that solves our supervised learning problem. Then we use the outputs of one of the fully connected layers near the output layer of the neural network model (before non-linearity) as embeddings of the input object.\n\nFor example, the ImageNet labeled dataset of images and a deep convolutional neural network (CNN) architecture, similar to AlexNet, is often used to train embeddings for In images. An illustration of the embedding layers for images is shown in Figure 20. this illustration, we have a deep CNN with two fully connected layers near the output. The neural network was trained to predict the object depicted in the image. To obtain an embedding of an image not used for training the model, we send that image (usually represented as three matrices of pixels, one per channel R, G, and B) to the input of the neural network, and then use the output of one of the fully connected layers before non-linearity. Which of the fully connected layers is better depends on the task you want to solve, and must be decided experimentally.\n\nBy following the above approach, we can train embeddings of any type. The data analyst only needs to ﬁgure out three things:\n\nwhat supervised learning problem to solve (for images, usually object classiﬁcation), • how to represent the input for the neural network (for images, matrices of pixels, one per channel), and\n\nwhat will be the architecture of the neural network before the fully connected layers (for images, usually a deep CNN).\n\n4.7.4 Choosing Embedding Dimensionality\n\nThe embedding dimensionality is usually determined experimentally or from experience. For example, Google, in its TensorFlow documentation, recommends the following rule of thumb:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35\n\nFigure 20: A neural network architecture for training image embeddings. The embedding layers are shown in green.\n\nd = 4√\n\nD,\n\nwhere d is the embedding dimensionality and D is the “number of categories.” The number of categories for word embeddings is the number of unique words in the corpus. For arbitrary embeddings, it’s the dimensionality of the original input. For example, if the number of the unique words in the corpus is D = 5000000 then the embedding dimensionality d = 4√ A more principled approach to choose the embedding dimensionality is to treat it as a hyperparameter tuned on a downstream task. For example, if you have a labeled corpus of documents, then you can optimize the embedding dimensionality by minimizing the number of prediction errors made by the classiﬁer trained on that labeled data, where the words in the documents are represented by the embeddings.\n\n5000000 = 47. In practice, values between 50 and 600 are often used.\n\n4.8 Dimensionality Reduction\n\nSometimes, it might be necessary to reduce the dimensionality of examples. This is diﬀerent from the problem of feature selection. In the latter, we analyze the properties of all existing features and remove those that, in our opinion, do not contribute much to the quality of the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36\n\nmodel. When we apply a dimensionality reduction technique to a dataset, we replace all features in the original feature vector with a new vector, of lower dimensionality, and of synthetic features.\n\nDimensionality reduction often results in increased learning speed and better generalization. In addition, it improves visualization of datasets: humans can only see in three dimensions.\n\nThere are several ways to reduce dimensionality. And depending on why we want to do that, some are more popular than others. The dimensionality reduction techniques are well described in the machine learning theory books, so I will only discuss when a data analyst should prefer one technique over the others.\n\n4.8.1 Fast Dimensionality Reduction with PCA\n\nPrincipal Component Analysis (PCA) is the oldest of the techniques. It is also, by far, the fastest option. Performance comparison tests show a very weak dependence of the speed of the PCA algorithm on the size of the dataset. Therefore, you can eﬀectively use PCA as a step preceding your model training, and ﬁnd the optimal value of the reduced dimensionality experimentally as part of the hyperparameter tuning process.\n\nPCA’s most signiﬁcant drawback is that the data must ﬁt in memory entirely for the algorithm to work. There’s an out-of-core version of PCA, called Incremental PCA that allows running the algorithm on batches of the dataset, loading in memory one batch at a time. Still, Incremental PCA is an order of magnitude slower than PCA. PCA is also less practical for visualization purposes as compared to the other two techniques considered below.\n\n4.8.2 Dimensionality Reduction for Visualization\n\nIf visualization is your goal, then you would prefer Uniform Manifold Approximation and Projection (UMAP) algorithm, or an autoencoder. Both can be speciﬁcally programmed to produce 2D or 3D feature vectors, while in PCA, the algorithm produces D so-called principal components (where D is the dimensionality of your data), and the analyst must pick the ﬁrst two or three principal components as features for visualization. UMAP is generally much faster than autoencoder, but the two techniques produce very diﬀerent looking visualizations, so you would prefer one over the other based on the properties of the speciﬁc dataset. Furthermore, like PCA, UMAP requires all data to be in memory, while autoencoder can be trained in batches.\n\nDimensionality reduction can also be task-speciﬁc. For example, we can reduce the dimen- sionality of pictures by using an image editor. Similarly, we can reduce the bit rate and the number of channels of the sound sequences.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37\n\n4.9 Scaling Features\n\nOnce all your features are numerical, you are almost ready to start working on your model. The only remaining step that might be helpful is scaling your features.\n\nFeature scaling is bringing all your features to the same, or very similar, ranges of values or distributions. Multiple experiments demonstrated that a learning algorithm applied to scaled features might produce a better model. While there’s no guarantee that scaling will have a positive impact on the quality of your model, it’s considered a best practice. Scaling can also increase the training speed of deep neural networks. It also assures that no individual feature dominates, especially in the initial iterations of gradient descent or other iterative optimization algorithms. Finally, scaling reduces the risk of numerical overﬂow, the problem that computers have when working with very small or very big numbers.\n\n4.9.1 Normalization\n\nNormalization is the process of converting an actual range of values, which a numerical feature can take, into a predeﬁned and artiﬁcial range of values, typically in the interval [−1,1] or [0,1].\n\nFor example, let the natural range of a feature be 350 to 1450. By subtracting 350 from every value of the feature, and dividing the result by 1100, we normalize those values to the range [0,1]. More generally, the normalization formula looks like this:\n\n¯x(j) ←\n\nx(j) − min(j) max(j) − min(j)\n\n,\n\nwhere x(j) is an original value of feature j in some example; min(j) and max(j) are, respectively, the minimum and the maximum value of the feature j in the training data.\n\nIf you prefer the range of [−1,1] then the normalization formula would look like this:\n\n¯x(j) ←\n\n2 × x(j) − max(j) − min(j) max(j) − min(j)\n\n,\n\nA drawback of normalization is that the values max(j) and min(j) are usually outliers, so normalization will “squeeze” the normal feature values into a very small range. One solution to this problem is to apply clipping, that is to pick “reasonable” values for max(j) and min(j) instead of using extreme values from the training data. Let a reasonable range for a feature be estimated as [a,b]. Before calculating the scaled value by using one of the above two formulas, the value x(j) of the feature is set (“clipped”) to a if x(j) is below a, or to b if it’s above b. A frequent way to estimate the values for a and b is winsorization. The technique is named after the engineer and biostatistician Charles Winsor (1895—1951). Winsorization consists of setting all outliers to a speciﬁed percentile of the data; for example,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38\n\n1\n\n2\n\n1\n\n2\n\na 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. In Python, winsorization could be applied to a list of numbers as follows:\n\nfrom scipy.stats.mstats import winsorize winsorize(list_of_numbers, limits=[0.05, 0.05]) The output of the winsorize function will be a list of numbers of the same length as the input, with the values of outliers “clipped.” A corresponding code in R is shown below:\n\nlibrary(DescTools) DescTools::Winsorize(vector_of_numbers, probs = c(0.05, 0.95)) Sometimes, the mean normalization is used:\n\n¯x(j) ←\n\nx(j) − µ(j) max(j) − min(j)\n\n,\n\nwhere µ(j) is the sample mean of the values of feature j.\n\n4.9.2 Standardization\n\nStandardization (or z-score normalization) is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution, with µ = 0 and σ = 1, where µ is the sample mean (the average value of the feature, averaged over all examples in the training data) and σ is the standard deviation from the sample mean.\n\nStandard scores (or z-scores) of features are calculated as follows:\n\nˆx(j) ←\n\nx(j) − µ(j) σ(j)\n\n,\n\nwhere µ(j) is the sample mean of the values of feature j, and σ(j) is the standard deviation of the values of feature j from the sample mean.\n\nIn addition, sometimes it’s helpful to apply simple mathematical transformations to the feature values prior to applying the scaling techniques described above. Such transformations include taking the logarithm of the feature, squaring it, or extracting the square root of the feature. The idea is to obtain a distribution as close to a normal distribution as possible.\n\nYou may wonder when you should use normalization, or when to use standardization. There’s no deﬁnitive answer to this question. In theory, normalization would work better for uniformly distributed data, while standardization tends to work best for normally distributed data. However, in practice, data is rarely distributed following a perfect curve. Usually, if your\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39\n\ndataset is not too big and you have time, you can try both and see which one performs better for your task. Feature scaling is usually beneﬁcial to most learning algorithms.\n\n4.10 Data Leakage in Feature Engineering\n\nData leakage during feature engineering can happen in several situations, including feature discretization and scaling.\n\n4.10.1 Possible Problems\n\nImagine that you use your entire dataset to calculate the ranges of each bin or the feature scaling factors. Then you split the dataset into training, validation, and test sets. If you proceed like that, the values of features in the training data will, in part, be obtained by using the examples that belong to the holdout sets. When your dataset is small enough, it might result in an overly optimistic performance of your model on the holdout data.\n\nNow imagine you are working with text, and that you use bag-of-words to create features with the entire dataset. After building the vocabulary, you split your data into the three sets. In this situation, the learning algorithm will be exposed to features based on tokens only present in the holdout sets. Again, the model will display artiﬁcially better performance than had you divided your data before feature engineering.\n\n4.10.2 Solution\n\nA solution, as you might have guessed, is ﬁrst to split the entire dataset into training and holdout sets, and only do feature engineering on the training data. This also applies when you use mean encoding to transform a categorical feature to a number: split the data ﬁrst and then compute the sample mean of the label, based on the training data only.\n\n4.11 Storing and Documenting Features\n\nEven if you plan to train the model right after you ﬁnish engineering features, it’s advised to design a schema ﬁle that provides a description of the features’ expected properties.\n\n4.11.1 Schema File\n\nA schema ﬁle is a document that describes features. This ﬁle is machine-readable, versioned, and updated each time someone makes signiﬁcant updates to features. Here are several examples of the properties that can be encoded in the schema:\n\nnames of features;\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40",
      "page_number": 116,
      "chapter_number": 14,
      "summary": "We will talk about regularization in more detail in the next chapter Key topics include feature, words, and learn. 4.6 Synthesizing Features\n\nThe learning algorithms implemented in the most popular machine learning package for Python, scikit-learn, only work with numerical features.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "features",
        "machine learning",
        "Word Embeddings",
        "WORD",
        "Learning Engineering",
        "Andriy Burkov Machine",
        "learning",
        "Burkov Machine",
        "feature vectors",
        "feature selection",
        "data",
        "Embeddings",
        "machine"
      ],
      "concepts": [
        "feature",
        "words",
        "learn",
        "binning",
        "bins",
        "bin",
        "ways",
        "way",
        "documents",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 11,
          "title": "Segment 11 (pages 93-100)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 51,
          "title": "Segment 51 (pages 454-461)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.43,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 124-132)",
      "start_page": 124,
      "end_page": 132,
      "detection_method": "topic_boundary",
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nfor each feature:\n\n– its type (categorical, numerical), – the fraction of examples that are expected to have that feature present, – minimum and maximum values, – sample mean and variance, – whether it allows zeros, – whether it allows undeﬁned values.\n\nAn example of a schema ﬁle for a four-dimensional dataset is shown below:\n\nfeature {\n\nname : \"height\" type : float min : 50.0 max : 300.0 mean : 160.0 variance : 17.0 zeroes : false undefined : false popularity : 1.0\n\n}\n\nfeature {\n\nname : \"color_red\" type : binary zeroes : true undefined : false popularity : 0.76\n\n}\n\nfeature {\n\nname : \"color_green\" type : binary zeroes : true undefined : false popularity : 0.65\n\n}\n\nfeature {\n\nname : \"color_blue\" type : binary zeroes : true undefined : false popularity : 0.81\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41\n\n35\n\n}\n\n4.11.2 Feature Store\n\nLarge and distributed organizations may use a feature store that allows keeping, document- ing, reusing, and sharing features across multiple data science teams and projects. The ways features are maintained and served can diﬀer signiﬁcantly across projects and teams. This introduces infrastructure complexity and often results in duplicated work. Large distributed organizations face some of these challenges:\n\nFeatures not being reused\n\nFeatures representing the same attribute of an entity are being implemented several times by diﬀerent engineers and teams, when existing work from other teams and existing machine learning pipelines could have been reused.\n\nFeature deﬁnitions vary\n\nDiﬀerent teams deﬁne features diﬀerently, and it’s not always possible to access the documentation of a feature.\n\nComputationally intensive features\n\nSome real-time machine learning models aren’t based on informative but computationally intensive features. Having those features available in a fast store would allow using such features in real-time, and not only in batch mode.\n\nInconsistency between training and serving\n\nThe model is usually trained using the historical data, but when served, is exposed to the real-time online data. The values of some features might depend on the entire historical dataset unavailable at the service time. For the model to work correctly, each feature must have the same value for the same input data entity, both in the oﬄine (development) and online (production) mode.\n\nFeature expiration is unknown\n\nWhen a new input example comes into the production environment, there is no way to know exactly which features need to be recomputed; rather the entire pipeline needs to be run to compute the values of all features needed for prediction.\n\nA feature store is a central vault for storing documented, curated, and access-controlled features within an organization. Each feature is described by four elements: 1) name, 2) description, 3) metadata, and 4) deﬁnition.\n\nThe feature name is a string that uniquely identiﬁes the feature, for example: “aver- age_session_length” or “document_length.”\n\nThe feature description is a natural language textual description of the object’s property it represents, for example, “The average length of the session for a user.” or “The number of words in the document.”\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42\n\nIn addition to those attributes in the schema ﬁle, the feature metadata may supply: why the feature was added to the model, how it contributes to generalization, the person’s name in the organization responsible for maintaining the feature’s data source,8 the input type (e.g., numerical, string, image), the output type (e.g., numerical scalar, categorical, numerical vector), whether the feature store must cache the value of the feature, and if yes, for how long. A feature can also be marked as available online and oﬄine, or just for oﬄine processing. Features available for online processing must be implemented in such a way that their value can be either: 1) read fast from a cache or a value store or 2) computed in real-time. Features that can be computed in real-time include, for example, squaring the input number, determining the shape of the word, or doing a search in the organization’s intranet.\n\nThe deﬁnition of the feature is the versioned code, such as Python or Java. It will be executed in a runtime environment and applied to the input to compute the feature value.\n\nA feature store allows data engineers to insert features. In turn, data analysts and machine learning engineers use an API to get feature values which they deem relevant. A feature store can provide features for a single online input. Or, the analyst working on a model oﬄine may want to convert the training data into a collection of feature vectors, and will send to the feature store a batch of inputs.\n\nFor reproducibility, feature values in a feature store are versioned. With feature value versioning, the data analyst is able to rebuild the model with the same feature values as those used to train the previous model version. After the feature value for a given input is updated, the previous value is not erased. Rather, it is saved with a timestamp indicating when that value was generated. Furthermore, a feature j used by model mB can itself be the output of some model mA. Once model mA changes, it is important to keep its older versions: model mB still might expect as input the outputs generated by an older version of mA. The feature store is located in the overall machine learning pipeline as shown in Figure 21. The architecture was inspired by Uber’s Michelangelo machine learning platform. It contains two feature stores, online and oﬄine, whose data is in sync. At Uber, the online feature store is updated frequently, in near real-time, by using the real-time data. In contrast, the oﬄine feature store is updated in batch-mode by using values of some features computed online, as well as with historical data from logs and oﬄine databases. An example of a feature computed online is “restaurant’s average meal preparation time over the last hour.” An example of a feature computed oﬄine is “restaurant’s average meal preparation time over the last seven days.” At Uber, the features from the oﬄine store are synced to the online store once or several times a day.\n\n4.12 Feature Engineering Best Practices\n\nThroughout the years, analysts and engineers have invented, experimented with, and validated various best practices. Today, they are recommended for nearly every machine learning\n\n8If the person responsible for the feature leaves the company, the product owner must be alerted automat-\n\nically.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n43\n\nFigure 21: The place of the feature store in the overall machine learning pipeline.\n\nproject. Using those best practices might not signiﬁcantly improve each project, but they will deﬁnitely not hurt. One best practice already considered in this chapter is to normalize or standardize the features.\n\n4.12.1 Generate Many Simple Features\n\nAt the beginning of the modeling process, try to engineer as many “simple” features as possible. A feature is simple when it doesn’t take signiﬁcant time to code. For example, the bag-of-words approach in document classiﬁcation generates thousands of features with just a couple lines of code. As long as your hardware has the capacity, use anything measurable as a feature. You cannot know in advance whether some quantity, in combination with other quantities, will be useful for prediction.\n\n4.12.2 Reuse Legacy Systems\n\nWhen replacing an old, non-machine-learning-based algorithm with a statistical model, use the output of the old algorithm as a feature for the new model. Make sure that the old algorithm doesn’t change anymore; otherwise, your model’s performance might be negatively aﬀected over time. If the old algorithm is too slow to be a feature, use the old algorithm’s inputs as features for the new model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n44\n\nUse an external system as a feature source only if you control the external system’s behavior. Otherwise, there’s a chance that the external system evolves with time, unbeknownst to you. Furthermore, an external system’s owner might decide to use the output of your model as the input for their model. This creates the hidden feedback loop, a situation where you inﬂuence the phenomenon from which you learn.\n\n4.12.3 Use IDs as Features when Needed. . .\n\nUse IDs as features when needed. This might seem counter-intuitive because unique IDs don’t contribute to generalization. However, the use of IDs allows creating one model that has one behavior in a general case, and diﬀerent behaviors in other cases.\n\nFor example, you want to make predictions about some location (city or village), and you have some properties of the location as features. By using location ID as a feature, you can add the training examples for one general location, and train the model to behave diﬀerently in other speciﬁc locations.\n\nHowever, avoid using example ID as a feature.\n\n4.12.4\n\n. . . But Reduce the Cardinality When Possible\n\nUse categorical features with many values (more than a dozen) only when you want the model to have diﬀerent “modes” of behavior that depend on that categorical feature. Typical examples of this are zip code (postal code) or country. You might consider using the categorical feature “Country” if you want the model to behave diﬀerently in Russia versus the United States, for otherwise similar inputs.9\n\nIf you have a categorical feature with many values, but you do not need a model that has several modes depending on that feature, try to reduce the cardinality (i.e., the number of distinct values) of that feature. There are several ways to do that. We already considered one of them, feature hashing, in Section 4.2.4. Other techniques are brieﬂy discussed below:\n\nGroup similar values\n\nTry to group some values into the same category. For example, if you think it’s unlikely that, within one region, diﬀerent locations might need diﬀerent predictions, then group all postal codes from the same state into one state code. Group states into regions.\n\nGroup the long tail\n\nLikewise, try to group the long tail of infrequent values under the name \"Other,\" or merge them with similar frequent values.\n\n9Often, what you want your model to do and what the data dictates are two very diﬀerent things. Even if you think that the model must make similar predictions independently of the country, in reality, you might get poor model performance because the distribution of labels in the training data is diﬀerent for diﬀerent countries.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n45\n\nRemove the feature\n\nIf all, or almost all, values of a categorical feature are unique, or one value dominates all other values, consider removing the feature entirely.\n\nThe reduction of a feature’s granularity should be made with care. Categorical features often have functional dependencies with other categorical features, and their predictive power often comes from their combinations. Take state and city as an example. If we decide to group or remove some values in the state feature, we might inadvertently destroy the information that would allow the model to distinguish one “Springﬁeld” from another.\n\n4.12.5 Use Counts with Caution\n\nUse features based on counts with caution. Some counts remain roughly in the same bounds over time. For example, in bag-of-words, if you use the count of each token instead of the binary value, then it’s not a problem, as long as the input document length doesn’t grow or shrink with time. But, if you have a feature like “Number of calls since subscription” for a customer of a growing mobile phone provider, some oldtimers can have a very high number of calls, compared to the newer customer base. On the other hand, the training data could have been built when the company was still young and didn’t have any oldtimers.\n\nThe same caution must be applied when you group feature values in bins based on how common those values are in the dataset. Infrequent values today may become more frequent over time, as more data is added. It is considered a best practice to re-evaluate the model and the features from time to time.\n\n4.12.6 Make Feature Selection When Necessary\n\nMake feature selection when it’s necessary. The reasons could be:\n\nthe need to have an explainable model (so you keep the most signiﬁcant predictors), • strict hardware requirements, such as RAM, hard drive space, or • short time available to experiment and/or rebuild the model in production, • you expect a signiﬁcant distribution shift between two model trainings.\n\nIf you decide to do feature selection, start with Boruta.\n\n4.12.7 Test the Code Carefully\n\nThe feature engineering code must be carefully tested. Unit tests should cover each feature extractor. Check that each feature is generated correctly using as many inputs as possible. For each boolean feature, check that it is true when it should be true and is false when it should be false. Check numerical features for a reasonable value range. Check for NaNs (Not-a-Number values), nulls, zeros, and empty values. A broken extractor for one feature\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n46\n\ncan result in arbitrarily poor performance of the model. Feature extractors are the ﬁrst place to look for a problem if the model’s behavior is strange.\n\nEach feature has to be tested for speed, memory consumption, and compatibility with the production environment. What works reasonably well in your local environment may perform poorly when deployed in production.\n\nOnce the model is deployed in the production environment, and each time it is loaded, you must rerun feature extractor tests. If a feature consumes some external resources like a database or an API, these resources might be unavailable on a speciﬁc production runtime instance. The feature extractor has to throw an exception and die if any resource during feature extraction is unavailable. Avoid silent failures that may remain unnoticed for a long time with model performance degrading or becoming completely wrong.\n\nIt is also recommended to perform regular runs of feature extractors on a ﬁxed test data to make sure that the feature value distribution remains the same.\n\n4.12.8 Keep Code, Model, and Data in Sync\n\nThe version of the feature extraction code must be in sync with the model’s version and the data used to build it. The three have to be deployed or rolled back at the same time. Each time the model is loaded in production, it’s useful to check that the three elements are in sync (that is, their versions are the same).\n\n4.12.9\n\nIsolate Feature Extraction Code\n\nThe feature extraction code must be independent of the remaining code that supports the model. It should be possible to update the code responsible for each feature without aﬀecting other features, the data processing pipeline, or the way the model is called. The only exception is when many features are generated in bulk, like in one-hot encoding and bag-of-words.\n\n4.12.10 Serialize Together Model and Feature Extractor\n\nWhen possible, jointly serialize (pickle in Python, RDS in R) the model and the feature extractor object that was used when the model was built. In the production environment, deserialize both and use them. When possible, avoid having several versions of the feature extraction code.\n\nIf your production environment doesn’t let you deserialize both the model and the feature extraction code, use the same feature extraction code when you train the model and serve it. Even a tiny diﬀerence between the code a data scientist used to train the model, and the optimized code the IT team might have written for the production environment, may result in signiﬁcant prediction error.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n47\n\nOnce the production code for feature extraction is ready, use it to retrain the model. Always completely retrain the model after any change in the feature extraction code.\n\n4.12.11 Log the Values of Features\n\nLog the feature values extracted in production for a random sample of online examples. When you work on a new version of the model, these values will be useful to control the quality of the training data. They will allow you to compare and ensure that the feature values logged in the production environment are the same as those you observed in the training data.\n\n4.13 Summary\n\nFeatures are values extracted from the data entities your model is designed to work with. Each feature represents a speciﬁc property of a data entity. Features are organized in feature vectors, and the model learns to perform mathematical operations on those feature vectors to generate the desired output.\n\nFor text, features can be generated in bulk by using techniques like bag-of-words. Numbers in the bag-of-words feature vectors mean the presence or absence of speciﬁc vocabulary words in the text document. Those numbers can be binary or contain more information, such as the frequency of each word in the document, or a TF-IDF value.\n\nMost machine learning algorithms and libraries require that all features are numerical. To convert categorical features to numbers, techniques such as one-hot encoding and mean encoding are used. If the categorical feature’s values are cyclical, like days of the week or hours in a day, a better alternative is to convert that cyclical feature into two features, using the sine-cosine transformation.\n\nFeature hashing is a way to convert text data, or categorical attributes with many values, into a feature vector of an arbitrary dimensionality. That can be useful when one-hot encoding or bag-of-words generate feature vectors with impractical dimensions.\n\nTopic modeling is a family of algorithmic techniques, such as LDA and LSA, that allow us to learn a model that converts any document into a vector of topics of a required dimensionality.\n\nA time series is an ordered sequence of observations. Each observation is marked with a time-related attribute, such as timestamp, date, year, and so on. Before neural networks reached their modern capacity to learn, analysts worked with time-series data using the shallow machine learning toolkit. The time-series had to be converted into “ﬂat” feature vectors. Nowadays, analysts use neural network architectures adapted to work with sequences, such as LSTM, CNN, and Transformer.\n\nGood features have high predictive power, can be computed fast, are reliable and uncorrelated.\n\nIt is important that the distribution of feature values in the training set is similar to the distribution of values the production model will receive. Furthermore, good features are\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n48\n\nunitary, easy to understand and maintain. The property of being unitary means that the feature represents a simple-to-understand and -explain quantity.\n\nTo increase the predictive power of the data, additional features can be synthesized by discretizing an existing numerical feature, clustering training examples, or applying simple transformations to existing features or combining pairs of features.\n\nFor text, features can be learned from unlabeled data in the form of word and document embeddings. More generally, embeddings can be trained for any type of data if we manage to formulate an appropriate prediction problem and train a deep model. Embedding vectors are then extracted from several rightmost (i.e., closest to the output) fully-connected layers.\n\nWise use of feature selection techniques remove features that don’t contribute to a model’s quality. Two common techniques are cutting the long tail and Boruta. L1 regularization also works as a features selection technique.\n\nDimensionality reduction can improve visualization of high-dimensional datasets. It can also improve the model’s predictive quality. Presently, such techniques as PCA, UMAP, and autoencoders are used for dimensionality reduction. PCA is very fast, but UMAP and autoencoders produce better visualizations.\n\nIt is considered best practices to scale features before training the model, store and document features in schema ﬁles or feature stores, and keep code, model, and training data in sync.\n\nFeature extraction code is one of the most important parts of a machine learning system. It must be extensively and systematically tested.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n49",
      "page_number": 124,
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 124-132). Key topics include feature, data, and values. 4.7.3 Embeddings of Anything\n\nThe following technique is commonly used to obtain embedding vectors for any object (and not just words or documents).",
      "keywords": [
        "Machine Learning Engineering",
        "feature",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Machine Learning",
        "Feature Store",
        "Learning Engineering",
        "data",
        "Feature Engineering",
        "Burkov Machine",
        "learning",
        "Andriy Burkov",
        "Feature Engineering Data",
        "dimensionality",
        "Embedding Dimensionality"
      ],
      "concepts": [
        "feature",
        "data",
        "values",
        "documents",
        "model",
        "trained",
        "learning",
        "embedding",
        "dimensionality",
        "dataset"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 16,
          "title": "Segment 16 (pages 141-148)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 17,
          "title": "Segment 17 (pages 139-148)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 21,
          "title": "Segment 21 (pages 174-182)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 10,
          "title": "Segment 10 (pages 81-92)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 9,
          "title": "Segment 9 (pages 72-80)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 133-142)",
      "start_page": 133,
      "end_page": 142,
      "detection_method": "topic_boundary",
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n5 Supervised Model Training (Part 1)\n\nModel training (or modeling) is the fourth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nIt’s clear that without training, no model will be built. However, model training is one of the most overrated activities in machine learning. On average, a machine learning engineer spends only 5 − 10% of their time on modeling, if at all. Successful data collection, preparation, and feature engineering are more important. Usually, modeling is simply applying an algorithm from scikit-learn or R to your data, and randomly trying several combinations of hyperparameters. So, if you skipped the preceding two chapters and jumped directly into modeling, please go back and read those chapters, they are important.\n\nAs indicated by this chapter’s title, I have divided supervised model training into two parts. In this ﬁrst part, we will consider learning preparation, choosing the learning algorithm, a shallow learning strategy, assessing model performance, bias-variance tradeoﬀ, regularization, the concept of the machine learning pipeline, and hyperparameter tuning.\n\n5.1 Before You Start Working on the Model\n\nBefore working on the model, you should validate schema conformity, deﬁne an achievable level of performance, choose a performance metric, and make several other decisions.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\n5.1.1 Validate Schema Conformity\n\nFirst, ensure the data conforms to the schema, as deﬁned by the schema ﬁle. Even if you initially prepared the data, it’s likely the original data and the current data are not the same. This diﬀerence can be explained by various factors, most probably:\n\nthe method used to persist the data, to hard drive or to database, contains an error; • the method you used to read the data, from where it was persisted, contains an error; • someone else may have changed the data, or the schema, without informing you.\n\nThese schema errors must be detected, identiﬁed, and corrected just as when a programming code error is detected. If needed, the entire data collection and preparation pipeline should be run from scratch, as we discussed at the end of Chapter 3 when talked about reproducibility.\n\n5.1.2 Deﬁne an Achievable Performance Level\n\nDeﬁning an achievable performance level is a crucial step. It gives you an idea of when to stop trying to improve the model. Here are some guidelines:\n\nif a human can label examples without too much eﬀort, math, or complex logic derivations, then you can hope to achieve human-level performance with your model; • if the information needed to make a labeling decision is fully contained in the features, you can expect to have near-zero error;\n\nif the input feature vector has a high number of signals (such as pixels in an image, or words in a document), you can expect to come close to near-zero error;\n\nif you have a computer program solving the same classiﬁcation or regression problem, you can expect your model to perform at least as well. Often the machine learning model performance can improve as more labeled data comes in; and,\n\nif you observe a similar, but diﬀerent system, you can expect to get a similar, but diﬀerent machine learning model performance.\n\n5.1.3 Choose a Performance Metric\n\nWe will talk about assessing the model performance later. For now, there are several ways — metrics — to estimate the level of model performance (its quality). There’s no single best metric you can use for every project. You will choose based on your data and the problem.\n\nIt is recommended to choose one, and only one, performance metric before you start working on the model. Then, compare diﬀerent models and track the overall progress by using this one metric.\n\nIn Section 5.5, you will read about the most popular and handy model performance metrics, and about the approaches allowing us to combine multiple metrics to obtain a single number.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nTraining epochError (%)2040608020406080\n\nHuman performance\n\nOur model (training)\n\nOur model (validation)\n\nTraining epochError (%)2040608020406080\n\nHuman performance\n\nOur model (training)\n\nOur model (validation)\n\n(a)\n\n(b)\n\nFigure 2: A model performance compared to a human-performance baseline: (a) the model looks good, so we can decide to regularize it or add more training examples; (b) the model isn’t performing well, so we need to add more features, or increase the model complexity.\n\n5.1.4 Choose the Right Baseline\n\nBefore you start working on a predictive model, it is important to establish baseline perfor- mance on your problem. A baseline is a model or an algorithm that provides a reference point for comparison.\n\nHaving a baseline gives an analyst conﬁdence that the machine-learning-based solution works. If the value of the performance metric for the machine learning model is better than the value obtained using the baseline, then machine learning provides value.\n\nComparing your current model’s performance to a baseline can orient the work in diﬀerent directions. Let’s say we know that human-level performance is achievable on our problem. We then take human performance as a baseline, as shown in Figure 2. In Figure 2a, the model looks good, so we can decide to regularize it or add more training examples. On the other hand, in Figure 2b, the model isn’t performing well, so we should add more features, or increase the model complexity.\n\nThe baseline is a model or an algorithm that gets an input, and outputs a prediction. The baseline’s prediction output must be of the same nature as the model’s prediction. Otherwise, you cannot compare them.\n\nA baseline doesn’t have to be the result of any learning algorithm. It can be a rule-based or heuristic algorithm, a simple statistic applied to the training data, or something else.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nThe two most commonly used baseline algorithms are:\n\nrandom prediction, and • zero rule.\n\nThe random prediction algorithm makes a prediction by randomly choosing a label from the collection of labels assigned to the training examples. In the classiﬁcation problem, it corresponds to randomly picking one class from all classes in the problem. In the regression problem it means selecting from all unique target values in the training data.\n\nThe zero rule algorithm yields a tighter baseline than the random prediction algorithm. This means that it usually improves the value of the metric as compared to random prediction. To make predictions, the zero rule algorithm uses more information about the problem.\n\nIn classiﬁcation, the zero rule algorithm strategy is to always predict the class most common in the training set, independently of the input value. It can look ineﬀective, but consider the following problem. Let the training data for your classiﬁcation problem contain 800 examples of the positive class, and 200 examples of the negative class. The zero rule algorithm will predict the positive class all the time, and the accuracy (one of the popular performance metrics that we will consider in Section 5.5.2) of the baseline will be 800/1000 = 0.8 or 80%, which is not bad for such a simple classiﬁer. Now you know that your statistical model, independently of how close it is to the optimum, must have an accuracy of at least 80%.\n\nNow, let’s consider the zero rule algorithm for regression. According to the zero rule algorithm, the strategy for regression is to predict the sample average of the target values observed in the training data. This strategy will likely have a lower error rate than random prediction.\n\nIf you work on a standard, so-called classical, prediction problem, you can use a state-of-the-art algorithm found in a popular library such as Python’s scikit-learn. For text classiﬁcation, for example, represent the text as bag-of-words, and then train a support vector machine model with a linear kernel. Then try to beat that result with your own more advanced approach. This approach would also work well with image classiﬁcation, machine translation, and other well-studies, so-called benchmark problems.\n\nFor a general numerical dataset, a linear model such as linear or logistic regression, or k-nearest neighbors, for k = 5, would be a decent baseline. For image classiﬁcation, a simple convolutional neural network (CNN), with three convolutional layers (32−64−32 units per layer, each convolutional layer followed by a max pooling layer and a dropout layer) and two fully connected layers at the end (one with 128 units, and one with the number of units corresponding to the number of desired outputs) would be a good baseline.\n\nYou could also use an existing rule-based system, or build your own simple rule-based system. For example, if the problem is to build a model that predicts whether a given website visitor will like a recommended article, a simple rule-based system could work as follows. Take all articles liked by the user, ﬁnd the top ten words in those articles according to their TF-IDF score, and then predict that the user will like an article if at least ﬁve of those ten can be found in the recommended article. Additionally, multiple specialized machine learning\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\nlibraries and APIs are available online. If they can be used directly, or repurposed to solve your problem, you should deﬁnitely consider them as a baseline.\n\nFinding a good human baseline is not always simple. You might use Amazon Mechanical Turk service. Mechanical Turk (MT) is a web-platform where people solve simple tasks for a reward. MT provides an API that you can call to get human predictions. The quality of such predictions can vary from very low to relatively high, depending on the task and the reward. MT is relatively inexpensive, so you can get predictions fast and in large numbers.\n\nTo increase the quality of the predictions provided by turkers (this is how MT human workers are called), some analysts use an ensemble of turkers. You can ask three or ﬁve turkers to label the same example, and then pick the majority class among the labels (or average labels for regression). A more expensive alternative is to ask domain experts (or an ensemble, for even better quality) to label your data.\n\n5.1.5 Split Data Into Three Sets\n\nRecall that three sets are generally needed to build a solid model. The ﬁrst, the training set, is used to train the model. It is the data the machine learning algorithm “sees.” The second and third are the holdout sets. The validation set is not seen by the machine learning algorithm. The data analyst uses it to estimate the performance of diﬀerent machine learning algorithms (or the same algorithm conﬁgured with diﬀerent values of hyperparameters) or models when applied to new data. The remaining test set, which is also not seen by the learning algorithm, is used at the end of the project to evaluate and report the performance of the model the best performing on the validation data.\n\nThe process of splitting the entire dataset into three sets is described in Section ?? of Chapter 3. Here, I only reiterate the two most important properties of that process:\n\n1. Validation and test sets must come from the same statistical distribution. That is, their properties have to be maximally similar, but the examples belonging to the two sets must be, obviously and ideally, diﬀerent and obtained independently of one another.\n\n2. Draw validation and test data from a distribution that looks much like the data you expect to observe once the model is deployed in production. It can be diﬀerent from the distribution of the training data.\n\nA couple of words about the latter point. Most of the time, the analyst simply shuﬄes the entire dataset, and then randomly ﬁlls the three sets from this shuﬄed data. In practice, however, it’s common to have many examples that do not look like the production data. Sometimes, these examples are abundant and/or inexpensive. Using this data in the project may result in distribution shift, and the analyst may or may not be aware of it.\n\nIf you are aware of distribution shift, you will place all those easily available examples into your training set, but will avoid using them in the validation and test sets. This way, you evaluate the models against the data that is similar to that in your production setting. Doing\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\notherwise might result in achieving overly optimistic values of the performance metric during model testing, and selecting for production a suboptimal model.\n\nThe distribution shift can be a hard problem to tackle. Using a diﬀerent data distribution for training could be a conscious choice because of the data availability. However, the analyst may be unaware that the statistical properties of the training and development data are diﬀerent. This often happens when the model is frequently updated after production deployment, and new examples are added to the training set. The properties of the data used to train the model, and that of the data used to validate and test it, can diverge over time. Section ?? in the next chapter provides guidance on how to handle that problem.\n\n5.1.6 Preconditions for Supervised Learning\n\nBefore you start working on your model, make sure the following conditions are satisﬁed:\n\n1. You have a labeled dataset. 2. You have split the dataset into three subsets: training, validation, and test. 3. Examples in the validation and test sets are statistically similar. 4. You engineered features and ﬁlled missed values using only the training data. 5. You converted all examples into numerical feature vectors.1 6. You have selected a performance metric that returns a single number (see Section 5.5). 7. You have a baseline.\n\n5.2 Representing Labels for Machine Learning\n\nIn the classical formulation of classiﬁcation, labels look like values of a categorical feature. For example, in image classiﬁcation, the labels could be “cat,” “dog,” “car,” “building,” and so on.\n\nSome machine learning algorithms, like those you ﬁnd in scikit-learn, accept labels in their natural form: strings. The library take care of transforming strings to numbers that are accepted by a speciﬁc learning algorithm.\n\nSome implementations, however, like those in neural networks, require the analyst to transform the labels to numbers.\n\n5.2.1 Multiclass Classiﬁcation\n\nIn the case of multiclass classiﬁcation (that is, when the model predicts only one label given an input feature vector), one-hot encoding is typically used to convert labels to\n\n1As mentioned in the previous chapter, most modern machine learning libraries and packages expect numerical feature vectors. However, some algorithms, like decision tree learning, can naturally work with categorical features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nbinary vectors. For example, let your classes be {dog,cat,other}, and you have the following data:\n\nImage\n\nLabel\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\ndog dog cat other cat\n\nOne-hot encoding would generate the following binary vectors for your classes:\n\ndog = [1,0,0], cat = [0,1,0], other = [0,0,1].\n\nAfter you convert categorical labels into binary vectors, your data becomes:\n\nImage\n\nLabel\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\n[1,0,0] [1,0,0] [0,1,0] [0,0,1] [0,1,0]\n\n5.2.2 Multi-label Classiﬁcation\n\nIn multi-label classiﬁcation, the model may predict several labels for one input at the same time (for example, an image can contain both a dog and a cat). In this case, you can use bag-of-words to represent the labels assigned to each example. Let your data be as follows:\n\nImage\n\nLabels\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\ndog, cat dog cat, other other cat, dog\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nAfter you convert labels into binary vectors, your data becomes:\n\nImage\n\nLabels\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\n[1,1,0] [1,0,0] [0,1,1] [0,0,1] [1,1,0]\n\nRead the documentation of the speciﬁc implementation of a learning algorithm to know the format of the input expected by the learning algorithm.\n\n5.3 Selecting the Learning Algorithm\n\nChoosing a machine learning algorithm can be a diﬃcult task. If you had a lot of time, you could try all of them. However, usually, the time to solve a problem is limited. To make an informed choice, you can ask yourself several questions before starting to work on the problem. Depending on your answers, you can shortlist some algorithms and try them on your data.\n\n5.3.1 Main Properties of a Learning Algorithm\n\nBelow are several questions and answers which may guide you in choosing a machine learning algorithm or model.\n\nExplainability\n\nDo the model predictions require explanation for a non-technical audience? The most accurate machine learning algorithms and models are so-called “black boxes.” They make very few prediction errors, but it may be diﬃcult to understand, and even harder to explain, why a model or an algorithm made a speciﬁc prediction. Examples of such models are deep neural networks and ensemble models.\n\nIn contrast, kNN, linear regression, and decision tree learning algorithms are not always the most accurate. However, their predictions are easy to inter- pret by a non-expert.\n\nIn-memory vs. out-of-memory\n\nCan your dataset be fully loaded into the RAM of your laptop or server? If yes, then you can choose from a wide variety of algorithms. Otherwise, you would prefer incremental learning algorithms that can improve the model by reading data gradually. Examples of such algorithms are Naïve Bayes and the algorithms for training neural networks.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\nNumber of features and examples\n\nHow many training examples do you have in your dataset? How many features does each example have? Some algorithms, including those used for training neural networks and random forests, can handle a huge number of examples and millions of features. Others, like the algorithms for training support vector machines (SVM), can be relatively modest in their capacity.\n\nNonlinearity of the data\n\nIs your data linearly separable? Can it be modeled using a linear model? If yes, SVM with the linear kernel, linear and logistic regression can be good choices. Otherwise, deep neural networks or ensemble models might work better.\n\nTraining speed\n\nHow much time is a learning algorithm allowed to use to build a model, and how often you will need to retrain the model on updated data? If training takes two days, and you need to retrain your model every 4 hours, then your model will never be up to date. Neural networks are slow to train. Simple algorithms like linear and logistic regression, or decision trees, are much faster.\n\nSpecialized libraries contain very eﬃcient implementations of some algorithms. You may prefer to do research online to ﬁnd such libraries. Some algorithms, such as random forest learning, beneﬁt from multiple CPU cores, so their training time can be signiﬁcantly reduced on a machine with dozens of cores. Some machine learning libraries leverage GPU (graphics processing unit) to speed up training.\n\nPrediction speed\n\nHow fast must the model be when generating predictions? Will your model be used in a production environment where very high throughput is required? Models like SVMs and linear and logistic regression models, and not-very-deep feedforward neural networks, are extremely fast at prediction time. Others, like kNN, ensemble algorithms, and very deep or recurrent neural networks, are slower.\n\nIf you don’t want to guess the best algorithm for your data, a popular way to choose one is by testing several candidate algorithms on the validation set as a hyperparameter. We talk about hyperparameter tuning in Section 5.6.\n\n5.3.2 Algorithm Spot-Checking\n\nShortlisting candidate learning algorithms for a given problem is sometimes called algorithm spot-checking. For the most eﬀective spot-checking, it is recommended to:\n\nselect algorithms based on diﬀerent principles (sometimes called orthogonal), such as instance-based algorithms, kernel-based, shallow learning, deep learning, ensembles; • try each algorithm with 3 − 5 diﬀerent values of the most sensitive hyperparameters (such as the number of neighbors k in k-nearest neighbors, penalty C in support vector machines, or decision threshold in logistic regression);\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "page_number": 133,
      "chapter_number": 16,
      "summary": "One best practice already considered in this chapter is to normalize or standardize the features Key topics include feature, modeling, and data. A feature is simple when it doesn’t take signiﬁcant time to code.",
      "keywords": [
        "feature",
        "model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "feature extraction code",
        "Andriy Burkov Machine",
        "machine learning model",
        "feature extraction",
        "Burkov Machine",
        "model performance",
        "data",
        "Learning Engineering",
        "learning",
        "learning model performance"
      ],
      "concepts": [
        "feature",
        "modeling",
        "data",
        "values",
        "useful",
        "code",
        "learn",
        "prediction",
        "predictions",
        "predictive"
      ],
      "similar_chapters": [
        {
          "book": "LLM-Engineers-Handbook",
          "chapter": 18,
          "title": "Segment 18 (pages 164-171)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 12,
          "title": "Segment 12 (pages 94-106)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 20,
          "title": "Segment 20 (pages 168-178)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 103-122)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 143-150)",
      "start_page": 143,
      "end_page": 150,
      "detection_method": "topic_boundary",
      "content": "Andriy Burkov\n\nMachine Learning Engineering - Draft\n\ng r o . n r a e l - t i k i c s\n\n: e c r u o S\n\n.\n\nn r a e l - t i k i c s\n\nr o f\n\nm a r g a i d\n\nn o i t c e l e s\n\nm h t i r o g l a\n\ng n i n r a e l\n\ne n i h c a M\n\n: 3\n\ne r u g i F\n\n12\n\nuse the same training/validation split for all experiments, • if the learning algorithm is not deterministic (such as the learning algorithms for neural networks and random forests), run several experiments, and then average the results; • once the project is over, note which algorithms performed the best, and use this information when working on a similar problem in the future.\n\nWhile you don’t know your problem well, try to solve it using as many orthogonal approaches as possible, rather than spending a lot of time on the most promising approach. It is generally a better idea to spend time experimenting with new algorithms and libraries, rather than trying to squeeze the maximum from the one with which you have the most experience.\n\nIf you don’t have time to carefully spot-check algorithms, one simple “hack” is to ﬁnd an eﬃcient implementation of a learning algorithm or a model that most modern papers claim to beat, when applied to a problem similar to yours, and use it for solving your problem.\n\nIf you use scikit-learn, you could try their algorithm selection diagram shown in Figure 3.\n\n5.4 Building a Pipeline\n\nMany modern machine learning packages and frameworks support the notion of a pipeline. A pipeline is a sequence of transformations the training data goes through, before it becomes a model. An example of a pipeline used to train a document classiﬁcation model out of a collection of labeled text documents is shown below:\n\nTokenization\n\nFeatureExtraction\n\nFeatureselection\n\nFeaturenormalization\n\nModeltraining\n\nModel\n\nFigure 4: A pipeline used to produce a model starting with raw data.\n\nEvery stage of a pipeline receives the output of the previous stage, except for the ﬁrst stage, whose input is the training dataset.\n\nBelow is a Python code fragment that constructs a simple scikit-learn pipeline. It consists of two steps: 1) dimensionality reduction using Principal Component Analysis (PCA), and 2) training a support vector machine (SVM) classiﬁer:\n\n1\n\n2\n\n3\n\nfrom sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.decomposition import PCA\n\n4\n\n5\n\n6\n\n# Define a pipeline pipe = Pipeline([('dim_reduction', PCA()), ('model_training', SVC())])\n\n7\n\n8\n\n# Train parameters of both PCA and SVC\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\n9\n\n10\n\n11\n\n12\n\npipe.fit(X, y)\n\n# Make a prediction pipe.predict(new_example)\n\nWhen the command pipe.predict(new_example) is executed, the input example is ﬁrst transformed into a reduced dimensionality vector using the PCA model. That reduced dimensionality vector is used as input to the SVM model. PCA and SVM models were trained, one after the other, when the command pipe.fit(X, y) were executed.\n\nUnfortunately, deﬁning and training pipelines in R is not as straightforward as in Python, so we don’t put the code in the book.\n\nThe pipeline can be saved to a ﬁle similar to saving a model. It will be deployed to production and used to generate predictions. In other words, during the scoring, the input example passes through the entire pipeline and “becomes” an output.\n\nAs you can see, the notion of a pipeline is a generalization of the notion of a model. From this point forward, unless stated otherwise, when I refer to model training, saving, deployment, serving, monitoring, or post-production maintenance, I mean the entire pipeline.\n\nBefore we consider the challenge of training a model, we need to decide how to measure the model quality. Often, we have a choice between several competing models, so-called model candidates, but only one will be deployed in production.\n\n5.5 Assessing Model Performance\n\nRemember, the holdout data consists of examples the learning algorithm didn’t see during training. If our model performs well on a holdout set, we can say our model generalizes well and is of good quality or, simply, that it’s good. The most common way to get a good model is to compare diﬀerent models by calculating a performance metric on the holdout data.\n\n5.5.1 Performance Metrics for Regression\n\nRegression and classiﬁcation models are assessed using diﬀerent metrics. Let’s ﬁrst consider performance metrics for regression: mean squared error (MSE), median absolute error (MAE), and almost correct predictions error rate (ACPER).\n\nThe metric most often used to quantify the performance of a regression model is the same as the cost function: mean squared error (MSE), deﬁned as,\n\nMSE(f) def=\n\n1\n\nN\n\nX\n\ni=1...N\n\n(f(xi) − yi)2,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(1)\n\n14\n\nwhere f is the model that takes a feature vector x as input and outputs a prediction, and i, ranging from 1 to N, denotes the index of an example from a dataset.\n\nA well-ﬁtting regression model predicts values close to the observed data values. The mean model, which always predicts the average of the training data labels, generally would be used if there were no informative features. Therefore, the regression model should ﬁt better than that of the mean model. Thus, the mean model acts as a baseline. If the regression model MSE is greater than the baseline MSE, then we have a problem in our regression model. It may be overﬁtting or underﬁtting (we consider these in Section 5.8). It could also be that the problem was deﬁned with an error, or the programming code contains a bug.\n\nIf the data contains outliers, the examples very far from the “true” regression line, they can signiﬁcantly aﬀect the value of MSE. By deﬁnition, the squared error for such outlying examples will be high. In such situations, it is better to apply a diﬀerent metric, the median absolute error, MdAE:\n\nMdAE def= median\n\n(cid:16)\n\n{|f(xi) − yi|}N i=1\n\n(cid:17)\n\n,\n\nwhere {|f(xi) − yi|}N i=1 to N, on which the evaluation of the model is performed.\n\ndenotes the set of absolute error values for all examples, from i = 1\n\nThe almost correct predictions error rate (ACPER) is the percentage of predictions that is within p percentage of the true value. To calculate ACPER, proceed as follows:\n\n1. Deﬁne a threshold percentage error that you consider acceptable (let’s say 2%). 2. For each true value of the target yi, the desired prediction should be between yi +0.02yi and yi − 0.02yi.\n\n3. By using all examples i = 1,...,N, calculate the percentage of predicted values fulﬁlling the above rule. This will give the value of the ACPER metric for your model.\n\n5.5.2 Performance Metrics for Classiﬁcation\n\nFor classiﬁcation, things are a little more complicated. The most widely used metrics to assess a classiﬁcation model are:\n\nprecision-recall, • accuracy, • cost-sensitive accuracy, and • area under the ROC curve (AUC).\n\nTo simplify, I will illustrate with a binary classiﬁcation problem. Where necessary, I show how to extend the approach to the multiclass case.\n\nFirst, we need to understand the confusion matrix.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\nA confusion matrix is a table that summarizes how successful the classiﬁcation model is at predicting examples belonging to various classes. One axis of the confusion matrix is the class that the model predicted; the other axis is the actual label. Let’s say, our model predicts classes “spam” and “not_spam”:\n\nspam (predicted)\n\nnot_spam (predicted)\n\nspam (actual) not_spam (actual)\n\n23 (TP) 12 (FP)\n\n1 (FN) 556 (TN)\n\nThe above matrix shows that out of 24 actual spam examples, the model correctly classiﬁed 23. In this case, we say that we have 23 true positives or TP = 23. The model incorrectly classiﬁed 1 spam example as not_spam. In this case, we have 1 false negative, or FN = 1. Similarly, out of 568 actual not_spam examples, the model classiﬁed correctly 556 and incorrectly 12 examples (556 true negatives, TN = 556, and 12 false positives, FP = 12).\n\nThe confusion matrix for multiclass classiﬁcation has as many rows and columns as there are diﬀerent classes. It can help you to determine mistake patterns. For example, a confusion matrix could reveal that a model trained to recognize diﬀerent species of animals tends to mistakenly predict “cat” instead of “panther,” or “mouse” instead of “rat.” In this case, you can add more labeled examples of these species to help the learning algorithm “see” the diﬀerence between those animals. Alternatively, you might add features that would help the learning algorithm do better at distinguishing between those pairs of species.\n\nThe confusion matrix is used to calculate three performance metrics: precision, recall, and accuracy. Precision and recall are most frequently used to assess a binary model.\n\nPrecision is the ratio of true positive predictions to the overall number of positive predictions:\n\nprecision def=\n\nTP TP + FP.\n\nRecall is the ratio of true positive predictions to the overall number of positive examples:\n\nrecall def=\n\nTP TP + FN.\n\nTo understand the meaning and the importance of precision and recall for model assessment, it’s useful to think about the prediction problem as the problem of research of documents in a database using a query. The precision is the proportion of relevant documents actually found in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine, compared to the total number of relevant documents that should have been returned.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\nIn spam detection, we want to have high precision, to avoid wrongly placing a legitimate message in our spam folder. We are willing to tolerate lower recall, since we can deal with some spam messages in our inbox.\n\nIn practice, we choose between high precision or high recall. It’s practically impossible to have both. This is called the precision-recall tradeoﬀ. We can achieve either by various means:\n\nby assigning a higher weigh to the examples of a speciﬁc class. For example, SVM in scikit-learn accepts weights of classes as input;\n\nby tuning hyperparameters to maximize either precision or recall on the validation set; • by varying the decision threshold for algorithms that return prediction scores. Let’s say we have a logistic regression model or a decision tree. To increase precision (at the cost of a lower recall), we can decide that the prediction will be positive only if the score returned by the model is higher than 0.9 (instead of the default value of 0.5).\n\nEven if precision and recall are deﬁned for binary classiﬁcation, you can also use them to assess a multiclass classiﬁcation model. First select a class for which you want to assess these metrics. Then you consider all examples of the selected class as positives and all examples of the remaining classes as negatives.\n\nIn practice, to compare the performance of two models, you would prefer to have only one number that represents the performance of each model. For example, you would like to avoid situations where the ﬁrst model has a higher precision, when the second model has a higher recall: if it’s the case, which model is better?\n\nOne way to compare models based on one number is to threshold the minimum acceptable value for one metric, say recall, and then only compare models based on the value of another metric. For example, say you will accept any model whose recall is above 90%. Then you will give preference to the model whose precision is the highest (assuming that its recall is above 90%). This technique is known as optimizing and satisﬁcing technique.\n\nSome practitioners use a combination of precision and recall called F-measure, also known as F-score. The traditional F-measure, or F1-score, is the harmonic mean of precision and recall:\n\nF1 =\n\n(cid:18)\n\n2 recall−1 + precision−1\n\n(cid:19)\n\n= 2 ×\n\nprecision × recall precision + recall\n\nMore generally, F-measure is parametrized with a positive real β, chosen such that recall is considered β times as important as precision:\n\nFβ = (1 + β2) ×\n\nprecision × recall (β2 × precision) + recall\n\nTwo commonly used values for β are 2, which weighs recall twice as high as precision, and 0.5, which weighs recall twice as low as precision.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\nYou should ﬁnd a way to combine the two metrics that works best for your problem. Besides F-score, there are other ways to obtain a single number by combining multiple metrics:\n\nsimple average, or weighted average of metrics; • threshold n − 1 metrics and optimize the nth (a generalization of the above optimizing and satisﬁcing technique);\n\ninvent your own domain-speciﬁc “recipe.”\n\nAccuracy is given by the number of correctly classiﬁed examples, divided by the total number of classiﬁed examples. In terms of the confusion matrix, it is given by:\n\naccuracy def=\n\nTP + TN TP + TN + FP + FN\n\nAccuracy is a useful metric when errors in predicting all classes are judged to be equally important. It’s the case, for example, for object recognition for a domestic robot: a chair is no more important than a table. In the case of the spam/not spam prediction, this probably would not be so. Likely, you would tolerate false negatives more than false positives. Remember, a false positive is when your friend sends you an email, but the model places it in the spam folder and you don’t see it. A false negative, a situation in which a spam message gets to the inbox, is less of a problem.\n\nFor dealing with the situations in which diﬀerent classes have diﬀerent importance, a useful metric is cost-sensitive accuracy. First, assign a cost (a positive number) to both types of mistakes: FP and FN. Then compute the counts TP, TN, FP, FN as usual, and multiply the counts for FP and FN by their corresponding costs before calculating the accuracy using Equation 2, above.\n\nAccuracy measures the performance of the model for all classes at once, and it conveniently returns a single number. However, accuracy is not a good performance metric when the data is imbalanced. In an imbalanced dataset, examples belonging to some class or a few classes constitute the vast majority, while other classes include very few examples. Imbalanced training data can signiﬁcantly and adversely aﬀect the model. We will talk more about dealing with the imbalanced data in Section ?? of Chapter 6.\n\nFor imbalanced data, a better metric is per-class accuracy. First, calculate the accuracy of prediction for each class {1,...,C}, and then take an average of C individual accuracy measures. For the above confusion matrix of the spam detection problem, the accuracy for the class “spam” is 23/(23 + 1) = 0.96, the accuracy for the class “not_spam” is 556/(12 + 556) = 0.98. The per-class accuracy is then (0.96 + 0.98)/2 = 0.97.\n\nPer-class accuracy will not be an appropriate model quality measure for a multiclass clas- siﬁcation problem where many classes have very few examples (roughly, less than a dozen examples per class). In that case, the accuracy values obtained for the binary classiﬁcation problems corresponding to these minority classes will not be statistically reliable.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(2)\n\n18\n\nCohen’s kappa statistic is a performance metric that applies to both multiclass and imbalanced learning problems. The advantage of this metric over accuracy is that Cohen’s kappa tells you how much better your classiﬁcation model is performing, compared to a classiﬁer that randomly guesses a class according to the frequency of each class.\n\nCohen’s kappa is deﬁned as:\n\nκ def= po − pe 1 − pe\n\n,\n\nwhere po is called the observed agreement, and pe is the expected agreement. Let’s look once again at a confusion matrix:\n\nclass1 (predicted)\n\nclass2 (predicted)\n\nclass1 (actual) class2 (actual)\n\na c\n\nb d\n\nThe observed agreement po is obtained from the confusion matrix as,\n\npo\n\ndef=\n\na + d a + b + c + d\n\n.\n\nThe expected agreement pe, in turn, is obtained as pe\n\ndef= pclass1 + pclass2, where,\n\npclass1\n\ndef=\n\na + b a + b + c + d\n\n×\n\na + c a + b + c + d\n\n,\n\nand\n\npclass2\n\ndef=\n\nc + d a + b + c + d\n\n×\n\nb + d a + b + c + d\n\nThe value of Cohen’s kappa is always less than or equal to 1. Values of 0 or less indicate that the model has a problem. While there is no universally accepted way to interpret the values of Cohen’s kappa, it’s usually considered that values between 0.61 and 0.80 indicate that the model is good, and values 0.81 or higher suggest that the model is very good.\n\nThe ROC curve (stands for “receiver operating characteristic;” the term comes from radar engineering) is a commonly-used method of assessing classiﬁcation models. ROC curves use a combination of the true positive rate (deﬁned exactly as recall) and false positive rate (the proportion of negative examples predicted incorrectly) to build up a summary picture of the classiﬁcation performance.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "page_number": 143,
      "chapter_number": 17,
      "summary": "A baseline is a model or an algorithm that provides a reference point for comparison Key topics include algorithm, model, and learning. It can be a rule-based or heuristic algorithm, a simple statistic applied to the training data, or something else.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "machine learning algorithm",
        "learning algorithm",
        "Andriy Burkov Machine",
        "learning",
        "model",
        "Learning Engineering",
        "algorithm",
        "Burkov Machine",
        "machine",
        "data",
        "image",
        "Training"
      ],
      "concepts": [
        "algorithm",
        "model",
        "learning",
        "data",
        "predictive",
        "prediction",
        "predictions",
        "training",
        "baseline",
        "label"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 24,
          "title": "Segment 24 (pages 203-210)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "Segment 33 (pages 281-288)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 103-122)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 19,
          "title": "Segment 19 (pages 375-397)",
          "relevance_score": 0.41,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 151-158)",
      "start_page": 151,
      "end_page": 158,
      "detection_method": "topic_boundary",
      "content": "The true positive rate (TPR) and the false positive rate (FPR) are respectively deﬁned as,\n\nTPR def=\n\nTP TP+FN\n\nand FPR def=\n\nFP FP+TN\n\nROC curves can only be used to assess classiﬁers that return a score (or a probability) of prediction. For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves.\n\nTo draw a ROC curve, you ﬁrst discretize the range of the score. For instance, you can discretize the range [0,1] like this: [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]. Then, use each discrete value as the prediction threshold for your model. For example, if you want to calculate TPR and FPR for the threshold equal to 0.7, you apply the model to each example and get the score. If the score is greater than or equal to 0.7, you predict the positive class. Otherwise, you predict the negative class.\n\nLook at the illustration in Figure 5. It’s easy to see that if the threshold equals 0, all our predictions will be positive, so both TPR and FPR will equal 1 (the upper right corner). On the other hand, if the threshold equals 1, then no positive prediction will be possible. Both TPR and FPR will equal 0, which corresponds to the lower-left corner.\n\nThe greater the area under the ROC curve (AUC), the better the classiﬁer. A classiﬁer with an AUC greater than 0.5 is better than a model that classiﬁes at random. If AUC is lower than 0.5, then something is wrong, most likely a bug in the code or wrong labels in the data. A perfect classiﬁer would have an AUC of 1. In practice, you obtain a good classiﬁer by selecting the value of the threshold that gives TPR close to 1 while keeping FPR near 0.\n\nROC curves are popular because they are relatively simple to understand. They capture more than one aspect of the classiﬁcation, by taking both false positives and false negatives into account. They allow the analyst to easily and visually compare diﬀerent model performances.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\nFigure 5: The area under the ROC curve (shown in grey).\n\n5.5.3 Performance Metrics for Ranking\n\nPrecision and recall can be naturally applied to the ranking problem. Recall that it’s convenient to think of these two metrics as measuring the quality of document search results. Precision is the proportion of relevant documents actually found in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine, compared to the total number of the relevant documents that should have been returned.\n\nThe drawback of measuring the quality of ranking models with precision and recall is that these metrics treat all retrieved documents equally. A relevant document listed at position k is worth just as much as a relevant document at the top of the list. This is usually not what\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\nwe want in document retrieval. When a human looks at search results, the few top-most results matter more than the results shown at the bottom of the list.\n\nDiscounted cumulative gain (DCG) is a popular measure of ranking quality in search engines. DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower positions.\n\nTo understand discounted cumulative gain, we introduce a measure called cumulative gain.\n\nCumulative gain (CG) is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position p is deﬁned as:\n\nCGp\n\ndef=\n\np X\n\nreli,\n\ni=1\n\nwhere reli is the graded relevance of the result at position i. Generally, graded relevance reﬂects the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as “not relevant,” “somewhat relevant,” “relevant,” or “very relevant”). To use it in the above formula, reli must be numeric, for example, ranging from 0 (the document at position i is entirely irrelevant to the query) to 1 (the document at position i is maximally relevant to the query). Alternatively, reli can be binary: 0 when the document is not relevant to the query, and 1 when relevant. Notice that CGp is independent of the position each document holds in the ranked result list. It only characterizes the documents ranked up to position p as relevant or irrelevant to the query.\n\nDiscounted cumulative gain is based on two assumptions:\n\n1. Highly relevant documents are more useful when appearing earlier in the result list. 2. Highly relevant documents are more useful than marginally relevant documents, while the latter, in turn, are more useful than non-relevant documents.\n\nFor a given search result, DCG accumulated at a particular rank position p is often deﬁned as:\n\nDCGp\n\ndef=\n\np X\n\ni=1\n\nlog\n\nreli (i + 1)\n\n2\n\n= rel1 +\n\np X\n\ni=2\n\nlog\n\nreli (i + 1).\n\n2\n\nAn alternative formulation of DCG, commonly used in industry and data science competitions such as Kaggle, places a stronger emphasis on retrieving relevant documents:\n\nDCGp\n\ndef=\n\np X\n\ni=1\n\n2reli − 1 log\n\n(i + 1).\n\n2\n\nFor a query, the normalized discounted cumulative gain (nDCG), is deﬁned as:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\nnDCGp\n\ndef=\n\nDCGp IDCGp\n\n,\n\nwhere IDCG is the ideal discounted cumulative gain,\n\nIDCGp\n\ndef=\n\n|RELp| X\n\ni=1\n\n2reli − 1 log\n\n(i + 1),\n\n2\n\nand RELp represents the list of the documents relevant to the query in the corpus up to position p (ordered by their relevance). So, RELp is the ideal ranking, up to position p, that the search engine ranking algorithm (or model) should have returned for the query. The nDCG values for all queries are usually averaged to obtain a performance measure for a search engine ranking algorithm or model.\n\nLet’s consider the following example. Let a search engine return a list of documents in response to a search query. We ask a ranker (a human) to judge each document’s relevance. The ranker must assign a score from 0 to 3, where 0 means not relevant, 3 means highly relevant, while 1 and 2 mean “somewhere in between.” Say the documents appeared in this order:\n\nD1,D2,D3,D4,D5.\n\nOur ranker provides the following relevance scores:\n\n3,1,0,3,2.\n\nThis means that document D1 has a relevance of 3, D2 has a relevance of 1, D3 has a relevance of 0, and so on. The cumulative gain of this search result, up to position p = 5, is,\n\nCG5 =\n\n5 X\n\nreli = 3 + 1 + 0 + 3 + 2 = 9.\n\ni=1\n\nYou can see that changing the order of any documents will not aﬀect the value of cumulative gain. Now we will calculate the discounted cumulative gain designed, with the presence of the logarithmic discounting, to have a higher value if highly relevant documents appear early in the for each i: result list. To calculate DCG5, let’s calculate the value of the expression\n\nreli log2(i+1)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\ni\n\n1 2 3 4 5\n\nreli\n\n3 1 0 3 2\n\nlog\n\n(i + 1)\n\n2 1.00 1.58 2.00 2.32 2.58\n\nreli log2(i+1) 3.00 0.63 0.00 1.29 0.77\n\nSo DCG5 of this ranking is given by 3.00 + 0.63 + 0.00 + 1.29 + 0.77 = 5.70. Now, if we switch the positions of D1 and D2, the value of DCG5 will become lower. This is because a less relevant document is now placed higher in the ranking, while a more relevant document is discounted more by being placed in a lower position.\n\nTo calculate the normalized discounted cumulative gain, nDCG5, we ﬁrst need to ﬁnd the value of the discounted cumulative gain of the ideal ordering, IDCG5. The ideal ordering, according to the relevance scores, is 3,3,2,1,0. The value of IDCG5 is then equal to 3.00 + 1.89 + 1.00 + 0.43 + 0.0 = 6.32. Finally, nDCG5 is given by,\n\nnDCG5 =\n\nDCG5 IDCG5\n\n=\n\n5.70 6.32\n\n= 0.90.\n\nTo obtain nDCG for a collection of test queries and the corresponding lists of search results, we average the values of nDCGp obtained for each individual query. The advantage of using the normalized discounted cumulative gain over other measures is that the values of nDCGp obtained for diﬀerent values of p are comparable. This property is useful when the number p of relevance scores, provided by the rankers, is diﬀerent for diﬀerent queries.\n\nNow that we have a performance metric, we can use it to compare models in the process known as hyperparameter tuning.\n\n5.6 Hyperparameter Tuning\n\nHyperparameters play an important role in the model training process. Some hyperparameters inﬂuence the speed of training, but the most important hyperparameters control the two tradeoﬀs: bias-variance and precision-recall.\n\nHyperparameters aren’t optimized by the learning algorithm itself. The data analyst “tunes” hyperparameters by experimenting with combinations of values, one per hyperparameter. Each machine learning model and each learning algorithm have a unique set of hyperparameters. Furthermore, every step in your entire machine learning pipeline, data pre-processing, feature extraction, model training, and making predictions, can have its own hyperparameters.\n\nFor example, in data pre-processing, the hyperparameters could specify whether to use data-augmentation or using which technique to ﬁll missing values. In feature engineering,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24\n\nHyperparameter 1 (e.g. learning rate)Hyperparameter 2 (e.g. number of trees)\n\npairs of discretevalues\n\nFigure 6: Grid search for two hyperparameters: each green circle represents a pair of hyperparameter values.\n\na hyperparameter could deﬁne which feature selection technique to apply. When making predictions with a model that returns a score, a hyperparameter could specify the decision threshold for each class.\n\nBelow, we consider several popular hyperparameter tuning techniques.\n\n5.6.1 Grid Search\n\nGrid search is the simplest hyperparameter tuning technique. It’s used when the number of hyperparameters and their range is not too large.\n\nWe explain it for the problem of tuning two numerical hyperparameters. The technique consists of discretizing each of the two hyperparameters, and then evaluating each pair of discrete values, as shown in Figure 6.\n\nEach evaluation consists of:\n\n1) conﬁguring a pipeline with a pair of hyperparameter values, 2) applying the pipeline to the training data and training a model, and 3) computing the performance metric for the model on the validation data.\n\nThe pair of hyperparameter values that results in the best performing model is then selected\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nfor training the ﬁnal model.\n\nThe Python code below uses grid search with cross-validation.2 It shows how to optimize the hyperparameters of the simple two-stage scikit-learn pipeline considered above:\n\nfrom sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.model_selection import GridSearchCV\n\n# Define a pipeline pipe = Pipeline([('dim_reduction', PCA()), ('model_training', SVC())])\n\n# Define hyperparamer values to try param_grid = dict(dim_reduction__n_components=[2, 5, 10], \\ model_training__C=[0.1, 10, 100])\n\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n# Make a prediction pipe.predict(new_example)\n\nIn the above example, we use grid search to try the values [2,5,10] of the hyperparameter n_components of PCA, and the values [0.1,10,100] of the hyperparameter C of SVM.\n\nTrying multiple combinations of hyperparameters could be time-consuming for large datasets. There are more eﬃcient techniques, such as random search, coarse-to-ﬁne search, and Bayesian hyperparameter optimization.\n\n5.6.2 Random Search\n\nRandom search diﬀers from grid search in that you do not provide a discrete set of values to explore for each hyperparameter. Instead, you provide a statistical distribution for each hyperparameter from which values are randomly sampled. Then set the total number of combinations you want to evaluate, as shown in Figure 7.\n\n2We talk about cross-validation in Subsection 5.6.5.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26\n\nHyperparameter 1Hyperparameter 2\n\nFigure 7: Random search for two hyperparameters and 16 pairs to test.\n\nHyperparameter 1Hyperparameter 2\n\nFigure 8: Coarse-to-ﬁne search for two hyperparameters: 16 coarse random search pairs to test and one grid search in the region of the highest value found using the random search.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "page_number": 151,
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 151-158). Key topics include model, examples, and prediction. A pipeline is a sequence of transformations the training data goes through, before it becomes a model.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "model",
        "Andriy Burkov Machine",
        "machine learning",
        "Learning Engineering",
        "Burkov Machine",
        "regression model",
        "recall",
        "precision",
        "learning",
        "classiﬁcation model",
        "Pipeline",
        "Andriy Burkov",
        "spam"
      ],
      "concepts": [
        "model",
        "examples",
        "prediction",
        "predictions",
        "predicted",
        "classes",
        "recall",
        "data",
        "metric",
        "precision"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 27,
          "title": "Segment 27 (pages 229-236)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 52,
          "title": "Segment 52 (pages 441-448)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 35,
          "title": "Segment 35 (pages 297-305)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 159-167)",
      "start_page": 159,
      "end_page": 167,
      "detection_method": "topic_boundary",
      "content": "5.6.3 Coarse-to-Fine Search\n\nIn practice, analysts often use a combination of grid search and random search called coarse- to-ﬁne search. This technique uses a coarse random search to ﬁrst ﬁnd the regions of high potential. Then, using a ﬁne grid search in these regions, one ﬁnds the best values for hyperparameters, as shown in Figure 8.\n\nYou can decide to only explore one high-potential region or several such regions, depending on the available time and computational resources.\n\n5.6.4 Other Techniques\n\nBayesian techniques diﬀer from random and grid searches in that they use past evaluation results to choose the next values to evaluate. In practice, this allows Bayesian hyperparameter optimization techniques to ﬁnd better values of hyperparameters in less time.\n\nThere are also gradient-based techniques, evolutionary optimization techniques, and other algorithmic hyperparameter tuning methods. Most modern machine learning libraries imple- ment one or more such techniques. There are also hyperparameter tuning libraries that can be used to tune hyperparameters of virtually any learning algorithm, including the algorithms you programmed yourself.\n\n5.6.5 Cross-Validation\n\nGrid search and other techniques of hyperparameter tuning discussed above are used when you have a good-sized validation set.3 When you don’t, a common technique of model evaluation is cross-validation. Indeed, when you have few training examples, it could be prohibitive to have both validation and test sets. You would prefer to use more data to train the model. In such a case, you should only split your data in two: a training and a test set. Then use cross-validation on the training set to simulate a validation set.\n\nCross-validation works as follows. First, you ﬁx the values of the hyperparameters to evaluate. Then you split your training set into several subsets of the same size. Each subset is called a fold. Typically, ﬁve-fold cross-validation is used, and you randomly split your training data into ﬁve folds: {F1,F2,...,F5}. Each Fk, k = 1,...,5, contains 20% of your training data. Then you train ﬁve models in a speciﬁc manner. To train the ﬁrst model, f1, you use all examples from folds F2, F3, F4, and F5 as the training set, and the examples from F1 as the validation set. To train the second model, f2, you use the examples from folds F1, F3, F4, and F5 to train, and the examples from F2 to validate. You continue training models fk iteratively4 for all remaining folds, and compute the value of the metric of interest on each\n\n3A decent validation set contains at least a hundred examples, and each class in the set is represented by\n\nat least a couple of dozen examples.\n\n4The process of cross-validation is easier to illustrate as an iterative process; though, one can, of course,\n\nbuild all ﬁve models F1 to F5 in parallel.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28\n\nvalidation set, from F1 to F5. Then you average the ﬁve values of the metric to get the ﬁnal value. More generally, in n-fold cross-validation, you train model fn on all folds, except for the n-th fold Fn. You can use grid search, random search, or any other such technique with cross-validation to ﬁnd the best values of hyperparameters. Once you have found those values, you typically use the entire training set to train the ﬁnal model by using the best values of hyperparameters found via cross-validation. Finally, you assess the ﬁnal model using the test set.\n\nWhile ﬁnding the best values of hyperparameters is tempting, it might be unrealistic to try all of them. Remember that time is precious, and perfect is often an enemy of good. Deploy a “good enough” model to production, then continue to run the search of the ideal values for hyperparameters (for weeks if it is what it takes).\n\nNow, let’s consider the challenge of training a shallow model.\n\n5.7 Shallow Model Training\n\nShallow models make predictions based directly on the values in the input feature vector. Most popular machine learning algorithms produce shallow models. The only kind of deep models commonly used are deep neural networks. We consider a strategy to train them in Section ?? of the next chapter.\n\n5.7.1 Shallow Model Training Strategy\n\nA typical model training strategy for shallow learning algorithms looks as follows:\n\n1. Deﬁne a performance metric P. 2. Shortlist learning algorithms. 3. Choose a hyperparameter tuning strategy T. 4. Pick a learning algorithm A. 5. Pick a combination H of hyperparameter values for algorithm A using strategy T. 6. Use the training set and train a model M using algorithm A parametrized with hyperparameter values H.\n\n7. Use the validation set and calculate the value of metric P for model M. 8. Decide: a. If there are still untested hyperparameter values, pick another combination H of\n\nhyperparameter values using strategy T and go back to step 6.\n\nb. Otherwise, pick a diﬀerent learning algorithm A and go back to step 5, or proceed\n\nto step 9 if there are no more learning algorithms to try.\n\n9. Return the model for which the value of metric P is maximized.\n\nIn the above strategy, step 1, you deﬁne the performance metric for your problem. As we have seen in Section 5.5, it is a mathematical function or a subroutine that takes a model and a dataset as input, and produces a numerical value that reﬂects how well the model works.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n1\n\n2\n\n3\n\n4\n\n5\n\nIn step 2, you choose candidate algorithms and then shortlist some of them (usually, two or three). To do that, you can use the selection criteria considered in Section 5.3.\n\nIn step 3, you choose a hyperparameter tuning strategy. It is a sequence of actions that generates the combinations of hyperparameter values to test. We have considered several hyperparameter-tuning strategies in Section 5.6.\n\n5.7.2 Saving and Restoring the Model\n\nOnce you trained a model or a pipeline, you must save it to a ﬁle so that it can be deployed to production and then used for scoring. Both model and pipeline can be serialized. In Python, Pickle is typically used for serialization (saving) and deserialization (restoring) of objects. In R, it’s RDS.\n\nHere’s how model serialization/deserialization is done in Python:\n\nimport pickle from sklearn.svm import SVC from sklearn import datasets\n\n# Prepare data X, y = datasets.load_iris(return_X_y=True)\n\n# Instantiate the model model = SVC()\n\n# Train the model model.fit(X, y)\n\n# Save the model to file pickle.dump(model, open(\"model_file.pkl\", \"wb\"))\n\n# Restore the model from file restored_model = pickle.load(open(\"model_file.pkl\", \"rb\"))\n\n# Make a prediction prediction = restored_model.predict(new_example)\n\nA similar code in R would look as follows:\n\nlibrary(\"e1071\")\n\n# Prepare data attach(iris) X <- subset(iris, select=-Species)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\ny <- Species\n\n# Train the model model <- svm(X,y)\n\n# Save the model to file saveRDS(model, \"./model_file.rds\")\n\n# Restore the model from file restored_model <- readRDS(\"./model_file.rds\")\n\n# Make a prediction prediction <- predict(restored_model, new_example) Now, let’s talk about the particularities of the model training process that analysts must take care of in practice to produce an optimal model.\n\n5.8 Bias-Variance Tradeoﬀ\n\nDeveloping a model includes both searching for an optimal algorithm, as well as ﬁnding the best performing hyperparameters. Tweaking the hyperparameters actually controls two tradeoﬀs. We already discussed the ﬁrst one: the precision-recall tradeoﬀ. The second one, equally important, is the bias-variance tradeoﬀ.\n\n5.8.1 Underﬁtting\n\nThe model is said to have a low bias if it ablely predicts the training data labels. If the model makes too many mistakes on the training data, we say that it has a high bias, or that the model underﬁts the training data. There could be several reasons for underﬁtting:\n\nthe model is too simple for the data (for example linear models often underﬁt); • the features are not informative enough; • you regularize too much (we talk about regularization in the next section).\n\nAn example of underﬁtting in regression is shown in Figure 9 (left). The regression line doesn’t repeat the bends of the line to which the data seemingly belongs. The model oversimpliﬁes the data. The possible solutions to the problem of underﬁtting include:\n\ntrying a more complex model, • engineering features with higher predictive power, • adding more training data, when possible, and • reducing regularization.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31\n\n4\n\n20\n\n10\n\n0\n\n10\n\n2\n\n0\n\n2\n\ntraining examples\n\n30\n\ndegree 1 (underfit)\n\ntraining examples\n\ndegree 2 (fit)\n\n30\n\n10\n\n2\n\n0\n\n20\n\n10\n\n4\n\n0\n\n2\n\n0\n\n4\n\n2\n\n10\n\n0\n\n20\n\n30\n\ndegree 15 (overfit)\n\ntraining examples\n\n2\n\n10\n\nUnderﬁtting\n\nGood ﬁt\n\nOverﬁtting\n\nFigure 9: Examples of underﬁtting (linear model), good ﬁt (quadratic model), and overﬁtting (polynomial of degree 15).\n\n5.8.2 Overﬁtting\n\nOverﬁtting is another problem a model can exhibit. The model that overﬁts usually predicts the training data labels very well, but works poorly on the holdout data.\n\nAn example of overﬁtting in regression is shown in Figure 9 (right). The regression line predicts almost perfectly the targets for almost all training examples, but will likely make signiﬁcant errors on new data if you decide to use it for predictions.\n\nYou will ﬁnd another name for overﬁtting in the literature: high variance. The model is unduly sensitive to small ﬂuctuations in the training set. If you sampled the training data diﬀerently, the result would be a signiﬁcantly diﬀerent model. These overﬁtting models perform poorly on the holdout data, since holdout and training data are sampled from the dataset independently of one another. So, the small ﬂuctuations in the training and holdout data are likely to be diﬀerent.\n\nSeveral reasons can lead to overﬁtting:\n\nthe model is too complex for the data. Very tall decision trees or a very deep neural network often overﬁt;\n\nthere are too many features and few training examples; and • you don’t regularize enough.\n\nSeveral solutions to overﬁtting are possible:\n\nuse a simpler model. Try linear instead of polynomial regression, or SVM with a linear kernel instead of radial basis function (RBF), or a neural network with fewer layers/units;5\n\n5While reducing the number of model parameters is generally recommended to reduce overﬁtting and improve the generalization of the model, the phenomenon of deep double descent sometimes proves otherwise. The phenomenon was observed in various architectures, including CNN and transformers: validation performance ﬁrst improves, then gets worse, and then improves again with increasing model size.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32\n\nreduce the dimensionality of examples in the dataset; • add more training data, if possible; and, • regularize the model.\n\n5.8.3 The Tradeoﬀ\n\nIn practice, by trying to reduce variance, you increase bias, and vice versa. In other words, reducing overﬁtting leads to underﬁtting, and the other way around. This is called the bias-variance tradeoﬀ: by trying too hard to build a model that performs perfectly on the training data, you end up with a model that performs poorly on the holdout data.\n\nWhile many factors determine whether the model performs well on the training data, the most important factor is the complexity of the model. A suﬃciently complex model will learn to memorize all training examples and their labels and, thus, will not make prediction errors when applied to the training data. It will have low bias. However, a model relying on memorization will not be able to correctly predict labels of previously unseen data. It will have high variance.\n\nAs the model complexity grows, the typical evolution of the average prediction error of a model when applied to the training and holdout data is shown in Figure 10.\n\nThe zone you would like to be in is the “zone of solutions,” the light-blue rectangle where both bias and variance are low. Once in this zone, you can ﬁne-tune the hyperparameters to reach the needed precision-recall ratio, or optimize another model performance metric appropriate for your problem.\n\nTo reach the zone of solutions, you can either,\n\nmove to the right by increasing the complexity of the model, and, by so doing, reducing its bias, or\n\nmove to the left by regularizing the model to reduce variance by making the model simpler (we talk about regularization in the next section).\n\nIf you work with shallow models, like linear regression, you can increase the complexity by switching to higher-order polynomial regression. Similarly, you can increase the depth of the decision tree, or use polynomial or RBF kernels, in support vector machine (SVM) instead of the linear kernel. Ensemble learning algorithms, based on the idea of boosting, allow bias reduction by combining several (usually, hundreds of) high-bias “weak” models.\n\nAs of July 2020, we don’t yet fully understand why it happens.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33\n\naverageerror\n\nlowermodel complexityhighermodel complexity\n\nholdouterrortrainingerrorhigh bias(underﬁtting)high variance(overﬁtting)zone ofsolutions\n\nFigure 10: Bias-variance tradeoﬀ.\n\nIf you work with neural networks, you can increase the model’s complexity by increasing its size: the number of units per layer, and the number of layers. Training a neural network model longer (i.e., for more epochs) also usually results in lower bias. The advantage of using neural networks, with respect to the bias-variance tradeoﬀ, is that you can slightly increase the size of the network and observe a slight decrease in bias. Most popular shallow models and the associated learning algorithms cannot provide you such ﬂexibility.\n\nIf, by increasing the complexity of your model, you ﬁnd yourself in the right-hand side of the graph in Figure 10, you have to reduce the variance of the model. The most common way to do that is to apply regularization.\n\n5.9 Regularization\n\nRegularization is an umbrella term for methods that force a learning algorithm to train a less complex model. In practice, it leads to higher bias, but signiﬁcantly reduces the variance.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34\n\nThe two most widely used types of regularization are L1 and L2 regularization. The idea is quite simple. To create a regularized model, we modify the objective function. This is the expression optimized by the learning algorithm when training the model. Regularization adds a penalizing term whose value is higher when the model is more complex.\n\nFor simplicity, we will illustrate regularization using linear regression, but same principle can be applied to a wide variety of models. Let x be a two-dimensional feature vector (cid:2)x(1),x(2)(cid:3). Recall the linear regression objective:\n\nmin w(1),w(2),b\n\n\" 1 N\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\ndef= f(xi), and f is the equation of the regression line. The equation In the above equation, fi of the linear regression line f will have the form f = w(1)x(1) + w(2)x(2) + b. The learning algorithm will deduce the values of parameters w(1), w(2), and b from the training data by minimizing the objective. A model is considered less complex if some of the parameters w(·) are close to or equal to zero.\n\n5.9.1 L1 and L2 Regularization\n\nAn L1-regularized objective in Equation 3 looks like this:\n\nmin w(1),w(2),b\n\n\"\n\nC ×\n\n(cid:16)\n\n|w(1)| + |w(2)|\n\n(cid:17)\n\n+\n\n1\n\nN\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\nwhere C is a hyperparameter that controls the importance of regularization. If we set C to zero, the model becomes a standard non-regularized linear regression model. On the other hand, if we set C to a high value, the learning algorithm will try to set most w(·) to a value close or equal to zero to minimize the objective. The model will become very simple, which can lead to underﬁtting. The role of the data analyst is to ﬁnd such a value of the hyperparameter C that doesn’t increase the bias too much, but reduces the variance to a level reasonable for the problem at hand.\n\nAn L2-regularized objective in our two-dimensional setting looks like this:\n\nmin w(1),w(2),b\n\n\"\n\nC ×\n\n(cid:16)\n\n(w(1))2 + (w(2))2(cid:17)\n\n+\n\n1\n\nN\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\nIn practice, L1 regularization produces a sparse model, assuming the value of hyperpa- rameter C is great enough. This is a model where most of its parameters equal exactly zero. So, as discussed in the previous chapter, L1 implicitly performs feature selection by\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(3)\n\n(4)\n\n(5)\n\n35\n\ndeciding which features are essential for prediction, and which are not. This property of L1 regularization is useful when we want to increase model explainability. However, if our goal is to maximize the model performance on the holdout data, then L2 usually gives better results.\n\nIn the literature, you will also see the names lasso for L1 and ridge regularization for L2.\n\n5.9.2 Other Forms of Regularization\n\nL1 and L2 regularization methods can be combined in what’s called elastic net regularization.\n\nIn addition to being widely used with linear models, L1 and L2 are often used with neural networks and many other types of models that directly minimize an objective function.\n\nNeural networks can also beneﬁt from two other regularization techniques: dropout and batch-normalization. There are also non-mathematical methods that have a regularization eﬀect: data augmentation and early stopping. We will talk about these techniques in more detail in the next chapter, when we consider training neural networks.\n\n5.10 Summary\n\nBefore starting to work on the model, you should make several checks and decisions. First, make sure that the data conforms to the schema, as deﬁned by the schema ﬁle. Then, deﬁne an achievable level of performance, and choose a performance metric. Ideally, it should represent the model performance as a single number. Furthermore, it is important to establish a baseline that provides a reference point to compare your machine learning models. Finally, split your data into three sets: train, validation, and test.\n\nMost modern implementations of classiﬁcation learning algorithms require that the training examples have numerical labels, so you typically must transform your labels into numerical vectors. Two popular ways to do that are one-hot encoding (for binary and multiclass problems) and bag-of-words (for multi-label problems).\n\nTo choose a machine learning algorithm that would work best for your problem, ask yourself the following questions:\n\nDo the model’s predictions have to be explainable to a non-technical audience? If yes, you would prefer using less accurate, but more explainable algorithms, such as kNN, linear regression, and decision tree learning.\n\nCan your dataset be fully loaded into the RAM of your laptop or server? If not, you would prefer incremental learning algorithms.\n\nHow many training examples do you have in your dataset, and how many features does each example have? Some algorithms, including those used for training neural networks and random forests, can handle a huge number of examples and millions of features. Others are relatively modest in their capacity.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36",
      "page_number": 159,
      "chapter_number": 19,
      "summary": "This chapter covers segment 19 (pages 159-167). Key topics include models, value, and searches.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "Machine Learning",
        "search",
        "Learning Engineering",
        "Burkov Machine",
        "hyperparameter",
        "Grid search",
        "model",
        "Learning",
        "Andriy Burkov",
        "relevant documents",
        "random search",
        "relevant"
      ],
      "concepts": [
        "models",
        "value",
        "searches",
        "technique",
        "ranking",
        "learning",
        "training",
        "important",
        "results",
        "validation"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "Segment 28 (pages 240-247)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 168-176)",
      "start_page": 168,
      "end_page": 176,
      "detection_method": "topic_boundary",
      "content": "Is your data linearly separable, or can it be modeled using a linear model? If yes, SVM with the linear kernel, linear and logistic regression, can be good choices. Otherwise, deep neural networks or ensemble models might work better.\n\nHow much time is a learning algorithm allowed to use to train a model? Neural networks are known to be slow to train. Simple algorithms like linear and logistic regression, or decision trees are much faster.\n\nHow fast must the scoring perform in production? Models like SVM, linear and logistic regression, as well as not very deep feedforward neural networks, are extremely fast at the prediction time. The scoring using deep and recurrent neural networks, as well as gradient boosting models, is slower.\n\nIf you don’t want to guess the best algorithm for your problem, a recommended approach is to spot-check several algorithms, and then test them on the validation set as a hyperparameter.\n\nA typical way to know how good is the model, is to calculate the value of a performance metric on the holdout data. There are performance metrics deﬁned for classiﬁcation and regressions models, as well as for ranking models.\n\nTweaking the values of hyperparameters controls two tradeoﬀs: precision-recall and bias- variance. By varying the complexity of the model, we can reach the so-called “zone of solutions,” a situation in which both bias and variance of the model are relatively low. The solution that optimizes the performance metric is usually found inside that zone.\n\nRegularization is an umbrella term for methods that force the learning algorithm to build a less complex model. In practice, that often leads to slightly higher bias, but signiﬁcantly reduces the variance. Two popular techniques of regularization are L1 and L2. In addition, neural networks beneﬁt from two other regularization techniques: dropout and batch normalization.\n\nMost modern machine learning packages and frameworks support the notion of a pipeline. A pipeline is a sequence of transformations the training data undergoes before it becomes a model. In a pipeline, each stage applies some transformation to the input it receives. Every stage receives the output of the previous stage, except for the ﬁrst stage. The ﬁrst stage receives the training dataset as input. The pipeline can be saved to a ﬁle similar to saving a model. It can be deployed to production and used to generate predictions.\n\nHyperparameters aren’t optimized by the learning algorithm itself. A data analyst must “tune” hyperparameters by experimenting with diﬀerent combinations of values. Grid search is the simplest and the most widely used hyperparameter tuning technique. It consists of discretizing the values of hyperparameters, and trying all combinations of values by 1) training a model for each combination of hyperparameters, and 2) computing the performance metric by applying each trained model to the validation set.\n\nA decent validation set contains at least a hundred examples, and each class in the set is represented by at least a couple of dozen examples. When you don’t have a decent validation set to tune your hyperparameters, you can use cross-validation.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n6 Supervised Model Training (Part 2)\n\nIn this second part of our conversation about supervised model training, we consider such topics as training deep models, stacking models, handling imbalanced datasets, distribution shift, model calibration, troubleshooting and error analysis, and other best practices.\n\nCompared to shallow models, the model training strategy for deep neural networks has more moving parts. On the other hand, it’s more principled and better amenable to automation.\n\n6.1 Deep Model Training Strategy\n\nModel training starts with shortlisting several network architectures, also known as network topologies. If you work with image data and you want to build your model from scratch, then a convolutional neural network (CNN) with at least one convolutional layer, followed by a max-pooling layer, and one fully connected layer may be your default topology choice.\n\nIf you work with text or other sequence data, such as time series, you have a choice between a CNN, a gated recurrent neural network (such as Long Short Term Memory, LSTM, or gated recurrent units, GRU), or a Transformer.\n\nInstead of training your model from scratch, you can also start with a pre-trained model. Companies like Google and Microsoft have trained very deep neural networks with architec- tures optimized for image or natural language processing tasks.\n\nAmong the most used pre-trained models for image processing tasks are VGG16 and VGG19 (based on the Visual Geometry Group, VGG, architecture), InceptionV3 (based on the GoogLeNet architecture), and ResNet50 (based on the residual network architecture).\n\nFor natural language text processing, such pre-trained models as Bi-directional Encoder Representations from Transformer, BERT, (based on the Transformer architecture) and Em- beddings from Language Models, ELMo (based on the bi-directional LSTM architecture) often improve the quality of the model, compared to training a model from scratch.\n\nAn advantage of using pre-trained models is that these were trained on huge quantities of data available to its creators, but likely unavailable to you. Even if your dataset is smaller and not exactly similar to the one used to pre-train the model, the parameters learned by the pre-trained models may still be useful.\n\nYou can use a pre-trained model in two ways:\n\n1) use its learned parameters to initialize your own model, or 2) use the pre-trained model as a feature extractor for your model.\n\nIf you use the pre-trained model the former way, it gives you more ﬂexibility. The downside is you end up training a very deep neural network. That requires signiﬁcant computational\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nresources. In the latter case, you “freeze” the parameters of the pre-trained model and only train the parameters of added layers.\n\n6.1.1 Neural Network Training Strategy\n\nUsing an existing model to create a new model is called transfer learning. We will talk more on this topic in Section 6.1.10. For the moment, assume you are building a model from scratch, based on the architecture of your choice. A common strategy to build a neural network looks as follows:\n\n1. Deﬁne a performance metric P. 2. Deﬁne the cost function C. 3. Pick a parameter-initialization strategy W. 4. Pick a cost-function optimization algorithm A. 5. Choose a hyperparameter tuning strategy T. 6. Pick a combination H of hyperparameter values using the tuning strategy T. 7. Train model M, using algorithm A, parametrized with hyperparameters H, to optimize cost function C.\n\n8. If there are still untested hyperparameter values, pick another combination H of hyperparameter values using strategy T, and repeat step 7.\n\n9. Return the model for which the metric P was optimized.\n\nNow let’s discuss some of the steps of the above strategy in detail.\n\n6.1.2 Performance Metric and Cost Function\n\nStep 1 is similar to step 1 of the shallow model training strategy (Section ??): we deﬁne a metric that would allow comparing the performance of two models on the holdout data, and select the better of the two. An example of a performance metric is F-score or Cohen’s kappa.\n\nIn step 2, we deﬁne what our learning algorithm will optimize in order to train a model. If our neural network is a regression model, then, in most cases, the cost function is the mean squared error (MSE) deﬁned in Equation ?? in the previous chapter. Let’s repeat it here:\n\nMSE(f) def=\n\n1\n\nN\n\nX\n\ni=1...N\n\n(f(xi) − yi)2.\n\nFor classiﬁcation, a typical choice for the cost function is either categorical cross-entropy (for multiclass classiﬁcation) or binary cross-entropy (for binary and multi-label classiﬁca- tion).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nRecall that when we train a neural network for multiclass classiﬁcation, we should represent labels using the one-hot encoding. Let C be the number of classes in our classiﬁcation problem. Let yi be a one-hot encoded label of example i, where i spans from 1 to N. Let yi,j denote the value in position j (where j spans from 1 to C) in example i. The categorical cross-entropy loss for classiﬁcation of example i is deﬁned as,\n\nCCEi\n\ndef= −\n\nC X\n\n[yi,j × log\n\n2\n\n(ˆyi,j)],\n\nj=1\n\nwhere ˆyi is the C-dimensional vector of prediction issued by the neural network for the input xi. The cost function is typically deﬁned as the sum of losses of individual examples:\n\nCCE def=\n\nN X\n\nCCEi .\n\ni=1\n\nIn binary classiﬁcation, the output of the neural network for the input feature vector xi, is a single value ˆyi, while the label of the example is a single value yi, just like in logistic regression. The binary cross-entropy loss for classiﬁcation of example i is deﬁned as,\n\nBCEi\n\ndef= −yi × log\n\n2\n\n(ˆyi) − (1 − yi) × log\n\n2\n\n(1 − ˆyi).\n\nSimilarly, the cost function for classiﬁcation of the training set is typically deﬁned as the sum of losses of individual examples:\n\nBCE def=\n\nN X\n\nBCEi .\n\ni=1\n\nBinary cross-entropy is also used in multi-label classiﬁcation. The labels are now C- dimensional bag-of-words vectors yi, while the predictions are C-dimensional vectors ˆyi, whose values ˆyi,j in each dimension j range between 0 and 1. The loss for the prediction of one label ˆyi is deﬁned as,\n\nBCEMi\n\ndef=\n\nC X\n\n[−yi,j × log\n\n2\n\n(ˆyi,j) − (1 − yi,j) × log\n\n2\n\n(1 − ˆyi,j)].\n\nj=1\n\nThe cost function for the classiﬁcation of the entire training set is typically deﬁned as the sum of losses of individual examples,\n\nBCEM def=\n\nN X\n\nBCEMi .\n\ni=1\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nNote that the output layers in multiclass and multi-label classiﬁcation are diﬀerent. In multiclass classiﬁcation, one softmax unit is used. It generates a C-dimensional vector whose values are bounded by the range (0,1), and whose sum equals 1. In multi-label classiﬁcation, the output layer contains C logistic units whose values also lie in the range (0,1), but their sum lies in the range (0,C).\n\nNeural Network Output\n\nThe curious reader may wish to better understand the logic behind choosing a speciﬁc loss function. This block will mathematically describe the output of a neural network.\n\nIn regression, the output layer contains only one unit. If the output value can be any number, from minus inﬁnity to inﬁnity, then the output unit will not contain non-linearity. On the other hand, if the neural network must predict a positive number, then the ReLU (rectiﬁed linear unit) non-linearity can be used. Let the output value of the output unit before non-linearity for the input example i be denoted as zi. Then the output after applying the ReLU non-linearity is given by max(0,zi). In a binary classiﬁcation, the output layer contains only one logistic unit. Let the output value of the output unit before non-linearity for the input example i be denoted as zi. The output ˆyi after applying the logistic nonlinearity is given by,\n\nˆyi\n\ndef=\n\n1 1 + e−zi\n\n,\n\nwhere e is the base of the natural logarithm, also known as Euler’s number.\n\nBinary and multi-label classiﬁcation models are deﬁned in a similar way. The only diﬀerence is that in multi-label classiﬁcation, the output layer contains C logistic units, one per class. If ˆyi,j denotes the output, after nonlinearity, of the logistic unit for class j, when input example is i, then the sum of ˆyi,j, for all j = 1,...,C, lies between 0 and C.\n\nIn the multiclass classiﬁcation, the output layer also produces C outputs. However, in this case, the output of each unit of the output layer is controlled by the softmax function. Let the output of the output unit j, before nonlinearity, for the input example i, be zi,j. Then the output ˆyi,j after nonlinearity is given by,\n\nˆyi,j\n\ndef=\n\nezi,j k=1 ezi,k\n\nPC\n\n.\n\nThe sum of ˆyi,j, for all j = 1,...C, equals 1.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\n6.1.3 Parameter-Initialization Strategies\n\nIn step 3, we select a parameter-initialization strategy. Before the training starts, the parameter values in all units are unknown. We must initialize them with some values. Training algorithms for neural networks, such as gradient descent and its stochastic variants that we consider in a few moments, are iterative in nature and require the analyst to specify some initial point from which to begin the iterations. This initialization might aﬀect the properties of the training model. You will likely choose from one of these strategies:\n\nones — all parameters are initialized to 1; • zeros — all parameters are initialized to 0; • random normal — parameters are initialized to values sampled from the normal distribution, typically with mean of 0 and standard deviation of 0.05;\n\nrandom uniform — parameters are initialized to values sampled from the uniform distribution with the range [−0.05,0.05];\n\nXavier normal — parameters are initialized to values sampled from the truncated normal distribution, centered on 0, with standard deviation equal to p2/(in + out) where “in” is the number of units in the preceding layer to which the current unit is connected (the one whose parameters you initialize); and “out” is the number of units on the subsequent layer to which the current unit is connected; and,\n\nXavier uniform — parameters are initialized to values sampled from a uniform distribution within [−limit,limit], where “limit” is p6/(in + out), and “in” and “out” are deﬁned as in Xavier normal, above.\n\nThere are other initialization strategies. If you work with a neural network training module such as TensorFlow, Keras, or PyTorch, they provide some parameter initializers, and also recommend default choices.\n\nThe bias term is usually initialized with a zero.\n\nWhile we know the parameter initialization aﬀects the model properties, we cannot predict which strategy will provide the best result for your problem. Random and Xavier initializers are the most common. It’s recommended to start your experiments with one of those two.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\nFigure 1: A local and a global minima of a function.\n\n6.1.4 Optimization Algorithms\n\nIn step 4, we select a cost-function optimization algorithm. When the cost function is diﬀerentiable (and it’s the case for all cost functions we considered above) gradient descent and stochastic gradient descent are two most frequently used optimization algorithms.\n\nGradient descent is an iterative optimization algorithm for ﬁnding a local minimum of any diﬀerentiable function. We say that f(x) has a local minimum at x = c if f(x) ≥ f(c) for every x in some open interval around x = c. An interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. An open interval does not include its endpoints and is denoted using parentheses. For example, (0,1) means “all numbers greater than 0 and less than 1.” The minimal value among all the local minima is called the global minimum. The diﬀerence between a local and a global minimum of a function is shown in Figure 1.\n\nFunctions and optimization\n\nIn this block, for the curious reader, we explain the basics of mathematical function and function optimization. If you only want to know the mechanics of training neural networks, you can safely skip it.\n\nA function is a relation that associates each element x of a set X, the domain of the function, to a single element y of another set Y, the codomain of the function. A function usually has a name. If the function is called f, this relation is denoted y = f(x), read “y equals f of x.” The element x is the argument, or input of the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nfunction, and y is the value of the function, or the output. The symbol that is used for representing the input is the variable of the function. We often say that f is a function of the variable x.\n\nA derivative f0 of a function f is a function or a value that describes how fast f increases or decreases. If the derivative is a constant value, like 5 or −3, then the function increases or decreases constantly, at any point x of its domain. If the derivative f0 is itself a function, then the function f can grow at a diﬀerent pace in diﬀerent regions of its domain. If the derivative f0 is positive at some point x, then the function f increases at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function neither decreases nor increases at x; the function’s slope at x is horizontal.\n\nThe process of ﬁnding a derivative is called diﬀerentiation.\n\nDerivatives for basic functions are known. For example if f(x) = x2, then f0(x) = 2x; if f(x) = 2x then f0(x) = 2; if f(x) = 2 then f0(x) = 0. The derivative of any function f(x) = c, where c is a constant value, is zero.\n\nIf the function we want to diﬀerentiate is not basic, we can ﬁnd its derivative using the chain rule. For instance if F(x) = f(g(x)), where f and g are some functions, then F 0(x) = f0(g(x))g0(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we ﬁnd F 0(x) = 2(5x + 1)g0(x) = 2(5x + 1)5 = 50x + 10.\n\nGradient is the generalization of derivatives for functions that take several inputs, or one input in the form of a vector or some other complex structure. A gradient of a function is a vector of partial derivatives. Finding a partial derivative of a function is the process of ﬁnding the derivative by focusing on one of the function’s inputs and considering all other inputs as constant values.\n\nFor example, if our function is deﬁned as f([x(1),x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as\n\n∂f ∂x(1)\n\n, is given by,\n\n∂f ∂x(1)\n\n= a + 0 + 0 = a,\n\nwhere a is the derivative of the function ax(1). The two zeros are respectively derivatives of bx(2) and c, because x(2) is considered constant when we calculate the derivative with respect to x(1), and the derivative of any constant is zero.\n\nSimilarly, the partial derivative of function f with respect to x(2),\n\n∂f ∂x(2)\n\n, is given by,\n\n∂f ∂x(2)\n\n= 0 + b + 0 = b.\n\nThe gradient of function f, denoted as ∇f is given by the vector [ ∂f\n\n∂x(1), ∂f\n\n∂x(2)\n\n].\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "page_number": 168,
      "chapter_number": 20,
      "summary": "This chapter covers segment 20 (pages 168-176). Key topics include model, data, and regularize. In Python, Pickle is typically used for serialization (saving) and deserialization (restoring) of objects.",
      "keywords": [
        "Model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Andriy Burkov Machine",
        "Machine Learning",
        "training data",
        "data",
        "training",
        "Learning Engineering",
        "Burkov Machine",
        "Learning",
        "regularization",
        "learning algorithm",
        "machine learning models",
        "Andriy Burkov"
      ],
      "concepts": [
        "model",
        "data",
        "regularize",
        "regularization",
        "regularized",
        "prediction",
        "predict",
        "predictions",
        "trained",
        "bias"
      ],
      "similar_chapters": [
        {
          "book": "Effective-Python",
          "chapter": 32,
          "title": "Segment 32 (pages 336-344)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 23,
          "title": "Segment 23 (pages 232-240)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 58,
          "title": "Segment 58 (pages 491-498)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 177-184)",
      "start_page": 177,
      "end_page": 184,
      "detection_method": "topic_boundary",
      "content": "Figure 2: The inﬂuence of the learning rate on the convergence: (a) too small, the convergence will be slow; (b) too large, no convergence; (c) the right value of learning rate.\n\nThe chain rule works with partial derivatives too.\n\nTo ﬁnd a local minimum of a function using gradient descent, we start at some random point in the domain of the function. Then we move proportionally to the negative of the gradient (or approximate gradient) of the function at the current point.\n\nGradient descent in machine learning proceeds in epochs. An epoch consists of using the training set entirely to update each parameter. In the ﬁrst epoch, we initialize the parameters of our neural network using one of the parameter-initialization strategies discussed above. The backpropagation algorithm computes the partial derivatives of each parameter using the chain rule for derivatives of complex functions.1 At each epoch, gradient descent updates all parameters using partial derivatives. The learning rate controls the signiﬁcance of an update. The process continues until convergence, the state when the values of parameters don’t change much after each epoch. Then the algorithm stops.\n\nGradient descent is sensitive to the choice of the learning rate α. Picking the right learning rate for your problem is not easy. If you select a value that is too high, you might not reach convergence at all. On the other hand, too small values of α can slow down the learning to the point of no observable progress. In Figure 2, you can see an illustration of gradient descent for one parameter of a neural network and three values of the learning rate. The value of the parameter at each iteration is shown as a blue circle. The number inside the\n\n1The explanation of backpropagation is beyond the scope of this book. You should only know that every modern software library for training neural networks contains an implementation of this algorithm. The curious reader can ﬁnd the explanation of backpropagation in the extended version of The Hundred-Page Machine Learning Book on its companion wiki.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\ncircle indicates the epoch. The red arrows indicate the direction of the gradient along the horizontal axis — the direction away from the minimum. The green arrows show the change in the value of the cost function after each epoch.\n\nTherefore, at each epoch, gradient descent moves the parameter value towards the minimum. If the learning rate is too small, the movement towards the minimum will be very slow (Figure 2a). If the learning rate is too large, the value of the parameter will oscillate away from the minimum (Figure 2b).\n\nGradient descent is rather slow for large datasets because it uses the entire dataset to compute the gradient of each parameter at each epoch. Fortunately, several signiﬁcant improvements to this algorithm have been proposed.\n\nMinibatch stochastic gradient descent (minibatch SGD) is a variant of the gradient descent algorithm. It approximates the gradient using small subsets of the training data called minibatches. This eﬀectively speeds up the computation. The size of the minibatch is a hyperparameter, and you can tune it. Powers of two, between 32 and a few hundred, are recommended: 32, 64, 128, 256, and so on.\n\nThe problem of choosing a value for the learning rate α is still present in the “vanilla” minibatch SGD. Learning can still stagnate at later epochs. Instead of reaching a local minimum, the gradient descent might keep oscillating around it due to too large updates. There are many learning rate decay schedules that allow updating the learning rate, as the learning progresses, by reducing it later in the epoch count. The beneﬁts of using a learning rate decay schedule include faster gradient descent convergence (faster learning) and higher model quality. Below, we consider several popular learning rate decay schedules.\n\n6.1.5 Learning Rate Decay Schedules\n\nLearning rate decay consists of gradually reducing the value of the learning rate α as the epochs progress. Consequently, the parameter updates become ﬁner. There are several techniques, known as schedules, to control α.\n\nTime-based learning rate decay schedules alter the learning rate depending on the learning rate of the previous epoch. The mathematical formula for the learning rate update, according to a popular time-based learning rate decay schedule, is:\n\nαn ←\n\nαn−1 1 + d × n\n\n,\n\nwhere αn is the new value of the learning rate, αn−1 is the value of the learning at the previous epoch n − 1, and d is the decay rate, a hyperparameter. For example, if the initial value of the learning rate α0 = 0.3, then the values of the learning rate at the ﬁrst ﬁve epochs are shown in the table below:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\nlearning rate\n\nepoch\n\n0.15 0.10 0.08 0.06 0.05\n\n1 2 3 4 5\n\nStep-based learning rate decay schedules change the learning rate according to some pre-deﬁned drop steps. The mathematical formula for the learning rate update, according to a popular step-based learning rate decay schedule, is:\n\nαn ← α0dﬂoor( 1+n\n\nr\n\n)\n\n,\n\nwhere αn is the learning rate at epoch n, α0 is the initial value of the learning rate, d is the decay rate that reﬂects how much the learning rate should change at each drop step (0.5 corresponds to halving), and r is the so-called drop rate deﬁning the length of drop steps (10 corresponds to a drop every 10 epochs). The ﬂoor operator in the above formula equals 0 if the value of its argument is less than 1.\n\nExponential learning rate decay schedules are similar to step-based. However, instead of drop steps, a decreasing exponential function is used. The mathematical formula for the learning rate update, according to a popular exponential learning rate decay schedule, is:\n\nαn ← α0e−d×n\n\nwhere d is the decay rate and e is Euler’s number.\n\nThere are several popular upgrades to minibatch SGD, such as Momentum, Root Mean Squared Propagation (RMSProp), and Adam. These algorithms update the learning rate automatically based on the performance of the learning process. You don’t have to worry about choosing the initial learning rate value, the decay schedule and rate, or other related hyperparameters. These algorithms have demonstrated good performance in practice, and practitioners often use them instead of manually tuning the learning rate.\n\nMomentum helps accelerate minibatch SGD by orienting the gradient descent to the relevant direction, and reducing oscillations. Instead of using only the current gradient’s epoch to guide the search, Momentum accumulates the gradient of past epochs to determine the direction to go. Momentum removes the need to manually adjust the learning rate.\n\nMore recent advancements in neural network cost function optimization algorithms include RMSProp and Adam, the latter being the most recent and versatile. It’s recommended to start training the model with Adam. Then, if the quality of the model doesn’t reach the acceptable level, try a diﬀerent cost function optimization algorithm.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\n6.1.6 Regularization\n\nIn neural networks, besides L1 and L2 regularization, you can use neural network-speciﬁc regularizers: dropout, early stopping, and batch-normalization. The latter is technically not a regularization technique, but it often has a regularization eﬀect on the model.\n\nThe concept of dropout is very simple. Each time you “run” a training example through the network, you temporarily exclude at random some units from the computation. The higher the percentage of units excluded, the stronger the regularization eﬀect. Popular neural network libraries allow you to add a dropout layer between two successive layers, or you can specify the dropout hyperparameter for a layer. The dropout hyperparameter varies in the range [0,1] and characterizes the fraction of units to randomly exclude from computation. The value of the hyperparameter has to be found experimentally. While simple, dropout’s ﬂexibility and regularizing eﬀect are phenomenal.\n\nEarly stopping trains a neural network by saving the preliminary model after every epoch. Models saved after each epoch are called checkpoints. Then it assesses each checkpoint’s performance on the validation set. You’ll ﬁnd during gradient descent that the cost decreases as the number of epochs increases. After some epoch, the model can start overﬁtting, and the model’s performance on the validation data can deteriorate. Remember the bias-variance illustration in Figure ?? in Chapter 5. By keeping a version of the model after each epoch, you can stop the training once you start observing a decreased performance on the validation set. Alternatively, you can keep running the training process for a ﬁxed number of epochs, and then pick the best checkpoint. Some machine learning practitioners rely on this technique. Others try to properly regularize the model using appropriate techniques.\n\nBatch normalization (which rather should be called batch standardization) consists of standardizing the outputs of each layer before the next layer receives them as input. In practice, batch normalization results in faster and more stable training, as well as some regularization eﬀect. So, it’s always a good idea to use batch normalization. In popular neural network libraries, you can often insert a batch normalization layer between two subse- quent layers.\n\nAnother regularization technique that can be applied to any learning algorithm is data augmentation. This technique is often used to regularize models that work with images. In practice, applying data augmentation often results in an increased model performance.\n\n6.1.7 Network Size Search and Hyperparameter Tuning\n\nStep 5 of the deep model training strategy is similar to that in the shallow model training strategy — choose a hyperparameter tuning strategy T.\n\nIt step 6, we pick a combination of hyperparameter values using strategy T. Typical parameters include the size of the minibatch, the value of the learning rate (if you use the vanilla minibatch SGD), or an algorithm that automatically updates the learning rate, such\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\nFigure 3: The neural network model training ﬂowchart.\n\nas Adam. You also decide the initial number of layers and units per layer. It’s recommended to start with something reasonable that would allow us to build the ﬁrst model fast enough. For example, two hidden layers and 128 units per layer might be a good starting point.\n\nStep 7 reads, “Build the training model M, using algorithm A, parametrized with hyper- parameters H, to optimize the cost function C.” This is the main diﬀerence with shallow learning. When you work with a shallow learning algorithm or a model, you can only tweak some built-in hyperparameters. You don’t have much control over the model architecture and complexity. With neural networks, you have all the control, and training a model is more a process than a single action. To build a deep model, you start with a reasonably-sized model, and then you follow the ﬂowchart shown in Figure 3.\n\nObserve that you start with some model, and then increase its size until it ﬁts the training data well. Then you evaluate the model on the validation data. If it performs well, according to the performance metric, you stop and return the model. Otherwise, you regularize and retrain the model.\n\nAs we have seen, regularization in neural networks is usually achieved in several ways. The most eﬀective is dropout, where you randomly remove some units from the network and make it simpler and “dumber.” A simpler model would work better on the holdout data, and this is your goal.\n\nSuppose, after several loops of regularization and model retrains, you don’t see any improve- ment in the model performance on the validation data. Check if it still ﬁts the training data. If it doesn’t, increase the size of the model, by increasing the size of individual layers, or by adding another layer. Continue until the model ﬁts the training data again. Then evaluate it again on the validation data. The process continues until a larger model doesn’t result in better validation data performance, no matter your actions. Then you stop and return the model, if validation data performance is satisfactory.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\nIf you are not satisﬁed with this performance, you can pick a diﬀerent combination of hyperparameters for step 8, and build a diﬀerent model. You will continue to test diﬀerent values of hyperparameters until there are no more values to test. Then you keep the best model among those you trained in the process. If the performance of the best model is still not satisfactory, try a diﬀerent network architecture, add more labeled data, or try transfer learning. We talk more on transfer learning in Section 6.1.10.\n\nThe properties of a trained neural network depend a lot on the choice of the values of hyperparameters. But before you choose speciﬁc values of hyperparameters, train a model, and validate its properties on the validation data, you must decide which hyperparameters are important enough for you to spend the time on.\n\nObviously, if you had inﬁnite time and computing resources, you would tune all hyperpa- rameters. However, in practice, you have ﬁnite time and, often, relatively modest resources. Which hyperparameters to tune?\n\nWhile there is no deﬁnitive answer to that question, there are several observations that might help you in choosing the hyperparameters to tune when you work on a speciﬁc model:\n\nyour model is more sensitive to some hyperparameters than to others; and • the choice is often between using the default value of a hyperparameter or changing it.\n\nThe libraries for training neural networks often come with default values for hyperparameters: stochastic gradient descent version (often, Adam), the parameter initialization strategy (often, random normal or random uniform), minibatch size (often, 32), and so on. Those defaults were chosen based on observations from practical experience. Open-source libraries and modules are often the fruit of the collaboration of many scientists and engineers. These talented and experienced people established “good” defaults for many hyperparameters when working with various datasets and practical problems.\n\nIf you decide to tune a hyperparameter, as opposed to using the default value, it makes more sense to tune the hyperparameters to which the model is sensitive. Table 1 shows2 several hyperparameters and approximate sensitivity of a neural network to those hyperparameters.\n\n6.1.8 Handling Multiple Inputs\n\nIn practice, machine learning engineers often work with multimodal data. For example, the input could be an image and a text, and the binary output could indicate whether the text describes the given image.\n\nIt’s hard to adapt shallow learning algorithms to work with multimodal data. For example, you can try to vectorize each input, by applying the corresponding feature engineering method. Then, concatenate two feature vectors to form one wider feature vector. If your image has features [i(1),i(2),i(3)], and your text has features [t(1),t(2),t(3),t(4)], your concatenated feature vector will be [i(1),i(2),i(3),t(1),t(2),t(3),t(4)].\n\n2Taken from the talk “Troubleshooting Deep Neural Networks” by Josh Tobin et al., January 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\nHyperparameter\n\nSensitivity\n\nLearning rate Learning rate schedule Loss function Units per layer Parameter initialization strategy Medium Medium Number of layers Medium Layer properties Medium Degree of regularization Low Choice of optimizer Low Optimizer properties Low Size of minibatch Low Choice of non-linearity\n\nHigh High High High\n\nTable 1: Approximate sensitivity of a model to some hyperparameters.\n\nWith neural networks, you have substantially more ﬂexibility. You can build two subnet- works, one for each input type. For example, a CNN subnetwork reads the image, while an RNN subnetwork reads the text. Both subnetworks have, as their last layer, an embedding. CNN has an image embedding, and RNN has a text embedding. You then concatenate the two embeddings, and ﬁnally add a classiﬁcation layer, such as softmax or logistic sigmoid, on top of the concatenated embeddings.\n\nNeural network libraries provide simple-to-use tools that allow concatenating or averaging layers from several subnetworks.\n\n6.1.9 Handling Multiple Outputs\n\nSometimes, you would like to predict multiple outputs for one input. Some problems with multiple outputs can be eﬀectively converted into a multi-label classiﬁcation problem. Those with labels of the same nature (like tags in social networks), or fake labels can be created as a full enumeration of combinations of original labels.\n\nHowever, in many cases, the outputs are multimodal, and their combinations cannot be eﬀectively enumerated. Consider the following example: you want to build a model that detects an object on an image, and returns its coordinates. In addition, the model has to return a tag describing the object, such as “person,” “cat,” or “hamster.” Your training example will be a feature vector representing an image and a label. The label could be represented as a vector of coordinates of the object, and another vector with a one-hot encoded tag.\n\nFor this, you can create one subnetwork that works as an encoder. It will read the input image using, for example, one or several convolution layers. The encoder’s last layer is the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\nimage embedding. Then you add two other subnetworks on top of the embedding layer: 1) one takes the embedding vector as input, and predicts the coordinates of the object, and 2) the other takes the embedding vector as input, and predicts the tag.\n\nThe ﬁrst subnetwork can have a ReLU as the last layer, which is good for predicting positive real numbers, such as coordinates. This subnetwork can use the mean squared error cost C1. The second subnetwork will take the same embedding vector as input, and will predict the probabilities for each tag. It can have a softmax as the last layer, which is appropriate for the multiclass classiﬁcation, and use the averaged negative log-likelihood cost C2 (also called cross-entropy cost). Alternatively, the coordinates could be in the range [0,1] (in which case the layer that predicts coordinates will have four logistic sigmoid outputs and average four binary cross-entropy cost functions), while the layer that predicts tags might solve a multi-label classiﬁcation problem (in which case it would also have several sigmoid outputs and average several binary cross entropy costs, one per tag).\n\nObviously, you are interested in accurate predictions of both the coordinates and the tags. However, it is impossible to optimize two cost functions at once. By trying to improve one, you risk hurting the other one, and vice-versa. What you can do is add another hyperparameter γ, in the range (0,1), and deﬁne the combined cost function as γ × C1 + (1 − γ) × C2. Then you tune the value for γ on the validation data, just like any other hyperparameter.\n\n6.1.10 Transfer Learning\n\nRecall, transfer learning consists of using a pre-trained model to build a new model. Pre-trained models are usually created using big data available to its creators, usually large organizations, but not necessarily available to you. The parameters learned by the pre-trained models can be useful for your task.\n\nA pre-trained model can be used in two ways:\n\n1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\n\nUsing Pre-Trained Model as Initializer\n\nAs discussed, the choice of parameter initialization strategy aﬀects the properties of the learned model. Pre-trained models, whether available on the Internet, or trained by you, usually perform well for solving the original learning problem.\n\nIf your current problem is similar to the one solved by the pre-trained model, chances are high that the optimal parameters for your current problem will not be too diﬀerent from the pre- trained parameters, especially in the initial neural network layers (those closest to the input).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "page_number": 177,
      "chapter_number": 21,
      "summary": "We will talk more on this topic in Section 6.1.10 Key topics include model, function, and functions. Covers function. If you work with text or other sequence data, such as time series, you have a choice between a CNN, a gated recurrent neural network (such as Long Short Term Memory, LSTM, or gated recurrent units, GRU), or a Transformer.",
      "keywords": [
        "Supervised Model Training",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "function",
        "Model",
        "Andriy Burkov Machine",
        "neural network",
        "model training strategy",
        "Supervised Model",
        "Model Training",
        "output",
        "cost function",
        "Learning Engineering",
        "Machine Learning",
        "Burkov Machine"
      ],
      "concepts": [
        "model",
        "function",
        "functions",
        "training",
        "values",
        "layer",
        "draft",
        "units",
        "strategy",
        "strategies"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 34-41)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 58,
          "title": "Segment 58 (pages 514-521)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 144-164)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 185-194)",
      "start_page": 185,
      "end_page": 194,
      "detection_method": "topic_boundary",
      "content": "inputlayer\n\noutputlayer\n\n...\n\n...\n\n...\n\n...\n\n(a)\n\ninputlayer\n\n...\n\noutputlayer\n\n...\n\n...\n\n...\n\n...\n\n...\n\n(b)\n\n...\n\nFigure 4: An illustration of transfer learning: (a) a pre-trained model and (b) your model, where you used the left part of the pre-trained model, and added new layers, including a diﬀerent output layer tailored for your problem.\n\nThe learning might go faster for your problem because gradient descent will search for the optimal parameter values in a smaller region of potentially good values.\n\nIf the pre-trained model was built using a training set much bigger than yours, searching in a region of potentially good values might also lead to a better generalization. Indeed, if some behavior of the model you want to build is not reﬂected in your training examples, this behavior could still be “inherited” from the pre-trained model.\n\nUsing Pre-Trained Model as Feature Extractor If you use a pre-trained model as an initializer for your model, it gives you more ﬂexibility. The gradient descent will modify the parameters in all layers, and, potentially, reach a better performance for your problem. The downside of that is you will often end up training a very deep neural network.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\nSome pre-trained models contain hundreds of layers and millions of parameters. Training a large network like that can be challenging. It will deﬁnitely require a signiﬁcant amount of computational resources. In addition, the problem of the vanishing gradient is more severe in a deep neural network than one with a couple hidden layers.\n\nIf you have a limited amount of computational resources, you might prefer using some layers of the pre-trained model as feature extractors for your model. In practice, it means that you only keep several initial layers of the pre-trained model, those closest to and including the input layer. You keep their parameters “frozen,” that is, unchanged and unchangeable. Then you add new layers on top of the frozen layers, including the output layer appropriate for your task. Only the parameters of the new layers will be updated by gradient descent during training on your data.\n\nAn illustration of the process is shown in Figure 4. The blue neural network is a pre-trained model. Some of the blue layers are reused in the new model with their parameters frozen; the green layers are added by the analyst and tailored to the problem at hand.\n\nThe analyst might decide to freeze the parameters of the entire blue part of the new network, and only train the parameters of the green part. Alternatively, several right-most blue layers could be set as trainable.\n\nHow many layers of the pre-trained model to use in the new model? Freeze how many layers? This is up to the analyst: it’s part of the decisions you’ll make about the architecture that will work best for your problem.\n\n6.2 Stacking Models\n\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model.\n\n6.2.1 Types of Ensemble Learning\n\nThere are ensemble learning algorithms, such as random forest learning and gradient boosting. They train an ensemble of several hundred to thousands of weak models, and obtain a strong model that has a signiﬁcantly better performance than the performance of each weak model. We will not discuss these algorithms here. If you are missing this knowledge, it can easily be found in a specialized machine learning book.3\n\nThe reason why combining multiple models can bring better performance is that, when several uncorrelated models agree, they are more likely to agree on the correct outcome. The key word here is “uncorrelated.” Ideally, base models should be obtained by using diﬀerent features, or be of a diﬀerent nature — for example, SVM and random forest. Combining\n\n3You can read about ensemble learning algorithms in Chapter 7 of The Hundred-Page Machine Learning\n\nBook.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19\n\ndiﬀerent versions of the decision tree learning algorithm, or several SVMs with diﬀerent hyperparameters, may not result in a signiﬁcant performance boost.\n\nThe goal of ensemble learning is to learn to combine the strengths of each base model. There are three ways to combine weakly correlated models into an ensemble model: 1) averaging, 2) majority vote, and 3) model stacking.\n\nAveraging works for regression, as well as those classiﬁcation models that return classiﬁcation scores. It consists of applying all your base models to the input x, and then averaging the predictions. To see if the averaged model works better than each individual algorithm, you can test it on the validation set using a metric of your choice.\n\nMajority vote works for classiﬁcation models. It consists of applying all your base models to the input x, and then returning the majority class among all predictions. In the case of a tie, you can either randomly pick one of the classes, or return an error message if misclassifying would incur a signiﬁcant loss for the business.\n\nModel stacking is an ensemble learning method that trains a strong model by inputting the outputs of other strong models. Let’s go into more detail about model stacking.\n\n6.2.2 An Algorithm of Model Stacking\n\nSay you want to combine classiﬁers f1, f2, and f3, all predicting the same set of classes. To create a synthetic training example (ˆxi, ˆyi) for the stacked model from the original training example (xi,yi), set ˆxi ← [f1(x),f2(x),f3(x)], and ˆyi ← yi. This is illustrated in Figure 5. If some of your base models return a class plus a class score, you can use those scores as additional input features for the stacked model.\n\nTo train the stacked model, use synthetic examples, and tune the hyperparameters of the stacked model using cross-validation. Make sure your stacked model performs better on the validation set than each of the stacked base models.\n\nIn addition to using diﬀerent machine learning algorithms and models, some base models, to be weakly correlated, can be trained by randomly sampling the examples and features of the original training set. Furthermore, the same learning algorithm, trained with very diﬀerent hyperparameter values, could produce suﬃciently uncorrelated models.\n\n6.2.3 Data Leakage in Model Stacking\n\nTo avoid data leakage, be careful when training a stacked model. To create the synthetic training set for the stacked model, follow a process similar to cross-validation. First, split all training data into ten or more blocks. The more blocks the better, but the process of training the model will be slower.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\nBase model 1(e.g. SVM)\n\nBase model 2(e.g. gradientboosting)\n\nBase model 3(e.g. neuralnetwork)\n\nStacking model(e.g. logisticregression)\n\nFigure 5: A stacking of three weakly correlated strong models.\n\nTemporarily exclude one block from the training data, and train the base models on the remaining blocks. Then apply the base models to the examples in the excluded block. Obtain the predictions, and build the synthetic training examples for the excluded block by using the predictions from the base models.\n\nRepeat the same process for each of the remaining blocks, and you will end up with the training set for the stacking model. The new synthetic training set will be of the same size as that of the original training set.\n\n6.3 Dealing With Distribution Shift\n\nRecall that the holdout data must resemble the data you will observe in production. Some- times, however, it is not available in suﬃciently large quantities. At the same time, you might have access to labeled data that is similar to the production data, but not exactly the same. For example, you might have lots of labeled images from the Web crawl collection, but your goal is to train a classiﬁer for Instagram photos. You might not have enough labeled Instagram photos for training, so you hope to train the model by using the Web crawl data, and then be able to use that model to classify the Instagram photos.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\n6.3.1 Types of Distribution Shift\n\nWhen the distributions of the training data and test data are not the same, we call it distribution shift. Dealing with a distribution shift is currently an open research area. Researchers distinguish three types of distribution shift:\n\ncovariate shift — shift in the values of features; • prior probability shift — shift in the values of the target; and • concept drift — shift in the relationship between the features and the label.\n\nYou may know your data is aﬀected by a distribution shift, but you don’t usually know what type of shift it is.\n\nIf the number of examples in the test set is relatively high compared to the size of the training set, you could randomly pick a certain fraction of test examples and transfer some to the training set and some to the validation set. Then you would train the model as usual. However, often you have a very high number of training examples and relatively few test examples. In that case, a more eﬀective approach is to use adversarial validation.\n\n6.3.2 Adversarial Validation\n\nWe prepare for adversarial validation as follows. We assume that the feature vectors in a training and a test examples contain the same number of features, and those features represent the same information. Split your original training set into two subsets: Training Set 1 and Training Set 2.\n\nCreate a Modiﬁed Training Set 1 by transforming the examples from Training Set 1 as follows. To each example in Training Set 1, add the original label as an additional feature, then assign the new label “Training” to that example.\n\nCreate a Modiﬁed Test Set by transforming the examples from the original test set as follows. To each example in the test set, add the original label as an additional feature, then assign the new label “Test” to that example.\n\nMerge the Modiﬁed Training Set 1 and the Modiﬁed Test Set to obtain a new Synthetic Training Set. You will use it for solving a binary classiﬁcation problem of distinguishing the “Training” examples from the “Test” examples. Use that Synthetic Training Set, and train a binary classiﬁer that returns a prediction score.\n\nObserve that the binary classiﬁer we have trained will predict, for a given original example, whether it’s a training or a test example. Apply that binary classiﬁer to the examples from Training Set 2. Identify the examples predicted as “Test,” which the binary model is most certain about. Use those examples as validation data for your original problem.\n\nRemove the examples from Training Set 1 which the binary model predicted “Training” with the highest certainty. Use the remaining examples in Training Set 1 as the training data for your original problem.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\nYou must experiment to ﬁnd out what is the ideal way to split the original training set into Training Set 1 and Training Set 2. You also must ﬁnd out how many examples from Training Set 1 to use for training, and how many of them to use for validation.\n\n6.4 Handling Imbalanced Datasets\n\nIn Section ?? of Chapter 3, we considered some techniques to handle imbalanced datasets, such as over- and undersampling, and generating synthetic data.\n\nIn this section, we will consider additional techniques that are applied during learning, as opposed to in the data collection and preparation stage.\n\n6.4.1 Class Weighting\n\nSome algorithms and models, such as support vector machine (SVM), decision trees, and random forests, allow the data analyst to provide weights for each class. The loss in the cost function is typically multiplied by the weight. The data analyst may, for example, provide greater weight to the minority class. This makes it harder for the learning algorithm to disregard examples of the minority class, because it would result in much higher cost than without class weighting.\n\nLet’s see how it works in support vector machines. Our problem is distinguishing between genuine and fraudulent e-commerce transactions. The examples of genuine transactions are much more frequent. If you use SVM with soft margin, you can deﬁne a cost for misclassiﬁed examples. The SVM algorithm tries to move the hyperplane to reduce the number of misclassiﬁed examples. If the misclassiﬁcation cost is the same for both classes, the “fraudulent” examples, in the minority, risk being misclassiﬁed to allow classifying more of the majority class correctly. This situation is illustrated in Figure 6a. This problem is observed for most learning algorithms applied to imbalanced datasets.\n\nIf you set higher the loss of minority misclassiﬁcation, then the model will try harder to avoid misclassifying those examples. But this will incur the cost of misclassiﬁcation of some majority class examples, as illustrated in Figure 6b.\n\n6.4.2 Ensemble of Resampled Datasets\n\nEnsemble learning is another way of mitigating the class imbalance problem. The analyst randomly chunks majority examples into H subsets, then creates H training sets. After training H models, the analyst then makes predictions by averaging (or taking the majority) of the outputs of H models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\n(a)\n\n(b)\n\nFigure 6: An illustration of an imbalanced problem. (a) Both classes have the same weight; (b) examples of the minority class have a higher weight.\n\nB\n\nC\n\nA\n\nA\n\nB\n\nOriginal dataResampled data\n\nC\n\nA\n\nA\n\nD\n\nA\n\nE\n\nE\n\nD\n\nFigure 7: An ensemble of resampled datasets.\n\nThe process for H = 4 is illustrated in Figure 7. Here, we transformed our imbalanced binary learning problem into four balanced problems by chunking the examples of the majority class into four subsets. The examples of the minority class are copied four times in their entirety.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24\n\nThis approach is simple and scalable: you can train and run your models on diﬀerent CPU cores or cluster nodes. Ensemble models also tend to produce a better prediction than each individual model in the ensemble.\n\n6.4.3 Other Techniques\n\nIf you use stochastic gradient descent, the class imbalance can be tackled in several ways. First, you can have diﬀerent learning rates for diﬀerent classes: a lower value for the examples of the majority class, and a higher value otherwise. Second, you can make several consecutive updates of the model parameters each time you encounter an example of a minority class.\n\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic that we considered in Section ?? in the previous chapter.\n\n6.5 Model Calibration\n\nSometimes it is important that the classiﬁcation model returns not just the predicted class, but also the probability that the predicted class is correct. Some models return a score along with the predicted class. Even if its value ranges between 0 and 1, it’s not always a probability.\n\n6.5.1 Well-Calibrated Models\n\nWe say that the model is well-calibrated when, for input example x and predicted label ˆy, it returns the score that can be interpreted as the true probability for x to belong to class ˆy.\n\nFor instance, a well-calibrated binary classiﬁer would generate a score of 0.8 for approximately 80% of the examples actually belonging to the positive class.\n\nMost machine learning algorithms train models that are not well-calibrated, as shown4 by the calibration plots in Figure 8.\n\nA calibration plot for a binary model allows seeing how well the model is calibrated. On the X-axis, there are bins that group examples by the predicted score. For example, if we have 10 bins, the left-most bin groups all examples for which the predicted score is in the range [0,0.1) while the right-most bin groups all examples for which the predicted score is in the range [0.9,1.0]. On the Y-axis, there are the fractions of positive examples in each bin.\n\n4The graph is adapted from https://scikit-learn.org/stable/modules/calibration.html.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25\n\nFigure 8: Calibration plots for models trained by several machine learning algorithms applied to a random binary dataset.\n\nFor multiclass classiﬁcation, we would have one calibration plot per class in a one-versus- rest way. One-versus-rest is common strategy for converting a binary classiﬁcation learning algorithm for solving multiclass classiﬁcation problems. The idea is to transform a multiclass problem into C binary classiﬁcation problems and build C binary classiﬁers. For example, if we have three classes, y ∈ {1,2,3}, we create three original dataset copies, and modify them. In the ﬁrst copy, we replace all labels not equal to 1 with a 0. In the second copy, we replace all labels not equal to 2 with a 0. In the third copy, we replace all labels not equal to 3 with a 0. Now we have three binary classiﬁcation problems where we want to learn to distinguish between labels 1 and 0, 2 and 0, and 3 and 0. As you can see, in each of the three binary classiﬁcation problems, the label 0 denotes the “rest” in “one-versus-rest.”\n\nWhen the model is well-calibrated, the calibration plot oscillates around the diagonal (shown as a dotted line in Figure 8). The closer the calibration plot is to the diagonal, the better the model is calibrated. Because a logistic regression model returns the true probabilities of the positive class, its calibration plot is closest to the diagonal. When the model is not well-calibrated, the calibration plot usually has a sigmoid-shape, as shown by the support vector machine and random forest models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26\n\n6.5.2 Calibration Techniques\n\nThere are two techniques often used to calibrate a binary model: Platt scaling and isotonic regression. The two are based on similar principles.\n\nLet us have a model f that we want to calibrate. First of all, we need a holdout dataset speciﬁcally set aside for calibration. To avoid overﬁtting, we cannot use training or validation data for calibration. Let this calibration dataset be of size M. Then, we apply the model f to each example i = 1,...,M and obtain, for each example i, the prediction fi. We build a new dataset Z, where each example is a pair (fi,yi), yi is the true label of example i, and labels have the values in the set {0,1}.\n\nThe only diﬀerence between Platt scaling and isotonic regression is that the former builds a logistic regression model by using the dataset Z, while the latter builds the isotonic regression of Z, that is, a non-decreasing function as close to the examples as possible. Once we have the calibration model z, obtained either using Platt scaling or isotonic regression, we can predict the calibrated probability for an input x as z(f(x)).\n\nNotice that a calibrated model may or may not result in better quality prediction for your problem. That depends on the chosen model performance metric.\n\nAccording to experiments:5 Platt scaling is most eﬀective when the distortion in the predicted probabilities is sigmoid-shaped. Isotonic regression can correct a wider range of distortions. Unfortunately, this extra power comes at a price. Analysis has shown that isotonic regression is more prone to overﬁtting, and thus performs worse than Platt scaling when data is scarce.\n\nExperiments with eight classiﬁcation problems also suggested that random forests, neural networks, and bagged decision trees are the best learning methods for predicting well- calibrated probabilities prior to calibration, but after calibration, the best methods are boosted trees, random forest, and SVM.\n\n6.6 Troubleshooting and Error Analysis\n\nTroubleshooting a machine learning pipeline is hard. It’s diﬃcult to diﬀerentiate whether the model performs poorly because your code contains a bug, or if there are problems with your training data, learning algorithm, or the way you designed your pipeline. Moreover, the same degradation in performance can be explained by various reasons. The results of the learning can be sensitive to small changes in hyperparameters or dataset makeup.\n\nBecause of these challenges, model training is usually an iterative process, where an analyst trains a model, observes its behavior, and makes adjustments based on observations.\n\n5Alexandru Niculescu-Mizil and Rich Caruana, “Predicting Good Probabilities With Supervised Learning”, appearing in Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "page_number": 185,
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 185-194). Key topics include model, learned, and layer. To ﬁnd a local minimum of a function using gradient descent, we start at some random point in the domain of the function.",
      "keywords": [
        "learning rate",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "learning rate decay",
        "learning",
        "machine learning",
        "model",
        "Learning Engineering",
        "Andriy Burkov Machine",
        "rate",
        "learning rate update",
        "Burkov Machine",
        "pre-trained model",
        "Machine Learning Book",
        "rate decay"
      ],
      "concepts": [
        "model",
        "learned",
        "layer",
        "epochs",
        "networks",
        "data",
        "training",
        "parameter",
        "performance",
        "performs"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "Segment 21 (pages 180-188)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 29,
          "title": "Segment 29 (pages 248-255)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 28,
          "title": "Segment 28 (pages 240-247)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 195-202)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 195-202)",
      "start_page": 195,
      "end_page": 202,
      "detection_method": "topic_boundary",
      "content": "6.6.1 Reasons for Poor Model Behavior\n\nIf your model does poorly on the training data (underﬁts it), common reasons are:\n\nthe model architecture or learning algorithm are not expressive enough (try more advanced learning algorithm, an ensemble method, or a deeper neural network);\n\nyou regularize too much (reduce regularization); • you have chosen suboptimal values for hyperparameters (tune hyperparameters); • the features you engineered don’t have enough predictive power (add more informative features);\n\nyou don’t have enough data for the model to generalize (try to get more data, use data augmentation, or transfer learning); or\n\nyou have a bug in your code (debug the code that deﬁnes and trains the model).\n\nIf your model does well on the training data, but poorly on the holdout data (overﬁts the training data), common reasons are:\n\nyou don’t have enough data for generalization (add more data or use data augmentation); • your model is under-regularized (add regularization or, for neural networks, both regularization and batch normalization);\n\nyour training data distribution is diﬀerent from the holdout data distribution (reduce the distribution shift);\n\nyou have chosen suboptimal values for hyperparameters (tune hyperparameters); or • your features have low predictive power (add features with high predictive power).\n\n6.6.2\n\nIterative Model Reﬁnement\n\nIf you have access to new labeled data (for example, you can label examples yourself, or easily request the help of a labeler) then, you can reﬁne the model using a simple iterative process:\n\n1. Train the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set (100−300 examples). 3. Find the most frequent error patterns on that small validation set. Remove those examples from the validation set, because your model will now overﬁt to them.\n\n4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed (most errors look dissimilar).\n\nIterative model reﬁnement is a simpliﬁed version of error analysis. A more principled approach is described below.\n\n6.6.3 Error Analysis\n\nErrors can be:\n\nuniform, and appear with the same rate in all use cases, or\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28\n\nfocused, and appear more frequently in certain types of use cases.\n\nFocused errors following a speciﬁc pattern are those that merit special attention. By ﬁxing an error pattern, you ﬁx it once for many examples. Focused errors, or error trends, usually happen when some use cases aren’t well-represented in the training data. For example, a face detection system developed by a major web camera provider worked better for white users than for black users. In another case, a human presence detection system equipped with a night vision system worked better during the day than at night, simply because the night training examples were less frequent in the training data.\n\nUniform errors cannot be entirely avoided, but important focused errors should be discovered before the model is deployed in production. This can be done by clustering test examples, and by testing the model on examples coming from diﬀerent clusters. The distribution of the production (online) data can be signiﬁcantly diﬀerent from the oﬄine data distribution used for model training/pre-deployment tests. So, the clusters that contain few examples in the oﬄine data might represent much more frequent use cases in the online scenario.\n\nIn Section ?? of Chapter 4, we discussed several techniques for dimensionality reduction. In addition to using clustering for spotting error trends, uniform manifold approximation and projection (UMAP) or autoencoder can be used. Use those techniques to reduce the dimen- sionality of the data to 2D, and then visually inspect the distribution of errors across a dataset.\n\nMore speciﬁcally, you can visualize the data on a 2D scatter plot, using diﬀerent colors for examples of diﬀerent classes. To identify error trends on a scatter plot, use diﬀerent markers depending on whether a model’s prediction was correct or not. For example, use circles to denote examples whose label was predicted correctly, and squares otherwise. This will allow you to see the regions of poor model performance. If you work with perceptive data, such as images or text, it is also helpful to visually examine some examples from those poor performance regions.\n\nWhether you are satisﬁed or dissatisﬁed by the model’s performance on the holdout data, you can always improve the model by analyzing individual errors. As discussed, the best way is to work iteratively, by considering 100 − 300 examples at a time. By considering a small number of examples at a time, you can iterate quickly, by retraining the model after each iteration, but still consider enough examples to spot obvious patterns.\n\nHow do you decide whether an error pattern is worth spending time to ﬁx it? You can base that decision on the error pattern frequencies. Let’s see how it works.\n\nLet your model have an accuracy of 80%, which corresponds to an error rate of 20%. If you ﬁx all error patterns, you can improve the model’s performance by at most 20 percentage points. If your small error-analysis batch was of 300 examples, your model made 0.2×300 = 60 errors.\n\nObserve the errors one by one, and try to get an idea of what particularities in the input led to a misclassiﬁcation of those 60 examples. To be even more concrete, let our classiﬁcation problem be to detect pedestrians-on-the-street images. Assume that in 60 out of the 300 images, the model failed to detect a pedestrian. After closer analysis, you discover two\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29\n\npatterns: 1) the image is blurry in 40 examples, and 2) the picture was taken during the nighttime in 5 examples. Now, should you spend time addressing both problems?\n\nIf you address the blurry-image problem (for example, by adding more labeled blurry images to your training data), you can hope to decrease your error by (40/60) × 20 = 13 percentage points. In the best-case scenario, after you solve the blurry-image misclassiﬁcation problem, your error becomes 20 − 13 = 7 percent, a signiﬁcant decrease from the initial 20% error.\n\nOn the other hand, if you solve the nighttime image problem, you can hope to decrease your error by 5/60 × 20 = 1.7 percentage points. So, in the best-case scenario, your model will make 20 − 1.7 = 18.3 percent errors, which might be signiﬁcant for some problems, or insigniﬁcant for others. The cost of gathering additional labeled night-time images can be signiﬁcant and might not be worth the eﬀort.\n\nTo ﬁx an error pattern, you can use one or a combination of techniques:\n\npreprocessing the input (e.g. image background removal, text spelling correction); • data augmentation (e.g., blurring or cropping of images); • labeling more training examples; and • engineering new features that would allow the learning algorithm to distinguish between “hard” cases.\n\n6.6.4 Error Analysis in Complex Systems\n\nLet’s say you work on a complex document classiﬁcation system that consists of three chained models as shown below:\n\nDetectlanguage\n\nClassify\n\nTranslate\n\nInput Output\n\nFigure 9: A complex document classiﬁcation system.\n\nLet the accuracy of the entire system be 73%. If the classiﬁcation is binary, the accuracy of 73% doesn’t seem high. On the other hand, if the classiﬁcation model (the rightmost block in Figure 9) supports thousands of classes, then the accuracy of 73% doesn’t seem too low. For some business cases, however, the user might expect human-like, or even superhuman performance.\n\nImagine that you are in a position, where the business expects a higher than 73% performance from the document classiﬁcation system you have built. To get the most out of your additional eﬀort, you must decide which part of the system needs improvement in the ﬁrst place.\n\nWhen the decision about something is made on several chained levels, like in the problem shown in Figure 9, and when those decisions are independent of one another, the accuracy multiplies.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30\n\nFor example, if the language predictor accuracy was 95%, the machine translation model accuracy6 was 90%, and the classiﬁer accuracy was 85%, then, in the case of independence of the three models, the overall accuracy of the entire three-stage system would be 0.95 × 0.90 × 0.85 = 0.73, or 73 percent. At ﬁrst glance, it seems obvious that the most gain in the entire system’s accuracy would come from maximizing the accuracy of the third model — the classiﬁer. However, in practice, some errors made by a given model might not signiﬁcantly aﬀect the overall performance of the system. For example, if the language predictor often confuses Spanish and Portuguese, the machine translation model could still be capable of generating an adequate translation for the third-stage classiﬁcation model.\n\nWhile working on the third-stage classiﬁer, you might have concluded that you reached its maximum performance, so it doesn’t make sense to continue. Now, which of the previous two models, the language detector and/or the machine translator, should you improve to increase the quality of the entire three-stage system?\n\nOne way to determine the upper bound of an entire system’s potential is to perform the error analysis by parts. You replace one model’s predictions with perfect labels, such as human-provided labels. Then you calculate how the entire system performs. For example, instead of using the machine translation system at stage two in Figure 9, you can ask a professional human translator to translate the text from the predicted language (if the prediction of the language was correct), or keep the original text (if the prediction of the language was wrong).\n\nLet’s say you asked a professional for a hundred translations. Now you can measure how perfect translations aﬀect the overall system performance. Let the accuracy of the entire system’s output become 74%. So, the potential gain from improved translation in overall system performance is only one percentage point. Reaching the human-level performance for a machine translation model can turn out to be a daunting task, not worth the eﬀort, especially when what we can achieve in the end is one percentage point gain for the entire system. So, you might prefer spending more time on building a better language predictor in stage 1, if the potential gain in overall system performance prediction quality is higher.\n\n6.6.5 Using Sliced Metrics\n\nIf the model will be applied to diﬀerent segments of the use cases, it should be separately tested for each segment. For example, if you want to predict the solvency of borrowers, you would want your model to be equally accurate for both male and female borrowers. To achieve that, you can split your validation data into several subsets, one subset per segment. Then compute the performance metric by separately applying your model to each subset.\n\nAlternatively, you can separately evaluate the model on each class by applying precision and recall metrics. Remember these metrics are deﬁned only for binary classiﬁcation. By isolating\n\n6Measuring the error of the machine translation system in practice is tricky as a translation is rarely entirely accurate or inaccurate. Instead, measures, such as BLEU (for Bilingual Evaluation Understudy Score) score, are used.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31\n\none class in your multiclass classiﬁcation problem, and labeling the other classes “Other,” you can individually compute the precision and recall for each class.\n\nIf you see that the value of the performance metric changes between segments or classes, you can try to ﬁx the problem by adding more labeled data to the segments or classes, where the performance of the model is unsatisfactory, or engineer additional features.\n\n6.6.6 Fixing Wrong Labels\n\nWhen humans label the training examples, the assigned labels can be wrong. This can cause poor model performance on the model on both training and holdout data. Indeed, if similar examples have conﬂicting labels — some correct and some incorrect — the learning algorithm can learn to predict the wrong label.\n\nHere is a simple way to identify the examples that have wrong labels. Apply the model to the training data from which it was built, and analyze the examples for which it made a diﬀerent prediction as compared to the labels provided by humans. If you see that some predictions are indeed correct, change those labels.\n\nIf you have time and resources, you could also examine the predictions with the score close to the decision threshold. Those are often mislabeled cases too.\n\nIf wrong labels in the training data is a serious issue, you can avoid it by asking several individuals to provide labels for the same training example. Only accept it if all individuals assigned the same label to that example. In less demanding situations, you can accept a label if the majority of individuals assigned it.\n\n6.6.7 Finding Additional Examples to Label\n\nAs discussed above, error analysis can reveal that more labeled data is needed from speciﬁc regions of feature space. You might have an abundance of unlabeled examples. How should you decide which examples to label so as to maximize the positive impact on the model?\n\nIf your model returns a prediction score, an eﬀective way is to use your best model to score the unlabeled examples. Then label those examples, whose prediction score is close to the prediction threshold.\n\nWhen the error analysis has revealed error patterns by means of visualization, then choose those examples which are surrounded by many examples with prediction errors.\n\n6.6.8 Troubleshooting Deep Learning\n\nTo avoid problems when training a deep model, follow a workﬂow shown below:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32\n\nFigure 10: A deep learning troubleshooting workﬂow.\n\nWhen possible, start small, for example, with a simple model using a high-level library, such as Keras. It should be very easy to validate visually, ideally ﬁtting on at most two screens.\n\nAlternatively, reuse an existing open-source architecture that was proven to work (pay attention to the code license!). Start with:\n\na small, normalized dataset ﬁtting in memory, • the most simple to use cost-function optimizer (e.g., Adam), • an initialization strategy (e.g., random normal), • the default values of the sensitive hyperparameters of both the cost-function optimizer and the layers, and\n\nno regularization.\n\nOnce you have your ﬁrst simplistic model architecture and dataset, temporarily reduce your training dataset even further, to the size of one minibatch. Then start the training. Make sure your simplistic model is capable of overﬁtting this training minibatch. If the overﬁtting of the minibatch doesn’t happen, it is a solid indicator that something is wrong with your code or data. Look for the following signs7 and their probable causes:\n\n7Adapted from the talk “Troubleshooting Deep Neural Networks” by Josh Tobin et al., January 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33\n\nSign Error goes up\n\nProbable causes Flipped the sign of the loss function or gradient Learning rate too high Softmax taken over wrong dimension\n\nError explodes Numerical issue\n\nLearning rate too high\n\nError oscillates Data or labels corrupted (e.g. zeroed or incorrectly shuﬄed)\n\nError plateaus\n\nLearning rate too high Learning rate too low Gradient not ﬂowing through the whole model Too much regularization Incorrect input to loss function Data or labels corrupted\n\nTable 2: Common issues and most common causes of problems with getting to overﬁt one minibatch by a neural network model.\n\nOnce your model overﬁts one minibatch, get back to the entire dataset, and train, evaluate, then tune hyperparameters until no improvements on the validation data are possible.\n\nIf the performance of the model is still unsatisfactory, update the model (e.g., by increasing its depth or width), or the training data (e.g., by changing the pre-processing, or adding features). Debug the change by overﬁtting one minibatch once again, then train, evaluate, and tune the new model. Keep iterating until you’re satisﬁed with the quality of the model.\n\nWhile you are searching for the best architecture for your model, it’s convenient not just to use a smaller training set, but also to simplify the problem by either,\n\ncreating a simple synthetic training set, or • reducing the number of classes or the resolution of input images (or video fragments), size of the texts, bitrate of the sound frequencies, and so on.\n\nAt the evaluation step of the deep learning troubleshooting workﬂow shown in Figure 10, verify if the poor model performance could be caused by one of the reasons listed in Section 6.6.1. Choose the next step depending on whether the performance can be improved by tuning hyperparameters, updating the model, features, or the training data.\n\n6.7 Best Practices\n\nIn this section, I gathered practical advice on training machine learning models. The best practices below aren’t strict prescriptions. They are rather recommendations that often save time, eﬀort, and might lead to higher quality results.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34\n\n6.7.1 Deliver a Good Model\n\nWhat is a good model? A good model has two properties:\n\nit has the desired quality according to the performance metric; and • it is safe to serve in a production environment.\n\nFor a model to be safe-to-serve means satisfying the following requirements:\n\nit will not crash or cause errors in the serving system when being loaded, or when loaded with bad or unexpected inputs;\n\nit will not use an unreasonable amount of resources (such as CPU, GPU, or RAM).\n\n6.7.2 Trust Popular Open Source Implementations\n\nModern open-source libraries and modules for machine learning in popular modern pro- gramming languages and platforms, such as Python, Java, and .NET, contain eﬃcient, industry-standard implementations of popular machine learning algorithms. They usually have permissive licenses. Additionally, open-source libraries and modules exist speciﬁcally for training neural networks.\n\nIt is only considered reasonable to create your own machine learning algorithms if you use an exotic or very new programming language. In addition, you might program from scratch if the model is intended to be executed in a very resource-constrained environment, or you need to run your model with a speed no existing implementation can provide.\n\nAvoid using multiple programming languages in the same project. Using diﬀerent programming languages increases the cost of testing, deployment, and maintenance. It also makes it diﬃcult to transfer project ownership between employees.\n\n6.7.3 Optimize a Business-Speciﬁc Performance Measure\n\nLearning algorithms try to reduce training data error. The data analyst, in turn, wants to minimize test data error. However, your client or employer typically wants you to optimize a business-speciﬁc performance metric.\n\nWhen you have minimized the validation error rate, focus on tuning hyperparameters that optimize a business-speciﬁc metric, even if it causes the validation error rate to increase.\n\n6.7.4 Upgrade From Scratch\n\nOnce deployed to production, some models have to be periodically updated with new data to adapt to the user’s needs. This new training data must be automatically collected by using scripts (as we discussed in Chapter 3 in Section ?? about reproducibility).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35",
      "page_number": 195,
      "chapter_number": 23,
      "summary": "This chapter covers segment 23 (pages 195-202). Key topics include model, examples, and learning. Averaging works for regression, as well as those classiﬁcation models that return classiﬁcation scores.",
      "keywords": [
        "training set",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "training",
        "synthetic training set",
        "original training set",
        "model",
        "machine learning",
        "Modiﬁed Training Set",
        "learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "base models",
        "Burkov Machine",
        "synthetic training"
      ],
      "concepts": [
        "model",
        "examples",
        "learning",
        "classes",
        "data",
        "sets",
        "trains",
        "predictions",
        "predicting",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 21,
          "title": "Segment 21 (pages 180-188)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 31,
          "title": "Segment 31 (pages 263-271)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 433-440)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 203-210)",
      "start_page": 203,
      "end_page": 210,
      "detection_method": "topic_boundary",
      "content": "Each time the data is updated, the hyperparameters must be tuned from scratch. Otherwise, the new data may yield suboptimal performance with old hyperparameters.\n\nSome models, such as neural networks, may be iteratively upgraded. However, avoid the practice of warm-starting. It consists of iteratively upgrading the existing model by using only new training examples and running additional training iterations.\n\nFurthermore, frequent model upgrades without retraining from scratch can lead to catas- trophic forgetting. It’s a situation in which the model that was once capable of something, “forgets” that capability because of learning something new.\n\nNote that upgrading the model is not the same as transfer learning. Analysts use transfer learning when the data used to build the pre-trained model, or adequate computing resources, are not available.\n\n6.7.5 Avoid Correction Cascades\n\nYou might have model mA that solves problem A, but you need a solution mB for a slightly diﬀerent problem B. It can be tempting to use the output of mA as input for mB, and only train mB on a small sample of examples that “correct” the output of mA for solving problem B. Such technique is called correction cascading, and it is not recommended. Model cascading makes it impossible to update model mA, without also updating model mB (and the rest of the cascade). The eﬀect a change in mA might have on mB is impossible to predict, but most likely it will be negative. Furthermore, the developer of model mB might not know about the change in model mA, and the developer of model mA might not know that model mB depends on it. The negative eﬀect on mB of the change in model mA may go unnoticed for a long time.\n\nInstead of building a correction cascade, it is recommended to update model mA to include the use cases for solving problem B. It would be wise to add features allowing the model to distinguish between the examples of problem B. One might also use transfer learning, or build an entirely independent model for solving problem B.\n\n6.7.6 Use Model Cascading With Caution\n\nIt’s important to note that model cascading is not always a bad practice. Using the output of one model, as one of many inputs for another model, is common. It might signiﬁcantly reduce time to market. However, cascading must be used with caution, because the update of one model in a cascade must involve an update of all models in the cascade, which can end up being costly in the long-term.\n\nTo mitigate the negative eﬀect of model cascading, two strategies are beneﬁcial:\n\n1. Analyze the information ﬂow in your software system and update, or retrain, the entire chain. Model mA’s updated output must be reﬂected in the training data for model mB.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36\n\n2. Control who can and who cannot make calls to model mA to prevent undeclared consumers from creating this issue. As Google’s engineers mentioned:8 “In the absence of barriers, engineers will naturally use the most convenient signal at hand, especially when working against deadline pressures.”\n\nFurthermore, a prediction output by a model should not be a plain number or a string. It should come with information about the production model, and how it should be consumed.\n\n6.7.7 Write Eﬃcient Code, Compile, and Parallelize\n\nBy writing fast and eﬃcient code, you can speed up the training by an order of magnitude, as compared to an ineﬃcient quick-and-dirty script you implemented during experimentation, just “to make it work.” Modern datasets are large, so you might wait for hours, even days, for data preprocessing. Training also can take days, or sometimes weeks.\n\nAlways write the code with eﬃciency in mind, even if it seems to be a function, a method, or a script that you will not run frequently. Some code that was supposed to run once might be called in a loop millions of times.\n\nAvoid using loops. For example, if you need to compute a dot product of two vectors, or multiply a matrix by a vector, use fast and eﬃcient dot-product or matrix-multiplication methods in scientiﬁc libraries and modules. Examples of such eﬃcient implementations are Python’s NumPy and SciPy libraries. Talented and skilled software engineers and scientists created these libraries and modules. They rely on low-level programming languages such as C, as well as hardware acceleration, and work blazingly fast.\n\nWhere possible, compile the code before executing it. Such libraries as PyPy and Numba for Python, or pqR for R, would compile the code into the OS (operating system) native binary code, which can signiﬁcantly increase the speed of data processing and model training.\n\nAnother important aspect is parallelization. If you work with modern libraries and modules, you can ﬁnd learning algorithms that exploit multicore CPUs. Some allow GPUs to speed up the training of neural networks and many other models. Training of some models, such as SVM, cannot be eﬀectively parallelized. In such cases, you can still exploit a multicore CPU by running multiple experiments in parallel. Run one experiment for each combination of hyperparameter values, geographical region, or user segment. Furthermore, compute each cross-validation fold in parallel with other folds.\n\nWhere possible, use a solid-state drive (SSD) to store the data. Use distributed computing; some implementations of learning algorithms are designed to run in distributed computing environments, such as Spark. Try to put all the needed data into the RAM of your laptop or server. It’s not uncommon today for data analysts to work on a server with 512 gigabytes or even one or more terabytes of RAM.\n\n8“Hidden Technical Debt in Machine Learning Systems” by Sculley et al. (2015).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37\n\nBy reducing to a minimum the time needed to train a model, you can spend more time tweaking your model, testing data pre-processing ideas, feature engineering, neural network architectures, and other creative activities. The greatest beneﬁt for the machine learning project lies in the human touch and intuition. The more you, as a human, can work instead of waiting, the higher the chances that your machine learning project will be a success.\n\nReduce glue code to a minimum. This how Google engineers put it. Machine learning researchers tend to develop general purpose solutions as self-contained packages. A wide variety of these are available as open-source packages or from in-house code, proprietary packages, and cloud-based platforms. Using generic packages often results in a glue-code system design pattern, in which a massive amount of supporting code is written to get data into and out of general-purpose packages.\n\nGlue code is costly in the long term. It tends to freeze a system to the peculiarities of a speciﬁc package. Testing alternatives may become prohibitively expensive. Using a generic package this way inhibits improvements. It becomes harder to take advantage of domain-speciﬁc properties, or to tweak the objective function, and to achieve a domain-speciﬁc goal. A mature system might become (at most) 5% machine learning code and (at least) 95% glue code. It may be less costly to create a clean native solution, rather than re-use a generic package.\n\nAn important strategy for combating glue code is to wrap black-box machine learning packages into common APIs used by the entire organization. Infrastructure becomes more reusable and it reduces the cost of changing packages.\n\nIt is recommended to learn to switch between at least two programming languages: one for fast prototyping (like Python) and one for fast implementation (like C++). Modern languages like Go, Kotlin, and Julia may work well for both cases, but at the time of the writing of this book, these two languages have not developed an ecosystem of machine learning projects, as compared to more established counterparts.\n\n6.7.8 Test on Both Newer and Older Data\n\nIf you used a data dump from some time ago to create training, validation, and test sets, observe how your model behaves with data collected before and after this period. If it’s radically worse, there’s a problem.\n\nData leakage and distribution shift could be among the most likely reasons. Recall that data leakage is when information unavailable in the future or in the past was used to engineer a feature. Distribution shift is when properties of the data change over time.\n\n6.7.9 More Data Beats Cleverer Algorithm\n\nWhen confronted to insuﬃcient model performance, to improve the performance of the model, analysts are often tempted into crafting a more sophisticated learning algorithm or a pipeline.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38\n\nIn practice, however, better results often come from getting more data, speciﬁcally, more labeled examples. If designed well, the data labeling process can allow a labeler to produce several thousand training examples daily. It can also be less expensive, compared to the expertise needed to invent a more advanced machine learning algorithm.\n\n6.7.10 New Data Beats Cleverer Features\n\nIf, despite adding more training examples and designing clever features, the performance of your model plateaus, think about diﬀerent information sources.\n\nFor example, if you want to predict whether user U will like a news article, try to add historical data about the user U as features. Or cluster all the users, and use the information on the k-nearest users to user U as new features. This is a simpler approach compared to programming very complex features, or combining existing features in a complex way.\n\n6.7.11 Embrace Tiny Progress\n\nMany tiny improvements to your model may give the expected result faster than looking for one revolutionary idea.\n\nFurthermore, by trying diﬀerent ideas, the analyst gets to know the data better, which might indeed help in ﬁnding that revolutionary idea.\n\n6.7.12 Facilitate Reproducibility\n\nMost machine learning algorithms are stochastic. For example, when we train a neural network, we initialize model parameters randomly; the minibatch stochastic gradient descent generates minibatches randomly; the decision trees in a random forest are built randomly; when we shuﬄe examples before splitting the data into three sets, we do it randomly; and so on. This means that when you train a model on the same data twice, you might end up having two diﬀerent models. In order to facilitate reproducibility, it’s recommended to set the value of the random seed used to initialize the pseudorandom number generator. If your random seed remains the same, then, if your data doesn’t change, you will obtain exactly the same model each time you train.\n\nThe random seed can be set as np.random.seed(15) (in NumPy and scikit-learn), tf.random.set_seed(15) in TensorFlow, torch.manual_seed(15) (in PyTorch), and set.seed(15) (in R). The seed value doesn’t matter as long as it remains constant. Even if a machine learning framework allows us to set the value of the random seed, there’s no guarantee that the code of the framework that uses randomization doesn’t change between versions of the framework. For reproducibility, each project’s dependencies should be isolated. It can be done in many ways: either by using tools such as virtualenv in Python and Packrat\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39\n\nin R, or by running machine learning experiments in standardized virtual machines or containers. We will talk more about virtualization in Section ?? in Chapter 8.\n\nWhen delivering the model, make sure it’s accompanied by all relevant information for reproducibility. Besides the description of the dataset and features, such as documentation and metadata considered in Sections ?? and ??, each model should contain the documentation with the following details:\n\na speciﬁcation of all hyperparameters, including the ranges considered, and the default values used,\n\nthe method used to select the best hyperparameter conﬁguration, • the deﬁnition of the speciﬁc measure or statistics used to evaluate the candidate models, and the value of it for the best model,\n\na description of the computing infrastructure used, and • the average runtime for each trained model, and an estimated cost of the training.\n\n6.8 Summary\n\nThe deep model training strategy has more moving parts, as compared to training shallow models. At the same time, it’s more principled and amenable to automation.\n\nInstead of training your model from scratch, it can be useful to start with a pre-trained model. Organizations with access to big data have trained and open-sourced very deep neural networks with architectures optimized for image or natural language processing tasks.\n\nA pre-trained model can be used in two ways: 1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\n\nUsing a pre-trained model to build your own is called transfer learning. The fact that deep models allow for transfer learning is one of the most important properties of deep learning.\n\nMinibatch stochastic gradient descent and its variants are the most frequently used cost function optimization algorithms for deep models.\n\nThe backpropagation algorithm computes the partial derivatives of each deep model parameter, using the chain rule for derivatives of complex functions. At each epoch, gradient descent updates all parameters using partial derivatives. The learning rate controls the signiﬁcance of an update. The process continues until convergence, the state where parameters’ values don’t change much after each epoch. Then the algorithm stops.\n\nThere are several popular upgrades to minibatch stochastic gradient descent, such as Momen- tum, RMSProp, and Adam. These algorithms update the learning rate automatically, based on the performance of the learning process. You do not need to choose the initial value of the learning rate, the decay schedule and rate, or the values of other related hyperparameters. These algorithms have demonstrated good performance in practice, and practitioners often use them instead of trying to manually tune the learning rate.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40\n\nIn addition to L1 and L2 regularization, neural networks beneﬁt from neural network-speciﬁc regularizers: dropout, early stopping, and batch-normalization. Dropout is a simple but very eﬀective regularization method. Using batch-normalization is a best practice.\n\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model. There are ensemble learning algorithms, such as random forest and gradient boosting, that build an ensemble of several hundred to thousands of weak models, and obtain a strong model that has a signiﬁcantly better performance than the performance of each weak model.\n\nStrong models can be combined into an ensemble model by averaging their outputs (for regression) or by taking a majority vote (for classiﬁcation). Model stacking, being the most eﬀective of the ensembling methods, consists of training a meta-model that takes the output of base models as input.\n\nIn addition to using over- and undersampling, imbalanced learning problems can be solved by applying class weighting and ensemble of resampled datasets. If you train your model using stochastic gradient descent, the class imbalance can be tackled in two additional ways: 1) by setting diﬀerent learning rates for diﬀerent classes, and 2) by making several consecutive updates of the model parameters each time you encounter an example of a minority class.\n\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic.\n\nTroubleshooting a machine learning pipeline can be hard. Poor performance can be caused by a bug in your code, training data errors, learning algorithm issues, or pipeline design. In addition, learning can be sensitive to small changes in hyperparameters and dataset makeup.\n\nErrors made by a machine learning model can be uniform and appear in all use cases with the same rate, or focused and appear in just certain types of use cases.\n\nFocused errors are those that merit special attention, because by ﬁxing an error pattern, you ﬁx it once for many examples.\n\nThe performance of the model can be iteratively improved using the following simple process:\n\n1. Train the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set (100−300 examples). 3. Find the most frequent error patterns on that small validation set. Remove those examples from the validation set, because your model will now overﬁt to them.\n\n4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed (most errors look dissimilar).\n\nIn complex machine learning systems, the error analysis is done by parts. We ﬁrst substitute the predictions of one model for the perfect labels (such as human-provided labels), and see how the performance of the entire system improves. If it improves signiﬁcantly, then more eﬀort must be put in improving that speciﬁc model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41\n\nFor reproducibility, set the random seed and make sure the model is accompanied by all relevant information.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "page_number": 203,
      "chapter_number": 24,
      "summary": "This chapter covers segment 24 (pages 203-210). Key topics include errors, model, and trains. 6.6.2\n\nIterative Model Reﬁnement\n\nIf you have access to new labeled data (for example, you can label examples yourself, or easily request the help of a labeler) then, you can reﬁne the model using a simple iterative process:\n\n1.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "Model",
        "Machine Learning",
        "Andriy Burkov Machine",
        "data",
        "training data",
        "error",
        "Learning Engineering",
        "learning",
        "Poor Model Behavior",
        "Burkov Machine",
        "machine translation model",
        "machine learning models",
        "training"
      ],
      "concepts": [
        "errors",
        "model",
        "trains",
        "labeling",
        "learning",
        "performance",
        "perform",
        "images",
        "predictive",
        "prediction"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 8,
          "title": "Segment 8 (pages 58-71)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "Segment 21 (pages 178-185)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 681-701)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 211-219)",
      "start_page": 211,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "content": "7 Model Evaluation\n\nStatistical models play an increasingly important role in the modern organization. When applied in a business context, a model can aﬀect the organization’s ﬁnancial indicators. However, it may also present a liability risk. Therefore, any model running in production must be carefully and continuously evaluated.\n\nModel evaluation is the ﬁfth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nDepending on the model’s applicative domain and the organization’s business goals and constraints, model evaluation may include the following tasks:\n\nEstimate legal risks of putting the model in production. For example, some model predictions may indirectly communicate conﬁdential information. Cyber attackers or competitors may attempt to reverse engineer the model’s training data. Additionally, when used for prediction, some features, such as age, gender, or race, might result in the organization being considered as biased or even discriminatory.\n\nStudy the main properties of distributions of the training data versus the production data. By comparing the statistical distribution of examples, features, and labels, in both train- ing and production data, is how distribution shift is detected. A signiﬁcant diﬀerence between the two indicates a need to update the training data, and retrain the model. • Evaluate the performance of the model. Before the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training. The external data must include both historical and online examples from the production environment. The context of evaluation on the real-time, online data should closely resemble the production environment. 3\n\nStudy the main properties of distributions of the training data versus the production data. By comparing the statistical distribution of examples, features, and labels, in both train- ing and production data, is how distribution shift is detected. A signiﬁcant diﬀerence between the two indicates a need to update the training data, and retrain the model. • Evaluate the performance of the model. Before the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training. The external data must include both historical and online examples from the production environment. The context of evaluation on the real-time, online data should closely resemble the production environment. 3\n\nMonitor the performance of the deployed model. The model’s performance may degrade over time. It is important to be able to detect this and, either upgrade the model by adding new data, or train an entirely diﬀerent model. Model monitoring must be a carefully designed automated process, and might include a human in the loop. We consider this in more detail in Chapter 9.\n\nIn this chapter, we look at some examples of the kinds of tricks that statisticians use during the model evaluation phase. Machine learning engineering is a developing discipline, and some questions still don’t have well established and easy to apply answers. In particular, the evaluation is presented from the point of view of an engineer, while each business has its own success criteria, which are unique. Before evaluating a machine learning solution, it is very important to ensure that the right people have done the most diﬃcult work in the project: ﬁguring out what success looks like and what are the right questions to ask in the form of business-appropriate metrics and objectives.\n\nA common reason for failure is engineers answering convenient questions with basic tools instead of answering the right questions with custom tools — something that may require consultation with a professional statistician after your project’s leaders and stakeholders have completed their part in your project. Note that some methods highlighted in this chapter, speciﬁcally those used in A/B testing (Section 7.2), are provided as examples only and might not be appropriate for your speciﬁc business problem. On important large-scale projects, it would be a mistake to try to do everything yourself. Timely collaboration with your leadership team and consulting a statistician is essential.\n\n7.1 Oﬄine and Online Evaluation\n\nIn Section ??, we overviewed the evaluation techniques applied in what’s called oﬄine model evaluation. An oﬄine model evaluation happens when the model is being trained by the analyst. The analyst tries out diﬀerent features, models, algorithms, and hyperparameters. Tools like confusion matrix and various performance metrics, such as precision, recall, and AUC, allow comparing candidate models, and guide the model training in the right direction.\n\nFirst, validation data is used to assess the chosen performance metric and compare models. Once the best model is identiﬁed, the test set is used, also in oﬄine mode, to again assess the best model’s performance. This ﬁnal oﬄine assessment guarantees post-deployment model performance. In this chapter, we talk, among other topics, about establishing statistical bounds on the oﬄine test performance of the model.\n\nA signiﬁcant part of the chapter is devoted to the online model evaluation, that is, testing and comparing models in production by using online data. The diﬀerence between oﬄine and online model evaluation, as well as the placement of each type of evaluation in a machine learning system, is illustrated in Figure 2.\n\nIn Figure 2, the historical data is ﬁrst used to train a deployment candidate. Then it is evaluated oﬄine, and, if the result is satisfactory, deployment candidate becomes the deployed\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\nFigure 2: The placement of oﬄine and online model evaluations in a machine learning system.\n\nmodel, and starts accepting user queries. Then, user queries and the model predictions are used for an online evaluation of the model. The online data is then used to improve the model. To close the loop, the online data is permanently copied to the oﬄine data repository.\n\nWhy do we evaluate both oﬄine and online? The oﬄine model evaluation reﬂects how well the analyst succeeded in ﬁnding the right features, learning algorithm, model, and values of hyperparameters. In other words, the oﬄine model evaluation reﬂects how good the model is from an engineering standpoint.\n\nOnline evaluation, on the other hand, focuses on measuring business outcomes, such as customer satisfaction, average online time, open rate, and click-through rate. This informa- tion may not be reﬂected in historical data, but it’s what the business really cares about. Furthermore, oﬄine evaluation doesn’t allow us to test the model in some conditions that can be observed online, such as connection and data loss, and call delays.\n\nThe performance results obtained on the historical data will hold after deployment only if the distribution of the data remains the same over time. In practice, however, it’s not always the case. Typical examples of a distribution shift include the ever-changing interests of the user of a mobile or online application, instability of ﬁnancial markets, climate change, or wear of a mechanical system whose properties the model is intended to predict.\n\nAs a consequence, the model must be continuously monitored once deployed in production. When a distribution shift happens, the model must be updated with new data and re-deployed. One way of doing such monitoring is to compare the performance of the model on online and historical data. If the performance on online data becomes signiﬁcantly worse, as compared to historical, it’s time to retrain the model.\n\nThere are diﬀerent forms of online evaluation, each serving a diﬀerent purpose. For example,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nruntime monitoring is checking whether the running system meets the runtime requirements.\n\nAnother common scenario is to monitor user behavior in response to diﬀerent versions of the model. One popular technique used in this scenario is A/B testing. We split the users of a system into two groups, A and B. The two groups are served the old and the new models, respectively. Then we apply a statistical signiﬁcance test to decide whether the performance of the new model is better than the old one.\n\nMulti-armed bandit (MAB) is another popular technique of online model evaluation. Similar to A/B testing, it identiﬁes the best performing models by exposing model candidates to a fraction of users. Then it gradually exposes the best model to more users, by keeping gathering performance statistics until it’s reliable.\n\n7.2 A/B Testing\n\nA/B testing is one of the most frequently used statistical techniques. When applied to online model evaluation, it allows us to answer such questions as, “Does the new model mB work better in production than the existing model mA?” or, “Which of the two model candidates works better in production?”\n\nA/B testing is often used on websites and mobile applications to test whether a speciﬁc change in the design or wording positively aﬀects business metrics such as user engagement, click-through rate, or sales rate.\n\nImagine we want to decide whether to replace an existing (old) model in production with a new model. The live traﬃc that contains input data for the model is split into two disjoint groups: A (control) and B (experiment). Group A traﬃc is routed to the old model, while group B traﬃc is routed to the new model.\n\nBy comparing the performance of the two models, a decision is made about whether the new model performs better than the old model. The performance is compared using statistical hypothesis testing.\n\nIn general, statistical hypothesis testing maintains a null hypothesis and an alternative hypothesis. An A/B test is usually formulated to answer the following question: “Does the new model lead to a statistically signiﬁcant change in this speciﬁc business metric?” The null hypothesis states that the new model doesn’t change the average value of the business metric. The alternative hypothesis states that the new model changes the average value of the metric.\n\nA/B test is not one test, but a family of tests. Depending on the business performance metric, a diﬀerent statistical toolkit is used. However, the principle of splitting the users into two groups, and measuring the statistical signiﬁcance of the diﬀerence in the metric values between diﬀerent groups, remains the same.\n\nThe description of all formulations of A/B tests is beyond the scope of this book. Here we will consider only two formulations, but they apply to a wide range of practical situations.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\n7.2.1 G-Test\n\nThe ﬁrst formulation of A/B test is based on the G-test. It is appropriate for a metric that counts the answer to a “yes” or “no” question. An advantage of the G-test is that you can ask any question, as long as only two answers are possible. Examples of questions:\n\nWhether the user bought the recommended article? • Whether the user has spent more than $50 during a month? • Whether the user renewed the subscription?\n\nLet’s see how to apply it. We want to decide whether the new model works better than the old one. To do that, we formulate a yes-or-no question that deﬁnes our metric. Then we randomly divide the users into groups A and B. The users of group A are routed to the environment running the old model, while the group’s B traﬃc is routed to the new model. Observe the actions of each user and record the answer as “yes” or “no.” Fill the following table:\n\nYes\n\nNo\n\nA\n\nB\n\nFigure 3: The counts of answers to the yes-or-no question by users from groups A and B.\n\nIn the above table, ˆayes is the number of users in group A, for which the answer to the question is “yes,” ˆbyes is the number of users in group B, for which the answer to the question is “yes,” ˆano is the number of users in group A, for which the answer to the question is “no,” and so on. Similarly, nyes = ˆayes + ˆbyes, nno = ˆano + ˆbno, na = ˆayes + ˆano, nb = ˆbyes + ˆbno, and, ﬁnally ntotal = nyes + nno = na + nb.\n\nNow, ﬁnd the expected numbers of “yes” and “no” answers for A and B, i.e. the number of “yes” and “no” we would get if versions A and B were equivalent.\n\nayes\n\nano\n\nbyes\n\nbno\n\ndef= na\n\ndef= na\n\ndef= nb\n\ndef= nb\n\nnyes ntotal nno ntotal nyes ntotal nno ntotal\n\n,\n\n,\n\n,\n\n.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(1)\n\n7\n\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\nNow, ﬁnd the value of the G-test as,1\n\nG def= 2\n\nˆayes ln\n\n(cid:18)ˆayes ayes\n\n(cid:19)\n\n+ ˆano ln\n\n(cid:18)ˆano ano\n\n(cid:19)\n\n+ ˆbyes ln\n\nˆbyes byes\n\n!\n\n+ ˆbno ln\n\nˆbno bno\n\n!!\n\nG is a measure of how diﬀerent the samples from A and B are. Statistically speaking, under the null hypothesis (A and B are equal), G follows a chi-square distribution with one degree of freedom:\n\nG ∼ χ2 1\n\nIn other words, if A and B were equal, we expect G to be small. A large value of G would make us suspicious that one of the models performs better than the other. For example, imagine you calculated G = 3.84. If A and B were equal (i.e. under the null hypothesis) the probability of observing G ≥ 3.84 is about 5%. We often refer to this probability as the p-value.\n\nIf the p-value is small enough (e.g., below 0.05) then the performances of the new and the old model are very likely diﬀerent (the null hypothesis is rejected). In this case, if byes is higher than ayes, then the new model is very likely to work better than the old model; otherwise, the old model is better.\n\nIf the p-value corresponding to the value of G is not small enough then the observed diﬀerence of performance between the new and the old model is not statistically signiﬁcant, and you can keep the old model in production.\n\nIt is convenient to ﬁnd the p-value of the G-test using a programming language of your choice. In Python, it can be done in the following way:\n\nfrom scipy.stats import chi2 def get_p_value(G):\n\np_value = 1 - chi2.cdf(G, 1) return p_value\n\nThe following code will work for R:\n\nget_p_value <- function(G) {\n\np_value <- pchisq(G, df=1, lower.tail=FALSE) return(p_value)\n\n}\n\nStatistically, the result of the G-test is valid if we have at least 10 “yes” and “no” results in each of the two groups, though this estimate should be taken with a grain of salt. If testing is not too expensive, then having about 1000 “yes” and “no” results in each of the two groups,\n\n1More details on the derivation of the above formula can be found in a statistics textbook or on Wikipedia.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n.\n\n8\n\n1\n\n2\n\n3\n\nwith at least 100 answers of each type in each group, should be enough. Note that the total number of answers in the two groups can be diﬀerent.\n\nIf you can’t reach at least 100 answers of each type in each group at a reasonable cost, you can use an approximation of the p-value of a very similar test using Monte-Carlo simulation.\n\nThe following code will work for R:\n\np_value <- chisq.test(x,\n\nsimulate.p.value = TRUE)$p.value\n\n}\n\nWhere x is the 2 × 2 contingency table shown in Figure 3.\n\nNote that it is possible to test more than two models (e.g. models A, B, and C) and more than two possible answers to the question that deﬁne our metric (e.g., “yes,” “no,” “maybe”). If we want to test k diﬀerent models and l diﬀerent possible answers, the G statistic would follow a chi-square distribution with (k − 1) × (l − 1) degrees of freedom. The problem here is that a test with multiple models and answers will tell you whether there is something diﬀerent somewhere between your models, but it will not tell you where is the diﬀerence. In practice, it is easier to compare your current model with only one new model and to formulate a question metric with a binary answer. More complex experiment testing is outside the scope of this book.\n\nNote that it could be tempting, when we have more than two models, to do binary comparisons of pairs of models using a test designed for comparing two models. This is not recommended, however, as it could be scientiﬁcally wrong. It’s better to consult a statistician.\n\n7.2.2 Z-Test\n\nThe second formulation of A/B test applies when the question for each user is, “How many?” or, “How much?” (as opposed to a yes-or-no question considered in the previous subsection). Examples of questions include:\n\n1. How much time a user has spent on the website during a session? 2. How much money a user has spent during a month? 3. How many news articles a user has read during a week?\n\nFor simplicity of illustration, let’s measure the time a user spends on a website where our model is deployed. As usual, users are routed to versions A and B of the website, where version A serves the old model and version B serves the new model. The null hypothesis is that users of both versions spend, on average, the same amount of time. The alternative hypothesis is that they spend more time on website B than on website A. Let nA be the number of users routed to version A and nB be the number of users routed to version B. Let i and j denote users from groups A and B respectively.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nTo compute the value of the Z-test, we ﬁrst compute sample mean and sample vari- ance for A and B. The sample mean is given by:\n\nˆµA\n\ndef=\n\n1 nA\n\nnAX\n\ni=1\n\nai,\n\nˆµB\n\ndef=\n\n1 nB\n\nnAX\n\nj=1\n\nbj,\n\nwhere ai and bj is the time spent on the website by, respectively, users i and j.\n\nThe sample variance for A and B is given, respectively, by,\n\nˆσ2 A\n\ndef=\n\n1 nA\n\nnAX\n\ni=1\n\n(ˆµA − ai)2,\n\nˆσ2 B\n\ndef=\n\n1 nB\n\nnBX\n\nj=1\n\n(ˆµB − bj)2.\n\nThe value of the Z-test is then given by,\n\nZ def=\n\nˆµB − ˆµA q ˆσ2 + ˆσ2 A B nA nB\n\n.\n\nThe larger Z, the more likely the diﬀerence between A and B is signiﬁcant. Under the null hypothesis (i.e. A and B are equivalent), Z approximately follows a standardized normal distribution,\n\nZ ≈ N(0,1)\n\nThis is true only if the sample size is large and if σ2 advice from a statistician.\n\nA ≈ σ2\n\nB. If not, it is recommended to ask\n\nAs for the G-test, we will use the p-value to decide whether or not Z is large enough to think that the time spent on B is really greater than time spent on A. To compute the p-value, you check the probability of getting a Z-value from this distribution that is at least as extreme (out of line with the null hypothesis) as the Z-value you calculated. For example, let’s imagine your sample gave you Z = 2.64. If A and B were equal, the probability of observing Z ≥ 2.64 is about 5%.\n\nTo see the result of the test, you compare the p-value with the signiﬁcance level you chose. If your signiﬁcance level is 5%, then if the p-value is below 0.05, we reject the null hypothesis\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(2)\n\n(3)\n\n10\n\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\nthat says that the diﬀerence in performance of the two models is not statistically signiﬁcant. Thus, the new model works better than the old one.\n\nIf the p-value is above or equal to 0.05, then we do not reject the null hypothesis. Note that this is not the same as accepting the null hypothesis. The two models could still be diﬀerent, we just didn’t get evidence in support of that. In this case, we will stick with the old model unless evidence changes our mind. No evidence means we keep doing what we were doing. Note also that we cannot simply keep gathering evidence until the p-value goes below 0.05, as it would not be scientiﬁcally sound. It’s recommended to consult a statistician and design a diﬀerent test.\n\nAs for signiﬁcance levels, there’s no universal consensus on which threshold is optimal. The values of 0.05 or 0.01 are commonly used in practice. They were favorites of a trendsetting statistician Ronald Fisher in the 1920s. You should select a higher or a lower value if it’s appropriate for your application. The lower the value, the more evidence it takes to change your mind.\n\nSimilar to the G-test, it is convenient to ﬁnd the p-value of the Z-test using a programming language. In Python, it can be done in the following way:\n\nfrom scipy.stats import norm def get_p_value(Z):\n\np_value = norm.sf(Z) return p_value\n\nThe following code will work for R:\n\nget_p_value <- function(Z) {\n\np_value <- 1-pnorm(Z) return(p_value)\n\n}\n\nFor best results, it is recommended to set nA and nB to a value 1000 or higher.\n\n7.2.3 Concluding Remarks and Warnings\n\nAs mentioned in the beginning of this chapter, some methods highlighted in this chapter are provided as examples only and might not be appropriate for your speciﬁc business problem. In particular, the two statistical tests presented above are taught in schools and are indeed often used in practice, but, unfortunately, not all of those uses are appropriate for your business problem. While pointing this out, Cassie Kozyrkov, the Chief Decision Scientist at Google and one of the reviewers of this chapter, emphasized that the above two tests are rarely a good idea to apply in practice because they only show that two models are diﬀerent, but they don’t show whether the diﬀerence is “of at least x.” If replacing the old model with the new one has a signiﬁcant cost or poses a risk, then just knowing that the new model is “somewhat” better is not enough to make a replacement decision. In this case, an adjusted\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "page_number": 211,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 211-219). Key topics include models, learning, and data. Otherwise, the new data may yield suboptimal performance with old hyperparameters.",
      "keywords": [
        "Machine Learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "model",
        "learning",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "data",
        "machine learning model",
        "Burkov Machine",
        "Machine",
        "Machine Learning Systems",
        "training",
        "machine learning project",
        "learning algorithms"
      ],
      "concepts": [
        "models",
        "learning",
        "data",
        "train",
        "performance",
        "performing",
        "useful",
        "packages",
        "processing",
        "features"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 23,
          "title": "Segment 23 (pages 195-202)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 220-227)",
      "start_page": 220,
      "end_page": 227,
      "detection_method": "topic_boundary",
      "content": "test must be speciﬁcally crafted for the problem at hand, and the best way to do is to consult a statistician.2\n\nCarefully test the programming code of your A/B test. You will only have a valid model evaluation if you implemented everything right. Otherwise, you will not know that something is wrong: your test will not reveal that it’s broken.\n\nAlso, make sure to apply measurements in groups A and B at the same time. Remember that traﬃc on a website behaves diﬀerently at diﬀerent times of the day, or at diﬀerent days of the week. For the purity of the experiment, avoid comparing measurements from diﬀerent times. The same reasoning applies to other possible measurable parameters that might signiﬁcantly aﬀect user behavior, such as country of residence, speed of internet connection, or version of web browser.\n\n7.3 Multi-Armed Bandit\n\nA more advanced, and often preferable way of online model evaluation and selection, is multi-armed bandit (MAB). A/B testing has one major drawback. The number of test results in groups A and B you need to calculate the value of the A/B test is high. A signiﬁcant portion of users routed to a suboptimal model would experience suboptimal behavior for a long time.\n\nIdeally, we would like to expose a user to a suboptimal model as few times as possible. At the same time, we need to expose users to each of the two models a number of times suﬃcient to get reliable estimates of both models’ performance. This is known as the exploration- exploitation dilemma: on one hand, we want to explore the models’ performance enough to be able to reliably choose the better one. On the other hand, we want to exploit the performance of the better model as much as possible.\n\nIn probability theory, the multi-armed bandit problem is a problem in which a ﬁxed and limited set of resources must be allocated between competing choices in a way that maximizes the expected reward. Each choice’s properties are only partially known at the time of allocation, and may become better understood as time passes and we allocate resources to the choice.\n\nLet’s see how the multi-armed bandit problem applies to an online evaluation of two models. (The approach for more than two models is the same.)\n\nThe limited set of resources we have are the users of our system. The competing choices, also called “arms,” are our models. We can allocate a resource to a choice (in other words, we can “play an arm”) by routing a user to a version of the system running a speciﬁc model. We want to maximize the expected reward, where the reward is given by the business performance metric. Examples might be the average amount of time spent on the website during a session, the average number of news articles read during a week, the percentage of users who purchased the recommended article, etc.\n\n2Unfortunately, in a compact book describing all special cases and tests would be impractical. Please\n\nconsult the book’s companion wiki from time to time. More statistical tests will be added over time.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nUCB1 (for Upper Conﬁdence Bound) is a popular algorithm for solving the multi-armed bandit problem. The algorithm dynamically chooses an arm, based on the performance of that arm in the past, and how much the algorithm knows about it. In other words, UCB1 routes the user to the best performing model more often when its conﬁdence about the model performance is high. Otherwise, UCB1 might route the user to a suboptimal model so as to get a more conﬁdent estimate of that model’s performance. Once the algorithm is conﬁdent enough about the performance of each model, it almost always routes users to the best performing model.\n\nThe mathematics of UCB1 works as follows. Let ca denote the number of times the arm a was played since the beginning, and let va denote the average reward obtained from playing that arm. The reward corresponds to the value of the business performance metric. For the purpose of illustration, let the metric be the average time spent by the user in the system during one session. The reward for playing an arm is, thus, a particular session duration.\n\nIn the beginning, ca and va are zero for all arms, a = 1,...,M. Once an arm a is played, a reward r is observed, and ca is incremented by 1; va is then updated as follows:\n\nva ←\n\nca − 1 ca\n\n× va +\n\nr ca\n\n.\n\nAt each time step (that is, when a new user logs in), the arm to play (that is, the version of the system the user will be routed to) is chosen as follows. If ca = 0 for some arm a, then this arm is played; otherwise, the arm with the greatest UCB value is played. The UCB value of an arm a, denoted as ua, is deﬁned as follows:\n\nua\n\ndef= va +\n\ns\n\n2 × log(c) ca\n\n, where c def=\n\nM X\n\na\n\nca.\n\nThe algorithm is proven to converge to the optimal solution. That is, UCB1 will end up playing the best performing arm most of the time.\n\nIn Python, the code that implements UCB1, would look as follows:\n\nclass UCB1():\n\ndef __init__(self, n_arms): self.c = [0]*n_arms self.v = [0.0]*n_arms self.M = n_arms return\n\ndef select_arm(self):\n\nfor a in range(self.M):\n\nif self.c[a] == 0:\n\nreturn a u = [0.0]*self.M\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nc = sum(self.c) for a in range(self.M):\n\nbonus = math.sqrt((2 * math.log(c)) / float(self.c[a])) u[a] = self.v[a] + bonus\n\nreturn u.index(max(u))\n\ndef update(self, a, r): self.c[a] += 1 v_a = ((self.c[a] - 1) / float(self.c[a])) * self.v[a] \\\n\n+ (r / float(self.c[a]))\n\nself.v[a] = v_a return\n\nThe corresponding code in R would look as shown below:\n\nsetClass(\"UCB1\", representation(count=\"numeric\", value=\"numeric\", M=\"numeric\"))\n\nsetGeneric(\"select_arm\", function(x) standardGeneric(\"select_arm\")) setMethod(\"select_arm\", \"UCB1\", function(x) {\n\nfor (a in seq(from = 1, to = x@M, by = 1)) {\n\nif(x@count[a] == 0) {\n\nreturn(a)\n\n}\n\n} u <- rep(0.0, x@M) count <- sum(x@count) for (a in seq(from = 1, to = x@M, by = 1)){\n\nprint(a) bonus <- sqrt((2 * log(count)) / x@count[a]) u[a] <- x@value[a] + bonus\n\n} match(c(max(u)),u)\n\n})\n\nsetGeneric(\"update\", function(x, a, r) standardGeneric(\"update\")) setMethod(\"update\", \"UCB1\", function(x, a, r) {\n\nx@count[a] <- x@count[a] + 1 v_a <- ((x@count[a] - 1) / x@count[a]) * x@value[a] + (r / x@count[a]) x@value[a] <- v_a\n\n})\n\nUCB1 <- function(M) {\n\nnew(\"UCB1\", count = rep(0, M), value = rep(0.0, M), M = M)\n\n}\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\n1\n\n2\n\n7.4 Statistical Bounds on the Model Performance\n\nWhen reporting the model performance, sometimes, besides the value of the metric, it is required to also provide the statistical bounds, also known as the statistical interval.\n\nA reader familiar with other books on machine learning or some popular online blogs might wonder, why we use the term “statistical interval” and not “conﬁdence interval.” The reason is that in some machine learning literature, what the authors call a “conﬁdence interval” is in fact a “credible interval.” The diﬀerence between the two is clear and important for a statistician because the two terms have diﬀerent meanings in frequentist and Bayesian statistics. In this book, I decided not to burden the reader with the subtlety of the diﬀerence between the two terms. For a non-expert in statistics, it would be beneﬁcial to think of the statistical interval as follows: a 95% statistical interval indicates that there’s a 95% chance the parameter you’re estimating is between the intervals bounds. Strictly speaking, this is the deﬁnition of the credible interval. A conﬁdence interval’s interpretation is subtly diﬀerent, most newcomers to statistics won’t start to appreciate the diﬀerence until they’re a few textbooks in. For our purposes, the above interpretation of a statistical interval will suﬃce.\n\nThere are several techniques that allow establishing statistical bounds for a model. Some techniques apply to classiﬁcation models, and some can be applied to regression models. We will consider several techniques in this section.\n\n7.4.1 Statistical Interval for the Classiﬁcation Error\n\nIf you report the error ratio “err” for a classiﬁcation model (where err def= 1 − accuracy), then the following technique can be used to obtain the statistical interval for “err.”\n\nLet N be the size of the test set. Then, with probability 99%, “err” lies in the interval,\n\n[err − δ,err + δ],\n\nwhere δ def= zN\n\nqerr(1−err) N\n\n, and zN = 2.58.\n\nThe value of zN depends on the required conﬁdence level. For the conﬁdence level of 99%, zN = 2.58. For other conﬁdence level values, the values of zN can be found in the table below:\n\nconﬁdence level zN\n\n80% 90% 95% 98% 99% 2.58 1.96 1.28\n\n1.64\n\n2.33\n\nAs with p-values, it is convenient to ﬁnd the value of zN using a programming language. In Python, it can be done in the following way:\n\nfrom scipy.stats import norm def get_z_N(confidence_level): # a value in (0,100)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\nz_N = norm.ppf(1-0.5*(1 - confidence_level/100.0)) return z_N\n\nThe following code will work for R:\n\nget_z_N <- function(confidence_level) {# a value in (0,100)\n\nz_N <- qnorm(1-0.5*(1 - confidence_level/100.0)) return(z_N)\n\n}\n\nIn theory, the above technique works even for very tiny test sets with N ≥ 30. However, a more accurate rule of thumb for obtaining the minimum size N of the test set is as follows: ﬁnd the value of N such that N × err(1 − err) ≥ 5. Intuitively, the greater the size of the test set, the lower our uncertainty about the true performance of the model.\n\n7.4.2 Bootstrapping Statistical Interval\n\nA popular technique for reporting the statistical interval for any metric, and which applies to both classiﬁcation and regression, is based on the idea of bootstrapping. Bootstrapping is a statistical procedure that consists of building B samples of a dataset, and then training a model or computing some statistic using those B samples. In particular, the random forest learning algorithm is based on this idea.\n\nHere’s how bootstrapping applies for building a statistical interval for a metric. Given the test set, we create B random samples Sb, one for each b = 1,...,B. To obtain a sample Sb for some b, we use sampling with replacement. Sampling with replacement means that we start with an empty set, and then pick at random an example from the test set and put its exact copy in Sb by keeping the original example in the test set. We keep picking examples at random and putting them to Sb until |Sb| = N.\n\nOnce we have B bootstrap samples of the test set, we compute the value of the performance metric mb using each sample Sb as the test set. Sort the B values in ascending order. Then ﬁnd the value S of the sum of all B values of the metric: S def= PB b=1 mb. To obtain a c percent statistical interval for the metric, pick the tightest interval between a minimum a and a maximum b such that the sum of the values mb that lie in that interval accounts for at least c percent of S. Our statistical interval is then given by [a,b].\n\nThe above paragraph might sound vague, so let’s illustrate it with an example. Let’s have B = 10. Let the values of the metric, computed by applying the model to B bootstrap samples, be [9.8,7.5,7.9,10.1,9.7,8.4,7.1,9.9,7.7,8.5]. First, we sort those values in the increasing order: [7.1,7.5,7.7,7.9,8.4,8.5,9.7,9.8,9.9,10.1]. Let our conﬁdence level c be 80%. Then, the minimum a of the statistical interval will be 7.46 and the maximum b will be 9.92. The above two values were found using the percentile function in Python:\n\nfrom numpy import percentile def get_interval(values, confidence_level):\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\n3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n# confidence_level is a value in (0,100) lower = percentile(values, (100.0-confidence_level)/2.0) upper = percentile(values, confidence_level+((100.0-confidence_level)/2.0)) return (lower, upper)\n\nThe same can be done in R by using the quantile function:\n\nget_interval <- function(values, confidence_level) {\n\n# confidence_level is a value in (0,100) cl <- confidence_level/100.0 quant <- quantile(values, probs = c((1.0-cl)/2.0, cl+((1.0-cl)/2.0)), names = FALSE)\n\nreturn(quant)\n\n}\n\nOnce you have the boundaries a = 7.46 and b = 9.92 of the statistical interval, you can report that the value of the metric for your model lies in the interval [7.46,9.92] with conﬁdence 80%.\n\nIn practice, analysts use conﬁdence levels of either 95% or 99%. The higher the conﬁdence, the wider the interval. The number B of bootstrap samples is usually set to 100.\n\n7.4.3 Bootstrapping Prediction Interval for Regression\n\nUntil now, we considered the statistical interval for an entire model and a given performance metric. In this section, we will use bootstrapping to compute the prediction interval for a regression model and a given feature vector x, which this model receives as input.\n\nWe want to answer the following question. Given a regression model f and an input feature vector x, what is an interval of values [fmin(x),fmax(x)] such that the prediction f(x) lies inside that interval with conﬁdence c percent?\n\nThe bootstrapping procedure here is similar. The only diﬀerence is that now we build B bootstrap samples of the training set (and not the test set). By using B bootstrap samples as B training sets, we build B regression models, one per bootstrap sample. Let the input feature vector be x. Fix a conﬁdence level c. Apply B models to x and obtain B predictions. Now, by using the same technique as above, ﬁnd the tightest interval between a minimum a and a maximum b such that the sum of the values of predictions that lie in the interval accounts for at least c percent of the sum of B predictions. Then return the prediction f(x), and state that, with conﬁdence c percent, it lies in the interval [a,b].\n\nAs previously, the conﬁdence level usually is either 95% or 99%. The number B of bootstrap samples is set to 100 (or as many as the time allows).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\n7.5 Evaluation of Test Set Adequacy\n\nIn traditional software engineering, tests are used to identify defects in the software. The collection of tests is constructed in such a way that they allow discovery of bugs in the code before the software reaches production. The same approach applies to the testing of all the code developed “around” the statistical model: the code that gets the input from the user, transforms it into features, and the code that interprets the outputs of the model, and serves the result to the user.\n\nHowever, an additional evaluation must be applied to the model itself. The test examples used to evaluate the model must also be designed in such a way that they allow the discovery of the model’s defective behavior before the model reaches production.\n\n7.5.1 Neuron Coverage\n\nWhen we evaluate a neural network, especially one to be used in a mission-critical scenario, such as a self-driving car or a space rocket, our test set must have good coverage. Neuron coverage of a test set for a neural network model is deﬁned as the ratio of the units (neurons) activated by the examples from the test set, to the total number of units. A good test set has close to 100% neuron coverage.\n\nA technique for building such a test set is to start with a set of unlabeled examples, and all units of the model uncovered. Then, iteratively, we\n\n1) randomly pick an unlabeled example i and label it, 2) send the feature vector xi to the input of the model, 3) observe which units in the model were activated by xi, 4) if the prediction was correct, mark those units as covered, 5) go back to step 1; continue iterating until the neuron coverage becomes close to 100%.\n\nA unit is considered activated when its output is above a certain threshold. For ReLU, it’s usually zero; for a logistic sigmoid, it’s 0.5.\n\n7.5.2 Mutation Testing\n\nIn software engineering, good test coverage for a software under test (SUT) can be determined using the approach known as mutation testing. Let’s have a set of tests designed to test an SUT. We generate several “mutants” of the SUT. A mutant is a version of the SUT in which we randomly make some modiﬁcations, such as replacing in the source code, a “+” with a “−”, a “<” with a “>”, delete the else command in an if-else statement, and so on. Then we apply the test set to each mutant, and see if at least one test breaks on that mutant. We say that we kill a mutant if one test breaks on it. We then compute the ratio of killed mutants in the entire collection of mutants. A good test set makes this ratio equal to 100%.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\nIn machine learning, a similar approach can be followed. However, to create a mutant statistical model, instead of modifying the code, we modify the training data. If the model is deep, we can also randomly remove or add a layer, or remove or replace an activation function. The training data can be modiﬁed by,\n\nadding duplicated examples, • falsifying the labels of some examples, • removing some examples, or • adding random noise to the values of some features.\n\nWe say that we kill a mutant if at least one test example gets a wrong prediction by that mutant statistical model.\n\n7.6 Evaluation of Model Properties\n\nWhen we measure the quality of the model according to some performance metric, such as accuracy or AUC, we evaluate its correctness property. Besides this commonly evaluated property of the model, it can be appropriate to evaluate other properties of the model, such as robustness and fairness.\n\n7.6.1 Robustness\n\nThe robustness of a machine learning model refers to the stability of the model performance after adding some noise to the input data. A robust model would exhibit the following behavior. If the input example is perturbed by adding random noise, the performance of the model would degrade proportionally to the level of noise.\n\nConsider an input feature vector x. Let us, before applying a model f to that input example, modify the values of some features, chosen randomly, by replacing them with a zero, to obtain a modiﬁed input x0. Continue randomly choosing and replacing values of features in x, as long as the Euclidean distance between x and x0 remains below some δ. Then apply the model f to x and x0 to obtain predictions f(x) and f(x0). Fix values of δ and (cid:15). The model f is said to be (cid:15)-robust to a δ-perturbation of the input, if, for any x and x0, such that kx − x0k ≤ δ, we have |f(x) − f(x0)| ≤ (cid:15).\n\nIf you have several models that perform similarly according to the performance metric, you would prefer to deploy in production a model that is (cid:15)-robust, when applied to the test data, with the smallest (cid:15). However, in practice, it’s not always clear how to set the appropriate value of δ. A more practical way to identify a robust model among several candidates is as follows.\n\nLet us say that a certain test set is δ-perturbed if we obtained it by applying a δ-perturbation to all examples in a certain original test set. Pick the model f you want tested for robustness. Set a reasonable value of ˆ(cid:15) such that, if the model prediction in production is not farther from the correct prediction than ˆ(cid:15), you would consider that acceptable. Start with a small value of\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "page_number": 220,
      "chapter_number": 26,
      "summary": "This chapter covers segment 26 (pages 220-227). Key topics include model, data, and statistics. Cyber attackers or competitors may attempt to reverse engineer the model’s training data.",
      "keywords": [
        "Model",
        "Machine learning engineering",
        "Model Evaluation",
        "Burkov Machine Learning",
        "online model evaluation",
        "machine learning",
        "Andriy Burkov Machine",
        "data",
        "online model",
        "learning engineering",
        "Burkov Machine",
        "Evaluation",
        "online",
        "online data",
        "oﬄine model"
      ],
      "concepts": [
        "model",
        "data",
        "statistics",
        "user",
        "business",
        "performance",
        "performing",
        "samples",
        "evaluation",
        "evaluated"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "Segment 21 (pages 178-185)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 31,
          "title": "Segment 31 (pages 263-271)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 228-235)",
      "start_page": 228,
      "end_page": 235,
      "detection_method": "topic_boundary",
      "content": "δ and build a δ-perturbed dataset. Find the minimum (cid:15) such that for each example x from the original test set and its counterpart x0 from the δ-perturbed test set, |f(x) − f(x0)| ≤ (cid:15).\n\nIf (cid:15) ≥ ˆ(cid:15), you have chosen too high a value for δ; set a lower value and start over.\n\nIf (cid:15) < ˆ(cid:15), then slightly increase δ, build a new δ-perturbed test set, ﬁnd (cid:15) for this new δ-perturbed test set, and continue increasing δ as long as (cid:15) remains less than ˆ(cid:15). Once you ﬁnd the value of δ = ˆδ where (cid:15) ≥ ˆ(cid:15), note that the model f you are testing for robustness is ˆ(cid:15)-robust to ˆδ-perturbation of the input. Now pick another model you want to test for robustness, and ﬁnd its ˆδ; continue like that until all models are tested. Once you have the value of ˆδ-perturbation for each model, deploy in production the model whose ˆδ is the greatest.\n\n7.6.2 Fairness\n\nMachine learning algorithms tend to learn what humans are teaching them. The teaching comes in the form of training examples. Humans have biases which may aﬀect how they collect and label data. Sometimes, bias is present in historical, cultural, or geographical data. This, in turn, as we have seen in Section ?? in Chapter 3, may lead to biased models.\n\nThe attributes that are sensitive and need protection from unfairness are called protected or sensitive attributes. Examples of legally-recognized and protected attributes include race, skin color, gender, religion, national origin, citizenship, age, pregnancy, familial status, disability status, veteran status, and genetic information.\n\nFairness is often domain-speciﬁc, and each domain may have its own regulations. Regulated domains include credit, education, employment, housing, and public accommodation.\n\nThe deﬁnition of fairness varies greatly, depending on the domain. At the time of writing this book, there is no ﬁrm consensus, in the scientiﬁc and technical literature, on what is fairness. Most commonly cited concepts are demographic parity and equal opportunity.\n\nDemographic parity (also known as statistical parity, or independence parity) means the proportion of each segment of a protected attribute receives a positive prediction from the model at equal rates.\n\nLet a positive prediction mean “acceptance to university,” or “granting a loan.” Mathemati- cally, demographic parity is deﬁned as follows. Let G1 and G2 be the two disjoint groups belonging to the test data, divided by a sensitive attribute j, such as gender. Let x(j) = 1 if x represents a woman, and x(j) = 0 otherwise. A binary model f under test satisﬁes demographic parity if Pr(f(xi) = 1|xi ∈ G1) = Pr(f(xk) = 1|xk ∈ G2). That is, as measured on the test data, the chance to predict 1 with the model f for women is the same as the chance to predict 1 for men.\n\nThe exclusion of the protected attributes from the feature vector in the training data doesn’t guarantee that the model will have demographic parity, as some of the remaining features\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\nmay be correlated with the excluded ones.\n\nEqual opportunity means each group gets a positive prediction from the model at equal rates, assuming that people in this group qualify for it.\n\nMathematically, a binary model f under test satisﬁes equal opportunity if Pr(f(xi) = 1|xi ∈ G1 and yi = 1) = Pr(f(xk) = 1|xk ∈ G2 and yk = 1), where yi and yk are the actual labels of the feature vectors xi and xk, respectively. The above equality means that, as measured on the test data, the chance to predict 1 by the model f for women who qualify for that prediction is the same as the chance to predict 1 for men who also qualify. In the terms of the confusion matrix, equal opportunity requires the true positive rate (TPR) to be equal for each value of the protected attribute.\n\n7.7 Summary\n\nAll statistical models running in production must be carefully and continuously evaluated.\n\nDepending on the model’s applicative domain and the organization’s goals and constraints, model evaluation will include the following tasks:\n\nestimate legal risks of putting the model in production, • understand the main properties of the distribution of the data used to train the model, • evaluate the performance of the model prior to deployment, and • monitor the performance of the deployed model.\n\nAn oﬄine model evaluation happens after the model was trained. It is based on the historical data. The online model evaluation consists of testing and comparing models in the production environment using online data.\n\nA popular technique of online model evaluation is A/B testing. When performing A/B testing, we split users into two groups, A and B. The two groups are served the old and the new models, respectively. Then we apply a statistical signiﬁcance test to decide whether the new model is statistically diﬀerent from the old model.\n\nMulti-armed bandit is another popular technique of online model evaluation. We start by randomly exposing all models to the users. Then we gradually reduce the exposure of the least- performing models until only one, the best performing model, gets served most of the time.\n\nIn addition to reporting training model performance metrics, one may also need to provide the statistical bounds known as the statistical interval.\n\nFor both classiﬁcation and regression models, a statistical interval for any metric can be computed using a popular technique called bootstrapping. It is a statistical procedure that consists of building B samples of a dataset, and then training a model and computing some statistic using each of those B samples.\n\nThe test examples used to evaluate the model must allow the discovery of defective behavior before the model reaches production. Such techniques as neuron coverage and mutation\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\ntesting can be used to evaluate the test set.\n\nWhen the model is used in a mission-critical system, or in regulated domains (such as credit, education, employment, housing, and public accommodation) accuracy, robustness, and fairness may have to be evaluated.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n8 Model Deployment\n\nOnce the model has been built and thoroughly tested, it can be deployed. Deploying a model means to make it available for accepting queries generated by the users of the production system. Once the production system accepts the query, the latter is transformed into a feature vector. The feature vector is then sent to the model as input for scoring. The result of the scoring then is returned to the user.\n\nModel deployment is the sixth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nA trained model can be deployed in various ways. It can be deployed on a server, or on a user’s device. It can be deployed for all users at once, or to a small fraction of users. Below, we consider all the options.\n\nA model can be deployed following several patterns:\n\nstatically, as a part of an installable software package, • dynamically on the user’s device, • dynamically on a server, or • via model streaming.\n\n8.1 Static Deployment\n\nThe static deployment of a machine learning model is very similar to traditional software deployment: you prepare an installable binary of the entire software. The model is packaged\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nas a resource available at the runtime. Depending on the operating system and the runtime environment, the objects of both the model and the feature extractor can be packaged as a part of a dynamic-link library (DLL on Windows), Shared Objects (*.so ﬁles on Linux), or be serialized and saved in the standard resource location for virtual machine-based systems, such as Java and .Net.\n\nStatic deployment has many advantages:\n\nthe software has direct access to the model, so the execution time is fast for the user, • the user data doesn’t have to be uploaded to the server at the time of prediction; this saves time and preserves privacy,\n\nthe model can be called when the user is oﬄine, and • the software vendor doesn’t have to care about keeping the model operational; it becomes the user’s responsibility.\n\nHowever, a static deployment also has several drawbacks. First and foremost, the separation of concerns between the machine learning code and the application code isn’t always obvious. This makes it harder to upgrade the model without also having to upgrade the entire application. Second, if the model has certain computational requirements for scoring (such as access to an accelerator or a GPU), it may add complexity and confusion as to where the static deployment can or cannot be used.\n\n8.2 Dynamic Deployment on User’s Device\n\nA dynamic deployment on devices is similar to a static deployment, in the sense the user runs a part of the system as a software application on their device. The diﬀerence is that in dynamic deployment, the model is not part of the binary code of the application. Thus it achieves better separation of concerns. Pushing model updates is done without updating the whole application running on the user’s device. Moreover, a dynamic deployment may allow the same piece of code to select the right model, based on the available compute resources.\n\nDynamic deployment can be achieved in several ways:\n\nby deploying model parameters, • by deploying a serialized object, and • by deploying to the browser.\n\n8.2.1 Deployment of Model Parameters\n\nIn this deployment scenario, the model ﬁle only contains the learned parameters, while the user’s device has installed a runtime environment for the model. Some machine learning packages, like TensorFlow, have a lightweight version that can run on mobile devices.\n\nAlternatively, frameworks such as Apple’s Core ML allow running models created using popular packages, including scikit-learn, Keras, and XGBoost, on Apple devices.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\n8.2.2 Deployment of a Serialized Object\n\nHere, the model ﬁle is a serialized object that the application would deserialize. The advantage of this approach is that you don’t need to have a runtime environment for your model on the user’s device. All needed dependencies will be deserialized with the object of the model.\n\nAn evident drawback is that an update might be quite “heavy,” which is a problem if your software system has millions of users.\n\n8.2.3 Deploying to Browser\n\nMost modern devices have access to a browser, either desktop or mobile. Some machine learning frameworks, such as TensorFlow.js, have versions that allow to train and run a model in a browser, by using JavaScript as a runtime.\n\nIt’s even possible to train a TensorFlow model in Python, and then deploy it to, and run it in the browser’s JavaScript runtime environment. Additionally, if a GPU (graphics processing unit) is available on the client’s device, Tensorﬂow.js can leverage it.\n\n8.2.4 Advantages and Drawbacks\n\nThe main advantage of dynamic deployment to users’ devices is that the calls to the model will be fast for the user. It also reduces the impact on the organization’s servers, as most computations are performed on the user’s device. Additionally, if the model is deployed to the browser, the organization’s infrastructure only needs to serve a web page that includes the model’s parameters. A downside of the browser-based deployment is that the bandwidth cost and application startup time might increase. The users must download the model’s parameters each time they start the web application, as opposed to doing it only once when they install an application.\n\nAnother drawback occurs during model updates. Recall, a serialized object can be quite voluminous. Some users may be oﬄine during the update, or even turn oﬀ all future updates. In that case, you may end up with diﬀerent users using very diﬀerent model versions. Now it becomes diﬃcult to upgrade the server-side part of the application.\n\nDeploying models on the user’s device means that the model easily becomes available for third-party analyses. They may try to reverse-engineer the model to reproduce its behavior. They may search for weaknesses by providing various inputs and observing the output. Or, they may adapt their data so the model predicts what they want.\n\nSuppose the mobile application allows the user to read news related to their interests. A content provider might try to reverse engineer the model, so that it now recommends more often the news from that content provider.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nFigure 2: Deploying a machine learning model as a web service on a virtual machine.\n\nAs with static deployment, deploying to a user’s device makes it diﬃcult to monitor the model performance.\n\n8.3 Dynamic Deployment on a Server\n\nBecause of the above complications, and problems with performance monitoring, the most frequent deployment pattern is to place the model on a server (or servers), and make it available as a Representational State Transfer application programming interface (REST API) in the form of a web service, or Google’s Remote Procedure Call (gRPC) service.\n\n8.3.1 Deployment on a Virtual Machine\n\nIn a typical web service architecture deployed in a cloud environment, the predictions are served in response to canonically-formatted HTTP requests. A web service running on a virtual machine receives a user request containing the input data, calls the machine learning system on that input data, and then transforms the output of the machine learning system into the output JavaScript Object Notation (JSON) or Extensible Markup Language (XML) string. To cope with high load, several identical virtual machines are running in parallel.\n\nA load balancer dispatches the incoming requests to a speciﬁc virtual machine, depending\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "page_number": 228,
      "chapter_number": 27,
      "summary": "In this case, we will stick with the old model unless evidence changes our mind Key topics include models, time, and statistically. Similar to the G-test, it is convenient to ﬁnd the p-value of the Z-test using a programming language.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "test set",
        "statistical interval",
        "model",
        "interval",
        "Machine Learning",
        "Learning Engineering",
        "Burkov Machine",
        "statistical",
        "Andriy Burkov",
        "level",
        "arm",
        "Conﬁdence"
      ],
      "concepts": [
        "models",
        "time",
        "statistically",
        "values",
        "let",
        "interval",
        "sets",
        "err",
        "follows",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 33,
          "title": "Segment 33 (pages 281-288)",
          "relevance_score": 0.31,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 11,
          "title": "Segment 11 (pages 86-93)",
          "relevance_score": 0.31,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 27,
          "title": "Segment 27 (pages 229-236)",
          "relevance_score": 0.31,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 21,
          "title": "Segment 21 (pages 178-185)",
          "relevance_score": 0.3,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 236-244)",
      "start_page": 236,
      "end_page": 244,
      "detection_method": "topic_boundary",
      "content": "on its availability. The virtual machines can be added and closed manually, or be a part of an autoscaling group that launches or terminates virtual machines based on their usage. Figure 2 illustrates that deployment pattern. Each instance, denoted as an orange square, contains all the code needed to run the feature extractor and the model. The instance also contains a web service that has access to that code.\n\nIn Python, a REST API web service is usually implemented using a web application framework such as Flask or FastAPI. An R equivalent is Plumber.\n\nTensorFlow, a popular framework used to train deep models, comes with TensorFlow Serving, a built-in gRPC service.\n\nThe advantage of deploying on a virtual machine is that the architecture of the software system is conceptually simple: it’s a typical web or gRPC service.\n\nAmong the downsides, there is a need to maintain servers (physical or virtual). If virtualization is used, then there is an additional computational overhead due to virtualization and running multiple operating systems. Another is network latency, which can be a serious issue, depending on how fast you need to process scoring results. Finally, deploying on a virtual machine has a relatively higher cost, compared to deployment in a container, or a serverless deployment that we consider below.\n\n8.3.2 Deployment in a Container\n\nA more modern alternative to a virtual-machine-based deployment is a container-based deployment. Working with containers is typically considered more resource-eﬃcient and ﬂexible than with virtual machines. A container is similar to a virtual machine, in the sense that it is also an isolated runtime environment with its own ﬁlesystem, CPU, memory, and process space. The main diﬀerence, however, is that all containers are running on the same virtual or physical machine and share the operating system, while each virtual machine runs its own instance of the operating system.\n\nThe deployment process looks as follows. The machine learning system and the web service are installed inside a container. Usually, a container is a Docker container, but there are alternatives. Then a container-orchestration system is used to run the containers on a cluster of physical or virtual servers. A typical choice of a container-orchestration system for running on-premises or in a cloud platform, is Kubernetes. Some cloud platforms provide both their own container-orchestration engine, such as AWS Fargate and Google Kubernetes Engine, and support Kubernetes natively.\n\nFigure 3 illustrates that deployment pattern. Here, the virtual or physical machines are organized into a cluster, whose resources are managed by the container orchestrator. New virtual or physical machines can be manually added to the cluster, or closed. If your software is deployed in a cloud environment, a cluster autoscaler can launch (and add to the cluster) or terminate virtual machines, based on the usage of the cluster.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\nFigure 3: Deploying a model as a web service in a container running on a cluster.\n\nDeployment in a container has the advantage of being more resource-eﬃcient as compared to the deployment on a virtual machine. It allows the possibility to automatically scale with scoring requests. It also allows us to scale-to-zero. The idea of the scale-to-zero is that a container can be reduced down to zero replicas when idle and brought back up if there is a request to serve. As a result, the resource consumption is low compared to always running services. This leads to less power consumption and saves cost of cloud resources.\n\nOne drawback is that the containerized deployment is generally seen as more complicated, and requires expertise.\n\n8.3.3 Serverless Deployment\n\nSeveral cloud services providers, including Amazon, Google, and Microsoft, oﬀer so-called serverless computing. It’s known under the name of Lambda-functions on Amazon Web Services, and Functions on Microsoft Azure and Google Cloud Platform.\n\nThe serverless deployment consists of preparing a zip archive with all the code needed to run\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nthe machine learning system (model, feature extractor, and scoring code). The zip archive must contain a ﬁle with a speciﬁc name that contains a speciﬁc function, or class-method deﬁnition with a speciﬁc signature (an entry point function). The zip archive is uploaded to the cloud platform and registered under a unique name.\n\nThe cloud platform provides an API to submit inputs to the serverless function. This speciﬁes its name, provides the payload, and yields the outputs. The cloud platform takes care of deploying the code and the model on an adequate computational resource, executing the code, and routing the output back to the client.\n\nUsually, the function’s execution time, zip ﬁle size, and amount of RAM available on the runtime are limited by the cloud service provider.\n\nThe zip ﬁle size limit can be a challenge. A typical machine learning model requires multiple heavyweight dependencies. Python’s libraries, to include Numpy, SciPy, and scikit-learn, are often needed for the model to be properly executed. Depending on the cloud platform, other supported programming languages can include Java, Go, PowerShell, Node.js, C#, and Ruby.\n\nThere are many advantages to relying on serverless deployment. The obvious advantage is that you don’t have to provision resources such as servers or virtual machines. You don’t have to install dependencies, maintain, or upgrade the system. Serverless systems are highly scalable and can easily and eﬀortlessly support thousands of requests per second. Serverless functions support both synchronous and asynchronous modes of operation.\n\nServerless deployment is also cost-eﬃcient: you only pay for compute-time. This may also be achieved with the previous two deployment patterns using autoscaling, but autoscaling has signiﬁcant latency. While the demand may drop, excessive virtual machines could still keep running before they are terminated.\n\nServerless deployment also simpliﬁes canary deployment, or canarying. In software engineering, canarying is a strategy when the updated code is pushed to just a small group of end-users, usually unaware. Because the new version is only distributed to a small number of users, its impact is relatively low, and changes can be reversed quickly, should the new code contain bugs. It is easy to set up two versions of serverless functions in production, and start sending low volume traﬃc to just one, and test it without aﬀecting many users. We will talk more about canarying in Section 8.4.\n\nRollbacks are also very simple in the serverless deployment because it is easy to switch back to the previous version of the function by replacing one zip archive.\n\nWe’ve discussed the zip archive size limit, and the RAM available on the runtime. These are important drawbacks to serverless deployment. Likewise, the unavailability of GPU access1 can be a signiﬁcant limitation for deploying deep models.\n\nOf course, complex software systems may combine deployment patterns. A deployment pattern appropriate for one model may be less optimal for another one. A combination of several deployment patterns is called a hybrid deployment pattern. Personal assistants\n\n1As of July 2020.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\nlike Google Home or Amazon Echo might have a model that recognizes the activation phrase (such as “OK, Google” or “Alexa”) deployed on the client’s device, and more complex models handle requests like “put song X on device Y” will instead run on the server. Alternatively, the deployment on the user’s mobile device might augment the video and add simple intelligent eﬀects in realtime. Server deployment would be used to apply more complex eﬀects, such as stabilization and super-resolution.\n\n8.3.4 Model Streaming\n\nModel streaming is a deployment pattern that can be seen as an inverse to the REST API. In REST API, the client sends a request to the server, and then waits for a response (a prediction).\n\nIn complex systems, there can be many models applied to the same input. Or, a model can input a prediction from another model. For example, the input may be a news article. One model can predict the topic of the article, another model can extract named entities, the third model can generate a summarization of the article, and so on.\n\nAccording to the REST API deployment pattern, we need one REST API per model. The client would call one API by sending a news article as a part of the request, and get the topic as response. Then the client calls another API by sending a news article, and gets the named entities as response; etc.\n\nStreaming works diﬀerently. Instead of having one REST API per model, all models, as well as the code needed to run them, are registered within a stream-processing engine (SPE). Examples are Apache Storm, Apache Spark, and Apache Flink. Or, they are packaged as an application based on a stream-processing library (SPL), such as Apache Samza, Apache Kafka Streams, and Akka Streams.\n\nThe descriptions of these SPEs and SPLs are beyond the scope of this book, but they all share the same property making them diﬀerent from the REST-API-based applications. In each stream-processing application, there is an implicit or explicit notion of data processing topology. The input data ﬂows in as an inﬁnite stream of data elements sent by the client. Following a predeﬁned topology, each data element in the stream undergoes a transformation in the nodes of the topology. Transformed, the ﬂow continues to other nodes.\n\nIn a stream-processing application, nodes transform their input in some way, and then either,\n\nsend the output to other nodes, or • send the output to the client, or • persist the output to the database or a ﬁlesystem.\n\nOne node could take a news article and predict its topic; another node could take both the news article and the predicted topic and generate a summary; and so on.\n\nThe diﬀerence between a REST-API-based application and a streaming-based one is shown in Figure 4. Figure 4a shows a client using a REST API, processing one data element, such as a news article, by sending a series of requests. One by one, various REST APIs receive\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10\n\nFigure 4: The diﬀerence between a REST API and streaming: (a) to process one data element, a client using a REST API sends a series of requests, one by one, and receives responses synchronously; (b) to process one data element, a client using streaming opens a connection, sends a request, and receives update events as they happen.\n\nrequests and produce responses synchronously. On the other hand, a client using streaming (Figure 4b) opens a connection to the streaming application, sends a request, and receives update events as they happen.\n\nOn the right-hand side of the streaming-based application in Figure 4b, there’s a topology that deﬁnes the data ﬂow in the application. Each input element sent by the client passes through all the nodes of the topology graph. Nodes can send updated events to the client, and/or persist data to the database or a ﬁlesystem.\n\nAn SPE-based streaming application runs on its own cluster of virtual or physical machines, and takes care of distributing the data processing load among the available resources. An SPL- based streaming application doesn’t need a dedicated cluster for data processing. It can be integrated with available resources, such as virtual or physical machines, or a container orchestrator (such as Kubernetes).\n\nREST APIs are usually employed to let clients send ad-hoc requests that don’t follow a\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\ncertain frequently-repeated pattern. It’s the best choice when the client wants the liberty of deciding what to do with the API response. On the other hand, if each request of the client is:\n\ntypical, • undergoes a certain pattern of transformations, especially multiple intermediate trans- formations, and\n\nalways results in the same actions, such as persistence of speciﬁc data elements to the ﬁlesystem or database, then streaming-based applications provide better resource- eﬃciency, lower latency, security, and fault-tolerance.\n\n8.4 Deployment Strategies\n\nTypical deployment strategies are:\n\nsingle deployment, • silent deployment, • canary deployment, and • multi-armed bandit.\n\nLet’s consider each of them.\n\n8.4.1 Single Deployment\n\nSingle deployment is the simplest one. Conceptually, once you have a new model, you serialize it into a ﬁle, and then replace the old ﬁle with the new one. You also replace the feature extractor, if needed.\n\nTo deploy on a server in a cloud environment, you prepare a new virtual machine, or a container running the new version of the model. Then you replace the virtual machine image or that of the container. Finally, you gradually close the old machines or containers, and let the autoscaler start the new ones.\n\nTo deploy on a physical server, you will upload a new model ﬁle (and the feature extraction object, if needed) on the server. Then you replace old ﬁles and old code with the new versions, and restart the web service.\n\nTo deploy on the user’s device, you push the new model ﬁle to the user’s device, along with any needed feature extraction object, and restart the software.\n\nIf you use interpretable code, the feature extractor object can be deployed by replacing one source code ﬁle with another one. To avoid redeploying the entire software appli- cation, on either the server or the user’s device, the feature extractor’s object can be serialized into a ﬁle. Then, on each startup, the software running the model would deserialize the feature extractor object.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\nSingle deployment has the advantage of being simple; however, it’s also the riskiest strategy. If the new model or the feature extractor contains a bug, all users will be aﬀected.\n\n8.4.2 Silent Deployment\n\nA counterpart of the single deployment is silent deployment. It deploys the new model version and new feature extractor, and keeps the old ones. Both versions run in parallel. However, the user will not be exposed to the new version until the switch is done. The predictions made by the new version are only logged. After some time, they are analyzed to detect possible bugs.\n\nSilent deployment has the beneﬁt of providing enough time to ensure the new model works as expected, without adversely aﬀecting any users. The drawback is the need to run twice as many models, which consumes more resources. Furthermore, for many applications, it’s impossible to evaluate the new model without exposing its predictions to the user.\n\n8.4.3 Canary Deployment\n\nRecall, canary deployment, or canarying, pushes the new model version and code to a small fraction of users, while keeping the old version running for most users. Contrary to the silent deployment, canary deployment allows validating the new model’s performance, and its predictions’ eﬀects. Contrary to the single deployment, canary deployment doesn’t aﬀect lots of users in case of possible bugs.\n\nBy opting for the canary deployment, you accept the additional complexity of having and maintaining several versions of the model deployed simultaneously.\n\nAn obvious drawback of the canary deployment is that it’s impossible for engineers to spot rare errors. If you deploy the new version to 5% of users, and a bug aﬀects 2% of users, then you have only 0.1% chance that the bug will be discovered.\n\n8.4.4 Multi-Armed Bandits\n\nAs seen in Section ?? of Chapter 7, multi-armed bandits (MAB) are a way to compare one or more versions of the model in the production environment, and select the best performing one. MABs have an interesting property: after an initial exploration period, during which the MAB algorithm gathers enough evidence to evaluate the performance of each model (arm), the best arm is eventually played all the time. It means that after the convergence of the MAB algorithm, most of the time, all users are routed to the software version running the best model.\n\nThe MAB algorithm, thus, solves two problems — online model evaluation and model deployment — simultaneously.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\n8.5 Automated Deployment, Versioning, and Metadata\n\nThe model is an important asset, but it’s never delivered alone. There are additional assets for production model testing that ensure the model is not broken.\n\n8.5.1 Model Accompanying Assets\n\nOnly deploy a model in production when it’s accompanied with the following assets:\n\nan end-to-end set that deﬁnes model inputs and outputs that must always work, • a conﬁdence test set that correctly deﬁnes model inputs and outputs, and is used to compute the value of the metric,\n\na performance metric whose value will be calculated on the conﬁdence test set by applying the model to it, and\n\nthe range of acceptable values of the performance metric.\n\nOnce the system using the model is initially evoked on an instance of a server or client’s device, an external process must call the model on the end-to-end test data and validate that all predictions are correct. Furthermore, the same external process must validate that the value of the performance metric computed by applying the model to the conﬁdence test set is within the range of acceptable values. If either of two evaluations fails, the model should not be served to the client.\n\n8.5.2 Version Sync\n\nThe versions of the following three elements must always be in sync:\n\n1) training data, 2) feature extractor, and 3) model.\n\nEach update to the data must produce a new version in the data repository. The model trained using a speciﬁc version of the data must be put into the model repository with the same version number as that of the data used to train the model.\n\nIf the feature extractor was not changed, its version still must be updated to be in sync with the data and the model. If the feature extractor was updated, then a new model must be built using an updated feature extractor, and the versions are incremented for the feature extractor, the model, and the training data (even if the latter wasn’t changed).\n\nThe deployment of a new model version must be automated by a script in a transactional way. Given a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment. The model must be applied to the end-to-end and conﬁdence test data by simulating a regular call from the outside. If there’s a prediction error for the end-to-end\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\ntest data, or the value of the metric is not within the range of acceptable values, the entire deployment has to be rolled back.\n\n8.5.3 Model Version Metadata\n\nEach model version must be accompanied with the following code and metadata:\n\nthe name and the version of the library or package used to train the model, • if Python was used to build the model, then requirements.txt of the virtual environment used to build the model (or, alternatively, a Docker image name pointing to a speciﬁc path on Docker Hub or in your Docker registry),\n\nthe name of the learning algorithm, and names and values of the hyperparameters, • the list of features required by the model, • the list of outputs, their types, and how the outputs should be consumed, • the version and location of the data used to train the model, • the version and location of the validation data used to tune model’s hyperparameters, • the model scoring code that runs the model on new data and outputs the prediction.\n\nThe metadata and the scoring code may be saved to a database or to a JSON/XML text ﬁle.\n\nFor audit purposes, the following information must also accompany each deployment:\n\nwho built the model and when, • who and when made the decision of deploying that model, and based on what grounds, • who reviewed the model for privacy and security compliance purposes.\n\n8.6 Model Deployment Best Practices\n\nIn this section, we discuss practical aspects of deploying machine learning systems in produc- tion. We also outline several useful and practical tips for model deployment.\n\n8.6.1 Algorithmic Eﬃciency\n\nMost data analysts work in Python or R. While there are web frameworks that allow building web services in those two languages, they are not considered the most eﬃcient languages.\n\nIndeed, when you use scientiﬁc packages in Python, much of their code was written in eﬃcient C or C++, and then compiled for your speciﬁc operating system. However, your own data preprocessing, feature extraction, and scoring code may not be as eﬃcient.\n\nFurthermore, not all algorithms are practical. While some algorithms can quickly solve the problem, others are too slow. For some problems, no fast algorithms can exist.\n\nThe subﬁeld of computer science called analysis of algorithms is concerned with determining and comparing the complexity of algorithms. The big O notation is used to classify\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "page_number": 236,
      "chapter_number": 28,
      "summary": "This chapter covers segment 28 (pages 236-244). Key topics include model, tested, and examples. However, to create a mutant statistical model, instead of modifying the code, we modify the training data.",
      "keywords": [
        "model",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning",
        "Andriy Burkov Machine",
        "machine learning model",
        "Burkov Machine",
        "Learning Engineering",
        "deployment",
        "machine",
        "cid",
        "learning",
        "Andriy Burkov",
        "model evaluation",
        "user"
      ],
      "concepts": [
        "model",
        "tested",
        "examples",
        "learning",
        "applicative",
        "application",
        "deploy",
        "device",
        "data",
        "statistical"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "Segment 35 (pages 702-724)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 245-254)",
      "start_page": 245,
      "end_page": 254,
      "detection_method": "topic_boundary",
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n1\n\n2\n\n3\n\nalgorithms, according to how their running time or space requirements grow, as the input size grows.\n\nFor example, let’s say we have the problem of ﬁnding the two most distant one-dimensional examples in the set of examples S of size N. One Python algorithm we could craft would look like this: def find_max_distance(S):\n\nresult = None max_distance = 0 for x1 in S:\n\nfor x2 in S:\n\nif abs(x1 - x2) >= max_distance: max_distance = abs(x1 - x2) result = (x1, x2)\n\nreturn result\n\nor, like this in R: find_max_distance <- function(S) {\n\nresult <- NULL max_distance <- 0 for (x1 in S) {\n\nfor (x2 in S) {\n\nif (abs(x1 - x2) >= max_distance) { max_distance <- abs(x1 - x2) result <- c(x1, x2)\n\n}\n\n}\n\n} result\n\n}\n\nIn the above algorithms, we loop over all values in S, and, at every iteration of the ﬁrst loop, we loop over all values in S once again. Therefore, the above algorithm makes N2 comparisons of numbers. If we take the time the comparison, abs, and assignment operations take as a unit time, then the time complexity (or, simply, complexity) of this algorithm is at most 5N2. At each iteration, we have one comparison, two abs, and two assignment operations (1 + 2 + 2 = 5). When the complexity of an algorithm is measured in the worst case, the big O notation is used. For the above algorithm, using big O notation, we say that the algorithm’s complexity is O(N2); the constants, like 5, are ignored.\n\nFor the same problem, we can craft another Python algorithm like this: def find_max_distance(S):\n\nresult = None min_x = float(\"inf\")\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n1\n\n2\n\n1\n\n2\n\n3\n\nmax_x = float(\"-inf\") for x in S:\n\nif x < min_x: min_x = x if x > max_x: max_x = x\n\nresult = (max_x, min_x) return result\n\nor in R, like this: find_max_distance <- function(S):\n\nresult <- NULL min_x <- Inf max_x <- -Inf for (x in S) {\n\nif (x < min_x) { min_x <- x\n\n} if (x > max_x) { max_x = x\n\n}\n\nresult <- c(max_x, min_x) result\n\nIn the above algorithms, we loop over all values in S only once, so the algorithm’s complexity is O(N). In this case, we say that the latter algorithm is more eﬃcient than the former.\n\nAn algorithm is called eﬃcient when its complexity is polynomial in the input size. Therefore both O(N) and O(N2) are eﬃcient because N is a polynomial of degree 1, while N2 is a polynomial of degree 2. However, for very large inputs, an O(N2) algorithm can still be slow. In the big data era, scientists and engineers often look for O(logN) algorithms.\n\nFrom a practical standpoint, when implementing an algorithm, you should avoid using loops whenever possible, and implement vectorization using NumPy or similar tools. For example, you should use operations on matrices and vectors, instead of loops. In Python, to compute w · x (a dot product of two vectors), you should type,\n\nimport numpy wx = numpy.dot(w,x)\n\nand not,\n\nwx = 0 for i in range(N): wx += w[i]*x[i]\n\nSimilarly, in R, you should type,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\n1\n\n23\n\n24\n\n25\n\nwx = w %*% x\n\nand not,\n\nwx <- 0 for (i in seq(N)):\n\nwx <- wx + w[i]*x[i]\n\nUse appropriate data structures. If the order of elements in a collection doesn’t matter, use set instead of list. In Python, the operation of verifying whether a speciﬁc example belongs to S is fast when S is a set, and is slow when S is a list.\n\nAnother important data structure to make your Python code more eﬃcient is dict. It is called a dictionary or a hash table in other languages. It allows you to deﬁne a collection of key-value pairs with very fast lookups for keys.\n\nUsing libraries is generally more reliable — you should only write your own code when you are a researcher, or when it’s truly needed. Scientiﬁc Python packages like NumPy, SciPy, and scikit-learn were built by experienced scientists and engineers with eﬃciency in mind. They have many methods implemented compiled C and C++ for maximum performance.\n\nIf you need to iterate over a vast collection of elements, use Python generators (or their R alternative in the iterators package) that create a function returning one element at a time, rather than all elements at once.\n\nUse the cProﬁle package in Python (or its R counterpart, lineprof) to ﬁnd code ineﬃciencies.\n\nFinally, when nothing can be improved in your code from the algorithmic perspective, you can further boost the speed by using:\n\nthe multiprocessing package in Python, or its R counterpart parallel, to run com- putations in parallel; or use a distributed processing framework such as Apache Spark, and\n\nPyPy, Numba or similar tools to compile your Python code (or the compiler package for R) into fast, optimized machine code.\n\n8.6.2 Deployment of Deep Models\n\nSometimes, to achieve the required speed, it might be necessary to do the scoring on a graphics processing unit (GPU). The cost of a GPU instance in a cloud environment is typically much higher than the cost of a “normal” instance. So only the model could be deployed in an environment with one or several GPUs optimized for fast scoring. The remainder of the application could be deployed separately in a CPU environment. This approach allows reducing the cost, but, at the same time, it might add a communication overhead between two parts of the application.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n8.6.3 Caching\n\nCaching is a standard practice in software engineering. Memory cache is used to store the result of a function call, so the next time that function is called with the same values of parameters, the result is read from the cache.\n\nCaching helps speed up the application when it contains resource-consuming functions that take time to process, or are frequently called with the same parameter values. In machine learning, such resource-consuming functions are models, especially when they run on GPUs.\n\nThe simplest cache may be implemented in the application itself. For example, in Python, the lru_cache decorator can wrap a function with a memoizing callable that saves up to the maxsize most recent calls:\n\nfrom functools import lru_cache\n\n# Read the model from file model = pickle.load(open(\"model_file.pkl\", \"rb\"))\n\n@lru_cache(maxsize=500) def run_model(input_example):\n\nreturn model.predict(input_example)\n\n# Now you can call run_model # on new data The ﬁrst time the function run_model is called for some input, model.predict will be called. For the subsequent calls of run_model with the same value of the input, the output will be read from cache that memorizes the result of maxsize most recent calls of model.predict.\n\nIn R, a similar result can be obtained using the memo function: library(memo)\n\nmodel <- readRDS(\"./model_file.rds\")\n\nrun_model <- function(input_example) {\n\nresult <- predict(model, input_example) result\n\n}\n\n# Create a memoized version of run_model run_model_memo <- memo(run_model, cache = lru_cache(500))\n\n# Now you can use run_model_memo # instead of run_model on new data\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nAlthough using lru_cache and similar approaches is very convenient for an analyst, typically, in large scale production systems, engineers employ general purpose scalable and conﬁgurable cache solutions such as Redis or Memcached.\n\n8.6.4 Delivery Format for Model and Code\n\nRecall, serialization is the most straightforward way to deliver the model and the feature extractor code to the production environment.\n\nEvery modern programming language has serialization tools. In Python, it’s pickle:\n\nimport pickle from sklearn import svm, datasets\n\nclassifier = svm.SVC() X, y = datasets.load_iris(return_X_y=True) classifier.fit(X, y)\n\n# Save model to file with open(\"model.pickle\",\"wb\") as outfile: pickle.dump(classifier, outfile)\n\n# Read model from file classifier2 = None with open(\"model.pickle\",\"rb\") as infile: classifier2 = pickle.load(infile)\n\nif classifier2:\n\nprediction = classifier2.predict(X[0:1])\n\nwhile in R, it’s RDS: library(\"e1071\")\n\nclassifier <- svm(Species ~ ., data = iris, kernel = 'linear')\n\n# Save model to file saveRDS(classifier, \"./model.rds\")\n\n# Read model from file classifier2 <- readRDS(\"./model.rds\")\n\nprediction <- predict(classifier2, iris[1,])\n\nIn scikit-learn, it may be better to use joblib’s replacement of pickle, which is more eﬃcient on objects that carry large NumPy arrays:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nfrom joblib import dump, load\n\n# Save model to file dump(classifier, \"model.joblib\")\n\n# Read model from file classifier2 = load(\"model.joblib\")\n\nThe same approach can be applied to save the serialized object of the feature extractor to a ﬁle, copy it to the production environment, and then read it from the ﬁle.\n\nFor some applications, the prediction speed is critical. In such cases, the production code is written in a compiled language, such as Java or C/C++. If a data analyst has built a model using Python or R, there are three options to deploy for production:\n\nrewrite the code in a compiled, production-environment programming language, • use a model representation standard such as PMML or PFA, or • use a specialized execution engine such as MLeap.\n\nThe Predictive Model Markup Language (PMML) is an XML-based predictive model interchange format that provides a way for data analysts to save and share models between PMML-compliant applications. PMML allows analysts to develop models within one vendor’s application, and then use them within other vendors’ applications, so that proprietary issues and incompatibilities are no longer a barrier to the model exchanges between applications.\n\nFor example, imagine you use Python to build an SVM model, and then save the model as a PMML ﬁle. Let the production runtime environment be a Java Virtual Machine (JVM). As long as PMML is supported by a machine learning library for JVM, and that library has an implementation of SVM, your model can be used in production directly. You don’t need to rewrite your code or retrain the model in a JVM language.\n\nThe Portable Format for Analytics (PFA) is a more recent standard for representing both statistical models and data transformation engines. PFA allows us to easily share models and machine learning pipelines across heterogeneous systems and provides algorithmic ﬂexibility. Models, pre- and post-processing transformations are all functions that can be arbitrarily composed, chained, or built into complex workﬂows. PFA has a form of a JavaScript Object Notation (JSON) or a YAML Ain’t Markup Language (YAML) conﬁguration ﬁle.\n\nThere are open source generic “evaluators” for models or pipelines saved as PMML or PFA formatted ﬁles. JPMML (for Java PMML) and Hadrian are two of the most widely adopted. Evaluators read the model or the pipeline from a ﬁle, execute it by applying it to the input data, and output the prediction.\n\nUnfortunately, PMML and PFA are not widely supported by the popular machine learning libraries and frameworks.2 For example, scikit-learn doesn’t support those standards, though side-projects such as SkLearn2PMML can convert scikit-learn objects to PMML.\n\n2As of July 2020.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\nAlternatively, such execution engines as MLeap can execute machine learning models and pipelines fast in a JVM environment. At the time of the writing of this book, MLeap could execute models and pipelines created in Apache Spark and scikit-learn.\n\nNow, let us brieﬂy outline several useful and practical tips for model deployment.\n\n8.6.5 Start With a Simple Model\n\nDeploying and applying the model in production can be more complex than it might seem. Once the infrastructure to serve a simple model is solid, a more complex model can then be trained and deployed.\n\nA simple interpretable model is easier to debug, especially for feature extractors and entire machine learning pipelines. Complex models and pipelines have many dependencies and large numbers of hyperparameters to tune, and are more prone to implementation and deployment errors.\n\n8.6.6 Test on Outsiders\n\nBefore putting your model in production, test your model on outsiders, and not just on the test data. Outsiders could be other team members or company employees. Alternatively, you can use crowdsourcing or a subset of your real customers who agreed to participate in experiments with new product features.\n\nTesting on outsiders will help you avoid personal bias, because you, as the creator of the model, are emotionally involved. It will also give your model an exposure to diﬀerent users (in cases when, for example, your whole team is male or Caucasian).\n\n8.7 Summary\n\nA model can be deployed following several patterns: statically, as a part of installable software, dynamically on the user’s device, dynamically on a server, or via model streaming.\n\nThe static deployment has many advantages, such as fast execution time, preserved user privacy, and the ability to call the model when the user is oﬄine. There are also a drawback: it’s harder to upgrade the model without also having to upgrade the entire application.\n\nThe principal advantage of the dynamic deployment on the users’ devices is that the calls to the model will be fast for the user. It also reduces the charge on the organization’s servers. Downsides include the diﬃculty to deliver updates to all users and the availability of the model for third-party analyses.\n\nAs with the static deployment, deploying the model on a user’s device makes it diﬃcult to monitor the performance of the model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\nDynamic deployment on a server can have one of the following forms: deployment on a virtual machine, deployment in a container, and serverless deployment.\n\nThe most popular deployment pattern is to deploy the model on a server and make it available as a REST API in the form of a web or a gRPC service. Here, the client sends a request to the server, and then waits for a response before sending another request.\n\nModel streaming is diﬀerent. All models are registered within a stream-processing engine or are packaged as an application based on a stream-processing library. Here, the client sends one request and receives updates as they happen.\n\nTypical deployment strategies are single deployment, silent deployment, canary deployment, and multi-armed bandit.\n\nIn single deployment, you serialize the new model into a ﬁle, and then replace the old one.\n\nSilent deployment consists of deploying the old and new versions, and running them in parallel. The user will not be exposed to the new version until the switch is done. The predictions made by the new version are only logged and analyzed. Thus, there is enough time to make sure that the new model works as expected without aﬀecting any user. A drawback is the need to run more models, which consumes more resources.\n\nCanary deployment consists of pushing the new version to a small fraction of the users, while keeping the old version running for most users. Canary deployment allows model performance validation and evaluating the users’ experience. It won’t aﬀect lots of users in case of possible bugs.\n\nMulti-armed bandits allow us to deploy the new model while keeping the old one. The algo- rithm replaces the old model with the new one only when it is certain that it performs better.\n\nThe deployment of a new model version must be automated by a script in a transactional way. Given a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment. The model must be applied to the end-to-end and conﬁdence test data by simulating a regular call from the outside. If there’s a prediction error for the end-to-end test data, or the value of the metric is not within the range of acceptable values, the entire deployment has to be rolled back.\n\nThe versions of training data, feature extractor, and model must always be in sync.\n\nAlgorithmic eﬃciency is an important consideration in model deployment. Experienced scientists and engineers built Python packages like NumPy, SciPy, and scikit-learn with eﬃciency in mind. Your own code may not be as reliable or eﬃcient. You should only write your own code when it’s absolutely necessary.\n\nIf you implement your own algorithmic code, avoid loops. Implement vectorization with NumPy or similar tools. Use appropriate data structures. If the order of elements in a collection doesn’t matter, use a set instead of a list. Using dictionaries (or hash tables) allows you to deﬁne a collection of key-value pairs with very fast lookups for keys.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\nCaching speeds up the application when it contains resource-consuming functions frequently called with the same parameter values. In machine learning, such resource-consuming functions are models, especially when they run on GPUs.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "page_number": 245,
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 245-254). Key topics include deploying, model, and data. Each instance, denoted as an orange square, contains all the code needed to run the feature extractor and the model.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Andriy Burkov Machine",
        "model",
        "machine learning",
        "deployment",
        "machine learning model",
        "virtual machine",
        "Burkov Machine",
        "machine learning system",
        "REST API",
        "machine",
        "Learning Engineering",
        "Andriy Burkov",
        "REST API deployment"
      ],
      "concepts": [
        "deploying",
        "model",
        "data",
        "process",
        "processing",
        "version",
        "versions",
        "resource",
        "machine",
        "streaming"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 5,
          "title": "Segment 5 (pages 82-102)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 359-366)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 56,
          "title": "Segment 56 (pages 498-505)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 255-262)",
      "start_page": 255,
      "end_page": 262,
      "detection_method": "topic_boundary",
      "content": "9 Model Serving, Monitoring, and Maintenance\n\nIn this chapter, we consider the best practices of serving, monitoring, and maintaining models in production. These are the last three stages in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nIn particular, we characterize the properties of a machine learning runtime, the environment in which the input data is applied to the model, and the modes of model serving, such as batch and on demand. Furthermore, we consider three major challenges of serving a model in real world: errors, change, and human nature. We describe what should be monitored in the production environment, and when and how update the model.\n\n9.1 Properties of the Model Serving Runtime\n\nThe model serving runtime is the environment in which the model is applied to the input data. The runtime properties are dictated by the model deployment pattern. However, an eﬀective runtime will have several additional properties that we discuss here.\n\n9.1.1 Security and Correctness\n\nThe runtime is responsible for authenticating the user’s identity, and authorizing their requests.\n\nThings to check are:\n\nwhether a speciﬁc user has authorized access to the models they want to run,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nwhether the names and the values of parameters passed correspond to the model’s speciﬁcation, and\n\nwhether those parameters and their values are currently available to the user.\n\n9.1.2 Ease of Deployment\n\nThe runtime must allow the model to be updated with minimal eﬀort and, ideally, without aﬀecting the entire application. If the model was deployed as a web service on a physical server, then a model update must be as simple as replacing one model ﬁle with another, and restarting the web service.\n\nIf the model was deployed as a virtual machine instance or container, then the instances or containers running the old version of the model should be replaceable by gradually stopping the running instances and starting new instances from a new image. The same principle applies to the orchestrated containers.\n\nTypically, a model streaming-based application is updated by streaming the new version of the model. To enable this, the streaming application must be stateful. Once a new version and the related components (such as feature extractor and scoring code) are streamed into the application, the state of the application changes, and now contains the new version of these assets. Modern stream-processing engines support stateful applications. The described architecture is schematically shown in Figure 2.\n\nFigure 2: Model streaming high-level architecture.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4\n\n9.1.3 Guarantees of Model Validity\n\nAn eﬀective runtime will automatically make sure that the model it executes is valid. Furthermore, it makes sure the model, the feature extractor, and other components are in sync. It must be validated on each startup of the web service or the streaming application, and periodically during the runtime. As discussed in Section ?? of Chapter 8, each model should be deployed accompanied by the following four assets: an end-to-end set, a conﬁdence test set, a performance metric, and its range of acceptable values.\n\nThe model should not be served in production (and must be immediately stopped if it is running) in either of the two conditions:\n\nat least one of the end-to-end test examples was not scored correctly, or • the value of the metric, calculated on the conﬁdence test set examples, is not within the acceptable range.\n\n9.1.4 Ease of Recovery\n\nAn eﬀective runtime allows easy recovery from errors by rolling back to previous versions.\n\nThe recovery from an unsuccessful deployment should be produced in the same way, and with the same ease, as the deployment of an updated model. The only diﬀerence is that, instead of the new model, the previous working version will be deployed.\n\n9.1.5 Avoidance of Training/Serving Skew\n\nIt is strongly recommended to avoid using two diﬀerent codebases, one for training the model, and one for scoring in production. When it concerns feature extraction, even a tiny diﬀerence between two versions of feature extractor code may lead to suboptimal or incorrect model performance.\n\nThe engineering team may reimplement the feature extractor code for production for many reasons. The most common is that the data analyst’s code is ineﬃcient or incompatible with the production ecosystem.\n\nThus, the runtime should allow easy access to the feature extraction code for various needs, including model retraining, ad-hoc model calls, and production. One way to implement it is by wrapping the feature extraction object into a separate web service.\n\nIf you cannot avoid using two diﬀerent codebases to generate features for training and production, then the runtime should allow for the logging of feature values generated in the production environment. Those values should then be used as training values.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\n9.1.6 Avoidance of Hidden Feedback Loops\n\nIn Section ?? of Chapter 4, we saw one example of a hidden feedback loop. Model mB used the output of model mA as a feature, without knowing that model mA also used the output of model mB as its feature.\n\nAnother kind of hidden feedback loop only involves one model. Let’s say we have a model that classiﬁes incoming email messages as spam or not spam. Let the user interface allow the user to mark messages as spam or not spam. Obviously, we want to use those marked messages to improve our model. However, by so doing, we risk creating a hidden feedback loop, and here is why.\n\nIn our application, the user will only mark a message as spam when they see it. However, users only see the messages that our model classiﬁed as not spam. Also, it is unlikely that the user will regularly go to the spam folder and mark some messages as not spam. So, the action of the user is signiﬁcantly aﬀected by our model, which makes the data we get from the user skewed: we inﬂuence the phenomenon from which we learn.\n\nTo avoid the skew, mark a small percentage of examples as “held-out,” and show all of them to the user without pre-applying the model. Then use only these held-out examples as additional training examples, including those to which the user didn’t react.\n\nIn a more general scenario, one model can indirectly aﬀect the data used to train another model. Let one model decide the order of books to display, while the other decides which reviews to display near each book. If the ﬁrst model puts a review of a certain book at the bottom of the list, the absence of a user’s response to the second model’s review may be caused by its low position on the page, and not by the quality of the review.\n\n9.2 Modes of Model Serving\n\nMachine learning models are served in either batch or on-demand mode. On-demand, a model can be served to either a human client or a machine.\n\n9.2.1 Serving in Batch Mode\n\nA model is usually served in batch mode when it is applied to large quantities of input data. One example could be when the model is used to exhaustively process the data of all users of a product or service. Or, when it systematically applies to all incoming events, such as tweets, or comments to online publications. Batch mode is more resource-eﬃcient compared to an on-demand mode, and is employed when some latency can be tolerated.\n\nWhen served in batch mode, the model usually accepts between a hundred and a thousand feature vectors at once. Experiment to ﬁnd the optimal batch size for speed. Typical sizes are powers of two: 32, 64, 128, etc.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\nThe outputs for the batch are usually saved to the database, as opposed to sending them to speciﬁc consumers. You would use the batch mode to:\n\ngenerate the list of weekly recommendations of new songs to all users of a music streaming service,\n\nclassify the ﬂow of incoming comments to online news articles and blog posts as spam or not spam,\n\nextract named entities from documents indexed by a search engine, and so on.\n\n9.2.2 Serving on Demand to a Human\n\nThe six steps of serving the model on demand to a human are as follows:\n\n1) validate the request, 2) gather the context, 3) transform the context into model input, 4) apply the model to the input, and get the output, 5) make sure that the output makes sense, 6) present the output to the user.\n\nBefore running a model in production for a request coming from a user, it might be necessary to verify whether that user has the correct permissions for this model.\n\nThe context represents the user’s situation when they send a request to the machine learning system, and in which the user will receive the system’s response.\n\nThe user can send the request to the machine learning system explicitly or implicitly. An exam- ple of an explicit request is when a music-streaming service’s user requests recommendations for similar songs to a given song. On the other hand, an implicit request is sent by a direct messenger application for suggested replies to the most recent message received by the user.\n\nA good context may be collected in real or near-real time. It will contain the information needed by the feature extractor to generate all the feature values the model expects. It also contains enough information for debugging, is compact enough to be saved in a log, and contains information that will be used to improve the model over time.\n\nLet’s see examples of a good context for several problems.\n\nDevice malfunctioning\n\nWhen detecting device malfunction, a good context contains vibration and noise levels, the task executed by the device, the user ID, the ﬁrmware version, the time passed since manufacturing and the last maintenance, and the number of uses since manufacturing and the last maintenance.\n\nEmergency room hospitalization\n\nTo decide whether the new patient should be admitted to an intensive care unit, a good context would include age, blood pressure, temperature, heart rate, pulse oximetry\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\nlevel, complete blood count, chemistry proﬁle, arterial blood gas test, blood alcohol level, medical history, and pregnancy.\n\nCredit risk assessment\n\nTo make an approval/rejection decision for a credit card application, a good context would include age, education, employment status, country residency status, annual salary, family status, outstanding debts, availability of other credit cards, whether the person is a homeowner or tenant, whether the person has declared bankruptcy, and whether, and how many times, the person missed past credit payments. Even if certain information is not needed for feature extraction, it is still pertinent for logging and debugging: client’s ID, date, and time of the day.\n\nAdvertisement display\n\nTo decide whether a speciﬁc advertisement should be displayed to a website user, a good context would include the webpage title, the user’s position on the web page, the screen resolution, the text on the webpage and the text visible to the user, how the user reached the webpage, and the time spent on it. For logging and debugging purposes, the context might include the browser version, operating system version, connection information, and date and time.\n\nA feature extractor transforms the context into the model input. Sometimes, the feature extractor is a part of the machine learning pipeline, as we discussed in Section ?? of Chapter 5. However, it’s common to build the feature extractor as a separate object.\n\nWhen the result of the scoring is to be served to a human client, it’s rarely presented directly. Usually, the scoring code transforms the model’s prediction into a form more easily interpreted, and that adds value to the client.\n\nBefore serving the model to a human, it’s common to measure the prediction conﬁdence score. If the conﬁdence is low, you can decide to not present anything: users tend to complain less about the errors they don’t see. Or, if the user expects an output, inform them about the low conﬁdence. Then prompt, “Are you sure?”\n\nPrompting is especially important when the system might initiate an action based on the prediction. If you are able to estimate the error’s possible cost and if the prediction conﬁdence is bounded by (0,1), then multiply (1 — conﬁdence) by the cost to see the possible impact of making a wrong action. For example, let the cost of making an error is estimated as 1000 dollars and the model outputs the conﬁdence score equal to 0.95, then the expected error cost value is (1 − 0.95) × 1000 = 50 dollars. You might put a threshold on the expected cost value for diﬀerent actions recommended by the model, and prompt the user if the expected cost is above the threshold.\n\nIn addition to measuring the model’s conﬁdence, calculate whether the value of the prediction makes sense. In Section 9.3, we will further detail what to check, and what the system’s reaction should be, if the output doesn’t make sense.\n\nIt is convenient to log the context in which the model was served, as well as the reaction of the user. This can help both to debug eventual problems, and improve the model by creating\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8\n\nFigure 3: On-demand model serving with a message broker.\n\nnew training examples.\n\n9.2.3 Serving on Demand to a Machine\n\nWhile building a REST API is appropriate for many cases, we often serve a machine by streaming. Indeed, a machine’s data requirements are usually standard and pre- determined. A well-designed, ﬁxed topology of a streaming application allows an eﬃcient use of available resources.\n\nServing on demand, to either machine or human, can be tricky. The demand may vary, from very high during the day, to very low during the night. If you use virtual resources in the cloud, autoscaling can help with adding more resources when needed, and then freeing them when demand decreases. However, autoscaling is not nimble enough to cope with accidental spikes.\n\nTo deal with such situations, on-demand architectures include a message broker, such as RabbitMQ or Apache Kafka. A message broker allows one process to write messages in a queue, and another to read from that queue. On-demand requests are placed in the input queue. The model runtime process periodically connects to the broker. It reads a batch of input data elements from the input queue and generates predictions for each element in batch mode. It then writes the predictions to the output queue. Another process periodically connects to the broker, reads the predictions from the output queue, and pushes them to users who sent the requests (Figure 3). In addition to allowing us to cope with demand spikes, such an approach is more resource-eﬃcient.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9\n\n9.3 Model Serving in Real World\n\nWhen real people interact with a software system in the real world, serving the model gets complicated. It’s usually impossible to predict all user actions and reactions. The architecture of a software system intended for the real world must be ready for three phenomena: errors, change, and human nature.\n\n9.3.1 Being Ready for Errors\n\nErrors are inevitable in any software. In machine-learning-based software, errors are an integral part of the solution: no model is perfect. Because we cannot ﬁx all errors, the only option is to embrace them.\n\nEmbracing errors means designing the software system in such a way that when an error happens, the system continues operating normally.\n\nThere are three “cannots” we must accept and embrace:\n\n1. We cannot always explain why an error happened. 2. We cannot reliably predict when it will happen, and even a high conﬁdence prediction can be false.\n\n3. We cannot always know how to ﬁx a speciﬁc error. If it’s ﬁxable, what kind and how much training data is needed?\n\nFurthermore, when an error happens, we cannot always expect that the incorrect prediction will at least be close or similar to the correct prediction. An error can be arbitrarily “crazy.” For example, a model for a self-driving car, at the speed of 120 km/h (~74 mph) with no obstacles, may predict that the best action is to stop and drive backward.\n\nTiny changes in the context may result in unexpected error patterns. For example, the model that recognizes dangerous situations on the factory ﬂoor may start making errors after the lightbulb near the camera is replaced. The previous lightbulb was incandescent, and the new one is ﬂuorescent.\n\nEven rare errors may impact users, if the number of users is large. Let the model have a 99% accuracy. If you have a million users, one percent of prediction errors will aﬀect thousands.\n\nIt’s rare that ﬁxing one error in a model results in new errors. However, there’s no guarantee.\n\nHow to design a system in the presence of inevitable errors?\n\n9.3.2 Dealing With Errors\n\nFirst of all, have a strategy that mitigates, at least, partially, a situation in which your system looks or acts “stupid.” For example, if your system talks to the user, like a personal assistant or a chatbot, it’s better to say, “I don’t know,” than to say something random. If the error\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "page_number": 255,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 255-262). Key topics include models, deployment, and data. In the big data era, scientists and engineers often look for O(logN) algorithms.",
      "keywords": [
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "model",
        "Andriy Burkov Machine",
        "Machine Learning",
        "Burkov Machine",
        "Learning Engineering",
        "Python",
        "Andriy Burkov",
        "machine learning models",
        "Machine",
        "max",
        "Deployment",
        "Learning",
        "result"
      ],
      "concepts": [
        "models",
        "deployment",
        "data",
        "algorithms",
        "algorithmic",
        "classifier",
        "product",
        "production",
        "useful",
        "engineering"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 263-271)",
      "start_page": 263,
      "end_page": 271,
      "detection_method": "topic_boundary",
      "content": "will be directly visible to the user, calculate the expected cost of the error, as discussed above, and do not display the prediction to the user if the cost is above a threshold.\n\nAlternatively, train a second model mB that predicts, for an input, that the ﬁrst model mA is likely to make an error on that input. The presence of a “safeguard” model mB is especially relevant if model mA is used in a mission critical system.\n\nThe error’s visibility is an important factor in deciding whether and how to hide it. For example, consider a system that downloads web pages from the internet and extracts some entities from them. Let the user be interested in being alerted when a kind of entity is detected. The model can make two kinds of errors: 1) extract an entity even if the document doesn’t contain it (false positive, FP), and 2) not extract an entity that is present in the document (false negative, FN). When the former error happens, the user receives an irrelevant alert and gets frustrated. If the latter, the user doesn’t receive any alert, remains unaware of the error, and avoids frustration. In this situation, you might prefer to optimize the model for precision, by keeping recall reasonably high.\n\nWhen you train a model, decide which kind of errors you would most like to avoid, and then optimize your hyperparameters, including the prediction threshold, accordingly.\n\nWhen your conﬁdence for the best prediction is low, consider presenting several options. This is why Google presents 10 search results at once. There are much higher chances for the most relevant link to be among those 10 search results, than for it to be in the ﬁrst position.\n\nAnother way to avoid user’s frustration with model errors is to dose the user’s exposure to the model. Measure the number of errors your model makes, and estimate how many errors per minute (day, week, or month) a user is ready to tolerate. Then limit the interactions the user will have with the model to keep the number of perceived errors below that level.\n\nFor situations when an error happened and might have been perceived, add a possibility for the user to report the error. Once the report is received, log the context in which the model was used, as well as the prediction of the model. Explain to the user what actions will be taken to prevent a similar error from happening in the future.\n\nIt’s appropriate to measure the user’s engagement with the system, log all interactions, and then analyze suspicious interactions oﬄine. This includes:\n\nwhether the user interacts with the system less than before, • whether the user ignored certain recommendations, and • whether the user spent adequate time in various settings.\n\nTo reduce an error’s negative impact even further, if the system allows it, give the user an option to undo an action recommended by the system. Extend this, if possible, to any automated action executed by the system on the user’s behalf.\n\nSoftware applications that act on their user’s behalf must be especially limited in their possible actions. Recall that machine-learning models’ errors can be arbitrarily “crazy” like in the example of a self-driving car that can suddenly decide to drive backward. Caution must be exercised in other critical scenarios involving health, safety, or money, such as bidding in\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11\n\nauctions or prescribing medication. If the model predicts to buy or sell more stocks than the moving average plus one standard deviations, it’s a good idea to send an alert and put an otherwise “automatic” action on hold. The same logic should apply if the model predicts to serve an unreasonably high dose of a drug to a patient, or change the speed of the car to a value substantially above or below usual.\n\nIf your system can automatically reject the model prediction, it’s best to implement some fallback strategy, in addition to informing the user about a failure (Figure 4). A less sophisticated model or a handcrafted heuristic may be used as a fallback. Of course, the output of the fallback strategy should also be validated, and also rejected if it seems unreasonable. In this case, an error message should be sent to the user.\n\nFigure 4: Real-world model serving ﬂowchart.\n\n9.3.3 Being Ready for, and Dealing With, Change\n\nThe performance of a system based on machine learning usually changes over time. In some applications, it can change in near-real-time.\n\nThere are two types of model change:\n\n1. Its quality could become better or worse.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12\n\n2. The predictions for some inputs can become diﬀerent.\n\nA typical reason for the model performance degradation over time is concept drift that we already considered in Section ?? of Chapter 3. The notion of what is a correct prediction may change because of the users’ preferences and interests. This would require retraining the model, using more recently labeled data.\n\nSome change can be perceived by the user as positive. Sometimes, the change can be negatively perceived, even if the system’s performance improved, from the engineering point of view. You might have added training examples, retrained the model and observed a better performance metric value. However, by adding new data, you involuntarily induced a data imbalance. Some classes are now underrepresented. Users interested in those classes’ predictions see decreased performance, and complain or even abandon your system.\n\nUsers become accustomed to certain behaviors. They might know what query to submit to the search engine to get an often-used document or a web application. That query was not necessarily the most optimal for the purpose, but it worked. Suppose you improved the relevancy of your search-result ranking algorithm. Now that query doesn’t return that speciﬁc document or application, or it puts it on the second page of the search results. The user can no longer ﬁnd the resource they once found easily, and get frustrated.\n\nIf you expect that the user might negatively perceive the change, give them time to adapt. Educate the user about the changes and what to expect from the new model. Or, it can be done by gradually introducing the changes. You might mix the predictions of the old model and the new model, and slowly decrease the proportion for the old model. Alternatively, you can run both the new and the old model in parallel, and let the user switch to the old model for some time before sunsetting it.\n\n9.3.4 Being Ready for, and Dealing With, Human Nature\n\nHuman nature is what makes eﬀective system engineering such a hard endeavor. Humans are unpredictable, often irrational, inconsistent, and have unclear expectations. A solid software system must anticipate that.\n\nAvoid Confusion\n\nThe system must be designed in such a way that the user doesn’t feel confused interacting with it. A model’s output must be served in an intuitive way, without assuming that the user knows anything about machine learning and AI. In fact, many users will assume that they work with typical software and will be surprised to see errors.\n\nManage Expectations\n\nOn the other hand, some users will have too high expectations. The main reason for that is advertisement. To attract attention, a product or a system based on machine learning is often displayed in advertisements as being “intelligent.” For example, personal assistants such as Apple Siri, Google Home, and Amazon Alexa are often shown in advertisements as having\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13\n\nhuman intelligence. Indeed, any machine-learning-based system might look very intelligent when inputs are carefully selected. Users can look at such advertisements and extrapolate what they see to situations in which the system isn’t designed to operate eﬀectively.\n\nAnother common reason users expect something spectacular (even without being promised) is that they worked with a similar (in their understanding) system that looked “very intelligent” to them. Such users would expect the same level of “intelligence” from your system.\n\nGain Trust\n\nSome users, especially experienced ones, will mistrust any system if they know it contains some “intelligence.” The main reason for that mistrust is past experience. Most so-called intelligent systems fail to deliver, and, because of that, some users expect failure when they ﬁrst encounter your system.\n\nAs a consequence, your system must gain each user’s conﬁdence, and this must be done early.\n\nA user experienced with “intelligent” systems will most likely make several simple tests of your system’s abilities. If your system fails, the user will not trust it. For example, if your system is a search engine, then a user would query their name or a document they authored to test your system. Or, if your system provides intelligence on organizations to corporate customers, a user will check how much your system knows about their organization, and whether the intelligence makes sense. A driver of a self-driving car will most likely test commands like “start the engine,” “follow that car,” “keep the current speed,” or “park on that street.” Depending on the nature of the service, you should anticipate such simple tests and make sure that your system passes them.\n\nManage User Fatigue\n\nUser fatigue can be another reason why you see decreasing interest in your system. Make sure that the system doesn’t excessively interrupt user experience with recommendations or requests for approval. Avoid showing everything you have to show in one shot. Whenever possible, let the user explicitly express their interest.\n\nFurthermore, not all actions that the system can handle automatically have to be handled this way. For example, if the system automates user’s interactions with other people, it might send private or restricted data as an email reply, or post it to an open forum. Before sharing on a user’s behalf, it makes sense to evaluate the information’s sensitivity. Use a model trained to detect such potentially sensitive texts and images. On the other extreme, a system can be too conservative and automatically ﬁlter out relevant information or ask the user to conﬁrm too many decisions which might result in user fatigue.\n\nBeware of the Creep Factor\n\nWhen users interact with a learning system, there’s a phenomenon known as creep factor. It means that the user perceives the model’s predictive capacity as too high. The user feels uncomfortable, especially when a prediction concerns their very private details. Make sure that the system doesn’t feel like “Big Brother” and doesn’t take too much responsibility.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14\n\n9.4 Model Monitoring\n\nA deployed model must be constantly monitored. Monitoring helps make sure that,\n\nthe model is served correctly, and • the performance of the model remains within acceptable limits.\n\n9.4.1 What Can Go Wrong?\n\nMonitoring should be designed to provide early warnings about issues with the model in production. More speciﬁcally, this includes:\n\nnew training data used to update the model made it perform worse; • the live data in production changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is being abused or under an adversarial attack.\n\nAdditional training data is not always good. A labeler may have incorrectly interpreted the labeling instructions. Or, one labeler’s decisions might be in contradiction with another labeler. Data automatically gathered to improve the model may be biased. Reasons for that could be, for example, a hidden feedback loop considered in Section 9.1.6 or a systematic value distortion discussed in Section ?? of Chapter 3.\n\nSometimes, the properties of the data in production gradually change, but the model doesn’t adapt. It remains based on older data, which is no longer representative. One reason for this is concept drift that we discussed in Section 9.3.\n\nA software engineer could ﬁx a bug in the feature extraction code, and update the feature extractor in production. But if the engineer fails to also update the production model, the performance may change in an unpredictable manner.\n\nEven if the feature extraction and the model are in sync, a disappearance or a change of some resource (database connection, database table, or external API) may aﬀect some of the features generated by that feature extractor.\n\nSome models, especially those deployed in e-commerce and media platforms, often become targets of adversarial attacks. Bad actors, such as unfair competitors, fraudsters, criminals, and foreign governments, may actively seek out weaknesses in a model and adjust their attacks accordingly. If your machine learning system learns from the user’s actions, then some may act to change the model behavior in their favor.\n\nFurthermore, attackers may want to examine the trained model in order to obtain information about the model’s training data. That training data might contain conﬁdential information about people and organizations.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15\n\nAnother form of abuse, which may be the hardest to prevent, is model dual use. As any software, a machine learning model can be used for good (as you intended) or for bad (often without your consent). For example, you might create and publicly release a model that makes one’s voice sound like a cartoon character. Fraudsters may adapt your result to fake the voice of a bank client, and execute a phone transaction on their behalf. Alternatively, you might create a model that recognizes pedestrians on the street. An automatic weapon manufacturer could use your model to detect people on the battleﬁeld.\n\n9.4.2 What and How to Monitor\n\nMonitoring must allow us to make sure that the model generates reasonable performance metrics when applied to the conﬁdence test set. This set should be regularly updated with new data to avoid possible distribution shift. Additionally, the model must be regularly tested on the examples from the end-to-end set.\n\nWhile it’s obvious that accuracy, precision, and recall are good candidates for monitoring, one metric is especially useful for measuring the change over time: prediction bias.\n\nIn a static world where nothing changes, the distribution of predicted classes would roughly equal the distribution of observed classes. This is especially true when the model is well- calibrated. If you observe otherwise, the model is exhibiting prediction bias. The latter might mean that the distribution of the training data labels and the production’s current class distribution are now diﬀerent. You must investigate the reasons for this change and make the necessary adjustments.\n\nMonitoring allows us to stay alert of abandoned or repurposed data sources. Some database columns might stop being populated. The deﬁnition or format of the data in some columns might change, while the unadapted models still assume the previous deﬁnitions and formats. To avoid that, the distribution of the values of every feature extracted from a database table must be monitored for a signiﬁcant shift. A shift of the distribution of both feature values and predictions can be detected by applying statistical tests such as the Chi-square independence test and Kolmogorov–Smirnov test. If a signiﬁcant distribution shift is detected, an alert must be sent to the stakeholders.\n\nThe numerical stability of the model should also be monitored. An alert should be triggered if NaNs (not-a-numbers) or an inﬁnity is observed.\n\nIt’s important to monitor computational performance of a machine learning system. Both dramatic and slow-leak regression should be detected, and warnings must be sent.\n\nMonitor and send alerts when the usage ﬂuctuations look suspicious. In particular:\n\nmonitor the number of model servings during an hour, and compare it to the corre- sponding value calculated one day earlier. Send a warning alert to the stakeholders if the number has changed by 30% or more. This threshold must be tuned for your use case to avoid generating excessive warnings;\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16\n\nmonitor the daily number of model servings and compare it to the corresponding value calculated one week earlier. Send a warning alert to the stakeholders if the number has changed by 15% or more. Tune the value for your use case.\n\nMonitoring these numbers helps detect undesirable change:\n\nminimal and maximal prediction values, • median, mean, and standard deviation prediction values over a given timeframe, • latency when calling the model API, and • memory consumption and CPU usage when performing predictions.\n\nAdditionally, to prevent distribution shift, the monitoring automation must:\n\n1) accumulate inputs by randomly putting some aside during a certain time period, 2) send those inputs for labeling, 3) run the model, and calculate the value of the performance metric, 4) alert the stakeholders if there is signiﬁcant performance degradation.\n\nRecommender systems need additional monitoring. These models oﬀer recommendations to website or application users. It can be useful to monitor click-though rate (CTR), that is, ratio of users who clicked on a recommendation to the number of total users who received recommendations from that model. If CTR is decreasing, the model must be updated.\n\nIt’s important to note that there is a tricky tradeoﬀ between being too conservative versus frequently alerting stakeholders about small changes in the metrics. If you alert too often, people might become tired of receiving alerts and eventually will start ignoring them. In non-mission-critical cases, it can be appropriate to allow the stakeholders to deﬁne their own thresholds that trigger alerts.\n\nLog monitoring events so the entire process is traceable. For visual model performance analysis, the monitoring tool’s user interface should provide trend charts showing how the model degradation evolves over time.\n\nOne of the monitoring tool’s properties should be the ability to compute and visualize metrics on slices of data. A slice is a subset of the data that includes only such examples in which a speciﬁc attribute has a certain value. For example, one slice could contain only the examples where the state attribute is Florida; another slice might contain only the data for women, and so on. The degradation of the model might only be observed in some slices, yet remain insigniﬁcant in others.\n\nBesides real-time monitoring, it’s important to also log data that:\n\nmight help ﬁnd a problem’s source, • is impossible to analyze in real-time, or • is helpful for improving existing models or training new ones.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17\n\n9.4.3 What to Log\n\nIt is important to log enough information to reproduce any erratic system behavior during a future analysis. If the model is served to a front-end user, such as a website visitor or a mobile application user, it’s worth saving the user’s context at the moment of the model serving. As discussed in Section 9.2, the context might include: the content of the webpage (or the state of the application), the user’s position on the web page, time of the day, where the user came from, and what they clicked before the model prediction was served.\n\nAdditionally, it is useful to include the model input, that is, the features extracted from the context, and the time it took to generate those features.\n\nThe log could also include:\n\nthe model’s output, and time it took to generate it, • the new context of the user, once they observed the model’s output, • the user’s reaction to the output.\n\nThe user’s reaction is the immediate action that followed the observation of the model output: what was clicked, and how much time after the output was served.\n\nIn large systems with thousands of users, where the model is served to each user hundreds of times a day, it can be prohibitive to log every event. It would be more practical to do stratiﬁed sampling. You ﬁrst decide which groups of events you want to log, and then you log only a certain percentage of events in each group. The groups can be groups of users or groups of contexts. Users can be grouped by age, gender, or seniority with the service (new clients vs. long-time clients). The groups of contexts could be early-morning, business-day, and late-night interactions.\n\nWhen you store users’ activity data in logs, the users should know what, when, and how it is stored, and for how long. If possible, data should be anonymized or aggregated without loss of utility. Access to sensitive data must be restricted only to those assigned to solve a speciﬁc problem during a speciﬁc time period. Avoid letting any analyst access sensitive data to solve unrelated business problems. It could lead to legal problems.\n\nMake sure users may opt-out from logging and analysis of their activity data. Diﬀerent data retention policies will apply to diﬀerent countries. Each country imposes its own restrictions on what can and cannot be stored about their citizens, or used for analysis.\n\n9.4.4 Monitor for Abuse\n\nSome people or organizations may use your model for their own business. Such users might send millions of daily requests, while a typical user would only send a dozen. Alternatively, some users might want to reverse-engineer the training data, or learn how to make the model produce a desired output.\n\nWays to prevent such abuse include,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18\n\nmaking users pay per request, • creating progressively longer pauses before responding to requests, or even • blocking some users.\n\nTo reach their own business goals, some attackers might try to manipulate your model. An attacker might submit data that changes the model in a way that only beneﬁts the attacker. As a result, the overall quality of the model might degrade.\n\nWays to prevent such abuse include,\n\nnot trusting the data from a user unless similar data comes from multiple users, • assigning a reputation score to each user, and not trusting the data obtained from users with low reputations, and\n\nclassifying user behavior as either normal or abnormal, and not accepting the data coming from users demonstrating abnormal behavior.\n\nThe attackers will try to bypass your defence by adapting their behavior. To eﬀectively defend your system, update your models regularly. Add both new data and new features that detect fraudulent transactions.\n\n9.5 Model Maintenance\n\nMost production models must be regularly updated. The rate depends on several factors:\n\nhow often it makes errors and how critical they are, • how “fresh” the model should be, so as to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to deploy the model, and • how much a model update contributes to the product and the achievement of user goals.\n\nIn this section, we talk about model maintenance: when and how to update the model after it’s deployed in production.\n\n9.5.1 When to Update\n\nWhen a model is deployed in production for the ﬁrst time, it’s often far from perfect. Inevitably, the model makes prediction errors. Some of them could be critical, so the model needs an update. Over time, a model could become more solid, and require fewer updates. However, some models should be constantly updated, so to speak, always be “fresh.”\n\nModel freshness depends on the business needs and the needs of the user. The recommender model on an e-commerce website must be updated after each purchase. If the user utilizes a model to get recommended content on a news website, the model might need to be updated weekly. On the other hand, a voice recognition/synthesis or a machine translation model could be updated less frequently.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "page_number": 263,
      "chapter_number": 31,
      "summary": "This chapter covers segment 31 (pages 263-271). Key topics include models, user, and version. In machine learning, such resource-consuming functions are models, especially when they run on GPUs.",
      "keywords": [
        "Model",
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "machine learning",
        "Andriy Burkov Machine",
        "Model Serving Runtime",
        "Model Serving",
        "Machine learning models",
        "Burkov Machine",
        "user",
        "Model Serving Machine",
        "Learning Engineering",
        "machine",
        "learning",
        "Serving Machine learning"
      ],
      "concepts": [
        "models",
        "user",
        "version",
        "versions",
        "requests",
        "request",
        "streaming",
        "blood",
        "values",
        "contains"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 30,
          "title": "Segment 30 (pages 257-264)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 272-280)",
      "start_page": 272,
      "end_page": 280,
      "detection_method": "topic_boundary",
      "content": "The speed of availability of new training data also aﬀects the rate of model updates. Even if new data comes in fast, such as the stream of comments on a popular website, it may take time and require signiﬁcant investment to get labeled data. Sometimes, labeling is automated but delayed, as in churn prediction, where the user’s decision to stay with or leave the service happens far in the future.\n\nSome models take signiﬁcant time to build, especially if the hyperparameter search is needed. It’s not uncommon to wait for days or even weeks to get a new version of the model. Use parallelizable machine learning algorithms and graphical processing units (GPU) to speed up the training. Modern libraries, such as thundersvm and cuML, allow the analyst to run shallow learning algorithms on GPUs, with a signiﬁcant gain in training time. If you cannot aﬀord to wait for days or weeks to get an updated model, using a less complex (and, therefore, less accurate) model might be your only choice.\n\nYou might decide to update the model less often if an update is costly. For example, in healthcare, getting labeled examples is complicated and expensive, due to regulations, privacy concerns, and expensive medical experts.\n\nNot all models are worth deploying. Sometimes the potential performance gain is not worth the user’s possible frustration. However, if the user disturbance is manageable, and the deployment is not costly, even a small improvement may result in a signiﬁcant business outcome in the long run.\n\n9.5.2 How to Update\n\nAs discussed, your software ideally allows the new model version to be deployed with- out stopping the entire system. In virtual or containerized infrastructure, this can be done by replacing the image of a virtual machine (VM) or a container in the repository, gradually closing VMs/containers, and letting the autoscaler instantiate a VM/container from an updated image.\n\nAn architecture of machine learning deployment and maintenance automation is schematically shown in Figure 5. Here, we have three repositories: data, code, and model; all three repositories are versioned. We also have two runtimes: model training and production. The model runs in the production runtime, which is load-balanced and auto-scaled. When an update of the model is needed, the model training runtime pulls the training data, as well as the model training code, from the data and code repositories, respectively. It then trains the new model and saves it in the model repository.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20\n\nFigure 5: A machine learning deployment and maintenance automation architecture.\n\nFigure 6: On-demand model serving and update with a message broker.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21\n\nOnce a new version of the model is placed in the repository, the production runtime pulls,\n\nthe new model, from the model repository; • the test data, from the data repository; and, • the code that applies the model to the test data, from the code repository.\n\nIf the new model passes the test, the old model is withdrawn from production. It is replaced with the new one with the appropriate deployment strategy, as discussed in Section ??. A/B testing or a multi-armed bandit algorithm can help make the replacement decision.\n\nThe distribution shift control database accumulates the inputs received by the model, as well as their scoring results. Once a suﬃcient number of examples is accumulated, that data is sent for validation to a human1 with the goal of detecting distribution shift.\n\nIn the model streaming scenario, the model update happens when the stream processor’s state is updated (see Section 9.1.2 and Figure 2).\n\nModel update in on-demand model serving with a message broker architecture is similar to that of model streaming (see Section 9.2.3 and Figure 3).\n\nFigure 6 illustrates a message-broker-based architecture that allows not just serving the model and updating it, but also contains a human labeler in the loop. The labeler receives unlabeled examples, samples some of them, assigns labels to sampled examples, and sends the annotated examples back to the message broker. The model training module reads the labeled examples from a queue. When their quantity is suﬃcient to signiﬁcantly update the model, it trains a new model, saves it in the model repository, and sends the “model ready” message to the broker. A model-serving process pulls the new model version from the repository, and discards the current model.\n\nLet’s outline a few additional considerations for successful model maintenance.\n\nMany companies use a continuous integration workﬂow in which the models are trained automatically as soon as new training data becomes available. It is recommended to retrain the model from scratch, by using the entire training data, instead of ﬁne-tuning an existing model on the new examples only.\n\nFor each training example, it’s recommended to store the labeler’s identity. Furthermore, attach the model version used to generate a speciﬁc value in the production database, to that value. Should a problem with the version model be discovered, knowing which database values it generated will allow reprocessing of those speciﬁc values only.\n\nIf a model is frequently re-trained, it is convenient to store pipeline’s hyperparameters in a conﬁguration system. Google recommends2 the following for a good conﬁguration system:\n\n1. It should be easy to specify a conﬁguration as a change from a previous conﬁguration. 2. It should be hard to make manual errors, omissions, or oversights.\n\n1Or to an automated tool, more accurate than the model, that cannot be deployed in production (e.g., too\n\nfragile, costly, or slow).\n\n2“Hidden Technical Debt in Machine Learning Systems” by Sculley et al. (2015).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22\n\n3. It should be easy to see, visually, the diﬀerence in conﬁguration between two models. 4. It should be easy to automatically assert and verify basic facts about a conﬁguration: number of features used, data dependencies, etc.\n\n5. It should be possible to detect unused or redundant settings. 6. Conﬁgurations should undergo a full code review and be checked into a repository.\n\nMake sure that the runtime environment has enough hard drive space and RAM for the updated model. Do not expect that the old version of the model and the new one will only diﬀer in performance. Be ready for the situation where the new model is much larger than the previous one. Similarly, do not expect that the new model will run as fast as the previous one. Ineﬃciency in the feature extraction code, an additional stage in the pipeline, or a diﬀerent choice of the algorithm may signiﬁcantly aﬀect the prediction speed.\n\nModels will inevitably make prediction errors. However, to the business or client, some errors are more costly than others. Once a new model version is deployed, validate it doesn’t make signiﬁcantly more costly errors than the previous model.\n\nCheck that the errors are distributed uniformly across the user categories. It’s undesirable if the new model negatively aﬀects more users from a minority or speciﬁc location.\n\nIf any of the above validations fail, it is not recommended to deploy the new model. Roll it back if the failure is detected after deployment and initiate an investigation. As discussed in Section 9.1, rolling back to the previous model must be as easy as deploying the new model.\n\nBeware of model cascading. As discussed in Section ?? of Chapter 6, if the one model’s outputs become inputs for another model, changing one model will aﬀect the performance the other. If your system is using model cascading, be sure to update all models in the cascade.\n\n9.6 Summary\n\nAn eﬀective runtime has the following properties. It is secure and correct, ensures ease of deployment and recovery, and provides guarantees of model validity. Furthermore, it avoids training/serving skew and hidden feedback loops.\n\nMachine learning models are served in either batch or on-demand mode. In on-demand mode, a model can be served to either a human client or a machine. A model is usually served in batch mode when it will be applied to big data and some latency is tolerable.\n\nWhen served on-demand to a human, a model is usually wrapped into a REST API. A machine’s data requirements are usually standard and pre-determined, so we often serve it by streaming.\n\nThe architecture of a software system intended for the real world must be ready for three phenomena: errors, change, and human nature.\n\nThe model deployed in production must be constantly monitored. The goals of monitoring are to make sure that the model is served correctly, and that the performance of the model\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23\n\nremains within acceptable limits.\n\nA variety of things might go wrong with the model in production, in particular:\n\nadditional training data made the model perform worse; • the properties of the production data changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is abused or is under an adversarial attack.\n\nAn automation must calculate values of the performance metrics critical for the business, and send alerts to the appropriate stakeholders if the values of those metrics change signiﬁcantly or fall below a threshold. In addition, the monitoring must reveal the distribution shift, numerical instability, and a decreasing computational performance.\n\nIt is important to log enough information to reproduce any erratic system behavior during an analysis in the future. If the model is served to a front-end user, it’s important to log the user’s context at the moment of the model serving. Additionally, it is useful to include the model input, that is, the features extracted from the context, and the time it took to generate those features. The log could also include the outputs obtained from the model, and time it took to generate it, the new context of the user once they observed the output of the model, and the reaction of the user to the output.\n\nSome users can utilize your model as a basis for their own business. They might reverse- engineer the training data, or learn how to “trick” your model. To prevent abuse:\n\ndon’t trust the data from a user unless similar data comes from multiple users, • assign a reputation score to each user and don’t trust the data obtained from users with low reputations,\n\nclassify user behavior as normal or abnormal, • make users pay per request, • make progressively longer pauses, and • block some users.\n\nMost machine learning models must be regularly or occasionally updated. The rate of updates depends on several factors:\n\nhow often it makes errors and how critical they are, • how “fresh” the model should be to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to train and deploy the model, and • how much a model update contributes to the achievement of user goals.\n\nAfter a model update, a good practice is to run the model against the examples in the end-to-end and conﬁdence test sets. It’s important to make sure that the outputs are either the same as before, or that the changes are as expected. It’s also important to validate that the new model doesn’t make signiﬁcantly more costly errors than the previous model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24\n\nCheck also that the errors are distributed uniformly across the user categories. It’s undesirable if the new model negatively aﬀects users from a minority or speciﬁc location.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25\n\n“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft\n\n10 Conclusion\n\nIn 2020, machine learning has become a mature and popular tool for solving business problems. What previously was available only to a handful of organizations, and considered “magic” by others, can today be created and used by a typical organization.\n\nThanks to open-source code, crowdsourcing, easily accessible books, online courses, and publicly available datasets, many scientists, engineers, and even home enthusiasts may now train machine learning models. If you are lucky, your problem can be solved by writing several lines of code, as demonstrated in many online tutorials.\n\nHowever, many things can go wrong in a machine learning project. Most are independent of the technology’s maturity or the analyst’s understanding of the machine learning algorithm.\n\nMachine learning texts, online tutorials, and courses are devoted to explaining how machine learning algorithms work and how to apply them to a dataset. Your success will probably depend on other factors. What data you can get and whether you can get enough of it, how you prepare it for learning, what features you engineer, whether your solution is scalable, maintainable, cannot be manipulated by attackers, and doesn’t make costly errors — these factors are much more important for an applied machine learning project.\n\nYet despite their magnitude, most modern machine learning books and courses often leave these aspects for self-study. Some provide only partial coverage, with just an application to solving a speciﬁc illustrative problem.\n\nIt’s a signiﬁcant gap in knowledge, and I tried to ﬁll it with this book.\n\n10.1 Takeaways\n\nWhat do I hope the reader takes away after reading this book?\n\nFirst of all, a strong understanding that all machine learning projects are unique. There’s no single recipe that will always work. Most of the time, the greatest challenges must be solved before you type from sklearn.linear_model import LogisticRegression: you must deﬁne your goal, select a baseline, gather relevant data, get it labeled with quality labels, and transform labeled data into training, validation, and test sets. The rest of the problem is solved after you type model.fit(X,y), by applying error analysis, evaluating the model, verifying it solves the problem, and works better than the existing solution.\n\nThe seasoned analyst or machine learning engineer understands that not all problems, business or otherwise, will be solved with machine learning. In fact, many problems can be solved more easily using a heuristic, a lookup in a database, or traditional software development. You probably should not use machine learning if the system’s every action, decision, or behavior, must be explained. With rare exceptions, machine learning models are blackboxes. They will not tell you why they predicted what they predicted, nor why they didn’t predict today what they predicted yesterday, nor how to ﬁx these issues.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3\n\nFurthermore, unless you can ﬁnd a public dataset and an open-source solution providing exactly what you need, machine learning is not the right approach for the shortest time to market. Sometimes the data needed to train and maintain a model is too hard or even impossible to get.\n\nOn the other hand, training data may be synthetically generated by using oversampling and data augmentation. These techniques are often applied when the data exhibits imbalance.\n\nBefore you start collecting data, ask these questions: is your data accessible, sizeable, usable, understandable, and reliable? Good data contains enough information for modeling, has good coverage of production use cases and few biases, is big enough to allow generalization, and not a result of the model itself.\n\nOr does your data come with high cost, bias, imbalance, missing attributes, and/or noisy labels? Data quality must be ensured before it’s used for training.\n\nThe machine learning project life cycle consists of the following stages: goal deﬁnition, data collection and preparation, feature engineering, model training, evaluation, deployment, serving, monitoring, and maintenance. At most stages, data leakage may arise. The analyst must be able to anticipate and prevent it.\n\nAfter data preparation, feature engineering is the second most important stage. For some data, such as natural language text, features may be generated in bulk by using techniques like bag-of-words. However, the most useful features are often handcrafted by the analyst domain knowledge. Put yourself into the “model’s shoes.”\n\nGood features have high predictive power, can be computed fast, are reliable and uncor- related. They are unitary, easy to understand and maintain. Feature extraction code is one of the most important parts of a machine learning system. It must be extensively and systematically tested.\n\nBest practices are to scale features, store and document them in schema ﬁles or feature stores, and keep code, model, and training data in sync.\n\nYou can synthesize new features by discretizing existing features, clustering training examples, and applying simple transformations to existing features, or combining pairs of them.\n\nBefore starting to work on a model, make sure that data conforms to the schema, then split it into three sets: training, validation, and test. Deﬁne an achievable level of performance, and choose a performance metric. It should reduce the model performance to a single number.\n\nMost machine learning algorithms, models, and pipelines have hyperparameters. They can signiﬁcantly inﬂuence the result of learning. However, these are not learned from data. The analyst sets their values during the hyperparameter tuning. In particular, tweaking these values controls two important tradeoﬀs: precision-recall and bias-variance. By varying the complexity of the model, you can reach the so-called “zone of solutions,” a situation where both bias and variance are relatively low. The solution that optimizes the performance metric is usually found in the neighborhood of the zone of solutions. Grid search is the simplest and most widely-used hyperparameter-tuning technique.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "page_number": 272,
      "chapter_number": 32,
      "summary": "This chapter covers segment 32 (pages 272-280). Key topics include models, users, and errors. Indeed, a machine’s data requirements are usually standard and pre- determined.",
      "keywords": [
        "Burkov Machine Learning",
        "Machine Learning Engineering",
        "model",
        "Andriy Burkov Machine",
        "user",
        "Machine Learning",
        "system",
        "Burkov Machine",
        "machine learning system",
        "Learning Engineering",
        "errors",
        "Machine",
        "machine learning model",
        "Andriy Burkov",
        "Learning"
      ],
      "concepts": [
        "models",
        "users",
        "errors",
        "predict",
        "prediction",
        "monitoring",
        "data",
        "value",
        "alerted",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 20,
          "title": "Segment 20 (pages 170-177)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 281-289)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 433-440)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 281-289)",
      "start_page": 281,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": "Instead of training a deep model from scratch, it can be useful to start with a pre-trained model. Using a pre-trained model to build your own is called transfer learning. The fact that deep models allow for transfer learning is one of its most important properties.\n\nTraining deep models can be tricky. Implementation errors can happen at many stages, from data preparation, to deﬁning a neural network topology. It’s recommended to start small. For example, implement a simple model using a high-level library. Apply the default hyperparameter values to a small normalized dataset ﬁtting in memory. Once you have your ﬁrst simplistic model architecture and dataset, temporarily reduce your training dataset even further, to the size of one minibatch. Then start the training. Make sure that your simple model is capable of overﬁtting this training minibatch.\n\nYour machine learning system’s performance may beneﬁt from model stacking. Ideally, base models used for stacking are obtained from algorithms or models of a diﬀerent nature, such as random forests, gradient boosting, support vector machines, and deep models. Many real-world production systems are based on stacked models.\n\nMachine learning model errors can be either uniform, and apply to all use cases with the same rate, or focused, and apply to certain use cases more frequently. By ﬁxing a focused error, you ﬁx it once for many examples.\n\nModel performance can be improved using the following simple, iterative process:\n\n1. Build the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set. 3. Find the most frequent error patterns on that small validation set. 4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed.\n\nThe model must be carefully evaluated before deployment, and continuously afterwards. Perform an oﬄine model evaluation when the model is initially trained, based on the historical data. An online model evaluation consists of testing and comparing models in the production environment, using online data. Two popular techniques of online model evaluation are A/B testing and multi-armed bandit. They allow us to determine whether the new model is better than the old one.\n\nA model may be deployed following several patterns: statically (as a part of an installable software package), dynamically on the user’s device or a server, or via model streaming. In addition, choose among strategies such as single deployment, silent deployment, canary deployment, and multi-armed bandit. Each pattern and strategy has its pros and cons, and should be chosen depending on your business application.\n\nAlgorithmic eﬃciency is also an important consideration for model deployment. Scientiﬁc Python packages like NumPy, SciPy, and scikit-learn were built by experienced scientists and engineers with eﬃciency in mind. They have many methods implemented in C for maximum eﬃciency. Avoid writing your own production code, when you can reuse a popular and mature library or package. For high eﬃciency, choose appropriate data structures and caching.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5\n\nFor some applications, the prediction speed is critical. In such cases, the production code is written in a compiled language, such as Java or C/C++. If a data analyst has built a model in Python or R, there are several options for production deployment: rewrite the code in a compiled programming language of the production environment, use a model representation standard such as PMML or PFA, or use a specialized execution engine such as MLeap.\n\nMachine learning models are served in either batch or on-demand mode. When served on-demand, a model is usually wrapped into a REST API. Serving a machine is often done by using a streaming architecture.\n\nWhen a software system is exposed to the real world, its architecture must be ready to eﬀectively react to errors, change, and human nature. A model must be constantly monitored. Monitoring must allow us to make sure that the model is served correctly and its performance remains within acceptable limits.\n\nIt is important to log enough information to reproduce any erratic system behavior during future analysis. If the model is served to a front-end user, it’s important to save the user’s context at the moment of the model serving.\n\nSome users can try to abuse your model to reach their own business goals. To prevent abuse, don’t trust the data coming from a user, unless similar data comes from multiple users. Assign a reputation score to each user and don’t trust the data obtained from users with low reputations. Classify user behavior as normal or abnormal, and make progressively longer pauses or block some users, if necessary.\n\nRegularly update your model by analyzing users’ behavior and input data, to make it more robust. Afterwards, run the new model against the end-to-end and conﬁdence test sets. Make sure that the outputs are as before, or that the changes are as expected. Validate that the new model doesn’t make signiﬁcantly more costly errors. Ensure errors are distributed uniformly across user categories. It’s undesirable if the new model aﬀects negatively most users from a minority or speciﬁc location.\n\n∗ ∗ ∗\n\nThe book stops here, but your learning doesn’t. Machine learning engineering is a relatively new ﬁeld of software engineering. Thanks to online publications and open source, I’m sure that new best practices, libraries, and frameworks simplifying or solidifying the stages of data preparation, model evaluation, deployment, serving, and monitoring, will appear during the upcoming years. Subscribe to my mailing list on this book’s companion website http://www.mlebook.com. You will regularly receive relevant links.\n\nPlease keep in mind that, like its predecessor The Hundred-Page Machine Learning Book, this book is distributed on the “read-ﬁrst, buy-later” principle. This means that you may download the entire text of the book from its companion website and read before buying. If you’re reading these concluding words from a PDF ﬁle, and cannot remember having paid for it, please consider buying the book. You may buy it from Amazon, Leanpub, and other major online book sellers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6\n\n10.2 What to Read Next\n\nThere are many great books on machine learning and artiﬁcial intelligence. Here, I will give you only several recommendations.\n\nIf you would like to get hands-on experience with practical machine learning in Python, there are two books:\n\n“Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” (2nd edition) by Aurélien Géron (O’Reilly Media, 2019), and\n\n“Python Machine Learning” (3rd edition) by Sebastian Raschka (Packt Publishing, 2019).\n\nFor R, the best choice is “Machine Learning with R” by Brett Lantz (Packt Publishing, 2019).\n\nTo get a deeper understanding of the underlying math behind various machine learning algorithms, I recommend,\n\n“Pattern Recognition and Machine Learning” by Christopher Bishop (Springer, 2006), and\n\n“An Introduction to Statistical Learning” by Gareth James et al. (Springer, 2013).\n\nTo get more detailed understanding of deep learning, I recommend,\n\n“Neural Networks and Deep Learning” by Michael Nielsen (online, 2005), and • “Generative Deep Learning” by David Foster (O’Reilly Media, 2019).\n\nIf your ambitions go far beyond machine learning and you want to sweep the whole ﬁeld of artiﬁcial intelligence, then “Artiﬁcial Intelligence: A Modern Approach” (4th Edition) by Stuart Russell and Peter Norvig (Pearson, 2020), known as AIMA, is your best book.\n\n10.3 Acknowledgements\n\nThe high quality of this book would be impossible without volunteering editors. I especially thank the following readers for their systematic contributions: Alexander Sack, Ana Fotina, Francesco Rinarelli, Yonas Mitike Kassa, Kelvin Sundli, Idris Aleem, and Tim Flocke.\n\nI thank scientiﬁc advisors, Veronique Tremblay and Maximilian Hudlberger, for the review and correction of the Model Evaluation chapter. I’m also grateful to Cassie Kozyrkov for her attentive and critical eye that allowed solidifying the section on statistical tests.\n\nOther wonderful people to whom I am grateful for their help are Jean Santos, Carlos Azevedo, Zakarie Hashi, Tridib Dutta, Zakariya Abu-Grin, Suhel Khan, Brad Ezard, Cole Holcomb, Oliver Proud, Michael Schock, Fernando Hannaka, Ayla Khan, Varuna Eswer, Stephen Fox, Brad Klassen, Felipe Duque, Alexandre Mundim, John Hill, Ryan Volpi, Gaurish Katlana, Harsha Srivatsa, Agrita Garnizone, Shyambhu Mukherjee, Christopher Thompson, Sylvain Truong, Niklas Hansson, Zhihao Wu, Max Schumacher, Piers Casimir, Harry Ritchie, Marko Peltojoki, Gregory V., Win Pet, Yihwa Kim, Timothée Bernard, Marwen Sallem, Daniel\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7\n\nBourguet, Aliza Rubenstein, Alice O., Juan Carlo Rebanal, Haider Al-Tahan, Josh Cooper, Venkata Yerubandi, Mahendren S., Abhijit Kumar, Mathieu Bouchard, Yacin Bahi, Samir Char, Luis Leopoldo Perez, Mitchell DeHaven, Martin Gubri, Guillermo Santamaría, Mustafa Murat Arat, Rex Donahey, Nathaniel Netirungroj, Aliza Rubenstein , Rahima Karimova, Darwin Brochero, Vaheid Wallets, Bharat Raghunathan, Carlos Salas, Ji Hui Yang, Jonas Atarust, Siddarth Sampangi, Utkarsh Mittal, Felipe Antunes, Larysa Visengeriyeva, Sorin Gatea, Mattia Pancerasa, Victor Zabalza, Dibyendu Mandal, and James Hoover.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "page_number": 281,
      "chapter_number": 33,
      "summary": "This chapter covers segment 33 (pages 281-289). Key topics include model, user, and data. When you store users’ activity data in logs, the users should know what, when, and how it is stored, and for how long.",
      "keywords": [
        "model",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "Machine Learning",
        "Machine learning models",
        "Andriy Burkov Machine",
        "user",
        "data",
        "Burkov Machine",
        "Learning Engineering",
        "model update",
        "Machine",
        "Learning",
        "training data",
        "model training"
      ],
      "concepts": [
        "model",
        "user",
        "data",
        "make",
        "making",
        "training",
        "update",
        "updated",
        "business",
        "time"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 33,
          "title": "Segment 33 (pages 281-289)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 33,
          "title": "Segment 33 (pages 282-291)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 10,
          "title": "Segment 10 (pages 78-85)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 290-296)",
      "start_page": 290,
      "end_page": 296,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 290,
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 290-296). Key topics include models, data, and train. Thanks to open-source code, crowdsourcing, easily accessible books, online courses, and publicly available datasets, many scientists, engineers, and even home enthusiasts may now train machine learning models.",
      "keywords": [
        "machine learning",
        "Machine Learning Engineering",
        "Burkov Machine Learning",
        "machine learning models",
        "learning",
        "machine",
        "model",
        "machine learning project",
        "Andriy Burkov Machine",
        "Learning Engineering",
        "machine learning algorithms",
        "machine learning system",
        "Burkov Machine",
        "data",
        "learning models"
      ],
      "concepts": [
        "models",
        "data",
        "train",
        "machines",
        "learned",
        "features",
        "errors",
        "user",
        "book",
        "deployment"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-25)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Machine Learning Design Patterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-9)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 2,
          "title": "Segment 2 (pages 9-18)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 6,
          "title": "Segment 6 (pages 42-49)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Foreword\n\nForeword by Cassie Kozyrkov, Chief Decision Scientist at Google, author of the course Making Friends with Machine Learning on Google Cloud Platform\n\nI’d like to let you in on a secret: when people say “machine learning” it sounds like there’s only one discipline here. Surprise! There are actually two machine learnings, and they are as diﬀerent as innovating in food recipes and inventing new kitchen appliances. Both are noble callings, as long as you don’t get them confused; imagine hiring a pastry chef to build you an oven or an electrical engineer to bake bread for you!\n\nThe bad news is that almost everyone does mix these two machine learnings up. No wonder so many businesses fail at machine learning as a result. What no one seems to tell beginners is that most machine learning courses and textbooks are about Machine Learning Research — how to build ovens (and microwaves, blenders, toasters, kettles...the kitchen sink!) from scratch, not how to cook things and innovate with recipes at enormous scale. In other words, if you’re looking for opportunities to create innovative ML-based solutions to business problems, you want the discipline called Applied Machine Learning, not Machine Learning Research, so most books won’t suit your needs.\n\nAnd now for the good news! You’re looking at one of the few true Applied Machine Learning books out there. That’s right, you found one! A real applied needle in the haystack of research-oriented stuﬀ. Excellent job, dear reader...unless what you were actually looking for is a book to help you learn the skills to design general purpose algorithms, in which case I hope the author won’t be too upset with me for telling you to ﬂee now and go pick up pretty much any other machine learning book. This one is diﬀerent.\n\nWhen I created Making Friends with Machine Learning in 2016, Google’s Applied Machine Learning course loved by more than ten thousand of our engineers and leaders, I gave it a very similar structure to the one in this book. That’s because doing things in the right order is crucial in the applied space. As you use your newfound data powers, tackling certain steps before you’ve completed others can lead to anything from wasted eﬀort to a project-demolishing kablooie. In fact, the similarity in table of contents between this book and my course is what originally convinced me to give this book a read. In a clear case of convergent evolution, I saw in the author a fellow thinker kept up at night by the lack of available resources on Applied Machine Learning, one of the most potentially-useful yet horribly-misunderstood areas of engineering, enough to want to do something about it. So, if you’re about to close this book, how about you do me a quick favor and at least ponder why the Table of Contents is arranged the way it is. You’ll learn something good just from that, I promise.\n\nSo, what’s in the rest of the book? The machine learning equivalent of a bumper guide to innovating in recipes to make food at scale. Since you haven’t read the book yet, I’ll put\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 3112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "it in culinary terms: you’ll need to ﬁgure out what’s worth cooking / what the objectives are (decision-making and product management), understand the suppliers and the customers (domain expertise and business acumen), how to process ingredients at scale (data engineering and analysis), how to try many diﬀerent ingredient-appliance combinations quickly to generate potential recipes (prototype phase ML engineering), how to check that the quality of the recipe is good enough to serve (statistics), how to turn a potential recipe into millions of dishes served eﬃciently (production phase ML engineering), and how to ensure that your dishes stay top notch even if the delivery truck brings you a ton of potatoes instead of the rice you ordered (reliability engineering). This book is one of the few to oﬀer perspectives on each step of the end-to-end process.\n\nNow would be a good moment for me to be blunt with you, dear reader. This book is pretty good. It is. Really. But it’s not perfect. It cuts corners on occasion — just like a professional machine learning engineer is wont to do — though on the whole it gets its message right. And, since it covers an area with rapidly-evolving best practices, it doesn’t pretend to oﬀer the last word on the subject. But even if it were terribly sloppy, it would still be worth reading. Given how few comprehensive guides to Applied Machine Learning are out there, a coherent introduction to these topics is worth its weight in gold. I’m so glad this one is here!\n\nOne of my favorite things about this book is how fully it embraces the most important thing you need to know about machine learning: mistakes are possible...and sometimes they hurt. As my colleagues in site reliability engineering love to say, “Hope is not a strategy.” Hoping that there will be no mistakes is the worst approach you can take. This book does so much better. It promptly shatters any false sense of security you were tempted to have about building an AI system that is more “intelligent” than you are. (Um, no. Just no.) Then it diligently takes you through a survey of all kinds of things that can go wrong in practice and how to prevent/detect/handle them. This book does a great job of outlining the importance of monitoring, how to approach model maintenance, what to do when things go wrong, how to think about fallback strategies for the kinds of mistakes you can’t anticipate, how to deal with adversaries who try to exploit your system, and how to manage the expectations of your human users (there’s also a section on what to do when your, er, users are machines). These are hugely important topics in practical machine learning, but they’re so often neglected in other books. Not here.\n\nIf you intend to use machine learning to solve business problems at scale, I’m delighted you got your hands on this book. Enjoy!\n\nCassie Kozyrkov\n\nSeptember 2020\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 2941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Preface\n\nDuring the past several years, machine learning (ML), for many, has become a synonym for artiﬁcial intelligence. Even though machine learning, as a ﬁeld of science, has existed for several decades, only a handful of organizations in the world have fully harnessed its potential. Despite the availability of modern open-source machine learning libraries, packages and frameworks supported by the leading organizations and broad communities of scientists and software engineers, most organizations are still struggling to apply machine learning for solving practical business problems.\n\nOne diﬃculty lies in the scarcity of talent. However, even when they have access to talented machine learning engineers and data analysts, in 2020, most organizations1 still spend between 31 and 90 days deploying one model, while 18 percent of companies are taking longer than 90 days — some spending more than a year productionizing. The main challenges organizations face when developing ML capabilities, such as model version control, reproducibility, and scaling, are rather engineering than scientiﬁc.\n\nThere are plenty of good books on machine learning, both theoretical and hands-on. From a typical machine learning book, you can learn the types of machine learning, major families of algorithms, how they work, and how to build models from data using those algorithms.\n\nA typical machine learning book is less concerned with the engineering aspects of implementing machine learning projects. Such questions as data collection, storage, preprocessing, feature engineering, as well as testing and debugging of models, their deployment to and retirement from production, runtime and post-production maintenance, are often left outside the scope of machine learning books.\n\nThis book intends to ﬁll that gap.\n\nWho This Book is For\n\nI assume that the reader of this book understands machine learning basics and is capable of building a model, given a properly formatted dataset using a favorite programming language or a machine learning library. If you don’t feel comfortable applying machine learning algorithms to data and don’t clearly see the diﬀerence between logistic regression, support vector machine, and random forest, I recommend starting your journey with The Hundred-Page Machine Learning Book, and then move to this book.\n\nThe target audience of this book is data analysts who lean towards a machine learning engineering role, machine learning engineers who want to bring more structure to their work, machine learning engineering students, as well as software architects who happen to deal with models provided by data analysts and machine learning engineers.\n\n1“2020 state of enterprise machine learning”, Algorithmia, 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 2794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "How to Use This Book\n\nThis book is a comprehensive review of machine learning engineering best practices and design patterns. I recommend reading it from beginning to end. However, you can read chapters in any order as they cover distinct aspects of the machine learning project lifecycle and do not have direct dependencies.\n\nShould You Buy This Book?\n\nLike its companion and precursor The Hundred-Page Machine Learning Book, this book is distributed on the “read-ﬁrst, buy-later” principle. I ﬁrmly believe that readers must be able to read a book before paying for it; otherwise, they buy a pig in a poke.\n\nThe “read-ﬁrst, buy-later” principle implies that you can freely download the book, read it, and share it with your friends and colleagues. If you read and liked the book, or found it helpful or useful in your work, business, or studies, then buy it.\n\nNow you are all set. Enjoy your reading!\n\nAndriy Burkov\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "1 Introduction\n\nThough the reader of this book should have a basic understanding of machine learning, it is still important to start with deﬁnitions, so that we are sure that we have a common understanding of the terms used throughout the book.\n\nBelow, I repeat some of the deﬁnitions from Chapter 2 of The Hundred-Page Machine Learning Book and also give several new ones. If you read my ﬁrst book, some parts of this chapter might sound familiar.\n\nAfter reading this chapter, we will understand the same way such concepts as supervised and unsupervised learning. We will agree on the data terminology, such as data used directly and indirectly, raw and tidy data, training and holdout data.\n\nWe will know when to use machine learning, when not to use it, and various forms of machine learning such as model- and instance-based, deep and shallow, classiﬁcation and regression, and others.\n\nFinally, we will deﬁne the scope of machine learning engineering and introduce the machine learning project lifecycle.\n\n1.1 Notation and Deﬁnitions\n\nLet’s start by stating the basic mathematical notation and deﬁne the terms and notions, to which we will often have recourse in this book.\n\n1.1.1 Data Structures\n\nA scalar1 is a simple numerical value, like 15 or −3.25. Variables or constants that take scalar values are denoted by an italic letter, like x or a.\n\nA vector is an ordered list of scalar values, called attributes. We denote a vector as a bold character, for example, x or w. Vectors can be visualized as arrows that point to some directions as well as points in a multi-dimensional space. Illustrations of three two-dimensional vectors, a = [2,3], b = [−2,5], and c = [1,0] are given in Figure 1. We denote an attribute of a vector as an italic value with an index, like this: w(j) or x(j). The index j denotes a speciﬁc dimension of the vector, the position of an attribute in the list. For instance, in the vector a shown in red in Figure 1, a(1) = 2 and a(2) = 3.\n\n1If a term is in bold, that means that the term can be found in the index at the end of the book.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Figure 1: Three vectors visualized as directions and as points.\n\nThe notation x(j) should not be confused with the power operator, such as the 2 in x2 (squared) or 3 in x3 (cubed). If we want to apply a power operator, say squared, to an indexed attribute of a vector, we write like this: (x(j))2.\n\nA variable can have two or more indices, like this: x(j) neural networks, we denote as x(j) l,u A matrix is a rectangular array of numbers arranged in rows and columns. Below is an example of a matrix with two rows and three columns,\n\nor like this x(k) i,j i the input feature j of unit u in layer l.\n\n. For example, in\n\nA =\n\n(cid:20)2 −2 5 3\n\n(cid:21) 1 0\n\n.\n\nMatrices are denoted with bold capital letters, such as A or W. You can notice from the above example of matrix A that matrices can be seen as regular structures composed of vectors. Indeed, the columns of matrix A above are vectors a, b, and c illustrated in Figure 1.\n\nA set is an unordered collection of unique elements. We denote a set as a calligraphic capital character, for example, S. A set of numbers can be ﬁnite (include a ﬁxed amount of values). In this case, it is denoted using accolades, for example, {1,3,18,23,235} or {x1,x2,x3,x4,...,xn}. Alternatively, a set can be inﬁnite and include all values in some inter- val. If a set includes all values between a and b, including a and b, it is denoted using brackets as [a,b]. If the set doesn’t include the values a and b, such a set is denoted using parentheses like this: (a,b). For example, the set [0,1] includes such values as 0, 0.0001, 0.25, 0.784, 0.9995, and 1.0. A special set denoted R includes all numbers from minus inﬁnity to plus inﬁnity.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 1733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "When an element x belongs to a set S, we write x ∈ S. We can obtain a new set S3 as an intersection of two sets S1 and S2. In this case, we write S3 ← S1 ∩ S2. For example {1,3,5,8} ∩ {1,8,4} gives the new set {1,8}. We can obtain a new set S3 as a union of two sets S1 and S2. S3 ← S1 ∪ S2. For example {1,3,5,8} ∪ {1,8,4} gives the new set {1,3,5,8,4}. The notation |S| means the size of set S, that is, the number of elements it contains.\n\nIn this case, we write\n\n1.1.2 Capital Sigma Notation\n\nThe summation over a collection X = {x1,x2,...,xn−1,xn} or over the attributes of a vector x = [x(1),x(2),...,x(m−1),x(m)] is denoted like this:\n\nn X\n\nxi\n\ndef= x1 + x2 + ... + xn−1 + xn, or else:\n\nm X\n\nx(j) def= x(1) + x(2) + ... + x(m−1) + x(m).\n\ni=1\n\nj=1\n\nThe notation def= means “is deﬁned as”.\n\nThe Euclidean norm of a vector x, denoted by kxk, characterizes the “size” or the “length” of the vector. It’s given by\n\nqPD\n\n(cid:0)x(j)(cid:1)2.\n\nj=1\n\nThe distance between two vectors a and b is given by the Euclidean distance:\n\nka − bk def=\n\nv u u t\n\nN X\n\n(cid:0)a(i) − b(i)(cid:1)2\n\n.\n\ni=1\n\n1.2 What is Machine Learning\n\nMachine learning is a subﬁeld of computer science that is concerned with building al- gorithms that, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans, or generated by another algo- rithm.\n\nMachine learning can also be deﬁned as the process of solving a practical problem by,\n\n1) collecting a dataset, and 2) algorithmically training a statistical model based on that dataset.\n\nThat statistical model is assumed to be used somehow to solve the practical problem. To save keystrokes, I use the terms “learning” and “machine learning” interchangeably. For the same reason, I often say “model” referring to a statistical model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 1880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Learning can be supervised, semi-supervised, unsupervised, and reinforcement.\n\n1.2.1 Supervised Learning\n\nIn supervised learning, the data analyst works with a collection of labeled examples {(x1,y1),(x2,y2),...,(xN,yN)}. Each element xi among N is called a feature vector. In computer science, a vector is a one-dimensional array. A one-dimensional array, in turn, is an ordered and indexed sequence of values. The length of that sequence of values, D, is called the vector’s dimensionality.\n\nA feature vector is a vector in which each dimension j from 1 to D contains a value that describes the example. Each such value is called a feature and is denoted as x(j). For instance, if each example x in our collection represents a person, then the ﬁrst feature, x(1), could contain height in cm, the second feature, x(2), could contain weight in kg, x(3) could contain gender, and so on. For all examples in the dataset, the feature at position j in the feature vector always contains the same kind of information. It means that if x(2) contains weight in kg in some example xi, then x(2) will also contain weight in kg in every example k xk, for all k from 1 to N. The label yi can be either an element belonging to a ﬁnite set of classes {1,2,...,C}, or a real number, or a more complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated, in this book yi is either one of a ﬁnite set of classes or a real number.2 You can think of a class as a category to which an example belongs.\n\ni\n\nFor instance, if your examples are email messages and your problem is spam detection, then you have two classes: spam and not_spam. In supervised learning, the problem of predicting a class is called classiﬁcation, while the problem of predicting a real number is called regression. The value that has to be predicted by a supervised model is called a target. An example of regression is a problem of predicting the salary of an employee given their work experience and knowledge. An example of classiﬁcation is when a doctor enters the charac- teristics of a patient into a software application, and the application returns the diagnosis.\n\nThe diﬀerence between classiﬁcation and regression is shown in Figure 2. In classiﬁcation, the learning algorithm looks for a line (or, more generally, a hypersurface) that separates examples of diﬀerent classes from one another. In regression, on the other hand, the learning algorithm looks to ﬁnd a line or a hypersurface that closely follows the training examples.\n\n2A real number is a quantity that can represent a distance along a line. Examples: 0, −256.34, 1000,\n\n1000.2.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "classiﬁcationregression\n\nFigure 2: Diﬀerence between classiﬁcation and regression.\n\nThe goal of a supervised learning algorithm is to use a dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing a label for this feature vector. For instance, a model created using a dataset of patients could take as input a feature vector describing a patient and output a probability that the patient has cancer.\n\nEven if the model is typically a mathematical function, when thinking about what the model does with the input, it is convenient to think that the model “looks” at the values of some features in the input and, based on experience with similar examples, outputs a value. That output value is a number or a class “the most similar” to the labels seen in the past in the examples with similar values of features. It looks simplistic, but the decision tree model and the k-nearest neighbors algorithm work almost like that.\n\n1.2.2 Unsupervised Learning\n\nIn unsupervised learning, the dataset is a collection of unlabeled examples {x1,x2,...,xN}. Again, x is a feature vector, and the goal of an unsupervised learning algorithm is to create a model that takes a feature vector x as input and either transforms it into another vector or into a value that can be used to solve a practical problem. For example, in clustering, the model returns the ID of the cluster for each feature vector in the dataset. Clustering is useful for ﬁnding groups of similar objects in a large collection of objects, such as images or text documents. By using clustering, for example, the analyst can sample a suﬃciently representative yet small subset of unlabeled examples from a large\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 1772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "collection of examples for manual labeling: a few examples are sampled from each cluster instead of sampling directly from the large collection and risking only sampling examples very similar to one another.\n\nIn dimensionality reduction, the model’s output is a feature vector with fewer dimensions than the input. For example, the scientist has a feature vector that is too complex to visualize (it has more than three dimensions). The dimensionality reduction model can transform that feature vector into a new feature vector (by preserving the information up to some extent) with only two or three dimensions. This new feature vector can be plotted on a graph.\n\nIn outlier detection, the output is a real number that indicates how the input feature vector is diﬀerent from a “typical” example in the dataset. Outlier detection is useful for solving a network intrusion problem (by detecting abnormal network packets that are diﬀerent from a typical packet in “normal” traﬃc) or detecting novelty (such as a document diﬀerent from the existing documents in a collection).\n\n1.2.3 Semi-Supervised Learning\n\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples. Usually, the quantity of unlabeled examples is much higher than the number of labeled examples. The goal of a semi-supervised learning algorithm is the same as the goal of the supervised learning algorithm. The hope here is that, by using many unlabeled examples, a learning algorithm can ﬁnd (we might say “produce” or “compute”) a better model.\n\n1.2.4 Reinforcement Learning\n\nReinforcement learning is a subﬁeld of machine learning where the machine (called an agent) “lives’’ in an environment and is capable of perceiving the state of that environment as a vector of features. The machine can execute actions in non-terminal states. Diﬀerent actions bring diﬀerent rewards and could also move the machine to another state of the environment. A common goal of a reinforcement learning algorithm is to learn an optimal policy.\n\nAn optimal policy is a function (similar to the model in supervised learning) that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average long-term reward.\n\nReinforcement learning solves a particular problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics.\n\nIn this book, for simplicity, most explanations are limited to supervised learning. However, all the material presented in the book is applicable to other types of machine learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 2706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "1.3 Data and Machine Learning Terminology\n\nNow let’s introduce the common data terminology (such as data used directly and indirectly, raw and tidy data, training and holdout data) and the terminology related to machine learning (such as baseline, hyperparameter, pipeline, and others).\n\n1.3.1 Data Used Directly and Indirectly\n\nThe data you will work with in your machine learning project can be used to form the examples x directly or indirectly.\n\nImagine that we build a named entity recognition system. The input of the model is a sequence of words; the output is the sequence of labels3 of the same length as the input. To make the data readable by a machine learning algorithm, we have to transform each natural language word into a machine-readable array of attributes, which we call a feature vector.4 Some features in the feature vector may contain the information that distinguishes that speciﬁc word from other words in the dictionary. Other features can contain additional attributes of the word in that speciﬁc sequence, such as its shape (lowercase, uppercase, capitalized, and so on). Or it can be binary attributes indicating whether this word is the ﬁrst word of some human name or the last word of the name of some location or organization. To create these latter binary features, we may decide to use some dictionaries, lookup tables, gazetteers, or other machine learning models making predictions about words.\n\nYou could already have noticed that the collection of word sequences is the data used to form training examples directly, while the data contained in dictionaries, lookup tables, and gazetteers is used indirectly: we can use it to extend feature vectors with additional features, but we cannot use it to create new feature vectors.\n\n1.3.2 Raw and Tidy Data\n\nAs we just discussed, directly used data is a collection of entities that constitute the basis of a dataset. Each entity in that collection can be transformed into a training example. Raw data is a collection of entities in their natural form; they cannot always be directly employable for machine learning. For instance, a Word document or a JPEG ﬁle are pieces of raw data; they cannot be directly used by a machine learning algorithm.5\n\n3Labels can be, for example, values from the set {“Location”, “Organization”, “Person”, “Other”}. 4The terms “attribute” and “feature” are often used interchangeably. In this book, I use the term “attribute” to describe a speciﬁc property of an example, while the term “feature” refers to value x(j) at position j in the feature vector x used by a machine learning algorithm.\n\n5The term “unstructured data” is often used to designate a data element that contains information whose type was not formally deﬁned. Examples of unstructured data are photos, images, videos, text messages, social media posts, PDFs, text documents, and emails. The term “semi-structured data” refers to data elements whose structure helps deriving types of some information encoded in those data elements. Examples of semi-structured data include log ﬁles, comma- and tab-delimited text ﬁles, as well as documents in JSON and XML formats.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 3201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "To be employable in machine learning, a necessary (but not suﬃcient) condition for the data is to be tidy. Tidy data can be seen as a spreadsheet, in which each row represents one example, and columns represent various attributes of an example, as shown in Figure 3. Sometimes raw data can be tidy, e.g., provided to you in the form of a spreadsheet. However, in practice, to obtain tidy data from raw data, data analysts often resort to the procedure called feature engineering, which is applied to the direct and, optionally, indirect data with the goal to transform each raw example into a feature vector x. Chapter 4 is devoted entirely to feature engineering.\n\nGermany\n\nGermany\n\nChina\n\n83M\n\n67M\n\nAisa\n\n3.7T\n\n...\n\n...\n\n1386M\n\n12.2T\n\nRegion\n\nPopulation\n\nEurope\n\nEurope\n\n...\n\nGDP\n\nCountry\n\nFrance\n\n2.6T\n\n...\n\nCountry\n\nFrance\n\n...\n\nChina\n\n83M\n\n67M\n\n2.6T\n\nPopulation\n\n...\n\n...\n\n3.7T\n\n12.2T\n\nRegion\n\nEurope\n\nEurope\n\n...\n\nAsia\n\nattributesexamples\n\n1386M\n\nGDP\n\nFigure 3: Tidy data: examples are rows and attributes are columns.\n\nIt’s important to note here that for some tasks, an example used by a learning algorithm can have a form of a sequence of vectors, a matrix, or a sequence of matrices. The notion of data tidiness for such algorithms is deﬁned similarly: you only replace “row of ﬁxed width in a spreadsheet” by a matrix of ﬁxed width and height, or a generalization of matrices to a higher dimension called a tensor.\n\nThe term “tidy data” was coined by Hadley Wickham in his paper with the same title.6\n\nAs I mentioned at the beginning of this subsection, data can be tidy, but still not usable by a particular machine learning algorithm. Most machine learning algorithms, in fact, only accept training data in the form of a collection of numerical feature vectors. Consider the data shown in Figure 3. The attribute “Region” is categorical and not numerical. The decision tree learning algorithm can work with categorical values of attributes, but most learning algorithms cannot. In Section ?? of Chapter 4, we will see how to transform a categorical attribute into a numerical feature.\n\nNote that in the academic machine learning literature, the word “example” typically refers to a tidy data example with an optionally assigned label. However, during the stage of data collection and labeling, which we consider in the next chapter, examples can still be in the raw form: images, texts, or rows with categorical attributes in a spreadsheet. In this book, when it’s important to highlight the diﬀerence, I will say raw example to indicate that a\n\n6Wickham, Hadley. “Tidy data.” Journal of Statistical Software 59.10 (2014): 1-23.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 2698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "piece of data was not transformed into a feature vector yet. Otherwise, assume that examples have the form of feature vectors.\n\n1.3.3 Training and Holdout Sets\n\nIn practice, data analysts work with three distinct sets of examples:\n\n1) training set, 2) validation set,7 and 3) test set.\n\nOnce you have got the data in the form of a collection of examples, the ﬁrst thing you do in your machine learning project is shuﬄe the examples and split the dataset into three distinct sets: training, validation, and test. The training set is usually the biggest one; the learning algorithm uses the training set to produce the model. The validation and test sets are roughly the same size, much smaller than the size of the training set. The learning algorithm is not allowed to use examples from the validation or test sets to train the model. That is why those two sets are also called holdout sets.\n\nThe reason to have three sets, and not one, is simple: when we train a model, we don’t want the model to only do well at predicting labels of examples the learning algorithm has already seen. A trivial algorithm that simply memorizes all training examples and then uses the memory to “predict” their labels will make no mistakes when asked to predict the labels of the training examples. However, such an algorithm would be useless in practice. What we really want is a model that is good at predicting examples that the learning algorithm didn’t see. In other words, we want good performance on a holdout set.8\n\nWe need two holdout sets and not one because we use the validation set to 1) choose the learning algorithm, and 2) ﬁnd the best conﬁguration values for that learning algorithm (known as hyperparameters). We use the test set to assess the model before delivering it to the client or putting it in production. That is why it’s important to make sure that no information from the validation or test sets is exposed to the learning algorithm. Otherwise, the validation and test results will most likely be too optimistic. This can indeed happen due to data leakage, an important phenomenon we consider in Section ?? of Chapter 3 and subsequent chapters.\n\n7In some literature, the validation set can also be called “development set.” Sometimes, when the labeled examples are scarce, analysts can decide to work without a validation set, as we will see in Chapter 5 in the section on cross-validation.\n\n8To be precise, we want the model to do well on most random samples from the statistical distribution to which our data belongs. We assume that if the model demonstrates good performance on a holdout set, randomly drawn from the unknown distribution of our data, there are high chances that our model will do well on other random samples of our data.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 2810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "1.3.4 Baseline\n\nIn machine learning, a baseline is a simple algorithm for solving a problem, usually based on a heuristic, simple summary statistics, randomization, or very basic machine learning algorithm. For example, if your problem is classiﬁcation, you can pick a baseline classiﬁer and measure its performance. This baseline performance will then become what you compare any future model to (usually, built using a more sophisticated approach).\n\n1.3.5 Machine Learning Pipeline\n\nA machine learning pipeline is a sequence of operations on the dataset that goes from its initial state to the model.\n\nA pipeline can include, among others, such stages as data partitioning, missing data im- putation, feature extraction, data augmentation, class imbalance reduction, dimensionality reduction, and model training.\n\nIn practice, when we deploy a model in production, we usually deploy an entire pipeline. Furthermore, an entire pipeline is usually optimized when hyperparameters are tuned.\n\n1.3.6 Parameters vs. Hyperparameters\n\nHyperparameters are inputs of machine learning algorithms or pipelines that inﬂuence the performance of the model. They don’t belong to the training data and cannot be learned from it. For example, the maximum depth of the tree in the decision tree learning algorithm, the misclassiﬁcation penalty in support vector machines, k in the k-nearest neighbors algorithm, the target dimensionality in dimensionality reduction, and the choice of the missing data imputation technique are all examples of hyperparameters.\n\nParameters, on the other hand, are variables that deﬁne the model trained by the learning algorithm. Parameters are directly modiﬁed by the learning algorithm based on the training data. The goal of learning is to ﬁnd such values of parameters that make the model optimal in a certain sense. Examples of parameters are w and b in the equation of linear regression y = wx+b. In this equation, x is the input of the model, and y is its output (the prediction).\n\n1.3.7 Classiﬁcation vs. Regression\n\nClassiﬁcation is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classiﬁcation.\n\nIn machine learning, the classiﬁcation problem is solved by a classiﬁcation learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 2498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "number that can be used by the analyst to deduce the label. An example of such a number is a probability of an input data element to have a speciﬁc label.\n\nIn a classiﬁcation problem, a label is a member of a ﬁnite set of classes. If the size of the set of classes is two (“sick”/“healthy”, “spam”/“not_spam”), we talk about binary classiﬁcation (also called binomial in some sources). Multiclass classiﬁcation (also called multinomial) is a classiﬁcation problem with three or more classes.9\n\nWhile some learning algorithms naturally allow for more than two classes, others are by nature binary classiﬁcation algorithms. There are strategies to turn a binary classiﬁcation learning algorithm into a multiclass one. I talk about one of them, one-versus-rest, in Section ?? of Chapter 6.\n\nRegression is a problem of predicting a real-valued quantity given an unlabeled example. Estimating house price valuation based on house features, such as area, number of bedrooms, location, and so on, is a famous example of regression.\n\nThe regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target.\n\n1.3.8 Model-Based vs. Instance-Based Learning\n\nMost supervised learning algorithms are model-based. A typical model is a support vector machine (SVM). Model-based learning algorithms use the training data to create a model with parameters learned from the training data. In SVM, the two parameters are w (a vector) and b (a real number). After the model is trained, it can be saved on disk while the training data can be discarded.\n\nInstance-based learning algorithms use the whole dataset as the model. One instance- based algorithm frequently used in practice is k-Nearest Neighbors (kNN). In classiﬁcation, to predict a label for an input example, the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw most often in this close neighborhood.\n\n1.3.9 Shallow vs. Deep Learning\n\nA shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most machine learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, speciﬁcally those that build neural networks with more than one layer between input and output. Such neural networks are called deep neural networks. In deep neural network learning (or, simply, deep learning),\n\n9There’s still one label per example, though.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers.\n\n1.3.10 Training vs. Scoring\n\nWhen we apply a machine learning algorithm to a dataset in order to obtain a model, we talk about model training or simply training.\n\nWhen we apply a trained model to an input example (or, sometimes, a sequence of examples) in order to obtain a prediction (or, predictions) or to somehow transform an input, we talk about scoring.\n\n1.4 When to Use Machine Learning\n\nMachine learning is a powerful tool for solving practical problems. However, like any tool, it should be used in the right context. Trying to solve all problems using machine learning would be a mistake.\n\nYou should consider using machine learning in one of the following situations.\n\n1.4.1 When the Problem Is Too Complex for Coding\n\nIn a situation where the problem is so complex or big that you cannot hope to write all the rules to solve it and where a partial solution is viable and interesting, you can try to solve the problem with machine learning.\n\nOne example is spam detection: it’s impossible to write the code that will implement such a logic that will eﬀectively detect spam messages and let genuine messages reach the inbox. There are just too many factors to consider. For instance, if you program your spam ﬁlter to reject all messages from people who are not in your contacts, you risk losing messages from someone who has got your business card at a conference. If you make an exception for messages containing speciﬁc keywords related to your work, you will probably miss a message from your child’s teacher, and so on.\n\nIf you still decide to directly program a solution to that complex problem, with time, you will have in your programming code so many conditions and exceptions from those conditions that maintaining that code will eventually become infeasible. In this situation, training a classiﬁer on examples “spam”/“not_spam” seems logical and the only viable choice.\n\nAnother diﬃculty for writing code to solve a problem lies in the fact that humans have a hard time with prediction problems based on input that has too many parameters; it’s especially true when those parameters are correlated in unknown ways. For example, take the problem of predicting whether a borrower will repay a loan. Hundreds of numbers represent each borrower: age, salary, account balance, frequency of past payments, married or not, number\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "of children, make and year of the car, mortgage balance, and so on. Some of those numbers may be important to make the decision, some may be less important alone, but become more important if considered in combination with some other numbers.\n\nWriting code that will make such decisions is hard because, even for an expert, it’s not clear how to combine, in an optimal way, all the attributes describing a person into a prediction.\n\n1.4.2 When the Problem Is Constantly Changing\n\nSome problems may continuously change with time so that the programming code must be regularly updated. That results in the frustration of software engineers working on the problem, an increased chance of introducing errors, diﬃculties of combining “previous” and “new” logic, and signiﬁcant overhead of testing and deploying updated solutions.\n\nFor example, you can have a task of scraping speciﬁc data elements from a collection of webpages. Let’s say that for each webpage in that collection, you write a set of ﬁxed data extraction rules in the following form: “pick the third <p> element from <body> and then pick the data from the second <div> inside that <p>.” If a website owner changes the design of a webpage, the data you scrape may end up in the second or the fourth <p> element, making your extraction rule wrong. If the collection of webpages you scrape is large (thousands of URLs), every day you will have rules that become wrong; you will end up endlessly ﬁxing those rules. Needless to say that very few software engineers would love to do such work on a daily basis.\n\n1.4.3 When It Is a Perceptive Problem\n\nToday, it’s hard to imagine someone trying to solve perceptive problems such as speech, image, and video recognition without using machine learning. Consider an image. It’s represented by millions of pixels. Each pixel is given by three numbers: the intensity of red, green, and blue channels. In the past, engineers tried to solve the problem of image recognition (detecting what’s on the picture) by applying handcrafted “ﬁlters” to square patches of pixels. If one ﬁlter, for example, the one that was designed to “detect” grass, generates a high value when applied to many pixel patches, while another ﬁlter, designed to detect brown fur, also returns high values for many patches, then we can say that there are high chances that the image represents a cow in a ﬁeld (I’m simplifying a bit).\n\nToday, perceptive problems are eﬀectively solved using machine learning models, such as neural networks. We consider the problem of training neural networks in Chapter 6.\n\n1.4.4 When It Is an Unstudied Phenomenon\n\nIf we need to be able to make predictions of some phenomenon that is not well-studied scientiﬁcally, but examples of it are observable, then machine learning might be an appropriate\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 2856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "(and, in some cases, the only available) option. For example, machine learning can be used to generate personalized mental health medication options based on the patient’s genetic and sensory data. Doctors might not necessarily be able to interpret such data to make an optimal recommendation, while a machine can discover patterns in data by analyzing thousands of patients and predicting which molecule has the highest chance to help a given patient.\n\nAnother example of observable but unstudied phenomena are logs of a complex computing system or a network. Such logs are generated by multiple independent or interdependent processes. For a human, it’s hard to make predictions about the future state of the system based on logs alone without having a model of each process and their interdependency. If the number of examples of historical log records is high enough (which is often the case), the machine can learn patterns hidden in logs and be able to make predictions without knowing anything about each process.\n\nFinally, making predictions about people based on their observed behavior is hard. In this problem, we obviously cannot have a model of a person’s brain, but we have readily available examples of expressions of the person’s ideas (in the form of online posts, comments, and other activities). Based on those expressions alone, a machine learning model deployed in a social network can recommend the content or other people to connect with.\n\n1.4.5 When the Problem Has a Simple Objective\n\nMachine learning is especially suitable for solving problems that you can formulate as a problem with a simple objective: such as yes/no decisions or a single number. In contrast, you cannot use machine learning to build a model that works as a general video game, like Mario, or a word processing software, like Word. This is due to too many diﬀerent decisions to make: what to display, where and when, what should happen as a reaction to the user’s input, what to write to or read from the hard drive, and so on; getting examples that illustrate all (or even most) of those decisions is practically infeasible.\n\n1.4.6 When It Is Cost-Eﬀective\n\nThree major sources of cost in machine learning are:\n\ncollecting, preparing, and cleaning the data, • training the model, • building and running the infrastructure to serve and and monitor the model, as well as labor resources to maintain it.\n\nThe cost of training the model includes human labor and, in some cases, the expensive hardware needed to train deep models. Model maintenance includes continuously monitoring the model and collecting additional data to keep the model up to date.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 2702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "1.5 When Not to Use Machine Learning\n\nThere are plenty of problems that cannot be solved using machine learning; it’s hard to characterize all of them. Here we only consider several hints.\n\nYou probably should not use machine learning when:\n\nevery action of the system or a decision made by it must be explainable, • every change in the system’s behavior compared to its past behavior in a similar situation must be explainable,\n\nthe cost of an error made by the system is too high, • you want to get to the market as fast as possible, • getting the right data is too hard or impossible, • you can solve the problem using traditional software development at a lower cost, • a simple heuristic would work reasonably well, • the phenomenon has too many outcomes while you cannot get a suﬃcient amount of examples to represent them (like in video games or word processing software),\n\nyou build a system that will not have to be improved frequently over time, • you can manually ﬁll an exhaustive lookup table by providing the expected output for any input (that is, the number of possible input values is not too large, or getting outputs is fast and cheap).\n\n1.6 What is Machine Learning Engineering\n\nMachine learning engineering (MLE) is the use of scientiﬁc principles, tools, and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE encompasses all stages from data collection, to model training, to making the model available for use by the product or the customers.\n\nTypically, a data analyst10 is concerned with understanding the business problem, building a model to solve it, and evaluating the model in a restricted development environment. A machine learning engineer, in turn, is concerned with sourcing the data from various systems and locations and preprocessing it, programming features, training an eﬀective model that will run in the production environment, coexist well with other production processes, be stable, maintainable, and easily accessible by diﬀerent types of users with diﬀerent use cases.\n\nIn other words, MLE includes any activity that lets machine learning algorithms be imple- mented as a part of an eﬀective production system.\n\nIn practice, machine learning engineers might be employed in such activities as rewriting a\n\n10Since circa 2013, data scientist has become a popular job title. Unfortunately, companies and experts don’t have an agreement on the deﬁnition of the term. Instead, I use the term “data analyst” by referring to a person capable of applying numerical or statistical analysis to data ready for analysis.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "data analyst’s code from rather slow R and Python11 into more eﬃcient Java or C++, scaling this code and making it more robust, packaging the code into an easy-to-deploy versioned package, optimizing the machine learning algorithm to make sure that it generates a model compatible with, and running correctly in, the organization’s production environment.\n\nIn many organizations, data analysts execute some of the MLE tasks, such as data collection, transformation, and feature engineering. On the other hand, machine learning engineers often execute some of the data analysis tasks, including learning algorithm selection, hyperparameter tuning, and model evaluation.\n\nWorking on a machine learning project is diﬀerent from working on a typical software engineering project. Unlike traditional software, where a program’s behavior usually is deterministic, machine learning applications incorporate models whose behavior may naturally degrade over time, or they can start behaving abnormally. Such abnormal behavior of the model might be explained by various reasons, including a fundamental change in the input data or an updated feature extractor that now returns a diﬀerent distribution of values or values of a diﬀerent type. They often say that machine learning systems “fail silently.” A machine learning engineer must be capable of preventing such failures or, when it’s impossible to prevent them entirely, know how to detect and handle them when they happen.\n\n1.7 Machine Learning Project Life Cycle\n\nA machine learning project starts with understanding the business objective. Usually, a business analyst works with the client12 and the data analyst to transform a business problem into an engineering project. The engineering project may or may not have a In this book, we, of course, consider engineering projects that machine learning part. have some machine learning involved.\n\nOnce an engineering project is deﬁned, this is where the scope of the machine learning engineering starts. In the scope of a broader engineering project, machine learning must ﬁrst have a well-deﬁned goal. The goal of machine learning is a speciﬁcation of what a statistical model receives as input, what it generates as output, and the criteria of acceptable (or unacceptable) behavior of the model.\n\nThe goal of machine learning is not necessarily the same as the business objective. The business objective is what the organization wants to achieve. For example, the business objective of Google with Gmail can be to make Gmail the most-used email service in the world. Google might create multiple machine learning engineering projects to achieve that business objective. The goal of one of those machine learning projects can be to distinguish Primary emails from Promotions with accuracy above 90%.\n\n11Many scientiﬁc modules in Python are indeed implemented in fast C/C++; however, data analyst’s own\n\nPython code can still be slow.\n\n12If the machine learning project supports a product developed and sold by the organization, then the\n\nbusiness analyst works with the product owner.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 3138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Figure 4: Machine learning project life cycle.\n\nOverall, a machine learning project life cycle, illustrated in Figure 4, consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\n\nIn Figure 4, the scope of machine learning engineering (and the scope of this book) is limited by the blue zone. The solid arrows show a typical ﬂow of the project stages. The dashed arrows indicate that at some stages, a decision can be made to go back in the process and either collect more data or collect diﬀerent data, and revise features (by decommissioning some of them and engineering new ones).\n\nEvery stage mentioned above will be considered in one of the book’s chapters. But ﬁrst, let’s discuss how to prioritize machine learning projects, deﬁne the project’s goal, and structure a machine learning team. The next chapter is devoted to these three questions.\n\n1.8 Summary\n\nA model-based machine learning algorithm takes a collection of training examples as input and outputs a model. An instance-based machine learning algorithm uses the entire training\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "dataset as a model. The training data is exposed to the machine learning algorithm, while holdout data isn’t.\n\nA supervised learning algorithm builds a model that takes a feature vector and outputs a prediction about that feature vector. An unsupervised learning algorithm builds a model that takes a feature vector as input and transforms it into something useful.\n\nClassiﬁcation is the problem of predicting, for an input example, one of a ﬁnite set of classes. Regression, in turn, is a problem of predicting a numerical target.\n\nData can be used directly or indirectly. Directly-used data is a basis for forming a dataset of examples. Indirectly-used data is used to enrich those examples.\n\nThe data for machine learning must be tidy. A tidy dataset can be seen as a spreadsheet where each row is an example, and each column is one of the properties of an example. In addition to being tidy, most machine learning algorithms require numerical data, as opposed to categorical. Feature engineering is the process of transforming data into a form that machine learning algorithms can use.\n\nA baseline is essential to make sure that the model works better than a simple heuristic.\n\nIn practice, machine learning is implemented as a pipeline that contains chained stages of data transformation, from data partitioning to missing-data imputation, to class imbalance and dimensionality reduction, to model training. The hyperparameters of the entire pipeline are usually optimized; the entire pipeline can be deployed and used for predictions.\n\nParameters of the model are optimized by the learning algorithm based on the training data. The values of hyperparameters cannot be learned by the learning algorithm and are, in turn, tuned by using the validation dataset. The test set is only used to assess the model’s performance and report it to the client or product owner.\n\nA shallow learning algorithm trains a model that makes predictions directly from the input features. A deep learning algorithm trains a layered model, in which each layer generates outputs by taking the outputs of the preceding layer as inputs.\n\nYou should consider using machine learning to solve a business problem when the problem is too complex for coding, the problem is constantly changing, it is a perceptive problem, it is an unstudied phenomenon, the problem has a simple objective, and it is cost-eﬀective.\n\nThere are many situations when machine learning should, probably, not be used: when explainability is needed, when errors are intolerable, when traditional software engineering is a less expensive option, when all inputs and outputs can be enumerated and saved in a database, and when data is hard to get or too expensive.\n\nMachine learning engineering (MLE) is the use of scientiﬁc principles, tools, and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE encompasses all stages from data collection, to model training, to making the model available for use by the product or the consumers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 3106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "A machine learning project life cycle consists of the following stages: 1) goal deﬁnition, 2) data collection and preparation, 3) feature engineering, 4) model training, 5) model evaluation, 6) model deployment, 7) model serving, 8) model monitoring, and 9) model maintenance.\n\nEvery stage will be considered in one of the book’s chapters.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "2 Before the Project Starts\n\nBefore a machine learning project starts, it must be prioritized. Prioritization is inevitable: the team and equipment capacity is limited, while the organization’s backlog of projects could be very long.\n\nTo prioritize a project, one has to estimate its complexity. With machine learning, accurate complexity estimation is rarely possible because of major unknowns, such as whether the required model quality is attainable in practice, how much data is needed, and what, and how many features are necessary.\n\nFurthermore, a machine learning project must have a well-deﬁned goal. Based on the goal of the project, the team could be adequately adjusted and resources provisioned.\n\nIn this chapter, we consider these and related activities that must be taken care of before a machine learning project starts.\n\n2.1 Prioritization of Machine Learning Projects\n\nThe key considerations in the prioritization of a machine learning project, are impact and cost.\n\n2.1.1\n\nImpact of Machine Learning\n\nThe impact of using machine learning in a broader engineering project is high when, 1) machine learning can replace a complex part in your engineering project or 2) there’s a great beneﬁt in getting inexpensive (but probably imperfect) predictions.\n\nFor example, a complex part of an existing system can be rule-based, with many nested rules and exceptions. Building and maintaining such a system can be extremely diﬃcult, time-consuming, and error-prone. It can also be a source of signiﬁcant frustration for software engineers when they are asked to maintain that part of the system. Can the rules be learned instead of programming them? Can an existing system be used to generate labeled data easily? If yes, such a machine learning project would have a high impact and low cost.\n\nInexpensive and imperfect predictions can be valuable, for example, in a system that dispatches a large number of requests. Let’s say many such requests are “easy” and can be solved quickly using some existing automation. The remaining requests are considered “diﬃcult” and must be addressed manually.\n\nA machine learning-based system that recognizes “easy” tasks and dispatches them to the automation will save a lot of time for humans who will only concentrate their eﬀort and time on diﬃcult requests. Even if the dispatcher makes an error in prediction, the diﬃcult request will reach the automation, the automation will fail on it, and the human will eventually receive that request. If the human gets an easy request by mistake, there’s no problem either: that easy request can still be sent to the automation or processed by the human.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 2701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "2.1.2 Cost of Machine Learning\n\nThree factors highly inﬂuence the cost of a machine learning project:\n\nthe diﬃculty of the problem, • the cost of data, and • the need for accuracy.\n\nGetting the right data in the right amount can be very costly, especially if manual labeling is involved. The need for high accuracy can translate into the requirement of getting more data or training a more complex model, such as a unique architecture of a deep neural network or a nontrivial ensembling architecture.\n\nWhen you think about the problem’s diﬃculty, the primary considerations are:\n\nwhether an implemented algorithm or a software library capable of solving the problem is available (if yes, the problem is greatly simpliﬁed),\n\nwhether signiﬁcant computation power is needed to build the model or to run it in the production environment.\n\nThe second driver of the cost is data. The following considerations have to be made:\n\ncan data be generated automatically (if yes, the problem is greatly simpliﬁed), • what is the cost of manual annotation of the data (i.e., assigning labels to unlabeled examples),\n\nhow many examples are needed (usually, that cannot be known in advance, but can be estimated from known published results or the organization’s own experience).\n\nFinally, one of the most inﬂuential cost factors is the desired accuracy of the model. The machine learning project’s cost grows superlinearly with the accuracy requirement, as illus- trated in Figure 1. Low accuracy can also be a source of signiﬁcant loss when the model is deployed in the production environment. The considerations to make:\n\nhow costly is each wrong prediction, and • what is the lowest accuracy level below which the model becomes impractical.\n\n2.2 Estimating Complexity of a Machine Learning Project\n\nThere is no standard complexity estimation method for a machine learning project, other than by comparison with other projects executed by the organization or reported in the literature.\n\n2.2.1 The Unknowns\n\nThere are several major unknowns that are almost impossible to guess with conﬁdence unless you worked on a similar project in the past or read about such a project. The unknowns are:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 2232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "AccuracyCost\n\n80%90%99%99.9%\n\nFigure 1: Superlinear growth of the cost as a function of accuracy requirement.\n\nwhether the required quality is attainable in practice, • how much data you will need to reach the required quality, • what features and how many features are necessary so that the model can learn and generalize suﬃciently,\n\nhow large the model should be (especially relevant for neural networks and ensemble architectures), and\n\nhow long will it take to train one model (in other words, how much time is needed to run one experiment) and how many experiments will be required to reach the desired level of performance.\n\nOne thing you can almost be sure of: if the required level of model accuracy (one of the popular model quality metrics we consider in Section ?? of Chapter 5) is above 99%, you can expect complications related to an insuﬃcient quantity of labeled data. In some problems, even 95% accuracy is considered very hard to reach. (Here we assume, of course, that the data is balanced, that is, there’s no class imbalance. We will discuss class imbalance in Section ?? of the next chapter.)\n\nAnother useful reference is the human performance on the task. This is typically a hard problem if you want your model to perform as well as a human.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "2.2.2 Simplifying the Problem\n\nOne way to make a more educated guess is to simplify the problem and solve a simpler problem ﬁrst. For example, assume that the problem is that of classifying a set of documents into 1000 topics. Run a pilot project by focusing on 10 topics ﬁrst, by considering documents belonging to other 990 topics as “Other.”1 Manually label the data for these 11 classes (10 real topics, plus “Other”). The logic here is that it’s much simpler for a human to keep in mind the deﬁnitions of only 10 topics compared to memorizing the diﬀerence between 1000 topics2.\n\nOnce you have simpliﬁed your problem to 11 classes, solve it, and measure time on every stage. Once you see that the problem for 11 classes is solvable, you can reasonably hope that it will be solvable for 1000 classes as well. Your saved measurements can then be used to estimate the time required to solve the full problem, though you cannot simply multiply this time by 100 to get an accurate estimate. The quantity of data needed to learn to distinguish between more classes usually grows superlinearly with the number of classes.\n\nAn alternative way of obtaining a simpler problem from a potentially complex one is to split the problem into several simple ones by using the natural slices in the available data. For example, let an organization have customers in multiple locations. If we want to train a model that predicts something about the customers, we can try to solve that problem only for one location, or for customers in a speciﬁc age range.\n\n2.2.3 Nonlinear Progress\n\nThe progress of a machine learning project is nonlinear. The prediction error usually decreases fast in the beginning, but then the progress gradually slows down.3 Sometimes you see no progress and decide to add additional features that could potentially depend on external databases or knowledge bases. While you are working on a new feature or labeling more data (or outsourcing this task), no progress in model performance is happening.\n\nBecause of this nonlinearity of progress, you should make sure that the product owner (or the client) understands the constraints and risks. Carefully log every activity and track the time it took. This will help not only in reporting, but also in the estimation of the complexity of similar projects in the future.\n\n1Putting examples belonging to 990 classes in one class will likely create a highly imbalanced dataset. If it’s the case, you would prefer to undersample the data in the class “Other.” We consider data undersampling in Section ?? of the next chapter.\n\n2To save even more time, apply clustering to the whole collection of unlabeled documents and only\n\nmanually label documents belonging to one or a few clusters.\n\n3The 80/20 rule of thumb often applies: 80% of progress is made using the ﬁrst 20% of resources.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "2.3 Deﬁning the Goal of a Machine Learning Project\n\nThe goal of a machine learning project is to build a model that solves, or helps solve, a business problem. Within a project, the model is often seen as a black box described by the structure of its input (or inputs) and output (or outputs), and the minimum acceptable level of performance (as measured by accuracy of prediction or another performance metric).\n\n2.3.1 What a Model Can Do\n\nThe model is typically used as a part of a system that serves some purpose. In particular, the model can be used within a broader system to:\n\nautomate (for example, by taking action on the user’s behalf or by starting or stopping a speciﬁc activity on a server),\n\nalert or prompt (for example, by asking the user if an action should be taken or by asking a system administrator if the traﬃc seems suspicious),\n\norganize, by presenting a set of items in an order that might be useful for a user (for example, by sorting pictures or documents in the order of similarity to a query or according to the user’s preferences),\n\nannotate (for instance, by adding contextual annotations to displayed information, or by highlighting, in a text, phrases relevant to the user’s task),\n\nextract (for example, by detecting smaller pieces of relevant information in a larger input, such as named entities in the text: proper names, companies, or locations), • recommend (for example, by detecting and showing to a user highly relevant items in a large collection based on item’s content or user’s reaction to the past recommendations), • classify (for example, by dispatching input examples into one, or several, of a predeﬁned set of distinctly-named groups),\n\nquantify (for example, by assigning a number, such as a price, to an object, such as a house),\n\nsynthesize (for example, by generating new text, image, sound, or another object similar to the objects in a collection),\n\nanswer an explicit question (for example, “Does this text describe that image?” or “Are these two images similar?”),\n\ntransform its input (for example, by reducing its dimensionality for visualization purposes, paraphrasing a long text as a short abstract, translating a sentence into another language, or augmenting an image by applying a ﬁlter to it),\n\ndetect a novelty or an anomaly.\n\nAlmost any business problem solvable with machine learning can be deﬁned in a form similar to one from the above list. If you cannot deﬁne your business problem in such a form, likely, machine learning is not the best solution in your case.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 2591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "2.3.2 Properties of a Successful Model\n\nA successful model has the following four properties:\n\nit respects the input and output speciﬁcations and the performance requirement, • it beneﬁts the organization (measured via cost reduction, increased sales or proﬁt), • it helps the user (measured via productivity, engagement, and sentiment), • it is scientiﬁcally rigorous.\n\nA scientiﬁcally rigorous model is characterized by a predictable behavior (for the input examples that are similar to the examples that were used for training) and is reproducible. The former property (predictability) means that if input feature vectors come from the same distribution of values as the training data, then the model, on average, has to make the same percentage of errors as observed on the holdout data when the model was trained. The latter property (reproducibility) means that a model with similar properties can be easily built once again from the same training data using the same algorithm and values of hyperparameters. The word “easily” means that no additional analysis, labeling, or coding is necessary to rebuild the model, only the compute power.\n\nWhen deﬁning the goal of machine learning, make sure you solve the right problem. To give an example of an incorrectly deﬁned goal, imagine your client has a cat and a dog and needs a system that lets their cat in the house but keeps their dog out. You might decide to train the model to distinguish cats from dogs. However, this model will also let any cat in and not just their cat. Alternatively, you may decide that because the client only has two animals, you will train a model that distinguishes between those two. In this case, because your classiﬁcation model is binary, a raccoon will be classiﬁed as either the dog or the cat. If it’s classiﬁed as the cat, it will be let in the house.4\n\nDeﬁning a single goal for a machine learning project could be challenging. Usually, within an organization, there will be multiple stakeholders having interest towards your project. An obvious stakeholder is the product owner. Let their objective be to increase the time the user spends on an online platform by at least 15%. At the same time, the executive VP would like to increase the revenue from advertisements by 20%. Furthermore, the ﬁnance team would like to reduce the monthly cloud bill by 10%. When deﬁning the goal of your machine learning project, you should ﬁnd the right balance between those possibly conﬂicting requirements and translate them into the choice of the model’s input and output, cost function, and performance metric.\n\n2.4 Structuring a Machine Learning Team\n\nThere are two cultures of structuring a machine learning team, depending on the organization.\n\n4This is why having the class “Other” your classiﬁcation problems is almost always a good idea.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 2883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "2.4.1 Two Cultures\n\nOne culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers. In such a culture, a software engineer doesn’t need to have deep expertise in machine learning, but has to understand the vocabulary of their fellow data analysts.\n\nAccording to other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\n\nThere are pros and cons in each culture. The proponents of the former say that each team member must be the best in what they do. A data analyst must be an expert in many machine learning techniques and have a deep understanding of the theory to come up with an eﬀective solution to most problems, fast and with minimal eﬀort. Similarly, a software engineer must have a deep understanding of various computing frameworks and be capable of writing eﬃcient and maintainable code.\n\nThe proponents of the latter say that scientists are hard to integrate with software engi- neering teams. Scientists care more about how accurate their solution is and often come up with solutions that are impractical and cannot be eﬀectively executed in the production environment. Also, because scientists don’t usually write eﬃcient, well-structured code, the latter has to be rewritten into production code by a software engineer; depending on the project, that can turn out to be a daunting task.\n\n2.4.2 Members of a Machine Learning Team\n\nBesides machine learning and software engineering skills, a machine learning team may include experts in data engineering (also known as data engineers) and experts in data labeling.\n\nData engineers are software engineers responsible for ETL (for Extract, Transform, Load). These three conceptual steps are part of a typical data pipeline. Data engineers use ETL techniques and create an automated pipeline, in which raw data is transformed into analysis- ready data. Data engineers design how to structure the data and how to integrate it from various resources. They write on-demand queries on that data, or wrap the most frequent queries into fast application programming interfaces (APIs) to make sure that the data is easily accessible by analysts and other data consumers. Typically, data engineers are not expected to know any machine learning.\n\nIn most big companies, data engineers work separately from machine learning engineers in a data engineering team.\n\nExperts in data labeling are responsible for four activities:\n\nmanually or semi-automatically assign labels to unlabeled examples according to the speciﬁcation provided by data analysts,\n\nbuild labeling tools,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 2720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "manage outsourced labelers, and • validate labeled examples for quality.\n\nA labeler is person responsible for assigning labels to unlabeled examples. Again, in big companies, data labeling experts may be organized in two or three diﬀerent teams: one or two teams of labelers (for example, one local and one outsourced) and a team of software engineers, plus a user experience (UX) specialist, responsible for building labeling tools.\n\nWhen possible, invite domain experts to work closely with scientists and engineers. Employ domain experts in your decision making about the inputs, outputs, and features of your model. Ask them what they think your model should predict. Just the fact that the data you can get access to can allow you to predict some quantity doesn’t mean the model will be useful for the business.\n\nDiscuss with the domain experts what they look for in the data to make a speciﬁc business decision; that will help you with feature engineering. Discuss also what clients pay for and what is a deal-breaker for them; that will help you to translate a business problem into a machine learning problem.\n\nFinally, there are DevOps engineers. They work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model mainte- nance. In smaller companies and startups, a DevOps engineer may be part of the machine learning team, or a machine learning engineer could be responsible for the DevOps activities. In big companies, DevOps engineers employed in machine learning projects usually work in a larger DevOps team. Some companies introduced the MLOps role, whose responsibility is to deploy machine learning models in production, upgrade those models, and build data processing pipelines involving machine learning models.\n\n2.5 Why Machine Learning Projects Fail\n\nAccording to various estimates made between 2017 and 2020, from 74% to 87% of machine learning and advanced analytics projects fail or don’t reach production. The reasons for a failure range from organizational to engineering. In this section, we consider the most impactful of them.\n\n2.5.1 Lack of Experienced Talent\n\nAs of 2020, both data science and machine learning engineering are relatively new disciplines. There’s still no standard way to teach them. Most organizations don’t know how to hire experts in machine learning and how to compare them. Most of the available talent on the market are people who completed one or several online courses and who don’t possess signiﬁcant practical experience. A signiﬁcant fraction of the workforce has superﬁcial expertise in machine learning obtained on toy datasets in a classroom context. Many don’t have experience with the entire machine learning project life cycle. On the other hand, experienced software engineers that\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 2869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "might exist in an organization don’t have expertise in handling data and machine learning models appropriately.\n\n2.5.2 Lack of Support by the Leadership\n\nAs discussed in the previous section on the two cultures, scientists and software engineers often have diﬀerent goals, motivations, and success criteria. They also work very diﬀerently. In a typical Agile organization, software engineering teams work in sprints with clearly deﬁned expected deliverables and little uncertainty.\n\nScientists, on the other hand, work in high uncertainty and move ahead with multiple experiments. Most of such experiments don’t result in any deliverable and, thus, can be seen by inexperienced leaders as no progress. Sometimes, after the model is built and deployed, the entire process has to start over because the model doesn’t result in the expected increase of the metric the business cares about. Again, this can lead to the perception of the scientist’s work by the leadership as wasted time and resources.\n\nFurthermore, in many organizations, leaders responsible for data science and artiﬁcial intelligence (AI), especially at the vice-president level, have a non-scientiﬁc or even non- engineering background. They don’t know how AI works, or have a very superﬁcial or overly optimistic understanding of it drawn from popular sources. They might have such a mindset that with enough resources, technical and human, AI can solve any problem in a short amount of time. When fast progress doesn’t happen, they easily blame scientists or entirely lose interest in AI as an ineﬀective tool with hard-to-predict and uncertain results.\n\nOften, the problem lies in the inability of scientists to communicate the results and challenges to upper management. Because they don’t share the vocabulary and have very diﬀerent levels of technical expertise, even a success presented badly can be seen as a failure.\n\nThis is why, in successful organizations, data scientists are good popularizers, while top-level managers, responsible for AI and analytics, often have a technical or scientiﬁc background.\n\n2.5.3 Missing Data Infrastructure\n\nData analysts and scientists work with data. The quality of the data is crucial for a machine learning project’s success. Enterprise data infrastructure must provide the analyst with simple ways to get quality data for training models. At the same time, the infrastructure must make sure that similar quality data will be available once the model is deployed in production.\n\nHowever, in practice, this is often not the case. Scientists obtain the data for training by using various ad-hoc scripts; they also use diﬀerent scripts and tools to combine various data sources. Once the model is ready, it turns out that it’s impossible, by using the available production infrastructure, to generate input examples for the model fast enough (or at all). We extensively talk about storing data and features in Chapters 3 and 4.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 2995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "2.5.4 Data Labeling Challenge\n\nIn most machine learning projects, analysts use labeled data. This data is usually custom, so labeling is executed speciﬁcally for each project. As of 2019, according to some reports,5 as many as 76% AI and data science teams label training data on their own, while 63% build their own labeling and annotation automation technology.\n\nThis results in a signiﬁcant time spent by skilled data scientists on data labeling and labeling tool development. This is a major challenge for the eﬀective execution of an AI project.\n\nSome companies outsource data labeling to third-party vendors. However, without proper quality validation, such labeled data can turn out to be of low quality or entirely wrong. Organizations, in order to maintain quality and consistency across datasets, have to invest in formal and standardized training of internal or third-party labelers. This, in turn, can slow down machine learning projects. Though, according to the same reports, companies that outsource data labeling are more likely to get their machine learning projects up to production.\n\n2.5.5 Siloed Organizations and Lack of Collaboration\n\nData needed for a machine learning project often resides within an organization in diﬀerent places with diﬀerent ownership, security constraints, and in diﬀerent formats. In siloed organizations, people responsible for diﬀerent data assets might not know one another. Lack of trust and collaboration results in friction when one department needs access to the data stored in a diﬀerent department. Furthermore, diﬀerent branches of one organization often have their own budgets, so collaboration becomes complicated because no side has an interest in spending their budget helping to the other side.\n\nEven within one branch of an organization, there are often several teams involved in a machine learning project at diﬀerent stages. For example, the data engineering team provides access to the data or individual features, the data science team works on modeling, ETL or DevOps work on the engineering aspects of deployment and monitoring, while the automation and internal tools teams develop tools and processes for a continuous model update. Lack of collaboration between any pair of the involved teams might result in the project being frozen for a long time. Typical reasons for mistrust between teams is the lack of understanding by the engineers of the tools and approaches used by the scientists and the lack of knowledge (or plain ignorance) by the scientists of software engineering good practices and design patterns.\n\n2.5.6 Technically Infeasible Projects\n\nBecause of the high cost of many machine learning projects (due to high expertise and infrastructure cost) some organizations, to “recoup the investment,” might target very ambitious goals: to completely transform the organization or the product or provide unrealistic\n\n5Alegion and Dimensional Research, “What data scientists tell us about AI model training today,” 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 3056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "return or investment. This results in very large-scale projects, involving collaboration between multiple teams, departments, and third-parties, and pushing those teams to their limits.\n\nAs a result, such overly ambitious projects could take months or even years to complete; some key players, including leaders and key scientists, might lose interest in the project or even leave the organization. The project could eventually be deprioritized, or, even when completed, be too late to the market. It is best, at least in the beginning, to focus on achievable projects, involving simple collaboration between teams, easy to scope, and targeting a simple business objective.\n\n2.5.7 Lack of Alignment Between Technical and Business Teams\n\nMany machine learning projects start without a clear understanding, by the technical team, of the business objective. Scientists usually frame the problem as classiﬁcation or regression with a technical objective, such as high accuracy or low mean squared error. Without continuous feedback from the business team on the achievement of a business objective (such as an increased click-through rate or user retention), the scientists often reach an initial level of model performance (according to the technical objective), and then they are not sure if they are making any useful progress and whether an additional eﬀort is worth it. In such situations, the projects end up being put on the shelf because time and resources were spent but the business team didn’t accept the result.\n\n2.6 Summary\n\nBefore a machine learning project starts, it must be prioritized, and the team working on the project must be built. The key considerations in the prioritization of a machine learning project, are impact and cost.\n\nThe impact of using machine learning is high when, 1) machine learning can replace a complex part in your engineering project, or 2) there’s a great beneﬁt in getting inexpensive (but probably imperfect) predictions.\n\nThe cost of the machine learning project is highly inﬂuenced by three factors: 1) the diﬃculty of the problem, 2) the cost of data, and 3) the needed model performance quality.\n\nThere is no standard method of estimation of how complex a machine learning project is other than by comparison with other projects executed by the organization or reported in the literature. There are several major unknowns that are almost impossible to guess: whether the required level of model performance is attainable in practice, how much data you will need to reach that level of performance, what features and how many features are needed, how large the model should be, and how long will it take to run one experiment and how many experiments will be needed to reach the desired level of performance.\n\nOne way to make a more educated guess is to simplify the problem and solve a simpler one.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "The progress of a machine learning project is nonlinear. The error usually decreases fast in the beginning, but then the progress slows down. Because of this nonlinearity of progress, it’s better to make sure that the client understands the constraints and the risks. Carefully log every activity and track the time it took. This will help not only in reporting but also in estimating complexity for similar projects in the future.\n\nThe goal of a machine learning project is to build a model that solves some business problem. In particular, the model can be used within a broader system to automate, alert or prompt, organize, annotate, extract, recommend, classify, quantify, synthesize, answer an explicit question, transform its input, and detect novelty or an anomaly. If you cannot frame the goal of machine learning in one of these forms, likely, machine learning is not the best solution.\n\nA successful model 1) respects the input and output speciﬁcations and the minimum perfor- mance requirement, 2) beneﬁts the organization and the user, and 3) is scientiﬁcally rigorous.\n\nThere are two cultures of structuring a machine learning team, depending on the organization. One culture says that a machine learning team has to be composed of data analysts who collaborate closely with software engineers. In such a culture, a software engineer doesn’t need to have profound expertise in machine learning but has to understand the vocabulary of their fellow data analysts or scientists. According to the other culture, all engineers in a machine learning team must have a combination of machine learning and software engineering skills.\n\nBesides having machine learning and software engineering skills, a machine learning team may include experts in data labeling and data engineering experts. DevOps engineers work closely with machine learning engineers to automate model deployment, loading, monitoring, and occasional or regular model maintenance.\n\nA machine learning projects can fail for many reasons, and most actually do. Typical reasons for a failure are:\n\nlack of experienced talent, • lack of support by the leadership, • missing data infrastructure, • data labeling challenge, • siloed organizations and lack of collaboration, • technically infeasible projects, and • lack of alignment between technical and business teams.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 2395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "3 Data Collection and Preparation\n\nBefore any machine learning activity can start, the analyst must collect and prepare the data. The data available to the analyst is not always “right” and is not always in a form that a machine learning algorithm can use. This chapter focuses on the second stage in the machine learning project life cycle, as shown below:\n\nFigure 1: Machine learning project life cycle.\n\nIn particular, we talk about the properties of good quality data, typical problems a dataset can have, and ways to prepare and store data for machine learning.\n\n3.1 Questions About the Data\n\nNow that you have a machine learning goal with well-deﬁned model input, output, and success criteria, you can start collecting the data needed to train your model. However, before you start collecting the data, there are some questions to answer.\n\n3.1.1\n\nIs the Data Accessible?\n\nDoes the data you need already exist? If yes, is it accessible (physically, contractually, ethically, or from a cost perspective)? If you are purchasing or re-using someone else’s data sources, have you considered how that data might be used or shared? Do you need to negotiate a new licensing agreement with the original supplier?\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "If the data is accessible, is it protected by copyright or other legal norms? If so, have you established who owns the copyright in your data? Might there be joint copyright?\n\nIs the data sensitive (e.g., concerning your organization’s projects, clients, or partners, or it is classiﬁed by the government), and are there any potential privacy issues? If so, have you discussed data sharing with the respondents from whom you collected the data? Can you preserve personal information for a long-term so that it can be used in the future?\n\nDo you need to share the data along with the model? If so, do you need to get written consent from owners or respondents?\n\nDo you need to anonymize data,1 for example, to remove personally identiﬁable infor- mation (PII), during analysis or in preparation for sharing?\n\nEven if it’s physically possible to get the data you need, don’t work with it until all the above questions are resolved.\n\n3.1.2\n\nIs the Data Sizeable?\n\nThe question for which you would like to have a deﬁnitive answer is whether there’s enough data. However, as we already found out, it’s usually not known how much data is needed to reach your goal, especially if the minimum model quality requirement is stringent.\n\nIf you have doubts about the immediate availability of suﬃcient data, ﬁnd out how frequently new data gets generated. For some projects, you can start with what’s initially available and, while you are working on feature engineering, modeling, and solving other relevant technical problems, new data might gradually come in. It can come in naturally, as the result of some observable or measurable process, or progressively be provided by your data labeling experts or a third-party data provider.\n\nConsider the estimated time needed to accomplish the project. Will a suﬃciently large dataset2 be gathered during this time? Base your answer on the experience working on similar projects or results reported in the literature.\n\nOne practical way to ﬁnd out if you have collected suﬃcient data is to plot learning curves. More speciﬁcally, plot the training and validation scores of your learning algorithm for varying numbers of training examples, as shown in Figure 2.\n\nBy looking at the learning curves, you will see your model’s performance will plateau after you reach a certain number of training examples. Upon reaching that number of training\n\n1An illustrative example is the content redistribution policy of Twitter. The policy restricts the sharing of tweet information other than tweet IDs and user IDs. Twitter wants the analysts always to pull fresh data using Twitter API. One possible explanation of such a restriction is that some users might want to delete a particular tweet because they changed their mind or found it too controversial. If that tweet has already been pulled and is shared on a public domain, then it might make that user vulnerable.\n\n2Don’t forget, in your estimates, that you need not just training but also holdout data to validate the model performance on the examples it wasn’t trained on. That holdout data also has to be sizeable to provide reliable, in a statistical sense, model quality estimates.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 3221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Figure 2: Learning curves for the Naïve Bayes learning algorithm applied to the standard \"digits\" dataset of scikit-learn.\n\nexamples, you will begin to experience diminishing returns from additional examples.\n\nIf you observe the performance of the learning algorithm plateaued, it might be a sign that collecting more data will not help in training a better model. I used the expression “might be” because two other explanations are possible:\n\nyou didn’t have enough informative features that your learning algorithm can leverage to build a more performant model, or\n\nyou used a learning algorithm incapable of training a complex enough model using the data you have.\n\nIn the former case, you might think about engineering additional features by combining the existing features in some clever ways, or by using information from indirect data sources, such as lookup tables and gazetteers. We consider techniques for synthesizing features in Section ?? of Chapter 4.\n\nIn the latter case, one possible approach would be to use an ensemble learning method or train a deep neural network. However, deep neural networks usually require more training data compared to shallow learning algorithms.\n\nSome practitioners use rules of thumb to estimate the number of training examples needed for a problem. Usually, they are using scaling factors applied to either,\n\nnumber of features, or • number of classes, or\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 1458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "number of trainable parameters in the model.\n\nSuch rules of thumb often work, but they are diﬀerent for diﬀerent problem domains. Each analyst adjusts the numbers based on experience. While you would discover by experience those “magical” scaling factors that work for you, the most frequently cited numbers in various online sources are:\n\n10 times the amount of features (this often exaggerates the size of the training set, but works well as an upper bound),\n\n100 or 1000 times the number of classes (this often underestimates the size), or • ten times the number of trainable parameters (usually applied to neural networks).\n\nKeep in mind that just because you have big data does not mean that you should use all of it. A smaller sample of big data can give good results in practice and accelerate the search for a better model. It’s important to ensure, though, that the sample is representative of the whole big dataset. Sampling strategies such as stratiﬁed and systematic sampling can lead to better results. We consider data sampling strategies in Section 3.10.\n\n3.1.3\n\nIs the Data Useable?\n\nThe data quality is one of the major factors aﬀecting the model performance. Imagine that you want to train a model that predicts a person’s gender, given their name. You might acquire a dataset of people that contains gender information. However, if you use this dataset blindly, you might realize that no matter how hard you try to improve the quality of your model, its performance on new data is low. What is the reason for such a weak performance?\n\nThe answer could be that the gender information was not factual but obtained using a rather low-quality statistical classiﬁer. In this case, the best you can achieve with your model is the performance of that low-quality classiﬁer.\n\nIf the dataset comes in the form of a spreadsheet, the ﬁrst thing to check is if the data in the spreadsheet is tidy. As discussed in the introduction, the dataset used for machine learning must be tidy. If it’s not the case for your data, you must transform it into tidy data using, as already mentioned, feature engineering.\n\nA tidy dataset can have missing values. Consider data imputation techniques to ﬁll the missing values. We will discuss several such techniques in Section 3.7.\n\nOne frequent problem with datasets compiled by a human is that people can decide to indicate missing values with some magic number like 9999 or −1. Such situations must be spotted during the visual analysis of the data, and those magic numbers have to be replaced using an appropriate data imputation technique.\n\nAnother property to validate is whether the dataset contains duplicates. Usually, duplicates are removed, unless you added them on purpose to balance an imbalanced problem. We consider this problem and methods to alleviate it in Section 3.9.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2885,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Data can be expired or be signiﬁcantly not up to date. For example, let your goal be to train a model that recognizes abnormality in the behavior of a complex piece of electronic appliance, such as a printer. You have measurements taken during the normal and abnormal functioning of a printer. However, these measurements have been recorded for a previous generation of printers, while the new generation has received several signiﬁcant upgrades since then. The model trained using such expired data from an older printer generation might perform worse when deployed on the new generation of printers.\n\nFinally, data can be incomplete or unrepresentative of the phenomenon. For example, a dataset of animal photos might contain pictures taken only during the summer or in a speciﬁc geography. A dataset of pedestrians for self-driving car systems might be created with engineers posing as pedestrians; in such a dataset, most situations would include only younger men, while children, women, and the elderly would be underrepresented or entirely absent.\n\nA company working on facial expression recognition might have the research and development oﬃce in a predominantly white location, so the dataset would only show faces of white men and women, while black or Asian people would be underrepresented. Engineers developing a posture recognition model for a camera might build the training dataset by taking pictures of people indoors, while the customers would typically use the camera outdoors.\n\nIn practice, data can only become useable for modeling after preprocessing; hence the importance of visual analysis of the dataset before you start modeling. Let’s say you work on a problem of predicting the topic in news articles. It’s likely you will scrape your data from news websites. It’s also likely that download dates would be saved in the same document as the news article text. Imagine also that the data engineer decides to loop over news topics mentioned on the websites and scrape one topic at a time. So, on Monday the arts-related articles were scraped, on Tuesday — sports, on Wednesday — technology, and so on.\n\nIf you don’t preprocess such data by removing the dates, the model can learn the date-topic correlation, and such a model will be of no practical use.\n\n3.1.4\n\nIs the Data Understandable?\n\nAs demonstrated in gender prediction, it’s crucial to understand from where each attribute in the dataset came. It is equally important to understand what each attribute exactly represents. One frequent problem observed in practice is when the variable that the analyst tries to predict is found among the features in the feature vector. How could that happen?\n\nImagine that you work on the problem of predicting the price of a house from its attributes such as the number of bedrooms, surface, location, year of construction, and so on. The attributes of each house were provided to you by the client, a large online real estate sales platform. The data has the form of an Excel spreadsheet. Without spending too much time analyzing each column, you remove only the transaction price from the attributes and use that value as the target you want to learn to predict. Very quickly you realize that the model is almost perfect: it predicts the transaction price with accuracy near 100%. You deliver the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 3372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "model to the client, they deploy it in production, and the tests show that the model is wrong most of the time. What happened?\n\nWhat did happen is called data leakage (also known as target leakage). After a more careful examination of the dataset, you realize that one of the columns in the spreadsheet contained the real estate agent’s commission. Of course, the model easily learned to convert this attribute into the house price perfectly. However, this information is not available in the production environment before the house is sold, because the commission depends on the selling price. In Section 3.2.8, we will consider the problem of data leakage in more detail.\n\n3.1.5\n\nIs the Data Reliable?\n\nThe reliability of a dataset varies depending on the procedure used to gather that dataset. Can you trust the labels? If the data was produced by the workers on Mechanical Turk (so-called “turkers”), then the reliability of such data might be very low. In some cases, the labels assigned to feature vectors might be obtained as a majority vote (or an average) of several turkers. If that’s the case, the data can be considered more reliable. However, it’s better to do additional validation of quality on a small random sample of the dataset.\n\nOn the other hand, if the data represents measurements made by some measuring devices, you can ﬁnd the details of each measurement’s accuracy in the technical documentation of the corresponding measuring device.\n\nThe reliability of labels can also be aﬀected by the delayed or indirect nature of the label. The label is considered delayed when the feature vector to which the label was assigned represents something that happened signiﬁcantly earlier than the time of label observation.\n\nTo be more concrete, take the churn prediction problem. Here, we have a feature vector describing a customer, and we want to predict whether the customer will leave at some point in the future (typically six months to one year from now). The feature vector represents what we know about the customer now, but the label (“left” or “stayed”) will be assigned in the future. This is an important property, because between now and the future, many events, not reﬂected in our feature vector might happen which would aﬀect the customer’s decision to stay or leave. Therefore delayed labels make our data less reliable.\n\nWhether a label is direct or indirect also aﬀects reliability, depending, of course, on what we are trying to predict. For example, let’s say our goal is to predict whether the website visitor will be interested in a webpage. We might acquire a certain dataset containing information about users, webpages, and labels “interested”/“not_interested” reﬂecting whether a speciﬁc user was interested in a particular webpage. A direct label would indeed indicate interest, while an indirect label could suggest some interest. For example, if the user pressed the “Like” button, we have the direct indicator of interest. However, if the user only clicked on the link, this could be an indicator of some interest, but it’s an indirect indicator. The user could have clicked by mistake, or because the link text was a clickbait, we cannot know for sure. If\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 3256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Original photo\n\nLabeled photo\n\nFigure 3: The unlabeled and labeled aerial photo. Photo credit: Tom Fisk.\n\nthe label is indirect, this also makes such data less reliable. Of course, it’s less reliable for predicting the interest, but can be perfectly reliable for predicting clicks.\n\nAnother source of unreliability in the data is feedback loops. A feedback loop is a property in the system design when the data used to train the model is obtained using the model itself. Again, imagine that you work on a problem of predicting whether a speciﬁc user of a website will like the content, and you only have indirect labels – clicks. If the model is already deployed on the website and the users click on links recommended by the model, this means that the new data indirectly reﬂects not only the interest of users to the content, but also how intensively the model recommended that content. If the model decided that a speciﬁc link is important enough to recommend to many users, more users would likely click on that link, especially if the recommendation was made repeatedly during several days or weeks.\n\n3.2 Common Problems With Data\n\nAs we have just seen, the data you will work with can have problems. In this section, we cite the most important of these problems and what you can do to alleviate them.\n\n3.2.1 High Cost\n\nGetting unlabeled data can be expensive; however, labeling data is the most expensive work, especially if the work is done manually.\n\nGetting unlabeled data becomes expensive when it nust be gathered speciﬁcally for your problem. Let’s say your goal is to know where diﬀerent types of commerce are located in a city. The best solution would be to buy this data from a government agency. However,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 1776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "for various reasons it can be complicated or even impossible: the government database may be incomplete or outdated. To get up-to-date data, you may decide to send cars equipped with cameras on the streets of a given city. They would take pictures of all buildings on the streets.\n\nAs you might imagine, such an enterprise is not cheap. Collecting pictures of the buildings is not enough. We need the type of commerce in every building. Now we need labeled data: “coﬀee house,” “bank,” “grocery,” “drug store,” “gas station,” etc. These must be assigned manually, and paying someone to do that work is expensive. By the way, Google has a clever technique outsourcing the labeling to random people with its free reCAPTCHA service. reCAPTCHA thus solves two problems: reducing spam on the Web and providing cheap labeled data to Google.\n\nIn Figure 3, you can see the work needed to label one image. The goal here is to segment a picture by assigning labels to every pixel from the following: “heavy truck,” “car or light truck,” “boat,” “building,” “container,” “other.” Labeling image in Figure 3 took me about 30 minutes. If there were more types, for example “motorcycle,” “tree,” “road,” it would take longer, and the labeling cost would be higher.\n\nWell-designed labeling tools will minimize mouse use (including menus activated by mouse clicks), maximize hotkeys, and reduce costs by increasing the speed of data labeling.\n\nWhenever possible, reduce decision-making to a yes/no answer. Instead of asking “Find all prices in this text”, extract all numbers from the text and then display each number, one by one, asking, “Is this number a price?” as shown in 4. If the labeler clicks “Not Sure,” you can save this example to analyze later or simply not use such examples for training the model.\n\nAnother trick allowing for accelerated labeling is noisy pre-labeling consisting of pre- labeling the example using the current best model. In this scenario, you start by labeling a certain quantity of examples “from scratch” (that is, without using any support). Then you build the ﬁrst model that works reasonably well, using this initial set of labeled examples. Next, use the current model and label each new example in place of the human labeler.3 Ask whether the automatically assigned label is correct. If the labeler clicks “Yes,” save this example as usual. If they click “No,” then ask to label this example manually. See the workﬂow chart illustrating this process in Figure 5. The goal of a good labeling process design is to make the labeling as streamlined as possible. Keeping the labeler engaged is also key. Show progress in the number of labels added, as well as the quality of the current best model. This engages the labeler and adds purpose to the labeling task.\n\n3This is why it’s called a “noisy” pre-labeling: the labels assigned to examples using a sub-optimal model\n\nwould not all be accurate and require a human validation.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 3006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Figure 4: An example of simple labeling interface.\n\nFigure 5: An example of noisy pre-labeling workﬂow.\n\n3.2.2 Bad Quality\n\nRemember that data quality is one of the major factors aﬀecting the performance of the model. I cannot stress it strongly enough.\n\nData quality has two components: raw data quality and labeling quality.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Some common problems with raw data are noise, bias, low predictive power, outdated examples, outliers, and leakage.\n\n3.2.3 Noise\n\nNoise in data is a corruption of examples. Images can be blurry or incomplete. Text can lose formatting, which makes some words concatenated or split. Audio data can have noise in the background. Poll answers can be incomplete or have missing attributes, such as the responder’s age or gender. Noise is often a random process that corrupts each example independently of other examples in the collection.\n\nIf tidy data has missing attributes, data imputation techniques can help in guessing values for those attributes. We will consider data imputation techniques in Section 3.7.1.\n\nBlurred images can be deblurred using speciﬁc image deblurring algorithms, though deep machine learning models, such as neural networks, can learn to deblur if needed. The same can be said about noise in audio data: it can be algorithmically suppressed.\n\nNoise is more a problem when the dataset is relatively small (thousands of examples or less), because the presence of noise can lead to overﬁtting: the algorithm may learn to model the noise contained in the training data, which is undesirable. In the big data context, on the other hand, noise, if it’s randomly applied to each example independently of other examples in the dataset, is typically “averaged out” over multiple examples. In that latter context, noise can bring a regularization eﬀect as it prevents the learning algorithm from relying too much on a small subset of input features.4\n\n3.2.4 Bias\n\nBias in data is an inconsistency with the phenomenon that data represents. This inconsistency may occur for a number of reasons (which are not mutually exclusive).\n\nTypes of Bias\n\nSelection bias is the tendency to skew your choice of data sources to those that are easily available, convenient, and/or cost-eﬀective. For example, you might want to know the opinion of the readers on your new book. You decide to send several initial chapters to the mailing list of your previous book’s readers. It’s very likely this select group will like your new book. However, this information doesn’t tell you much about a general reader.\n\nA real-life example of selection bias is an image generated by the Photo Upsampling via Latent Space Exploration (PULSE) algorithm that uses a neural network model to upscale (increase the resolution) of images. When Internet users tested it, they discovered that an\n\n4This is, by the way, the rationale behind the increase in performance brought by the dropout regularization\n\ntechnique in deep learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 2668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "upscaled image of a black person could, in some cases, represent a white person, as illustrated by Barack Obama’s upscaled photo shown in Figure 6.\n\nFigure 6: An illustration of the eﬀect the selection bias can have on the trained model. Image: Twitter / @Chicken3gg.\n\nThe above example shows that it cannot be assumed that machine learning model is correct, simply because machine learning algorithms are impartial and the trained models are based on data. If the data has a bias, it will most likely be reﬂected in the model.\n\nSelf-selection bias is a form of selection bias where you get the data from sources that “volunteered” to provide it. Most poll data has this type of bias. For example, you want to train a model that predicts the behavior of successful entrepreneurs. You decide to ﬁrst ask entrepreneurs whether they are successful or not. Then you only keep the data obtained from those who declared themselves successful. The problem here is that most likely, really successful entrepreneurs don’t have time to answer your questions, while those who claim themselves successful can be wrong on that matter.\n\nHere’s another example. Let’s say, you want to train a model that predicts whether a book will be liked by the readers. You can use the rating users gave to similar books in the past. However, it turns out that unhappy users tend to provide disproportionally low ratings. The data will be biased towards too many very low ratings as compared to the quantity of mid-range ratings, as shown in Figure 7. The bias is compounded by the fact that we tend to rate only when the experience was either very good or very bad.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 1696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Figure 7: The distribution of ratings given by the readers to a popular AI book on Amazon.\n\nOmitted variable bias happens when your featurized data doesn’t have a feature necessary for accurate prediction. For example, let’s assume that you are working on a churn prediction model and you want to predict whether a customer cancels their subscription within six months. You train a model, and it’s accurate enough; however, several weeks after deployment you see many unexpected false negatives. You investigate the decreased model performance and discover a new competitor now oﬀers a very similar service for a lower price. This feature wasn’t initially available to your model, therefore important information for accurate prediction was missing.\n\nSponsorship or funding bias aﬀects the data produced by a sponsored agency. For example, let a famous video game company sponsor a news agency to provide news about the video game industry. If you try to make a prediction about the video game industry, you might include in your data the story produced by this sponsored agency.\n\nHowever, sponsored news agencies tend to suppress bad news about their sponsor and exaggerate their achievements. As a result, the model’s performance will be suboptimal.\n\nSampling bias (also known as distribution shift) occurs when the distribution of examples used for training doesn’t reﬂect the distribution of the inputs the model will receive in production. This type of bias is frequently observed in practice. For example, you are working on a system that classiﬁes documents according to a taxonomy of several hundred topics. You might decide to create a collection of documents in which an equal amount of documents represents each topic. Once you ﬁnish the work on the model, you observe 5% error. Soon after deployment, you see the wrong assignment to about 30% of documents. Why did this happen?\n\nOne of the possible reasons is sampling bias: one or two frequent topics in production data might account for 80% of all input. If your model doesn’t perform well for these frequent topics, then your system will make more errors in production than you initially expected.\n\nPrejudice or stereotype bias is often observed in data obtained from historical sources,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 2309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "such as books or photo archives, or from online activity such as social media, online forums, and comments to online publications.\n\nUsing a photo archive to train a model that distinguishes men from women might show, for example, men more frequently in work or outdoor contexts, and women more often at home indoors. If we use such biased data, our model will have more diﬃculty recognizing a woman outdoors or a man at home.\n\nA famous example of this type of bias is looking for associations for words using word embeddings trained with an algorithm like word2vec. The model predicts that king − man+woman ≈ queen, but at the same time, that programmer−man+woman ≈ homemaker.\n\nSystematic value distortion is bias usually occurring with the device making measurements or observations. This results in a machine learning model making suboptimal predictions when deployed in the production environment.\n\nFor example, the training data is gathered using a camera with a white balance which makes white look yellowish. In production, however, engineers decide to use a higher-quality camera which “sees” white as white. Because your model was trained on lower-quality pictures, the predictions using higher-quality input will be suboptimal.\n\nThis should not be confused with noisy data. Noise is the result of a random process that distorts the data. When you have a suﬃciently large dataset, noise becomes less of a problem because it might average out. On the other hand, if the measurements are consistently skewed in one direction, then it damages training data, and ultimately results in a poor-quality model.\n\nExperimenter bias is the tendency to search for, interpret, favor, or recall information in a way that aﬃrms one’s prior beliefs or hypotheses. Applied to machine learning, experimenter bias often occurs when each example in the dataset is obtained from the answers to a survey given by a particular person, one example per person.\n\nUsually, each survey contains multiple questions. The form of those questions can signiﬁcantly aﬀect the responses. The simplest way for a question to aﬀect the response is to provide limited response options: “Which kind of pizza do you like: pepperoni, all meats, or vegetarian?” This doesn’t leave the choice of giving a diﬀerent answer, or even “Other.”\n\nAlternatively, a survey question might be constructed with a built-in slant. Instead of asking, “Do you recycle?” an analyst with a experimenter bias might ask, “Do you dodge from recycling?” In the former case, the respondent is more likely to give an honest answer, compared to the latter.\n\nFurthermore, experimenter bias might happen when the analyst is briefed in advance to support a particular conclusion (for example, the one in favor of doing “business as usual”). In that situation, they can exclude speciﬁc variables from the analysis as unreliable or noisy.\n\nLabeling bias happens when labels are assigned to unlabeled examples by a biased process or person. For example, if you ask several labelers to assign a topic to a document by reading the document, some labelers can indeed read the document entirely and assign well-thought\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 3204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "labels. In contrast, others could just try to quickly “scan” the text, spot some keyphrases and choose the topic that corresponds the best to the selected keyphrases. Because each person’s brain pays more attention to keyphrases from a speciﬁc domain or domains and less to others, the labels assigned by labelers who scan the text without reading will be biased.\n\nAlternatively, some labelers would be more interested in reading documents on some topics that they personally prefer. If it’s the case, a labeler might skip uninteresting documents, and the latter will be underrepresented in your data.\n\nWays to Avoid Bias\n\nIt is usually impossible to know exactly what biases are present in a dataset. Furthermore, even knowing there are biases, avoiding them is a challenging task. First of all, be prepared.\n\nA good habit is to question everything: who created the data, what were their motivations and quality criteria, and more importantly, how and why the data was created. If the data is a result of some research, question the research method and make sure that it doesn’t contribute to any of the biases described above.\n\nSelection bias can be avoided by systematically questioning the reason why a speciﬁc data source was chosen. If the reason is simplicity or low cost, then pay careful attention. Recall the example whether a speciﬁc customer would subscribe to your new oﬀering. Training the model using only the data about your current customers is likely a bad idea, because your existing customers are more loyal to your brand than a random potential customer. Your estimates of model’s quality will be overly optimistic.\n\nSelf-selection bias cannot be completely eliminated. It usually appears in surveys; the mere consent of the responder to answer the questions represents self-selection bias. The longer the survey, the less likely the respondent will answer with a high degree of attention. Therefore, keep your survey short and provide an incentive to give quality answers.\n\nPre-select responders to reduce self-selection. Don’t ask entrepreneurs whether they consider themselves successful. Rather, build a list based on references from experts or publications, and only contact those individuals.\n\nIt’s tough to avoid the omitted variable bias completely, because, as they say, “we don’t know what we don’t know.” One approach is to use all available information, that is, to include in your feature vector as many features as possible, even those you deem unnecessary. This could make your feature vector very wide (i.e., of many dimensions) and sparse (i.e., when the values in most dimensions are zero). Still, if you use a well-tuned regularization, your model will “decide” which features are important, and which ones aren’t.\n\nAlternatively, let us suspect that a particular variable would be important for accurate predictions, and leaving it out of our model could result in an omitted variable bias. Suppose getting that data is problematic. Try using a proxy variable in lieu of the omitted variable. For instance, if we want to train a model that predicts the price of a used car, and we cannot get the car’s age, use, instead, the length of ownership by its current owner. The amount of time the current owner owned the car can be taken as a proxy for the age of the vehicle.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 3367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Sponsorship bias can be reduced by carefully investigating the data source, speciﬁcally the source owner’s incentive to provide the data. For example, it’s known that publications on tobacco and pharmaceutical drugs are very often sponsored by tobacco and pharmaceutical companies, or their opponents. The same can be said about news companies, especially those that depend on the advertisement revenue or have an undisclosed business model.\n\nSampling bias can be avoided by researching the real proportion of various properties in the data that will be observed in production, and then sampling the training data by keeping similar proportions.\n\nPrejudice or stereotype bias can be controlled. When developing the training model to distinguish pictures of women from men, a data analyst could choose to under-sample the number of women indoors, or oversample the number of men at home. In other words, prejudice or stereotype bias is reduced by exposing the learning algorithm to a more even- handed distribution of examples.\n\nSystematic value distortion bias can be alleviated by having multiple measuring devices, or hiring humans trained to compare the output of measuring or observing devices.\n\nExperimenter bias can be avoided by letting multiple people validate the questions asked in the survey. Ask yourself: “Do I feel uncomfortable or constrained answering this question?”\n\nFurthermore, despite more diﬃculties in analysis, opt for open-ended questions rather than yes/no or multiple-choice questions. If you still prefer to give responders a choice of answers, include the option “Other” and a place to write a diﬀerent answer.\n\nLabeling bias can be avoided by asking several labelers to identify the same example. Ask the labelers why they decided to assign a speciﬁc label to examples that produced diﬀerent results. If you see that some labelers refer to certain keyphrases, rather than trying to paraphrase the entire document, you can identify those who are quickly scanning instead of reading.\n\nYou can also compare the frequency of skipped documents for diﬀerent labelers. If you see that a labeler skips documents more often than the average, ask if they encountered technical problems, or simply were not interested in some topics.\n\nYou cannot entirely avoid bias in data. There’s no silver bullet. As a general rule, keep a human in the loop, especially if your model aﬀects people’s lives.\n\nRecall that there is a temptation among the data analysts to assume that machine learning models are inherently fair because they make decisions based on evidence and math, as opposed to often messy or irrational human judgments. This is, unfortunately, not always the case: inevitably, a model trained on biased data will produce biased results.\n\nIt is the duty of people training the model to ensure that the output is fair. But what’s fair, you may ask? Unfortunately, again, there is no silver bullet measurement that would always detect unfairness. Choosing an appropriate deﬁnition of model fairness is always problem-speciﬁc and requires human judgment. In Section ?? of Chapter 7, we consider several deﬁnitions of fairness in machine learning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 3223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Human involvement in all stages of data gathering and preparation is the best approach to make sure that the possible damage caused by machine learning is minimized.\n\n3.2.5 Low Predictive Power\n\nLow predictive power is an issue that you often don’t consider until you have spent fruitless energy trying to train a good model. Does the model underperform because it is not expressive enough? Does the data not contain enough information from which to learn? You don’t know.\n\nSuppose the goal is to predict whether a listener will like a new song on a music streaming service. Your data is the name of the artist, the song title, lyrics, and whether that song is in their playlist. The model you train with this data will be far from perfect.\n\nArtists who are not in the listener’s playlist are unlikely to receive a high score from the model. Furthermore, many users will only add some songs of a speciﬁc artist to their playlist. Their musical preferences are signiﬁcantly inﬂuenced by the song arrangement, choice of instruments, sound eﬀects, tone of voice, and subtle changes in tonality, rhythm, and beat. These are properties of songs that cannot be found in lyrics, title, or the artist’s name; they have to be extracted from the sound ﬁle.\n\nOn the other hand, extracting these relevant features from an audio ﬁle is challenging. Even with modern neural networks, recommending songs based on how they sound is considered a hard task for artiﬁcial intelligence. Typically, song recommendations are developed by comparing playlists of diﬀerent listeners and ﬁnding those with similar compositions.\n\nConsider a diﬀerent example of low predictive power. Let’s say we want to train a model that will predict where to point the telescope and observe something interesting. Our data are photos of various regions of the sky where something unusual was captured in the past. Based on these photos alone, it’s very unlikely that we will be able to train a model that accurately predicts such an event. However, if we add to this data the measurements of various sensors, such as those measuring radiofrequency signals from diﬀerent zones, or particle bursts, it is more likely we will be able to make better predictions.\n\nYour work may be especially challenging the ﬁrst time working with the dataset. If you cannot obtain acceptable results, no matter how complex the model becomes, it may be time to consider the problem of low predictive power. Engineer as many additional features as possible (apply your creativity!). Consider indirect data sources to enrich feature vectors.\n\n3.2.6 Outdated Examples\n\nOnce you build the model and deploy it in production, the model usually performs well for some time. This period depends entirely on the phenomenon you are modeling.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 2826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Typically, as we will discuss in Section ?? of Chapter 9, a certain model quality monitoring procedure is deployed in the production environment. Once an erratic behavior is detected, new training data is added to adjust the model; the model is then retrained and redeployed.\n\nOften, the cause of an error is explained by the ﬁniteness of the training set. In such cases, additional training examples will solidify the model. However, in many practical scenarios, the model starts to make errors because of concept drift. Concept drift is a fundamental change in the statistical relationship between the features and the label.\n\nImagine your model predicts whether a user will like certain content on a website. Over time, the preferences of some users may start to change, perhaps due to aging, or because a user discovers something new (I didn’t listen to jazz three years ago, now I do!). The examples added to the training data in the past no longer reﬂect some user’s preferences and start hurting the model performance, rather than contributing to it. This is concept drift. Consider it if you see a decreasing trend in model performance on new data.\n\nCorrect the model by removing the outdated examples from the training data. Sort your training examples, most recent ﬁrst. Deﬁne an additional hyperparameter — what percentage of the most recent examples to use to retrain the model — and tune it using grid search, or another hyperparameter tuning technique.\n\nConcept drift is an example of a broader problem known as distribution shift. We consider hyperparameter tuning and other types of distribution shift in Sections ?? and ??.\n\n3.2.7 Outliers\n\nOutliers are examples that look dissimilar to the majority of examples from the dataset. It’s up to the data analyst to deﬁne “dissimilar.” Typically, dissimilarity is measured by some distance metric, such as Euclidean distance.\n\nIn practice, however, what seems to be an outlier in the original feature vector space can be a typical example in a feature vector space transformed using tools such as a kernel function. Feature space transformation is often explicitly done by a kernel-based model, such as support vector machine (SVM), or implicitly by a deep neural network.\n\nShallow algorithms, such as linear or logistic regression, and some ensemble methods, such as AdaBoost, are particularly sensitive to outliers. SVM has one deﬁnition that is less sensitive to outliers: a special penalty hyperparameter regulates the inﬂuence of misclassiﬁed examples (which often happen to be outliers) on the decision boundary. If this penalty value is low, the SVM algorithm may completely ignore outliers from consideration when drawing the decision boundary (an imaginary hyperplane separating positive and negative examples). If it’s too low, even some regular examples can end up on the wrong side of the decision boundary. The best value for that hyperparameter should be found by the analyst using a hyperparameter tuning technique.\n\nA suﬃciently complex neural network can learn to behave diﬀerently for each outlier in the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 3141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "dataset and, at the same time, still work well for the regular examples. It’s not the desired outcome, as the model becomes unnecessarily complex for the task. More complexity results in longer training and prediction time, and poorer generalization after production deployment.\n\nWhether to exclude outliers from the training data, or to use machine learning algorithms and models robust to outliers, is debatable. Deleting examples from a dataset is not considered scientiﬁcally or methodologically sound, especially in small datasets. In the big data context, on the other hand, outliers don’t typically have a signiﬁcant inﬂuence on the model.\n\nFrom a practical standpoint, if excluding some training examples results in better performance of the model on the holdout data, the exclusion may be justiﬁed. Which examples to consider for exclusion can be decided based on a certain similarity measure. A modern approach to getting such a measure is to build an autoencoder and use the reconstruction error5 as the measure of (dis)similarity: the higher the reconstruction error for a given example, the more dissimilar it is to the dataset.\n\n3.2.8 Data Leakage\n\nData leakage, also called target leakage, is a problem aﬀecting several stages of the machine learning life cycle, from data collection to model evaluation. In this section, I will only describe how this problem manifests itself at the data collection and preparation stages. In the subsequent chapters, I will describe its other forms.\n\nInformation available at theprediction time\n\nInformation unavailable at theprediction timeTimePrediction time\n\nContamination\n\nFigure 8: Data leakage in a nutshell.\n\nData leakage in supervised learning is the unintentional introduction of information about the target that should not be made available. We call it “contamination” (Figure 8). Training on contaminated data leads to overly optimistic expectations about the model performance.\n\n5An autoencoder model is trained to reconstruct its input from an embedding vector. The hyperparame-\n\nters of the autoencoder are tuned to minimize the reconstruction error of the holdout data.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 2192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "3.3 What Is Good Data\n\nWe already considered questions to answer about the data before to start collecting it and the common problems with the data an analyst might encounter. But what constitutes good data for a machine learning project? Below we take a look at several properties of good data.\n\n3.3.1 Good Data Is Informative\n\nGood data contains enough information that can be used for modeling. For example, if you want to train a model that predicts whether the customer will buy a speciﬁc product, you will need to possess both the properties of the product in question and If you only have the the properties of the products customers purchased in the past. properties of the product and a customer’s location and name, then the predictions will be the same for all users from the same location.\n\nIf you have enough training examples, then the model can potentially derive the gender and ethnicity from the name and make diﬀerent predictions for men, women, locations, and ethnicities, but not to each customer individually.\n\n3.3.2 Good Data Has Good Coverage\n\nGood data has good coverage of what you want to do with the model. For example, if you’re going to use the model to classify web pages by topic and you have a thousand topics of interest, then your data has to contain examples of documents on each of the thousand topics in quantity suﬃcient for the algorithm to be able to learn the diﬀerence between topics.\n\nImagine a diﬀerent situation. Let’s say that for a particular topic, you only have one or a couple of documents. Let each document contain a unique ID in the text. In such a scenario, the learning algorithm will not be sure what it must look at in each document to understand to which topic it belongs. Maybe the IDs? They look like good diﬀerentiators. If the algorithm decides to use IDs to separate these couple examples from the rest of the dataset, then the learned model will not be able to generalize: it will not see any of those IDs ever again.\n\n3.3.3 Good Data Reﬂects Real Inputs\n\nGood data reﬂects real inputs that the model will see in production. For example, if you build a system that recognizes cars on the road and all pictures you have were taken during the working hours, then it’s unlikely that you will have many examples of night pictures. Once you deploy the model in production, pictures will start coming from all times of the day, and your model will more frequently make errors on night pictures. Also, remember the problem of a cat, a dog, and a raccoon: if your model doesn’t know anything about raccoons, it will predict their pictures as either dogs or cats.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 2675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "3.3.4 Good Data Is Unbiased\n\nGood data is as unbiased as possible. This property can look similar to the previous one. Still, bias can be present in both the data you use for training and the data that the model is applied to in the production environment.\n\nWe discussed several sources of bias in data and how to deal with it in Section 3.2. A user interface can also be a source of bias. For example, you want to predict the popularity of a news article, and use the click rate as a feature. If some news article was displayed on the top of the page, the number of clicks it got would often be higher compared to another news article displayed on the bottom, even if the latter is more engaging.\n\n3.3.5 Good Data Is Not a Result of a Feedback Loop\n\nGood data is not a result of the model itself. This echoes the problem of the feedback loop discussed above. For example, you cannot train a model that predicts the gender of a person from their name, and then use the prediction to label a new training example.\n\nAlternatively, if you use the model to decide which email messages are important to the user and highlight those important messages, you should not directly take the clicks on those emails as a signal that the email is important. The user might have clicked on them because the model highlighted them.\n\n3.3.6 Good Data Has Consistent Labels\n\nGood data has consistent labels. Inconsistency in labeling can come from several sources:\n\nDiﬀerent people do labeling according to diﬀerent criteria. Even if people believe that they use the same criteria, diﬀerent people often interpret the same criteria diﬀerently.6 • The deﬁnition of some classes evolved over time. This results in a situation when two very similar feature vectors receive two diﬀerent labels.\n\nMisinterpretation of user’s motives. For example, assume that the user ignored a recommended news article. As a consequence, this news article receives a negative label. However, the motive of the user for ignoring this recommendation might be that they already knew the story and not that they are uninterested in the topic of the story.\n\n3.3.7 Good Data Is Big Enough\n\nGood data is big enough to allow generalization. Sometimes, nothing can be done to increase the accuracy of the model. No matter how much data you throw on the learning algorithm: the information contained in the data has low predictive power for your problem. However,\n\n6Recall the example of Mechanical Turk we considered in Section 3.1. To improve the reliability of labels\n\nassigned by diﬀerent people, one can use a majority vote (or an average) of several labelers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 2672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "more often, you can get a very accurate model if you pass from thousands of examples to millions or hundreds of millions. You cannot know how much data you need before you start working on your problem and see the progress.\n\n3.3.8 Summary of Good Data\n\nFor the convenience of future reference, let me once again repeat the properties of good data:\n\nit contains enough information that can be used for modeling, • it has good coverage of what you want to do with the model, • it reﬂects real inputs that the model will see in production, • it is as unbiased as possible, • it is not a result of the model itself, • it has consistent labels, and • it is big enough to allow generalization.\n\n3.4 Dealing With Interaction Data\n\nInteraction data is the data you can collect from user interactions with the system your model supports. You are considered lucky if you can gather good data from interactions of the user with the system.\n\nGood interaction data contains information on three aspects:\n\ncontext of interaction, • action of the user in that context, and • outcome of interaction.\n\nAs an example, assume that you build a search engine, and your model reranks search results for each user individually. A reranking model takes as input the list of links returned by the search engine, based on keywords provided by the user and outputs another list in which the items change order. Usually, a reranked model “knows” something about the user and their preferences and can reorder the generic search results for each user individually according to that user’s learned preferences. The context here is the search query and the hundred documents presented to the user in a speciﬁc order. The action is a click of the user on a particular document link. The outcome is how much time the user spent reading the document and whether the user hit “back.” Another action is the click on the “next page” link.\n\nThe intuition is that the ranking was good if the user clicked on some link and spent signiﬁcant time reading the page. The ranking was not so good if the user clicked on a link to a result and then hit “back” quickly. The ranking was bad if the user clicked on the “next page” link. This data can be used to improve the ranking algorithm and make it more personalized.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 2329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Asia\n\nFrance\n\nGermany\n\n...\n\nChina\n\n83M\n\n67M\n\n38,800\n\n...\n\n...\n\n...\n\n1386M\n\n8,802\n\nRegion\n\nEurope\n\n44,578\n\nCountry\n\nGDP\n\n...\n\nPopulation\n\nEurope\n\n...\n\n12.2T\n\n3.7T\n\n...\n\n...\n\n...\n\n2.6T\n\n...\n\nGDP percapita\n\nFigure 9: An example of the target (GDP) being a simple function of two features: Population and GDP per capita.\n\n3.5 Causes of Data Leakage\n\nLet’s discuss the three most frequent causes of data leakage that can happen during data collection and preparation: 1) target being a function of a feature, 2) feature hiding the target, and 3) feature coming from the future.\n\n3.5.1 Target is a Function of a Feature\n\nGross Domestic Product (GDP) is deﬁned as the monetary measure of all ﬁnished goods and services in a country within a speciﬁc period. Let our goal be to predict a country’s GDP based on various attributes: area, population, geographic region, and so on. An example of such data is shown in Figure 9. If you don’t do a careful analysis of each attribute and its relation to GDP, you might let a leakage happen: in the data in Figure 9, two columns, Population and GDP per capita, multiplied, equal GDP. The model you will train will perfectly predict GDP by looking at these two columns only. The fact that you let GDP be one of the features, though in a slightly modiﬁed form (devised by the population), constitutes contamination and, therefore, leads to data leakage.\n\nA simpler example is when you have a copy of the target among features, just put in a diﬀerent format. Imagine you train a model to predict the yearly salary, given the attributes of an employee. The training data is a table that contains both monthly and yearly salary, among many other attributes. If you forget to remove the monthly salary from the list of features, that attribute alone will perfectly predict the yearly salary, making you believe your model is perfect. Once the model is put in production, it will likely stop receiving information about a person’s monthly salary: otherwise, the modeling would not be needed.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 2075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "8,543\n\n3653\n\n11,987\n\nM18-25\n\nF25-35\n\n18879\n\n...\n\n2\n\n1\n\nGender\n\nM\n\n...\n\nF\n\nCustomer ID\n\n...\n\nGroup\n\n...\n\n...\n\nF\n\n...\n\n...\n\nF65+\n\n6,775\n\nYearlySpendings\n\n1350\n\n2365\n\n...\n\nYearlyPageviews\n\n...\n\n...\n\nFigure 10: An example of the target being hidden in one of the features.\n\n3.5.2 Feature Hides the Target\n\nSometimes the target is not a function of one or more features, but rather is “hidden” in one of the features. Consider the dataset in Figure 10.\n\nIn this scenario, you use customer data to predict their gender. Look at the column Group. If you closely investigate the data in the column Group, you will see that it represents a demographic value to which each existing customer was related in the past. If the data about a customer’s gender and age is factual (as opposed to being guessed by another model that might be available in production), then the column Group constitutes a form of data leakage, when the value you want to predict is “hidden” in the value of a feature.\n\nOn the other hand, if the Group values are predictions provided by another, possibly less accurate model, then you can use this attribute to build a potentially stronger model. This is called model stacking, and we will consider this topic in Section ?? in Chapter 6.\n\n3.5.3 Feature From the Future\n\nFeature from the future is a kind of data leakage that is hard to catch if you don’t have a clear understanding of the business goal. Imagine a client asked you to train a model that predicts whether a borrower will pay back the loan, based on attributes such as age, gender, education, salary, marital status, and so on. An example of such data is shown in Figure 11.\n\nIf you don’t make an eﬀort to understand the business context in which your model will be used, you might decide to use all available attributes to predict the value in the column Will Pay Loan, including the data from the column Late Payment Reminders. Your model will look accurate at testing time and you send it to the client, who will later report that the model doesn’t work well in the production environment.\n\nAfter investigation, you ﬁnd out that, in the production environment, the value of Late Payment Reminders is always zero. This makes sense because the client uses your model before the borrower gets the credit, so no reminders have yet been made! However, your\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "...\n\nF25-35\n\n65723\n\n...\n\n2\n\n1\n\n...\n\nBorrower ID\n\nM35-50\n\nN\n\nWill Pay Loan\n\n1\n\nY\n\n3\n\nMaster's\n\n...\n\n0\n\nLate PaymentReminders\n\nN\n\n...\n\n...\n\n...\n\nM25-35\n\n...\n\nEducation\n\nHigh school\n\nMaster's\n\n...\n\nDemographicGroup\n\n...\n\nFigure 11: A feature unavailable at the prediction time: Late Payment Reminders.\n\nmodel most likely learned to make the “No” prediction when Late Payment Reminders is 1 or more and pays less attention to the other features.\n\nHere is another example. Let’s say you have a news website and you want to predict the ranking of news you serve to the user, so as to maximize the number of clicks on stories. If in your training data, you have positional features for each news item served in the past (e.g., the x − y position of the title, and the abstract block on the webpage), such information will not be available on the serving time, because you don’t know the positions of articles on the page before you rank them.\n\nUnderstanding the business context in which the model will be used is, thus, crucial to avoid data leakage.\n\n3.6 Data Partitioning\n\nAs discussed in Section ?? of the ﬁrst chapter, in practical machine learning, we typically use three disjoint sets of examples: training set, validation set, and test set.\n\nTraining dataTestdata\n\nValidationdata\n\nEntire dataset\n\nFigure 12: The entire dataset partitioned into a training, validation and test sets.\n\nThe training set is used by the machine learning algorithm to train the model.\n\nThe validation set is needed to ﬁnd the best values for the hyperparameters of the machine learning pipeline. The analyst tries diﬀerent combinations of hyperparameter values one by\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26",
      "content_length": 1702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "one, trains a model by using each combination, and notes the model performance on the validation set. The hyperparameters that maximize the model performance are then used to train the model for production. We consider techniques of hyperparameter tuning in more detail in Section ?? of Chapter 5.\n\nThe test set is used for reporting: once you have your best model, you test its performance on the test set and report the results.\n\nValidation and test sets are often referred to as holdout sets: they contain the examples that the learning algorithm is not allowed to see.\n\nTo obtain good partitions of your entire dataset into these three disjoint sets, as schematically illustrated in Figure 12, partitioning has to satisfy several conditions.\n\nCondition 1: Split was applied to raw data.\n\nOnce you have access to raw examples, and before everything else, do the split. This will allow avoiding data leakage, as we will see later.\n\nCondition 2: Data was randomized before the split.\n\nRandomly shuﬄe your examples ﬁrst, then do the split.\n\nCondition 3: Validation and test sets follow the same distribution.\n\nWhen you select the best values of hyperparameters using the validation set, you want that this selection yields a model that works well in production. The examples in the test set are your best representatives of the production data. Hence the need for the validation and test sets to follow the same distribution.\n\nCondition 4: Leakage during the split was avoided.\n\nData leakage can happen even during the data partitioning. Below, we will see what forms of leakage can happen at that stage.\n\nThere is no ideal ratio for the split. In older literature (pre-big data), you might ﬁnd the recommended splits of either 70%/15%/15% or 80%/10%/10% (for training, validation, and test sets, respectively, in proportion to the entire dataset).\n\nToday, in the era of the Internet and cheap labor (e.g., Mechanical Turk or crowdsourcing), organizations, scientists, and even enthusiasts at home can get access to millions of training examples. That makes it wasteful only to use 70% or 80% of the available data for training.\n\nThe validation and test data are only used to calculate statistics reﬂecting the performance of the model. Those two sets just need to be large enough to provide reliable statistics. How much is debatable. As a rule of thumb, having a dozen examples per class is a desirable minimum. If you can have a hundred examples per class in each of the two holdout sets, you have a solid setup and the statistics calculated based on such sets are reliable.\n\nThe percentage of the split can also be dependent on the chosen machine learning algorithm or model. Deep learning models tend to signiﬁcantly improve when exposed to more training data. This is less true for shallow algorithms and models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "content_length": 2875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Your proportions may depend on the size of the dataset. A small dataset of less than a thousand examples would do best with 90% of the data used for training. In this case, you might decide to not have a distinct validation set, and instead simulate with the cross- validation technique. We will talk more about that in Section ?? in Chapter 5.\n\nIt’s worth mentioning that when you split time-series data into the three datasets, you must execute the split so that the order of observations in each example is preserved during the shuﬄing. Otherwise, for most predictive problems, your data will be broken, and no learning will be possible. We talk more about time series in Section ?? in Chapter 4.\n\n3.6.1 Leakage During Partitioning\n\nAs you already know, data leakage may happen at any stage, from data collection to model evaluation. The data partitioning stage is no exception.\n\nGroup leakage may occur during partitioning. Imagine you have magnetic resonance images of the brains of multiple patients. Each image is labeled with certain brain disease, and the same patient may be represented by several images taken at diﬀerent times. If you apply the partitioning technique discussed above (shuﬄe, then split), images of the same patient might appear in both the training and holdout data.\n\nThe model might learn from the particularities of the patient rather than the disease. The model would remember that patient A’s brain has speciﬁc brain convolutions, and if they have a speciﬁc disease in the training data, the model successfully predicts this disease in the validation data by recognizing patient A from just the brain convolutions.\n\nThe solution to group leakage is group partitioning. It consists of keeping all patient examples together in one set: either training or holdout. Once again, you can see how important it is for the data analyst to know as much as possible about the data.\n\n3.7 Dealing with Missing Attributes\n\nSometimes, the data comes to the analyst in a tidy form, such as an Excel spreadsheet,7 but you might ﬁnd some attributes missing. This often happens when the dataset was handcrafted, and the person forgot to ﬁll some values or didn’t get them measured.\n\nThe list of typical approaches of dealing with missing values for an attribute include:\n\nremoving the examples with missing attributes from the dataset (this can be done if your dataset is big enough to safely sacriﬁce some data);\n\nusing a learning algorithm that can deal with missing attribute values (such as the decision tree learning algorithm); • using a data imputation technique.\n\n7The fact that your raw dataset is contained in an Excel spreadsheet doesn’t guarantee that the data is\n\ntidy. One property of tidiness is that each row represents one example.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28",
      "content_length": 2819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "3.7.1 Data Imputation Techniques\n\nTo impute the value of a missing numerical attribute, one technique consists in replacing the missing value by the average value of this attribute in the rest of the dataset. Mathematically it looks as follows. Let j be an attribute that is missing in some examples in the original dataset, and let S(j) be the set of size N(j) that contains only those examples from the original dataset in which the value of the attribute j is present. Then the missing value ˆx(j) of the attribute j is given by,\n\nˆx(j) ←\n\n1 N(j)\n\nX\n\nx(j) i\n\n,\n\ni∈S(j)\n\nwhere N(j) < N and the summation is made only over those examples where the value of the attribute j is present. An illustration of this technique is given in Figure 13, where two examples (at row 1 and 3) have the Height attribute missing. The average value, 177, will be imputed in the empty cells.\n\nAge\n\nWeight\n\n2\n\nRow\n\n5\n\n169\n\nSalary\n\n3\n\n18\n\n175\n\n21\n\n19,000\n\n60\n\n65\n\n34\n\n187\n\n43\n\nHeight\n\n1\n\n4\n\n65\n\n70\n\n35,000\n\n66\n\n76,500\n\n87\n\n26,900\n\n94,800\n\nFigure 13: Replacing the missing value by an average value of this attribute in the dataset.\n\nAnother technique is to replace the missing value with a value outside the normal range of values. For example, if the regular range is [0,1], you can set the missing value to 2 or −1; if the attribute is categorical, such as days of the week, then a missing value can be replaced by the value “Unknown.” Here, the learning algorithm learns what to do when the attribute has a value diﬀerent from regular values. If the attribute is numerical, another technique is replacing the missing value with a value in the middle of the range. For example, if the range for an attribute is [−1,1], you can set the missing value to be equal to 0. Here, the idea is that the value in the middle of the range will not signiﬁcantly aﬀect the prediction.\n\nA more advanced technique is to use the missing value as the target variable for a regression problem. (In this case, we assume all attributes are numerical.) You can use the remaining ] to form a feature vector ˆxi, set ˆyi ← x(j) attributes [x(1) ,\n\n,x(2) i\n\n,...,x(j−1) i\n\n,x(j+1) i\n\n,...,x(D)\n\ni\n\ni\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\ni\n\n29",
      "content_length": 2216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "where j is the attribute with a missing value. Then you build a regression model to predict ˆy from ˆx. Of course, to build training examples (ˆx, ˆy), you only use those examples from the original dataset, in which the value of attribute j is present.\n\nFinally, if you have a signiﬁcantly large dataset and just a few attributes with missing values, you can add a synthetic binary indicator attribute for each original attribute with missing values. Let’s say that examples in your dataset are D-dimensional, and attribute at position j = 12 has missing values. For each example x, you then add the attribute at position j = D + 1, which is equal to 1 if the value of the attribute at position 12 is present in x and 0 otherwise. The missing value then can be replaced by 0 or any value of your choice.\n\nAt prediction time, if your example is not complete, you should use the same data imputation technique to ﬁll the missing values as the technique you used to complete the training data.\n\nBefore you start working on the learning problem, you cannot tell which data imputation technique will work best. Try several techniques, build several models, and select the one that works best (using the validation set to compare models).\n\n3.7.2 Leakage During Imputation\n\nIf you use the imputation techniques that compute some statistic of one attribute (such as average) or several attributes (by solving the regression problem), the leakage happens if you use the whole dataset to compute this statistic. Using all available examples, you contaminate the training data with information obtained from the validation and test examples.\n\nThis type of leakage is not as signiﬁcant as other types discussed earlier. However, you still have to be aware of it and avoid it by partitioning ﬁrst, and then computing the imputation statistic only on the training set.\n\n3.8 Data Augmentation\n\nFor some types of data, it’s quite easy to get more labeled examples without additional labeling. The strategy is called data augmentation, and it’s most eﬀective when applied to images. It consists of applying simple operations, such as crop or ﬂip, to the original images to obtain new images.\n\n3.8.1 Data Augmentation for Images\n\nIn Figure 14, you can see examples of operations that can be easily applied to a given image to obtain one or more new images: ﬂip, rotation, crop, color shift, noise addition, perspective change, contrast change, and information loss.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30",
      "content_length": 2504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Figure 14: Examples of data augmentation techniques. Photo credit: Alfonso Escalante.\n\nFlipping, of course, has to be done only with respect to the axis for which the meaning of the image is preserved. If it’s a football, you can ﬂip with respect to both axes,8 but if it’s a car or a pedestrian, then you should only ﬂip with respect to the vertical axis.\n\nRotation should be applied with slight angles to simulate an incorrect horizon calibration. You can rotate an image in both directions.\n\nCrops can be randomly applied multiple times to the same image by keeping a signiﬁcant part of the object(s) of interest in the cropped images.\n\nIn color shift, nuances of red-green-blue (RGB) are slightly changed to simulate diﬀerent lighting conditions. Contrast change (both decreasing and increasing) and Gaussian noise of diﬀerent intensity can also be applied multiple times to the same image.\n\nBy randomly removing parts of an image, we can simulate situations when an object is\n\n8Unless the context, like grass, makes ﬂipping according to the horizontal axis irrelevant.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "recognizable but not entirely visible because of an obstacle.\n\nAnother popular technique of data augmentation that seems counterintuitive, but works very well in practice, is mixup. As the name suggests, the technique consists of training the model on a mix of the images from the training set. More precisely, instead of training the model on the raw images, we take two images (that could be of the same class or not) and use for training their linear combination:\n\nmixup_image = t × image1 + (1 − t) × image2,\n\nwhere t is a real number between 0 and 1. The target of that mixup image is a combination of the original targets obtained using the same value of t:\n\nmixup_target = t × target1 + (1 − t) × target2.\n\nExperiments9 on the ImageNet-2012, CIFAR-10, and several other datasets showed that mixup improves the generalization of neural network models. The authors of the mixup also found that it increases the robustness to adversarial examples and stabilizes the training of generative adversarial networks (GANs).\n\nIn addition to the techniques shown in Figure 14, if you expect the input images in your production system will come overcompressed, you can simulate overcompression by using some frequently used lossy compression methods and ﬁle formats, such as JPEG or GIF.\n\nOnly training data undergoes augmentation. Of course, it’s impractical to generate all these additional examples in advance and store them. In practice, the data augmentation techniques are applied to the original data on-the-ﬂy during training.\n\n3.8.2 Data Augmentation for Text\n\nWhen it comes to text data augmentations, it is not as straightforward. We need to use appropriate transformation techniques to preserve the contextual and grammatical structure of natural language texts.\n\nOne technique involves replacing random words in a sentence with their close synonyms. For the sentence, “The car stopped near a shopping mall.” some equivalent sentences are:\n\n“The automobile stopped near a shopping mall.”\n\n“The car stopped near a shopping center.”\n\n“The auto stopped near a mall.”\n\n9More details on the mixup technique can be found in Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. “mixup: Beyond empirical risk minimization.” arXiv preprint arXiv:1710.09412 (2017).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32",
      "content_length": 2336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "A similar technique uses hypernyms instead of synonyms. A hypernym is a word that has more general meaning. For example, “mammal” is a hypernym for “whale” and “cat”; “vehicle” is a hypernym for “car” and “bus.” From our example above, we could create the fol- lowing sentences:\n\n“The vehicle stopped near a shopping mall.”\n\n“The car stopped near a building.”\n\nIf you represent words or documents in your dataset using word or document embeddings, you can apply slight Gaussian noise to randomly chosen embedding features to make a variation of the same word or document. You can tune the number of features to modify and the noise intensity as hyperparameters by optimizing the performance on validation data.\n\nAlternatively, to replace a given word w in the sentence, you can ﬁnd k nearest neighbors to the word w in the word embedding space and generate k new sentences by replacing the word w with its respective neighbor. The nearest neighbors can be found using a measure such as cosine similarity or Euclidean distance. The choice of the measure and the value of k, can be tuned as hyperparameters.\n\nA modern alternative to the k-nearest-neighbors approach described above is to use a deep pre-trained model such as Bidirectional Encoder Representations from Transformers (BERT). Models like BERT are trained to predict a masked word given other words in a sentence. One can use BERT to generate k most likely predictions for a masked word and then use them as synonyms for data augmentation.\n\nSimilarly, if your problem is document classiﬁcation, and you have a large corpus of unlabeled documents, but only a small corpus of labeled documents, you can do as follows. First, build document embeddings for all documents in your large corpus. Use doc2vec or any other technique of document embedding. Then, for each labeled document d in your dataset, ﬁnd k closest unlabeled documents in the document embedding space and label them with the same label as d. Again, tune k on the validation data.\n\nAnother useful text data augmentation technique is back translation. To create a new example from a text written in English (it can be a sentence or a document), ﬁrst translate it into another language l using a machine translation system. Then translate it back from l into English. If the text obtained through back translation is diﬀerent from the original text, you add it to the dataset by assigning the same label as the original text.\n\nThere are also data augmentation techniques for other data types, such as audio and video: addition of noise, shifting an audio or a video clip in time, slowing it down or accelerating, changing pitch for audio and color balance for video, to name a few. Describing these techniques in detail is out of the scope of this book. You should just be aware that data augmentation can be applied to any media data, and not only images and text.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33",
      "content_length": 2942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "3.9 Dealing With Imbalanced Data\n\nClass imbalance is a condition in the data that can signiﬁcantly aﬀect the performance of the model, independently of the chosen learning algorithm. The problem is a very uneven distribution of labels in the training data.\n\nThis is the case, for example, when your classiﬁer has to distinguish between genuine and fraudulent e-commerce transactions: the examples of genuine transactions are much more frequent. Typically, a machine learning algorithm tries to classify most training examples correctly. The algorithm is pushed to do so because it needs to minimize a cost function that typically assigns a positive loss value to each misclassiﬁed example. If the loss is the same for the misclassiﬁcation of a minority class example as it is for the misclassiﬁcation of a majority class, then it’s very likely that the learning algorithm decides to “give up” on many minority class examples in order to make fewer mistakes in the majority class.\n\nWhile there is no formal deﬁnition of imbalanced data, consider the following rule of thumb. If there are two classes, then balanced data would mean half of the dataset representing each class. A slight class imbalance is usually not a problem. So, if 60% examples belong to one class and 40% belong to the other, and you use a popular machine learning algorithm in its standard formulation, it should not cause any signiﬁcant performance degradation. However, when the class imbalance is high, for example when 90% examples are of one class, and 10% are of the other, using the standard formulation of the learning algorithm that usually equally weights errors made in both classes may not be as eﬀective and would need modiﬁcation.\n\n3.9.1 Oversampling\n\nA technique used frequently to mitigate class imbalance is oversampling. By making multi- ple copies of minority class examples, it increases their weight, as illustrated in Figure 15a. You might also create synthetic examples by sampling feature values of several examples of the minority class and combining them to obtain a new example of that class. Two popular algo- rithms that oversample the minority class by creating synthetic examples: Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic Sampling Method (ADASYN).\n\nSMOTE and ADASYN work similarly in many ways. For a given example xi of the minority class, they pick k nearest neighbors. Let’s denote this set of k examples as Sk. The synthetic example xnew is deﬁned as xi + λ(xzi − xi), where xzi is an example of the minority class chosen randomly from Sk. The interpolation hyperparameter λ is an arbitrary number in the range [0,1]. (See an illustration for λ = 0.5 in Figure 16.)\n\nBoth SMOTE and ADASYN randomly pick among all possible xi in the dataset. In ADASYN, the number of synthetic examples generated for each xi is proportional to the number of examples in Sk, which are not from the minority class. Therefore, more synthetic examples are generated in the area where the minority class examples are rare.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34",
      "content_length": 3093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Original dataOversampled data\n\nOriginal dataUndersampled data\n\nFigure 15: Undersampling (left) and oversampling (right).\n\n3.9.2 Undersampling\n\nAn opposite approach, undersampling, is to remove from the training set some examples of the majority class (Figure 15b).\n\nThe undersampling can be done randomly; that is, the examples to remove from the majority class can be chosen at random. Alternatively, examples to withdraw from the majority class can be selected based on some property. One such property is Tomek links. A Tomek link exists between two examples xi and xj belonging to two diﬀerent classes if there’s no other example xk in the dataset closer to either xi or xj than the latter two are to each other. The closeness can be deﬁned using a metric such as cosine similarity or Euclidean distance.\n\nIn Figure 17, you can see how removing examples from the majority class based on Tomek links helps to establish a clear margin between examples of two classes.\n\nCluster-based undersampling works as follows. Decide on the number of examples you want to have in the majority class resulting from undersampling. Let that number be k. Run a centroid-based clustering algorithm on the majority examples only with k being the desired number of clusters. Then replace all examples in the majority classes with the k centroids. An example of a centroid-based clustering algorithm is k-nearest neighbors.\n\n3.9.3 Hybrid Strategies\n\nYou can develop your hybrid strategies (by combining both over- and undersampling) and possibly get better results. One such strategy consists of using ADASYN to oversample, and then Tomek links to undersample.\n\nAnother possible strategy consists of combining cluster-based undersampling with SMOTE.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35",
      "content_length": 1788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Figure 16: An illustration of a synthetic example generation for SMOTE and ADASYN. (Built using a script adapted from Guillaume Lemaitre.)\n\n3.10 Data Sampling Strategies\n\nWhen you have a large data asset, so-called big data, it’s not always practical or necessary to work with the entire data asset. Instead, you can draw a smaller data sample that contains enough information for learning.\n\nSimilarly, when you undersample the majority class to adjust for data imbalance, the smaller data sample should be representative of the entire majority class. In this section, we discuss several sampling strategies, their properties, advantages, and drawbacks.\n\nIn There are two main strategies: probability sampling and nonprobability sampling. probability sampling, all examples have a chance to be selected. These techniques involve randomness.\n\nNonprobability sampling is not random. To build a sample, it follows a ﬁxed deterministic sequence of heuristic actions. This means that some examples don’t have a chance of being selected, no matter how many samples you build.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "(a) original data\n\n(b) Tomek links\n\n(c) undersampled data\n\nFigure 17: Undersampling with Tomek links.\n\nHistorically, nonprobability methods were more manageable for a human to execute manually. Nowadays this advantage is not signiﬁcant. Data analysts use computers and software that greatly simplify sampling, even from big data. The main drawback of nonprobability sampling methods is that they include non-representative samples and might systematically exclude important examples. These drawbacks outweigh the possible advantages of nonprobability sampling methods. Therefore, in this book I will only present probability sampling methods.\n\n3.10.1 Simple Random Sampling\n\nSimple random sampling is the most straightforward method, and the one I refer to when I say “sample randomly.” Here, each example from the entire dataset is chosen purely by chance; each example has an equal chance of being selected.\n\nOne way of obtaining a simple random sample is to assign a number to each example, and then use a random number generator to decide which examples to select. For example, if your entire dataset contains 1000 examples, tagged from 0 to 999, use groups of three digits from the random number generator to select an example. So, if the ﬁrst three numbers from the random number generator were 0, 5, and 7, choose the example numbered 57, and so on.\n\nSimplicity is the great advantage of this sampling method, and it can be easily implemented as any programming language can serve as a random number generator. A disadvantage of simple random sampling is that you may not select enough examples that would have a particular property of interest. Consider the situation where you extract a sample from a large imbalanced dataset. In so doing, you accidentally fail to capture a suﬃcient number of examples from the minority class - or any at all.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "3.10.2 Systematic Sampling\n\nTo implement systematic sampling (also known as interval sampling), you create a list containing all examples. From that list, you randomly select the ﬁrst example xstart from the ﬁrst k elements on the list. Then, you select every kth item on the list starting from xstart. You choose such a value of k that will give you a sample of the desired size.\n\nAn advantage of the systematic sampling over the simple random sampling is that it draws examples from the whole range of values. However, systematic sampling is inappropriate if the list of examples has periodicity or repetitive patterns. In the latter case, the obtained sample can exhibit a bias. However, if the list of examples is randomized, then systematic sampling often results in a better sample than simple random sampling.\n\n3.10.3 Stratiﬁed Sampling\n\nIf you know about the existence of several groups (e.g., gender, location, or age) in your data, you should have examples from each of those groups in your sample. In stratiﬁed sampling, you ﬁrst divide your dataset into groups (called strata) and then randomly select examples from each stratum, like in simple random sampling. The number of examples to select from each stratum is proportional to the size of the stratum.\n\nStratiﬁed sampling often improves the representativeness of the sample by reducing its bias; in the worst of cases, the resulting sample is of no less quality than the results of simple random sampling. However, to deﬁne strata, the analyst has to understand the properties of the dataset. Furthermore, it can be diﬃcult to decide which attributes will deﬁne the strata.\n\nIf you don’t know how to best deﬁne the strata, you can use clustering. The only decision you have to make is how many clusters you need. This technique is also useful to choose the unlabeled examples to send for labeling to a human labeler. It often happens that we have millions of unlabeled examples, and few resources available for labeling. Choose examples carefully, so that each stratum or cluster is represented in our labeled data.\n\nStratiﬁed sampling is the slowest of the three methods due to the additional overhead of working with several independent strata. However, its potential beneﬁt of producing a less biased sample typically outweighs its drawbacks.\n\n3.11 Storing Data\n\nKeeping data safe is insurance for your organization’s business: if you lose a business-critical model for any reason, such as a disaster or human mistake (the ﬁle with the model was accidentally erased or overwritten), having the data will allow you to rebuild that model easily.\n\nWhen sensitive data or personally identiﬁable information (PII) is provided by customers or business partners, it must be stored in not just a safe but also a secure location. Jointly with\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38",
      "content_length": 2860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "the DBA or DevOps engineers, access to sensitive data can be restricted by username and, if needed, IP address. Access to a relational database might also be limited on the per row and per column basis.\n\nIt’s also recommended to limit access to read-only and add-only operations, by restricting write and erase operations to speciﬁc users.\n\nIf the data is collected on mobile devices, it might be necessary to store it on the mobile device until the owner connects to wiﬁ. This data might need to be encrypted so that other applications cannot access it. Once the user is connected to wiﬁ, the data has to be synchronized with a secure server by using cryptographic protocols, such as Transport Layer Security (TLS). Each data element on the mobile device has to be marked with a timestamp to allow its proper synchronization with the data on the server.\n\n3.11.1 Data Formats\n\nData for machine learning can be stored in various formats. Data used indirectly, such as dictionaries or gazetteers, may be stored as a table in a relational database, a collection in a key-value store, or a structured text ﬁle.\n\nThe tidy data is usually stored as comma-separated values (CSV) or tab-separated values (TSV) ﬁles. In this case, all examples are stored in one ﬁle. Alternatively, collection of XML (Extensible Markup Language) ﬁles or JSON (JavaScript Object Notation) ﬁles can contain one example per ﬁle.\n\nIn addition to general-purpose formats, certain popular machine learning packages use proprietary data formats to store tidy data. Other machine learning packages often provide application programming interfaces (APIs) to one or several such proprietary data formats. The most frequently supported formats are ARFF (Attribute-Relation File Format used in the Weka machine learning package) and the LIBSVM (Library for Support Vector Machines) format, which is the default format used by the LIBSVM and LIBLINEAR (Library for Large Linear Classiﬁcation) machine learning libraries.\n\nThe data in the LIBSVM format consists of one ﬁle containing all examples. Each line of that ﬁle represents a labeled feature vector using the following format:\n\nlabel index1:value1 index2:value2 ...\n\nwhere indexX:valueY speciﬁes the value Y of the feature at position (dimension) X. If the value at some position is zero, it can be omitted. This data format is especially convenient for sparse data consisting of examples in which the values of most features are zero.\n\nFurthermore, diﬀerent programming languages come with data serialization capabilities. The data for a speciﬁc machine learning package can be persisted on the hard drive using a serialization object or function provided by the programming language or library. When needed, the data can be deserialized in its original form. For example, in Python, a popular\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39",
      "content_length": 2868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "general-purpose serialization module is Pickle; R has built-in saveRDS and readRDS functions. Diﬀerent data analysis packages can also oﬀer their own serialization/deserialization tools.\n\nIn Java, any object that implements the java.io.Serializable interface can be serialized into a ﬁle and then deserialized when needed.\n\n3.11.2 Data Storage Levels\n\nBefore deciding how and where to store the data, it’s essential to choose the appropriate storage level. Storage can be organized in diﬀerent levels of abstraction: from the lowest level, the ﬁlesystem, to the highest level, such as data lake.\n\nFilesystem is the foundational level of storage. The fundamental unit of data on that level is a ﬁle. A ﬁle can be text or binary, is not versioned, and can be easily erased or overwritten.\n\nA ﬁlesystem can be local or networked. A networked ﬁlesystem can be simple or distributed.\n\nA local ﬁlesystem can be as simple as a locally mounted disk containing all the ﬁles needed for your machine learning project.\n\nA distributed ﬁlesystem, such as NFS (Network File System), CephFS (Ceph File System) or HDFS, can be accessed over the network by multiple physical or virtual machines. Files in a distributed ﬁlesystem are stored and accessed over multiple machines in the network.\n\nDespite its simplicity, ﬁlesystem-level storage is appropriate for a many use cases, including:\n\nFile sharing\n\nThe simplicity of ﬁlesystem-level storage and support for standard protocols allows you to store and share data with a small group of colleagues with minimal eﬀort.\n\nLocal archiving\n\nFilesystem-level storage is a cost-eﬀective option for archiving data, thanks to the availability and accessibility of scale-out NAS solutions.\n\nData protection\n\nFilesystem-level storage is a viable data protection solution thanks to built-in redundancy and replication.\n\nParallel access to the data on the ﬁlesystem level is fast for retrieval access but slow for storage, so it’s an appropriate storage level for smaller teams and data.\n\nObject storage is an application programming interface (API) deﬁned over a ﬁlesystem. Using an API, you can programmatically execute such operations on ﬁles as GET, PUT, or DELETE without worrying where the ﬁles are actually stored. The API is typically provided by an API service available on the network and accessible by HTTP or, more generally, TCP/IP or a diﬀerent communication protocol suite.\n\nThe fundamental unit of data in an object storage level is an object. Objects are usually binary: images, sound, or video ﬁles, and other data elements having a particular format.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40",
      "content_length": 2646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Such features as versioning and redundancy can be built into the API service. The access to the data stored on the object storage level can often be done in parallel, but the access is not as fast as on the ﬁlesystem level.\n\nCanonical examples of object storage are Amazon S3 and Google Cloud Storage (GCS). Alternatively, Ceph is a storage platform that implements object storage on a single dis- tributed computer cluster and provides interfaces for both object- and ﬁlesystem-level storage. It’s often used as an alternative to S3 and GCS in on-premises computing systems.\n\nThe database level of data storage allows persistent, fast, and scalable storage of structured data with fast parallel access for both storage and retrieval.\n\nA modern database management system (DBMS) stores data in random-access memory (RAM), but software ensures that data is persisted (and operations on data are logged) to disk and never lost.\n\nThe fundamental unit of data at this level is a row. A row has a unique ID and contains values in columns. In a relational database, rows are organized in tables. Rows can have references to other rows in the same or diﬀerent tables.\n\nDatabases are not exceptionally well suited for storing binary data, though rather small binary objects can sometimes be stored in a column in the form of a blob (for Binary Large OBject). Blob is a collection of binary data stored as a single entity. More often, though, a row stores references to binary objects stored elsewhere — in a ﬁlesystem or object storage.\n\nThe four most frequently used DBMS in the industry are Oracle, MySQL, Microsoft SQL Server, and PostgresSQL. They all support SQL (Structured Query Language), an interface for accessing and modifying data stored in the databases, as well as creating, modifying, and erasing databases.10\n\nA data lake is a repository of data stored in its natural or raw format, usually in the form of object blobs or ﬁles. A data lake is typically an unstructured aggregation of data from multiple sources, including databases, logs, or intermediary data obtained as a result of expensive transformations of the original data.\n\nThe data is saved in the data lake in its raw format, including the structured data. To read data from a data lake, the analyst needs to write the programming code that reads and parses the data stored in a ﬁle or a blob. Writing a script to parse the data ﬁle or a blob is an approach called schema on read, as opposed to the schema on write in DBMS. In a DBMS, the schema of data is deﬁned beforehand, and, at each write, the DBMS makes sure that the data corresponds to the schema.\n\n10The SQL Server uses its proprietary Transact SQL (T-SQL) while Oracle uses Procedural Language SQL\n\n(PL/SQL).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41",
      "content_length": 2795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "3.11.3 Data Versioning\n\nIf data is held and updated in multiple places, you might need to keep track of versions. Versioning the data is also needed if you frequently update the model by collecting more data, especially in an automated way. This happens when you work on automated driving, spam detection, or personalized recommendations, for example. The new data comes from a human driving a car, or the user cleaning up their electronic mail, or recent video streaming. Sometimes, after an update of the data, the new model performs worse, and you would like to investigate why by switching from one version of the data to another.\n\nData versioning is also critical in supervised learning when the labeling is done by multiple labelers. Some labelers might assign very diﬀerent labels to similar examples, which typically hurts the performance of the model. You would like to keep the examples annotated by diﬀerent labelers separately and only merge them when you build the model. Careful analysis of the model performance may show that labelers didn’t provide quality or consistent labels. Exclude such data from the training data, or relabel it, and data versioning will allow this with minimal eﬀort.\n\nData versioning can be implemented in several levels of complexity, from the most basic to the most elaborate.\n\nLevel 0: data is unversioned.\n\nAt this level, data may reside on a local ﬁlesystem, object storage, or in a database. The advantage of having unversioned data is the speed and simplicity of dealing with the data. Still, that advantage is outweighed by potential problems you might encounter when working on your model. Most likely, your ﬁrst problem will be the inability to make versioned deployments. As we will discuss in Chapter 8, model deployments must be versioned. A deployed machine learning model is a mix of code and data. If the code is versioned, the data must be too. Otherwise, the deployment will be unversioned.\n\nIf you don’t version deployments, you will not be able to get back to the previous level of performance in case of any problem with the model. Therefore, unversioned data is not recommended.\n\nLevel 1: data is versioned as a snapshot at training time.\n\nAt this level, data is versioned by storing, at training time, a snapshot of everything needed to train a model. Such an approach allows you to version deployed models and get back to past performance. You should keep track of each version in some document, typically an Excel spreadsheet. That document should describe the location of the snapshot of both code and data, hyperparameter values, and other metadata needed to reproduce the experiment if needed. If you don’t have many models and don’t update them too frequently, this level of versioning could be a viable strategy. Otherwise, it’s not recommended.\n\nLevel 2: both data and code are versioned as one asset.\n\nAt this level of versioning, small data assets, such as dictionaries, gazetteers, and small\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42",
      "content_length": 3023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "datasets, are stored jointly with the code in a version control system, such as Git or Mercurial. Large ﬁles are stored in object storage, such as S3 or GCS, with unique IDs. The training data is stored as JSON, XML, or another standard format, and includes relevant metadata such as labels, the identity of the labeler, time of labeling, the tool used to label the data, and so on.\n\nTools like Git Large File Storage (LFS) automatically replace large ﬁles such as audio samples, videos, large datasets, and graphics with text pointers, inside Git, while storing the ﬁle contents on a remote server.\n\nThe version of the dataset is deﬁned by the git signatures of the code and the data ﬁle. It can also be helpful to add a timestamp to identify a needed version easily.\n\nLevel 3: using or building a specialized data versioning solution.\n\nData versioning software such as DVC and Pachyderm provide additional tools for data versioning. They typically interoperate with code versioning software, such as Git.\n\nLevel 2 of versioning is a recommended way of implementing versioning for most projects. If you feel like Level 2 is not suﬃcient for your needs, explore Level 3 solutions, or consider building your own. Otherwise, that approach is not recommended, as it adds complexity to what is already a complex engineering project.\n\n3.11.4 Documentation and Metadata\n\nWhile you are actively working on a machine learning project, you are often capable of remembering important details about the data. However, once the project goes to production and you switch to another project, this information will eventually become less detailed.\n\nBefore you switch to another project, you should make sure that others can understand your data and use it properly.\n\nIf the data is self-explanatory, then you might leave it undocumented. However, it’s rather rare that someone who didn’t create a dataset can easily understand it and know how to use it just by looking at it.\n\nDocumentation has to accompany any data asset that was used to train a model. This documentation has to contain the following details:\n\nwhat that data means, • how it was collected, or methods used to create it (instructions to labelers and methods for quality control),\n\nthe details of train-validation-test splits, • details of all pre-processing steps, • an explanation of any data that were excluded, • what format is used to store the data, • types of attributes or features (which values are allowed for each attribute or feature), • number of examples,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n43",
      "content_length": 2578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "possible values for labels or the allowable range for a numerical target.\n\n3.11.5 Data Lifecycle\n\nSome data can be stored indeﬁnitely. However, in some business contexts, you might be allowed to store some data for a speciﬁc time, and then you might have to erase it. If such restrictions apply to the data you work with, you have to make sure that a reliable alerting system is in place. That alerting system has to contact the person responsible for the data erasure and have a backup plan in case that person is not available. Don’t forget that the consequences for not erasing data can sometimes be very serious for the organization.\n\nFor every sensitive data asset, a data lifecycle document has to describe the asset, the circle of persons who have access to that data asset, both during and after the project development. The document has to describe how long the data asset will be stored and whether it has to be explicitly destroyed.\n\n3.12 Data Manipulation Best Practices\n\nTo conclude this chapter, we consider two remaining best practices: reproducibility and “data ﬁrst, algorithm second.”\n\n3.12.1 Reproducibility\n\nReproducibility should be an important concern in everything you do, including data collection and preparation. You should avoid transforming data manually, or using powerful tools included in text editors or command line shells, such as regular expressions, “quick and dirty” ad hoc awk or sed commands, and piped expressions.\n\nUsually, the data collection and transformation activities consist of multiple stages. These include downloading data from web APIs or databases, replacing multiword expressions by unique tokens, removing stop-words and noise, cropping and unblurring images, imputation of missing values, and so on. Each step in this multistage process has to be implemented as a software script, such as Python or R script with their inputs and outputs. If you are organized like that in your work, it will allow you to keep track of all changes in the data. If during any stage something wrong happens to the data, you can always ﬁx the script and run the entire data processing pipeline from scratch.\n\nOn the other hand, manual interventions can be hard to reproduce. These are diﬃcult to apply to updated data, or scale for much more data (once you can aﬀord getting more data or a diﬀerent dataset).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n44",
      "content_length": 2402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "3.12.2 Data First, Algorithm Second\n\nRemember that in the industry, contrary to academia, “data ﬁrst, algorithm second,” so focus most of your eﬀort and time on getting more data of wide variety and high quality, instead of trying to squeeze the maximum out of a learning algorithm.\n\nData augmentation, when implemented well, will most likely contribute more to the quality of the model than the search for the best hyperparameter values or model architecture.\n\n3.13 Summary\n\nBefore you start collecting the data, there are ﬁve questions to answer: is the data you will work with accessible, sizeable, useable, understandable, and reliable.\n\nCommon problems with data are high cost, bias, low predictive power, outdated examples, outliers, and leakage.\n\nGood data contains enough information that can be used for modeling, has good coverage of what you want to do with the model, and reﬂects real inputs that the model will see in production. It is as unbiased as possible and not a result of the model itself, has consistent labels, and is big enough to allow generalization.\n\nGood interaction data contains information on three aspects: context of interaction, action of the user in that context, and outcome of the interaction.\n\nTo obtain a good partition of your entire dataset into training, validation and test sets, the process of partitioning has to satisfy several conditions: 1) data was randomized before the split, 2) split was applied to raw data, 3) validation and test sets follow the same distribution, and 4) leakage was avoided.\n\nData imputation techniques can be used to deal with missing attributes in the data.\n\nData augmentation techniques are often used to get more labeled examples without additional manual labeling. The techniques usually apply to image data, but could also be applied to text and other types of perceptive data.\n\nClass imbalance can signiﬁcantly aﬀect the performance of the model. Learning algorithms perform suboptimally when the training data suﬀers from class imbalance. Such techniques as over- and undersampling can help to overcome the class imbalance problem.\n\nWhen you work with big data, it’s not always practical and necessary to work with the entire data asset. Instead, draw a smaller sample of data that contains enough information for learning. Diﬀerent data sampling strategies can be used for that, in particular simple random sampling, systematic sampling, stratiﬁed sampling, and cluster sampling.\n\nData can be stored in diﬀerent data formats and on several data storage levels. Data versioning is a critical element in supervised learning when the labeling is done by multiple labelers. Diﬀerent labelers may provide labels of varying quality, so it’s important to keep\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n45",
      "content_length": 2790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "track of who created which labeled example. Data versioning can be implemented with several levels of complexity, from the most basic to the most elaborate: unversioned (level 0), versioned as a snapshot at training time (level 1), versioned as one asset containing both data and code (level 2), and versioned by using or building a specialized data versioning solution (level 3).\n\nLevel 2 is recommended for most projects.\n\nDocumentation has to accompany any data asset that was used to train a model. That documentation has to contain the following details: what that data means, how it was collected, or methods used to create it (instructions to labelers and methods for quality control), the details of train-validation-test splits and of all pre-processing steps. It must also contain an explanation of any data that were excluded, what format is used to store the data, types of attributes or features, number of examples, and possible values for labels or the allowable range for a numerical target.\n\nFor every sensitive data asset, a data lifecycle document has to describe the asset, the circle of persons who have access to that data asset both during and after the project development.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n46",
      "content_length": 1254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "4 Feature Engineering\n\nAfter data collection and preparation, feature engineering is the second most important activity in machine learning. It’s also the third stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nFeature engineering is a process of ﬁrst conceptually and then programmatically transforming a raw example into a feature vector. It consists of conceptualizing a feature and then writing the programming code that would transform the entire raw example, with potentially the help of some indirect data, into a feature.\n\n4.1 Why Engineer Features\n\nTo be more speciﬁc, consider the problem of recognizing movie titles in tweets. Say you have a vast collection of movie titles; this is data to use indirectly. You also have a collection of tweets; this data will be used directly to create examples. First, build an index of movie titles for fast string matching.1 Then ﬁnd all movie title matches in your tweets. Now stipulate that your examples are matches, and your machine learning problem is that of binary classiﬁcation: whether a match is a movie, or is not a movie.\n\nConsider the following tweet:\n\n1To build an index for fast string matching, you can, for example, use the Aho–Corasick algorithm.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Figure 2: A tweet from Kyle.\n\nOur movie title matching index would help us ﬁnd the following matches: “avatar,” “the terminator,” “It,” and “her”. That gives us four unlabeled examples. You can label those four examples: {(avatar,False),(the terminator,True),(It,False),(her,False)}. However, a machine learning algorithm cannot learn anything from the movie title alone (neither can a human): it needs a context. You might decide that the ﬁve words preceding the match and the ﬁve words following it are a suﬃciently informative context. In machine learning jargon, we call such a context a “ten-word window” around a match. You can tune the width of the window as a hyperparameter.\n\nNow, your examples are labeled matches in their context. However, a learning algorithm cannot be applied to such data. Machine learning algorithms can only apply to feature vectors. This is why you resort to feature engineering.\n\n4.2 How to Engineer Features\n\nFeature engineering is a creative process where the analyst applies their imagination, intuition, and domain expertise. In our illustrative problem of movie title recognition in tweets, we used our intuition to ﬁx the width of the window around the match to ten. Now, we need to be even more creative to transform string sequences into numerical vectors.\n\n4.2.1 Feature Engineering for Text\n\nWhen it comes to text, scientists and engineers often use simple feature engineering tricks. Two such tricks are one-hot encoding and bag-of-words.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 1540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Generally speaking, one-hot encoding transforms a categorical attribute into several binary ones. Let’s say your dataset has an attribute “Color” with possible values “red,” “yellow,” and “green.” We transform each value into a three-dimensional binary vector, as shown below:\n\nred = [1,0,0] yellow = [0,1,0] green = [0,0,1].\n\nIn a spreadsheet, instead of one column headed with the attribute “Color,” you will use three synthetic columns, with the values 1 or 0. The advantage is you now have a vast range of machine learning algorithms at your disposal, for only a handful of learning algorithms support categorical attributes.\n\nBag-of-words is a generalization of applying the one-hot encoding technique to text data. Instead of representing one attribute as a binary vector, you use this technique to represent an entire text document as a binary vector. Let’s see how it works.\n\nImagine that you have a collection of six text documents, as shown below:\n\nLove, love is a verb\n\nLove is a doing word\n\nFeathers on my breath\n\nGentle impulsion\n\nShakes me, makes me lighter\n\nFeathers on my breath\n\nDocument 1\n\nDocument 3\n\nDocument 4\n\nDocument 5\n\nDocument 6\n\nDocument 2\n\nFigure 3: A collection of six documents.\n\nLet your problem be to build a text classiﬁer by topic. A classiﬁcation learning algorithm expects inputs to be labeled feature vectors, so you have to transform the text document collection into a feature vector collection. Bag-of-words allows you to do just that.\n\nFirst, tokenize the texts. Tokenization is a procedure of splitting a text into pieces called “tokens.” A tokenizer is software that takes a string as input, and returns a sequence of tokens extracted from that string. Typically, tokens are words, but it’s not strictly necessary. It can be a punctuation mark, a word, or, in some cases, a combination of words, such as a company (e.g., McDonald’s) or a place (e.g., Red Square). Let’s use a simple tokenizer that extracts words and ignores everything else. We obtain the following collection:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 2076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Document 1\n\n[Love, love, is a verb]\n\nDocument 2\n\n[Love, is, a, doing, word]\n\n[Feathers, on, my, breath]\n\n[Gentle, impulsion]\n\n[Shakes, me, makes, me lighter]\n\n[Feathers, on, my, breath]\n\nDocument 6\n\nDocument 3\n\nDocument 4\n\nDocument 5\n\nFigure 4: The collection of tokenized documents.\n\nThe next step is to build a vocabulary. It contains 16 tokens:2\n\na gentle love on\n\nbreath impulsion makes shakes\n\ndoing is me verb\n\nfeathers lighter my word\n\nNow order your vocabulary in some way and assign a unique index to each token. I ordered the tokens alphabetically:\n\n10\n\nbreathdoingfeathersgentleimpulsionislighterlovemakesmemyonshakesverbword\n\n16\n\n14\n\na\n\n3\n\n6\n\n15\n\n1\n\n7\n\n2\n\n9\n\n5\n\n4\n\n11\n\n12\n\n13\n\n8\n\nFigure 5: Ordered and indexed tokens.\n\nEach token in the vocabulary has a unique index, from 1 to 16. We transform our collection into a collection of binary feature vectors, as shown below:\n\n2I decided to ignore capitalization, but you, as an analyst, might choose to treat the two tokens “Love”\n\nand “love” as two separate vocabulary entities.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "0\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n16\n\nDocument 2\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n15\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\nDocument 5\n\n1\n\nDocument 3\n\n1\n\nDocument 6\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nDocument 1\n\n1\n\n0\n\n0\n\n0\n\nDocument 4\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0aword...\n\n0\n\n0\n\n0\n\n0\n\n1\n\nFigure 6: Feature vectors.\n\nThe 1 is in a speciﬁc position if the corresponding token is present in the text. Otherwise, the feature at that position has a 0.\n\nFor instance, document 1 “Love, love is a verb” is represented by the following feature vector:\n\n[1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0]\n\nUse the corresponding labeled feature vectors as the training data, which any classiﬁcation learning algorithm can work with.\n\nThere are several bag-of-words “ﬂavors.” The above binary-value model often works well. Alternatives to binary values include 1) counts of tokens, 2) frequencies of tokens, or 3) TF-IDF (term frequency-inverse document frequency). If you use the counts of words, then the feature value for “love” in Document 1 “Love, love is a verb” would be 2, representing the number of times the word “love” appears in the document. If applying frequencies of tokens, the value for “love” would be 2/5 = 0.4, assuming that the tokenizer extracted two “love” tokens, and ﬁve total tokens from Document 1. The TF–IDF value increases proportionally to the frequency of a word in the document and is oﬀset by the number of documents in the corpus that contain that word. This adjusts for some words, such as prepositions and pronouns, appearing more frequently in general. I will not go into further detail on TF-IDF, but would recommend the interested reader to learn more about it online.\n\nA straightforward extension of the bag-of-words technique is bag-of-n-grams. An n-gram is a sequence of n words taken from the corpus. If n = 2, and you ignore the punctuation, then all two-grams (usually called bigrams) that can be found in the text “No, I am your father.” are as follows: [“No I,” “I am,” “am your,” “your father”]. The three-grams are [“No\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 2232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "I am,” “I am your,” “am your father”]. By mixing all n-grams, up to a certain n, with tokens in one dictionary, we obtain a bag of n-grams that we can tokenize the same way as we deal with a bag-of-words model.\n\nBecause sequences of words are often less common than individual words, using n-grams creates a more sparse feature vector. At the same time, n-grams allow the machine learning algorithm to learn a more nuanced model. For example, the expressions “this movie was not good and boring” and “this movie was good and not boring” have opposite meaning, but would result in the same bag-of-words vectors, based solely on words. If we consider bigrams of words, then bag-of-words vectors of bigrams for those two expressions would be diﬀerent.\n\n4.2.2 Why Bag-of-Words Works\n\nFeature vectors only work when certain rules are followed. One rule is that a feature at position j in a feature vector must represent the same property in all examples in the dataset. If that feature represents the height in cm of a certain person in a dataset, where each example represents a diﬀerent person, then that must hold true in all other examples. The feature at position j must always represent the height in cm, and nothing else.\n\nThe bag-of-words technique works the same way. Each feature represents the same property of a document: whether a speciﬁc token is present or absent in a document.\n\nAnother rule is that similar feature vectors must represent similar entities in the dataset. This property is also respected when using the bag-of-words technique. Two identical documents will have identical feature vectors. Likewise, two texts regarding the same topic will have higher chances to have similar feature vectors, because they will share more words than those of two diﬀerent topics.\n\n4.2.3 Converting Categorical Features to Numbers\n\nOne-hot encoding is not the only way to convert categorical features to numbers, and it’s not always the best way.\n\nMean encoding, also known as bin counting or feature calibration, is another technique. First, the sample mean of the label is calculated using all examples where the feature has value z. Each value z of the categorical feature is then replaced by that sample mean value. The advantage of this technique is that the data dimensionality doesn’t increase, and by design, the numerical value contains some information about the label.\n\nIf you work on a binary classiﬁcation problem, in addition to sample mean, you can use other useful quantities: the raw counts of the positive class for a given value of z, the odds ratio, and the log-odds ratio. The odds ratio (OR) is usually deﬁned between two random variables. In a general sense, OR is a statistic that quantiﬁes the strength of the association between two events A and B. Two events are considered independent if the OR equals 1, that is, the odds of one event are the same in either the presence or absence of the other event.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 2993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "In application to quantifying a categorical feature, we can calculate the odds ratio between the value z of a categorical feature (event A) and the positive label (event B). Let’s illustrate that with an example. Let our problem be to predict whether an email message is spam or not spam. Let’s assume that we have a labeled dataset of email messages, and we engineered a feature that contains the most frequent word in each email message. Let us ﬁnd the numerical value that would replace the categorical value “infected” of this feature. We ﬁrst build the contingency table for “infected” and “spam”:\n\nSpam145contains\"infected\"doesn't contain\"infected\"Not Spam83462909Total1533255Total49129173408\n\nFigure 7: Contingency table for “infected” and “spam.”\n\nThe odds-ratio of “infected” and “spam” is given by:\n\nodds ratio(infected,spam) =\n\n145/8 346/2909\n\n= 152.4.\n\nAs you can see, the odds ratio, depending on the values in the contingency table, can be extremely low (near zero) or extremely high (an arbitrarily high positive value). To avoid numerical overﬂow issues, analysts often use the log-odds ratio:\n\nlog odds ratio(infected,spam) = log(145/8) − log(346/2909)\n\n= log(145) − log(8) − log(346) + log(2909) = 2.2.\n\nNow you can replace the value “infected” in the above categorical feature with the value of 2.2. You can proceed the same way for other values of that categorical feature and convert all of them into log-odds ratio values.\n\nSometimes, categorical features are ordered, but not cyclical. Examples include school marks (from “A” to “E”) and seniority levels (“junior,” “mid-level,” “senior”). Instead of using one-hot encoding, it’s convenient to represent them with meaningful numbers. Use uniform numbers in the [0,1] range, like 1/3 for “junior”, 2/3 for “mid-level” and 1 for “senior.” If some values should be farther apart, you can reﬂect that with diﬀerent ratios. If “senior” should be farther from “mid-level” than “mid-level” from “junior,” you might use 1/5, 2/5, 1 for “junior,” “mid-level,” and “senior,” respectively. This is why domain knowledge is important.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 2150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "When categorical features are cyclical, integer encoding does not work well. For example, try converting Monday through Sunday to the integers 1 through 7. The diﬀerence between Sunday and Saturday is 1, while the diﬀerence between Monday and Sunday is −6. However, our reasoning suggests the same diﬀerence of 1, because Monday is just one day past Sunday.\n\nInstead, use the sine-cosine transformation. It converts a cyclical feature into two synthetic features. Let p denote the integer value of our cyclical feature. Replace the value p of the cyclical feature with the following two values:\n\npsin = sin\n\n(cid:18)2 × π × p max(p)\n\n(cid:19)\n\n,pcos = cos\n\n(cid:18)2 × π × p max(p)\n\n(cid:19)\n\n.\n\nThe table below contains the values of psin and pcos for the seven days of the week:\n\np\n\npcos psin 0.78 0.62 1 0.97 −0.22 2 3 0.43 −0.9 4 −0.43 −0.9 5 −0.97 −0.22 0.62 6 −0.78 1 7\n\n0\n\nFigure 8 contains the scatter plot built using the above table. You can see the cyclical nature of the two new features.\n\nNow, in your tidy data, replace “Monday” with two values [0.78,0.62], “Tuesday” with [0.97,−0.22], and so on. The dataset has added another dimension, but the model’s predictive quality is signiﬁcantly better, compared to integer encoding.\n\n4.2.4 Feature Hashing\n\nFeature hashing, or hashing trick, converts text data, or categorical attributes with many values, into a feature vector of arbitrary dimensionality. One-hot encoding and bag-of-words have a drawback: many unique values will create high-dimensional feature vectors. For example, if there are one million unique tokens in a collection of text documents, bag-of-words will produce feature vectors that each have a dimensionality of one million. Working with such high-dimensional data might be very computationally expensive.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 1846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Figure 8: The sine-cosine transformed feature that represents the days of the week.\n\nValue 2\n\nHash function\n\nFeature 0\n\nValue K\n\nValue K-1\n\nValue 3...\n\nOriginal valuesFeatures\n\nFeature 5\n\nFeature 4\n\nFeature 3\n\nFeature 1\n\nFeature 0\n\nValue 1\n\nFigure 9: An illustration of the hashing trick for the desired dimensionality of 5 for the original cardinality K of values of an attribute.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "To keep your data manageable, you can use the hashing trick that works as follows. First you decide on the desired dimensionality of your feature vectors. Then, using a hash function, you ﬁrst convert all values of your categorical attribute (or all tokens in your collection of documents) into a number, and then you convert this number into an index of your feature vector. The process is illustrated in Figure 9.\n\nLet’s illustrate how it would work for converting a text “Love is a doing word” into a feature vector. Let us have a hash function h that takes a string as input and outputs a non-negative integer, and let the desired dimensionality be 5. By applying the hash function to each word and applying the modulo of 5 to obtain the index of the word, we get:\n\nh(love) mod 5 = 0\n\nh(is) mod 5 = 3\n\nh(a) mod 5 = 1\n\nh(doing) mod 5 = 3 h(word) mod 5 = 4.\n\nThen we build the feature vector as,\n\n[1,1,0,2,1].\n\nIndeed, h(love) mod 5 = 0 means that we have one word in dimension 0 of the feature vector; h(is) mod 5 = 3 and h(doing) mod 5 = 3 means that we have two words in dimension 3 of the feature vector, and so on. As you can see, there is a collision between words “is” and “doing”: they both are represented by dimension 3. The lower the desired dimensionality, the higher are the chances of collision. This is the trade-oﬀ between speed and quality of learning.\n\nCommonly used hash functions are MurmurHash3, Jenkins, CityHash, and MD5.\n\n4.2.5 Topic Modeling\n\nTopic modeling is a family of techniques that uses unlabeled data, typically in the form of natural language text documents. The model learns to represent a document as a vector of topics. For example, in a collection of news articles, the ﬁve major topics could be “sports,” “politics,” “entertainment,” “ﬁnance,” and “technology”. Then, each document could be represented as a ﬁve-dimensional feature vector, one dimension per topic:\n\n[0.04,0.5,0.1,0.3,0.06]\n\nThe above feature vector represents a document that mixes two major topics: politics (with a weight of 0.5) and ﬁnance (with a weight of 0.3). Topic modeling algorithms, such as Latent\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 2173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n1\n\n2\n\n3\n\n4\n\n5\n\nSemantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), learn by analyzing the unlabeled documents. These two algorithms produce similar outputs, but are based on diﬀerent mathematical models. LSA uses singular value decomposition (SVD) of the word-to-document matrix (constructed using a binary bag-of-words or TF-IDF). LDA uses a hierarchical Bayesian model, in which each document is a mixture of several topics, and each word’s presence is attributable to one of the topics.\n\nLet us illustrate how it works in Python and R. Below is a Python code for LSA:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD\n\nclass LSA():\n\ndef __init__(self, docs):\n\n# Convert documents to TF-IDF vectors self.TF_IDF = TfidfVectorizer() self.TF_IDF.fit(docs) vectors = self.TF_IDF.transform(docs)\n\n# Build the LSA topic model self.LSA_model = TruncatedSVD(n_components=50) self.LSA_model.fit(vectors) return\n\ndef get_features(self, new_docs):\n\n# Get topic-based features for new documents new_vectors = self.TF_IDF.transform(new_docs) return self.LSA_model.transform(new_vectors)\n\n# Later, in production, instantiate LSA model docs = [\"This is a text.\", \"This another one.\"] LSA_featurizer = LSA(docs)\n\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LSA_features = LSA_featurizer.get_features(new_docs)\n\nThe corresponding code3 in R is shown below:\n\nlibrary(tm) library(lsa)\n\nget_features <- function(LSA_model, new_docs){\n\n# new_docs can be passed as a tm::Corpus object or as a vector\n\n3The R code for LSA and LDA is courtesy of Julian Amon.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n# holding character strings representing documents: if(!inherits(new_docs, \"Corpus\")) new_docs <- VCorpus(VectorSource(new_docs)) tdm_test <- TermDocumentMatrix(\n\nnew_docs, control = list(\n\ndictionary = rownames(LSA_model$tk), weighting = weightTfIdf\n\n)\n\n) txt_mat <- as.textmatrix(as.matrix(tdm_test)) crossprod(t(crossprod(txt_mat, LSA_model$tk)), diag(1/LSA_model$sk))\n\n}\n\n# Train LSA model using docs docs <- c(\"This is a text.\", \"This another one.\") corpus <- VCorpus(VectorSource(docs)) tdm_train <- TermDocumentMatrix( corpus, control = list(weighting = weightTfIdf)) txt_mat <- as.textmatrix(as.matrix(tdm_train)) LSA_fit <- lsa(txt_mat, dims = 2)\n\n# Later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LSA_features <- get_features(LSA_fit, new_docs)\n\nBelow is a Python code for LDA:\n\nfrom sklearn.feature_extraction.text import CountVectorizer from sklearn.decomposition import LatentDirichletAllocation\n\nclass LDA():\n\ndef __init__(self, docs):\n\n# Convert documents to TF-IDF vectors self.TF = CountVectorizer() self.TF.fit(docs) vectors = self.TF.transform(docs) # Build the LDA topic model self.LDA_model = LatentDirichletAllocation(n_components=50) self.LDA_model.fit(vectors) return\n\ndef get_features(self, new_docs):\n\n# Get topic-based features for new documents new_vectors = self.TF.transform(new_docs) return self.LDA_model.transform(new_vectors)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 1647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n# Later, in production, instantiate LDA model docs = [\"This is a text.\", \"This another one.\"] LDA_featurizer = LDA(docs)\n\n# Get topic-based features for new_docs new_docs = [\"This is a third text.\", \"This is a fourth one.\"] LDA_features = LDA_featurizer.get_features(new_docs)\n\nAnd here is the corresponding code in R:\n\nlibrary(tm) library(topicmodels)\n\n# Generate feature for new_docs by using LDA_model get_features <- function(LDA_mode, new_docs){\n\n# new_docs can be passed as tm::Corpus object or as a vector # holding character strings representing documents: if(!inherits(new_docs, \"Corpus\")) new_docs <- VCorpus(VectorSource(new_docs)) new_dtm <- DocumentTermMatrix(new_docs, control = list(weighting = weightTf)) posterior(LDA_mode, newdata = new_dtm)$topics\n\n}\n\n# train LDA model using docs docs <- c(\"This is a text.\", \"This another one.\") corpus <- VCorpus(VectorSource(docs)) dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTf)) LDA_fit <- LDA(dtm, k = 5)\n\n# later, in production, get topic-based features for new_docs new_docs <- c(\"This is a third text.\", \"This is a fourth one.\") LDA_features <- get_features(LDA_fit, new_docs)\n\nIn the above listings, docs is a collection of text documents. It can, for example, be a list of strings, where each string is a document.\n\n4.2.6 Features for Time-Series\n\nTime-series data is diﬀerent from the traditional supervised learning data, which has a form of unordered collections of independent observations. A time series is an ordered sequence of observations, and each is marked with a time-related attribute, such as timestamp, date, month-year, year, and so on. An example of a time-series data is given in Figure 10.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Stock Price...14.715.916.8DateS&P 500...3,3523,3473,52117.93,2982020-01-112020-01-122020-01-122016-01-132020-01-13Dow Jones...28,61129,00128,12728,31216.83,54017.93,6872016-01-152020-01-1427,99828,564......2016-01-16...14.53,3452020-01-1228,583\n\nFigure 10: An example of time-series data in the form of an event stream.\n\nStock Price...DateS&P 500...17.43,4102020-01-112020-01-13Dow Jones...28,22016.83,54017.93,6872016-01-152020-01-1427,99828,564......2016-01-16...15.03,3482020-01-1228,732\n\nFigure 11: Classical time series obtained by aggregating the event stream from Figure 10.\n\nIn Figure 10, each row corresponds to the cost of a certain stock at a moment in time, as well as the values of two indices: S&P 500 and Dow Jones. The observations were made irregularly: on 2020-01-12, three observations were made. On 2020-01-13, there were two observations. In the classical time-series data, observations are evenly spaced over time,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "17.43,41028,22017.93,68728,56417.43,41028,22015.03,34828,73216.83,54017.93,68727,99828,564𝑖+1example𝑖example𝑖+2example𝑡−1𝑡−2𝑡−1𝑡−2𝑡−1𝑡−2\n\nFigure 12: Time-series chunked into segments of length w = 2.\n\nsuch as one observation per second, per minute, per day, and so on. If observations are irregular, such time-series data is called a point process or an event stream.\n\nIt’s usually possible to convert an event stream into the classical time-series data by aggregating observations. Examples of aggregation operators are COUNT and AVERAGE. By applying the AVERAGE operator to the event stream data in Figure 10, we obtain the classical time-series data shown in Figure 11.\n\nWhile it’s possible to directly work with event streams, bringing time series to the classical form makes it simpler to apply further aggregations and generate features for machine learning.\n\nAnalysts typically use time-series data to solve two kinds of prediction problems. Given a sequence of recent observations:\n\npredict something about the next observation (for example, given the stock price and the value of stock indices for the last seven days, predict the stock price for tomorrow), or • predict something about the phenomenon that generated that sequence (for example, given a user’s connection log to a software system, predict whether they are likely to cancel their subscription during the current quarter).\n\nBefore neural networks reached their modern learning capacity, analysts worked with time- series data using the shallow machine learning toolkit. To transform a time-series into training data in the form of feature vectors, two decisions must be made:\n\nhow many of the consecutive observations are needed to make an accurate prediction (so-called prediction window), and\n\nhow to convert a sequence of observations into a ﬁxed-dimensionality feature vector.\n\nThere’s no simple way to answer either question. Usually decisions are made based on the subject-matter expert’s knowledge, or by using a hyperparameter tuning technique. However, some recipes work for many time-series data. Below is one such recipe:\n\n1) chunk the entire time series into segments of length w,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "2) create a training example e from each segment s, 3) for each e, calculate various statistics on the observations in s.\n\nWe take Figure 11’s data and chunk it into segments of length w = 2, where w the length of the prediction window. Figure 12 shows that each segment is now a separate example.\n\nIn practice, w is usually larger than 2. Let’s say our prediction window has a length of seven. The statistics calculated at step (3) of the above recipe could be:\n\naverage (e.g., the mean or median of the stock price during the last seven days), • spread (e.g., standard deviation, median absolute deviation, or interquartile range of the values of the S&P 500 index during the last seven days),\n\noutliers (e.g., the fraction of observations, in which the values of the Dow Jones index was atypically low; for example, more than two standard deviations from the mean), • growth (e.g., whether the values of the S&P 500 index have grown between the day t − 6 and t, days t − 3 and t, and between t − 1 and t.\n\nvisual (e.g., how diﬀerent the curve of the stock price values is from a known visual image, such as a hat, or head and shoulders).\n\nNow you see why converting a time series into a classical form is recommended: the above statistics are only meaningful when calculated on the comparable values.\n\nIt should be noted that in the modern neural-network era, analysts most often prefer to train deep neural networks. Long short-term memory (LSTM), convolutional neural network (CNN), and Transformer are popular choices of architecture for a time-series model. These can read arbitrary length time-series as input, and generate a prediction based on the entire sequence. Similarly, neural networks are often applied to texts by reading them word-by-word, or character-by-character. Words and characters are usually represented as embedding vectors; the latter are learned from large corpora of text documents. We will talk about embeddings in Section 4.7.1.\n\n4.2.7 Use Your Creativity\n\nAs I mentioned at the beginning of this section, feature engineering is a creative process. As an analyst, you are in the best position to determine what are good features for your prediction model. Put yourself “in the shoes” of a learning algorithm and imagine what you would look at in your data to decide which label to assign.\n\nSay you are classifying emails as important or unimportant. You might notice that a signiﬁcant number of important messages come from the government revenue agency on the ﬁrst Monday of each month. Create a feature “government ﬁrst monday.” Let it equal 1 when the email came from the government revenue agency on the ﬁrst Monday of a month, and 0 otherwise. Alternatively, you might notice that an email with more than one smiley is rarely important. Create a feature “contains smileys.” Let it equal 1 when an email contains more than one smiley, and 0 otherwise.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 2945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "4.3 Stacking Features\n\nBack to our problem of movie title classiﬁcation in tweets. Each example has three parts:\n\n1) ﬁve words4 that precede the extracted potential movie title (the left context), 2) the extracted potential movie title (the extraction), 3) ﬁve words that follow the extracted movie title (the right context).\n\nTo represent such multi-part examples, we ﬁrst transform each part into a feature vector, and then stack the three feature vectors next to one another to obtain the feature vector for the entire example.\n\n4.3.1 Stacking Feature Vectors\n\nIn our movie title classiﬁcation problem, we ﬁrst collect all the left contexts. We then apply bag-of-words to transform each left context into a binary feature vector. Next, collect all extractions and, using bag-of-words, transform each extraction into a binary feature vector. Then we collect all the right contexts and apply bag-of-words to transform each right context into a binary feature vector. Finally, we concatenate each example, joining the feature vectors of the left context, the extraction, and the right context. We obtain the ﬁnal feature vector that represents the entire example, as shown in Figure 13.\n\nNote that the three feature vectors (one from each part of the example) are created indepen- dently of one another. This means that the vocabulary of tokens is diﬀerent for each part and, therefore, the feature vector dimensionality of each part may also be diﬀerent.\n\nThe order in which you concatenate feature vectors doesn’t matter. The left context features can be placed in the middle or right side of the ﬁnal feature vector. However, you must keep the same concatenation order in all examples. This ensures each feature represents the same property from one example to another.\n\n4.3.2 Stacking Individual Features\n\nUntil now, we engineered features in bulk. One-hot encoding and bag-of-words often generate thousands of features. This is a very time-eﬃcient way of engineering features, but some problems require more to obtain feature vectors with high enough predictive power. We consider the predictive power of a feature in the next section.\n\nImagine that you already have a classiﬁer mA that takes an entire tweet as input and predicts its topic. Let one of the topics be cinema. You might want to enrich the feature vectors in your movie title classiﬁcation problem with this additional information available from the classiﬁer mA. In this case, you will engineer one feature that can be described as “whether\n\n4In practice, the context to the left or the right of the potential movie title can for some examples be\n\nshorter than ﬁve words, because it’s either the beginning or the end of the tweet.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 2757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "010avatar is sarah connor from0...0\n\n0001...1\n\n0100...0\n\nbag ofwordsconcatenationconcatenation\n\n000the terminator1...1\n\nfeature vector ofthe extraction\n\nfeature vector ofthe left context\n\nfeature vector ofthe entire example\n\nbag ofwords\n\n001it is her favorite movie0...1\n\n0010...1\n\nbag ofwords\n\nfeature vector ofthe right context\n\nFigure 13: Creating and stacking feature vectors.\n\nthe topic of the tweet is cinema” and that feature will also be binary: 1 if the topic predicted by mA for the entire tweet is cinema, and 0 otherwise. Again, we concatenate the three partial feature vectors, as shown in Figure 14.\n\nYou might come up with many more useful features for title classiﬁcation in tweets. Examples of such features are:\n\nthe average IMDB score of the movie, • the number of votes for the movie on IMDB, • the Rotten Tomato score of the movie, • whether the movie is recent (or the number that represents the release year), • whether the tweet text contains other movie titles, and • whether the tweet text includes the names of actors or directors.\n\nAll these additional features, as long as they are numerical, can be concatenated to the feature vector. The only condition is that they are concatenated in the same order in all examples.\n\n4.4 Properties of Good Features\n\nNot all features are created equal. In this section, we consider the properties of a good feature.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "0\n\nB-o-w 1B-o-w 2B-o-w MIs cinema?Example 1\n\n1\n\n0\n\n0\n\n...\n\n0\n\n0\n\n0\n\n0\n\n1\n\n...\n\n1\n\n0\n\n0\n\n1\n\n...\n\n0\n\n0\n\n...\n\n...\n\n...\n\n1\n\n...\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n...\n\n0Example 2\n\n...\n\n1\n\n0\n\n1\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n1\n\n...\n\n...\n\n...\n\n...\n\n0\n\n0\n\n1\n\nB-o-w M\n\n...\n\n...\n\n–1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n...\n\n0\n\n...\n\n0\n\n...\n\n1Example N\n\n1\n\n0\n\n1\n\n...\n\n1\n\nFigure 14: Single feature stacking.\n\n4.4.1 High Predictive Power\n\nFirst of all, a good feature has high predictive power. In Chapter 3, you read about predictive power as a property of data. However, a feature can also have high or low predictive power. Let’s say you want to predict whether a patient has cancer. Among other features, you know the make of the person’s car and whether the person is married. These two features are not good predictors for cancer, so our machine learning algorithm will not learn a meaningful relationship between these features and the label. Predictive power is a property of the feature with respect to the problem. The make of the person’s car and whether the person is married could have high predictive power if the problem were diﬀerent.\n\n4.4.2 Fast Computability\n\nGood features can be computed fast. Let’s say you want to predict the topic of a tweet. A tweet is short, and a bag-of-words-based feature vector will be sparse. A sparse vector is a vector whose values in most dimensions are zero. If your dataset is small and the texts are short, the learning algorithm will have a hard time seeing patterns in sparse vectors because they contain little information compared to their size. The information in one sparse vector is rarely contained in the same dimensions as the information in another sparse vector, even if they represent similar concepts.\n\nTo reduce sparsity, you might want to augment your sparse feature vectors with additional non-zero values. To do that, you might send the tweet text to Wikipedia as a search query, and then extract other words from the search results. Wikipedia’s API doesn’t give any guarantee for the speed of response, so it could take several seconds to get a response. For real-time systems, feature extraction must be fast: a less informative feature computed in a fraction of a millisecond is often preferred to a feature with a high predictive power that takes seconds to compute. If your application must be fast, the features obtained from Wikipedia might not be appropriate for your task.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 2493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "4.4.3 Reliability\n\nA good feature must also be reliable. Again, in our Wikipedia example, we cannot have a guarantee that the website will respond at all: it can be down, on planned maintenance, or the API may be temporarily overused and is rejecting requests. Therefore, we cannot trust that Wikipedia-based features will always be available and complete. Thus, we cannot call such features reliable. One unreliable feature can reduce the quality of predictions made by your model. Furthermore, some predictions can become entirely wrong if the value of an important feature is missing.\n\n4.4.4 Uncorrelatedness\n\nCorrelation of two features means their values are related. If the growth of one feature implies the growth of the other, and the inverse is also true, then the two features are correlated.\n\nOnce the model is in production, its performance may change because the input data’s properties may change over time. When many of your features are highly correlated, even a minor change in the input data’s properties may result in signiﬁcant changes in the model’s behavior.\n\nSometimes the model was built under strict time constraints, so the developer used all possible sources of features. With time, maintaining those sources can become costly. It’s generally recommended to eliminate redundant or highly correlated features. Feature selection techniques help reduce such features.\n\n4.4.5 Other Properties\n\nAn essential property of a good feature is that the distribution of its values in the training set is similar to the distribution it will receive in production. For example, a tweet’s date might be necessary for some predictions about it. However, if you apply the model built on historical tweets to predict something about current tweets, the date of your production examples will always be out of the training distribution, which can result in a signiﬁcant error.5\n\nFinally, features that you design should be unitary, easy to understand, and maintain. Unitary means the feature represents a certain simple-to-understand and -explain quantity. For example, when classifying a car’s type given its characteristics, you may use such unitary features as weight, length, width, and color. A feature like “length divided by weight” is not unitary, as it’s composed of two unitary features.\n\n5The date information often is relevant for machine learning and can still be included in the training data. For instance, you could consider engineering cyclical features like “hour of the day,” “day of the week,” “month of the year.” For the prediction problems in which time seasonality has predictive power, having such features can be useful.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 2710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Some learning algorithms may beneﬁt from combining features. However, it’s preferable to do this in a dedicated stage in the model training pipeline. We will consider feature combination and generation of synthetic features later in this chapter.\n\n4.5 Feature Selection\n\nNot all features will be equally important for your problem. For instance, in the problem of detecting movies in tweets, the length of the movie might not be a very important feature. At the same time, when you use bag-of-words, the vocabulary can be very large, while most tokens will appear in the collection of texts only once. If the learning algorithm “sees” that some feature has a non-zero value only in a couple of training examples, it is doubtful the algorithm will learn any useful pattern from that feature. However, if the feature vector is very wide (contains thousands or millions of features), the training time can become prohibitively long. Furthermore, the overall size of the training data can become too large to ﬁt in the RAM of a conventional server.\n\nIf we could estimate the importance of features, we would keep only the most important ones. That would allow us to save time, ﬁt more examples in memory, and improve the model’s quality. Below, we consider some feature selection techniques.\n\n4.5.1 Cutting the Long Tail\n\nTypically, if a feature contains information (e.g., a non-zero value) only for a handful of examples, such a feature could be removed from the feature vector. In bag-of-words, you can build a graph with the distribution of token counts, and then cut oﬀ the so-called long tail, as shown in Figure 15.\n\nA long tail of a distribution is such a part of that distribution that contains elements with substantially lower counts compared to a smaller group of elements with the highest counts. This smaller group is called the head of the distribution, and their aggregated counts make for at least half of all the counts.\n\nThe decision on a threshold for deﬁning the long tail is somewhat subjective. You can set it as a hyperparameter for your problem and discover the optimal value experimentally. On the other hand, the decision can be made by looking at the distribution of counts, as shown in Figure 15a. As you can see, I cut oﬀ the long tail at a point where the distribution of the elements in the tail has become visually ﬂat (Figure 15b).\n\nWhether to cut the long tail, and where to do it, is debatable. In classiﬁcation problems with many classes, the diﬀerence between some classes can be very subtle. Even features whose values are rarely non-zero may become important. However, removing long-tail features often results in faster learning and a better model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 2742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "(a) distribution of word counts in English\n\n(b) the long tail\n\nFigure 15: The distribution of word counts in a collection of texts in English (a) and the long tail (b, zone in blue). The highest count corresponds to “the” (a count of 615); the lowest count corresponds to “zambia” (a count of 1).\n\n4.5.2 Boruta\n\nCutting the long tail is not the only way to select important features and remove less important ones. One popular tool used in Kaggle competitions is Boruta. Boruta iteratively trains random forest models and runs statistical tests to identify features as important and unimportant. The tool exists both in the form of an R package and a Python module.\n\nBoruta works as a wrapper around the random forest learning algorithm, hence its name — Boruta is a spirit of the forests in Slavic mythology. To understand the Boruta algorithm, let’s ﬁrst recall how the random forest learning algorithm works.\n\nRandom forest is based on the idea of bagging. It makes many random samples of the training set and then trains a diﬀerent statistical model on each sample. The prediction is then made by taking the majority vote (for classiﬁcation) or an average (for regression) of all models. The only substantial diﬀerence of random forest from the vanilla bagging algorithm is that in the former, the trained statistical models are decision trees. At each split of the decision tree, a random subset of all features is considered.\n\nOne useful feature of the random forest is its built-in capability to estimate the importance of each feature. Below, I will explain how this estimation works for the case of classiﬁcation.\n\nThe algorithm works in two stages. First, it classiﬁes all training examples from the original training set. Each decision tree in the random forest model votes only on the classiﬁcation of examples that weren’t used to build that tree. After a tree is tested, the number of correct predictions is recorded for that tree.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 2002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "At the second stage, the values of a certain feature are randomly permuted across examples, and the tests are repeated. The number of correct predictions is once again recorded for each tree. The importance of the feature for a single tree is then computed as the diﬀerence between the number of correct classiﬁcations between the original and permuted setting, divided by the number of examples. To obtain the feature importance score, the feature importance measures for individual trees are averaged. While not strictly necessary, it’s convenient to use z-scores instead of the raw importance scores.\n\nTo obtain a z-score for a feature, we ﬁrst ﬁnd the average value and the standard deviation of individual feature scores for individual trees. The feature’s z-score is obtained by subtracting the average value from the score, and then dividing it by the standard deviation.\n\nYou might stop here and use the z-scores of each feature as the criterion to keep it (the higher, the better). However, in practice, the importance score alone often doesn’t reﬂect meaningful correlations between features and the target. Therefore, we need a diﬀerent tool to distinguish the truly important features from the non-important ones, and, as you could guess, Boruta provides that tool.\n\nThe underlying idea of Boruta is simple: we ﬁrst extend the list of features by adding a randomized copy of each original feature, and then build a classiﬁer based on this extended dataset. To assess the importance of an original feature, we compare it to all randomized features. Only features for which the importance is higher than that of the randomized features — and statistically signiﬁcant — are considered truly important.\n\nBelow, I outline the main steps of the Boruta algorithm in the way it was described by its authors6 with adaptations for consistency and clarity:\n\nThe Boruta Algorithm\n\nBuild extended training feature vectors, where each original feature is replicated. Randomly permute the values of the replicated features across the training examples to remove any correlation between the replicated variables and the target.\n\nPerform several random forest learning runs. The replicated features are random- ized before each run by applying the same random feature value permutation process as in the previous step.\n\nFor each run, compute the importance (z-score) of all original and replicated features.\n\n– A feature is deemed important for a single run if its importance is higher than\n\nthe maximal importance among all replicated features.\n\nPerform a statistical test for all original features.\n\n6Miron B. Kursa, Aleksander Jankowski, Witold R. Rudnicki, “Boruta - A System for Feature Selection,”\n\npublished in Fundamenta Informaticae 101 in 2010, pages 271–285.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25",
      "content_length": 2821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "– The null hypothesis is that the feature’s importance is equal to the maximal\n\nimportance of the replicated features (MIRA).\n\n– The statistical test is a two-sided equality test - the hypothesis may be rejected either when the importance of the feature is signiﬁcantly higher or signiﬁcantly lower than MIRA.\n\n– For each original feature, we count and record the number of hits.\n\n– The number of hits for a feature is the number of runs in which the importance\n\nof that feature was higher than MIRA.\n\n– The expected number of hits for R runs is E(R) = 0.5R with standard\n\n√\n\ndeviation S =\n\n0.25R (binomial distribution with p = q = 0.5).\n\n– An original feature is deemed important (accepted) when the number of hits is signiﬁcantly higher than the expected number of hits and is deemed unimportant (rejected) when the number of hits is signiﬁcantly lower than the expected. (It is possible to compute limits for accepting and rejecting feature for any number of runs for the desired conﬁdence level.)\n\nRemove the features which are deemed unimportant from the feature vectors (both original and replicated).\n\nPerform the same procedure for a predeﬁned number of iterations, or until all features are either rejected or conclusively deemed important, whichever comes ﬁrst.\n\nBoruta worked well for many Kaggle competitions; therefore, you can consider it a universally applicable tool for feature selection. One thing worth noting, though, before using Boruta in production: Boruta is a heuristic. There are no theoretical guarantees for its performance. If you want to be sure that Boruta doesn’t harm, run it multiple times and make sure that the feature selection is stable (i.e., consistent across multiple Boruta applications to your data). If the feature selection is not stable, make sure that the number of trees in the random forest is large enough to generate stable results.\n\nThough Boruta is an eﬀective method of feature selection, it’s not the only one used by practitioners. You will ﬁnd the description of several other methods in the book’s companion wiki in an extended version of this chapter.\n\n4.5.3 L1-Regularization\n\nRegularization is an umbrella term for a range of techniques that improve the general- ization of the model. Generalization, in turn, is the model’s ability to correctly predict the label for unseen examples.\n\nWhile regularization doesn’t let you identify important features, some regularization tech- niques, such as L1, allow the machine learning algorithm to learn to ignore some features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26",
      "content_length": 2587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Depending on the kind of model you train, L1 may apply diﬀerently, but the main principle remains the same: L1 penalizes the model for being too complex.\n\nIn practice, L1 regularization produces a sparse model, which is a model that has most of its parameters equal to zero. Therefore, L1 implicitly performs feature selection by deciding which features are essential for prediction, and which ones are not. We will talk about regularization in more detail in the next chapter.\n\n4.5.4 Task-Speciﬁc Feature Selection\n\nFeature selection can also be task-speciﬁc. For example, we can remove some features from bag-of-words vectors representing natural language texts by excluding the dimensions corresponding to stop words. Stop words are the words that are too generic or common for the problem we are trying to solve. Frequent examples of stop words are articles, prepositions, and pronouns. Dictionaries of stop words for most languages are available online.\n\nTo further reduce the feature vector dimensionality obtained from the text data, sometimes it’s practical to preprocess the text by replacing infrequent words (e.g., those whose count in the corpus is below three) with the same synthetic token, for example RARE_WORD.\n\n4.6 Synthesizing Features\n\nThe learning algorithms implemented in the most popular machine learning package for Python, scikit-learn, only work with numerical features. But it can still be useful to convert numerical features into categorical ones.\n\n4.6.1 Feature Discretization\n\nThe reasons to discretize a real-valued numerical feature can be numerous. For example, some feature selection techniques only apply to categorical features. A successful discretization adds useful information to the learning algorithm when the training dataset is relatively small. Numerous studies show that discretization can lead to improved predictive accuracy. It is also simpler for a human to interpret a model’s prediction if it is based on discrete groups of values, such as age groups or salary ranges.\n\nBinning, also known as bucketing, is a popular technique that allows transforming a numerical feature into a categorical one by replacing numerical values in a speciﬁc range by a constant categorical value.\n\nThere are three typical approaches to binning:\n\nuniform binning, • k-means-based binning, and • quantile-based binning.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "content_length": 2408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "uniform: bins of the same width\n\nk-means: each bin is a clusterquantile: each bin contains 4 examplesbin 1bin 2bin 3bin 1bin 2bin 3bin 1bin 2bin 3\n\nFigure 16: Three binning approaches: uniform, k-means-based, and quantile-based.\n\nIn all three cases, you should decide how many bins you want to have. Consider an illustration in Figure 16. Here, we have a numerical feature j and 12 values of this feature, one for each of the 12 examples in our dataset. Let’s say we decided to have three bins. In uniform binning, all bins for a feature have identical widths, as illustrated in Figure 16 on the top.\n\nIn k-means-based binning, values in each bin belong to the nearest one-dimensional k-means cluster, as shown in Figure 16 in the middle.\n\nIn quantile-based binning, all bins have the same number of examples, as shown in Figure 16 at the bottom.\n\nIn uniform binning, once the model is deployed in production, if the value of the feature in the input feature vector is below or above the range of any bin, then the closest bin is assigned, which is either the leftmost or the rightmost bin.\n\nRemember, most modern machine learning algorithm implementations require numerical features. The bins must be transformed back to numerical values by using a technique like one-hot encoding.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "GenderMFFFUser ID12435Age18252821.........Date Subscribed2016-01-122017-08-232019-12-192016-11-30......UserUser ID2422Order ID1234Amount23187.58.3.........Order Date2017-09-132018-11-232019-12-192016-11-30......OrderUser ID4234Call ID1235Call Duration5523547614.........Call Date2016-01-122016-01-132016-12-172016-11-30......CallM19...2019-12-18443342019-12-19...\n\nFigure 17: Relational data for churn analysis.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "GenderFUser ID24Age25User featuresM19Mean OrderAmount12.918Std DevOrder Amount7.10Mean CallDuration235134.3Std Dev CallDuration0142.7\n\nFigure 18: Synthetic features based on sample mean and standard deviation.\n\n4.6.2 Synthesizing Features from Relational Data\n\nData analysts often work with data in a relational database. For example, a mobile phone operator wants to know whether a customer will soon abandon the subscription. This problem is known as churn analysis. We have to represent each customer as a vector of features.\n\nLet’s say the data on the users is contained in three relational tables: User, Order, and Call, as shown in Figure 17.\n\nThe table User already contains two potentially useful features: Gender and Age. We can also create synthetic features using the data from tables Order and Call. As you can see, user 2 has three rows in table Order, while user 4 has one row in table Order, but three rows in table Calls. In order to create a feature that represents one user, we have to reduce those several records into one value. A typical approach is to compute various statistics from the data coming from multiple rows and use the value of each statistic as a feature. The most commonly used statistics are sample mean and standard deviation. (Standard deviation is the square root of sample variance.)\n\nTo give a concrete example, I have calculated the values of four features for users 2 and 4. You can ﬁnd them in Figure 18.\n\nSometimes, a relational database can have a deeper structure. For example, a user can have orders, while each order can have ordered items. In such a case, we can compute a statistic of a statistic. For example, one feature can be created by ﬁrst calculating the standard deviation of item prices in each order, and then by taking the average of those standard deviations for a speciﬁc user. You can combine the statistics in arbitrary ways: the mean of the mean, the standard deviation of the mean, the standard deviation of the standard deviation, and so on. The same principle applies to the database whose table structure is deeper than two levels.\n\nOnce you have generated features based on all possible combinations of statistics, you can select the most useful ones by using one of the feature selection methods.\n\nIf you want to increase the predictive power of your feature vectors, or when your training set is rather small, you can synthesize additional features that would help in predictions. There are two typical ways to synthesize additional features: from the data, or from other features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30",
      "content_length": 2613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "4.6.3 Synthesizing Features from the Data\n\nOne technique commonly used to synthesize one or more additional features is clustering. Let us use the k-means clustering. Choose a value for k. If your ultimate goal is to build a classiﬁcation model, a common way to assign a value for k is to use the number C of classes. In regression, use your intuition or apply any technique allowing to determine the right value of clusters in your data, such as prediction strength or the elbow method. Apply k-means clustering to the feature vectors in your training data. Then add k additional features to your feature vectors. The additional feature D + j, where j = 1,...,k, will be binary and equal to 1 if the corresponding feature vector belongs to cluster j.\n\nYou can synthesize even more features by applying diﬀerent clustering algorithms, or by restarting k-means multiple times from randomly chosen starting points.\n\n4.6.4 Synthesizing Features from Other Features\n\nNeural networks are notorious for their ability to learn complex features by combining simple features in unordinary ways. They combine simple features by letting their values undergo several levels of nested nonlinear transformations. If you have data in abundance, you can train a deep multilayer perceptron model that will learn to cleverly combine the basic unitary features it receives as input.\n\nIf you don’t have an inﬁnite supply of training examples (often the case in practice), very deep neural networks lose their appeal.7 In the case of smaller to moderately large datasets (the number of training examples varies between a thousand and a hundred thousand), you might prefer to use a shallow learning algorithm and “help” your learning algorithm learn by providing a richer set of features.\n\nIn practice, the most common way to obtain new features from the existing features is to apply a simple transformation to one or a pair of existing features. Three typical simple transformations that apply to a numerical feature j in example i are 1) discretization of the feature, 2) squaring the feature, and 3) computing the sample mean and the standard deviation of feature j from k-nearest neighbors of the example i found by using some metric like Euclidean distance or cosine similarity.\n\nTransformations that apply to a pair of numerical features are simple arithmetic operators: +, −, ×, and ÷ (a technique also known as feature-crossing). For example, you can obtain the value of a new feature q in example i, where q > D, by combining the values of features 2 and 6 in the following way: x(q) . I selected features 2 and 6, as well as the i transformation ÷ arbitrarily. If the number D of original features is not too large, you can generate all possible transformations (by considering all pairs of features and all arithmetic operators). Then, by using one of the feature selection methods, select those that increase the quality of the model.\n\ndef= x(2)\n\ni ÷ x(6)\n\ni\n\n7Unless you use deep pre-trained models in transfer learning, as we will discuss in the next chapter.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31",
      "content_length": 3109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "4.7 Learning Features from Data\n\nSometimes, useful features can be learned from data. Learning features from data is especially eﬀective when we can get access to large collections of relevant labeled or unlabeled data, such as text corpora or collections of images from the Web.\n\n4.7.1 Word Embeddings\n\nIn Chapter 3, we used word embeddings for data augmentation. Word embeddings are feature vectors that represent words. Similar words have similar feature vectors, where similarity is given by a certain measure, such as cosine similarity. Word embeddings are learned from large corpora of text documents. A shallow neural network with one hidden layer (called the embedding layer) is trained to predict a word, given its surrounding words, or to predict the surrounding words, given the word in the middle. Once the neural network is trained, the parameters of the embedding layer are used as word embeddings. There are many algorithms to learn word embeddings from data. The most widely used algorithm, invented at Google, with the code available in open source, is word2vec. Pre-trained word2vec embeddings for many languages are available for download.\n\nOnce you have a collection of word embeddings for some language, you can use them to represent individual words in sentences or documents written in that language, instead of using one-hot encoding.\n\nLet’s see how word embeddings are trained by one version of the word2vec algorithm called skip-gram. In word embedding learning, our goal is to build a model that we can use to convert a one-hot encoding of a word into a word embedding. Let our dictionary contain 10,000 words. The one-hot vector for each word is a 10,000-dimensional vector of all zeros, except for one dimension that contains a 1. Diﬀerent words have a 1 in diﬀerent dimensions.\n\nConsider a sentence: “I am attentively reading the book on machine learning engineering.” Now, take the same sentence, but remove one word, say “book.” Our sentence becomes: “I am attentively reading the · on machine learning engineering.” Now let’s only keep the three words before the · and the three words after it: “attentively reading the · on machine learning.” Looking at this six-word window around the ·, if I ask you to guess what · stands for, you would probably say: “book,” “article,” or “paper.” That’s how the context words let you predict the word they surround. It’s also how the machine can learn that words “book,” “paper,” and “article” have a similar meaning. They share similar contexts in multiple texts.\n\nIt turns out that it works the other way around too: a word can predict the context surrounding it. The piece “attentively reading the · on machine learning” is called a skip-gram, with window size 6 (3 + 3). By using the documents available on the Web, we can easily create hundreds of millions of skip-grams.\n\nIn our Let’s denote a skip-gram in the following way: sentence, x−3 is the one-hot vector for “attentively,” x−2 corresponds to “reading,” x is the\n\n[x−3,x−2,x−1,x,x+1,x+2,x+3].\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32",
      "content_length": 3084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "skipped word (·), x+1 is “on,” and so on. A skip-gram with window size 4 will look like this: [x−2,x−1,x,x+1,x+2]. It can also be schematically depicted, as shown in Figure 19. It is a fully-connected network, like the multilayer perceptron. The input word is denoted as · in the skip-gram. The neural network has to learn to predict the context words of the skip-gram, given the input’s central word.\n\nThe activation function used in the output layer is softmax. The cost function is the negative log-likelihood. The embedding for a word is given by the parameters of the embedding layer that apply when a one-hot encoded word is given as the input to the model.\n\nOne problem with word embeddings trained using word2vec is that the set of word embeddings is ﬁxed, and you cannot use the model for out-of-vocabulary words, that is, the words that weren’t present in the corpus used to train word embeddings. There are other architectures of neural networks that allow obtaining embeddings for any word, including out-of-vocabulary words. One such architecture, often used in practice, is fastText. It was invented at Facebook, and the code is available in open source.\n\nThe key diﬀerence between word2vec and fastText is that word2vec treats each word in the corpus as a unitary entity, and learns a vector for each word. Alternatively, fastText treats each word as an average of embedding vectors representing character n-grams that word is composed of. For example, the embedding for the word “mouse” is an average of the embedding vectors of the n-grams “<mo,” “mou,” “<mou,” “mous,” “<mous,” “mouse,” “<mouse,” “mouse>,” “ous,” “ouse,” “ouse>,” “use,” “use>,” “se>” (assuming that the sizes of the smallest and the largest n-gram are, respectively, 3 and 6).\n\nWord embeddings are an eﬀective way of representing natural language texts for use in such neural network architectures as recurrent neural networks (RNN) and convolutional neural networks (CNN) adapted for working with sequences. However, if you want to use word embeddings for representing variable-length texts for shallow learning algorithms (which require the input feature vectors of ﬁxed dimensions), you would have to apply some aggregation operation to word vectors, such as weighted sum or average. The representation of a text document obtained as an average of the words composing that document turns out to be not very useful in practice.\n\n4.7.2 Document Embeddings\n\nA popular way of obtaining an embedding for a sentence or an entire document is to use the doc2vec neural network architecture, also invented at Google and available in open source. The architecture of doc2vec is very similar to word2vec. The only major diﬀerence is that now there are two embedding vectors, one for the document ID and one for the word. The prediction of the surrounding words for an input word is made by, ﬁrst, averaging the two embedding vectors (the document embedding vector and the word embedding vector), and then predicting the surrounding words from that average. To average the two vectors, they\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33",
      "content_length": 3124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Figure 19: The skip-gram model with window size 4 and the embedding layer of 300 units.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34",
      "content_length": 144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "must be of the same dimensionality. Interestingly, this makes it possible to compare not just document vectors (by ﬁnding the cosine similarity), but also a document and a word vector. The word vectors trained that way are very similar to those trained using word2vec.\n\nTo obtain an embedding for a new document, not belonging to the corpus of documents used to train document embeddings, this new document is ﬁrst added to the corpus. It gets a new document ID assigned to it. Then the existing model is additionally trained for several epochs with all trained parameters being frozen but the new ones, corresponding to the new document ID. The input document ID is provided as a one-hot encoding.\n\n4.7.3 Embeddings of Anything\n\nThe following technique is commonly used to obtain embedding vectors for any object (and not just words or documents). First, we formulate a supervised learning problem that takes our objects as input and outputs a prediction. Then we build a labeled dataset and train a neural network model that solves our supervised learning problem. Then we use the outputs of one of the fully connected layers near the output layer of the neural network model (before non-linearity) as embeddings of the input object.\n\nFor example, the ImageNet labeled dataset of images and a deep convolutional neural network (CNN) architecture, similar to AlexNet, is often used to train embeddings for In images. An illustration of the embedding layers for images is shown in Figure 20. this illustration, we have a deep CNN with two fully connected layers near the output. The neural network was trained to predict the object depicted in the image. To obtain an embedding of an image not used for training the model, we send that image (usually represented as three matrices of pixels, one per channel R, G, and B) to the input of the neural network, and then use the output of one of the fully connected layers before non-linearity. Which of the fully connected layers is better depends on the task you want to solve, and must be decided experimentally.\n\nBy following the above approach, we can train embeddings of any type. The data analyst only needs to ﬁgure out three things:\n\nwhat supervised learning problem to solve (for images, usually object classiﬁcation), • how to represent the input for the neural network (for images, matrices of pixels, one per channel), and\n\nwhat will be the architecture of the neural network before the fully connected layers (for images, usually a deep CNN).\n\n4.7.4 Choosing Embedding Dimensionality\n\nThe embedding dimensionality is usually determined experimentally or from experience. For example, Google, in its TensorFlow documentation, recommends the following rule of thumb:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35",
      "content_length": 2780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Figure 20: A neural network architecture for training image embeddings. The embedding layers are shown in green.\n\nd = 4√\n\nD,\n\nwhere d is the embedding dimensionality and D is the “number of categories.” The number of categories for word embeddings is the number of unique words in the corpus. For arbitrary embeddings, it’s the dimensionality of the original input. For example, if the number of the unique words in the corpus is D = 5000000 then the embedding dimensionality d = 4√ A more principled approach to choose the embedding dimensionality is to treat it as a hyperparameter tuned on a downstream task. For example, if you have a labeled corpus of documents, then you can optimize the embedding dimensionality by minimizing the number of prediction errors made by the classiﬁer trained on that labeled data, where the words in the documents are represented by the embeddings.\n\n5000000 = 47. In practice, values between 50 and 600 are often used.\n\n4.8 Dimensionality Reduction\n\nSometimes, it might be necessary to reduce the dimensionality of examples. This is diﬀerent from the problem of feature selection. In the latter, we analyze the properties of all existing features and remove those that, in our opinion, do not contribute much to the quality of the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "model. When we apply a dimensionality reduction technique to a dataset, we replace all features in the original feature vector with a new vector, of lower dimensionality, and of synthetic features.\n\nDimensionality reduction often results in increased learning speed and better generalization. In addition, it improves visualization of datasets: humans can only see in three dimensions.\n\nThere are several ways to reduce dimensionality. And depending on why we want to do that, some are more popular than others. The dimensionality reduction techniques are well described in the machine learning theory books, so I will only discuss when a data analyst should prefer one technique over the others.\n\n4.8.1 Fast Dimensionality Reduction with PCA\n\nPrincipal Component Analysis (PCA) is the oldest of the techniques. It is also, by far, the fastest option. Performance comparison tests show a very weak dependence of the speed of the PCA algorithm on the size of the dataset. Therefore, you can eﬀectively use PCA as a step preceding your model training, and ﬁnd the optimal value of the reduced dimensionality experimentally as part of the hyperparameter tuning process.\n\nPCA’s most signiﬁcant drawback is that the data must ﬁt in memory entirely for the algorithm to work. There’s an out-of-core version of PCA, called Incremental PCA that allows running the algorithm on batches of the dataset, loading in memory one batch at a time. Still, Incremental PCA is an order of magnitude slower than PCA. PCA is also less practical for visualization purposes as compared to the other two techniques considered below.\n\n4.8.2 Dimensionality Reduction for Visualization\n\nIf visualization is your goal, then you would prefer Uniform Manifold Approximation and Projection (UMAP) algorithm, or an autoencoder. Both can be speciﬁcally programmed to produce 2D or 3D feature vectors, while in PCA, the algorithm produces D so-called principal components (where D is the dimensionality of your data), and the analyst must pick the ﬁrst two or three principal components as features for visualization. UMAP is generally much faster than autoencoder, but the two techniques produce very diﬀerent looking visualizations, so you would prefer one over the other based on the properties of the speciﬁc dataset. Furthermore, like PCA, UMAP requires all data to be in memory, while autoencoder can be trained in batches.\n\nDimensionality reduction can also be task-speciﬁc. For example, we can reduce the dimen- sionality of pictures by using an image editor. Similarly, we can reduce the bit rate and the number of channels of the sound sequences.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37",
      "content_length": 2679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "4.9 Scaling Features\n\nOnce all your features are numerical, you are almost ready to start working on your model. The only remaining step that might be helpful is scaling your features.\n\nFeature scaling is bringing all your features to the same, or very similar, ranges of values or distributions. Multiple experiments demonstrated that a learning algorithm applied to scaled features might produce a better model. While there’s no guarantee that scaling will have a positive impact on the quality of your model, it’s considered a best practice. Scaling can also increase the training speed of deep neural networks. It also assures that no individual feature dominates, especially in the initial iterations of gradient descent or other iterative optimization algorithms. Finally, scaling reduces the risk of numerical overﬂow, the problem that computers have when working with very small or very big numbers.\n\n4.9.1 Normalization\n\nNormalization is the process of converting an actual range of values, which a numerical feature can take, into a predeﬁned and artiﬁcial range of values, typically in the interval [−1,1] or [0,1].\n\nFor example, let the natural range of a feature be 350 to 1450. By subtracting 350 from every value of the feature, and dividing the result by 1100, we normalize those values to the range [0,1]. More generally, the normalization formula looks like this:\n\n¯x(j) ←\n\nx(j) − min(j) max(j) − min(j)\n\n,\n\nwhere x(j) is an original value of feature j in some example; min(j) and max(j) are, respectively, the minimum and the maximum value of the feature j in the training data.\n\nIf you prefer the range of [−1,1] then the normalization formula would look like this:\n\n¯x(j) ←\n\n2 × x(j) − max(j) − min(j) max(j) − min(j)\n\n,\n\nA drawback of normalization is that the values max(j) and min(j) are usually outliers, so normalization will “squeeze” the normal feature values into a very small range. One solution to this problem is to apply clipping, that is to pick “reasonable” values for max(j) and min(j) instead of using extreme values from the training data. Let a reasonable range for a feature be estimated as [a,b]. Before calculating the scaled value by using one of the above two formulas, the value x(j) of the feature is set (“clipped”) to a if x(j) is below a, or to b if it’s above b. A frequent way to estimate the values for a and b is winsorization. The technique is named after the engineer and biostatistician Charles Winsor (1895—1951). Winsorization consists of setting all outliers to a speciﬁed percentile of the data; for example,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38",
      "content_length": 2625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "1\n\n2\n\n1\n\n2\n\na 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. In Python, winsorization could be applied to a list of numbers as follows:\n\nfrom scipy.stats.mstats import winsorize winsorize(list_of_numbers, limits=[0.05, 0.05]) The output of the winsorize function will be a list of numbers of the same length as the input, with the values of outliers “clipped.” A corresponding code in R is shown below:\n\nlibrary(DescTools) DescTools::Winsorize(vector_of_numbers, probs = c(0.05, 0.95)) Sometimes, the mean normalization is used:\n\n¯x(j) ←\n\nx(j) − µ(j) max(j) − min(j)\n\n,\n\nwhere µ(j) is the sample mean of the values of feature j.\n\n4.9.2 Standardization\n\nStandardization (or z-score normalization) is the procedure during which the feature values are rescaled so that they have the properties of a standard normal distribution, with µ = 0 and σ = 1, where µ is the sample mean (the average value of the feature, averaged over all examples in the training data) and σ is the standard deviation from the sample mean.\n\nStandard scores (or z-scores) of features are calculated as follows:\n\nˆx(j) ←\n\nx(j) − µ(j) σ(j)\n\n,\n\nwhere µ(j) is the sample mean of the values of feature j, and σ(j) is the standard deviation of the values of feature j from the sample mean.\n\nIn addition, sometimes it’s helpful to apply simple mathematical transformations to the feature values prior to applying the scaling techniques described above. Such transformations include taking the logarithm of the feature, squaring it, or extracting the square root of the feature. The idea is to obtain a distribution as close to a normal distribution as possible.\n\nYou may wonder when you should use normalization, or when to use standardization. There’s no deﬁnitive answer to this question. In theory, normalization would work better for uniformly distributed data, while standardization tends to work best for normally distributed data. However, in practice, data is rarely distributed following a perfect curve. Usually, if your\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39",
      "content_length": 2157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "dataset is not too big and you have time, you can try both and see which one performs better for your task. Feature scaling is usually beneﬁcial to most learning algorithms.\n\n4.10 Data Leakage in Feature Engineering\n\nData leakage during feature engineering can happen in several situations, including feature discretization and scaling.\n\n4.10.1 Possible Problems\n\nImagine that you use your entire dataset to calculate the ranges of each bin or the feature scaling factors. Then you split the dataset into training, validation, and test sets. If you proceed like that, the values of features in the training data will, in part, be obtained by using the examples that belong to the holdout sets. When your dataset is small enough, it might result in an overly optimistic performance of your model on the holdout data.\n\nNow imagine you are working with text, and that you use bag-of-words to create features with the entire dataset. After building the vocabulary, you split your data into the three sets. In this situation, the learning algorithm will be exposed to features based on tokens only present in the holdout sets. Again, the model will display artiﬁcially better performance than had you divided your data before feature engineering.\n\n4.10.2 Solution\n\nA solution, as you might have guessed, is ﬁrst to split the entire dataset into training and holdout sets, and only do feature engineering on the training data. This also applies when you use mean encoding to transform a categorical feature to a number: split the data ﬁrst and then compute the sample mean of the label, based on the training data only.\n\n4.11 Storing and Documenting Features\n\nEven if you plan to train the model right after you ﬁnish engineering features, it’s advised to design a schema ﬁle that provides a description of the features’ expected properties.\n\n4.11.1 Schema File\n\nA schema ﬁle is a document that describes features. This ﬁle is machine-readable, versioned, and updated each time someone makes signiﬁcant updates to features. Here are several examples of the properties that can be encoded in the schema:\n\nnames of features;\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40",
      "content_length": 2173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nfor each feature:\n\n– its type (categorical, numerical), – the fraction of examples that are expected to have that feature present, – minimum and maximum values, – sample mean and variance, – whether it allows zeros, – whether it allows undeﬁned values.\n\nAn example of a schema ﬁle for a four-dimensional dataset is shown below:\n\nfeature {\n\nname : \"height\" type : float min : 50.0 max : 300.0 mean : 160.0 variance : 17.0 zeroes : false undefined : false popularity : 1.0\n\n}\n\nfeature {\n\nname : \"color_red\" type : binary zeroes : true undefined : false popularity : 0.76\n\n}\n\nfeature {\n\nname : \"color_green\" type : binary zeroes : true undefined : false popularity : 0.65\n\n}\n\nfeature {\n\nname : \"color_blue\" type : binary zeroes : true undefined : false popularity : 0.81\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "35\n\n}\n\n4.11.2 Feature Store\n\nLarge and distributed organizations may use a feature store that allows keeping, document- ing, reusing, and sharing features across multiple data science teams and projects. The ways features are maintained and served can diﬀer signiﬁcantly across projects and teams. This introduces infrastructure complexity and often results in duplicated work. Large distributed organizations face some of these challenges:\n\nFeatures not being reused\n\nFeatures representing the same attribute of an entity are being implemented several times by diﬀerent engineers and teams, when existing work from other teams and existing machine learning pipelines could have been reused.\n\nFeature deﬁnitions vary\n\nDiﬀerent teams deﬁne features diﬀerently, and it’s not always possible to access the documentation of a feature.\n\nComputationally intensive features\n\nSome real-time machine learning models aren’t based on informative but computationally intensive features. Having those features available in a fast store would allow using such features in real-time, and not only in batch mode.\n\nInconsistency between training and serving\n\nThe model is usually trained using the historical data, but when served, is exposed to the real-time online data. The values of some features might depend on the entire historical dataset unavailable at the service time. For the model to work correctly, each feature must have the same value for the same input data entity, both in the oﬄine (development) and online (production) mode.\n\nFeature expiration is unknown\n\nWhen a new input example comes into the production environment, there is no way to know exactly which features need to be recomputed; rather the entire pipeline needs to be run to compute the values of all features needed for prediction.\n\nA feature store is a central vault for storing documented, curated, and access-controlled features within an organization. Each feature is described by four elements: 1) name, 2) description, 3) metadata, and 4) deﬁnition.\n\nThe feature name is a string that uniquely identiﬁes the feature, for example: “aver- age_session_length” or “document_length.”\n\nThe feature description is a natural language textual description of the object’s property it represents, for example, “The average length of the session for a user.” or “The number of words in the document.”\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "In addition to those attributes in the schema ﬁle, the feature metadata may supply: why the feature was added to the model, how it contributes to generalization, the person’s name in the organization responsible for maintaining the feature’s data source,8 the input type (e.g., numerical, string, image), the output type (e.g., numerical scalar, categorical, numerical vector), whether the feature store must cache the value of the feature, and if yes, for how long. A feature can also be marked as available online and oﬄine, or just for oﬄine processing. Features available for online processing must be implemented in such a way that their value can be either: 1) read fast from a cache or a value store or 2) computed in real-time. Features that can be computed in real-time include, for example, squaring the input number, determining the shape of the word, or doing a search in the organization’s intranet.\n\nThe deﬁnition of the feature is the versioned code, such as Python or Java. It will be executed in a runtime environment and applied to the input to compute the feature value.\n\nA feature store allows data engineers to insert features. In turn, data analysts and machine learning engineers use an API to get feature values which they deem relevant. A feature store can provide features for a single online input. Or, the analyst working on a model oﬄine may want to convert the training data into a collection of feature vectors, and will send to the feature store a batch of inputs.\n\nFor reproducibility, feature values in a feature store are versioned. With feature value versioning, the data analyst is able to rebuild the model with the same feature values as those used to train the previous model version. After the feature value for a given input is updated, the previous value is not erased. Rather, it is saved with a timestamp indicating when that value was generated. Furthermore, a feature j used by model mB can itself be the output of some model mA. Once model mA changes, it is important to keep its older versions: model mB still might expect as input the outputs generated by an older version of mA. The feature store is located in the overall machine learning pipeline as shown in Figure 21. The architecture was inspired by Uber’s Michelangelo machine learning platform. It contains two feature stores, online and oﬄine, whose data is in sync. At Uber, the online feature store is updated frequently, in near real-time, by using the real-time data. In contrast, the oﬄine feature store is updated in batch-mode by using values of some features computed online, as well as with historical data from logs and oﬄine databases. An example of a feature computed online is “restaurant’s average meal preparation time over the last hour.” An example of a feature computed oﬄine is “restaurant’s average meal preparation time over the last seven days.” At Uber, the features from the oﬄine store are synced to the online store once or several times a day.\n\n4.12 Feature Engineering Best Practices\n\nThroughout the years, analysts and engineers have invented, experimented with, and validated various best practices. Today, they are recommended for nearly every machine learning\n\n8If the person responsible for the feature leaves the company, the product owner must be alerted automat-\n\nically.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n43",
      "content_length": 3373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Figure 21: The place of the feature store in the overall machine learning pipeline.\n\nproject. Using those best practices might not signiﬁcantly improve each project, but they will deﬁnitely not hurt. One best practice already considered in this chapter is to normalize or standardize the features.\n\n4.12.1 Generate Many Simple Features\n\nAt the beginning of the modeling process, try to engineer as many “simple” features as possible. A feature is simple when it doesn’t take signiﬁcant time to code. For example, the bag-of-words approach in document classiﬁcation generates thousands of features with just a couple lines of code. As long as your hardware has the capacity, use anything measurable as a feature. You cannot know in advance whether some quantity, in combination with other quantities, will be useful for prediction.\n\n4.12.2 Reuse Legacy Systems\n\nWhen replacing an old, non-machine-learning-based algorithm with a statistical model, use the output of the old algorithm as a feature for the new model. Make sure that the old algorithm doesn’t change anymore; otherwise, your model’s performance might be negatively aﬀected over time. If the old algorithm is too slow to be a feature, use the old algorithm’s inputs as features for the new model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n44",
      "content_length": 1315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Use an external system as a feature source only if you control the external system’s behavior. Otherwise, there’s a chance that the external system evolves with time, unbeknownst to you. Furthermore, an external system’s owner might decide to use the output of your model as the input for their model. This creates the hidden feedback loop, a situation where you inﬂuence the phenomenon from which you learn.\n\n4.12.3 Use IDs as Features when Needed. . .\n\nUse IDs as features when needed. This might seem counter-intuitive because unique IDs don’t contribute to generalization. However, the use of IDs allows creating one model that has one behavior in a general case, and diﬀerent behaviors in other cases.\n\nFor example, you want to make predictions about some location (city or village), and you have some properties of the location as features. By using location ID as a feature, you can add the training examples for one general location, and train the model to behave diﬀerently in other speciﬁc locations.\n\nHowever, avoid using example ID as a feature.\n\n4.12.4\n\n. . . But Reduce the Cardinality When Possible\n\nUse categorical features with many values (more than a dozen) only when you want the model to have diﬀerent “modes” of behavior that depend on that categorical feature. Typical examples of this are zip code (postal code) or country. You might consider using the categorical feature “Country” if you want the model to behave diﬀerently in Russia versus the United States, for otherwise similar inputs.9\n\nIf you have a categorical feature with many values, but you do not need a model that has several modes depending on that feature, try to reduce the cardinality (i.e., the number of distinct values) of that feature. There are several ways to do that. We already considered one of them, feature hashing, in Section 4.2.4. Other techniques are brieﬂy discussed below:\n\nGroup similar values\n\nTry to group some values into the same category. For example, if you think it’s unlikely that, within one region, diﬀerent locations might need diﬀerent predictions, then group all postal codes from the same state into one state code. Group states into regions.\n\nGroup the long tail\n\nLikewise, try to group the long tail of infrequent values under the name \"Other,\" or merge them with similar frequent values.\n\n9Often, what you want your model to do and what the data dictates are two very diﬀerent things. Even if you think that the model must make similar predictions independently of the country, in reality, you might get poor model performance because the distribution of labels in the training data is diﬀerent for diﬀerent countries.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n45",
      "content_length": 2703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Remove the feature\n\nIf all, or almost all, values of a categorical feature are unique, or one value dominates all other values, consider removing the feature entirely.\n\nThe reduction of a feature’s granularity should be made with care. Categorical features often have functional dependencies with other categorical features, and their predictive power often comes from their combinations. Take state and city as an example. If we decide to group or remove some values in the state feature, we might inadvertently destroy the information that would allow the model to distinguish one “Springﬁeld” from another.\n\n4.12.5 Use Counts with Caution\n\nUse features based on counts with caution. Some counts remain roughly in the same bounds over time. For example, in bag-of-words, if you use the count of each token instead of the binary value, then it’s not a problem, as long as the input document length doesn’t grow or shrink with time. But, if you have a feature like “Number of calls since subscription” for a customer of a growing mobile phone provider, some oldtimers can have a very high number of calls, compared to the newer customer base. On the other hand, the training data could have been built when the company was still young and didn’t have any oldtimers.\n\nThe same caution must be applied when you group feature values in bins based on how common those values are in the dataset. Infrequent values today may become more frequent over time, as more data is added. It is considered a best practice to re-evaluate the model and the features from time to time.\n\n4.12.6 Make Feature Selection When Necessary\n\nMake feature selection when it’s necessary. The reasons could be:\n\nthe need to have an explainable model (so you keep the most signiﬁcant predictors), • strict hardware requirements, such as RAM, hard drive space, or • short time available to experiment and/or rebuild the model in production, • you expect a signiﬁcant distribution shift between two model trainings.\n\nIf you decide to do feature selection, start with Boruta.\n\n4.12.7 Test the Code Carefully\n\nThe feature engineering code must be carefully tested. Unit tests should cover each feature extractor. Check that each feature is generated correctly using as many inputs as possible. For each boolean feature, check that it is true when it should be true and is false when it should be false. Check numerical features for a reasonable value range. Check for NaNs (Not-a-Number values), nulls, zeros, and empty values. A broken extractor for one feature\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n46",
      "content_length": 2584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "can result in arbitrarily poor performance of the model. Feature extractors are the ﬁrst place to look for a problem if the model’s behavior is strange.\n\nEach feature has to be tested for speed, memory consumption, and compatibility with the production environment. What works reasonably well in your local environment may perform poorly when deployed in production.\n\nOnce the model is deployed in the production environment, and each time it is loaded, you must rerun feature extractor tests. If a feature consumes some external resources like a database or an API, these resources might be unavailable on a speciﬁc production runtime instance. The feature extractor has to throw an exception and die if any resource during feature extraction is unavailable. Avoid silent failures that may remain unnoticed for a long time with model performance degrading or becoming completely wrong.\n\nIt is also recommended to perform regular runs of feature extractors on a ﬁxed test data to make sure that the feature value distribution remains the same.\n\n4.12.8 Keep Code, Model, and Data in Sync\n\nThe version of the feature extraction code must be in sync with the model’s version and the data used to build it. The three have to be deployed or rolled back at the same time. Each time the model is loaded in production, it’s useful to check that the three elements are in sync (that is, their versions are the same).\n\n4.12.9\n\nIsolate Feature Extraction Code\n\nThe feature extraction code must be independent of the remaining code that supports the model. It should be possible to update the code responsible for each feature without aﬀecting other features, the data processing pipeline, or the way the model is called. The only exception is when many features are generated in bulk, like in one-hot encoding and bag-of-words.\n\n4.12.10 Serialize Together Model and Feature Extractor\n\nWhen possible, jointly serialize (pickle in Python, RDS in R) the model and the feature extractor object that was used when the model was built. In the production environment, deserialize both and use them. When possible, avoid having several versions of the feature extraction code.\n\nIf your production environment doesn’t let you deserialize both the model and the feature extraction code, use the same feature extraction code when you train the model and serve it. Even a tiny diﬀerence between the code a data scientist used to train the model, and the optimized code the IT team might have written for the production environment, may result in signiﬁcant prediction error.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n47",
      "content_length": 2608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Once the production code for feature extraction is ready, use it to retrain the model. Always completely retrain the model after any change in the feature extraction code.\n\n4.12.11 Log the Values of Features\n\nLog the feature values extracted in production for a random sample of online examples. When you work on a new version of the model, these values will be useful to control the quality of the training data. They will allow you to compare and ensure that the feature values logged in the production environment are the same as those you observed in the training data.\n\n4.13 Summary\n\nFeatures are values extracted from the data entities your model is designed to work with. Each feature represents a speciﬁc property of a data entity. Features are organized in feature vectors, and the model learns to perform mathematical operations on those feature vectors to generate the desired output.\n\nFor text, features can be generated in bulk by using techniques like bag-of-words. Numbers in the bag-of-words feature vectors mean the presence or absence of speciﬁc vocabulary words in the text document. Those numbers can be binary or contain more information, such as the frequency of each word in the document, or a TF-IDF value.\n\nMost machine learning algorithms and libraries require that all features are numerical. To convert categorical features to numbers, techniques such as one-hot encoding and mean encoding are used. If the categorical feature’s values are cyclical, like days of the week or hours in a day, a better alternative is to convert that cyclical feature into two features, using the sine-cosine transformation.\n\nFeature hashing is a way to convert text data, or categorical attributes with many values, into a feature vector of an arbitrary dimensionality. That can be useful when one-hot encoding or bag-of-words generate feature vectors with impractical dimensions.\n\nTopic modeling is a family of algorithmic techniques, such as LDA and LSA, that allow us to learn a model that converts any document into a vector of topics of a required dimensionality.\n\nA time series is an ordered sequence of observations. Each observation is marked with a time-related attribute, such as timestamp, date, year, and so on. Before neural networks reached their modern capacity to learn, analysts worked with time-series data using the shallow machine learning toolkit. The time-series had to be converted into “ﬂat” feature vectors. Nowadays, analysts use neural network architectures adapted to work with sequences, such as LSTM, CNN, and Transformer.\n\nGood features have high predictive power, can be computed fast, are reliable and uncorrelated.\n\nIt is important that the distribution of feature values in the training set is similar to the distribution of values the production model will receive. Furthermore, good features are\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n48",
      "content_length": 2898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "unitary, easy to understand and maintain. The property of being unitary means that the feature represents a simple-to-understand and -explain quantity.\n\nTo increase the predictive power of the data, additional features can be synthesized by discretizing an existing numerical feature, clustering training examples, or applying simple transformations to existing features or combining pairs of features.\n\nFor text, features can be learned from unlabeled data in the form of word and document embeddings. More generally, embeddings can be trained for any type of data if we manage to formulate an appropriate prediction problem and train a deep model. Embedding vectors are then extracted from several rightmost (i.e., closest to the output) fully-connected layers.\n\nWise use of feature selection techniques remove features that don’t contribute to a model’s quality. Two common techniques are cutting the long tail and Boruta. L1 regularization also works as a features selection technique.\n\nDimensionality reduction can improve visualization of high-dimensional datasets. It can also improve the model’s predictive quality. Presently, such techniques as PCA, UMAP, and autoencoders are used for dimensionality reduction. PCA is very fast, but UMAP and autoencoders produce better visualizations.\n\nIt is considered best practices to scale features before training the model, store and document features in schema ﬁles or feature stores, and keep code, model, and training data in sync.\n\nFeature extraction code is one of the most important parts of a machine learning system. It must be extensively and systematically tested.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n49",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "5 Supervised Model Training (Part 1)\n\nModel training (or modeling) is the fourth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nIt’s clear that without training, no model will be built. However, model training is one of the most overrated activities in machine learning. On average, a machine learning engineer spends only 5 − 10% of their time on modeling, if at all. Successful data collection, preparation, and feature engineering are more important. Usually, modeling is simply applying an algorithm from scikit-learn or R to your data, and randomly trying several combinations of hyperparameters. So, if you skipped the preceding two chapters and jumped directly into modeling, please go back and read those chapters, they are important.\n\nAs indicated by this chapter’s title, I have divided supervised model training into two parts. In this ﬁrst part, we will consider learning preparation, choosing the learning algorithm, a shallow learning strategy, assessing model performance, bias-variance tradeoﬀ, regularization, the concept of the machine learning pipeline, and hyperparameter tuning.\n\n5.1 Before You Start Working on the Model\n\nBefore working on the model, you should validate schema conformity, deﬁne an achievable level of performance, choose a performance metric, and make several other decisions.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "5.1.1 Validate Schema Conformity\n\nFirst, ensure the data conforms to the schema, as deﬁned by the schema ﬁle. Even if you initially prepared the data, it’s likely the original data and the current data are not the same. This diﬀerence can be explained by various factors, most probably:\n\nthe method used to persist the data, to hard drive or to database, contains an error; • the method you used to read the data, from where it was persisted, contains an error; • someone else may have changed the data, or the schema, without informing you.\n\nThese schema errors must be detected, identiﬁed, and corrected just as when a programming code error is detected. If needed, the entire data collection and preparation pipeline should be run from scratch, as we discussed at the end of Chapter 3 when talked about reproducibility.\n\n5.1.2 Deﬁne an Achievable Performance Level\n\nDeﬁning an achievable performance level is a crucial step. It gives you an idea of when to stop trying to improve the model. Here are some guidelines:\n\nif a human can label examples without too much eﬀort, math, or complex logic derivations, then you can hope to achieve human-level performance with your model; • if the information needed to make a labeling decision is fully contained in the features, you can expect to have near-zero error;\n\nif the input feature vector has a high number of signals (such as pixels in an image, or words in a document), you can expect to come close to near-zero error;\n\nif you have a computer program solving the same classiﬁcation or regression problem, you can expect your model to perform at least as well. Often the machine learning model performance can improve as more labeled data comes in; and,\n\nif you observe a similar, but diﬀerent system, you can expect to get a similar, but diﬀerent machine learning model performance.\n\n5.1.3 Choose a Performance Metric\n\nWe will talk about assessing the model performance later. For now, there are several ways — metrics — to estimate the level of model performance (its quality). There’s no single best metric you can use for every project. You will choose based on your data and the problem.\n\nIt is recommended to choose one, and only one, performance metric before you start working on the model. Then, compare diﬀerent models and track the overall progress by using this one metric.\n\nIn Section 5.5, you will read about the most popular and handy model performance metrics, and about the approaches allowing us to combine multiple metrics to obtain a single number.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 2578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Training epochError (%)2040608020406080\n\nHuman performance\n\nOur model (training)\n\nOur model (validation)\n\nTraining epochError (%)2040608020406080\n\nHuman performance\n\nOur model (training)\n\nOur model (validation)\n\n(a)\n\n(b)\n\nFigure 2: A model performance compared to a human-performance baseline: (a) the model looks good, so we can decide to regularize it or add more training examples; (b) the model isn’t performing well, so we need to add more features, or increase the model complexity.\n\n5.1.4 Choose the Right Baseline\n\nBefore you start working on a predictive model, it is important to establish baseline perfor- mance on your problem. A baseline is a model or an algorithm that provides a reference point for comparison.\n\nHaving a baseline gives an analyst conﬁdence that the machine-learning-based solution works. If the value of the performance metric for the machine learning model is better than the value obtained using the baseline, then machine learning provides value.\n\nComparing your current model’s performance to a baseline can orient the work in diﬀerent directions. Let’s say we know that human-level performance is achievable on our problem. We then take human performance as a baseline, as shown in Figure 2. In Figure 2a, the model looks good, so we can decide to regularize it or add more training examples. On the other hand, in Figure 2b, the model isn’t performing well, so we should add more features, or increase the model complexity.\n\nThe baseline is a model or an algorithm that gets an input, and outputs a prediction. The baseline’s prediction output must be of the same nature as the model’s prediction. Otherwise, you cannot compare them.\n\nA baseline doesn’t have to be the result of any learning algorithm. It can be a rule-based or heuristic algorithm, a simple statistic applied to the training data, or something else.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 1911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "The two most commonly used baseline algorithms are:\n\nrandom prediction, and • zero rule.\n\nThe random prediction algorithm makes a prediction by randomly choosing a label from the collection of labels assigned to the training examples. In the classiﬁcation problem, it corresponds to randomly picking one class from all classes in the problem. In the regression problem it means selecting from all unique target values in the training data.\n\nThe zero rule algorithm yields a tighter baseline than the random prediction algorithm. This means that it usually improves the value of the metric as compared to random prediction. To make predictions, the zero rule algorithm uses more information about the problem.\n\nIn classiﬁcation, the zero rule algorithm strategy is to always predict the class most common in the training set, independently of the input value. It can look ineﬀective, but consider the following problem. Let the training data for your classiﬁcation problem contain 800 examples of the positive class, and 200 examples of the negative class. The zero rule algorithm will predict the positive class all the time, and the accuracy (one of the popular performance metrics that we will consider in Section 5.5.2) of the baseline will be 800/1000 = 0.8 or 80%, which is not bad for such a simple classiﬁer. Now you know that your statistical model, independently of how close it is to the optimum, must have an accuracy of at least 80%.\n\nNow, let’s consider the zero rule algorithm for regression. According to the zero rule algorithm, the strategy for regression is to predict the sample average of the target values observed in the training data. This strategy will likely have a lower error rate than random prediction.\n\nIf you work on a standard, so-called classical, prediction problem, you can use a state-of-the-art algorithm found in a popular library such as Python’s scikit-learn. For text classiﬁcation, for example, represent the text as bag-of-words, and then train a support vector machine model with a linear kernel. Then try to beat that result with your own more advanced approach. This approach would also work well with image classiﬁcation, machine translation, and other well-studies, so-called benchmark problems.\n\nFor a general numerical dataset, a linear model such as linear or logistic regression, or k-nearest neighbors, for k = 5, would be a decent baseline. For image classiﬁcation, a simple convolutional neural network (CNN), with three convolutional layers (32−64−32 units per layer, each convolutional layer followed by a max pooling layer and a dropout layer) and two fully connected layers at the end (one with 128 units, and one with the number of units corresponding to the number of desired outputs) would be a good baseline.\n\nYou could also use an existing rule-based system, or build your own simple rule-based system. For example, if the problem is to build a model that predicts whether a given website visitor will like a recommended article, a simple rule-based system could work as follows. Take all articles liked by the user, ﬁnd the top ten words in those articles according to their TF-IDF score, and then predict that the user will like an article if at least ﬁve of those ten can be found in the recommended article. Additionally, multiple specialized machine learning\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 3383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "libraries and APIs are available online. If they can be used directly, or repurposed to solve your problem, you should deﬁnitely consider them as a baseline.\n\nFinding a good human baseline is not always simple. You might use Amazon Mechanical Turk service. Mechanical Turk (MT) is a web-platform where people solve simple tasks for a reward. MT provides an API that you can call to get human predictions. The quality of such predictions can vary from very low to relatively high, depending on the task and the reward. MT is relatively inexpensive, so you can get predictions fast and in large numbers.\n\nTo increase the quality of the predictions provided by turkers (this is how MT human workers are called), some analysts use an ensemble of turkers. You can ask three or ﬁve turkers to label the same example, and then pick the majority class among the labels (or average labels for regression). A more expensive alternative is to ask domain experts (or an ensemble, for even better quality) to label your data.\n\n5.1.5 Split Data Into Three Sets\n\nRecall that three sets are generally needed to build a solid model. The ﬁrst, the training set, is used to train the model. It is the data the machine learning algorithm “sees.” The second and third are the holdout sets. The validation set is not seen by the machine learning algorithm. The data analyst uses it to estimate the performance of diﬀerent machine learning algorithms (or the same algorithm conﬁgured with diﬀerent values of hyperparameters) or models when applied to new data. The remaining test set, which is also not seen by the learning algorithm, is used at the end of the project to evaluate and report the performance of the model the best performing on the validation data.\n\nThe process of splitting the entire dataset into three sets is described in Section ?? of Chapter 3. Here, I only reiterate the two most important properties of that process:\n\n1. Validation and test sets must come from the same statistical distribution. That is, their properties have to be maximally similar, but the examples belonging to the two sets must be, obviously and ideally, diﬀerent and obtained independently of one another.\n\n2. Draw validation and test data from a distribution that looks much like the data you expect to observe once the model is deployed in production. It can be diﬀerent from the distribution of the training data.\n\nA couple of words about the latter point. Most of the time, the analyst simply shuﬄes the entire dataset, and then randomly ﬁlls the three sets from this shuﬄed data. In practice, however, it’s common to have many examples that do not look like the production data. Sometimes, these examples are abundant and/or inexpensive. Using this data in the project may result in distribution shift, and the analyst may or may not be aware of it.\n\nIf you are aware of distribution shift, you will place all those easily available examples into your training set, but will avoid using them in the validation and test sets. This way, you evaluate the models against the data that is similar to that in your production setting. Doing\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 3168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "otherwise might result in achieving overly optimistic values of the performance metric during model testing, and selecting for production a suboptimal model.\n\nThe distribution shift can be a hard problem to tackle. Using a diﬀerent data distribution for training could be a conscious choice because of the data availability. However, the analyst may be unaware that the statistical properties of the training and development data are diﬀerent. This often happens when the model is frequently updated after production deployment, and new examples are added to the training set. The properties of the data used to train the model, and that of the data used to validate and test it, can diverge over time. Section ?? in the next chapter provides guidance on how to handle that problem.\n\n5.1.6 Preconditions for Supervised Learning\n\nBefore you start working on your model, make sure the following conditions are satisﬁed:\n\n1. You have a labeled dataset. 2. You have split the dataset into three subsets: training, validation, and test. 3. Examples in the validation and test sets are statistically similar. 4. You engineered features and ﬁlled missed values using only the training data. 5. You converted all examples into numerical feature vectors.1 6. You have selected a performance metric that returns a single number (see Section 5.5). 7. You have a baseline.\n\n5.2 Representing Labels for Machine Learning\n\nIn the classical formulation of classiﬁcation, labels look like values of a categorical feature. For example, in image classiﬁcation, the labels could be “cat,” “dog,” “car,” “building,” and so on.\n\nSome machine learning algorithms, like those you ﬁnd in scikit-learn, accept labels in their natural form: strings. The library take care of transforming strings to numbers that are accepted by a speciﬁc learning algorithm.\n\nSome implementations, however, like those in neural networks, require the analyst to transform the labels to numbers.\n\n5.2.1 Multiclass Classiﬁcation\n\nIn the case of multiclass classiﬁcation (that is, when the model predicts only one label given an input feature vector), one-hot encoding is typically used to convert labels to\n\n1As mentioned in the previous chapter, most modern machine learning libraries and packages expect numerical feature vectors. However, some algorithms, like decision tree learning, can naturally work with categorical features.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 2442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "binary vectors. For example, let your classes be {dog,cat,other}, and you have the following data:\n\nImage\n\nLabel\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\ndog dog cat other cat\n\nOne-hot encoding would generate the following binary vectors for your classes:\n\ndog = [1,0,0], cat = [0,1,0], other = [0,0,1].\n\nAfter you convert categorical labels into binary vectors, your data becomes:\n\nImage\n\nLabel\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\n[1,0,0] [1,0,0] [0,1,0] [0,0,1] [0,1,0]\n\n5.2.2 Multi-label Classiﬁcation\n\nIn multi-label classiﬁcation, the model may predict several labels for one input at the same time (for example, an image can contain both a dog and a cat). In this case, you can use bag-of-words to represent the labels assigned to each example. Let your data be as follows:\n\nImage\n\nLabels\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\ndog, cat dog cat, other other cat, dog\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "After you convert labels into binary vectors, your data becomes:\n\nImage\n\nLabels\n\nimage_1.jpg image_2.jpg image_3.jpg image_4.jpg image_5.jpg\n\n[1,1,0] [1,0,0] [0,1,1] [0,0,1] [1,1,0]\n\nRead the documentation of the speciﬁc implementation of a learning algorithm to know the format of the input expected by the learning algorithm.\n\n5.3 Selecting the Learning Algorithm\n\nChoosing a machine learning algorithm can be a diﬃcult task. If you had a lot of time, you could try all of them. However, usually, the time to solve a problem is limited. To make an informed choice, you can ask yourself several questions before starting to work on the problem. Depending on your answers, you can shortlist some algorithms and try them on your data.\n\n5.3.1 Main Properties of a Learning Algorithm\n\nBelow are several questions and answers which may guide you in choosing a machine learning algorithm or model.\n\nExplainability\n\nDo the model predictions require explanation for a non-technical audience? The most accurate machine learning algorithms and models are so-called “black boxes.” They make very few prediction errors, but it may be diﬃcult to understand, and even harder to explain, why a model or an algorithm made a speciﬁc prediction. Examples of such models are deep neural networks and ensemble models.\n\nIn contrast, kNN, linear regression, and decision tree learning algorithms are not always the most accurate. However, their predictions are easy to inter- pret by a non-expert.\n\nIn-memory vs. out-of-memory\n\nCan your dataset be fully loaded into the RAM of your laptop or server? If yes, then you can choose from a wide variety of algorithms. Otherwise, you would prefer incremental learning algorithms that can improve the model by reading data gradually. Examples of such algorithms are Naïve Bayes and the algorithms for training neural networks.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 1905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Number of features and examples\n\nHow many training examples do you have in your dataset? How many features does each example have? Some algorithms, including those used for training neural networks and random forests, can handle a huge number of examples and millions of features. Others, like the algorithms for training support vector machines (SVM), can be relatively modest in their capacity.\n\nNonlinearity of the data\n\nIs your data linearly separable? Can it be modeled using a linear model? If yes, SVM with the linear kernel, linear and logistic regression can be good choices. Otherwise, deep neural networks or ensemble models might work better.\n\nTraining speed\n\nHow much time is a learning algorithm allowed to use to build a model, and how often you will need to retrain the model on updated data? If training takes two days, and you need to retrain your model every 4 hours, then your model will never be up to date. Neural networks are slow to train. Simple algorithms like linear and logistic regression, or decision trees, are much faster.\n\nSpecialized libraries contain very eﬃcient implementations of some algorithms. You may prefer to do research online to ﬁnd such libraries. Some algorithms, such as random forest learning, beneﬁt from multiple CPU cores, so their training time can be signiﬁcantly reduced on a machine with dozens of cores. Some machine learning libraries leverage GPU (graphics processing unit) to speed up training.\n\nPrediction speed\n\nHow fast must the model be when generating predictions? Will your model be used in a production environment where very high throughput is required? Models like SVMs and linear and logistic regression models, and not-very-deep feedforward neural networks, are extremely fast at prediction time. Others, like kNN, ensemble algorithms, and very deep or recurrent neural networks, are slower.\n\nIf you don’t want to guess the best algorithm for your data, a popular way to choose one is by testing several candidate algorithms on the validation set as a hyperparameter. We talk about hyperparameter tuning in Section 5.6.\n\n5.3.2 Algorithm Spot-Checking\n\nShortlisting candidate learning algorithms for a given problem is sometimes called algorithm spot-checking. For the most eﬀective spot-checking, it is recommended to:\n\nselect algorithms based on diﬀerent principles (sometimes called orthogonal), such as instance-based algorithms, kernel-based, shallow learning, deep learning, ensembles; • try each algorithm with 3 − 5 diﬀerent values of the most sensitive hyperparameters (such as the number of neighbors k in k-nearest neighbors, penalty C in support vector machines, or decision threshold in logistic regression);\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 2749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Andriy Burkov\n\nMachine Learning Engineering - Draft\n\ng r o . n r a e l - t i k i c s\n\n: e c r u o S\n\n.\n\nn r a e l - t i k i c s\n\nr o f\n\nm a r g a i d\n\nn o i t c e l e s\n\nm h t i r o g l a\n\ng n i n r a e l\n\ne n i h c a M\n\n: 3\n\ne r u g i F\n\n12",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "use the same training/validation split for all experiments, • if the learning algorithm is not deterministic (such as the learning algorithms for neural networks and random forests), run several experiments, and then average the results; • once the project is over, note which algorithms performed the best, and use this information when working on a similar problem in the future.\n\nWhile you don’t know your problem well, try to solve it using as many orthogonal approaches as possible, rather than spending a lot of time on the most promising approach. It is generally a better idea to spend time experimenting with new algorithms and libraries, rather than trying to squeeze the maximum from the one with which you have the most experience.\n\nIf you don’t have time to carefully spot-check algorithms, one simple “hack” is to ﬁnd an eﬃcient implementation of a learning algorithm or a model that most modern papers claim to beat, when applied to a problem similar to yours, and use it for solving your problem.\n\nIf you use scikit-learn, you could try their algorithm selection diagram shown in Figure 3.\n\n5.4 Building a Pipeline\n\nMany modern machine learning packages and frameworks support the notion of a pipeline. A pipeline is a sequence of transformations the training data goes through, before it becomes a model. An example of a pipeline used to train a document classiﬁcation model out of a collection of labeled text documents is shown below:\n\nTokenization\n\nFeatureExtraction\n\nFeatureselection\n\nFeaturenormalization\n\nModeltraining\n\nModel\n\nFigure 4: A pipeline used to produce a model starting with raw data.\n\nEvery stage of a pipeline receives the output of the previous stage, except for the ﬁrst stage, whose input is the training dataset.\n\nBelow is a Python code fragment that constructs a simple scikit-learn pipeline. It consists of two steps: 1) dimensionality reduction using Principal Component Analysis (PCA), and 2) training a support vector machine (SVM) classiﬁer:\n\n1\n\n2\n\n3\n\nfrom sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.decomposition import PCA\n\n4\n\n5\n\n6\n\n# Define a pipeline pipe = Pipeline([('dim_reduction', PCA()), ('model_training', SVC())])\n\n7\n\n8\n\n# Train parameters of both PCA and SVC\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "9\n\n10\n\n11\n\n12\n\npipe.fit(X, y)\n\n# Make a prediction pipe.predict(new_example)\n\nWhen the command pipe.predict(new_example) is executed, the input example is ﬁrst transformed into a reduced dimensionality vector using the PCA model. That reduced dimensionality vector is used as input to the SVM model. PCA and SVM models were trained, one after the other, when the command pipe.fit(X, y) were executed.\n\nUnfortunately, deﬁning and training pipelines in R is not as straightforward as in Python, so we don’t put the code in the book.\n\nThe pipeline can be saved to a ﬁle similar to saving a model. It will be deployed to production and used to generate predictions. In other words, during the scoring, the input example passes through the entire pipeline and “becomes” an output.\n\nAs you can see, the notion of a pipeline is a generalization of the notion of a model. From this point forward, unless stated otherwise, when I refer to model training, saving, deployment, serving, monitoring, or post-production maintenance, I mean the entire pipeline.\n\nBefore we consider the challenge of training a model, we need to decide how to measure the model quality. Often, we have a choice between several competing models, so-called model candidates, but only one will be deployed in production.\n\n5.5 Assessing Model Performance\n\nRemember, the holdout data consists of examples the learning algorithm didn’t see during training. If our model performs well on a holdout set, we can say our model generalizes well and is of good quality or, simply, that it’s good. The most common way to get a good model is to compare diﬀerent models by calculating a performance metric on the holdout data.\n\n5.5.1 Performance Metrics for Regression\n\nRegression and classiﬁcation models are assessed using diﬀerent metrics. Let’s ﬁrst consider performance metrics for regression: mean squared error (MSE), median absolute error (MAE), and almost correct predictions error rate (ACPER).\n\nThe metric most often used to quantify the performance of a regression model is the same as the cost function: mean squared error (MSE), deﬁned as,\n\nMSE(f) def=\n\n1\n\nN\n\nX\n\ni=1...N\n\n(f(xi) − yi)2,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(1)\n\n14",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "where f is the model that takes a feature vector x as input and outputs a prediction, and i, ranging from 1 to N, denotes the index of an example from a dataset.\n\nA well-ﬁtting regression model predicts values close to the observed data values. The mean model, which always predicts the average of the training data labels, generally would be used if there were no informative features. Therefore, the regression model should ﬁt better than that of the mean model. Thus, the mean model acts as a baseline. If the regression model MSE is greater than the baseline MSE, then we have a problem in our regression model. It may be overﬁtting or underﬁtting (we consider these in Section 5.8). It could also be that the problem was deﬁned with an error, or the programming code contains a bug.\n\nIf the data contains outliers, the examples very far from the “true” regression line, they can signiﬁcantly aﬀect the value of MSE. By deﬁnition, the squared error for such outlying examples will be high. In such situations, it is better to apply a diﬀerent metric, the median absolute error, MdAE:\n\nMdAE def= median\n\n(cid:16)\n\n{|f(xi) − yi|}N i=1\n\n(cid:17)\n\n,\n\nwhere {|f(xi) − yi|}N i=1 to N, on which the evaluation of the model is performed.\n\ndenotes the set of absolute error values for all examples, from i = 1\n\nThe almost correct predictions error rate (ACPER) is the percentage of predictions that is within p percentage of the true value. To calculate ACPER, proceed as follows:\n\n1. Deﬁne a threshold percentage error that you consider acceptable (let’s say 2%). 2. For each true value of the target yi, the desired prediction should be between yi +0.02yi and yi − 0.02yi.\n\n3. By using all examples i = 1,...,N, calculate the percentage of predicted values fulﬁlling the above rule. This will give the value of the ACPER metric for your model.\n\n5.5.2 Performance Metrics for Classiﬁcation\n\nFor classiﬁcation, things are a little more complicated. The most widely used metrics to assess a classiﬁcation model are:\n\nprecision-recall, • accuracy, • cost-sensitive accuracy, and • area under the ROC curve (AUC).\n\nTo simplify, I will illustrate with a binary classiﬁcation problem. Where necessary, I show how to extend the approach to the multiclass case.\n\nFirst, we need to understand the confusion matrix.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 2358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "A confusion matrix is a table that summarizes how successful the classiﬁcation model is at predicting examples belonging to various classes. One axis of the confusion matrix is the class that the model predicted; the other axis is the actual label. Let’s say, our model predicts classes “spam” and “not_spam”:\n\nspam (predicted)\n\nnot_spam (predicted)\n\nspam (actual) not_spam (actual)\n\n23 (TP) 12 (FP)\n\n1 (FN) 556 (TN)\n\nThe above matrix shows that out of 24 actual spam examples, the model correctly classiﬁed 23. In this case, we say that we have 23 true positives or TP = 23. The model incorrectly classiﬁed 1 spam example as not_spam. In this case, we have 1 false negative, or FN = 1. Similarly, out of 568 actual not_spam examples, the model classiﬁed correctly 556 and incorrectly 12 examples (556 true negatives, TN = 556, and 12 false positives, FP = 12).\n\nThe confusion matrix for multiclass classiﬁcation has as many rows and columns as there are diﬀerent classes. It can help you to determine mistake patterns. For example, a confusion matrix could reveal that a model trained to recognize diﬀerent species of animals tends to mistakenly predict “cat” instead of “panther,” or “mouse” instead of “rat.” In this case, you can add more labeled examples of these species to help the learning algorithm “see” the diﬀerence between those animals. Alternatively, you might add features that would help the learning algorithm do better at distinguishing between those pairs of species.\n\nThe confusion matrix is used to calculate three performance metrics: precision, recall, and accuracy. Precision and recall are most frequently used to assess a binary model.\n\nPrecision is the ratio of true positive predictions to the overall number of positive predictions:\n\nprecision def=\n\nTP TP + FP.\n\nRecall is the ratio of true positive predictions to the overall number of positive examples:\n\nrecall def=\n\nTP TP + FN.\n\nTo understand the meaning and the importance of precision and recall for model assessment, it’s useful to think about the prediction problem as the problem of research of documents in a database using a query. The precision is the proportion of relevant documents actually found in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine, compared to the total number of relevant documents that should have been returned.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 2451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "In spam detection, we want to have high precision, to avoid wrongly placing a legitimate message in our spam folder. We are willing to tolerate lower recall, since we can deal with some spam messages in our inbox.\n\nIn practice, we choose between high precision or high recall. It’s practically impossible to have both. This is called the precision-recall tradeoﬀ. We can achieve either by various means:\n\nby assigning a higher weigh to the examples of a speciﬁc class. For example, SVM in scikit-learn accepts weights of classes as input;\n\nby tuning hyperparameters to maximize either precision or recall on the validation set; • by varying the decision threshold for algorithms that return prediction scores. Let’s say we have a logistic regression model or a decision tree. To increase precision (at the cost of a lower recall), we can decide that the prediction will be positive only if the score returned by the model is higher than 0.9 (instead of the default value of 0.5).\n\nEven if precision and recall are deﬁned for binary classiﬁcation, you can also use them to assess a multiclass classiﬁcation model. First select a class for which you want to assess these metrics. Then you consider all examples of the selected class as positives and all examples of the remaining classes as negatives.\n\nIn practice, to compare the performance of two models, you would prefer to have only one number that represents the performance of each model. For example, you would like to avoid situations where the ﬁrst model has a higher precision, when the second model has a higher recall: if it’s the case, which model is better?\n\nOne way to compare models based on one number is to threshold the minimum acceptable value for one metric, say recall, and then only compare models based on the value of another metric. For example, say you will accept any model whose recall is above 90%. Then you will give preference to the model whose precision is the highest (assuming that its recall is above 90%). This technique is known as optimizing and satisﬁcing technique.\n\nSome practitioners use a combination of precision and recall called F-measure, also known as F-score. The traditional F-measure, or F1-score, is the harmonic mean of precision and recall:\n\nF1 =\n\n(cid:18)\n\n2 recall−1 + precision−1\n\n(cid:19)\n\n= 2 ×\n\nprecision × recall precision + recall\n\nMore generally, F-measure is parametrized with a positive real β, chosen such that recall is considered β times as important as precision:\n\nFβ = (1 + β2) ×\n\nprecision × recall (β2 × precision) + recall\n\nTwo commonly used values for β are 2, which weighs recall twice as high as precision, and 0.5, which weighs recall twice as low as precision.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "You should ﬁnd a way to combine the two metrics that works best for your problem. Besides F-score, there are other ways to obtain a single number by combining multiple metrics:\n\nsimple average, or weighted average of metrics; • threshold n − 1 metrics and optimize the nth (a generalization of the above optimizing and satisﬁcing technique);\n\ninvent your own domain-speciﬁc “recipe.”\n\nAccuracy is given by the number of correctly classiﬁed examples, divided by the total number of classiﬁed examples. In terms of the confusion matrix, it is given by:\n\naccuracy def=\n\nTP + TN TP + TN + FP + FN\n\nAccuracy is a useful metric when errors in predicting all classes are judged to be equally important. It’s the case, for example, for object recognition for a domestic robot: a chair is no more important than a table. In the case of the spam/not spam prediction, this probably would not be so. Likely, you would tolerate false negatives more than false positives. Remember, a false positive is when your friend sends you an email, but the model places it in the spam folder and you don’t see it. A false negative, a situation in which a spam message gets to the inbox, is less of a problem.\n\nFor dealing with the situations in which diﬀerent classes have diﬀerent importance, a useful metric is cost-sensitive accuracy. First, assign a cost (a positive number) to both types of mistakes: FP and FN. Then compute the counts TP, TN, FP, FN as usual, and multiply the counts for FP and FN by their corresponding costs before calculating the accuracy using Equation 2, above.\n\nAccuracy measures the performance of the model for all classes at once, and it conveniently returns a single number. However, accuracy is not a good performance metric when the data is imbalanced. In an imbalanced dataset, examples belonging to some class or a few classes constitute the vast majority, while other classes include very few examples. Imbalanced training data can signiﬁcantly and adversely aﬀect the model. We will talk more about dealing with the imbalanced data in Section ?? of Chapter 6.\n\nFor imbalanced data, a better metric is per-class accuracy. First, calculate the accuracy of prediction for each class {1,...,C}, and then take an average of C individual accuracy measures. For the above confusion matrix of the spam detection problem, the accuracy for the class “spam” is 23/(23 + 1) = 0.96, the accuracy for the class “not_spam” is 556/(12 + 556) = 0.98. The per-class accuracy is then (0.96 + 0.98)/2 = 0.97.\n\nPer-class accuracy will not be an appropriate model quality measure for a multiclass clas- siﬁcation problem where many classes have very few examples (roughly, less than a dozen examples per class). In that case, the accuracy values obtained for the binary classiﬁcation problems corresponding to these minority classes will not be statistically reliable.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(2)\n\n18",
      "content_length": 2923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Cohen’s kappa statistic is a performance metric that applies to both multiclass and imbalanced learning problems. The advantage of this metric over accuracy is that Cohen’s kappa tells you how much better your classiﬁcation model is performing, compared to a classiﬁer that randomly guesses a class according to the frequency of each class.\n\nCohen’s kappa is deﬁned as:\n\nκ def= po − pe 1 − pe\n\n,\n\nwhere po is called the observed agreement, and pe is the expected agreement. Let’s look once again at a confusion matrix:\n\nclass1 (predicted)\n\nclass2 (predicted)\n\nclass1 (actual) class2 (actual)\n\na c\n\nb d\n\nThe observed agreement po is obtained from the confusion matrix as,\n\npo\n\ndef=\n\na + d a + b + c + d\n\n.\n\nThe expected agreement pe, in turn, is obtained as pe\n\ndef= pclass1 + pclass2, where,\n\npclass1\n\ndef=\n\na + b a + b + c + d\n\n×\n\na + c a + b + c + d\n\n,\n\nand\n\npclass2\n\ndef=\n\nc + d a + b + c + d\n\n×\n\nb + d a + b + c + d\n\nThe value of Cohen’s kappa is always less than or equal to 1. Values of 0 or less indicate that the model has a problem. While there is no universally accepted way to interpret the values of Cohen’s kappa, it’s usually considered that values between 0.61 and 0.80 indicate that the model is good, and values 0.81 or higher suggest that the model is very good.\n\nThe ROC curve (stands for “receiver operating characteristic;” the term comes from radar engineering) is a commonly-used method of assessing classiﬁcation models. ROC curves use a combination of the true positive rate (deﬁned exactly as recall) and false positive rate (the proportion of negative examples predicted incorrectly) to build up a summary picture of the classiﬁcation performance.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "The true positive rate (TPR) and the false positive rate (FPR) are respectively deﬁned as,\n\nTPR def=\n\nTP TP+FN\n\nand FPR def=\n\nFP FP+TN\n\nROC curves can only be used to assess classiﬁers that return a score (or a probability) of prediction. For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves.\n\nTo draw a ROC curve, you ﬁrst discretize the range of the score. For instance, you can discretize the range [0,1] like this: [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]. Then, use each discrete value as the prediction threshold for your model. For example, if you want to calculate TPR and FPR for the threshold equal to 0.7, you apply the model to each example and get the score. If the score is greater than or equal to 0.7, you predict the positive class. Otherwise, you predict the negative class.\n\nLook at the illustration in Figure 5. It’s easy to see that if the threshold equals 0, all our predictions will be positive, so both TPR and FPR will equal 1 (the upper right corner). On the other hand, if the threshold equals 1, then no positive prediction will be possible. Both TPR and FPR will equal 0, which corresponds to the lower-left corner.\n\nThe greater the area under the ROC curve (AUC), the better the classiﬁer. A classiﬁer with an AUC greater than 0.5 is better than a model that classiﬁes at random. If AUC is lower than 0.5, then something is wrong, most likely a bug in the code or wrong labels in the data. A perfect classiﬁer would have an AUC of 1. In practice, you obtain a good classiﬁer by selecting the value of the threshold that gives TPR close to 1 while keeping FPR near 0.\n\nROC curves are popular because they are relatively simple to understand. They capture more than one aspect of the classiﬁcation, by taking both false positives and false negatives into account. They allow the analyst to easily and visually compare diﬀerent model performances.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Figure 5: The area under the ROC curve (shown in grey).\n\n5.5.3 Performance Metrics for Ranking\n\nPrecision and recall can be naturally applied to the ranking problem. Recall that it’s convenient to think of these two metrics as measuring the quality of document search results. Precision is the proportion of relevant documents actually found in the list of all returned documents. The recall is the ratio of the relevant documents returned by the search engine, compared to the total number of the relevant documents that should have been returned.\n\nThe drawback of measuring the quality of ranking models with precision and recall is that these metrics treat all retrieved documents equally. A relevant document listed at position k is worth just as much as a relevant document at the top of the list. This is usually not what\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "we want in document retrieval. When a human looks at search results, the few top-most results matter more than the results shown at the bottom of the list.\n\nDiscounted cumulative gain (DCG) is a popular measure of ranking quality in search engines. DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower positions.\n\nTo understand discounted cumulative gain, we introduce a measure called cumulative gain.\n\nCumulative gain (CG) is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position p is deﬁned as:\n\nCGp\n\ndef=\n\np X\n\nreli,\n\ni=1\n\nwhere reli is the graded relevance of the result at position i. Generally, graded relevance reﬂects the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as “not relevant,” “somewhat relevant,” “relevant,” or “very relevant”). To use it in the above formula, reli must be numeric, for example, ranging from 0 (the document at position i is entirely irrelevant to the query) to 1 (the document at position i is maximally relevant to the query). Alternatively, reli can be binary: 0 when the document is not relevant to the query, and 1 when relevant. Notice that CGp is independent of the position each document holds in the ranked result list. It only characterizes the documents ranked up to position p as relevant or irrelevant to the query.\n\nDiscounted cumulative gain is based on two assumptions:\n\n1. Highly relevant documents are more useful when appearing earlier in the result list. 2. Highly relevant documents are more useful than marginally relevant documents, while the latter, in turn, are more useful than non-relevant documents.\n\nFor a given search result, DCG accumulated at a particular rank position p is often deﬁned as:\n\nDCGp\n\ndef=\n\np X\n\ni=1\n\nlog\n\nreli (i + 1)\n\n2\n\n= rel1 +\n\np X\n\ni=2\n\nlog\n\nreli (i + 1).\n\n2\n\nAn alternative formulation of DCG, commonly used in industry and data science competitions such as Kaggle, places a stronger emphasis on retrieving relevant documents:\n\nDCGp\n\ndef=\n\np X\n\ni=1\n\n2reli − 1 log\n\n(i + 1).\n\n2\n\nFor a query, the normalized discounted cumulative gain (nDCG), is deﬁned as:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 2371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "nDCGp\n\ndef=\n\nDCGp IDCGp\n\n,\n\nwhere IDCG is the ideal discounted cumulative gain,\n\nIDCGp\n\ndef=\n\n|RELp| X\n\ni=1\n\n2reli − 1 log\n\n(i + 1),\n\n2\n\nand RELp represents the list of the documents relevant to the query in the corpus up to position p (ordered by their relevance). So, RELp is the ideal ranking, up to position p, that the search engine ranking algorithm (or model) should have returned for the query. The nDCG values for all queries are usually averaged to obtain a performance measure for a search engine ranking algorithm or model.\n\nLet’s consider the following example. Let a search engine return a list of documents in response to a search query. We ask a ranker (a human) to judge each document’s relevance. The ranker must assign a score from 0 to 3, where 0 means not relevant, 3 means highly relevant, while 1 and 2 mean “somewhere in between.” Say the documents appeared in this order:\n\nD1,D2,D3,D4,D5.\n\nOur ranker provides the following relevance scores:\n\n3,1,0,3,2.\n\nThis means that document D1 has a relevance of 3, D2 has a relevance of 1, D3 has a relevance of 0, and so on. The cumulative gain of this search result, up to position p = 5, is,\n\nCG5 =\n\n5 X\n\nreli = 3 + 1 + 0 + 3 + 2 = 9.\n\ni=1\n\nYou can see that changing the order of any documents will not aﬀect the value of cumulative gain. Now we will calculate the discounted cumulative gain designed, with the presence of the logarithmic discounting, to have a higher value if highly relevant documents appear early in the for each i: result list. To calculate DCG5, let’s calculate the value of the expression\n\nreli log2(i+1)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 1652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "i\n\n1 2 3 4 5\n\nreli\n\n3 1 0 3 2\n\nlog\n\n(i + 1)\n\n2 1.00 1.58 2.00 2.32 2.58\n\nreli log2(i+1) 3.00 0.63 0.00 1.29 0.77\n\nSo DCG5 of this ranking is given by 3.00 + 0.63 + 0.00 + 1.29 + 0.77 = 5.70. Now, if we switch the positions of D1 and D2, the value of DCG5 will become lower. This is because a less relevant document is now placed higher in the ranking, while a more relevant document is discounted more by being placed in a lower position.\n\nTo calculate the normalized discounted cumulative gain, nDCG5, we ﬁrst need to ﬁnd the value of the discounted cumulative gain of the ideal ordering, IDCG5. The ideal ordering, according to the relevance scores, is 3,3,2,1,0. The value of IDCG5 is then equal to 3.00 + 1.89 + 1.00 + 0.43 + 0.0 = 6.32. Finally, nDCG5 is given by,\n\nnDCG5 =\n\nDCG5 IDCG5\n\n=\n\n5.70 6.32\n\n= 0.90.\n\nTo obtain nDCG for a collection of test queries and the corresponding lists of search results, we average the values of nDCGp obtained for each individual query. The advantage of using the normalized discounted cumulative gain over other measures is that the values of nDCGp obtained for diﬀerent values of p are comparable. This property is useful when the number p of relevance scores, provided by the rankers, is diﬀerent for diﬀerent queries.\n\nNow that we have a performance metric, we can use it to compare models in the process known as hyperparameter tuning.\n\n5.6 Hyperparameter Tuning\n\nHyperparameters play an important role in the model training process. Some hyperparameters inﬂuence the speed of training, but the most important hyperparameters control the two tradeoﬀs: bias-variance and precision-recall.\n\nHyperparameters aren’t optimized by the learning algorithm itself. The data analyst “tunes” hyperparameters by experimenting with combinations of values, one per hyperparameter. Each machine learning model and each learning algorithm have a unique set of hyperparameters. Furthermore, every step in your entire machine learning pipeline, data pre-processing, feature extraction, model training, and making predictions, can have its own hyperparameters.\n\nFor example, in data pre-processing, the hyperparameters could specify whether to use data-augmentation or using which technique to ﬁll missing values. In feature engineering,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 2320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Hyperparameter 1 (e.g. learning rate)Hyperparameter 2 (e.g. number of trees)\n\npairs of discretevalues\n\nFigure 6: Grid search for two hyperparameters: each green circle represents a pair of hyperparameter values.\n\na hyperparameter could deﬁne which feature selection technique to apply. When making predictions with a model that returns a score, a hyperparameter could specify the decision threshold for each class.\n\nBelow, we consider several popular hyperparameter tuning techniques.\n\n5.6.1 Grid Search\n\nGrid search is the simplest hyperparameter tuning technique. It’s used when the number of hyperparameters and their range is not too large.\n\nWe explain it for the problem of tuning two numerical hyperparameters. The technique consists of discretizing each of the two hyperparameters, and then evaluating each pair of discrete values, as shown in Figure 6.\n\nEach evaluation consists of:\n\n1) conﬁguring a pipeline with a pair of hyperparameter values, 2) applying the pipeline to the training data and training a model, and 3) computing the performance metric for the model on the validation data.\n\nThe pair of hyperparameter values that results in the best performing model is then selected\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nfor training the ﬁnal model.\n\nThe Python code below uses grid search with cross-validation.2 It shows how to optimize the hyperparameters of the simple two-stage scikit-learn pipeline considered above:\n\nfrom sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.model_selection import GridSearchCV\n\n# Define a pipeline pipe = Pipeline([('dim_reduction', PCA()), ('model_training', SVC())])\n\n# Define hyperparamer values to try param_grid = dict(dim_reduction__n_components=[2, 5, 10], \\ model_training__C=[0.1, 10, 100])\n\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n# Make a prediction pipe.predict(new_example)\n\nIn the above example, we use grid search to try the values [2,5,10] of the hyperparameter n_components of PCA, and the values [0.1,10,100] of the hyperparameter C of SVM.\n\nTrying multiple combinations of hyperparameters could be time-consuming for large datasets. There are more eﬃcient techniques, such as random search, coarse-to-ﬁne search, and Bayesian hyperparameter optimization.\n\n5.6.2 Random Search\n\nRandom search diﬀers from grid search in that you do not provide a discrete set of values to explore for each hyperparameter. Instead, you provide a statistical distribution for each hyperparameter from which values are randomly sampled. Then set the total number of combinations you want to evaluate, as shown in Figure 7.\n\n2We talk about cross-validation in Subsection 5.6.5.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Hyperparameter 1Hyperparameter 2\n\nFigure 7: Random search for two hyperparameters and 16 pairs to test.\n\nHyperparameter 1Hyperparameter 2\n\nFigure 8: Coarse-to-ﬁne search for two hyperparameters: 16 coarse random search pairs to test and one grid search in the region of the highest value found using the random search.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "5.6.3 Coarse-to-Fine Search\n\nIn practice, analysts often use a combination of grid search and random search called coarse- to-ﬁne search. This technique uses a coarse random search to ﬁrst ﬁnd the regions of high potential. Then, using a ﬁne grid search in these regions, one ﬁnds the best values for hyperparameters, as shown in Figure 8.\n\nYou can decide to only explore one high-potential region or several such regions, depending on the available time and computational resources.\n\n5.6.4 Other Techniques\n\nBayesian techniques diﬀer from random and grid searches in that they use past evaluation results to choose the next values to evaluate. In practice, this allows Bayesian hyperparameter optimization techniques to ﬁnd better values of hyperparameters in less time.\n\nThere are also gradient-based techniques, evolutionary optimization techniques, and other algorithmic hyperparameter tuning methods. Most modern machine learning libraries imple- ment one or more such techniques. There are also hyperparameter tuning libraries that can be used to tune hyperparameters of virtually any learning algorithm, including the algorithms you programmed yourself.\n\n5.6.5 Cross-Validation\n\nGrid search and other techniques of hyperparameter tuning discussed above are used when you have a good-sized validation set.3 When you don’t, a common technique of model evaluation is cross-validation. Indeed, when you have few training examples, it could be prohibitive to have both validation and test sets. You would prefer to use more data to train the model. In such a case, you should only split your data in two: a training and a test set. Then use cross-validation on the training set to simulate a validation set.\n\nCross-validation works as follows. First, you ﬁx the values of the hyperparameters to evaluate. Then you split your training set into several subsets of the same size. Each subset is called a fold. Typically, ﬁve-fold cross-validation is used, and you randomly split your training data into ﬁve folds: {F1,F2,...,F5}. Each Fk, k = 1,...,5, contains 20% of your training data. Then you train ﬁve models in a speciﬁc manner. To train the ﬁrst model, f1, you use all examples from folds F2, F3, F4, and F5 as the training set, and the examples from F1 as the validation set. To train the second model, f2, you use the examples from folds F1, F3, F4, and F5 to train, and the examples from F2 to validate. You continue training models fk iteratively4 for all remaining folds, and compute the value of the metric of interest on each\n\n3A decent validation set contains at least a hundred examples, and each class in the set is represented by\n\nat least a couple of dozen examples.\n\n4The process of cross-validation is easier to illustrate as an iterative process; though, one can, of course,\n\nbuild all ﬁve models F1 to F5 in parallel.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28",
      "content_length": 2896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "validation set, from F1 to F5. Then you average the ﬁve values of the metric to get the ﬁnal value. More generally, in n-fold cross-validation, you train model fn on all folds, except for the n-th fold Fn. You can use grid search, random search, or any other such technique with cross-validation to ﬁnd the best values of hyperparameters. Once you have found those values, you typically use the entire training set to train the ﬁnal model by using the best values of hyperparameters found via cross-validation. Finally, you assess the ﬁnal model using the test set.\n\nWhile ﬁnding the best values of hyperparameters is tempting, it might be unrealistic to try all of them. Remember that time is precious, and perfect is often an enemy of good. Deploy a “good enough” model to production, then continue to run the search of the ideal values for hyperparameters (for weeks if it is what it takes).\n\nNow, let’s consider the challenge of training a shallow model.\n\n5.7 Shallow Model Training\n\nShallow models make predictions based directly on the values in the input feature vector. Most popular machine learning algorithms produce shallow models. The only kind of deep models commonly used are deep neural networks. We consider a strategy to train them in Section ?? of the next chapter.\n\n5.7.1 Shallow Model Training Strategy\n\nA typical model training strategy for shallow learning algorithms looks as follows:\n\n1. Deﬁne a performance metric P. 2. Shortlist learning algorithms. 3. Choose a hyperparameter tuning strategy T. 4. Pick a learning algorithm A. 5. Pick a combination H of hyperparameter values for algorithm A using strategy T. 6. Use the training set and train a model M using algorithm A parametrized with hyperparameter values H.\n\n7. Use the validation set and calculate the value of metric P for model M. 8. Decide: a. If there are still untested hyperparameter values, pick another combination H of\n\nhyperparameter values using strategy T and go back to step 6.\n\nb. Otherwise, pick a diﬀerent learning algorithm A and go back to step 5, or proceed\n\nto step 9 if there are no more learning algorithms to try.\n\n9. Return the model for which the value of metric P is maximized.\n\nIn the above strategy, step 1, you deﬁne the performance metric for your problem. As we have seen in Section 5.5, it is a mathematical function or a subroutine that takes a model and a dataset as input, and produces a numerical value that reﬂects how well the model works.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29",
      "content_length": 2519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n1\n\n2\n\n3\n\n4\n\n5\n\nIn step 2, you choose candidate algorithms and then shortlist some of them (usually, two or three). To do that, you can use the selection criteria considered in Section 5.3.\n\nIn step 3, you choose a hyperparameter tuning strategy. It is a sequence of actions that generates the combinations of hyperparameter values to test. We have considered several hyperparameter-tuning strategies in Section 5.6.\n\n5.7.2 Saving and Restoring the Model\n\nOnce you trained a model or a pipeline, you must save it to a ﬁle so that it can be deployed to production and then used for scoring. Both model and pipeline can be serialized. In Python, Pickle is typically used for serialization (saving) and deserialization (restoring) of objects. In R, it’s RDS.\n\nHere’s how model serialization/deserialization is done in Python:\n\nimport pickle from sklearn.svm import SVC from sklearn import datasets\n\n# Prepare data X, y = datasets.load_iris(return_X_y=True)\n\n# Instantiate the model model = SVC()\n\n# Train the model model.fit(X, y)\n\n# Save the model to file pickle.dump(model, open(\"model_file.pkl\", \"wb\"))\n\n# Restore the model from file restored_model = pickle.load(open(\"model_file.pkl\", \"rb\"))\n\n# Make a prediction prediction = restored_model.predict(new_example)\n\nA similar code in R would look as follows:\n\nlibrary(\"e1071\")\n\n# Prepare data attach(iris) X <- subset(iris, select=-Species)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\ny <- Species\n\n# Train the model model <- svm(X,y)\n\n# Save the model to file saveRDS(model, \"./model_file.rds\")\n\n# Restore the model from file restored_model <- readRDS(\"./model_file.rds\")\n\n# Make a prediction prediction <- predict(restored_model, new_example) Now, let’s talk about the particularities of the model training process that analysts must take care of in practice to produce an optimal model.\n\n5.8 Bias-Variance Tradeoﬀ\n\nDeveloping a model includes both searching for an optimal algorithm, as well as ﬁnding the best performing hyperparameters. Tweaking the hyperparameters actually controls two tradeoﬀs. We already discussed the ﬁrst one: the precision-recall tradeoﬀ. The second one, equally important, is the bias-variance tradeoﬀ.\n\n5.8.1 Underﬁtting\n\nThe model is said to have a low bias if it ablely predicts the training data labels. If the model makes too many mistakes on the training data, we say that it has a high bias, or that the model underﬁts the training data. There could be several reasons for underﬁtting:\n\nthe model is too simple for the data (for example linear models often underﬁt); • the features are not informative enough; • you regularize too much (we talk about regularization in the next section).\n\nAn example of underﬁtting in regression is shown in Figure 9 (left). The regression line doesn’t repeat the bends of the line to which the data seemingly belongs. The model oversimpliﬁes the data. The possible solutions to the problem of underﬁtting include:\n\ntrying a more complex model, • engineering features with higher predictive power, • adding more training data, when possible, and • reducing regularization.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "4\n\n20\n\n10\n\n0\n\n10\n\n2\n\n0\n\n2\n\ntraining examples\n\n30\n\ndegree 1 (underfit)\n\ntraining examples\n\ndegree 2 (fit)\n\n30\n\n10\n\n2\n\n0\n\n20\n\n10\n\n4\n\n0\n\n2\n\n0\n\n4\n\n2\n\n10\n\n0\n\n20\n\n30\n\ndegree 15 (overfit)\n\ntraining examples\n\n2\n\n10\n\nUnderﬁtting\n\nGood ﬁt\n\nOverﬁtting\n\nFigure 9: Examples of underﬁtting (linear model), good ﬁt (quadratic model), and overﬁtting (polynomial of degree 15).\n\n5.8.2 Overﬁtting\n\nOverﬁtting is another problem a model can exhibit. The model that overﬁts usually predicts the training data labels very well, but works poorly on the holdout data.\n\nAn example of overﬁtting in regression is shown in Figure 9 (right). The regression line predicts almost perfectly the targets for almost all training examples, but will likely make signiﬁcant errors on new data if you decide to use it for predictions.\n\nYou will ﬁnd another name for overﬁtting in the literature: high variance. The model is unduly sensitive to small ﬂuctuations in the training set. If you sampled the training data diﬀerently, the result would be a signiﬁcantly diﬀerent model. These overﬁtting models perform poorly on the holdout data, since holdout and training data are sampled from the dataset independently of one another. So, the small ﬂuctuations in the training and holdout data are likely to be diﬀerent.\n\nSeveral reasons can lead to overﬁtting:\n\nthe model is too complex for the data. Very tall decision trees or a very deep neural network often overﬁt;\n\nthere are too many features and few training examples; and • you don’t regularize enough.\n\nSeveral solutions to overﬁtting are possible:\n\nuse a simpler model. Try linear instead of polynomial regression, or SVM with a linear kernel instead of radial basis function (RBF), or a neural network with fewer layers/units;5\n\n5While reducing the number of model parameters is generally recommended to reduce overﬁtting and improve the generalization of the model, the phenomenon of deep double descent sometimes proves otherwise. The phenomenon was observed in various architectures, including CNN and transformers: validation performance ﬁrst improves, then gets worse, and then improves again with increasing model size.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "reduce the dimensionality of examples in the dataset; • add more training data, if possible; and, • regularize the model.\n\n5.8.3 The Tradeoﬀ\n\nIn practice, by trying to reduce variance, you increase bias, and vice versa. In other words, reducing overﬁtting leads to underﬁtting, and the other way around. This is called the bias-variance tradeoﬀ: by trying too hard to build a model that performs perfectly on the training data, you end up with a model that performs poorly on the holdout data.\n\nWhile many factors determine whether the model performs well on the training data, the most important factor is the complexity of the model. A suﬃciently complex model will learn to memorize all training examples and their labels and, thus, will not make prediction errors when applied to the training data. It will have low bias. However, a model relying on memorization will not be able to correctly predict labels of previously unseen data. It will have high variance.\n\nAs the model complexity grows, the typical evolution of the average prediction error of a model when applied to the training and holdout data is shown in Figure 10.\n\nThe zone you would like to be in is the “zone of solutions,” the light-blue rectangle where both bias and variance are low. Once in this zone, you can ﬁne-tune the hyperparameters to reach the needed precision-recall ratio, or optimize another model performance metric appropriate for your problem.\n\nTo reach the zone of solutions, you can either,\n\nmove to the right by increasing the complexity of the model, and, by so doing, reducing its bias, or\n\nmove to the left by regularizing the model to reduce variance by making the model simpler (we talk about regularization in the next section).\n\nIf you work with shallow models, like linear regression, you can increase the complexity by switching to higher-order polynomial regression. Similarly, you can increase the depth of the decision tree, or use polynomial or RBF kernels, in support vector machine (SVM) instead of the linear kernel. Ensemble learning algorithms, based on the idea of boosting, allow bias reduction by combining several (usually, hundreds of) high-bias “weak” models.\n\nAs of July 2020, we don’t yet fully understand why it happens.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33",
      "content_length": 2296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "averageerror\n\nlowermodel complexityhighermodel complexity\n\nholdouterrortrainingerrorhigh bias(underﬁtting)high variance(overﬁtting)zone ofsolutions\n\nFigure 10: Bias-variance tradeoﬀ.\n\nIf you work with neural networks, you can increase the model’s complexity by increasing its size: the number of units per layer, and the number of layers. Training a neural network model longer (i.e., for more epochs) also usually results in lower bias. The advantage of using neural networks, with respect to the bias-variance tradeoﬀ, is that you can slightly increase the size of the network and observe a slight decrease in bias. Most popular shallow models and the associated learning algorithms cannot provide you such ﬂexibility.\n\nIf, by increasing the complexity of your model, you ﬁnd yourself in the right-hand side of the graph in Figure 10, you have to reduce the variance of the model. The most common way to do that is to apply regularization.\n\n5.9 Regularization\n\nRegularization is an umbrella term for methods that force a learning algorithm to train a less complex model. In practice, it leads to higher bias, but signiﬁcantly reduces the variance.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "The two most widely used types of regularization are L1 and L2 regularization. The idea is quite simple. To create a regularized model, we modify the objective function. This is the expression optimized by the learning algorithm when training the model. Regularization adds a penalizing term whose value is higher when the model is more complex.\n\nFor simplicity, we will illustrate regularization using linear regression, but same principle can be applied to a wide variety of models. Let x be a two-dimensional feature vector (cid:2)x(1),x(2)(cid:3). Recall the linear regression objective:\n\nmin w(1),w(2),b\n\n\" 1 N\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\ndef= f(xi), and f is the equation of the regression line. The equation In the above equation, fi of the linear regression line f will have the form f = w(1)x(1) + w(2)x(2) + b. The learning algorithm will deduce the values of parameters w(1), w(2), and b from the training data by minimizing the objective. A model is considered less complex if some of the parameters w(·) are close to or equal to zero.\n\n5.9.1 L1 and L2 Regularization\n\nAn L1-regularized objective in Equation 3 looks like this:\n\nmin w(1),w(2),b\n\n\"\n\nC ×\n\n(cid:16)\n\n|w(1)| + |w(2)|\n\n(cid:17)\n\n+\n\n1\n\nN\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\nwhere C is a hyperparameter that controls the importance of regularization. If we set C to zero, the model becomes a standard non-regularized linear regression model. On the other hand, if we set C to a high value, the learning algorithm will try to set most w(·) to a value close or equal to zero to minimize the objective. The model will become very simple, which can lead to underﬁtting. The role of the data analyst is to ﬁnd such a value of the hyperparameter C that doesn’t increase the bias too much, but reduces the variance to a level reasonable for the problem at hand.\n\nAn L2-regularized objective in our two-dimensional setting looks like this:\n\nmin w(1),w(2),b\n\n\"\n\nC ×\n\n(cid:16)\n\n(w(1))2 + (w(2))2(cid:17)\n\n+\n\n1\n\nN\n\n×\n\nN X\n\ni=1\n\n(fi − yi)2\n\n#\n\n,\n\nIn practice, L1 regularization produces a sparse model, assuming the value of hyperpa- rameter C is great enough. This is a model where most of its parameters equal exactly zero. So, as discussed in the previous chapter, L1 implicitly performs feature selection by\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(3)\n\n(4)\n\n(5)\n\n35",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "deciding which features are essential for prediction, and which are not. This property of L1 regularization is useful when we want to increase model explainability. However, if our goal is to maximize the model performance on the holdout data, then L2 usually gives better results.\n\nIn the literature, you will also see the names lasso for L1 and ridge regularization for L2.\n\n5.9.2 Other Forms of Regularization\n\nL1 and L2 regularization methods can be combined in what’s called elastic net regularization.\n\nIn addition to being widely used with linear models, L1 and L2 are often used with neural networks and many other types of models that directly minimize an objective function.\n\nNeural networks can also beneﬁt from two other regularization techniques: dropout and batch-normalization. There are also non-mathematical methods that have a regularization eﬀect: data augmentation and early stopping. We will talk about these techniques in more detail in the next chapter, when we consider training neural networks.\n\n5.10 Summary\n\nBefore starting to work on the model, you should make several checks and decisions. First, make sure that the data conforms to the schema, as deﬁned by the schema ﬁle. Then, deﬁne an achievable level of performance, and choose a performance metric. Ideally, it should represent the model performance as a single number. Furthermore, it is important to establish a baseline that provides a reference point to compare your machine learning models. Finally, split your data into three sets: train, validation, and test.\n\nMost modern implementations of classiﬁcation learning algorithms require that the training examples have numerical labels, so you typically must transform your labels into numerical vectors. Two popular ways to do that are one-hot encoding (for binary and multiclass problems) and bag-of-words (for multi-label problems).\n\nTo choose a machine learning algorithm that would work best for your problem, ask yourself the following questions:\n\nDo the model’s predictions have to be explainable to a non-technical audience? If yes, you would prefer using less accurate, but more explainable algorithms, such as kNN, linear regression, and decision tree learning.\n\nCan your dataset be fully loaded into the RAM of your laptop or server? If not, you would prefer incremental learning algorithms.\n\nHow many training examples do you have in your dataset, and how many features does each example have? Some algorithms, including those used for training neural networks and random forests, can handle a huge number of examples and millions of features. Others are relatively modest in their capacity.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36",
      "content_length": 2699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Is your data linearly separable, or can it be modeled using a linear model? If yes, SVM with the linear kernel, linear and logistic regression, can be good choices. Otherwise, deep neural networks or ensemble models might work better.\n\nHow much time is a learning algorithm allowed to use to train a model? Neural networks are known to be slow to train. Simple algorithms like linear and logistic regression, or decision trees are much faster.\n\nHow fast must the scoring perform in production? Models like SVM, linear and logistic regression, as well as not very deep feedforward neural networks, are extremely fast at the prediction time. The scoring using deep and recurrent neural networks, as well as gradient boosting models, is slower.\n\nIf you don’t want to guess the best algorithm for your problem, a recommended approach is to spot-check several algorithms, and then test them on the validation set as a hyperparameter.\n\nA typical way to know how good is the model, is to calculate the value of a performance metric on the holdout data. There are performance metrics deﬁned for classiﬁcation and regressions models, as well as for ranking models.\n\nTweaking the values of hyperparameters controls two tradeoﬀs: precision-recall and bias- variance. By varying the complexity of the model, we can reach the so-called “zone of solutions,” a situation in which both bias and variance of the model are relatively low. The solution that optimizes the performance metric is usually found inside that zone.\n\nRegularization is an umbrella term for methods that force the learning algorithm to build a less complex model. In practice, that often leads to slightly higher bias, but signiﬁcantly reduces the variance. Two popular techniques of regularization are L1 and L2. In addition, neural networks beneﬁt from two other regularization techniques: dropout and batch normalization.\n\nMost modern machine learning packages and frameworks support the notion of a pipeline. A pipeline is a sequence of transformations the training data undergoes before it becomes a model. In a pipeline, each stage applies some transformation to the input it receives. Every stage receives the output of the previous stage, except for the ﬁrst stage. The ﬁrst stage receives the training dataset as input. The pipeline can be saved to a ﬁle similar to saving a model. It can be deployed to production and used to generate predictions.\n\nHyperparameters aren’t optimized by the learning algorithm itself. A data analyst must “tune” hyperparameters by experimenting with diﬀerent combinations of values. Grid search is the simplest and the most widely used hyperparameter tuning technique. It consists of discretizing the values of hyperparameters, and trying all combinations of values by 1) training a model for each combination of hyperparameters, and 2) computing the performance metric by applying each trained model to the validation set.\n\nA decent validation set contains at least a hundred examples, and each class in the set is represented by at least a couple of dozen examples. When you don’t have a decent validation set to tune your hyperparameters, you can use cross-validation.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37",
      "content_length": 3225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "6 Supervised Model Training (Part 2)\n\nIn this second part of our conversation about supervised model training, we consider such topics as training deep models, stacking models, handling imbalanced datasets, distribution shift, model calibration, troubleshooting and error analysis, and other best practices.\n\nCompared to shallow models, the model training strategy for deep neural networks has more moving parts. On the other hand, it’s more principled and better amenable to automation.\n\n6.1 Deep Model Training Strategy\n\nModel training starts with shortlisting several network architectures, also known as network topologies. If you work with image data and you want to build your model from scratch, then a convolutional neural network (CNN) with at least one convolutional layer, followed by a max-pooling layer, and one fully connected layer may be your default topology choice.\n\nIf you work with text or other sequence data, such as time series, you have a choice between a CNN, a gated recurrent neural network (such as Long Short Term Memory, LSTM, or gated recurrent units, GRU), or a Transformer.\n\nInstead of training your model from scratch, you can also start with a pre-trained model. Companies like Google and Microsoft have trained very deep neural networks with architec- tures optimized for image or natural language processing tasks.\n\nAmong the most used pre-trained models for image processing tasks are VGG16 and VGG19 (based on the Visual Geometry Group, VGG, architecture), InceptionV3 (based on the GoogLeNet architecture), and ResNet50 (based on the residual network architecture).\n\nFor natural language text processing, such pre-trained models as Bi-directional Encoder Representations from Transformer, BERT, (based on the Transformer architecture) and Em- beddings from Language Models, ELMo (based on the bi-directional LSTM architecture) often improve the quality of the model, compared to training a model from scratch.\n\nAn advantage of using pre-trained models is that these were trained on huge quantities of data available to its creators, but likely unavailable to you. Even if your dataset is smaller and not exactly similar to the one used to pre-train the model, the parameters learned by the pre-trained models may still be useful.\n\nYou can use a pre-trained model in two ways:\n\n1) use its learned parameters to initialize your own model, or 2) use the pre-trained model as a feature extractor for your model.\n\nIf you use the pre-trained model the former way, it gives you more ﬂexibility. The downside is you end up training a very deep neural network. That requires signiﬁcant computational\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 2686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "resources. In the latter case, you “freeze” the parameters of the pre-trained model and only train the parameters of added layers.\n\n6.1.1 Neural Network Training Strategy\n\nUsing an existing model to create a new model is called transfer learning. We will talk more on this topic in Section 6.1.10. For the moment, assume you are building a model from scratch, based on the architecture of your choice. A common strategy to build a neural network looks as follows:\n\n1. Deﬁne a performance metric P. 2. Deﬁne the cost function C. 3. Pick a parameter-initialization strategy W. 4. Pick a cost-function optimization algorithm A. 5. Choose a hyperparameter tuning strategy T. 6. Pick a combination H of hyperparameter values using the tuning strategy T. 7. Train model M, using algorithm A, parametrized with hyperparameters H, to optimize cost function C.\n\n8. If there are still untested hyperparameter values, pick another combination H of hyperparameter values using strategy T, and repeat step 7.\n\n9. Return the model for which the metric P was optimized.\n\nNow let’s discuss some of the steps of the above strategy in detail.\n\n6.1.2 Performance Metric and Cost Function\n\nStep 1 is similar to step 1 of the shallow model training strategy (Section ??): we deﬁne a metric that would allow comparing the performance of two models on the holdout data, and select the better of the two. An example of a performance metric is F-score or Cohen’s kappa.\n\nIn step 2, we deﬁne what our learning algorithm will optimize in order to train a model. If our neural network is a regression model, then, in most cases, the cost function is the mean squared error (MSE) deﬁned in Equation ?? in the previous chapter. Let’s repeat it here:\n\nMSE(f) def=\n\n1\n\nN\n\nX\n\ni=1...N\n\n(f(xi) − yi)2.\n\nFor classiﬁcation, a typical choice for the cost function is either categorical cross-entropy (for multiclass classiﬁcation) or binary cross-entropy (for binary and multi-label classiﬁca- tion).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 2018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Recall that when we train a neural network for multiclass classiﬁcation, we should represent labels using the one-hot encoding. Let C be the number of classes in our classiﬁcation problem. Let yi be a one-hot encoded label of example i, where i spans from 1 to N. Let yi,j denote the value in position j (where j spans from 1 to C) in example i. The categorical cross-entropy loss for classiﬁcation of example i is deﬁned as,\n\nCCEi\n\ndef= −\n\nC X\n\n[yi,j × log\n\n2\n\n(ˆyi,j)],\n\nj=1\n\nwhere ˆyi is the C-dimensional vector of prediction issued by the neural network for the input xi. The cost function is typically deﬁned as the sum of losses of individual examples:\n\nCCE def=\n\nN X\n\nCCEi .\n\ni=1\n\nIn binary classiﬁcation, the output of the neural network for the input feature vector xi, is a single value ˆyi, while the label of the example is a single value yi, just like in logistic regression. The binary cross-entropy loss for classiﬁcation of example i is deﬁned as,\n\nBCEi\n\ndef= −yi × log\n\n2\n\n(ˆyi) − (1 − yi) × log\n\n2\n\n(1 − ˆyi).\n\nSimilarly, the cost function for classiﬁcation of the training set is typically deﬁned as the sum of losses of individual examples:\n\nBCE def=\n\nN X\n\nBCEi .\n\ni=1\n\nBinary cross-entropy is also used in multi-label classiﬁcation. The labels are now C- dimensional bag-of-words vectors yi, while the predictions are C-dimensional vectors ˆyi, whose values ˆyi,j in each dimension j range between 0 and 1. The loss for the prediction of one label ˆyi is deﬁned as,\n\nBCEMi\n\ndef=\n\nC X\n\n[−yi,j × log\n\n2\n\n(ˆyi,j) − (1 − yi,j) × log\n\n2\n\n(1 − ˆyi,j)].\n\nj=1\n\nThe cost function for the classiﬁcation of the entire training set is typically deﬁned as the sum of losses of individual examples,\n\nBCEM def=\n\nN X\n\nBCEMi .\n\ni=1\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Note that the output layers in multiclass and multi-label classiﬁcation are diﬀerent. In multiclass classiﬁcation, one softmax unit is used. It generates a C-dimensional vector whose values are bounded by the range (0,1), and whose sum equals 1. In multi-label classiﬁcation, the output layer contains C logistic units whose values also lie in the range (0,1), but their sum lies in the range (0,C).\n\nNeural Network Output\n\nThe curious reader may wish to better understand the logic behind choosing a speciﬁc loss function. This block will mathematically describe the output of a neural network.\n\nIn regression, the output layer contains only one unit. If the output value can be any number, from minus inﬁnity to inﬁnity, then the output unit will not contain non-linearity. On the other hand, if the neural network must predict a positive number, then the ReLU (rectiﬁed linear unit) non-linearity can be used. Let the output value of the output unit before non-linearity for the input example i be denoted as zi. Then the output after applying the ReLU non-linearity is given by max(0,zi). In a binary classiﬁcation, the output layer contains only one logistic unit. Let the output value of the output unit before non-linearity for the input example i be denoted as zi. The output ˆyi after applying the logistic nonlinearity is given by,\n\nˆyi\n\ndef=\n\n1 1 + e−zi\n\n,\n\nwhere e is the base of the natural logarithm, also known as Euler’s number.\n\nBinary and multi-label classiﬁcation models are deﬁned in a similar way. The only diﬀerence is that in multi-label classiﬁcation, the output layer contains C logistic units, one per class. If ˆyi,j denotes the output, after nonlinearity, of the logistic unit for class j, when input example is i, then the sum of ˆyi,j, for all j = 1,...,C, lies between 0 and C.\n\nIn the multiclass classiﬁcation, the output layer also produces C outputs. However, in this case, the output of each unit of the output layer is controlled by the softmax function. Let the output of the output unit j, before nonlinearity, for the input example i, be zi,j. Then the output ˆyi,j after nonlinearity is given by,\n\nˆyi,j\n\ndef=\n\nezi,j k=1 ezi,k\n\nPC\n\n.\n\nThe sum of ˆyi,j, for all j = 1,...C, equals 1.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "6.1.3 Parameter-Initialization Strategies\n\nIn step 3, we select a parameter-initialization strategy. Before the training starts, the parameter values in all units are unknown. We must initialize them with some values. Training algorithms for neural networks, such as gradient descent and its stochastic variants that we consider in a few moments, are iterative in nature and require the analyst to specify some initial point from which to begin the iterations. This initialization might aﬀect the properties of the training model. You will likely choose from one of these strategies:\n\nones — all parameters are initialized to 1; • zeros — all parameters are initialized to 0; • random normal — parameters are initialized to values sampled from the normal distribution, typically with mean of 0 and standard deviation of 0.05;\n\nrandom uniform — parameters are initialized to values sampled from the uniform distribution with the range [−0.05,0.05];\n\nXavier normal — parameters are initialized to values sampled from the truncated normal distribution, centered on 0, with standard deviation equal to p2/(in + out) where “in” is the number of units in the preceding layer to which the current unit is connected (the one whose parameters you initialize); and “out” is the number of units on the subsequent layer to which the current unit is connected; and,\n\nXavier uniform — parameters are initialized to values sampled from a uniform distribution within [−limit,limit], where “limit” is p6/(in + out), and “in” and “out” are deﬁned as in Xavier normal, above.\n\nThere are other initialization strategies. If you work with a neural network training module such as TensorFlow, Keras, or PyTorch, they provide some parameter initializers, and also recommend default choices.\n\nThe bias term is usually initialized with a zero.\n\nWhile we know the parameter initialization aﬀects the model properties, we cannot predict which strategy will provide the best result for your problem. Random and Xavier initializers are the most common. It’s recommended to start your experiments with one of those two.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 2145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Figure 1: A local and a global minima of a function.\n\n6.1.4 Optimization Algorithms\n\nIn step 4, we select a cost-function optimization algorithm. When the cost function is diﬀerentiable (and it’s the case for all cost functions we considered above) gradient descent and stochastic gradient descent are two most frequently used optimization algorithms.\n\nGradient descent is an iterative optimization algorithm for ﬁnding a local minimum of any diﬀerentiable function. We say that f(x) has a local minimum at x = c if f(x) ≥ f(c) for every x in some open interval around x = c. An interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. An open interval does not include its endpoints and is denoted using parentheses. For example, (0,1) means “all numbers greater than 0 and less than 1.” The minimal value among all the local minima is called the global minimum. The diﬀerence between a local and a global minimum of a function is shown in Figure 1.\n\nFunctions and optimization\n\nIn this block, for the curious reader, we explain the basics of mathematical function and function optimization. If you only want to know the mechanics of training neural networks, you can safely skip it.\n\nA function is a relation that associates each element x of a set X, the domain of the function, to a single element y of another set Y, the codomain of the function. A function usually has a name. If the function is called f, this relation is denoted y = f(x), read “y equals f of x.” The element x is the argument, or input of the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "function, and y is the value of the function, or the output. The symbol that is used for representing the input is the variable of the function. We often say that f is a function of the variable x.\n\nA derivative f0 of a function f is a function or a value that describes how fast f increases or decreases. If the derivative is a constant value, like 5 or −3, then the function increases or decreases constantly, at any point x of its domain. If the derivative f0 is itself a function, then the function f can grow at a diﬀerent pace in diﬀerent regions of its domain. If the derivative f0 is positive at some point x, then the function f increases at this point. If the derivative of f is negative at some x, then the function decreases at this point. The derivative of zero at x means that the function neither decreases nor increases at x; the function’s slope at x is horizontal.\n\nThe process of ﬁnding a derivative is called diﬀerentiation.\n\nDerivatives for basic functions are known. For example if f(x) = x2, then f0(x) = 2x; if f(x) = 2x then f0(x) = 2; if f(x) = 2 then f0(x) = 0. The derivative of any function f(x) = c, where c is a constant value, is zero.\n\nIf the function we want to diﬀerentiate is not basic, we can ﬁnd its derivative using the chain rule. For instance if F(x) = f(g(x)), where f and g are some functions, then F 0(x) = f0(g(x))g0(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2. By applying the chain rule, we ﬁnd F 0(x) = 2(5x + 1)g0(x) = 2(5x + 1)5 = 50x + 10.\n\nGradient is the generalization of derivatives for functions that take several inputs, or one input in the form of a vector or some other complex structure. A gradient of a function is a vector of partial derivatives. Finding a partial derivative of a function is the process of ﬁnding the derivative by focusing on one of the function’s inputs and considering all other inputs as constant values.\n\nFor example, if our function is deﬁned as f([x(1),x(2)]) = ax(1) + bx(2) + c, then the partial derivative of function f with respect to x(1), denoted as\n\n∂f ∂x(1)\n\n, is given by,\n\n∂f ∂x(1)\n\n= a + 0 + 0 = a,\n\nwhere a is the derivative of the function ax(1). The two zeros are respectively derivatives of bx(2) and c, because x(2) is considered constant when we calculate the derivative with respect to x(1), and the derivative of any constant is zero.\n\nSimilarly, the partial derivative of function f with respect to x(2),\n\n∂f ∂x(2)\n\n, is given by,\n\n∂f ∂x(2)\n\n= 0 + b + 0 = b.\n\nThe gradient of function f, denoted as ∇f is given by the vector [ ∂f\n\n∂x(1), ∂f\n\n∂x(2)\n\n].\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 2647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Figure 2: The inﬂuence of the learning rate on the convergence: (a) too small, the convergence will be slow; (b) too large, no convergence; (c) the right value of learning rate.\n\nThe chain rule works with partial derivatives too.\n\nTo ﬁnd a local minimum of a function using gradient descent, we start at some random point in the domain of the function. Then we move proportionally to the negative of the gradient (or approximate gradient) of the function at the current point.\n\nGradient descent in machine learning proceeds in epochs. An epoch consists of using the training set entirely to update each parameter. In the ﬁrst epoch, we initialize the parameters of our neural network using one of the parameter-initialization strategies discussed above. The backpropagation algorithm computes the partial derivatives of each parameter using the chain rule for derivatives of complex functions.1 At each epoch, gradient descent updates all parameters using partial derivatives. The learning rate controls the signiﬁcance of an update. The process continues until convergence, the state when the values of parameters don’t change much after each epoch. Then the algorithm stops.\n\nGradient descent is sensitive to the choice of the learning rate α. Picking the right learning rate for your problem is not easy. If you select a value that is too high, you might not reach convergence at all. On the other hand, too small values of α can slow down the learning to the point of no observable progress. In Figure 2, you can see an illustration of gradient descent for one parameter of a neural network and three values of the learning rate. The value of the parameter at each iteration is shown as a blue circle. The number inside the\n\n1The explanation of backpropagation is beyond the scope of this book. You should only know that every modern software library for training neural networks contains an implementation of this algorithm. The curious reader can ﬁnd the explanation of backpropagation in the extended version of The Hundred-Page Machine Learning Book on its companion wiki.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 2137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "circle indicates the epoch. The red arrows indicate the direction of the gradient along the horizontal axis — the direction away from the minimum. The green arrows show the change in the value of the cost function after each epoch.\n\nTherefore, at each epoch, gradient descent moves the parameter value towards the minimum. If the learning rate is too small, the movement towards the minimum will be very slow (Figure 2a). If the learning rate is too large, the value of the parameter will oscillate away from the minimum (Figure 2b).\n\nGradient descent is rather slow for large datasets because it uses the entire dataset to compute the gradient of each parameter at each epoch. Fortunately, several signiﬁcant improvements to this algorithm have been proposed.\n\nMinibatch stochastic gradient descent (minibatch SGD) is a variant of the gradient descent algorithm. It approximates the gradient using small subsets of the training data called minibatches. This eﬀectively speeds up the computation. The size of the minibatch is a hyperparameter, and you can tune it. Powers of two, between 32 and a few hundred, are recommended: 32, 64, 128, 256, and so on.\n\nThe problem of choosing a value for the learning rate α is still present in the “vanilla” minibatch SGD. Learning can still stagnate at later epochs. Instead of reaching a local minimum, the gradient descent might keep oscillating around it due to too large updates. There are many learning rate decay schedules that allow updating the learning rate, as the learning progresses, by reducing it later in the epoch count. The beneﬁts of using a learning rate decay schedule include faster gradient descent convergence (faster learning) and higher model quality. Below, we consider several popular learning rate decay schedules.\n\n6.1.5 Learning Rate Decay Schedules\n\nLearning rate decay consists of gradually reducing the value of the learning rate α as the epochs progress. Consequently, the parameter updates become ﬁner. There are several techniques, known as schedules, to control α.\n\nTime-based learning rate decay schedules alter the learning rate depending on the learning rate of the previous epoch. The mathematical formula for the learning rate update, according to a popular time-based learning rate decay schedule, is:\n\nαn ←\n\nαn−1 1 + d × n\n\n,\n\nwhere αn is the new value of the learning rate, αn−1 is the value of the learning at the previous epoch n − 1, and d is the decay rate, a hyperparameter. For example, if the initial value of the learning rate α0 = 0.3, then the values of the learning rate at the ﬁrst ﬁve epochs are shown in the table below:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 2676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "learning rate\n\nepoch\n\n0.15 0.10 0.08 0.06 0.05\n\n1 2 3 4 5\n\nStep-based learning rate decay schedules change the learning rate according to some pre-deﬁned drop steps. The mathematical formula for the learning rate update, according to a popular step-based learning rate decay schedule, is:\n\nαn ← α0dﬂoor( 1+n\n\nr\n\n)\n\n,\n\nwhere αn is the learning rate at epoch n, α0 is the initial value of the learning rate, d is the decay rate that reﬂects how much the learning rate should change at each drop step (0.5 corresponds to halving), and r is the so-called drop rate deﬁning the length of drop steps (10 corresponds to a drop every 10 epochs). The ﬂoor operator in the above formula equals 0 if the value of its argument is less than 1.\n\nExponential learning rate decay schedules are similar to step-based. However, instead of drop steps, a decreasing exponential function is used. The mathematical formula for the learning rate update, according to a popular exponential learning rate decay schedule, is:\n\nαn ← α0e−d×n\n\nwhere d is the decay rate and e is Euler’s number.\n\nThere are several popular upgrades to minibatch SGD, such as Momentum, Root Mean Squared Propagation (RMSProp), and Adam. These algorithms update the learning rate automatically based on the performance of the learning process. You don’t have to worry about choosing the initial learning rate value, the decay schedule and rate, or other related hyperparameters. These algorithms have demonstrated good performance in practice, and practitioners often use them instead of manually tuning the learning rate.\n\nMomentum helps accelerate minibatch SGD by orienting the gradient descent to the relevant direction, and reducing oscillations. Instead of using only the current gradient’s epoch to guide the search, Momentum accumulates the gradient of past epochs to determine the direction to go. Momentum removes the need to manually adjust the learning rate.\n\nMore recent advancements in neural network cost function optimization algorithms include RMSProp and Adam, the latter being the most recent and versatile. It’s recommended to start training the model with Adam. Then, if the quality of the model doesn’t reach the acceptable level, try a diﬀerent cost function optimization algorithm.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 2313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "6.1.6 Regularization\n\nIn neural networks, besides L1 and L2 regularization, you can use neural network-speciﬁc regularizers: dropout, early stopping, and batch-normalization. The latter is technically not a regularization technique, but it often has a regularization eﬀect on the model.\n\nThe concept of dropout is very simple. Each time you “run” a training example through the network, you temporarily exclude at random some units from the computation. The higher the percentage of units excluded, the stronger the regularization eﬀect. Popular neural network libraries allow you to add a dropout layer between two successive layers, or you can specify the dropout hyperparameter for a layer. The dropout hyperparameter varies in the range [0,1] and characterizes the fraction of units to randomly exclude from computation. The value of the hyperparameter has to be found experimentally. While simple, dropout’s ﬂexibility and regularizing eﬀect are phenomenal.\n\nEarly stopping trains a neural network by saving the preliminary model after every epoch. Models saved after each epoch are called checkpoints. Then it assesses each checkpoint’s performance on the validation set. You’ll ﬁnd during gradient descent that the cost decreases as the number of epochs increases. After some epoch, the model can start overﬁtting, and the model’s performance on the validation data can deteriorate. Remember the bias-variance illustration in Figure ?? in Chapter 5. By keeping a version of the model after each epoch, you can stop the training once you start observing a decreased performance on the validation set. Alternatively, you can keep running the training process for a ﬁxed number of epochs, and then pick the best checkpoint. Some machine learning practitioners rely on this technique. Others try to properly regularize the model using appropriate techniques.\n\nBatch normalization (which rather should be called batch standardization) consists of standardizing the outputs of each layer before the next layer receives them as input. In practice, batch normalization results in faster and more stable training, as well as some regularization eﬀect. So, it’s always a good idea to use batch normalization. In popular neural network libraries, you can often insert a batch normalization layer between two subse- quent layers.\n\nAnother regularization technique that can be applied to any learning algorithm is data augmentation. This technique is often used to regularize models that work with images. In practice, applying data augmentation often results in an increased model performance.\n\n6.1.7 Network Size Search and Hyperparameter Tuning\n\nStep 5 of the deep model training strategy is similar to that in the shallow model training strategy — choose a hyperparameter tuning strategy T.\n\nIt step 6, we pick a combination of hyperparameter values using strategy T. Typical parameters include the size of the minibatch, the value of the learning rate (if you use the vanilla minibatch SGD), or an algorithm that automatically updates the learning rate, such\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 3114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Figure 3: The neural network model training ﬂowchart.\n\nas Adam. You also decide the initial number of layers and units per layer. It’s recommended to start with something reasonable that would allow us to build the ﬁrst model fast enough. For example, two hidden layers and 128 units per layer might be a good starting point.\n\nStep 7 reads, “Build the training model M, using algorithm A, parametrized with hyper- parameters H, to optimize the cost function C.” This is the main diﬀerence with shallow learning. When you work with a shallow learning algorithm or a model, you can only tweak some built-in hyperparameters. You don’t have much control over the model architecture and complexity. With neural networks, you have all the control, and training a model is more a process than a single action. To build a deep model, you start with a reasonably-sized model, and then you follow the ﬂowchart shown in Figure 3.\n\nObserve that you start with some model, and then increase its size until it ﬁts the training data well. Then you evaluate the model on the validation data. If it performs well, according to the performance metric, you stop and return the model. Otherwise, you regularize and retrain the model.\n\nAs we have seen, regularization in neural networks is usually achieved in several ways. The most eﬀective is dropout, where you randomly remove some units from the network and make it simpler and “dumber.” A simpler model would work better on the holdout data, and this is your goal.\n\nSuppose, after several loops of regularization and model retrains, you don’t see any improve- ment in the model performance on the validation data. Check if it still ﬁts the training data. If it doesn’t, increase the size of the model, by increasing the size of individual layers, or by adding another layer. Continue until the model ﬁts the training data again. Then evaluate it again on the validation data. The process continues until a larger model doesn’t result in better validation data performance, no matter your actions. Then you stop and return the model, if validation data performance is satisfactory.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 2171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "If you are not satisﬁed with this performance, you can pick a diﬀerent combination of hyperparameters for step 8, and build a diﬀerent model. You will continue to test diﬀerent values of hyperparameters until there are no more values to test. Then you keep the best model among those you trained in the process. If the performance of the best model is still not satisfactory, try a diﬀerent network architecture, add more labeled data, or try transfer learning. We talk more on transfer learning in Section 6.1.10.\n\nThe properties of a trained neural network depend a lot on the choice of the values of hyperparameters. But before you choose speciﬁc values of hyperparameters, train a model, and validate its properties on the validation data, you must decide which hyperparameters are important enough for you to spend the time on.\n\nObviously, if you had inﬁnite time and computing resources, you would tune all hyperpa- rameters. However, in practice, you have ﬁnite time and, often, relatively modest resources. Which hyperparameters to tune?\n\nWhile there is no deﬁnitive answer to that question, there are several observations that might help you in choosing the hyperparameters to tune when you work on a speciﬁc model:\n\nyour model is more sensitive to some hyperparameters than to others; and • the choice is often between using the default value of a hyperparameter or changing it.\n\nThe libraries for training neural networks often come with default values for hyperparameters: stochastic gradient descent version (often, Adam), the parameter initialization strategy (often, random normal or random uniform), minibatch size (often, 32), and so on. Those defaults were chosen based on observations from practical experience. Open-source libraries and modules are often the fruit of the collaboration of many scientists and engineers. These talented and experienced people established “good” defaults for many hyperparameters when working with various datasets and practical problems.\n\nIf you decide to tune a hyperparameter, as opposed to using the default value, it makes more sense to tune the hyperparameters to which the model is sensitive. Table 1 shows2 several hyperparameters and approximate sensitivity of a neural network to those hyperparameters.\n\n6.1.8 Handling Multiple Inputs\n\nIn practice, machine learning engineers often work with multimodal data. For example, the input could be an image and a text, and the binary output could indicate whether the text describes the given image.\n\nIt’s hard to adapt shallow learning algorithms to work with multimodal data. For example, you can try to vectorize each input, by applying the corresponding feature engineering method. Then, concatenate two feature vectors to form one wider feature vector. If your image has features [i(1),i(2),i(3)], and your text has features [t(1),t(2),t(3),t(4)], your concatenated feature vector will be [i(1),i(2),i(3),t(1),t(2),t(3),t(4)].\n\n2Taken from the talk “Troubleshooting Deep Neural Networks” by Josh Tobin et al., January 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 3089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Hyperparameter\n\nSensitivity\n\nLearning rate Learning rate schedule Loss function Units per layer Parameter initialization strategy Medium Medium Number of layers Medium Layer properties Medium Degree of regularization Low Choice of optimizer Low Optimizer properties Low Size of minibatch Low Choice of non-linearity\n\nHigh High High High\n\nTable 1: Approximate sensitivity of a model to some hyperparameters.\n\nWith neural networks, you have substantially more ﬂexibility. You can build two subnet- works, one for each input type. For example, a CNN subnetwork reads the image, while an RNN subnetwork reads the text. Both subnetworks have, as their last layer, an embedding. CNN has an image embedding, and RNN has a text embedding. You then concatenate the two embeddings, and ﬁnally add a classiﬁcation layer, such as softmax or logistic sigmoid, on top of the concatenated embeddings.\n\nNeural network libraries provide simple-to-use tools that allow concatenating or averaging layers from several subnetworks.\n\n6.1.9 Handling Multiple Outputs\n\nSometimes, you would like to predict multiple outputs for one input. Some problems with multiple outputs can be eﬀectively converted into a multi-label classiﬁcation problem. Those with labels of the same nature (like tags in social networks), or fake labels can be created as a full enumeration of combinations of original labels.\n\nHowever, in many cases, the outputs are multimodal, and their combinations cannot be eﬀectively enumerated. Consider the following example: you want to build a model that detects an object on an image, and returns its coordinates. In addition, the model has to return a tag describing the object, such as “person,” “cat,” or “hamster.” Your training example will be a feature vector representing an image and a label. The label could be represented as a vector of coordinates of the object, and another vector with a one-hot encoded tag.\n\nFor this, you can create one subnetwork that works as an encoder. It will read the input image using, for example, one or several convolution layers. The encoder’s last layer is the\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 2155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "image embedding. Then you add two other subnetworks on top of the embedding layer: 1) one takes the embedding vector as input, and predicts the coordinates of the object, and 2) the other takes the embedding vector as input, and predicts the tag.\n\nThe ﬁrst subnetwork can have a ReLU as the last layer, which is good for predicting positive real numbers, such as coordinates. This subnetwork can use the mean squared error cost C1. The second subnetwork will take the same embedding vector as input, and will predict the probabilities for each tag. It can have a softmax as the last layer, which is appropriate for the multiclass classiﬁcation, and use the averaged negative log-likelihood cost C2 (also called cross-entropy cost). Alternatively, the coordinates could be in the range [0,1] (in which case the layer that predicts coordinates will have four logistic sigmoid outputs and average four binary cross-entropy cost functions), while the layer that predicts tags might solve a multi-label classiﬁcation problem (in which case it would also have several sigmoid outputs and average several binary cross entropy costs, one per tag).\n\nObviously, you are interested in accurate predictions of both the coordinates and the tags. However, it is impossible to optimize two cost functions at once. By trying to improve one, you risk hurting the other one, and vice-versa. What you can do is add another hyperparameter γ, in the range (0,1), and deﬁne the combined cost function as γ × C1 + (1 − γ) × C2. Then you tune the value for γ on the validation data, just like any other hyperparameter.\n\n6.1.10 Transfer Learning\n\nRecall, transfer learning consists of using a pre-trained model to build a new model. Pre-trained models are usually created using big data available to its creators, usually large organizations, but not necessarily available to you. The parameters learned by the pre-trained models can be useful for your task.\n\nA pre-trained model can be used in two ways:\n\n1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\n\nUsing Pre-Trained Model as Initializer\n\nAs discussed, the choice of parameter initialization strategy aﬀects the properties of the learned model. Pre-trained models, whether available on the Internet, or trained by you, usually perform well for solving the original learning problem.\n\nIf your current problem is similar to the one solved by the pre-trained model, chances are high that the optimal parameters for your current problem will not be too diﬀerent from the pre- trained parameters, especially in the initial neural network layers (those closest to the input).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "inputlayer\n\noutputlayer\n\n...\n\n...\n\n...\n\n...\n\n(a)\n\ninputlayer\n\n...\n\noutputlayer\n\n...\n\n...\n\n...\n\n...\n\n...\n\n(b)\n\n...\n\nFigure 4: An illustration of transfer learning: (a) a pre-trained model and (b) your model, where you used the left part of the pre-trained model, and added new layers, including a diﬀerent output layer tailored for your problem.\n\nThe learning might go faster for your problem because gradient descent will search for the optimal parameter values in a smaller region of potentially good values.\n\nIf the pre-trained model was built using a training set much bigger than yours, searching in a region of potentially good values might also lead to a better generalization. Indeed, if some behavior of the model you want to build is not reﬂected in your training examples, this behavior could still be “inherited” from the pre-trained model.\n\nUsing Pre-Trained Model as Feature Extractor If you use a pre-trained model as an initializer for your model, it gives you more ﬂexibility. The gradient descent will modify the parameters in all layers, and, potentially, reach a better performance for your problem. The downside of that is you will often end up training a very deep neural network.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 1258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Some pre-trained models contain hundreds of layers and millions of parameters. Training a large network like that can be challenging. It will deﬁnitely require a signiﬁcant amount of computational resources. In addition, the problem of the vanishing gradient is more severe in a deep neural network than one with a couple hidden layers.\n\nIf you have a limited amount of computational resources, you might prefer using some layers of the pre-trained model as feature extractors for your model. In practice, it means that you only keep several initial layers of the pre-trained model, those closest to and including the input layer. You keep their parameters “frozen,” that is, unchanged and unchangeable. Then you add new layers on top of the frozen layers, including the output layer appropriate for your task. Only the parameters of the new layers will be updated by gradient descent during training on your data.\n\nAn illustration of the process is shown in Figure 4. The blue neural network is a pre-trained model. Some of the blue layers are reused in the new model with their parameters frozen; the green layers are added by the analyst and tailored to the problem at hand.\n\nThe analyst might decide to freeze the parameters of the entire blue part of the new network, and only train the parameters of the green part. Alternatively, several right-most blue layers could be set as trainable.\n\nHow many layers of the pre-trained model to use in the new model? Freeze how many layers? This is up to the analyst: it’s part of the decisions you’ll make about the architecture that will work best for your problem.\n\n6.2 Stacking Models\n\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model.\n\n6.2.1 Types of Ensemble Learning\n\nThere are ensemble learning algorithms, such as random forest learning and gradient boosting. They train an ensemble of several hundred to thousands of weak models, and obtain a strong model that has a signiﬁcantly better performance than the performance of each weak model. We will not discuss these algorithms here. If you are missing this knowledge, it can easily be found in a specialized machine learning book.3\n\nThe reason why combining multiple models can bring better performance is that, when several uncorrelated models agree, they are more likely to agree on the correct outcome. The key word here is “uncorrelated.” Ideally, base models should be obtained by using diﬀerent features, or be of a diﬀerent nature — for example, SVM and random forest. Combining\n\n3You can read about ensemble learning algorithms in Chapter 7 of The Hundred-Page Machine Learning\n\nBook.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 2760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "diﬀerent versions of the decision tree learning algorithm, or several SVMs with diﬀerent hyperparameters, may not result in a signiﬁcant performance boost.\n\nThe goal of ensemble learning is to learn to combine the strengths of each base model. There are three ways to combine weakly correlated models into an ensemble model: 1) averaging, 2) majority vote, and 3) model stacking.\n\nAveraging works for regression, as well as those classiﬁcation models that return classiﬁcation scores. It consists of applying all your base models to the input x, and then averaging the predictions. To see if the averaged model works better than each individual algorithm, you can test it on the validation set using a metric of your choice.\n\nMajority vote works for classiﬁcation models. It consists of applying all your base models to the input x, and then returning the majority class among all predictions. In the case of a tie, you can either randomly pick one of the classes, or return an error message if misclassifying would incur a signiﬁcant loss for the business.\n\nModel stacking is an ensemble learning method that trains a strong model by inputting the outputs of other strong models. Let’s go into more detail about model stacking.\n\n6.2.2 An Algorithm of Model Stacking\n\nSay you want to combine classiﬁers f1, f2, and f3, all predicting the same set of classes. To create a synthetic training example (ˆxi, ˆyi) for the stacked model from the original training example (xi,yi), set ˆxi ← [f1(x),f2(x),f3(x)], and ˆyi ← yi. This is illustrated in Figure 5. If some of your base models return a class plus a class score, you can use those scores as additional input features for the stacked model.\n\nTo train the stacked model, use synthetic examples, and tune the hyperparameters of the stacked model using cross-validation. Make sure your stacked model performs better on the validation set than each of the stacked base models.\n\nIn addition to using diﬀerent machine learning algorithms and models, some base models, to be weakly correlated, can be trained by randomly sampling the examples and features of the original training set. Furthermore, the same learning algorithm, trained with very diﬀerent hyperparameter values, could produce suﬃciently uncorrelated models.\n\n6.2.3 Data Leakage in Model Stacking\n\nTo avoid data leakage, be careful when training a stacked model. To create the synthetic training set for the stacked model, follow a process similar to cross-validation. First, split all training data into ten or more blocks. The more blocks the better, but the process of training the model will be slower.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 2673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Base model 1(e.g. SVM)\n\nBase model 2(e.g. gradientboosting)\n\nBase model 3(e.g. neuralnetwork)\n\nStacking model(e.g. logisticregression)\n\nFigure 5: A stacking of three weakly correlated strong models.\n\nTemporarily exclude one block from the training data, and train the base models on the remaining blocks. Then apply the base models to the examples in the excluded block. Obtain the predictions, and build the synthetic training examples for the excluded block by using the predictions from the base models.\n\nRepeat the same process for each of the remaining blocks, and you will end up with the training set for the stacking model. The new synthetic training set will be of the same size as that of the original training set.\n\n6.3 Dealing With Distribution Shift\n\nRecall that the holdout data must resemble the data you will observe in production. Some- times, however, it is not available in suﬃciently large quantities. At the same time, you might have access to labeled data that is similar to the production data, but not exactly the same. For example, you might have lots of labeled images from the Web crawl collection, but your goal is to train a classiﬁer for Instagram photos. You might not have enough labeled Instagram photos for training, so you hope to train the model by using the Web crawl data, and then be able to use that model to classify the Instagram photos.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "6.3.1 Types of Distribution Shift\n\nWhen the distributions of the training data and test data are not the same, we call it distribution shift. Dealing with a distribution shift is currently an open research area. Researchers distinguish three types of distribution shift:\n\ncovariate shift — shift in the values of features; • prior probability shift — shift in the values of the target; and • concept drift — shift in the relationship between the features and the label.\n\nYou may know your data is aﬀected by a distribution shift, but you don’t usually know what type of shift it is.\n\nIf the number of examples in the test set is relatively high compared to the size of the training set, you could randomly pick a certain fraction of test examples and transfer some to the training set and some to the validation set. Then you would train the model as usual. However, often you have a very high number of training examples and relatively few test examples. In that case, a more eﬀective approach is to use adversarial validation.\n\n6.3.2 Adversarial Validation\n\nWe prepare for adversarial validation as follows. We assume that the feature vectors in a training and a test examples contain the same number of features, and those features represent the same information. Split your original training set into two subsets: Training Set 1 and Training Set 2.\n\nCreate a Modiﬁed Training Set 1 by transforming the examples from Training Set 1 as follows. To each example in Training Set 1, add the original label as an additional feature, then assign the new label “Training” to that example.\n\nCreate a Modiﬁed Test Set by transforming the examples from the original test set as follows. To each example in the test set, add the original label as an additional feature, then assign the new label “Test” to that example.\n\nMerge the Modiﬁed Training Set 1 and the Modiﬁed Test Set to obtain a new Synthetic Training Set. You will use it for solving a binary classiﬁcation problem of distinguishing the “Training” examples from the “Test” examples. Use that Synthetic Training Set, and train a binary classiﬁer that returns a prediction score.\n\nObserve that the binary classiﬁer we have trained will predict, for a given original example, whether it’s a training or a test example. Apply that binary classiﬁer to the examples from Training Set 2. Identify the examples predicted as “Test,” which the binary model is most certain about. Use those examples as validation data for your original problem.\n\nRemove the examples from Training Set 1 which the binary model predicted “Training” with the highest certainty. Use the remaining examples in Training Set 1 as the training data for your original problem.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 2752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "You must experiment to ﬁnd out what is the ideal way to split the original training set into Training Set 1 and Training Set 2. You also must ﬁnd out how many examples from Training Set 1 to use for training, and how many of them to use for validation.\n\n6.4 Handling Imbalanced Datasets\n\nIn Section ?? of Chapter 3, we considered some techniques to handle imbalanced datasets, such as over- and undersampling, and generating synthetic data.\n\nIn this section, we will consider additional techniques that are applied during learning, as opposed to in the data collection and preparation stage.\n\n6.4.1 Class Weighting\n\nSome algorithms and models, such as support vector machine (SVM), decision trees, and random forests, allow the data analyst to provide weights for each class. The loss in the cost function is typically multiplied by the weight. The data analyst may, for example, provide greater weight to the minority class. This makes it harder for the learning algorithm to disregard examples of the minority class, because it would result in much higher cost than without class weighting.\n\nLet’s see how it works in support vector machines. Our problem is distinguishing between genuine and fraudulent e-commerce transactions. The examples of genuine transactions are much more frequent. If you use SVM with soft margin, you can deﬁne a cost for misclassiﬁed examples. The SVM algorithm tries to move the hyperplane to reduce the number of misclassiﬁed examples. If the misclassiﬁcation cost is the same for both classes, the “fraudulent” examples, in the minority, risk being misclassiﬁed to allow classifying more of the majority class correctly. This situation is illustrated in Figure 6a. This problem is observed for most learning algorithms applied to imbalanced datasets.\n\nIf you set higher the loss of minority misclassiﬁcation, then the model will try harder to avoid misclassifying those examples. But this will incur the cost of misclassiﬁcation of some majority class examples, as illustrated in Figure 6b.\n\n6.4.2 Ensemble of Resampled Datasets\n\nEnsemble learning is another way of mitigating the class imbalance problem. The analyst randomly chunks majority examples into H subsets, then creates H training sets. After training H models, the analyst then makes predictions by averaging (or taking the majority) of the outputs of H models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 2412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "(a)\n\n(b)\n\nFigure 6: An illustration of an imbalanced problem. (a) Both classes have the same weight; (b) examples of the minority class have a higher weight.\n\nB\n\nC\n\nA\n\nA\n\nB\n\nOriginal dataResampled data\n\nC\n\nA\n\nA\n\nD\n\nA\n\nE\n\nE\n\nD\n\nFigure 7: An ensemble of resampled datasets.\n\nThe process for H = 4 is illustrated in Figure 7. Here, we transformed our imbalanced binary learning problem into four balanced problems by chunking the examples of the majority class into four subsets. The examples of the minority class are copied four times in their entirety.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "This approach is simple and scalable: you can train and run your models on diﬀerent CPU cores or cluster nodes. Ensemble models also tend to produce a better prediction than each individual model in the ensemble.\n\n6.4.3 Other Techniques\n\nIf you use stochastic gradient descent, the class imbalance can be tackled in several ways. First, you can have diﬀerent learning rates for diﬀerent classes: a lower value for the examples of the majority class, and a higher value otherwise. Second, you can make several consecutive updates of the model parameters each time you encounter an example of a minority class.\n\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic that we considered in Section ?? in the previous chapter.\n\n6.5 Model Calibration\n\nSometimes it is important that the classiﬁcation model returns not just the predicted class, but also the probability that the predicted class is correct. Some models return a score along with the predicted class. Even if its value ranges between 0 and 1, it’s not always a probability.\n\n6.5.1 Well-Calibrated Models\n\nWe say that the model is well-calibrated when, for input example x and predicted label ˆy, it returns the score that can be interpreted as the true probability for x to belong to class ˆy.\n\nFor instance, a well-calibrated binary classiﬁer would generate a score of 0.8 for approximately 80% of the examples actually belonging to the positive class.\n\nMost machine learning algorithms train models that are not well-calibrated, as shown4 by the calibration plots in Figure 8.\n\nA calibration plot for a binary model allows seeing how well the model is calibrated. On the X-axis, there are bins that group examples by the predicted score. For example, if we have 10 bins, the left-most bin groups all examples for which the predicted score is in the range [0,0.1) while the right-most bin groups all examples for which the predicted score is in the range [0.9,1.0]. On the Y-axis, there are the fractions of positive examples in each bin.\n\n4The graph is adapted from https://scikit-learn.org/stable/modules/calibration.html.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Figure 8: Calibration plots for models trained by several machine learning algorithms applied to a random binary dataset.\n\nFor multiclass classiﬁcation, we would have one calibration plot per class in a one-versus- rest way. One-versus-rest is common strategy for converting a binary classiﬁcation learning algorithm for solving multiclass classiﬁcation problems. The idea is to transform a multiclass problem into C binary classiﬁcation problems and build C binary classiﬁers. For example, if we have three classes, y ∈ {1,2,3}, we create three original dataset copies, and modify them. In the ﬁrst copy, we replace all labels not equal to 1 with a 0. In the second copy, we replace all labels not equal to 2 with a 0. In the third copy, we replace all labels not equal to 3 with a 0. Now we have three binary classiﬁcation problems where we want to learn to distinguish between labels 1 and 0, 2 and 0, and 3 and 0. As you can see, in each of the three binary classiﬁcation problems, the label 0 denotes the “rest” in “one-versus-rest.”\n\nWhen the model is well-calibrated, the calibration plot oscillates around the diagonal (shown as a dotted line in Figure 8). The closer the calibration plot is to the diagonal, the better the model is calibrated. Because a logistic regression model returns the true probabilities of the positive class, its calibration plot is closest to the diagonal. When the model is not well-calibrated, the calibration plot usually has a sigmoid-shape, as shown by the support vector machine and random forest models.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n26",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "6.5.2 Calibration Techniques\n\nThere are two techniques often used to calibrate a binary model: Platt scaling and isotonic regression. The two are based on similar principles.\n\nLet us have a model f that we want to calibrate. First of all, we need a holdout dataset speciﬁcally set aside for calibration. To avoid overﬁtting, we cannot use training or validation data for calibration. Let this calibration dataset be of size M. Then, we apply the model f to each example i = 1,...,M and obtain, for each example i, the prediction fi. We build a new dataset Z, where each example is a pair (fi,yi), yi is the true label of example i, and labels have the values in the set {0,1}.\n\nThe only diﬀerence between Platt scaling and isotonic regression is that the former builds a logistic regression model by using the dataset Z, while the latter builds the isotonic regression of Z, that is, a non-decreasing function as close to the examples as possible. Once we have the calibration model z, obtained either using Platt scaling or isotonic regression, we can predict the calibrated probability for an input x as z(f(x)).\n\nNotice that a calibrated model may or may not result in better quality prediction for your problem. That depends on the chosen model performance metric.\n\nAccording to experiments:5 Platt scaling is most eﬀective when the distortion in the predicted probabilities is sigmoid-shaped. Isotonic regression can correct a wider range of distortions. Unfortunately, this extra power comes at a price. Analysis has shown that isotonic regression is more prone to overﬁtting, and thus performs worse than Platt scaling when data is scarce.\n\nExperiments with eight classiﬁcation problems also suggested that random forests, neural networks, and bagged decision trees are the best learning methods for predicting well- calibrated probabilities prior to calibration, but after calibration, the best methods are boosted trees, random forest, and SVM.\n\n6.6 Troubleshooting and Error Analysis\n\nTroubleshooting a machine learning pipeline is hard. It’s diﬃcult to diﬀerentiate whether the model performs poorly because your code contains a bug, or if there are problems with your training data, learning algorithm, or the way you designed your pipeline. Moreover, the same degradation in performance can be explained by various reasons. The results of the learning can be sensitive to small changes in hyperparameters or dataset makeup.\n\nBecause of these challenges, model training is usually an iterative process, where an analyst trains a model, observes its behavior, and makes adjustments based on observations.\n\n5Alexandru Niculescu-Mizil and Rich Caruana, “Predicting Good Probabilities With Supervised Learning”, appearing in Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n27",
      "content_length": 2880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "6.6.1 Reasons for Poor Model Behavior\n\nIf your model does poorly on the training data (underﬁts it), common reasons are:\n\nthe model architecture or learning algorithm are not expressive enough (try more advanced learning algorithm, an ensemble method, or a deeper neural network);\n\nyou regularize too much (reduce regularization); • you have chosen suboptimal values for hyperparameters (tune hyperparameters); • the features you engineered don’t have enough predictive power (add more informative features);\n\nyou don’t have enough data for the model to generalize (try to get more data, use data augmentation, or transfer learning); or\n\nyou have a bug in your code (debug the code that deﬁnes and trains the model).\n\nIf your model does well on the training data, but poorly on the holdout data (overﬁts the training data), common reasons are:\n\nyou don’t have enough data for generalization (add more data or use data augmentation); • your model is under-regularized (add regularization or, for neural networks, both regularization and batch normalization);\n\nyour training data distribution is diﬀerent from the holdout data distribution (reduce the distribution shift);\n\nyou have chosen suboptimal values for hyperparameters (tune hyperparameters); or • your features have low predictive power (add features with high predictive power).\n\n6.6.2\n\nIterative Model Reﬁnement\n\nIf you have access to new labeled data (for example, you can label examples yourself, or easily request the help of a labeler) then, you can reﬁne the model using a simple iterative process:\n\n1. Train the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set (100−300 examples). 3. Find the most frequent error patterns on that small validation set. Remove those examples from the validation set, because your model will now overﬁt to them.\n\n4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed (most errors look dissimilar).\n\nIterative model reﬁnement is a simpliﬁed version of error analysis. A more principled approach is described below.\n\n6.6.3 Error Analysis\n\nErrors can be:\n\nuniform, and appear with the same rate in all use cases, or\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n28",
      "content_length": 2344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "focused, and appear more frequently in certain types of use cases.\n\nFocused errors following a speciﬁc pattern are those that merit special attention. By ﬁxing an error pattern, you ﬁx it once for many examples. Focused errors, or error trends, usually happen when some use cases aren’t well-represented in the training data. For example, a face detection system developed by a major web camera provider worked better for white users than for black users. In another case, a human presence detection system equipped with a night vision system worked better during the day than at night, simply because the night training examples were less frequent in the training data.\n\nUniform errors cannot be entirely avoided, but important focused errors should be discovered before the model is deployed in production. This can be done by clustering test examples, and by testing the model on examples coming from diﬀerent clusters. The distribution of the production (online) data can be signiﬁcantly diﬀerent from the oﬄine data distribution used for model training/pre-deployment tests. So, the clusters that contain few examples in the oﬄine data might represent much more frequent use cases in the online scenario.\n\nIn Section ?? of Chapter 4, we discussed several techniques for dimensionality reduction. In addition to using clustering for spotting error trends, uniform manifold approximation and projection (UMAP) or autoencoder can be used. Use those techniques to reduce the dimen- sionality of the data to 2D, and then visually inspect the distribution of errors across a dataset.\n\nMore speciﬁcally, you can visualize the data on a 2D scatter plot, using diﬀerent colors for examples of diﬀerent classes. To identify error trends on a scatter plot, use diﬀerent markers depending on whether a model’s prediction was correct or not. For example, use circles to denote examples whose label was predicted correctly, and squares otherwise. This will allow you to see the regions of poor model performance. If you work with perceptive data, such as images or text, it is also helpful to visually examine some examples from those poor performance regions.\n\nWhether you are satisﬁed or dissatisﬁed by the model’s performance on the holdout data, you can always improve the model by analyzing individual errors. As discussed, the best way is to work iteratively, by considering 100 − 300 examples at a time. By considering a small number of examples at a time, you can iterate quickly, by retraining the model after each iteration, but still consider enough examples to spot obvious patterns.\n\nHow do you decide whether an error pattern is worth spending time to ﬁx it? You can base that decision on the error pattern frequencies. Let’s see how it works.\n\nLet your model have an accuracy of 80%, which corresponds to an error rate of 20%. If you ﬁx all error patterns, you can improve the model’s performance by at most 20 percentage points. If your small error-analysis batch was of 300 examples, your model made 0.2×300 = 60 errors.\n\nObserve the errors one by one, and try to get an idea of what particularities in the input led to a misclassiﬁcation of those 60 examples. To be even more concrete, let our classiﬁcation problem be to detect pedestrians-on-the-street images. Assume that in 60 out of the 300 images, the model failed to detect a pedestrian. After closer analysis, you discover two\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n29",
      "content_length": 3450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "patterns: 1) the image is blurry in 40 examples, and 2) the picture was taken during the nighttime in 5 examples. Now, should you spend time addressing both problems?\n\nIf you address the blurry-image problem (for example, by adding more labeled blurry images to your training data), you can hope to decrease your error by (40/60) × 20 = 13 percentage points. In the best-case scenario, after you solve the blurry-image misclassiﬁcation problem, your error becomes 20 − 13 = 7 percent, a signiﬁcant decrease from the initial 20% error.\n\nOn the other hand, if you solve the nighttime image problem, you can hope to decrease your error by 5/60 × 20 = 1.7 percentage points. So, in the best-case scenario, your model will make 20 − 1.7 = 18.3 percent errors, which might be signiﬁcant for some problems, or insigniﬁcant for others. The cost of gathering additional labeled night-time images can be signiﬁcant and might not be worth the eﬀort.\n\nTo ﬁx an error pattern, you can use one or a combination of techniques:\n\npreprocessing the input (e.g. image background removal, text spelling correction); • data augmentation (e.g., blurring or cropping of images); • labeling more training examples; and • engineering new features that would allow the learning algorithm to distinguish between “hard” cases.\n\n6.6.4 Error Analysis in Complex Systems\n\nLet’s say you work on a complex document classiﬁcation system that consists of three chained models as shown below:\n\nDetectlanguage\n\nClassify\n\nTranslate\n\nInput Output\n\nFigure 9: A complex document classiﬁcation system.\n\nLet the accuracy of the entire system be 73%. If the classiﬁcation is binary, the accuracy of 73% doesn’t seem high. On the other hand, if the classiﬁcation model (the rightmost block in Figure 9) supports thousands of classes, then the accuracy of 73% doesn’t seem too low. For some business cases, however, the user might expect human-like, or even superhuman performance.\n\nImagine that you are in a position, where the business expects a higher than 73% performance from the document classiﬁcation system you have built. To get the most out of your additional eﬀort, you must decide which part of the system needs improvement in the ﬁrst place.\n\nWhen the decision about something is made on several chained levels, like in the problem shown in Figure 9, and when those decisions are independent of one another, the accuracy multiplies.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n30",
      "content_length": 2456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "For example, if the language predictor accuracy was 95%, the machine translation model accuracy6 was 90%, and the classiﬁer accuracy was 85%, then, in the case of independence of the three models, the overall accuracy of the entire three-stage system would be 0.95 × 0.90 × 0.85 = 0.73, or 73 percent. At ﬁrst glance, it seems obvious that the most gain in the entire system’s accuracy would come from maximizing the accuracy of the third model — the classiﬁer. However, in practice, some errors made by a given model might not signiﬁcantly aﬀect the overall performance of the system. For example, if the language predictor often confuses Spanish and Portuguese, the machine translation model could still be capable of generating an adequate translation for the third-stage classiﬁcation model.\n\nWhile working on the third-stage classiﬁer, you might have concluded that you reached its maximum performance, so it doesn’t make sense to continue. Now, which of the previous two models, the language detector and/or the machine translator, should you improve to increase the quality of the entire three-stage system?\n\nOne way to determine the upper bound of an entire system’s potential is to perform the error analysis by parts. You replace one model’s predictions with perfect labels, such as human-provided labels. Then you calculate how the entire system performs. For example, instead of using the machine translation system at stage two in Figure 9, you can ask a professional human translator to translate the text from the predicted language (if the prediction of the language was correct), or keep the original text (if the prediction of the language was wrong).\n\nLet’s say you asked a professional for a hundred translations. Now you can measure how perfect translations aﬀect the overall system performance. Let the accuracy of the entire system’s output become 74%. So, the potential gain from improved translation in overall system performance is only one percentage point. Reaching the human-level performance for a machine translation model can turn out to be a daunting task, not worth the eﬀort, especially when what we can achieve in the end is one percentage point gain for the entire system. So, you might prefer spending more time on building a better language predictor in stage 1, if the potential gain in overall system performance prediction quality is higher.\n\n6.6.5 Using Sliced Metrics\n\nIf the model will be applied to diﬀerent segments of the use cases, it should be separately tested for each segment. For example, if you want to predict the solvency of borrowers, you would want your model to be equally accurate for both male and female borrowers. To achieve that, you can split your validation data into several subsets, one subset per segment. Then compute the performance metric by separately applying your model to each subset.\n\nAlternatively, you can separately evaluate the model on each class by applying precision and recall metrics. Remember these metrics are deﬁned only for binary classiﬁcation. By isolating\n\n6Measuring the error of the machine translation system in practice is tricky as a translation is rarely entirely accurate or inaccurate. Instead, measures, such as BLEU (for Bilingual Evaluation Understudy Score) score, are used.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n31",
      "content_length": 3337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "one class in your multiclass classiﬁcation problem, and labeling the other classes “Other,” you can individually compute the precision and recall for each class.\n\nIf you see that the value of the performance metric changes between segments or classes, you can try to ﬁx the problem by adding more labeled data to the segments or classes, where the performance of the model is unsatisfactory, or engineer additional features.\n\n6.6.6 Fixing Wrong Labels\n\nWhen humans label the training examples, the assigned labels can be wrong. This can cause poor model performance on the model on both training and holdout data. Indeed, if similar examples have conﬂicting labels — some correct and some incorrect — the learning algorithm can learn to predict the wrong label.\n\nHere is a simple way to identify the examples that have wrong labels. Apply the model to the training data from which it was built, and analyze the examples for which it made a diﬀerent prediction as compared to the labels provided by humans. If you see that some predictions are indeed correct, change those labels.\n\nIf you have time and resources, you could also examine the predictions with the score close to the decision threshold. Those are often mislabeled cases too.\n\nIf wrong labels in the training data is a serious issue, you can avoid it by asking several individuals to provide labels for the same training example. Only accept it if all individuals assigned the same label to that example. In less demanding situations, you can accept a label if the majority of individuals assigned it.\n\n6.6.7 Finding Additional Examples to Label\n\nAs discussed above, error analysis can reveal that more labeled data is needed from speciﬁc regions of feature space. You might have an abundance of unlabeled examples. How should you decide which examples to label so as to maximize the positive impact on the model?\n\nIf your model returns a prediction score, an eﬀective way is to use your best model to score the unlabeled examples. Then label those examples, whose prediction score is close to the prediction threshold.\n\nWhen the error analysis has revealed error patterns by means of visualization, then choose those examples which are surrounded by many examples with prediction errors.\n\n6.6.8 Troubleshooting Deep Learning\n\nTo avoid problems when training a deep model, follow a workﬂow shown below:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n32",
      "content_length": 2421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Figure 10: A deep learning troubleshooting workﬂow.\n\nWhen possible, start small, for example, with a simple model using a high-level library, such as Keras. It should be very easy to validate visually, ideally ﬁtting on at most two screens.\n\nAlternatively, reuse an existing open-source architecture that was proven to work (pay attention to the code license!). Start with:\n\na small, normalized dataset ﬁtting in memory, • the most simple to use cost-function optimizer (e.g., Adam), • an initialization strategy (e.g., random normal), • the default values of the sensitive hyperparameters of both the cost-function optimizer and the layers, and\n\nno regularization.\n\nOnce you have your ﬁrst simplistic model architecture and dataset, temporarily reduce your training dataset even further, to the size of one minibatch. Then start the training. Make sure your simplistic model is capable of overﬁtting this training minibatch. If the overﬁtting of the minibatch doesn’t happen, it is a solid indicator that something is wrong with your code or data. Look for the following signs7 and their probable causes:\n\n7Adapted from the talk “Troubleshooting Deep Neural Networks” by Josh Tobin et al., January 2019.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n33",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Sign Error goes up\n\nProbable causes Flipped the sign of the loss function or gradient Learning rate too high Softmax taken over wrong dimension\n\nError explodes Numerical issue\n\nLearning rate too high\n\nError oscillates Data or labels corrupted (e.g. zeroed or incorrectly shuﬄed)\n\nError plateaus\n\nLearning rate too high Learning rate too low Gradient not ﬂowing through the whole model Too much regularization Incorrect input to loss function Data or labels corrupted\n\nTable 2: Common issues and most common causes of problems with getting to overﬁt one minibatch by a neural network model.\n\nOnce your model overﬁts one minibatch, get back to the entire dataset, and train, evaluate, then tune hyperparameters until no improvements on the validation data are possible.\n\nIf the performance of the model is still unsatisfactory, update the model (e.g., by increasing its depth or width), or the training data (e.g., by changing the pre-processing, or adding features). Debug the change by overﬁtting one minibatch once again, then train, evaluate, and tune the new model. Keep iterating until you’re satisﬁed with the quality of the model.\n\nWhile you are searching for the best architecture for your model, it’s convenient not just to use a smaller training set, but also to simplify the problem by either,\n\ncreating a simple synthetic training set, or • reducing the number of classes or the resolution of input images (or video fragments), size of the texts, bitrate of the sound frequencies, and so on.\n\nAt the evaluation step of the deep learning troubleshooting workﬂow shown in Figure 10, verify if the poor model performance could be caused by one of the reasons listed in Section 6.6.1. Choose the next step depending on whether the performance can be improved by tuning hyperparameters, updating the model, features, or the training data.\n\n6.7 Best Practices\n\nIn this section, I gathered practical advice on training machine learning models. The best practices below aren’t strict prescriptions. They are rather recommendations that often save time, eﬀort, and might lead to higher quality results.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n34",
      "content_length": 2161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "6.7.1 Deliver a Good Model\n\nWhat is a good model? A good model has two properties:\n\nit has the desired quality according to the performance metric; and • it is safe to serve in a production environment.\n\nFor a model to be safe-to-serve means satisfying the following requirements:\n\nit will not crash or cause errors in the serving system when being loaded, or when loaded with bad or unexpected inputs;\n\nit will not use an unreasonable amount of resources (such as CPU, GPU, or RAM).\n\n6.7.2 Trust Popular Open Source Implementations\n\nModern open-source libraries and modules for machine learning in popular modern pro- gramming languages and platforms, such as Python, Java, and .NET, contain eﬃcient, industry-standard implementations of popular machine learning algorithms. They usually have permissive licenses. Additionally, open-source libraries and modules exist speciﬁcally for training neural networks.\n\nIt is only considered reasonable to create your own machine learning algorithms if you use an exotic or very new programming language. In addition, you might program from scratch if the model is intended to be executed in a very resource-constrained environment, or you need to run your model with a speed no existing implementation can provide.\n\nAvoid using multiple programming languages in the same project. Using diﬀerent programming languages increases the cost of testing, deployment, and maintenance. It also makes it diﬃcult to transfer project ownership between employees.\n\n6.7.3 Optimize a Business-Speciﬁc Performance Measure\n\nLearning algorithms try to reduce training data error. The data analyst, in turn, wants to minimize test data error. However, your client or employer typically wants you to optimize a business-speciﬁc performance metric.\n\nWhen you have minimized the validation error rate, focus on tuning hyperparameters that optimize a business-speciﬁc metric, even if it causes the validation error rate to increase.\n\n6.7.4 Upgrade From Scratch\n\nOnce deployed to production, some models have to be periodically updated with new data to adapt to the user’s needs. This new training data must be automatically collected by using scripts (as we discussed in Chapter 3 in Section ?? about reproducibility).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n35",
      "content_length": 2295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Each time the data is updated, the hyperparameters must be tuned from scratch. Otherwise, the new data may yield suboptimal performance with old hyperparameters.\n\nSome models, such as neural networks, may be iteratively upgraded. However, avoid the practice of warm-starting. It consists of iteratively upgrading the existing model by using only new training examples and running additional training iterations.\n\nFurthermore, frequent model upgrades without retraining from scratch can lead to catas- trophic forgetting. It’s a situation in which the model that was once capable of something, “forgets” that capability because of learning something new.\n\nNote that upgrading the model is not the same as transfer learning. Analysts use transfer learning when the data used to build the pre-trained model, or adequate computing resources, are not available.\n\n6.7.5 Avoid Correction Cascades\n\nYou might have model mA that solves problem A, but you need a solution mB for a slightly diﬀerent problem B. It can be tempting to use the output of mA as input for mB, and only train mB on a small sample of examples that “correct” the output of mA for solving problem B. Such technique is called correction cascading, and it is not recommended. Model cascading makes it impossible to update model mA, without also updating model mB (and the rest of the cascade). The eﬀect a change in mA might have on mB is impossible to predict, but most likely it will be negative. Furthermore, the developer of model mB might not know about the change in model mA, and the developer of model mA might not know that model mB depends on it. The negative eﬀect on mB of the change in model mA may go unnoticed for a long time.\n\nInstead of building a correction cascade, it is recommended to update model mA to include the use cases for solving problem B. It would be wise to add features allowing the model to distinguish between the examples of problem B. One might also use transfer learning, or build an entirely independent model for solving problem B.\n\n6.7.6 Use Model Cascading With Caution\n\nIt’s important to note that model cascading is not always a bad practice. Using the output of one model, as one of many inputs for another model, is common. It might signiﬁcantly reduce time to market. However, cascading must be used with caution, because the update of one model in a cascade must involve an update of all models in the cascade, which can end up being costly in the long-term.\n\nTo mitigate the negative eﬀect of model cascading, two strategies are beneﬁcial:\n\n1. Analyze the information ﬂow in your software system and update, or retrain, the entire chain. Model mA’s updated output must be reﬂected in the training data for model mB.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n36",
      "content_length": 2782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "2. Control who can and who cannot make calls to model mA to prevent undeclared consumers from creating this issue. As Google’s engineers mentioned:8 “In the absence of barriers, engineers will naturally use the most convenient signal at hand, especially when working against deadline pressures.”\n\nFurthermore, a prediction output by a model should not be a plain number or a string. It should come with information about the production model, and how it should be consumed.\n\n6.7.7 Write Eﬃcient Code, Compile, and Parallelize\n\nBy writing fast and eﬃcient code, you can speed up the training by an order of magnitude, as compared to an ineﬃcient quick-and-dirty script you implemented during experimentation, just “to make it work.” Modern datasets are large, so you might wait for hours, even days, for data preprocessing. Training also can take days, or sometimes weeks.\n\nAlways write the code with eﬃciency in mind, even if it seems to be a function, a method, or a script that you will not run frequently. Some code that was supposed to run once might be called in a loop millions of times.\n\nAvoid using loops. For example, if you need to compute a dot product of two vectors, or multiply a matrix by a vector, use fast and eﬃcient dot-product or matrix-multiplication methods in scientiﬁc libraries and modules. Examples of such eﬃcient implementations are Python’s NumPy and SciPy libraries. Talented and skilled software engineers and scientists created these libraries and modules. They rely on low-level programming languages such as C, as well as hardware acceleration, and work blazingly fast.\n\nWhere possible, compile the code before executing it. Such libraries as PyPy and Numba for Python, or pqR for R, would compile the code into the OS (operating system) native binary code, which can signiﬁcantly increase the speed of data processing and model training.\n\nAnother important aspect is parallelization. If you work with modern libraries and modules, you can ﬁnd learning algorithms that exploit multicore CPUs. Some allow GPUs to speed up the training of neural networks and many other models. Training of some models, such as SVM, cannot be eﬀectively parallelized. In such cases, you can still exploit a multicore CPU by running multiple experiments in parallel. Run one experiment for each combination of hyperparameter values, geographical region, or user segment. Furthermore, compute each cross-validation fold in parallel with other folds.\n\nWhere possible, use a solid-state drive (SSD) to store the data. Use distributed computing; some implementations of learning algorithms are designed to run in distributed computing environments, such as Spark. Try to put all the needed data into the RAM of your laptop or server. It’s not uncommon today for data analysts to work on a server with 512 gigabytes or even one or more terabytes of RAM.\n\n8“Hidden Technical Debt in Machine Learning Systems” by Sculley et al. (2015).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n37",
      "content_length": 2999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "By reducing to a minimum the time needed to train a model, you can spend more time tweaking your model, testing data pre-processing ideas, feature engineering, neural network architectures, and other creative activities. The greatest beneﬁt for the machine learning project lies in the human touch and intuition. The more you, as a human, can work instead of waiting, the higher the chances that your machine learning project will be a success.\n\nReduce glue code to a minimum. This how Google engineers put it. Machine learning researchers tend to develop general purpose solutions as self-contained packages. A wide variety of these are available as open-source packages or from in-house code, proprietary packages, and cloud-based platforms. Using generic packages often results in a glue-code system design pattern, in which a massive amount of supporting code is written to get data into and out of general-purpose packages.\n\nGlue code is costly in the long term. It tends to freeze a system to the peculiarities of a speciﬁc package. Testing alternatives may become prohibitively expensive. Using a generic package this way inhibits improvements. It becomes harder to take advantage of domain-speciﬁc properties, or to tweak the objective function, and to achieve a domain-speciﬁc goal. A mature system might become (at most) 5% machine learning code and (at least) 95% glue code. It may be less costly to create a clean native solution, rather than re-use a generic package.\n\nAn important strategy for combating glue code is to wrap black-box machine learning packages into common APIs used by the entire organization. Infrastructure becomes more reusable and it reduces the cost of changing packages.\n\nIt is recommended to learn to switch between at least two programming languages: one for fast prototyping (like Python) and one for fast implementation (like C++). Modern languages like Go, Kotlin, and Julia may work well for both cases, but at the time of the writing of this book, these two languages have not developed an ecosystem of machine learning projects, as compared to more established counterparts.\n\n6.7.8 Test on Both Newer and Older Data\n\nIf you used a data dump from some time ago to create training, validation, and test sets, observe how your model behaves with data collected before and after this period. If it’s radically worse, there’s a problem.\n\nData leakage and distribution shift could be among the most likely reasons. Recall that data leakage is when information unavailable in the future or in the past was used to engineer a feature. Distribution shift is when properties of the data change over time.\n\n6.7.9 More Data Beats Cleverer Algorithm\n\nWhen confronted to insuﬃcient model performance, to improve the performance of the model, analysts are often tempted into crafting a more sophisticated learning algorithm or a pipeline.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n38",
      "content_length": 2925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "In practice, however, better results often come from getting more data, speciﬁcally, more labeled examples. If designed well, the data labeling process can allow a labeler to produce several thousand training examples daily. It can also be less expensive, compared to the expertise needed to invent a more advanced machine learning algorithm.\n\n6.7.10 New Data Beats Cleverer Features\n\nIf, despite adding more training examples and designing clever features, the performance of your model plateaus, think about diﬀerent information sources.\n\nFor example, if you want to predict whether user U will like a news article, try to add historical data about the user U as features. Or cluster all the users, and use the information on the k-nearest users to user U as new features. This is a simpler approach compared to programming very complex features, or combining existing features in a complex way.\n\n6.7.11 Embrace Tiny Progress\n\nMany tiny improvements to your model may give the expected result faster than looking for one revolutionary idea.\n\nFurthermore, by trying diﬀerent ideas, the analyst gets to know the data better, which might indeed help in ﬁnding that revolutionary idea.\n\n6.7.12 Facilitate Reproducibility\n\nMost machine learning algorithms are stochastic. For example, when we train a neural network, we initialize model parameters randomly; the minibatch stochastic gradient descent generates minibatches randomly; the decision trees in a random forest are built randomly; when we shuﬄe examples before splitting the data into three sets, we do it randomly; and so on. This means that when you train a model on the same data twice, you might end up having two diﬀerent models. In order to facilitate reproducibility, it’s recommended to set the value of the random seed used to initialize the pseudorandom number generator. If your random seed remains the same, then, if your data doesn’t change, you will obtain exactly the same model each time you train.\n\nThe random seed can be set as np.random.seed(15) (in NumPy and scikit-learn), tf.random.set_seed(15) in TensorFlow, torch.manual_seed(15) (in PyTorch), and set.seed(15) (in R). The seed value doesn’t matter as long as it remains constant. Even if a machine learning framework allows us to set the value of the random seed, there’s no guarantee that the code of the framework that uses randomization doesn’t change between versions of the framework. For reproducibility, each project’s dependencies should be isolated. It can be done in many ways: either by using tools such as virtualenv in Python and Packrat\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n39",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "in R, or by running machine learning experiments in standardized virtual machines or containers. We will talk more about virtualization in Section ?? in Chapter 8.\n\nWhen delivering the model, make sure it’s accompanied by all relevant information for reproducibility. Besides the description of the dataset and features, such as documentation and metadata considered in Sections ?? and ??, each model should contain the documentation with the following details:\n\na speciﬁcation of all hyperparameters, including the ranges considered, and the default values used,\n\nthe method used to select the best hyperparameter conﬁguration, • the deﬁnition of the speciﬁc measure or statistics used to evaluate the candidate models, and the value of it for the best model,\n\na description of the computing infrastructure used, and • the average runtime for each trained model, and an estimated cost of the training.\n\n6.8 Summary\n\nThe deep model training strategy has more moving parts, as compared to training shallow models. At the same time, it’s more principled and amenable to automation.\n\nInstead of training your model from scratch, it can be useful to start with a pre-trained model. Organizations with access to big data have trained and open-sourced very deep neural networks with architectures optimized for image or natural language processing tasks.\n\nA pre-trained model can be used in two ways: 1) its learned parameters can be used to initialize your own model, or 2) it can be used as a feature extractor for your model.\n\nUsing a pre-trained model to build your own is called transfer learning. The fact that deep models allow for transfer learning is one of the most important properties of deep learning.\n\nMinibatch stochastic gradient descent and its variants are the most frequently used cost function optimization algorithms for deep models.\n\nThe backpropagation algorithm computes the partial derivatives of each deep model parameter, using the chain rule for derivatives of complex functions. At each epoch, gradient descent updates all parameters using partial derivatives. The learning rate controls the signiﬁcance of an update. The process continues until convergence, the state where parameters’ values don’t change much after each epoch. Then the algorithm stops.\n\nThere are several popular upgrades to minibatch stochastic gradient descent, such as Momen- tum, RMSProp, and Adam. These algorithms update the learning rate automatically, based on the performance of the learning process. You do not need to choose the initial value of the learning rate, the decay schedule and rate, or the values of other related hyperparameters. These algorithms have demonstrated good performance in practice, and practitioners often use them instead of trying to manually tune the learning rate.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n40",
      "content_length": 2854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "In addition to L1 and L2 regularization, neural networks beneﬁt from neural network-speciﬁc regularizers: dropout, early stopping, and batch-normalization. Dropout is a simple but very eﬀective regularization method. Using batch-normalization is a best practice.\n\nEnsemble learning is training an ensemble model, which is a combination of several base models, each individually performing worse than the ensemble model. There are ensemble learning algorithms, such as random forest and gradient boosting, that build an ensemble of several hundred to thousands of weak models, and obtain a strong model that has a signiﬁcantly better performance than the performance of each weak model.\n\nStrong models can be combined into an ensemble model by averaging their outputs (for regression) or by taking a majority vote (for classiﬁcation). Model stacking, being the most eﬀective of the ensembling methods, consists of training a meta-model that takes the output of base models as input.\n\nIn addition to using over- and undersampling, imbalanced learning problems can be solved by applying class weighting and ensemble of resampled datasets. If you train your model using stochastic gradient descent, the class imbalance can be tackled in two additional ways: 1) by setting diﬀerent learning rates for diﬀerent classes, and 2) by making several consecutive updates of the model parameters each time you encounter an example of a minority class.\n\nFor imbalanced learning problems, the performance of the model is measured using adapted performance metrics such as per-class accuracy and Cohen’s kappa statistic.\n\nTroubleshooting a machine learning pipeline can be hard. Poor performance can be caused by a bug in your code, training data errors, learning algorithm issues, or pipeline design. In addition, learning can be sensitive to small changes in hyperparameters and dataset makeup.\n\nErrors made by a machine learning model can be uniform and appear in all use cases with the same rate, or focused and appear in just certain types of use cases.\n\nFocused errors are those that merit special attention, because by ﬁxing an error pattern, you ﬁx it once for many examples.\n\nThe performance of the model can be iteratively improved using the following simple process:\n\n1. Train the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set (100−300 examples). 3. Find the most frequent error patterns on that small validation set. Remove those examples from the validation set, because your model will now overﬁt to them.\n\n4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed (most errors look dissimilar).\n\nIn complex machine learning systems, the error analysis is done by parts. We ﬁrst substitute the predictions of one model for the perfect labels (such as human-provided labels), and see how the performance of the entire system improves. If it improves signiﬁcantly, then more eﬀort must be put in improving that speciﬁc model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n41",
      "content_length": 3155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "For reproducibility, set the random seed and make sure the model is accompanied by all relevant information.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n42",
      "content_length": 165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "7 Model Evaluation\n\nStatistical models play an increasingly important role in the modern organization. When applied in a business context, a model can aﬀect the organization’s ﬁnancial indicators. However, it may also present a liability risk. Therefore, any model running in production must be carefully and continuously evaluated.\n\nModel evaluation is the ﬁfth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nDepending on the model’s applicative domain and the organization’s business goals and constraints, model evaluation may include the following tasks:\n\nEstimate legal risks of putting the model in production. For example, some model predictions may indirectly communicate conﬁdential information. Cyber attackers or competitors may attempt to reverse engineer the model’s training data. Additionally, when used for prediction, some features, such as age, gender, or race, might result in the organization being considered as biased or even discriminatory.\n\nStudy the main properties of distributions of the training data versus the production data. By comparing the statistical distribution of examples, features, and labels, in both train- ing and production data, is how distribution shift is detected. A signiﬁcant diﬀerence between the two indicates a need to update the training data, and retrain the model. • Evaluate the performance of the model. Before the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training. The external data must include both historical and online examples from the production environment. The context of evaluation on the real-time, online data should closely resemble the production environment. 3\n\nStudy the main properties of distributions of the training data versus the production data. By comparing the statistical distribution of examples, features, and labels, in both train- ing and production data, is how distribution shift is detected. A signiﬁcant diﬀerence between the two indicates a need to update the training data, and retrain the model. • Evaluate the performance of the model. Before the model is deployed in production, its predictive performance must be evaluated on the external data, that is, data not used for training. The external data must include both historical and online examples from the production environment. The context of evaluation on the real-time, online data should closely resemble the production environment. 3",
      "content_length": 2527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Monitor the performance of the deployed model. The model’s performance may degrade over time. It is important to be able to detect this and, either upgrade the model by adding new data, or train an entirely diﬀerent model. Model monitoring must be a carefully designed automated process, and might include a human in the loop. We consider this in more detail in Chapter 9.\n\nIn this chapter, we look at some examples of the kinds of tricks that statisticians use during the model evaluation phase. Machine learning engineering is a developing discipline, and some questions still don’t have well established and easy to apply answers. In particular, the evaluation is presented from the point of view of an engineer, while each business has its own success criteria, which are unique. Before evaluating a machine learning solution, it is very important to ensure that the right people have done the most diﬃcult work in the project: ﬁguring out what success looks like and what are the right questions to ask in the form of business-appropriate metrics and objectives.\n\nA common reason for failure is engineers answering convenient questions with basic tools instead of answering the right questions with custom tools — something that may require consultation with a professional statistician after your project’s leaders and stakeholders have completed their part in your project. Note that some methods highlighted in this chapter, speciﬁcally those used in A/B testing (Section 7.2), are provided as examples only and might not be appropriate for your speciﬁc business problem. On important large-scale projects, it would be a mistake to try to do everything yourself. Timely collaboration with your leadership team and consulting a statistician is essential.\n\n7.1 Oﬄine and Online Evaluation\n\nIn Section ??, we overviewed the evaluation techniques applied in what’s called oﬄine model evaluation. An oﬄine model evaluation happens when the model is being trained by the analyst. The analyst tries out diﬀerent features, models, algorithms, and hyperparameters. Tools like confusion matrix and various performance metrics, such as precision, recall, and AUC, allow comparing candidate models, and guide the model training in the right direction.\n\nFirst, validation data is used to assess the chosen performance metric and compare models. Once the best model is identiﬁed, the test set is used, also in oﬄine mode, to again assess the best model’s performance. This ﬁnal oﬄine assessment guarantees post-deployment model performance. In this chapter, we talk, among other topics, about establishing statistical bounds on the oﬄine test performance of the model.\n\nA signiﬁcant part of the chapter is devoted to the online model evaluation, that is, testing and comparing models in production by using online data. The diﬀerence between oﬄine and online model evaluation, as well as the placement of each type of evaluation in a machine learning system, is illustrated in Figure 2.\n\nIn Figure 2, the historical data is ﬁrst used to train a deployment candidate. Then it is evaluated oﬄine, and, if the result is satisfactory, deployment candidate becomes the deployed\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 3222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Figure 2: The placement of oﬄine and online model evaluations in a machine learning system.\n\nmodel, and starts accepting user queries. Then, user queries and the model predictions are used for an online evaluation of the model. The online data is then used to improve the model. To close the loop, the online data is permanently copied to the oﬄine data repository.\n\nWhy do we evaluate both oﬄine and online? The oﬄine model evaluation reﬂects how well the analyst succeeded in ﬁnding the right features, learning algorithm, model, and values of hyperparameters. In other words, the oﬄine model evaluation reﬂects how good the model is from an engineering standpoint.\n\nOnline evaluation, on the other hand, focuses on measuring business outcomes, such as customer satisfaction, average online time, open rate, and click-through rate. This informa- tion may not be reﬂected in historical data, but it’s what the business really cares about. Furthermore, oﬄine evaluation doesn’t allow us to test the model in some conditions that can be observed online, such as connection and data loss, and call delays.\n\nThe performance results obtained on the historical data will hold after deployment only if the distribution of the data remains the same over time. In practice, however, it’s not always the case. Typical examples of a distribution shift include the ever-changing interests of the user of a mobile or online application, instability of ﬁnancial markets, climate change, or wear of a mechanical system whose properties the model is intended to predict.\n\nAs a consequence, the model must be continuously monitored once deployed in production. When a distribution shift happens, the model must be updated with new data and re-deployed. One way of doing such monitoring is to compare the performance of the model on online and historical data. If the performance on online data becomes signiﬁcantly worse, as compared to historical, it’s time to retrain the model.\n\nThere are diﬀerent forms of online evaluation, each serving a diﬀerent purpose. For example,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 2114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "runtime monitoring is checking whether the running system meets the runtime requirements.\n\nAnother common scenario is to monitor user behavior in response to diﬀerent versions of the model. One popular technique used in this scenario is A/B testing. We split the users of a system into two groups, A and B. The two groups are served the old and the new models, respectively. Then we apply a statistical signiﬁcance test to decide whether the performance of the new model is better than the old one.\n\nMulti-armed bandit (MAB) is another popular technique of online model evaluation. Similar to A/B testing, it identiﬁes the best performing models by exposing model candidates to a fraction of users. Then it gradually exposes the best model to more users, by keeping gathering performance statistics until it’s reliable.\n\n7.2 A/B Testing\n\nA/B testing is one of the most frequently used statistical techniques. When applied to online model evaluation, it allows us to answer such questions as, “Does the new model mB work better in production than the existing model mA?” or, “Which of the two model candidates works better in production?”\n\nA/B testing is often used on websites and mobile applications to test whether a speciﬁc change in the design or wording positively aﬀects business metrics such as user engagement, click-through rate, or sales rate.\n\nImagine we want to decide whether to replace an existing (old) model in production with a new model. The live traﬃc that contains input data for the model is split into two disjoint groups: A (control) and B (experiment). Group A traﬃc is routed to the old model, while group B traﬃc is routed to the new model.\n\nBy comparing the performance of the two models, a decision is made about whether the new model performs better than the old model. The performance is compared using statistical hypothesis testing.\n\nIn general, statistical hypothesis testing maintains a null hypothesis and an alternative hypothesis. An A/B test is usually formulated to answer the following question: “Does the new model lead to a statistically signiﬁcant change in this speciﬁc business metric?” The null hypothesis states that the new model doesn’t change the average value of the business metric. The alternative hypothesis states that the new model changes the average value of the metric.\n\nA/B test is not one test, but a family of tests. Depending on the business performance metric, a diﬀerent statistical toolkit is used. However, the principle of splitting the users into two groups, and measuring the statistical signiﬁcance of the diﬀerence in the metric values between diﬀerent groups, remains the same.\n\nThe description of all formulations of A/B tests is beyond the scope of this book. Here we will consider only two formulations, but they apply to a wide range of practical situations.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "7.2.1 G-Test\n\nThe ﬁrst formulation of A/B test is based on the G-test. It is appropriate for a metric that counts the answer to a “yes” or “no” question. An advantage of the G-test is that you can ask any question, as long as only two answers are possible. Examples of questions:\n\nWhether the user bought the recommended article? • Whether the user has spent more than $50 during a month? • Whether the user renewed the subscription?\n\nLet’s see how to apply it. We want to decide whether the new model works better than the old one. To do that, we formulate a yes-or-no question that deﬁnes our metric. Then we randomly divide the users into groups A and B. The users of group A are routed to the environment running the old model, while the group’s B traﬃc is routed to the new model. Observe the actions of each user and record the answer as “yes” or “no.” Fill the following table:\n\nYes\n\nNo\n\nA\n\nB\n\nFigure 3: The counts of answers to the yes-or-no question by users from groups A and B.\n\nIn the above table, ˆayes is the number of users in group A, for which the answer to the question is “yes,” ˆbyes is the number of users in group B, for which the answer to the question is “yes,” ˆano is the number of users in group A, for which the answer to the question is “no,” and so on. Similarly, nyes = ˆayes + ˆbyes, nno = ˆano + ˆbno, na = ˆayes + ˆano, nb = ˆbyes + ˆbno, and, ﬁnally ntotal = nyes + nno = na + nb.\n\nNow, ﬁnd the expected numbers of “yes” and “no” answers for A and B, i.e. the number of “yes” and “no” we would get if versions A and B were equivalent.\n\nayes\n\nano\n\nbyes\n\nbno\n\ndef= na\n\ndef= na\n\ndef= nb\n\ndef= nb\n\nnyes ntotal nno ntotal nyes ntotal nno ntotal\n\n,\n\n,\n\n,\n\n.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(1)\n\n7",
      "content_length": 1747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\nNow, ﬁnd the value of the G-test as,1\n\nG def= 2\n\nˆayes ln\n\n(cid:18)ˆayes ayes\n\n(cid:19)\n\n+ ˆano ln\n\n(cid:18)ˆano ano\n\n(cid:19)\n\n+ ˆbyes ln\n\nˆbyes byes\n\n!\n\n+ ˆbno ln\n\nˆbno bno\n\n!!\n\nG is a measure of how diﬀerent the samples from A and B are. Statistically speaking, under the null hypothesis (A and B are equal), G follows a chi-square distribution with one degree of freedom:\n\nG ∼ χ2 1\n\nIn other words, if A and B were equal, we expect G to be small. A large value of G would make us suspicious that one of the models performs better than the other. For example, imagine you calculated G = 3.84. If A and B were equal (i.e. under the null hypothesis) the probability of observing G ≥ 3.84 is about 5%. We often refer to this probability as the p-value.\n\nIf the p-value is small enough (e.g., below 0.05) then the performances of the new and the old model are very likely diﬀerent (the null hypothesis is rejected). In this case, if byes is higher than ayes, then the new model is very likely to work better than the old model; otherwise, the old model is better.\n\nIf the p-value corresponding to the value of G is not small enough then the observed diﬀerence of performance between the new and the old model is not statistically signiﬁcant, and you can keep the old model in production.\n\nIt is convenient to ﬁnd the p-value of the G-test using a programming language of your choice. In Python, it can be done in the following way:\n\nfrom scipy.stats import chi2 def get_p_value(G):\n\np_value = 1 - chi2.cdf(G, 1) return p_value\n\nThe following code will work for R:\n\nget_p_value <- function(G) {\n\np_value <- pchisq(G, df=1, lower.tail=FALSE) return(p_value)\n\n}\n\nStatistically, the result of the G-test is valid if we have at least 10 “yes” and “no” results in each of the two groups, though this estimate should be taken with a grain of salt. If testing is not too expensive, then having about 1000 “yes” and “no” results in each of the two groups,\n\n1More details on the derivation of the above formula can be found in a statistics textbook or on Wikipedia.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n.\n\n8",
      "content_length": 2137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "1\n\n2\n\n3\n\nwith at least 100 answers of each type in each group, should be enough. Note that the total number of answers in the two groups can be diﬀerent.\n\nIf you can’t reach at least 100 answers of each type in each group at a reasonable cost, you can use an approximation of the p-value of a very similar test using Monte-Carlo simulation.\n\nThe following code will work for R:\n\np_value <- chisq.test(x,\n\nsimulate.p.value = TRUE)$p.value\n\n}\n\nWhere x is the 2 × 2 contingency table shown in Figure 3.\n\nNote that it is possible to test more than two models (e.g. models A, B, and C) and more than two possible answers to the question that deﬁne our metric (e.g., “yes,” “no,” “maybe”). If we want to test k diﬀerent models and l diﬀerent possible answers, the G statistic would follow a chi-square distribution with (k − 1) × (l − 1) degrees of freedom. The problem here is that a test with multiple models and answers will tell you whether there is something diﬀerent somewhere between your models, but it will not tell you where is the diﬀerence. In practice, it is easier to compare your current model with only one new model and to formulate a question metric with a binary answer. More complex experiment testing is outside the scope of this book.\n\nNote that it could be tempting, when we have more than two models, to do binary comparisons of pairs of models using a test designed for comparing two models. This is not recommended, however, as it could be scientiﬁcally wrong. It’s better to consult a statistician.\n\n7.2.2 Z-Test\n\nThe second formulation of A/B test applies when the question for each user is, “How many?” or, “How much?” (as opposed to a yes-or-no question considered in the previous subsection). Examples of questions include:\n\n1. How much time a user has spent on the website during a session? 2. How much money a user has spent during a month? 3. How many news articles a user has read during a week?\n\nFor simplicity of illustration, let’s measure the time a user spends on a website where our model is deployed. As usual, users are routed to versions A and B of the website, where version A serves the old model and version B serves the new model. The null hypothesis is that users of both versions spend, on average, the same amount of time. The alternative hypothesis is that they spend more time on website B than on website A. Let nA be the number of users routed to version A and nB be the number of users routed to version B. Let i and j denote users from groups A and B respectively.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 2571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "To compute the value of the Z-test, we ﬁrst compute sample mean and sample vari- ance for A and B. The sample mean is given by:\n\nˆµA\n\ndef=\n\n1 nA\n\nnAX\n\ni=1\n\nai,\n\nˆµB\n\ndef=\n\n1 nB\n\nnAX\n\nj=1\n\nbj,\n\nwhere ai and bj is the time spent on the website by, respectively, users i and j.\n\nThe sample variance for A and B is given, respectively, by,\n\nˆσ2 A\n\ndef=\n\n1 nA\n\nnAX\n\ni=1\n\n(ˆµA − ai)2,\n\nˆσ2 B\n\ndef=\n\n1 nB\n\nnBX\n\nj=1\n\n(ˆµB − bj)2.\n\nThe value of the Z-test is then given by,\n\nZ def=\n\nˆµB − ˆµA q ˆσ2 + ˆσ2 A B nA nB\n\n.\n\nThe larger Z, the more likely the diﬀerence between A and B is signiﬁcant. Under the null hypothesis (i.e. A and B are equivalent), Z approximately follows a standardized normal distribution,\n\nZ ≈ N(0,1)\n\nThis is true only if the sample size is large and if σ2 advice from a statistician.\n\nA ≈ σ2\n\nB. If not, it is recommended to ask\n\nAs for the G-test, we will use the p-value to decide whether or not Z is large enough to think that the time spent on B is really greater than time spent on A. To compute the p-value, you check the probability of getting a Z-value from this distribution that is at least as extreme (out of line with the null hypothesis) as the Z-value you calculated. For example, let’s imagine your sample gave you Z = 2.64. If A and B were equal, the probability of observing Z ≥ 2.64 is about 5%.\n\nTo see the result of the test, you compare the p-value with the signiﬁcance level you chose. If your signiﬁcance level is 5%, then if the p-value is below 0.05, we reject the null hypothesis\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n(2)\n\n(3)\n\n10",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "1\n\n2\n\n3\n\n4\n\n1\n\n2\n\n3\n\n4\n\nthat says that the diﬀerence in performance of the two models is not statistically signiﬁcant. Thus, the new model works better than the old one.\n\nIf the p-value is above or equal to 0.05, then we do not reject the null hypothesis. Note that this is not the same as accepting the null hypothesis. The two models could still be diﬀerent, we just didn’t get evidence in support of that. In this case, we will stick with the old model unless evidence changes our mind. No evidence means we keep doing what we were doing. Note also that we cannot simply keep gathering evidence until the p-value goes below 0.05, as it would not be scientiﬁcally sound. It’s recommended to consult a statistician and design a diﬀerent test.\n\nAs for signiﬁcance levels, there’s no universal consensus on which threshold is optimal. The values of 0.05 or 0.01 are commonly used in practice. They were favorites of a trendsetting statistician Ronald Fisher in the 1920s. You should select a higher or a lower value if it’s appropriate for your application. The lower the value, the more evidence it takes to change your mind.\n\nSimilar to the G-test, it is convenient to ﬁnd the p-value of the Z-test using a programming language. In Python, it can be done in the following way:\n\nfrom scipy.stats import norm def get_p_value(Z):\n\np_value = norm.sf(Z) return p_value\n\nThe following code will work for R:\n\nget_p_value <- function(Z) {\n\np_value <- 1-pnorm(Z) return(p_value)\n\n}\n\nFor best results, it is recommended to set nA and nB to a value 1000 or higher.\n\n7.2.3 Concluding Remarks and Warnings\n\nAs mentioned in the beginning of this chapter, some methods highlighted in this chapter are provided as examples only and might not be appropriate for your speciﬁc business problem. In particular, the two statistical tests presented above are taught in schools and are indeed often used in practice, but, unfortunately, not all of those uses are appropriate for your business problem. While pointing this out, Cassie Kozyrkov, the Chief Decision Scientist at Google and one of the reviewers of this chapter, emphasized that the above two tests are rarely a good idea to apply in practice because they only show that two models are diﬀerent, but they don’t show whether the diﬀerence is “of at least x.” If replacing the old model with the new one has a signiﬁcant cost or poses a risk, then just knowing that the new model is “somewhat” better is not enough to make a replacement decision. In this case, an adjusted\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 2567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "test must be speciﬁcally crafted for the problem at hand, and the best way to do is to consult a statistician.2\n\nCarefully test the programming code of your A/B test. You will only have a valid model evaluation if you implemented everything right. Otherwise, you will not know that something is wrong: your test will not reveal that it’s broken.\n\nAlso, make sure to apply measurements in groups A and B at the same time. Remember that traﬃc on a website behaves diﬀerently at diﬀerent times of the day, or at diﬀerent days of the week. For the purity of the experiment, avoid comparing measurements from diﬀerent times. The same reasoning applies to other possible measurable parameters that might signiﬁcantly aﬀect user behavior, such as country of residence, speed of internet connection, or version of web browser.\n\n7.3 Multi-Armed Bandit\n\nA more advanced, and often preferable way of online model evaluation and selection, is multi-armed bandit (MAB). A/B testing has one major drawback. The number of test results in groups A and B you need to calculate the value of the A/B test is high. A signiﬁcant portion of users routed to a suboptimal model would experience suboptimal behavior for a long time.\n\nIdeally, we would like to expose a user to a suboptimal model as few times as possible. At the same time, we need to expose users to each of the two models a number of times suﬃcient to get reliable estimates of both models’ performance. This is known as the exploration- exploitation dilemma: on one hand, we want to explore the models’ performance enough to be able to reliably choose the better one. On the other hand, we want to exploit the performance of the better model as much as possible.\n\nIn probability theory, the multi-armed bandit problem is a problem in which a ﬁxed and limited set of resources must be allocated between competing choices in a way that maximizes the expected reward. Each choice’s properties are only partially known at the time of allocation, and may become better understood as time passes and we allocate resources to the choice.\n\nLet’s see how the multi-armed bandit problem applies to an online evaluation of two models. (The approach for more than two models is the same.)\n\nThe limited set of resources we have are the users of our system. The competing choices, also called “arms,” are our models. We can allocate a resource to a choice (in other words, we can “play an arm”) by routing a user to a version of the system running a speciﬁc model. We want to maximize the expected reward, where the reward is given by the business performance metric. Examples might be the average amount of time spent on the website during a session, the average number of news articles read during a week, the percentage of users who purchased the recommended article, etc.\n\n2Unfortunately, in a compact book describing all special cases and tests would be impractical. Please\n\nconsult the book’s companion wiki from time to time. More statistical tests will be added over time.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 3067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nUCB1 (for Upper Conﬁdence Bound) is a popular algorithm for solving the multi-armed bandit problem. The algorithm dynamically chooses an arm, based on the performance of that arm in the past, and how much the algorithm knows about it. In other words, UCB1 routes the user to the best performing model more often when its conﬁdence about the model performance is high. Otherwise, UCB1 might route the user to a suboptimal model so as to get a more conﬁdent estimate of that model’s performance. Once the algorithm is conﬁdent enough about the performance of each model, it almost always routes users to the best performing model.\n\nThe mathematics of UCB1 works as follows. Let ca denote the number of times the arm a was played since the beginning, and let va denote the average reward obtained from playing that arm. The reward corresponds to the value of the business performance metric. For the purpose of illustration, let the metric be the average time spent by the user in the system during one session. The reward for playing an arm is, thus, a particular session duration.\n\nIn the beginning, ca and va are zero for all arms, a = 1,...,M. Once an arm a is played, a reward r is observed, and ca is incremented by 1; va is then updated as follows:\n\nva ←\n\nca − 1 ca\n\n× va +\n\nr ca\n\n.\n\nAt each time step (that is, when a new user logs in), the arm to play (that is, the version of the system the user will be routed to) is chosen as follows. If ca = 0 for some arm a, then this arm is played; otherwise, the arm with the greatest UCB value is played. The UCB value of an arm a, denoted as ua, is deﬁned as follows:\n\nua\n\ndef= va +\n\ns\n\n2 × log(c) ca\n\n, where c def=\n\nM X\n\na\n\nca.\n\nThe algorithm is proven to converge to the optimal solution. That is, UCB1 will end up playing the best performing arm most of the time.\n\nIn Python, the code that implements UCB1, would look as follows:\n\nclass UCB1():\n\ndef __init__(self, n_arms): self.c = [0]*n_arms self.v = [0.0]*n_arms self.M = n_arms return\n\ndef select_arm(self):\n\nfor a in range(self.M):\n\nif self.c[a] == 0:\n\nreturn a u = [0.0]*self.M\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nc = sum(self.c) for a in range(self.M):\n\nbonus = math.sqrt((2 * math.log(c)) / float(self.c[a])) u[a] = self.v[a] + bonus\n\nreturn u.index(max(u))\n\ndef update(self, a, r): self.c[a] += 1 v_a = ((self.c[a] - 1) / float(self.c[a])) * self.v[a] \\\n\n+ (r / float(self.c[a]))\n\nself.v[a] = v_a return\n\nThe corresponding code in R would look as shown below:\n\nsetClass(\"UCB1\", representation(count=\"numeric\", value=\"numeric\", M=\"numeric\"))\n\nsetGeneric(\"select_arm\", function(x) standardGeneric(\"select_arm\")) setMethod(\"select_arm\", \"UCB1\", function(x) {\n\nfor (a in seq(from = 1, to = x@M, by = 1)) {\n\nif(x@count[a] == 0) {\n\nreturn(a)\n\n}\n\n} u <- rep(0.0, x@M) count <- sum(x@count) for (a in seq(from = 1, to = x@M, by = 1)){\n\nprint(a) bonus <- sqrt((2 * log(count)) / x@count[a]) u[a] <- x@value[a] + bonus\n\n} match(c(max(u)),u)\n\n})\n\nsetGeneric(\"update\", function(x, a, r) standardGeneric(\"update\")) setMethod(\"update\", \"UCB1\", function(x, a, r) {\n\nx@count[a] <- x@count[a] + 1 v_a <- ((x@count[a] - 1) / x@count[a]) * x@value[a] + (r / x@count[a]) x@value[a] <- v_a\n\n})\n\nUCB1 <- function(M) {\n\nnew(\"UCB1\", count = rep(0, M), value = rep(0.0, M), M = M)\n\n}\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "1\n\n2\n\n7.4 Statistical Bounds on the Model Performance\n\nWhen reporting the model performance, sometimes, besides the value of the metric, it is required to also provide the statistical bounds, also known as the statistical interval.\n\nA reader familiar with other books on machine learning or some popular online blogs might wonder, why we use the term “statistical interval” and not “conﬁdence interval.” The reason is that in some machine learning literature, what the authors call a “conﬁdence interval” is in fact a “credible interval.” The diﬀerence between the two is clear and important for a statistician because the two terms have diﬀerent meanings in frequentist and Bayesian statistics. In this book, I decided not to burden the reader with the subtlety of the diﬀerence between the two terms. For a non-expert in statistics, it would be beneﬁcial to think of the statistical interval as follows: a 95% statistical interval indicates that there’s a 95% chance the parameter you’re estimating is between the intervals bounds. Strictly speaking, this is the deﬁnition of the credible interval. A conﬁdence interval’s interpretation is subtly diﬀerent, most newcomers to statistics won’t start to appreciate the diﬀerence until they’re a few textbooks in. For our purposes, the above interpretation of a statistical interval will suﬃce.\n\nThere are several techniques that allow establishing statistical bounds for a model. Some techniques apply to classiﬁcation models, and some can be applied to regression models. We will consider several techniques in this section.\n\n7.4.1 Statistical Interval for the Classiﬁcation Error\n\nIf you report the error ratio “err” for a classiﬁcation model (where err def= 1 − accuracy), then the following technique can be used to obtain the statistical interval for “err.”\n\nLet N be the size of the test set. Then, with probability 99%, “err” lies in the interval,\n\n[err − δ,err + δ],\n\nwhere δ def= zN\n\nqerr(1−err) N\n\n, and zN = 2.58.\n\nThe value of zN depends on the required conﬁdence level. For the conﬁdence level of 99%, zN = 2.58. For other conﬁdence level values, the values of zN can be found in the table below:\n\nconﬁdence level zN\n\n80% 90% 95% 98% 99% 2.58 1.96 1.28\n\n1.64\n\n2.33\n\nAs with p-values, it is convenient to ﬁnd the value of zN using a programming language. In Python, it can be done in the following way:\n\nfrom scipy.stats import norm def get_z_N(confidence_level): # a value in (0,100)\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 2502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "3\n\n4\n\n1\n\n2\n\n3\n\n4\n\n1\n\n2\n\nz_N = norm.ppf(1-0.5*(1 - confidence_level/100.0)) return z_N\n\nThe following code will work for R:\n\nget_z_N <- function(confidence_level) {# a value in (0,100)\n\nz_N <- qnorm(1-0.5*(1 - confidence_level/100.0)) return(z_N)\n\n}\n\nIn theory, the above technique works even for very tiny test sets with N ≥ 30. However, a more accurate rule of thumb for obtaining the minimum size N of the test set is as follows: ﬁnd the value of N such that N × err(1 − err) ≥ 5. Intuitively, the greater the size of the test set, the lower our uncertainty about the true performance of the model.\n\n7.4.2 Bootstrapping Statistical Interval\n\nA popular technique for reporting the statistical interval for any metric, and which applies to both classiﬁcation and regression, is based on the idea of bootstrapping. Bootstrapping is a statistical procedure that consists of building B samples of a dataset, and then training a model or computing some statistic using those B samples. In particular, the random forest learning algorithm is based on this idea.\n\nHere’s how bootstrapping applies for building a statistical interval for a metric. Given the test set, we create B random samples Sb, one for each b = 1,...,B. To obtain a sample Sb for some b, we use sampling with replacement. Sampling with replacement means that we start with an empty set, and then pick at random an example from the test set and put its exact copy in Sb by keeping the original example in the test set. We keep picking examples at random and putting them to Sb until |Sb| = N.\n\nOnce we have B bootstrap samples of the test set, we compute the value of the performance metric mb using each sample Sb as the test set. Sort the B values in ascending order. Then ﬁnd the value S of the sum of all B values of the metric: S def= PB b=1 mb. To obtain a c percent statistical interval for the metric, pick the tightest interval between a minimum a and a maximum b such that the sum of the values mb that lie in that interval accounts for at least c percent of S. Our statistical interval is then given by [a,b].\n\nThe above paragraph might sound vague, so let’s illustrate it with an example. Let’s have B = 10. Let the values of the metric, computed by applying the model to B bootstrap samples, be [9.8,7.5,7.9,10.1,9.7,8.4,7.1,9.9,7.7,8.5]. First, we sort those values in the increasing order: [7.1,7.5,7.7,7.9,8.4,8.5,9.7,9.8,9.9,10.1]. Let our conﬁdence level c be 80%. Then, the minimum a of the statistical interval will be 7.46 and the maximum b will be 9.92. The above two values were found using the percentile function in Python:\n\nfrom numpy import percentile def get_interval(values, confidence_level):\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 2742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n# confidence_level is a value in (0,100) lower = percentile(values, (100.0-confidence_level)/2.0) upper = percentile(values, confidence_level+((100.0-confidence_level)/2.0)) return (lower, upper)\n\nThe same can be done in R by using the quantile function:\n\nget_interval <- function(values, confidence_level) {\n\n# confidence_level is a value in (0,100) cl <- confidence_level/100.0 quant <- quantile(values, probs = c((1.0-cl)/2.0, cl+((1.0-cl)/2.0)), names = FALSE)\n\nreturn(quant)\n\n}\n\nOnce you have the boundaries a = 7.46 and b = 9.92 of the statistical interval, you can report that the value of the metric for your model lies in the interval [7.46,9.92] with conﬁdence 80%.\n\nIn practice, analysts use conﬁdence levels of either 95% or 99%. The higher the conﬁdence, the wider the interval. The number B of bootstrap samples is usually set to 100.\n\n7.4.3 Bootstrapping Prediction Interval for Regression\n\nUntil now, we considered the statistical interval for an entire model and a given performance metric. In this section, we will use bootstrapping to compute the prediction interval for a regression model and a given feature vector x, which this model receives as input.\n\nWe want to answer the following question. Given a regression model f and an input feature vector x, what is an interval of values [fmin(x),fmax(x)] such that the prediction f(x) lies inside that interval with conﬁdence c percent?\n\nThe bootstrapping procedure here is similar. The only diﬀerence is that now we build B bootstrap samples of the training set (and not the test set). By using B bootstrap samples as B training sets, we build B regression models, one per bootstrap sample. Let the input feature vector be x. Fix a conﬁdence level c. Apply B models to x and obtain B predictions. Now, by using the same technique as above, ﬁnd the tightest interval between a minimum a and a maximum b such that the sum of the values of predictions that lie in the interval accounts for at least c percent of the sum of B predictions. Then return the prediction f(x), and state that, with conﬁdence c percent, it lies in the interval [a,b].\n\nAs previously, the conﬁdence level usually is either 95% or 99%. The number B of bootstrap samples is set to 100 (or as many as the time allows).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "7.5 Evaluation of Test Set Adequacy\n\nIn traditional software engineering, tests are used to identify defects in the software. The collection of tests is constructed in such a way that they allow discovery of bugs in the code before the software reaches production. The same approach applies to the testing of all the code developed “around” the statistical model: the code that gets the input from the user, transforms it into features, and the code that interprets the outputs of the model, and serves the result to the user.\n\nHowever, an additional evaluation must be applied to the model itself. The test examples used to evaluate the model must also be designed in such a way that they allow the discovery of the model’s defective behavior before the model reaches production.\n\n7.5.1 Neuron Coverage\n\nWhen we evaluate a neural network, especially one to be used in a mission-critical scenario, such as a self-driving car or a space rocket, our test set must have good coverage. Neuron coverage of a test set for a neural network model is deﬁned as the ratio of the units (neurons) activated by the examples from the test set, to the total number of units. A good test set has close to 100% neuron coverage.\n\nA technique for building such a test set is to start with a set of unlabeled examples, and all units of the model uncovered. Then, iteratively, we\n\n1) randomly pick an unlabeled example i and label it, 2) send the feature vector xi to the input of the model, 3) observe which units in the model were activated by xi, 4) if the prediction was correct, mark those units as covered, 5) go back to step 1; continue iterating until the neuron coverage becomes close to 100%.\n\nA unit is considered activated when its output is above a certain threshold. For ReLU, it’s usually zero; for a logistic sigmoid, it’s 0.5.\n\n7.5.2 Mutation Testing\n\nIn software engineering, good test coverage for a software under test (SUT) can be determined using the approach known as mutation testing. Let’s have a set of tests designed to test an SUT. We generate several “mutants” of the SUT. A mutant is a version of the SUT in which we randomly make some modiﬁcations, such as replacing in the source code, a “+” with a “−”, a “<” with a “>”, delete the else command in an if-else statement, and so on. Then we apply the test set to each mutant, and see if at least one test breaks on that mutant. We say that we kill a mutant if one test breaks on it. We then compute the ratio of killed mutants in the entire collection of mutants. A good test set makes this ratio equal to 100%.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 2628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "In machine learning, a similar approach can be followed. However, to create a mutant statistical model, instead of modifying the code, we modify the training data. If the model is deep, we can also randomly remove or add a layer, or remove or replace an activation function. The training data can be modiﬁed by,\n\nadding duplicated examples, • falsifying the labels of some examples, • removing some examples, or • adding random noise to the values of some features.\n\nWe say that we kill a mutant if at least one test example gets a wrong prediction by that mutant statistical model.\n\n7.6 Evaluation of Model Properties\n\nWhen we measure the quality of the model according to some performance metric, such as accuracy or AUC, we evaluate its correctness property. Besides this commonly evaluated property of the model, it can be appropriate to evaluate other properties of the model, such as robustness and fairness.\n\n7.6.1 Robustness\n\nThe robustness of a machine learning model refers to the stability of the model performance after adding some noise to the input data. A robust model would exhibit the following behavior. If the input example is perturbed by adding random noise, the performance of the model would degrade proportionally to the level of noise.\n\nConsider an input feature vector x. Let us, before applying a model f to that input example, modify the values of some features, chosen randomly, by replacing them with a zero, to obtain a modiﬁed input x0. Continue randomly choosing and replacing values of features in x, as long as the Euclidean distance between x and x0 remains below some δ. Then apply the model f to x and x0 to obtain predictions f(x) and f(x0). Fix values of δ and (cid:15). The model f is said to be (cid:15)-robust to a δ-perturbation of the input, if, for any x and x0, such that kx − x0k ≤ δ, we have |f(x) − f(x0)| ≤ (cid:15).\n\nIf you have several models that perform similarly according to the performance metric, you would prefer to deploy in production a model that is (cid:15)-robust, when applied to the test data, with the smallest (cid:15). However, in practice, it’s not always clear how to set the appropriate value of δ. A more practical way to identify a robust model among several candidates is as follows.\n\nLet us say that a certain test set is δ-perturbed if we obtained it by applying a δ-perturbation to all examples in a certain original test set. Pick the model f you want tested for robustness. Set a reasonable value of ˆ(cid:15) such that, if the model prediction in production is not farther from the correct prediction than ˆ(cid:15), you would consider that acceptable. Start with a small value of\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 2719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "δ and build a δ-perturbed dataset. Find the minimum (cid:15) such that for each example x from the original test set and its counterpart x0 from the δ-perturbed test set, |f(x) − f(x0)| ≤ (cid:15).\n\nIf (cid:15) ≥ ˆ(cid:15), you have chosen too high a value for δ; set a lower value and start over.\n\nIf (cid:15) < ˆ(cid:15), then slightly increase δ, build a new δ-perturbed test set, ﬁnd (cid:15) for this new δ-perturbed test set, and continue increasing δ as long as (cid:15) remains less than ˆ(cid:15). Once you ﬁnd the value of δ = ˆδ where (cid:15) ≥ ˆ(cid:15), note that the model f you are testing for robustness is ˆ(cid:15)-robust to ˆδ-perturbation of the input. Now pick another model you want to test for robustness, and ﬁnd its ˆδ; continue like that until all models are tested. Once you have the value of ˆδ-perturbation for each model, deploy in production the model whose ˆδ is the greatest.\n\n7.6.2 Fairness\n\nMachine learning algorithms tend to learn what humans are teaching them. The teaching comes in the form of training examples. Humans have biases which may aﬀect how they collect and label data. Sometimes, bias is present in historical, cultural, or geographical data. This, in turn, as we have seen in Section ?? in Chapter 3, may lead to biased models.\n\nThe attributes that are sensitive and need protection from unfairness are called protected or sensitive attributes. Examples of legally-recognized and protected attributes include race, skin color, gender, religion, national origin, citizenship, age, pregnancy, familial status, disability status, veteran status, and genetic information.\n\nFairness is often domain-speciﬁc, and each domain may have its own regulations. Regulated domains include credit, education, employment, housing, and public accommodation.\n\nThe deﬁnition of fairness varies greatly, depending on the domain. At the time of writing this book, there is no ﬁrm consensus, in the scientiﬁc and technical literature, on what is fairness. Most commonly cited concepts are demographic parity and equal opportunity.\n\nDemographic parity (also known as statistical parity, or independence parity) means the proportion of each segment of a protected attribute receives a positive prediction from the model at equal rates.\n\nLet a positive prediction mean “acceptance to university,” or “granting a loan.” Mathemati- cally, demographic parity is deﬁned as follows. Let G1 and G2 be the two disjoint groups belonging to the test data, divided by a sensitive attribute j, such as gender. Let x(j) = 1 if x represents a woman, and x(j) = 0 otherwise. A binary model f under test satisﬁes demographic parity if Pr(f(xi) = 1|xi ∈ G1) = Pr(f(xk) = 1|xk ∈ G2). That is, as measured on the test data, the chance to predict 1 with the model f for women is the same as the chance to predict 1 for men.\n\nThe exclusion of the protected attributes from the feature vector in the training data doesn’t guarantee that the model will have demographic parity, as some of the remaining features\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 3074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "may be correlated with the excluded ones.\n\nEqual opportunity means each group gets a positive prediction from the model at equal rates, assuming that people in this group qualify for it.\n\nMathematically, a binary model f under test satisﬁes equal opportunity if Pr(f(xi) = 1|xi ∈ G1 and yi = 1) = Pr(f(xk) = 1|xk ∈ G2 and yk = 1), where yi and yk are the actual labels of the feature vectors xi and xk, respectively. The above equality means that, as measured on the test data, the chance to predict 1 by the model f for women who qualify for that prediction is the same as the chance to predict 1 for men who also qualify. In the terms of the confusion matrix, equal opportunity requires the true positive rate (TPR) to be equal for each value of the protected attribute.\n\n7.7 Summary\n\nAll statistical models running in production must be carefully and continuously evaluated.\n\nDepending on the model’s applicative domain and the organization’s goals and constraints, model evaluation will include the following tasks:\n\nestimate legal risks of putting the model in production, • understand the main properties of the distribution of the data used to train the model, • evaluate the performance of the model prior to deployment, and • monitor the performance of the deployed model.\n\nAn oﬄine model evaluation happens after the model was trained. It is based on the historical data. The online model evaluation consists of testing and comparing models in the production environment using online data.\n\nA popular technique of online model evaluation is A/B testing. When performing A/B testing, we split users into two groups, A and B. The two groups are served the old and the new models, respectively. Then we apply a statistical signiﬁcance test to decide whether the new model is statistically diﬀerent from the old model.\n\nMulti-armed bandit is another popular technique of online model evaluation. We start by randomly exposing all models to the users. Then we gradually reduce the exposure of the least- performing models until only one, the best performing model, gets served most of the time.\n\nIn addition to reporting training model performance metrics, one may also need to provide the statistical bounds known as the statistical interval.\n\nFor both classiﬁcation and regression models, a statistical interval for any metric can be computed using a popular technique called bootstrapping. It is a statistical procedure that consists of building B samples of a dataset, and then training a model and computing some statistic using each of those B samples.\n\nThe test examples used to evaluate the model must allow the discovery of defective behavior before the model reaches production. Such techniques as neuron coverage and mutation\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 2798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "testing can be used to evaluate the test set.\n\nWhen the model is used in a mission-critical system, or in regulated domains (such as credit, education, employment, housing, and public accommodation) accuracy, robustness, and fairness may have to be evaluated.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "8 Model Deployment\n\nOnce the model has been built and thoroughly tested, it can be deployed. Deploying a model means to make it available for accepting queries generated by the users of the production system. Once the production system accepts the query, the latter is transformed into a feature vector. The feature vector is then sent to the model as input for scoring. The result of the scoring then is returned to the user.\n\nModel deployment is the sixth stage in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nA trained model can be deployed in various ways. It can be deployed on a server, or on a user’s device. It can be deployed for all users at once, or to a small fraction of users. Below, we consider all the options.\n\nA model can be deployed following several patterns:\n\nstatically, as a part of an installable software package, • dynamically on the user’s device, • dynamically on a server, or • via model streaming.\n\n8.1 Static Deployment\n\nThe static deployment of a machine learning model is very similar to traditional software deployment: you prepare an installable binary of the entire software. The model is packaged\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "as a resource available at the runtime. Depending on the operating system and the runtime environment, the objects of both the model and the feature extractor can be packaged as a part of a dynamic-link library (DLL on Windows), Shared Objects (*.so ﬁles on Linux), or be serialized and saved in the standard resource location for virtual machine-based systems, such as Java and .Net.\n\nStatic deployment has many advantages:\n\nthe software has direct access to the model, so the execution time is fast for the user, • the user data doesn’t have to be uploaded to the server at the time of prediction; this saves time and preserves privacy,\n\nthe model can be called when the user is oﬄine, and • the software vendor doesn’t have to care about keeping the model operational; it becomes the user’s responsibility.\n\nHowever, a static deployment also has several drawbacks. First and foremost, the separation of concerns between the machine learning code and the application code isn’t always obvious. This makes it harder to upgrade the model without also having to upgrade the entire application. Second, if the model has certain computational requirements for scoring (such as access to an accelerator or a GPU), it may add complexity and confusion as to where the static deployment can or cannot be used.\n\n8.2 Dynamic Deployment on User’s Device\n\nA dynamic deployment on devices is similar to a static deployment, in the sense the user runs a part of the system as a software application on their device. The diﬀerence is that in dynamic deployment, the model is not part of the binary code of the application. Thus it achieves better separation of concerns. Pushing model updates is done without updating the whole application running on the user’s device. Moreover, a dynamic deployment may allow the same piece of code to select the right model, based on the available compute resources.\n\nDynamic deployment can be achieved in several ways:\n\nby deploying model parameters, • by deploying a serialized object, and • by deploying to the browser.\n\n8.2.1 Deployment of Model Parameters\n\nIn this deployment scenario, the model ﬁle only contains the learned parameters, while the user’s device has installed a runtime environment for the model. Some machine learning packages, like TensorFlow, have a lightweight version that can run on mobile devices.\n\nAlternatively, frameworks such as Apple’s Core ML allow running models created using popular packages, including scikit-learn, Keras, and XGBoost, on Apple devices.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "8.2.2 Deployment of a Serialized Object\n\nHere, the model ﬁle is a serialized object that the application would deserialize. The advantage of this approach is that you don’t need to have a runtime environment for your model on the user’s device. All needed dependencies will be deserialized with the object of the model.\n\nAn evident drawback is that an update might be quite “heavy,” which is a problem if your software system has millions of users.\n\n8.2.3 Deploying to Browser\n\nMost modern devices have access to a browser, either desktop or mobile. Some machine learning frameworks, such as TensorFlow.js, have versions that allow to train and run a model in a browser, by using JavaScript as a runtime.\n\nIt’s even possible to train a TensorFlow model in Python, and then deploy it to, and run it in the browser’s JavaScript runtime environment. Additionally, if a GPU (graphics processing unit) is available on the client’s device, Tensorﬂow.js can leverage it.\n\n8.2.4 Advantages and Drawbacks\n\nThe main advantage of dynamic deployment to users’ devices is that the calls to the model will be fast for the user. It also reduces the impact on the organization’s servers, as most computations are performed on the user’s device. Additionally, if the model is deployed to the browser, the organization’s infrastructure only needs to serve a web page that includes the model’s parameters. A downside of the browser-based deployment is that the bandwidth cost and application startup time might increase. The users must download the model’s parameters each time they start the web application, as opposed to doing it only once when they install an application.\n\nAnother drawback occurs during model updates. Recall, a serialized object can be quite voluminous. Some users may be oﬄine during the update, or even turn oﬀ all future updates. In that case, you may end up with diﬀerent users using very diﬀerent model versions. Now it becomes diﬃcult to upgrade the server-side part of the application.\n\nDeploying models on the user’s device means that the model easily becomes available for third-party analyses. They may try to reverse-engineer the model to reproduce its behavior. They may search for weaknesses by providing various inputs and observing the output. Or, they may adapt their data so the model predicts what they want.\n\nSuppose the mobile application allows the user to read news related to their interests. A content provider might try to reverse engineer the model, so that it now recommends more often the news from that content provider.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Figure 2: Deploying a machine learning model as a web service on a virtual machine.\n\nAs with static deployment, deploying to a user’s device makes it diﬃcult to monitor the model performance.\n\n8.3 Dynamic Deployment on a Server\n\nBecause of the above complications, and problems with performance monitoring, the most frequent deployment pattern is to place the model on a server (or servers), and make it available as a Representational State Transfer application programming interface (REST API) in the form of a web service, or Google’s Remote Procedure Call (gRPC) service.\n\n8.3.1 Deployment on a Virtual Machine\n\nIn a typical web service architecture deployed in a cloud environment, the predictions are served in response to canonically-formatted HTTP requests. A web service running on a virtual machine receives a user request containing the input data, calls the machine learning system on that input data, and then transforms the output of the machine learning system into the output JavaScript Object Notation (JSON) or Extensible Markup Language (XML) string. To cope with high load, several identical virtual machines are running in parallel.\n\nA load balancer dispatches the incoming requests to a speciﬁc virtual machine, depending\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "on its availability. The virtual machines can be added and closed manually, or be a part of an autoscaling group that launches or terminates virtual machines based on their usage. Figure 2 illustrates that deployment pattern. Each instance, denoted as an orange square, contains all the code needed to run the feature extractor and the model. The instance also contains a web service that has access to that code.\n\nIn Python, a REST API web service is usually implemented using a web application framework such as Flask or FastAPI. An R equivalent is Plumber.\n\nTensorFlow, a popular framework used to train deep models, comes with TensorFlow Serving, a built-in gRPC service.\n\nThe advantage of deploying on a virtual machine is that the architecture of the software system is conceptually simple: it’s a typical web or gRPC service.\n\nAmong the downsides, there is a need to maintain servers (physical or virtual). If virtualization is used, then there is an additional computational overhead due to virtualization and running multiple operating systems. Another is network latency, which can be a serious issue, depending on how fast you need to process scoring results. Finally, deploying on a virtual machine has a relatively higher cost, compared to deployment in a container, or a serverless deployment that we consider below.\n\n8.3.2 Deployment in a Container\n\nA more modern alternative to a virtual-machine-based deployment is a container-based deployment. Working with containers is typically considered more resource-eﬃcient and ﬂexible than with virtual machines. A container is similar to a virtual machine, in the sense that it is also an isolated runtime environment with its own ﬁlesystem, CPU, memory, and process space. The main diﬀerence, however, is that all containers are running on the same virtual or physical machine and share the operating system, while each virtual machine runs its own instance of the operating system.\n\nThe deployment process looks as follows. The machine learning system and the web service are installed inside a container. Usually, a container is a Docker container, but there are alternatives. Then a container-orchestration system is used to run the containers on a cluster of physical or virtual servers. A typical choice of a container-orchestration system for running on-premises or in a cloud platform, is Kubernetes. Some cloud platforms provide both their own container-orchestration engine, such as AWS Fargate and Google Kubernetes Engine, and support Kubernetes natively.\n\nFigure 3 illustrates that deployment pattern. Here, the virtual or physical machines are organized into a cluster, whose resources are managed by the container orchestrator. New virtual or physical machines can be manually added to the cluster, or closed. If your software is deployed in a cloud environment, a cluster autoscaler can launch (and add to the cluster) or terminate virtual machines, based on the usage of the cluster.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 3016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Figure 3: Deploying a model as a web service in a container running on a cluster.\n\nDeployment in a container has the advantage of being more resource-eﬃcient as compared to the deployment on a virtual machine. It allows the possibility to automatically scale with scoring requests. It also allows us to scale-to-zero. The idea of the scale-to-zero is that a container can be reduced down to zero replicas when idle and brought back up if there is a request to serve. As a result, the resource consumption is low compared to always running services. This leads to less power consumption and saves cost of cloud resources.\n\nOne drawback is that the containerized deployment is generally seen as more complicated, and requires expertise.\n\n8.3.3 Serverless Deployment\n\nSeveral cloud services providers, including Amazon, Google, and Microsoft, oﬀer so-called serverless computing. It’s known under the name of Lambda-functions on Amazon Web Services, and Functions on Microsoft Azure and Google Cloud Platform.\n\nThe serverless deployment consists of preparing a zip archive with all the code needed to run\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "the machine learning system (model, feature extractor, and scoring code). The zip archive must contain a ﬁle with a speciﬁc name that contains a speciﬁc function, or class-method deﬁnition with a speciﬁc signature (an entry point function). The zip archive is uploaded to the cloud platform and registered under a unique name.\n\nThe cloud platform provides an API to submit inputs to the serverless function. This speciﬁes its name, provides the payload, and yields the outputs. The cloud platform takes care of deploying the code and the model on an adequate computational resource, executing the code, and routing the output back to the client.\n\nUsually, the function’s execution time, zip ﬁle size, and amount of RAM available on the runtime are limited by the cloud service provider.\n\nThe zip ﬁle size limit can be a challenge. A typical machine learning model requires multiple heavyweight dependencies. Python’s libraries, to include Numpy, SciPy, and scikit-learn, are often needed for the model to be properly executed. Depending on the cloud platform, other supported programming languages can include Java, Go, PowerShell, Node.js, C#, and Ruby.\n\nThere are many advantages to relying on serverless deployment. The obvious advantage is that you don’t have to provision resources such as servers or virtual machines. You don’t have to install dependencies, maintain, or upgrade the system. Serverless systems are highly scalable and can easily and eﬀortlessly support thousands of requests per second. Serverless functions support both synchronous and asynchronous modes of operation.\n\nServerless deployment is also cost-eﬃcient: you only pay for compute-time. This may also be achieved with the previous two deployment patterns using autoscaling, but autoscaling has signiﬁcant latency. While the demand may drop, excessive virtual machines could still keep running before they are terminated.\n\nServerless deployment also simpliﬁes canary deployment, or canarying. In software engineering, canarying is a strategy when the updated code is pushed to just a small group of end-users, usually unaware. Because the new version is only distributed to a small number of users, its impact is relatively low, and changes can be reversed quickly, should the new code contain bugs. It is easy to set up two versions of serverless functions in production, and start sending low volume traﬃc to just one, and test it without aﬀecting many users. We will talk more about canarying in Section 8.4.\n\nRollbacks are also very simple in the serverless deployment because it is easy to switch back to the previous version of the function by replacing one zip archive.\n\nWe’ve discussed the zip archive size limit, and the RAM available on the runtime. These are important drawbacks to serverless deployment. Likewise, the unavailability of GPU access1 can be a signiﬁcant limitation for deploying deep models.\n\nOf course, complex software systems may combine deployment patterns. A deployment pattern appropriate for one model may be less optimal for another one. A combination of several deployment patterns is called a hybrid deployment pattern. Personal assistants\n\n1As of July 2020.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 3230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "like Google Home or Amazon Echo might have a model that recognizes the activation phrase (such as “OK, Google” or “Alexa”) deployed on the client’s device, and more complex models handle requests like “put song X on device Y” will instead run on the server. Alternatively, the deployment on the user’s mobile device might augment the video and add simple intelligent eﬀects in realtime. Server deployment would be used to apply more complex eﬀects, such as stabilization and super-resolution.\n\n8.3.4 Model Streaming\n\nModel streaming is a deployment pattern that can be seen as an inverse to the REST API. In REST API, the client sends a request to the server, and then waits for a response (a prediction).\n\nIn complex systems, there can be many models applied to the same input. Or, a model can input a prediction from another model. For example, the input may be a news article. One model can predict the topic of the article, another model can extract named entities, the third model can generate a summarization of the article, and so on.\n\nAccording to the REST API deployment pattern, we need one REST API per model. The client would call one API by sending a news article as a part of the request, and get the topic as response. Then the client calls another API by sending a news article, and gets the named entities as response; etc.\n\nStreaming works diﬀerently. Instead of having one REST API per model, all models, as well as the code needed to run them, are registered within a stream-processing engine (SPE). Examples are Apache Storm, Apache Spark, and Apache Flink. Or, they are packaged as an application based on a stream-processing library (SPL), such as Apache Samza, Apache Kafka Streams, and Akka Streams.\n\nThe descriptions of these SPEs and SPLs are beyond the scope of this book, but they all share the same property making them diﬀerent from the REST-API-based applications. In each stream-processing application, there is an implicit or explicit notion of data processing topology. The input data ﬂows in as an inﬁnite stream of data elements sent by the client. Following a predeﬁned topology, each data element in the stream undergoes a transformation in the nodes of the topology. Transformed, the ﬂow continues to other nodes.\n\nIn a stream-processing application, nodes transform their input in some way, and then either,\n\nsend the output to other nodes, or • send the output to the client, or • persist the output to the database or a ﬁlesystem.\n\nOne node could take a news article and predict its topic; another node could take both the news article and the predicted topic and generate a summary; and so on.\n\nThe diﬀerence between a REST-API-based application and a streaming-based one is shown in Figure 4. Figure 4a shows a client using a REST API, processing one data element, such as a news article, by sending a series of requests. One by one, various REST APIs receive\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 2961,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Figure 4: The diﬀerence between a REST API and streaming: (a) to process one data element, a client using a REST API sends a series of requests, one by one, and receives responses synchronously; (b) to process one data element, a client using streaming opens a connection, sends a request, and receives update events as they happen.\n\nrequests and produce responses synchronously. On the other hand, a client using streaming (Figure 4b) opens a connection to the streaming application, sends a request, and receives update events as they happen.\n\nOn the right-hand side of the streaming-based application in Figure 4b, there’s a topology that deﬁnes the data ﬂow in the application. Each input element sent by the client passes through all the nodes of the topology graph. Nodes can send updated events to the client, and/or persist data to the database or a ﬁlesystem.\n\nAn SPE-based streaming application runs on its own cluster of virtual or physical machines, and takes care of distributing the data processing load among the available resources. An SPL- based streaming application doesn’t need a dedicated cluster for data processing. It can be integrated with available resources, such as virtual or physical machines, or a container orchestrator (such as Kubernetes).\n\nREST APIs are usually employed to let clients send ad-hoc requests that don’t follow a\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "certain frequently-repeated pattern. It’s the best choice when the client wants the liberty of deciding what to do with the API response. On the other hand, if each request of the client is:\n\ntypical, • undergoes a certain pattern of transformations, especially multiple intermediate trans- formations, and\n\nalways results in the same actions, such as persistence of speciﬁc data elements to the ﬁlesystem or database, then streaming-based applications provide better resource- eﬃciency, lower latency, security, and fault-tolerance.\n\n8.4 Deployment Strategies\n\nTypical deployment strategies are:\n\nsingle deployment, • silent deployment, • canary deployment, and • multi-armed bandit.\n\nLet’s consider each of them.\n\n8.4.1 Single Deployment\n\nSingle deployment is the simplest one. Conceptually, once you have a new model, you serialize it into a ﬁle, and then replace the old ﬁle with the new one. You also replace the feature extractor, if needed.\n\nTo deploy on a server in a cloud environment, you prepare a new virtual machine, or a container running the new version of the model. Then you replace the virtual machine image or that of the container. Finally, you gradually close the old machines or containers, and let the autoscaler start the new ones.\n\nTo deploy on a physical server, you will upload a new model ﬁle (and the feature extraction object, if needed) on the server. Then you replace old ﬁles and old code with the new versions, and restart the web service.\n\nTo deploy on the user’s device, you push the new model ﬁle to the user’s device, along with any needed feature extraction object, and restart the software.\n\nIf you use interpretable code, the feature extractor object can be deployed by replacing one source code ﬁle with another one. To avoid redeploying the entire software appli- cation, on either the server or the user’s device, the feature extractor’s object can be serialized into a ﬁle. Then, on each startup, the software running the model would deserialize the feature extractor object.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 2077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Single deployment has the advantage of being simple; however, it’s also the riskiest strategy. If the new model or the feature extractor contains a bug, all users will be aﬀected.\n\n8.4.2 Silent Deployment\n\nA counterpart of the single deployment is silent deployment. It deploys the new model version and new feature extractor, and keeps the old ones. Both versions run in parallel. However, the user will not be exposed to the new version until the switch is done. The predictions made by the new version are only logged. After some time, they are analyzed to detect possible bugs.\n\nSilent deployment has the beneﬁt of providing enough time to ensure the new model works as expected, without adversely aﬀecting any users. The drawback is the need to run twice as many models, which consumes more resources. Furthermore, for many applications, it’s impossible to evaluate the new model without exposing its predictions to the user.\n\n8.4.3 Canary Deployment\n\nRecall, canary deployment, or canarying, pushes the new model version and code to a small fraction of users, while keeping the old version running for most users. Contrary to the silent deployment, canary deployment allows validating the new model’s performance, and its predictions’ eﬀects. Contrary to the single deployment, canary deployment doesn’t aﬀect lots of users in case of possible bugs.\n\nBy opting for the canary deployment, you accept the additional complexity of having and maintaining several versions of the model deployed simultaneously.\n\nAn obvious drawback of the canary deployment is that it’s impossible for engineers to spot rare errors. If you deploy the new version to 5% of users, and a bug aﬀects 2% of users, then you have only 0.1% chance that the bug will be discovered.\n\n8.4.4 Multi-Armed Bandits\n\nAs seen in Section ?? of Chapter 7, multi-armed bandits (MAB) are a way to compare one or more versions of the model in the production environment, and select the best performing one. MABs have an interesting property: after an initial exploration period, during which the MAB algorithm gathers enough evidence to evaluate the performance of each model (arm), the best arm is eventually played all the time. It means that after the convergence of the MAB algorithm, most of the time, all users are routed to the software version running the best model.\n\nThe MAB algorithm, thus, solves two problems — online model evaluation and model deployment — simultaneously.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "8.5 Automated Deployment, Versioning, and Metadata\n\nThe model is an important asset, but it’s never delivered alone. There are additional assets for production model testing that ensure the model is not broken.\n\n8.5.1 Model Accompanying Assets\n\nOnly deploy a model in production when it’s accompanied with the following assets:\n\nan end-to-end set that deﬁnes model inputs and outputs that must always work, • a conﬁdence test set that correctly deﬁnes model inputs and outputs, and is used to compute the value of the metric,\n\na performance metric whose value will be calculated on the conﬁdence test set by applying the model to it, and\n\nthe range of acceptable values of the performance metric.\n\nOnce the system using the model is initially evoked on an instance of a server or client’s device, an external process must call the model on the end-to-end test data and validate that all predictions are correct. Furthermore, the same external process must validate that the value of the performance metric computed by applying the model to the conﬁdence test set is within the range of acceptable values. If either of two evaluations fails, the model should not be served to the client.\n\n8.5.2 Version Sync\n\nThe versions of the following three elements must always be in sync:\n\n1) training data, 2) feature extractor, and 3) model.\n\nEach update to the data must produce a new version in the data repository. The model trained using a speciﬁc version of the data must be put into the model repository with the same version number as that of the data used to train the model.\n\nIf the feature extractor was not changed, its version still must be updated to be in sync with the data and the model. If the feature extractor was updated, then a new model must be built using an updated feature extractor, and the versions are incremented for the feature extractor, the model, and the training data (even if the latter wasn’t changed).\n\nThe deployment of a new model version must be automated by a script in a transactional way. Given a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment. The model must be applied to the end-to-end and conﬁdence test data by simulating a regular call from the outside. If there’s a prediction error for the end-to-end\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 2437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "test data, or the value of the metric is not within the range of acceptable values, the entire deployment has to be rolled back.\n\n8.5.3 Model Version Metadata\n\nEach model version must be accompanied with the following code and metadata:\n\nthe name and the version of the library or package used to train the model, • if Python was used to build the model, then requirements.txt of the virtual environment used to build the model (or, alternatively, a Docker image name pointing to a speciﬁc path on Docker Hub or in your Docker registry),\n\nthe name of the learning algorithm, and names and values of the hyperparameters, • the list of features required by the model, • the list of outputs, their types, and how the outputs should be consumed, • the version and location of the data used to train the model, • the version and location of the validation data used to tune model’s hyperparameters, • the model scoring code that runs the model on new data and outputs the prediction.\n\nThe metadata and the scoring code may be saved to a database or to a JSON/XML text ﬁle.\n\nFor audit purposes, the following information must also accompany each deployment:\n\nwho built the model and when, • who and when made the decision of deploying that model, and based on what grounds, • who reviewed the model for privacy and security compliance purposes.\n\n8.6 Model Deployment Best Practices\n\nIn this section, we discuss practical aspects of deploying machine learning systems in produc- tion. We also outline several useful and practical tips for model deployment.\n\n8.6.1 Algorithmic Eﬃciency\n\nMost data analysts work in Python or R. While there are web frameworks that allow building web services in those two languages, they are not considered the most eﬃcient languages.\n\nIndeed, when you use scientiﬁc packages in Python, much of their code was written in eﬃcient C or C++, and then compiled for your speciﬁc operating system. However, your own data preprocessing, feature extraction, and scoring code may not be as eﬃcient.\n\nFurthermore, not all algorithms are practical. While some algorithms can quickly solve the problem, others are too slow. For some problems, no fast algorithms can exist.\n\nThe subﬁeld of computer science called analysis of algorithms is concerned with determining and comparing the complexity of algorithms. The big O notation is used to classify\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n1\n\n2\n\n3\n\nalgorithms, according to how their running time or space requirements grow, as the input size grows.\n\nFor example, let’s say we have the problem of ﬁnding the two most distant one-dimensional examples in the set of examples S of size N. One Python algorithm we could craft would look like this: def find_max_distance(S):\n\nresult = None max_distance = 0 for x1 in S:\n\nfor x2 in S:\n\nif abs(x1 - x2) >= max_distance: max_distance = abs(x1 - x2) result = (x1, x2)\n\nreturn result\n\nor, like this in R: find_max_distance <- function(S) {\n\nresult <- NULL max_distance <- 0 for (x1 in S) {\n\nfor (x2 in S) {\n\nif (abs(x1 - x2) >= max_distance) { max_distance <- abs(x1 - x2) result <- c(x1, x2)\n\n}\n\n}\n\n} result\n\n}\n\nIn the above algorithms, we loop over all values in S, and, at every iteration of the ﬁrst loop, we loop over all values in S once again. Therefore, the above algorithm makes N2 comparisons of numbers. If we take the time the comparison, abs, and assignment operations take as a unit time, then the time complexity (or, simply, complexity) of this algorithm is at most 5N2. At each iteration, we have one comparison, two abs, and two assignment operations (1 + 2 + 2 = 5). When the complexity of an algorithm is measured in the worst case, the big O notation is used. For the above algorithm, using big O notation, we say that the algorithm’s complexity is O(N2); the constants, like 5, are ignored.\n\nFor the same problem, we can craft another Python algorithm like this: def find_max_distance(S):\n\nresult = None min_x = float(\"inf\")\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n1\n\n2\n\n1\n\n2\n\n3\n\nmax_x = float(\"-inf\") for x in S:\n\nif x < min_x: min_x = x if x > max_x: max_x = x\n\nresult = (max_x, min_x) return result\n\nor in R, like this: find_max_distance <- function(S):\n\nresult <- NULL min_x <- Inf max_x <- -Inf for (x in S) {\n\nif (x < min_x) { min_x <- x\n\n} if (x > max_x) { max_x = x\n\n}\n\nresult <- c(max_x, min_x) result\n\nIn the above algorithms, we loop over all values in S only once, so the algorithm’s complexity is O(N). In this case, we say that the latter algorithm is more eﬃcient than the former.\n\nAn algorithm is called eﬃcient when its complexity is polynomial in the input size. Therefore both O(N) and O(N2) are eﬃcient because N is a polynomial of degree 1, while N2 is a polynomial of degree 2. However, for very large inputs, an O(N2) algorithm can still be slow. In the big data era, scientists and engineers often look for O(logN) algorithms.\n\nFrom a practical standpoint, when implementing an algorithm, you should avoid using loops whenever possible, and implement vectorization using NumPy or similar tools. For example, you should use operations on matrices and vectors, instead of loops. In Python, to compute w · x (a dot product of two vectors), you should type,\n\nimport numpy wx = numpy.dot(w,x)\n\nand not,\n\nwx = 0 for i in range(N): wx += w[i]*x[i]\n\nSimilarly, in R, you should type,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "1\n\n23\n\n24\n\n25\n\nwx = w %*% x\n\nand not,\n\nwx <- 0 for (i in seq(N)):\n\nwx <- wx + w[i]*x[i]\n\nUse appropriate data structures. If the order of elements in a collection doesn’t matter, use set instead of list. In Python, the operation of verifying whether a speciﬁc example belongs to S is fast when S is a set, and is slow when S is a list.\n\nAnother important data structure to make your Python code more eﬃcient is dict. It is called a dictionary or a hash table in other languages. It allows you to deﬁne a collection of key-value pairs with very fast lookups for keys.\n\nUsing libraries is generally more reliable — you should only write your own code when you are a researcher, or when it’s truly needed. Scientiﬁc Python packages like NumPy, SciPy, and scikit-learn were built by experienced scientists and engineers with eﬃciency in mind. They have many methods implemented compiled C and C++ for maximum performance.\n\nIf you need to iterate over a vast collection of elements, use Python generators (or their R alternative in the iterators package) that create a function returning one element at a time, rather than all elements at once.\n\nUse the cProﬁle package in Python (or its R counterpart, lineprof) to ﬁnd code ineﬃciencies.\n\nFinally, when nothing can be improved in your code from the algorithmic perspective, you can further boost the speed by using:\n\nthe multiprocessing package in Python, or its R counterpart parallel, to run com- putations in parallel; or use a distributed processing framework such as Apache Spark, and\n\nPyPy, Numba or similar tools to compile your Python code (or the compiler package for R) into fast, optimized machine code.\n\n8.6.2 Deployment of Deep Models\n\nSometimes, to achieve the required speed, it might be necessary to do the scoring on a graphics processing unit (GPU). The cost of a GPU instance in a cloud environment is typically much higher than the cost of a “normal” instance. So only the model could be deployed in an environment with one or several GPUs optimized for fast scoring. The remainder of the application could be deployed separately in a CPU environment. This approach allows reducing the cost, but, at the same time, it might add a communication overhead between two parts of the application.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 2313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n8.6.3 Caching\n\nCaching is a standard practice in software engineering. Memory cache is used to store the result of a function call, so the next time that function is called with the same values of parameters, the result is read from the cache.\n\nCaching helps speed up the application when it contains resource-consuming functions that take time to process, or are frequently called with the same parameter values. In machine learning, such resource-consuming functions are models, especially when they run on GPUs.\n\nThe simplest cache may be implemented in the application itself. For example, in Python, the lru_cache decorator can wrap a function with a memoizing callable that saves up to the maxsize most recent calls:\n\nfrom functools import lru_cache\n\n# Read the model from file model = pickle.load(open(\"model_file.pkl\", \"rb\"))\n\n@lru_cache(maxsize=500) def run_model(input_example):\n\nreturn model.predict(input_example)\n\n# Now you can call run_model # on new data The ﬁrst time the function run_model is called for some input, model.predict will be called. For the subsequent calls of run_model with the same value of the input, the output will be read from cache that memorizes the result of maxsize most recent calls of model.predict.\n\nIn R, a similar result can be obtained using the memo function: library(memo)\n\nmodel <- readRDS(\"./model_file.rds\")\n\nrun_model <- function(input_example) {\n\nresult <- predict(model, input_example) result\n\n}\n\n# Create a memoized version of run_model run_model_memo <- memo(run_model, cache = lru_cache(500))\n\n# Now you can use run_model_memo # instead of run_model on new data\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nAlthough using lru_cache and similar approaches is very convenient for an analyst, typically, in large scale production systems, engineers employ general purpose scalable and conﬁgurable cache solutions such as Redis or Memcached.\n\n8.6.4 Delivery Format for Model and Code\n\nRecall, serialization is the most straightforward way to deliver the model and the feature extractor code to the production environment.\n\nEvery modern programming language has serialization tools. In Python, it’s pickle:\n\nimport pickle from sklearn import svm, datasets\n\nclassifier = svm.SVC() X, y = datasets.load_iris(return_X_y=True) classifier.fit(X, y)\n\n# Save model to file with open(\"model.pickle\",\"wb\") as outfile: pickle.dump(classifier, outfile)\n\n# Read model from file classifier2 = None with open(\"model.pickle\",\"rb\") as infile: classifier2 = pickle.load(infile)\n\nif classifier2:\n\nprediction = classifier2.predict(X[0:1])\n\nwhile in R, it’s RDS: library(\"e1071\")\n\nclassifier <- svm(Species ~ ., data = iris, kernel = 'linear')\n\n# Save model to file saveRDS(classifier, \"./model.rds\")\n\n# Read model from file classifier2 <- readRDS(\"./model.rds\")\n\nprediction <- predict(classifier2, iris[1,])\n\nIn scikit-learn, it may be better to use joblib’s replacement of pickle, which is more eﬃcient on objects that carry large NumPy arrays:\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 1465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nfrom joblib import dump, load\n\n# Save model to file dump(classifier, \"model.joblib\")\n\n# Read model from file classifier2 = load(\"model.joblib\")\n\nThe same approach can be applied to save the serialized object of the feature extractor to a ﬁle, copy it to the production environment, and then read it from the ﬁle.\n\nFor some applications, the prediction speed is critical. In such cases, the production code is written in a compiled language, such as Java or C/C++. If a data analyst has built a model using Python or R, there are three options to deploy for production:\n\nrewrite the code in a compiled, production-environment programming language, • use a model representation standard such as PMML or PFA, or • use a specialized execution engine such as MLeap.\n\nThe Predictive Model Markup Language (PMML) is an XML-based predictive model interchange format that provides a way for data analysts to save and share models between PMML-compliant applications. PMML allows analysts to develop models within one vendor’s application, and then use them within other vendors’ applications, so that proprietary issues and incompatibilities are no longer a barrier to the model exchanges between applications.\n\nFor example, imagine you use Python to build an SVM model, and then save the model as a PMML ﬁle. Let the production runtime environment be a Java Virtual Machine (JVM). As long as PMML is supported by a machine learning library for JVM, and that library has an implementation of SVM, your model can be used in production directly. You don’t need to rewrite your code or retrain the model in a JVM language.\n\nThe Portable Format for Analytics (PFA) is a more recent standard for representing both statistical models and data transformation engines. PFA allows us to easily share models and machine learning pipelines across heterogeneous systems and provides algorithmic ﬂexibility. Models, pre- and post-processing transformations are all functions that can be arbitrarily composed, chained, or built into complex workﬂows. PFA has a form of a JavaScript Object Notation (JSON) or a YAML Ain’t Markup Language (YAML) conﬁguration ﬁle.\n\nThere are open source generic “evaluators” for models or pipelines saved as PMML or PFA formatted ﬁles. JPMML (for Java PMML) and Hadrian are two of the most widely adopted. Evaluators read the model or the pipeline from a ﬁle, execute it by applying it to the input data, and output the prediction.\n\nUnfortunately, PMML and PFA are not widely supported by the popular machine learning libraries and frameworks.2 For example, scikit-learn doesn’t support those standards, though side-projects such as SkLearn2PMML can convert scikit-learn objects to PMML.\n\n2As of July 2020.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 2792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Alternatively, such execution engines as MLeap can execute machine learning models and pipelines fast in a JVM environment. At the time of the writing of this book, MLeap could execute models and pipelines created in Apache Spark and scikit-learn.\n\nNow, let us brieﬂy outline several useful and practical tips for model deployment.\n\n8.6.5 Start With a Simple Model\n\nDeploying and applying the model in production can be more complex than it might seem. Once the infrastructure to serve a simple model is solid, a more complex model can then be trained and deployed.\n\nA simple interpretable model is easier to debug, especially for feature extractors and entire machine learning pipelines. Complex models and pipelines have many dependencies and large numbers of hyperparameters to tune, and are more prone to implementation and deployment errors.\n\n8.6.6 Test on Outsiders\n\nBefore putting your model in production, test your model on outsiders, and not just on the test data. Outsiders could be other team members or company employees. Alternatively, you can use crowdsourcing or a subset of your real customers who agreed to participate in experiments with new product features.\n\nTesting on outsiders will help you avoid personal bias, because you, as the creator of the model, are emotionally involved. It will also give your model an exposure to diﬀerent users (in cases when, for example, your whole team is male or Caucasian).\n\n8.7 Summary\n\nA model can be deployed following several patterns: statically, as a part of installable software, dynamically on the user’s device, dynamically on a server, or via model streaming.\n\nThe static deployment has many advantages, such as fast execution time, preserved user privacy, and the ability to call the model when the user is oﬄine. There are also a drawback: it’s harder to upgrade the model without also having to upgrade the entire application.\n\nThe principal advantage of the dynamic deployment on the users’ devices is that the calls to the model will be fast for the user. It also reduces the charge on the organization’s servers. Downsides include the diﬃculty to deliver updates to all users and the availability of the model for third-party analyses.\n\nAs with the static deployment, deploying the model on a user’s device makes it diﬃcult to monitor the performance of the model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 2394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Dynamic deployment on a server can have one of the following forms: deployment on a virtual machine, deployment in a container, and serverless deployment.\n\nThe most popular deployment pattern is to deploy the model on a server and make it available as a REST API in the form of a web or a gRPC service. Here, the client sends a request to the server, and then waits for a response before sending another request.\n\nModel streaming is diﬀerent. All models are registered within a stream-processing engine or are packaged as an application based on a stream-processing library. Here, the client sends one request and receives updates as they happen.\n\nTypical deployment strategies are single deployment, silent deployment, canary deployment, and multi-armed bandit.\n\nIn single deployment, you serialize the new model into a ﬁle, and then replace the old one.\n\nSilent deployment consists of deploying the old and new versions, and running them in parallel. The user will not be exposed to the new version until the switch is done. The predictions made by the new version are only logged and analyzed. Thus, there is enough time to make sure that the new model works as expected without aﬀecting any user. A drawback is the need to run more models, which consumes more resources.\n\nCanary deployment consists of pushing the new version to a small fraction of the users, while keeping the old version running for most users. Canary deployment allows model performance validation and evaluating the users’ experience. It won’t aﬀect lots of users in case of possible bugs.\n\nMulti-armed bandits allow us to deploy the new model while keeping the old one. The algo- rithm replaces the old model with the new one only when it is certain that it performs better.\n\nThe deployment of a new model version must be automated by a script in a transactional way. Given a version of the model to deploy, the deployment script will fetch the model and the feature extraction object from the respective repositories and copy them to the production environment. The model must be applied to the end-to-end and conﬁdence test data by simulating a regular call from the outside. If there’s a prediction error for the end-to-end test data, or the value of the metric is not within the range of acceptable values, the entire deployment has to be rolled back.\n\nThe versions of training data, feature extractor, and model must always be in sync.\n\nAlgorithmic eﬃciency is an important consideration in model deployment. Experienced scientists and engineers built Python packages like NumPy, SciPy, and scikit-learn with eﬃciency in mind. Your own code may not be as reliable or eﬃcient. You should only write your own code when it’s absolutely necessary.\n\nIf you implement your own algorithmic code, avoid loops. Implement vectorization with NumPy or similar tools. Use appropriate data structures. If the order of elements in a collection doesn’t matter, use a set instead of a list. Using dictionaries (or hash tables) allows you to deﬁne a collection of key-value pairs with very fast lookups for keys.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 3132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Caching speeds up the application when it contains resource-consuming functions frequently called with the same parameter values. In machine learning, such resource-consuming functions are models, especially when they run on GPUs.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "9 Model Serving, Monitoring, and Maintenance\n\nIn this chapter, we consider the best practices of serving, monitoring, and maintaining models in production. These are the last three stages in the machine learning project life cycle:\n\nFigure 1: Machine learning project life cycle.\n\nIn particular, we characterize the properties of a machine learning runtime, the environment in which the input data is applied to the model, and the modes of model serving, such as batch and on demand. Furthermore, we consider three major challenges of serving a model in real world: errors, change, and human nature. We describe what should be monitored in the production environment, and when and how update the model.\n\n9.1 Properties of the Model Serving Runtime\n\nThe model serving runtime is the environment in which the model is applied to the input data. The runtime properties are dictated by the model deployment pattern. However, an eﬀective runtime will have several additional properties that we discuss here.\n\n9.1.1 Security and Correctness\n\nThe runtime is responsible for authenticating the user’s identity, and authorizing their requests.\n\nThings to check are:\n\nwhether a speciﬁc user has authorized access to the models they want to run,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "whether the names and the values of parameters passed correspond to the model’s speciﬁcation, and\n\nwhether those parameters and their values are currently available to the user.\n\n9.1.2 Ease of Deployment\n\nThe runtime must allow the model to be updated with minimal eﬀort and, ideally, without aﬀecting the entire application. If the model was deployed as a web service on a physical server, then a model update must be as simple as replacing one model ﬁle with another, and restarting the web service.\n\nIf the model was deployed as a virtual machine instance or container, then the instances or containers running the old version of the model should be replaceable by gradually stopping the running instances and starting new instances from a new image. The same principle applies to the orchestrated containers.\n\nTypically, a model streaming-based application is updated by streaming the new version of the model. To enable this, the streaming application must be stateful. Once a new version and the related components (such as feature extractor and scoring code) are streamed into the application, the state of the application changes, and now contains the new version of these assets. Modern stream-processing engines support stateful applications. The described architecture is schematically shown in Figure 2.\n\nFigure 2: Model streaming high-level architecture.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "9.1.3 Guarantees of Model Validity\n\nAn eﬀective runtime will automatically make sure that the model it executes is valid. Furthermore, it makes sure the model, the feature extractor, and other components are in sync. It must be validated on each startup of the web service or the streaming application, and periodically during the runtime. As discussed in Section ?? of Chapter 8, each model should be deployed accompanied by the following four assets: an end-to-end set, a conﬁdence test set, a performance metric, and its range of acceptable values.\n\nThe model should not be served in production (and must be immediately stopped if it is running) in either of the two conditions:\n\nat least one of the end-to-end test examples was not scored correctly, or • the value of the metric, calculated on the conﬁdence test set examples, is not within the acceptable range.\n\n9.1.4 Ease of Recovery\n\nAn eﬀective runtime allows easy recovery from errors by rolling back to previous versions.\n\nThe recovery from an unsuccessful deployment should be produced in the same way, and with the same ease, as the deployment of an updated model. The only diﬀerence is that, instead of the new model, the previous working version will be deployed.\n\n9.1.5 Avoidance of Training/Serving Skew\n\nIt is strongly recommended to avoid using two diﬀerent codebases, one for training the model, and one for scoring in production. When it concerns feature extraction, even a tiny diﬀerence between two versions of feature extractor code may lead to suboptimal or incorrect model performance.\n\nThe engineering team may reimplement the feature extractor code for production for many reasons. The most common is that the data analyst’s code is ineﬃcient or incompatible with the production ecosystem.\n\nThus, the runtime should allow easy access to the feature extraction code for various needs, including model retraining, ad-hoc model calls, and production. One way to implement it is by wrapping the feature extraction object into a separate web service.\n\nIf you cannot avoid using two diﬀerent codebases to generate features for training and production, then the runtime should allow for the logging of feature values generated in the production environment. Those values should then be used as training values.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 2337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "9.1.6 Avoidance of Hidden Feedback Loops\n\nIn Section ?? of Chapter 4, we saw one example of a hidden feedback loop. Model mB used the output of model mA as a feature, without knowing that model mA also used the output of model mB as its feature.\n\nAnother kind of hidden feedback loop only involves one model. Let’s say we have a model that classiﬁes incoming email messages as spam or not spam. Let the user interface allow the user to mark messages as spam or not spam. Obviously, we want to use those marked messages to improve our model. However, by so doing, we risk creating a hidden feedback loop, and here is why.\n\nIn our application, the user will only mark a message as spam when they see it. However, users only see the messages that our model classiﬁed as not spam. Also, it is unlikely that the user will regularly go to the spam folder and mark some messages as not spam. So, the action of the user is signiﬁcantly aﬀected by our model, which makes the data we get from the user skewed: we inﬂuence the phenomenon from which we learn.\n\nTo avoid the skew, mark a small percentage of examples as “held-out,” and show all of them to the user without pre-applying the model. Then use only these held-out examples as additional training examples, including those to which the user didn’t react.\n\nIn a more general scenario, one model can indirectly aﬀect the data used to train another model. Let one model decide the order of books to display, while the other decides which reviews to display near each book. If the ﬁrst model puts a review of a certain book at the bottom of the list, the absence of a user’s response to the second model’s review may be caused by its low position on the page, and not by the quality of the review.\n\n9.2 Modes of Model Serving\n\nMachine learning models are served in either batch or on-demand mode. On-demand, a model can be served to either a human client or a machine.\n\n9.2.1 Serving in Batch Mode\n\nA model is usually served in batch mode when it is applied to large quantities of input data. One example could be when the model is used to exhaustively process the data of all users of a product or service. Or, when it systematically applies to all incoming events, such as tweets, or comments to online publications. Batch mode is more resource-eﬃcient compared to an on-demand mode, and is employed when some latency can be tolerated.\n\nWhen served in batch mode, the model usually accepts between a hundred and a thousand feature vectors at once. Experiment to ﬁnd the optimal batch size for speed. Typical sizes are powers of two: 32, 64, 128, etc.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "The outputs for the batch are usually saved to the database, as opposed to sending them to speciﬁc consumers. You would use the batch mode to:\n\ngenerate the list of weekly recommendations of new songs to all users of a music streaming service,\n\nclassify the ﬂow of incoming comments to online news articles and blog posts as spam or not spam,\n\nextract named entities from documents indexed by a search engine, and so on.\n\n9.2.2 Serving on Demand to a Human\n\nThe six steps of serving the model on demand to a human are as follows:\n\n1) validate the request, 2) gather the context, 3) transform the context into model input, 4) apply the model to the input, and get the output, 5) make sure that the output makes sense, 6) present the output to the user.\n\nBefore running a model in production for a request coming from a user, it might be necessary to verify whether that user has the correct permissions for this model.\n\nThe context represents the user’s situation when they send a request to the machine learning system, and in which the user will receive the system’s response.\n\nThe user can send the request to the machine learning system explicitly or implicitly. An exam- ple of an explicit request is when a music-streaming service’s user requests recommendations for similar songs to a given song. On the other hand, an implicit request is sent by a direct messenger application for suggested replies to the most recent message received by the user.\n\nA good context may be collected in real or near-real time. It will contain the information needed by the feature extractor to generate all the feature values the model expects. It also contains enough information for debugging, is compact enough to be saved in a log, and contains information that will be used to improve the model over time.\n\nLet’s see examples of a good context for several problems.\n\nDevice malfunctioning\n\nWhen detecting device malfunction, a good context contains vibration and noise levels, the task executed by the device, the user ID, the ﬁrmware version, the time passed since manufacturing and the last maintenance, and the number of uses since manufacturing and the last maintenance.\n\nEmergency room hospitalization\n\nTo decide whether the new patient should be admitted to an intensive care unit, a good context would include age, blood pressure, temperature, heart rate, pulse oximetry\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 2426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "level, complete blood count, chemistry proﬁle, arterial blood gas test, blood alcohol level, medical history, and pregnancy.\n\nCredit risk assessment\n\nTo make an approval/rejection decision for a credit card application, a good context would include age, education, employment status, country residency status, annual salary, family status, outstanding debts, availability of other credit cards, whether the person is a homeowner or tenant, whether the person has declared bankruptcy, and whether, and how many times, the person missed past credit payments. Even if certain information is not needed for feature extraction, it is still pertinent for logging and debugging: client’s ID, date, and time of the day.\n\nAdvertisement display\n\nTo decide whether a speciﬁc advertisement should be displayed to a website user, a good context would include the webpage title, the user’s position on the web page, the screen resolution, the text on the webpage and the text visible to the user, how the user reached the webpage, and the time spent on it. For logging and debugging purposes, the context might include the browser version, operating system version, connection information, and date and time.\n\nA feature extractor transforms the context into the model input. Sometimes, the feature extractor is a part of the machine learning pipeline, as we discussed in Section ?? of Chapter 5. However, it’s common to build the feature extractor as a separate object.\n\nWhen the result of the scoring is to be served to a human client, it’s rarely presented directly. Usually, the scoring code transforms the model’s prediction into a form more easily interpreted, and that adds value to the client.\n\nBefore serving the model to a human, it’s common to measure the prediction conﬁdence score. If the conﬁdence is low, you can decide to not present anything: users tend to complain less about the errors they don’t see. Or, if the user expects an output, inform them about the low conﬁdence. Then prompt, “Are you sure?”\n\nPrompting is especially important when the system might initiate an action based on the prediction. If you are able to estimate the error’s possible cost and if the prediction conﬁdence is bounded by (0,1), then multiply (1 — conﬁdence) by the cost to see the possible impact of making a wrong action. For example, let the cost of making an error is estimated as 1000 dollars and the model outputs the conﬁdence score equal to 0.95, then the expected error cost value is (1 − 0.95) × 1000 = 50 dollars. You might put a threshold on the expected cost value for diﬀerent actions recommended by the model, and prompt the user if the expected cost is above the threshold.\n\nIn addition to measuring the model’s conﬁdence, calculate whether the value of the prediction makes sense. In Section 9.3, we will further detail what to check, and what the system’s reaction should be, if the output doesn’t make sense.\n\nIt is convenient to log the context in which the model was served, as well as the reaction of the user. This can help both to debug eventual problems, and improve the model by creating\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 3155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Figure 3: On-demand model serving with a message broker.\n\nnew training examples.\n\n9.2.3 Serving on Demand to a Machine\n\nWhile building a REST API is appropriate for many cases, we often serve a machine by streaming. Indeed, a machine’s data requirements are usually standard and pre- determined. A well-designed, ﬁxed topology of a streaming application allows an eﬃcient use of available resources.\n\nServing on demand, to either machine or human, can be tricky. The demand may vary, from very high during the day, to very low during the night. If you use virtual resources in the cloud, autoscaling can help with adding more resources when needed, and then freeing them when demand decreases. However, autoscaling is not nimble enough to cope with accidental spikes.\n\nTo deal with such situations, on-demand architectures include a message broker, such as RabbitMQ or Apache Kafka. A message broker allows one process to write messages in a queue, and another to read from that queue. On-demand requests are placed in the input queue. The model runtime process periodically connects to the broker. It reads a batch of input data elements from the input queue and generates predictions for each element in batch mode. It then writes the predictions to the output queue. Another process periodically connects to the broker, reads the predictions from the output queue, and pushes them to users who sent the requests (Figure 3). In addition to allowing us to cope with demand spikes, such an approach is more resource-eﬃcient.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n9",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "9.3 Model Serving in Real World\n\nWhen real people interact with a software system in the real world, serving the model gets complicated. It’s usually impossible to predict all user actions and reactions. The architecture of a software system intended for the real world must be ready for three phenomena: errors, change, and human nature.\n\n9.3.1 Being Ready for Errors\n\nErrors are inevitable in any software. In machine-learning-based software, errors are an integral part of the solution: no model is perfect. Because we cannot ﬁx all errors, the only option is to embrace them.\n\nEmbracing errors means designing the software system in such a way that when an error happens, the system continues operating normally.\n\nThere are three “cannots” we must accept and embrace:\n\n1. We cannot always explain why an error happened. 2. We cannot reliably predict when it will happen, and even a high conﬁdence prediction can be false.\n\n3. We cannot always know how to ﬁx a speciﬁc error. If it’s ﬁxable, what kind and how much training data is needed?\n\nFurthermore, when an error happens, we cannot always expect that the incorrect prediction will at least be close or similar to the correct prediction. An error can be arbitrarily “crazy.” For example, a model for a self-driving car, at the speed of 120 km/h (~74 mph) with no obstacles, may predict that the best action is to stop and drive backward.\n\nTiny changes in the context may result in unexpected error patterns. For example, the model that recognizes dangerous situations on the factory ﬂoor may start making errors after the lightbulb near the camera is replaced. The previous lightbulb was incandescent, and the new one is ﬂuorescent.\n\nEven rare errors may impact users, if the number of users is large. Let the model have a 99% accuracy. If you have a million users, one percent of prediction errors will aﬀect thousands.\n\nIt’s rare that ﬁxing one error in a model results in new errors. However, there’s no guarantee.\n\nHow to design a system in the presence of inevitable errors?\n\n9.3.2 Dealing With Errors\n\nFirst of all, have a strategy that mitigates, at least, partially, a situation in which your system looks or acts “stupid.” For example, if your system talks to the user, like a personal assistant or a chatbot, it’s better to say, “I don’t know,” than to say something random. If the error\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n10",
      "content_length": 2411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "will be directly visible to the user, calculate the expected cost of the error, as discussed above, and do not display the prediction to the user if the cost is above a threshold.\n\nAlternatively, train a second model mB that predicts, for an input, that the ﬁrst model mA is likely to make an error on that input. The presence of a “safeguard” model mB is especially relevant if model mA is used in a mission critical system.\n\nThe error’s visibility is an important factor in deciding whether and how to hide it. For example, consider a system that downloads web pages from the internet and extracts some entities from them. Let the user be interested in being alerted when a kind of entity is detected. The model can make two kinds of errors: 1) extract an entity even if the document doesn’t contain it (false positive, FP), and 2) not extract an entity that is present in the document (false negative, FN). When the former error happens, the user receives an irrelevant alert and gets frustrated. If the latter, the user doesn’t receive any alert, remains unaware of the error, and avoids frustration. In this situation, you might prefer to optimize the model for precision, by keeping recall reasonably high.\n\nWhen you train a model, decide which kind of errors you would most like to avoid, and then optimize your hyperparameters, including the prediction threshold, accordingly.\n\nWhen your conﬁdence for the best prediction is low, consider presenting several options. This is why Google presents 10 search results at once. There are much higher chances for the most relevant link to be among those 10 search results, than for it to be in the ﬁrst position.\n\nAnother way to avoid user’s frustration with model errors is to dose the user’s exposure to the model. Measure the number of errors your model makes, and estimate how many errors per minute (day, week, or month) a user is ready to tolerate. Then limit the interactions the user will have with the model to keep the number of perceived errors below that level.\n\nFor situations when an error happened and might have been perceived, add a possibility for the user to report the error. Once the report is received, log the context in which the model was used, as well as the prediction of the model. Explain to the user what actions will be taken to prevent a similar error from happening in the future.\n\nIt’s appropriate to measure the user’s engagement with the system, log all interactions, and then analyze suspicious interactions oﬄine. This includes:\n\nwhether the user interacts with the system less than before, • whether the user ignored certain recommendations, and • whether the user spent adequate time in various settings.\n\nTo reduce an error’s negative impact even further, if the system allows it, give the user an option to undo an action recommended by the system. Extend this, if possible, to any automated action executed by the system on the user’s behalf.\n\nSoftware applications that act on their user’s behalf must be especially limited in their possible actions. Recall that machine-learning models’ errors can be arbitrarily “crazy” like in the example of a self-driving car that can suddenly decide to drive backward. Caution must be exercised in other critical scenarios involving health, safety, or money, such as bidding in\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n11",
      "content_length": 3368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "auctions or prescribing medication. If the model predicts to buy or sell more stocks than the moving average plus one standard deviations, it’s a good idea to send an alert and put an otherwise “automatic” action on hold. The same logic should apply if the model predicts to serve an unreasonably high dose of a drug to a patient, or change the speed of the car to a value substantially above or below usual.\n\nIf your system can automatically reject the model prediction, it’s best to implement some fallback strategy, in addition to informing the user about a failure (Figure 4). A less sophisticated model or a handcrafted heuristic may be used as a fallback. Of course, the output of the fallback strategy should also be validated, and also rejected if it seems unreasonable. In this case, an error message should be sent to the user.\n\nFigure 4: Real-world model serving ﬂowchart.\n\n9.3.3 Being Ready for, and Dealing With, Change\n\nThe performance of a system based on machine learning usually changes over time. In some applications, it can change in near-real-time.\n\nThere are two types of model change:\n\n1. Its quality could become better or worse.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n12",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "2. The predictions for some inputs can become diﬀerent.\n\nA typical reason for the model performance degradation over time is concept drift that we already considered in Section ?? of Chapter 3. The notion of what is a correct prediction may change because of the users’ preferences and interests. This would require retraining the model, using more recently labeled data.\n\nSome change can be perceived by the user as positive. Sometimes, the change can be negatively perceived, even if the system’s performance improved, from the engineering point of view. You might have added training examples, retrained the model and observed a better performance metric value. However, by adding new data, you involuntarily induced a data imbalance. Some classes are now underrepresented. Users interested in those classes’ predictions see decreased performance, and complain or even abandon your system.\n\nUsers become accustomed to certain behaviors. They might know what query to submit to the search engine to get an often-used document or a web application. That query was not necessarily the most optimal for the purpose, but it worked. Suppose you improved the relevancy of your search-result ranking algorithm. Now that query doesn’t return that speciﬁc document or application, or it puts it on the second page of the search results. The user can no longer ﬁnd the resource they once found easily, and get frustrated.\n\nIf you expect that the user might negatively perceive the change, give them time to adapt. Educate the user about the changes and what to expect from the new model. Or, it can be done by gradually introducing the changes. You might mix the predictions of the old model and the new model, and slowly decrease the proportion for the old model. Alternatively, you can run both the new and the old model in parallel, and let the user switch to the old model for some time before sunsetting it.\n\n9.3.4 Being Ready for, and Dealing With, Human Nature\n\nHuman nature is what makes eﬀective system engineering such a hard endeavor. Humans are unpredictable, often irrational, inconsistent, and have unclear expectations. A solid software system must anticipate that.\n\nAvoid Confusion\n\nThe system must be designed in such a way that the user doesn’t feel confused interacting with it. A model’s output must be served in an intuitive way, without assuming that the user knows anything about machine learning and AI. In fact, many users will assume that they work with typical software and will be surprised to see errors.\n\nManage Expectations\n\nOn the other hand, some users will have too high expectations. The main reason for that is advertisement. To attract attention, a product or a system based on machine learning is often displayed in advertisements as being “intelligent.” For example, personal assistants such as Apple Siri, Google Home, and Amazon Alexa are often shown in advertisements as having\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n13",
      "content_length": 2968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "human intelligence. Indeed, any machine-learning-based system might look very intelligent when inputs are carefully selected. Users can look at such advertisements and extrapolate what they see to situations in which the system isn’t designed to operate eﬀectively.\n\nAnother common reason users expect something spectacular (even without being promised) is that they worked with a similar (in their understanding) system that looked “very intelligent” to them. Such users would expect the same level of “intelligence” from your system.\n\nGain Trust\n\nSome users, especially experienced ones, will mistrust any system if they know it contains some “intelligence.” The main reason for that mistrust is past experience. Most so-called intelligent systems fail to deliver, and, because of that, some users expect failure when they ﬁrst encounter your system.\n\nAs a consequence, your system must gain each user’s conﬁdence, and this must be done early.\n\nA user experienced with “intelligent” systems will most likely make several simple tests of your system’s abilities. If your system fails, the user will not trust it. For example, if your system is a search engine, then a user would query their name or a document they authored to test your system. Or, if your system provides intelligence on organizations to corporate customers, a user will check how much your system knows about their organization, and whether the intelligence makes sense. A driver of a self-driving car will most likely test commands like “start the engine,” “follow that car,” “keep the current speed,” or “park on that street.” Depending on the nature of the service, you should anticipate such simple tests and make sure that your system passes them.\n\nManage User Fatigue\n\nUser fatigue can be another reason why you see decreasing interest in your system. Make sure that the system doesn’t excessively interrupt user experience with recommendations or requests for approval. Avoid showing everything you have to show in one shot. Whenever possible, let the user explicitly express their interest.\n\nFurthermore, not all actions that the system can handle automatically have to be handled this way. For example, if the system automates user’s interactions with other people, it might send private or restricted data as an email reply, or post it to an open forum. Before sharing on a user’s behalf, it makes sense to evaluate the information’s sensitivity. Use a model trained to detect such potentially sensitive texts and images. On the other extreme, a system can be too conservative and automatically ﬁlter out relevant information or ask the user to conﬁrm too many decisions which might result in user fatigue.\n\nBeware of the Creep Factor\n\nWhen users interact with a learning system, there’s a phenomenon known as creep factor. It means that the user perceives the model’s predictive capacity as too high. The user feels uncomfortable, especially when a prediction concerns their very private details. Make sure that the system doesn’t feel like “Big Brother” and doesn’t take too much responsibility.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n14",
      "content_length": 3134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "9.4 Model Monitoring\n\nA deployed model must be constantly monitored. Monitoring helps make sure that,\n\nthe model is served correctly, and • the performance of the model remains within acceptable limits.\n\n9.4.1 What Can Go Wrong?\n\nMonitoring should be designed to provide early warnings about issues with the model in production. More speciﬁcally, this includes:\n\nnew training data used to update the model made it perform worse; • the live data in production changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is being abused or under an adversarial attack.\n\nAdditional training data is not always good. A labeler may have incorrectly interpreted the labeling instructions. Or, one labeler’s decisions might be in contradiction with another labeler. Data automatically gathered to improve the model may be biased. Reasons for that could be, for example, a hidden feedback loop considered in Section 9.1.6 or a systematic value distortion discussed in Section ?? of Chapter 3.\n\nSometimes, the properties of the data in production gradually change, but the model doesn’t adapt. It remains based on older data, which is no longer representative. One reason for this is concept drift that we discussed in Section 9.3.\n\nA software engineer could ﬁx a bug in the feature extraction code, and update the feature extractor in production. But if the engineer fails to also update the production model, the performance may change in an unpredictable manner.\n\nEven if the feature extraction and the model are in sync, a disappearance or a change of some resource (database connection, database table, or external API) may aﬀect some of the features generated by that feature extractor.\n\nSome models, especially those deployed in e-commerce and media platforms, often become targets of adversarial attacks. Bad actors, such as unfair competitors, fraudsters, criminals, and foreign governments, may actively seek out weaknesses in a model and adjust their attacks accordingly. If your machine learning system learns from the user’s actions, then some may act to change the model behavior in their favor.\n\nFurthermore, attackers may want to examine the trained model in order to obtain information about the model’s training data. That training data might contain conﬁdential information about people and organizations.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n15",
      "content_length": 2512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Another form of abuse, which may be the hardest to prevent, is model dual use. As any software, a machine learning model can be used for good (as you intended) or for bad (often without your consent). For example, you might create and publicly release a model that makes one’s voice sound like a cartoon character. Fraudsters may adapt your result to fake the voice of a bank client, and execute a phone transaction on their behalf. Alternatively, you might create a model that recognizes pedestrians on the street. An automatic weapon manufacturer could use your model to detect people on the battleﬁeld.\n\n9.4.2 What and How to Monitor\n\nMonitoring must allow us to make sure that the model generates reasonable performance metrics when applied to the conﬁdence test set. This set should be regularly updated with new data to avoid possible distribution shift. Additionally, the model must be regularly tested on the examples from the end-to-end set.\n\nWhile it’s obvious that accuracy, precision, and recall are good candidates for monitoring, one metric is especially useful for measuring the change over time: prediction bias.\n\nIn a static world where nothing changes, the distribution of predicted classes would roughly equal the distribution of observed classes. This is especially true when the model is well- calibrated. If you observe otherwise, the model is exhibiting prediction bias. The latter might mean that the distribution of the training data labels and the production’s current class distribution are now diﬀerent. You must investigate the reasons for this change and make the necessary adjustments.\n\nMonitoring allows us to stay alert of abandoned or repurposed data sources. Some database columns might stop being populated. The deﬁnition or format of the data in some columns might change, while the unadapted models still assume the previous deﬁnitions and formats. To avoid that, the distribution of the values of every feature extracted from a database table must be monitored for a signiﬁcant shift. A shift of the distribution of both feature values and predictions can be detected by applying statistical tests such as the Chi-square independence test and Kolmogorov–Smirnov test. If a signiﬁcant distribution shift is detected, an alert must be sent to the stakeholders.\n\nThe numerical stability of the model should also be monitored. An alert should be triggered if NaNs (not-a-numbers) or an inﬁnity is observed.\n\nIt’s important to monitor computational performance of a machine learning system. Both dramatic and slow-leak regression should be detected, and warnings must be sent.\n\nMonitor and send alerts when the usage ﬂuctuations look suspicious. In particular:\n\nmonitor the number of model servings during an hour, and compare it to the corre- sponding value calculated one day earlier. Send a warning alert to the stakeholders if the number has changed by 30% or more. This threshold must be tuned for your use case to avoid generating excessive warnings;\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n16",
      "content_length": 3047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "monitor the daily number of model servings and compare it to the corresponding value calculated one week earlier. Send a warning alert to the stakeholders if the number has changed by 15% or more. Tune the value for your use case.\n\nMonitoring these numbers helps detect undesirable change:\n\nminimal and maximal prediction values, • median, mean, and standard deviation prediction values over a given timeframe, • latency when calling the model API, and • memory consumption and CPU usage when performing predictions.\n\nAdditionally, to prevent distribution shift, the monitoring automation must:\n\n1) accumulate inputs by randomly putting some aside during a certain time period, 2) send those inputs for labeling, 3) run the model, and calculate the value of the performance metric, 4) alert the stakeholders if there is signiﬁcant performance degradation.\n\nRecommender systems need additional monitoring. These models oﬀer recommendations to website or application users. It can be useful to monitor click-though rate (CTR), that is, ratio of users who clicked on a recommendation to the number of total users who received recommendations from that model. If CTR is decreasing, the model must be updated.\n\nIt’s important to note that there is a tricky tradeoﬀ between being too conservative versus frequently alerting stakeholders about small changes in the metrics. If you alert too often, people might become tired of receiving alerts and eventually will start ignoring them. In non-mission-critical cases, it can be appropriate to allow the stakeholders to deﬁne their own thresholds that trigger alerts.\n\nLog monitoring events so the entire process is traceable. For visual model performance analysis, the monitoring tool’s user interface should provide trend charts showing how the model degradation evolves over time.\n\nOne of the monitoring tool’s properties should be the ability to compute and visualize metrics on slices of data. A slice is a subset of the data that includes only such examples in which a speciﬁc attribute has a certain value. For example, one slice could contain only the examples where the state attribute is Florida; another slice might contain only the data for women, and so on. The degradation of the model might only be observed in some slices, yet remain insigniﬁcant in others.\n\nBesides real-time monitoring, it’s important to also log data that:\n\nmight help ﬁnd a problem’s source, • is impossible to analyze in real-time, or • is helpful for improving existing models or training new ones.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n17",
      "content_length": 2584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "9.4.3 What to Log\n\nIt is important to log enough information to reproduce any erratic system behavior during a future analysis. If the model is served to a front-end user, such as a website visitor or a mobile application user, it’s worth saving the user’s context at the moment of the model serving. As discussed in Section 9.2, the context might include: the content of the webpage (or the state of the application), the user’s position on the web page, time of the day, where the user came from, and what they clicked before the model prediction was served.\n\nAdditionally, it is useful to include the model input, that is, the features extracted from the context, and the time it took to generate those features.\n\nThe log could also include:\n\nthe model’s output, and time it took to generate it, • the new context of the user, once they observed the model’s output, • the user’s reaction to the output.\n\nThe user’s reaction is the immediate action that followed the observation of the model output: what was clicked, and how much time after the output was served.\n\nIn large systems with thousands of users, where the model is served to each user hundreds of times a day, it can be prohibitive to log every event. It would be more practical to do stratiﬁed sampling. You ﬁrst decide which groups of events you want to log, and then you log only a certain percentage of events in each group. The groups can be groups of users or groups of contexts. Users can be grouped by age, gender, or seniority with the service (new clients vs. long-time clients). The groups of contexts could be early-morning, business-day, and late-night interactions.\n\nWhen you store users’ activity data in logs, the users should know what, when, and how it is stored, and for how long. If possible, data should be anonymized or aggregated without loss of utility. Access to sensitive data must be restricted only to those assigned to solve a speciﬁc problem during a speciﬁc time period. Avoid letting any analyst access sensitive data to solve unrelated business problems. It could lead to legal problems.\n\nMake sure users may opt-out from logging and analysis of their activity data. Diﬀerent data retention policies will apply to diﬀerent countries. Each country imposes its own restrictions on what can and cannot be stored about their citizens, or used for analysis.\n\n9.4.4 Monitor for Abuse\n\nSome people or organizations may use your model for their own business. Such users might send millions of daily requests, while a typical user would only send a dozen. Alternatively, some users might want to reverse-engineer the training data, or learn how to make the model produce a desired output.\n\nWays to prevent such abuse include,\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n18",
      "content_length": 2770,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "making users pay per request, • creating progressively longer pauses before responding to requests, or even • blocking some users.\n\nTo reach their own business goals, some attackers might try to manipulate your model. An attacker might submit data that changes the model in a way that only beneﬁts the attacker. As a result, the overall quality of the model might degrade.\n\nWays to prevent such abuse include,\n\nnot trusting the data from a user unless similar data comes from multiple users, • assigning a reputation score to each user, and not trusting the data obtained from users with low reputations, and\n\nclassifying user behavior as either normal or abnormal, and not accepting the data coming from users demonstrating abnormal behavior.\n\nThe attackers will try to bypass your defence by adapting their behavior. To eﬀectively defend your system, update your models regularly. Add both new data and new features that detect fraudulent transactions.\n\n9.5 Model Maintenance\n\nMost production models must be regularly updated. The rate depends on several factors:\n\nhow often it makes errors and how critical they are, • how “fresh” the model should be, so as to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to deploy the model, and • how much a model update contributes to the product and the achievement of user goals.\n\nIn this section, we talk about model maintenance: when and how to update the model after it’s deployed in production.\n\n9.5.1 When to Update\n\nWhen a model is deployed in production for the ﬁrst time, it’s often far from perfect. Inevitably, the model makes prediction errors. Some of them could be critical, so the model needs an update. Over time, a model could become more solid, and require fewer updates. However, some models should be constantly updated, so to speak, always be “fresh.”\n\nModel freshness depends on the business needs and the needs of the user. The recommender model on an e-commerce website must be updated after each purchase. If the user utilizes a model to get recommended content on a news website, the model might need to be updated weekly. On the other hand, a voice recognition/synthesis or a machine translation model could be updated less frequently.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n19",
      "content_length": 2338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "The speed of availability of new training data also aﬀects the rate of model updates. Even if new data comes in fast, such as the stream of comments on a popular website, it may take time and require signiﬁcant investment to get labeled data. Sometimes, labeling is automated but delayed, as in churn prediction, where the user’s decision to stay with or leave the service happens far in the future.\n\nSome models take signiﬁcant time to build, especially if the hyperparameter search is needed. It’s not uncommon to wait for days or even weeks to get a new version of the model. Use parallelizable machine learning algorithms and graphical processing units (GPU) to speed up the training. Modern libraries, such as thundersvm and cuML, allow the analyst to run shallow learning algorithms on GPUs, with a signiﬁcant gain in training time. If you cannot aﬀord to wait for days or weeks to get an updated model, using a less complex (and, therefore, less accurate) model might be your only choice.\n\nYou might decide to update the model less often if an update is costly. For example, in healthcare, getting labeled examples is complicated and expensive, due to regulations, privacy concerns, and expensive medical experts.\n\nNot all models are worth deploying. Sometimes the potential performance gain is not worth the user’s possible frustration. However, if the user disturbance is manageable, and the deployment is not costly, even a small improvement may result in a signiﬁcant business outcome in the long run.\n\n9.5.2 How to Update\n\nAs discussed, your software ideally allows the new model version to be deployed with- out stopping the entire system. In virtual or containerized infrastructure, this can be done by replacing the image of a virtual machine (VM) or a container in the repository, gradually closing VMs/containers, and letting the autoscaler instantiate a VM/container from an updated image.\n\nAn architecture of machine learning deployment and maintenance automation is schematically shown in Figure 5. Here, we have three repositories: data, code, and model; all three repositories are versioned. We also have two runtimes: model training and production. The model runs in the production runtime, which is load-balanced and auto-scaled. When an update of the model is needed, the model training runtime pulls the training data, as well as the model training code, from the data and code repositories, respectively. It then trains the new model and saves it in the model repository.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n20",
      "content_length": 2555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Figure 5: A machine learning deployment and maintenance automation architecture.\n\nFigure 6: On-demand model serving and update with a message broker.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n21",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Once a new version of the model is placed in the repository, the production runtime pulls,\n\nthe new model, from the model repository; • the test data, from the data repository; and, • the code that applies the model to the test data, from the code repository.\n\nIf the new model passes the test, the old model is withdrawn from production. It is replaced with the new one with the appropriate deployment strategy, as discussed in Section ??. A/B testing or a multi-armed bandit algorithm can help make the replacement decision.\n\nThe distribution shift control database accumulates the inputs received by the model, as well as their scoring results. Once a suﬃcient number of examples is accumulated, that data is sent for validation to a human1 with the goal of detecting distribution shift.\n\nIn the model streaming scenario, the model update happens when the stream processor’s state is updated (see Section 9.1.2 and Figure 2).\n\nModel update in on-demand model serving with a message broker architecture is similar to that of model streaming (see Section 9.2.3 and Figure 3).\n\nFigure 6 illustrates a message-broker-based architecture that allows not just serving the model and updating it, but also contains a human labeler in the loop. The labeler receives unlabeled examples, samples some of them, assigns labels to sampled examples, and sends the annotated examples back to the message broker. The model training module reads the labeled examples from a queue. When their quantity is suﬃcient to signiﬁcantly update the model, it trains a new model, saves it in the model repository, and sends the “model ready” message to the broker. A model-serving process pulls the new model version from the repository, and discards the current model.\n\nLet’s outline a few additional considerations for successful model maintenance.\n\nMany companies use a continuous integration workﬂow in which the models are trained automatically as soon as new training data becomes available. It is recommended to retrain the model from scratch, by using the entire training data, instead of ﬁne-tuning an existing model on the new examples only.\n\nFor each training example, it’s recommended to store the labeler’s identity. Furthermore, attach the model version used to generate a speciﬁc value in the production database, to that value. Should a problem with the version model be discovered, knowing which database values it generated will allow reprocessing of those speciﬁc values only.\n\nIf a model is frequently re-trained, it is convenient to store pipeline’s hyperparameters in a conﬁguration system. Google recommends2 the following for a good conﬁguration system:\n\n1. It should be easy to specify a conﬁguration as a change from a previous conﬁguration. 2. It should be hard to make manual errors, omissions, or oversights.\n\n1Or to an automated tool, more accurate than the model, that cannot be deployed in production (e.g., too\n\nfragile, costly, or slow).\n\n2“Hidden Technical Debt in Machine Learning Systems” by Sculley et al. (2015).\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n22",
      "content_length": 3082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "3. It should be easy to see, visually, the diﬀerence in conﬁguration between two models. 4. It should be easy to automatically assert and verify basic facts about a conﬁguration: number of features used, data dependencies, etc.\n\n5. It should be possible to detect unused or redundant settings. 6. Conﬁgurations should undergo a full code review and be checked into a repository.\n\nMake sure that the runtime environment has enough hard drive space and RAM for the updated model. Do not expect that the old version of the model and the new one will only diﬀer in performance. Be ready for the situation where the new model is much larger than the previous one. Similarly, do not expect that the new model will run as fast as the previous one. Ineﬃciency in the feature extraction code, an additional stage in the pipeline, or a diﬀerent choice of the algorithm may signiﬁcantly aﬀect the prediction speed.\n\nModels will inevitably make prediction errors. However, to the business or client, some errors are more costly than others. Once a new model version is deployed, validate it doesn’t make signiﬁcantly more costly errors than the previous model.\n\nCheck that the errors are distributed uniformly across the user categories. It’s undesirable if the new model negatively aﬀects more users from a minority or speciﬁc location.\n\nIf any of the above validations fail, it is not recommended to deploy the new model. Roll it back if the failure is detected after deployment and initiate an investigation. As discussed in Section 9.1, rolling back to the previous model must be as easy as deploying the new model.\n\nBeware of model cascading. As discussed in Section ?? of Chapter 6, if the one model’s outputs become inputs for another model, changing one model will aﬀect the performance the other. If your system is using model cascading, be sure to update all models in the cascade.\n\n9.6 Summary\n\nAn eﬀective runtime has the following properties. It is secure and correct, ensures ease of deployment and recovery, and provides guarantees of model validity. Furthermore, it avoids training/serving skew and hidden feedback loops.\n\nMachine learning models are served in either batch or on-demand mode. In on-demand mode, a model can be served to either a human client or a machine. A model is usually served in batch mode when it will be applied to big data and some latency is tolerable.\n\nWhen served on-demand to a human, a model is usually wrapped into a REST API. A machine’s data requirements are usually standard and pre-determined, so we often serve it by streaming.\n\nThe architecture of a software system intended for the real world must be ready for three phenomena: errors, change, and human nature.\n\nThe model deployed in production must be constantly monitored. The goals of monitoring are to make sure that the model is served correctly, and that the performance of the model\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n23",
      "content_length": 2940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "remains within acceptable limits.\n\nA variety of things might go wrong with the model in production, in particular:\n\nadditional training data made the model perform worse; • the properties of the production data changed, but the model didn’t; • the feature extraction code was signiﬁcantly updated, but the model didn’t adapt; • a resource needed to generate a feature changed or became unavailable; • the model is abused or is under an adversarial attack.\n\nAn automation must calculate values of the performance metrics critical for the business, and send alerts to the appropriate stakeholders if the values of those metrics change signiﬁcantly or fall below a threshold. In addition, the monitoring must reveal the distribution shift, numerical instability, and a decreasing computational performance.\n\nIt is important to log enough information to reproduce any erratic system behavior during an analysis in the future. If the model is served to a front-end user, it’s important to log the user’s context at the moment of the model serving. Additionally, it is useful to include the model input, that is, the features extracted from the context, and the time it took to generate those features. The log could also include the outputs obtained from the model, and time it took to generate it, the new context of the user once they observed the output of the model, and the reaction of the user to the output.\n\nSome users can utilize your model as a basis for their own business. They might reverse- engineer the training data, or learn how to “trick” your model. To prevent abuse:\n\ndon’t trust the data from a user unless similar data comes from multiple users, • assign a reputation score to each user and don’t trust the data obtained from users with low reputations,\n\nclassify user behavior as normal or abnormal, • make users pay per request, • make progressively longer pauses, and • block some users.\n\nMost machine learning models must be regularly or occasionally updated. The rate of updates depends on several factors:\n\nhow often it makes errors and how critical they are, • how “fresh” the model should be to be useful, • how fast new training data becomes available, • how much time it takes to retrain a model, • how costly it is to train and deploy the model, and • how much a model update contributes to the achievement of user goals.\n\nAfter a model update, a good practice is to run the model against the examples in the end-to-end and conﬁdence test sets. It’s important to make sure that the outputs are either the same as before, or that the changes are as expected. It’s also important to validate that the new model doesn’t make signiﬁcantly more costly errors than the previous model.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n24",
      "content_length": 2763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Check also that the errors are distributed uniformly across the user categories. It’s undesirable if the new model negatively aﬀects users from a minority or speciﬁc location.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n25",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "“Intheory,thereisnodifferencebetweentheoryandpractice.Butinpractice,thereis.”—BenjaminBrewster“Theperfectprojectplanispossibleifoneﬁrstdocumentsalistofalltheunknowns.”—BillLangley“Whenyou’refundraising,it’sAI.Whenyou’rehiring,it’sML.Whenyou’reimplementing,it’slinearregression.Whenyou’redebugging,it’sprintf().”—BaronSchwartzThebookisdistributedonthe“readﬁrst,buylater”principle.AndriyBurkovMachineLearningEngineering-Draft",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "10 Conclusion\n\nIn 2020, machine learning has become a mature and popular tool for solving business problems. What previously was available only to a handful of organizations, and considered “magic” by others, can today be created and used by a typical organization.\n\nThanks to open-source code, crowdsourcing, easily accessible books, online courses, and publicly available datasets, many scientists, engineers, and even home enthusiasts may now train machine learning models. If you are lucky, your problem can be solved by writing several lines of code, as demonstrated in many online tutorials.\n\nHowever, many things can go wrong in a machine learning project. Most are independent of the technology’s maturity or the analyst’s understanding of the machine learning algorithm.\n\nMachine learning texts, online tutorials, and courses are devoted to explaining how machine learning algorithms work and how to apply them to a dataset. Your success will probably depend on other factors. What data you can get and whether you can get enough of it, how you prepare it for learning, what features you engineer, whether your solution is scalable, maintainable, cannot be manipulated by attackers, and doesn’t make costly errors — these factors are much more important for an applied machine learning project.\n\nYet despite their magnitude, most modern machine learning books and courses often leave these aspects for self-study. Some provide only partial coverage, with just an application to solving a speciﬁc illustrative problem.\n\nIt’s a signiﬁcant gap in knowledge, and I tried to ﬁll it with this book.\n\n10.1 Takeaways\n\nWhat do I hope the reader takes away after reading this book?\n\nFirst of all, a strong understanding that all machine learning projects are unique. There’s no single recipe that will always work. Most of the time, the greatest challenges must be solved before you type from sklearn.linear_model import LogisticRegression: you must deﬁne your goal, select a baseline, gather relevant data, get it labeled with quality labels, and transform labeled data into training, validation, and test sets. The rest of the problem is solved after you type model.fit(X,y), by applying error analysis, evaluating the model, verifying it solves the problem, and works better than the existing solution.\n\nThe seasoned analyst or machine learning engineer understands that not all problems, business or otherwise, will be solved with machine learning. In fact, many problems can be solved more easily using a heuristic, a lookup in a database, or traditional software development. You probably should not use machine learning if the system’s every action, decision, or behavior, must be explained. With rare exceptions, machine learning models are blackboxes. They will not tell you why they predicted what they predicted, nor why they didn’t predict today what they predicted yesterday, nor how to ﬁx these issues.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n3",
      "content_length": 2971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Furthermore, unless you can ﬁnd a public dataset and an open-source solution providing exactly what you need, machine learning is not the right approach for the shortest time to market. Sometimes the data needed to train and maintain a model is too hard or even impossible to get.\n\nOn the other hand, training data may be synthetically generated by using oversampling and data augmentation. These techniques are often applied when the data exhibits imbalance.\n\nBefore you start collecting data, ask these questions: is your data accessible, sizeable, usable, understandable, and reliable? Good data contains enough information for modeling, has good coverage of production use cases and few biases, is big enough to allow generalization, and not a result of the model itself.\n\nOr does your data come with high cost, bias, imbalance, missing attributes, and/or noisy labels? Data quality must be ensured before it’s used for training.\n\nThe machine learning project life cycle consists of the following stages: goal deﬁnition, data collection and preparation, feature engineering, model training, evaluation, deployment, serving, monitoring, and maintenance. At most stages, data leakage may arise. The analyst must be able to anticipate and prevent it.\n\nAfter data preparation, feature engineering is the second most important stage. For some data, such as natural language text, features may be generated in bulk by using techniques like bag-of-words. However, the most useful features are often handcrafted by the analyst domain knowledge. Put yourself into the “model’s shoes.”\n\nGood features have high predictive power, can be computed fast, are reliable and uncor- related. They are unitary, easy to understand and maintain. Feature extraction code is one of the most important parts of a machine learning system. It must be extensively and systematically tested.\n\nBest practices are to scale features, store and document them in schema ﬁles or feature stores, and keep code, model, and training data in sync.\n\nYou can synthesize new features by discretizing existing features, clustering training examples, and applying simple transformations to existing features, or combining pairs of them.\n\nBefore starting to work on a model, make sure that data conforms to the schema, then split it into three sets: training, validation, and test. Deﬁne an achievable level of performance, and choose a performance metric. It should reduce the model performance to a single number.\n\nMost machine learning algorithms, models, and pipelines have hyperparameters. They can signiﬁcantly inﬂuence the result of learning. However, these are not learned from data. The analyst sets their values during the hyperparameter tuning. In particular, tweaking these values controls two important tradeoﬀs: precision-recall and bias-variance. By varying the complexity of the model, you can reach the so-called “zone of solutions,” a situation where both bias and variance are relatively low. The solution that optimizes the performance metric is usually found in the neighborhood of the zone of solutions. Grid search is the simplest and most widely-used hyperparameter-tuning technique.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n4",
      "content_length": 3223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Instead of training a deep model from scratch, it can be useful to start with a pre-trained model. Using a pre-trained model to build your own is called transfer learning. The fact that deep models allow for transfer learning is one of its most important properties.\n\nTraining deep models can be tricky. Implementation errors can happen at many stages, from data preparation, to deﬁning a neural network topology. It’s recommended to start small. For example, implement a simple model using a high-level library. Apply the default hyperparameter values to a small normalized dataset ﬁtting in memory. Once you have your ﬁrst simplistic model architecture and dataset, temporarily reduce your training dataset even further, to the size of one minibatch. Then start the training. Make sure that your simple model is capable of overﬁtting this training minibatch.\n\nYour machine learning system’s performance may beneﬁt from model stacking. Ideally, base models used for stacking are obtained from algorithms or models of a diﬀerent nature, such as random forests, gradient boosting, support vector machines, and deep models. Many real-world production systems are based on stacked models.\n\nMachine learning model errors can be either uniform, and apply to all use cases with the same rate, or focused, and apply to certain use cases more frequently. By ﬁxing a focused error, you ﬁx it once for many examples.\n\nModel performance can be improved using the following simple, iterative process:\n\n1. Build the model using the best values of hyperparameters identiﬁed so far. 2. Test the model by applying it to a small subset of the validation set. 3. Find the most frequent error patterns on that small validation set. 4. Generate new features, or add more training data to ﬁx the observed error patterns. 5. Repeat until no frequent error patterns are observed.\n\nThe model must be carefully evaluated before deployment, and continuously afterwards. Perform an oﬄine model evaluation when the model is initially trained, based on the historical data. An online model evaluation consists of testing and comparing models in the production environment, using online data. Two popular techniques of online model evaluation are A/B testing and multi-armed bandit. They allow us to determine whether the new model is better than the old one.\n\nA model may be deployed following several patterns: statically (as a part of an installable software package), dynamically on the user’s device or a server, or via model streaming. In addition, choose among strategies such as single deployment, silent deployment, canary deployment, and multi-armed bandit. Each pattern and strategy has its pros and cons, and should be chosen depending on your business application.\n\nAlgorithmic eﬃciency is also an important consideration for model deployment. Scientiﬁc Python packages like NumPy, SciPy, and scikit-learn were built by experienced scientists and engineers with eﬃciency in mind. They have many methods implemented in C for maximum eﬃciency. Avoid writing your own production code, when you can reuse a popular and mature library or package. For high eﬃciency, choose appropriate data structures and caching.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n5",
      "content_length": 3247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "For some applications, the prediction speed is critical. In such cases, the production code is written in a compiled language, such as Java or C/C++. If a data analyst has built a model in Python or R, there are several options for production deployment: rewrite the code in a compiled programming language of the production environment, use a model representation standard such as PMML or PFA, or use a specialized execution engine such as MLeap.\n\nMachine learning models are served in either batch or on-demand mode. When served on-demand, a model is usually wrapped into a REST API. Serving a machine is often done by using a streaming architecture.\n\nWhen a software system is exposed to the real world, its architecture must be ready to eﬀectively react to errors, change, and human nature. A model must be constantly monitored. Monitoring must allow us to make sure that the model is served correctly and its performance remains within acceptable limits.\n\nIt is important to log enough information to reproduce any erratic system behavior during future analysis. If the model is served to a front-end user, it’s important to save the user’s context at the moment of the model serving.\n\nSome users can try to abuse your model to reach their own business goals. To prevent abuse, don’t trust the data coming from a user, unless similar data comes from multiple users. Assign a reputation score to each user and don’t trust the data obtained from users with low reputations. Classify user behavior as normal or abnormal, and make progressively longer pauses or block some users, if necessary.\n\nRegularly update your model by analyzing users’ behavior and input data, to make it more robust. Afterwards, run the new model against the end-to-end and conﬁdence test sets. Make sure that the outputs are as before, or that the changes are as expected. Validate that the new model doesn’t make signiﬁcantly more costly errors. Ensure errors are distributed uniformly across user categories. It’s undesirable if the new model aﬀects negatively most users from a minority or speciﬁc location.\n\n∗ ∗ ∗\n\nThe book stops here, but your learning doesn’t. Machine learning engineering is a relatively new ﬁeld of software engineering. Thanks to online publications and open source, I’m sure that new best practices, libraries, and frameworks simplifying or solidifying the stages of data preparation, model evaluation, deployment, serving, and monitoring, will appear during the upcoming years. Subscribe to my mailing list on this book’s companion website http://www.mlebook.com. You will regularly receive relevant links.\n\nPlease keep in mind that, like its predecessor The Hundred-Page Machine Learning Book, this book is distributed on the “read-ﬁrst, buy-later” principle. This means that you may download the entire text of the book from its companion website and read before buying. If you’re reading these concluding words from a PDF ﬁle, and cannot remember having paid for it, please consider buying the book. You may buy it from Amazon, Leanpub, and other major online book sellers.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n6",
      "content_length": 3137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "10.2 What to Read Next\n\nThere are many great books on machine learning and artiﬁcial intelligence. Here, I will give you only several recommendations.\n\nIf you would like to get hands-on experience with practical machine learning in Python, there are two books:\n\n“Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” (2nd edition) by Aurélien Géron (O’Reilly Media, 2019), and\n\n“Python Machine Learning” (3rd edition) by Sebastian Raschka (Packt Publishing, 2019).\n\nFor R, the best choice is “Machine Learning with R” by Brett Lantz (Packt Publishing, 2019).\n\nTo get a deeper understanding of the underlying math behind various machine learning algorithms, I recommend,\n\n“Pattern Recognition and Machine Learning” by Christopher Bishop (Springer, 2006), and\n\n“An Introduction to Statistical Learning” by Gareth James et al. (Springer, 2013).\n\nTo get more detailed understanding of deep learning, I recommend,\n\n“Neural Networks and Deep Learning” by Michael Nielsen (online, 2005), and • “Generative Deep Learning” by David Foster (O’Reilly Media, 2019).\n\nIf your ambitions go far beyond machine learning and you want to sweep the whole ﬁeld of artiﬁcial intelligence, then “Artiﬁcial Intelligence: A Modern Approach” (4th Edition) by Stuart Russell and Peter Norvig (Pearson, 2020), known as AIMA, is your best book.\n\n10.3 Acknowledgements\n\nThe high quality of this book would be impossible without volunteering editors. I especially thank the following readers for their systematic contributions: Alexander Sack, Ana Fotina, Francesco Rinarelli, Yonas Mitike Kassa, Kelvin Sundli, Idris Aleem, and Tim Flocke.\n\nI thank scientiﬁc advisors, Veronique Tremblay and Maximilian Hudlberger, for the review and correction of the Model Evaluation chapter. I’m also grateful to Cassie Kozyrkov for her attentive and critical eye that allowed solidifying the section on statistical tests.\n\nOther wonderful people to whom I am grateful for their help are Jean Santos, Carlos Azevedo, Zakarie Hashi, Tridib Dutta, Zakariya Abu-Grin, Suhel Khan, Brad Ezard, Cole Holcomb, Oliver Proud, Michael Schock, Fernando Hannaka, Ayla Khan, Varuna Eswer, Stephen Fox, Brad Klassen, Felipe Duque, Alexandre Mundim, John Hill, Ryan Volpi, Gaurish Katlana, Harsha Srivatsa, Agrita Garnizone, Shyambhu Mukherjee, Christopher Thompson, Sylvain Truong, Niklas Hansson, Zhihao Wu, Max Schumacher, Piers Casimir, Harry Ritchie, Marko Peltojoki, Gregory V., Win Pet, Yihwa Kim, Timothée Bernard, Marwen Sallem, Daniel\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n7",
      "content_length": 2556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Bourguet, Aliza Rubenstein, Alice O., Juan Carlo Rebanal, Haider Al-Tahan, Josh Cooper, Venkata Yerubandi, Mahendren S., Abhijit Kumar, Mathieu Bouchard, Yacin Bahi, Samir Char, Luis Leopoldo Perez, Mitchell DeHaven, Martin Gubri, Guillermo Santamaría, Mustafa Murat Arat, Rex Donahey, Nathaniel Netirungroj, Aliza Rubenstein , Rahima Karimova, Darwin Brochero, Vaheid Wallets, Bharat Raghunathan, Carlos Salas, Ji Hui Yang, Jonas Atarust, Siddarth Sampangi, Utkarsh Mittal, Felipe Antunes, Larysa Visengeriyeva, Sorin Gatea, Mattia Pancerasa, Victor Zabalza, Dibyendu Mandal, and James Hoover.\n\nAndriy Burkov\n\nMachine Learning Engineering - Draft\n\n8",
      "content_length": 650,
      "extraction_method": "Unstructured"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}