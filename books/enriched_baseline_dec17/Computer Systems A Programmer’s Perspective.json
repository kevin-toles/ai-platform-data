{
  "metadata": {
    "title": "Computer Systems A Programmer’s Perspective",
    "source_file": "Computer Systems A Programmer’s Perspective_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-20)",
      "start_page": 1,
      "end_page": 20,
      "summary": "Programs Are Translated by Other Programs into Different Forms\nIt Pays to Understand How Compilation Systems Work\nProcessors Read and Interpret Instructions Stored in Memory\nRunning the hello Program\nThe Operating System Manages the Hardware\nVirtual Memory\nProgram Structure and Execution\nBit-Level Operations in C\nLogical Operations in C\nShift Operations in C\nFloating-Point Operations\nFloating Point in C\nMachine-Level Representation of Programs\nProgram Encodings\nMachine-Level Code\nCode Examples\nArithmetic and Logical Operations\nSpecial Arithmetic Operations\nAccessing the Condition Codes\nCombining Control and Data in Machine-Level Programs\n3.10.3 Out-of-Bounds Memory References and Buffer Overﬂow\nFloating-Point Code\n3.11.1 Floating-Point Movement and Conversion Operations\n3.11.2 Floating-Point Code in Procedures\n3.11.3 Floating-Point Arithmetic Operations\n3.11.5 Using Bitwise Operations in Floating-Point Code\n3.11.6 Floating-Point Comparison Operations\n3.11.7 Observations about Floating-Point Code\nY86-64 Programs\nLogic Design and the Hardware Control Language HCL\nOptimizing Program Performance\nExpressing Program Performance\nProgram Example\nAn Abstract Model of Processor Operation\nUnderstanding Memory Performance\n5.14.1 Program Proﬁling\nThe Memory Hierarchy\nLocality of References to Program Data\nThe Memory Hierarchy\nCaching in the Memory Hierarchy\nSummary of Memory Hierarchy Concepts\nCache Memories\nGeneric Cache Memory Organization\nWriting Cache-Friendly Code\nPutting It Together: The Impact of Caches on Program Performance\nExploiting Locality in Your Programs\nRunning Programs on a System\nExceptions in Linux/x86-64 Systems\nProcess Control\nLoading and Running Programs\nUsing fork and execve to Run Programs\nVirtual Memory\nLinux Virtual Memory System\nMemory Mapping\nUser-Level Memory Mapping with the mmap Function\nDynamic Memory Allocation\nWhy Dynamic Memory Allocation?\n9.10.3 Conservative Mark&Sweep for C Programs\nCommon Memory-Related Bugs in C Programs\n9.11.2 Reading Uninitialized Memory\nbetween Programs\nNetwork Programming\nThe Client-Server Programming Model\nConcurrent Programming\nConcurrent Programming with Processes\nConcurrent Programming with I/O Multiplexing\nConcurrent Programming with Threads\nShared Variables in Threaded Programs\n12.4.1 Threads Memory Model\n12.4.2 Mapping Variables to Memory\n12.7.3 Using Existing Library Functions in Threaded Programs\nothers who want to be able to write better programs by learning what is going on\nmance, and utility of your application programs.\nMany systems books are written\ntems software, including the operating system, compiler, and network interface.\nprogrammers can use their knowledge of a system to write better programs.\nwho go on to implement systems hardware and software.\nMost systems books also\ntecture, the operating system, the compiler, or the network.\nYou will be able to write programs that make better\nuse of the capabilities provided by the operating system and systems software,\nthat operate correctly across a wide range of operating conditions and run-time\nparameters, that run faster, and that avoid the ﬂaws that make programs vulner-\nsuch as compilers, computer architecture, operating systems, embedded systems,\nThis book focuses on systems that execute x86-64 machine code.\nWe consider how these machines execute C programs on Linux.\nof a number of operating systems having their heritage in the Unix operating\nAdvice on the C programming language\nTo help readers whose background in C programming is weak (or nonexistent), we have also included\nof operating systems include Solaris, FreeBSD, and MacOS X.\nthese operating systems have maintained a high level of compatibility through the\nThe text contains numerous programming examples that have been compiled\nand run on Linux systems.\nVMWare) that allow programs written for one operating system (the guest OS)\nJava and C share similar syntax and control statements.\nHowever, there are aspects of C (particularly pointers, explicit dynamic memory\nRegardless of your programming\nSeveral of the early chapters in the book explore the interactions between C\nprograms and their machine-language counterparts.\nassembly-language programming.\nLearning how computer systems work from a programmer’s perspective is great\nthe only way to learn systems is to do systems, either working concrete problems\nor writing and running programs on real systems.\ncode/intro/hello.c\ncode/intro/hello.c\nA typical code example.\nLittle or no programming required.\ncode.\nEach code example in the text was formatted directly, without any manual\nintervention, from a C program compiled with gcc and tested on a Linux system.\naltogether, so your compiler might generate different machine code; but the\nIn the text, the ﬁlenames of the source programs are documented\nFor example, the program in\nFigure 1 can be found in the ﬁle hello.c in directory code/intro/.\nyou to try running the example programs on your system as you encounter them.\ntion on data representations in Chapter 2, while Web Aside arch:vlog contains\nThe CS:APP book consists of 12 chapters designed to capture the core ideas in\nChapter 1: A Tour of Computer Systems.\nworld” program.\nbit-level operations of C to demonstrate the principles and applications of\nit represents values and the mathematical properties of ﬂoating-point oper-\ning reliable programs.\nFor example, programmers and compilers cannot re-\nArithmetic overﬂow is a common source of programming\nChapter 3: Machine-Level Representation of Programs.\nthe x86-64 machine code generated by a C compiler.\nuse the machine-level view of programs as a way to understand common\nFor example, where did C, Linux, and the Internet come from?\nOther asides give real-world examples, such as how a ﬂoating-point error crashed\ngrammer, the compiler, and the operating system can take to reduce these\nprogrammer, because you will understand how programs are represented\nThis chapter covers basic combinational and\nthen introduce pipelining, where the different steps required to process an\nChapter 5: Optimizing Program Performance.\nThis chapter introduces a number\ngrammers learn to write their C code in such a way that a compiler can then\nthe work to be done by a program and hence should be standard practice\nwhen writing any program for any machine.\ngenerated machine code, thereby improving their performance on modern\na simple operational model of how modern out-of-order processors work,\nand show how to measure the potential performance of a program in terms\nof the critical paths through a graphical representation of a program.\nwill be surprised how much you can speed up a program by simple transfor-\nmations of the C code.\nChapter 6: The Memory Hierarchy.\nIn practice, a memory system is a hierarchy of\nshow you how to improve the performance of application programs by\nThis chapter covers both static and dynamic linking, including\nlocation, static libraries, shared object libraries, position-independent code,\nLinking is not covered in most systems texts,\nChapter 8: Exceptional Control Flow.\nbeyond the single-program model by introducing the general concept of\nWe cover examples of exceptional\nof a process, an abstraction of an executing program.\ncation programs.\nprogram execution.\nChapter 9: Virtual Memory.\nIn particular, we cover the operation of storage\nallocators such as the standard-library malloc and free operations.",
      "keywords": [
        "Practice Problems",
        "Memory",
        "Memory System",
        "Programs",
        "Problems",
        "Systems",
        "Homework Problems",
        "System",
        "Virtual Memory System",
        "Code",
        "Computer Systems",
        "Program",
        "Linux Memory System",
        "Bibliographic Notes",
        "Virtual Memory"
      ],
      "concepts": [
        "programs",
        "program",
        "programming",
        "memory",
        "memories",
        "operating",
        "operations",
        "operation",
        "operate",
        "operational"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 11,
          "title": "",
          "score": 0.905,
          "base_score": 0.755,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.702,
          "base_score": 0.702,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.654,
          "base_score": 0.654,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "systems",
          "programs",
          "memory",
          "floating",
          "floating point"
        ],
        "semantic": [],
        "merged": [
          "systems",
          "programs",
          "memory",
          "floating",
          "floating point"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.5075712385951184,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.711931+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 21-38)",
      "start_page": 21,
      "end_page": 38,
      "summary": "the virtual memory space is just an array of bytes that the program can\nof programs containing memory referencing errors such as storage leaks\nsystems texts present only part of the virtual memory story.\nChapter 10: System-Level I/O.\ncover the C standard I/O library and its relationship to Linux I/O, focusing\non limitations of standard I/O that make it unsuitable for network program-\nthe next two chapters on network and concurrent programming.\nChapter 11: Network Programming.\nNetwork programs also provide a compelling context for con-\nthrough network programming that gets you to the point where you can\nChapter 12: Concurrent Programming.\nprogramming to express parallelism in an application program, enabling\nrunning C programs under Linux (and related operating systems) has proved to\nchanges in hardware technology, compilers, program library interfaces, and the\nChapter 3: Machine-Level Representation of Programs.\nhave included, for the ﬁrst time, a presentation of the machine-level support\nfor programs operating on ﬂoating-point data.\nChapter 4: Processor Architecture.\nChapter 5: Optimizing Program Performance.\ncontrol logic, the model of program performance we developed based on a\ndata-ﬂow representation of programs has become a more reliable predictor\nChapter 10: System-Level I/O.\nChapter 11: Network Programming.\nindependent and thread-safe network programming using the modern\nChapter 12: Concurrent Programming.\nthread-level parallelism to make programs run faster on multi-core ma-\nThe ICS course has been taught every semester since then.\nstudents take the course each semester.\nbecome a prerequisite for most upper-level systems courses in CS and ECE.\nThe idea with ICS was to introduce students to computers in a different way.\nengineers, would be required to use and program computers on a daily basis.\ncorrectness, or utility of user-level C programs.\nics such as machine language were in; but instead of focusing on how to write\nassembly language by hand, we would look at how a C compiler translates C con-\nas both hardware and systems software, covering such topics as linking, loading,\nprocesses, signals, performance optimization, virtual memory, I/O, and network\nand concurrent programming.\nThis approach allowed us to teach the ICS course in a way that is practical,\nFor Instructors: Courses Based on the Book\nInstructors can use the CS:APP book to teach a number of different types of\nsystems courses.\nA computer organization course with traditional topics covered in an un-\nture, assembly language, and memory systems are covered.\nresentations are related back to the data types and operations of C programs,\nby a C compiler rather than handwritten assembly code.\non the performance of application programs.\nformance of their C programs.\nThe baseline ICS course, designed to produce enlightened programmers who\nsystem on the performance and correctness of their application programs.\nA signiﬁcant difference from ORG+ is that low-level processor architecture\nThe baseline ICS course with additional coverage of systems programming\ntopics such as system-level I/O, network programming, and concurrent pro-\nevery chapter in CS:APP except low-level processor architecture.\n⊙(c)\n⊙(c)\nNetwork programming\nConcurrent programming\nFive systems courses based on the CS:APP book.\nICS+ is the 15-213 course\nfollows: (a) hardware only; (b) no dynamic storage allocation; (c) no dynamic linking;\nA systems programming course.\nsis on systems programming, including process control, dynamic linking,\nsystem-level I/O, network programming, and concurrent programming.\nIf students have no experience in C (e.g., they have only\nprogrammed in Java), you could spend several weeks on C and then cover the\nThe ICS+ course at Carnegie Mellon receives very high evaluations from students.\nMedian scores of 5.0/5.0 and means of 4.6/5.0 are typical for the student course\nexamples of the labs that are provided with the book.\nData Lab. This lab requires students to implement simple logical and arithmetic\nlab helps students understand the bit-level representations of C data types\nBinary Bomb Lab. A binary bomb is a program provided to students as an object-\nThe lab teaches students to\nBuffer Overﬂow Lab. Students are required to modify the run-time behavior of\nPerformance Lab. Students must optimize the performance of an application ker-\nstudents experience with low-level program optimization.\nCache Lab. In this alternative to the performance lab, students write a general-\nShell Lab. Students implement their own Unix shell program with job control,\nThis lab gives students a clear understanding of data\ncepts from the course, such as byte ordering, ﬁle I/O, process control, signals,\nICS course over the years and who have provided so much insightful feedback\nasync-signal safety and protocol-independent network programming.\nthe ICS course at Carnegie Mellon for their insightful feedback and encourage-\nA special thanks to our 15-213 students, whose infec-\nGuy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mowry taught the course\nKesden provided helpful feedback on the impact of ICS on the OS course.\nA very special thanks to Al Davis (University of\nA special thanks to their students as well!\nPrograms Are Translated by Other Programs into Different Forms\nIt Pays to Understand How Compilation Systems Work\ngether to run application programs.\nperformance of their programs.\nand its impact on your application programs.\nto optimize your C code by using clever tricks that exploit the designs of modern\nprocessors and memory systems.\nIn their classic text on the C programming language [61], Kernighan and\nRitchie introduce readers to C using the hello program shown in Figure 1.1.\nAlthough hello is a very simple program, every major part of the system must\nbook is to help you understand what happens and why when you run hello on\nWe begin our study of systems by tracing the lifetime of the hello program,\nAs we follow the lifetime of the program, we will\ncode/intro/hello.c\ncode/intro/hello.c\nThe hello program.\nThe ASCII text representation of hello.c. 1.1\nOur hello program begins life as a source program (or source ﬁle) that the\nprogrammer creates with an editor and saves in a text ﬁle called hello.c. The\nsource program is a sequence of bits, each with a value of 0 or 1, organized in 8-bit\nEach byte represents some text character in the program.\nFigure 1.2 shows the ASCII representation of the hello.c program.\nThe hello.c program is stored in a ﬁle as a sequence of bytes.\nFiles such as hello.c that consist exclusively\nThe representation of hello.c illustrates a fundamental idea: All information\nin a system—including disk ﬁles, programs stored in memory, user data stored in\nOrigins of the C programming language\nlanguage and a set of library functions known as the C standard library.\n. C was closely tied with the Unix operating system.\nC was developed from the beginning as the\nsystem programming language for Unix.\nsystem), and all of its supporting tools and libraries, were written in C.\nuniversities in the late 1970s and early 1980s, many people were exposed to C and found that they\nSince Unix was written almost entirely in C, it could be easily ported to new machines,\n. C is a small, simple language.The design was controlled by a single person, rather than a committee,\nThe simplicity of C made it relatively easy to learn and to port to different computers.\n. C was designed for a practical purpose.\nC was designed to implement the Unix operating system.\nLater, other people found that they could write the programs they wanted, without the language\nC is the language of choice for system-level programming, and there is a huge installed base of\napplication-level programs as well.\nC pointers are a common source of confusion and programming errors.\nNewer languages such as C++ and Java\naddress these issues for application-level programs.\nPrograms Are Translated by Other Programs\nThe hello program begins life as a high-level C program because it can be read\nHowever, in order to run hello.c\non the system, the individual C statements must be translated by other programs\ninto a sequence of low-level machine-language instructions.\nthen packaged in a form called an executable object program and stored as a binary\nObject programs are also referred to as executable object ﬁles.\nhello.c\nhello.o\nprogram\nprogram\nprogram\nprograms\nprogram\nlinux> gcc -o hello hello.c\nHere, the gcc compiler driver reads the source ﬁle hello.c and translates it into\nan executable object ﬁle hello.\nThe programs that perform the four phases\n. Preprocessing phase.The preprocessor (cpp) modiﬁes the original C program\n#include <stdio.h> command in line 1 of hello.c tells the preprocessor\ninto the program text.\nThe result is another C program, typically with the .i\nThe compiler (cc1) translates the text ﬁle hello.i into\nthe text ﬁle hello.s, which contains an assembly-language program.\nprogram includes the following deﬁnition of function main:\nit provides a common output language for different compilers for different\nFor example, C compilers and Fortran compilers both\nNext, the assembler (as) translates hello.s into machine-\nprogram, and stores the result in the object ﬁle hello.o. This ﬁle is a binary\nwere to view hello.o with a text editor, it would appear to be gibberish.\nSupported languages include C, C++, Fortran, Java, Pascal, Objective-C, and Ada. The GNU project is a remarkable achievement, and yet it is often overlooked.\nis part of the standard C library provided by every C compiler.\nmust somehow be merged with our hello.o program.\nIt Pays to Understand How Compilation Systems Work\nFor simple programs such as hello.c, we can rely on the compilation system to\nreasons why programmers need to understand how compilation systems work:\n. Optimizing program performance.\nthe inner workings of the compiler in order to write efﬁcient code.\nin order to make good coding decisions in our C programs, we do need a\nbasic understanding of machine-level code and how the compiler translates\ndifferent C statements into machine code.\nIn Chapter 3, we introduce x86-64, the machine language of recent gen-\ncompilers translate different C constructs into this language.\nyou will learn how to tune the performance of your C programs by making\nsimple transformations to the C code that help the compiler do its job better.\ntem, how C compilers store data arrays in memory, and how your C programs\ning programming errors are related to the operation of the linker, especially\nyou deﬁne two global variables in different C ﬁles with the same name?\nA ﬁrst step in learning secure programming is to understand the con-\nsequences of the way data and control information are stored on the program\nChapter 3 as part of our study of assembly language.\nAt this point, our hello.c source program has been translated by the compilation\nsystem into an executable object ﬁle called hello that is stored on disk.\nthe executable ﬁle on a Unix system, we type its name to an application program\nPC: program counter, USB:\nin this case, the shell loads and runs the hello program and then waits for it to\nterminate.The hello program printsitsmessagetothescreenandthenterminates.\nTo understand what happens to our hello program when we run it, we need\nthroughout the course of the book.",
      "keywords": [
        "program",
        "system",
        "systems",
        "programs",
        "University",
        "ICS",
        "computer systems",
        "students",
        "Unix operating system",
        "programming",
        "Network Programming",
        "memory",
        "Lab",
        "ﬁle",
        "operating system"
      ],
      "concepts": [
        "university",
        "universities",
        "universal",
        "program",
        "programs",
        "programming",
        "programmed",
        "students",
        "student",
        "chapters"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.878,
          "base_score": 0.728,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.702,
          "base_score": 0.702,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "hello",
          "students",
          "course",
          "ics",
          "lab"
        ],
        "semantic": [],
        "merged": [
          "hello",
          "students",
          "course",
          "ics",
          "lab"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4358497777381152,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712042+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 39-59)",
      "start_page": 39,
      "end_page": 59,
      "summary": "data and programs.\nInitially, the executable hello program resides on the disk.\nsets in the device itself or on the system’s main printed circuit board (often called\nChapter 6 has more to say about how I/O devices such as disks work.\nChapter 10, you will learn how to use the Unix I/O interface to access devices from\nyour application programs.\nMain Memory\nThe main memory is a temporary storage device that holds both a program and\nthe data it manipulates while the processor is executing the program.\ninstructions that constitute a program can consist of a variable number of bytes.\nFor example, on an x86-64 machine running Linux, data of type short\nChapter 6 has more to say about how memory technologies such as DRAM\nchips work, and how they are combined to form main memory.\nProcessor\nThe central processing unit (CPU), or simply processor, is the engine that inter-\nprets (or executes) instructions stored in main memory.\nstorage device (or register) called the program counter (PC).\nmain memory.2\npower is shut off, a processor repeatedly executes the instruction pointed at by the\nprogram counter and updates the program counter to point to the next instruction.\nA processor appears to operate according to a very simple instruction execution\nIn this model, instructions execute\nin strict sequence, and executing a single instruction involves performing a series\nThe processor reads the instruction from memory pointed at by the\nprogram counter (PC), interprets the bits in the instruction, performs some simple\ninstruction, which may or may not be contiguous in memory to the instruction that\nmain memory, the register ﬁle, and the arithmetic/logic unit (ALU).\n. Load: Copy a byte or a word from main memory into a register, overwriting\n. Store: Copy a byte or a word from a register to a location in main memory,\n. Operate: Copy the contents of two registers to the ALU, perform an arithmetic\nmechanisms to speed up program execution.\nabstraction provided by the machine’s instruction set architecture.\nthe performance of machine-language programs.\nRunning the hello Program\nGiven this simple view of a system’s hardware organization and operation, we can\nbegin to understand what happens when we run our example program.\nInitially, the shell program is executing its instructions, waiting for us to type a\nAs we type the characters ./hello at the keyboard, the shell program\nreads each one into a register and then stores it in memory, as shown in Figure 1.5.\nThe shell then loads the executable hello ﬁle by\nexecuting a sequence of instructions that copies the code and data in the hello\nMemory bus\nmemory\nobject ﬁle from disk to main memory.\nter 6), the data travel directly from disk to main memory, without passing through\nthe processor.\nOnce the code and data in the hello object ﬁle are loaded into memory,\nthe processor begins executing the machine-language instructions in the hello\nprogram’s main routine.\nstring from memory to the register ﬁle, and from there to the display device, where\nthe hello program are originally stored on disk.\nWhen the program is loaded,\nthey are copied to main memory.\nAs the processor runs the program, instruc-\ntions are copied from main memory into the processor.\nhello,world\\n, originally on disk, is copied to main memory and then copied\nfrom main memory to the display device.\nMemory bus\nmemory\nLoading the executable from disk into main memory.\nMemory bus\nmemory\nCache memories.\nmemories\nMemory bus\nmemory\nlarger than the main memory, but it might take the processor 10,000,000 times\nlonger to read a word from disk than from memory.\nas opposed to billions of bytes in the main memory.\nHowever, the processor can\nread data from the register ﬁle almost 100 times faster than from memory.\nprocessors run faster than it is to make main memory run faster.\nfaster storage devices called cache memories (or simply caches) that serve as\nFigure 1.8 shows the cache memories in a typical system.\ntimes longer for the processor to access the L2 cache than the L1 cache, but this is\nstill 5 to 10 times faster than accessing the main memory.\nimplemented with a hardware technology known as static random access memory\nNewer and more powerful systems even have three levels of cache: L1,\nprograms to access data and code in localized regions.\ndata that are likely to be accessed often, we can perform most memory operations\nOne of the most important lessons in this book is that application program-\nmers who are aware of cache memories can exploit them to improve the perfor-\nretrieved from cache memory.\nMain memory holds disk blocks \nMain memory\nAn example of a memory hierarchy.\nThis notion of inserting a smaller, faster storage device (e.g., cache memory)\nbetween the processor and a larger, slower device (e.g., main memory) turns out\norganized as a memory hierarchy similar to Figure 1.9.\nWe show three levels of caching L1 to L3, occupying memory\nMain memory occupies level 4, and so on.\nThe main idea of a memory hierarchy is that storage at one level serves as a\nis a cache for the main memory, which is a cache for the disk.\nsystems with distributed ﬁle systems, the local disk serves as a cache for data stored\nperformance, programmers can exploit their understanding of the entire memory\nWhen the shell loaded and ran the hello program,\nApplication programs\nMain memory\nProcessor\nMain memory\nProcessor\nVirtual memory\nkeyboard, display, disk, or main memory directly.\na layer of software interposed between the application program and the hardware,\nfundamental abstractions shown in Figure 1.11: processes, virtual memory, and\nAs this ﬁgure suggests, ﬁles are abstractions for I/O devices, virtual memory\nis an abstraction for both the main memory and disk I/O devices, and processes\nare abstractions for the processor, main memory, and I/O devices.\nWhen a program such as hello runs on a modern system, the operating system\nprovides the illusion that the program is the only one running on the system.\nprogram appears to have exclusive use of both the processor, main memory, and\nThe processor appears to execute the instructions in the program, one\nAnd the code and data of the program appear\nto be the only objects in the system’s memory.\nA process is the operating system’s abstraction for a running program.\nmost systems, there are more processes to run than there are CPUs to run them.\nnotion of a shell as a user-level process, were borrowed from Multics but implemented in a smaller,\nlanguage interface for Unix system calls, shell programs and utilities, threads, and network program-\nTraditional systems could only execute one program at a time, while newer multi-\ncore processors can execute several programs simultaneously.\nsingle CPU can appear to execute multiple processes concurrently by having the\nmemory.\nAt any point in time, a uniprocessor system can only execute the code\nand the hello process.\nWhen we ask it to run the hello program, the shell carries\ncreates a new hello process and its context, and then passes control to the new\nhello process.\nsystem code that is always resident in memory.\nWhen an application program\nexecutes a special system call instruction, transferring control to the kernel.\nkernel then performs the requested operation and returns back to the application\nprogram.\nof code and data structures that the system uses to manage all the processes.\nImplementing the process abstraction requires close cooperation between\nboth the low-level hardware and the operating system software.\nhow this works, and how applications can create and control their own processes,\nsystems a process can actually consist of multiple execution units, called threads,\neach running in the context of the process and sharing the same code and global\nThreads are an increasingly important programming model because of the\nrequirement for concurrency in network servers, because it is easier to share data\nprograms run faster when multiple processors are available, as we will discuss in\nMemory\nProgram\nKernel virtual memory\nwrite threaded programs, in Chapter 12.\nVirtual Memory\nVirtual memory is an abstraction that provides each process with the illusion that it\nhas exclusive use of the main memory.\nmemory, which is known as its virtual address space.\nin the operating system that is common to all processes.\naddress space holds the code and data deﬁned by the user’s process.\n. Program code and data.Code begins at the same ﬁxed address for all processes,\n. Heap.The code and data areas are followed immediately by the run-time heap.\nheaps in detail when we learn about managing virtual memory in Chapter 9.\nexpands and contracts dynamically during the execution of the program.\n. Kernel virtual memory.The top region of the address space is reserved for the\nApplication programs are not allowed to read or write the contents of\nFor virtual memory to work, a sophisticated interaction is required between\nof a process’s virtual memory on disk and then use the main memory as a cache\noperation of modern systems.\nFurther, the same program will run on different systems that use\nthe system copies a sequence of bytes from main memory to the network adapter,\nthese data to its main memory.\nMemory bus\nmemory\nruns the hello program\nReturning to our hello example, we could use the familiar telnet application\nFrom this point, running the hello program remotely involves\nstring from the network, it passes it along to the remote shell program.\nremote shell runs the hello program and passes the output line back to the telnet\norder to achieve the ultimate goal of running application programs.\nConsider a system in which executing some application requires time\nBuilding on the process abstraction, we are able to devise systems where multiple\nprograms execute at the same time, leading to concurrency.\ncan even have multiple control ﬂows executing within a single process.\nfor concurrent execution has been found in computer systems since the advent\nAll processors\nMain memory\ncache as well as the interface to main memory.\nhaving multiple copies of some of the CPU hardware, such as program counters\nprocessor requires around 20,000 clock cycles to shift between different threads,\na hyperthreaded processor decides which of its threads to execute on a cycle-by-\nFor example, if one thread must wait for some data to be loaded into a cache, the\ntel Core i7 processor can have each core executing two threads, and so a four-core\nprogram faster, but only if that program is expressed in terms of multiple threads\napplication programs that can exploit the thread-level parallelism available with\nuse to provide a sharing of processing resources and to enable more parallelism\nin program execution.\nAt a much lower level of abstraction, modern processors can execute multiple\ninstructions at one time, a property known as instruction-level parallelism.\nmultiple (typically 3–10) clock cycles to execute a single instruction.\nprocessors can sustain execution rates of 2–4 instructions per clock cycle.\nmore, but the processor uses a number of clever tricks to process as many as 100\ninstructions at a time.\nactions required to execute an instruction are partitioned into different steps and\nthe processor hardware is organized as a series of stages, each performing one\nProcessors that can sustain execution rates faster than 1 instruction per cycle\nIn Chapter 5, we will describe a high-level model of such processors.\nperformance of their programs.\nSingle-Instruction, Multiple-Data (SIMD) Parallelism\nAt the lowest level, many modern processors have special hardware that allows\na single instruction to cause multiple operations to be performed in parallel, a\nmode known as single-instruction, multiple-data (SIMD) parallelism.\nwrite programs using special vector data types supported in compilers such as gcc.\nDifferent program-\nMain memory\nProcessor\nVirtual memory\nOn the processor side, the instruction set\narchitecture provides an abstraction of the actual processor hardware.\nabstraction, a machine-code program behaves as if it were executed on a proces-\nsor that performs just one instruction at a time.\nmore elaborate, executing multiple instructions in parallel, but always in a way\ntion model, different processor implementations can execute the same machine\nan abstraction of I/O devices, virtual memory as an abstraction of program mem-\nory, and processes as an abstraction of a running program.\ncomputer, including the operating system, the processor, and the programs.\nprograms designed for multiple operating systems (such as Microsoft Windows,\nto run application programs.\nProcessors read and interpret binary instructions that are stored in main mem-\nSince computers spend most of their time copying data between memory, I/O\ncache memories, DRAM main memory, and disk storage.\nof their C programs by understanding and exploiting the memory hierarchy.\n(2) Virtual memory is an abstraction for both main\nmemory and disks.\n(3) Processes are abstractions for the processor, main memory,\nputer itself, comprising a processor and a memory subsystem.\nsider how machine-level instructions manipulate data and how a com-\npiler translates C programs into these instructions.\nhow hardware resources are used to execute instructions.\nmize program performance by writing C programs that, when compiled,\napplication programs are represented and executed.",
      "keywords": [
        "Main Memory",
        "system",
        "Operating System",
        "Memory",
        "program",
        "systems",
        "processor",
        "main",
        "Virtual memory",
        "Devices",
        "programs",
        "computer systems",
        "Cache",
        "process",
        "Operating"
      ],
      "concepts": [
        "programs",
        "programming",
        "processor",
        "processors",
        "processing",
        "processes",
        "process",
        "memory",
        "memories",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.777,
          "base_score": 0.627,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 11,
          "title": "",
          "score": 0.699,
          "base_score": 0.699,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "main memory",
          "memory",
          "main",
          "processor",
          "hello"
        ],
        "semantic": [],
        "merged": [
          "main memory",
          "memory",
          "main",
          "processor",
          "hello"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41089163516623917,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712101+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 60-78)",
      "start_page": 60,
      "end_page": 78,
      "summary": "binary number system, we can use groups of bits to encode nonnegative numbers.\nComputer representations use a limited number of bits to encode a number,\ncomputers (those using a 32-bit representation for data type int), computing the\nFor example, the C expression\nof values that can be represented and the properties of the different arithmetic\nComputers use several different binary representations to encode numeric\ninto machine-level programming in Chapter 3.\nnipulating the bit-level representations of numbers.\nproperties as the range of representable numbers, their bit-level representations,\nThe C++ programming language is built upon C, using the exact same numeric\nfor C++.\nWhereas the C standards\n8 bits, or bytes, as the smallest addressable unit of memory.\nprogram views memory as a very large array of bytes, referred to as virtual\nEvery byte of memory is identiﬁed by a unique number, known as its\npresented to the machine-level program.\nprovide the program with what appears to be a monolithic byte array.\nFor example, the value of a pointer in C—whether it points to an integer,\na structure, or some other program object—is the virtual address of the ﬁrst byte\nThe C compiler also associates type information with\neach pointer, so that it can generate different machine-level code to access the\nvalue stored at the location designated by the pointer depending on the type of\nAlthough the C compiler maintains this type information, the actual\nmachine-level program it generates has no information about data types.\ntreats each program object as a block of bytes and the program itself as a sequence\nof bytes.\nThe evolution of the C programming language\nAs was described in an aside on page 40, the C programming language was ﬁrst developed by Dennis\nin order to have access to the low-level representations of different data types.\nis named “ISO C11,” again adding more data types and features.\ndifferent versions of the C language, based on different command-line options, as shown in Figure 2.1.\nFor example, to compile program prog.c according to ISO C11, we could give the command line\nThe role of pointers in C\nJust like a variable, a pointer has two aspects: its value and its type.\nvalue indicates the location of some object, while its type indicates what kind of object (e.g., integer or\nA single byte consists of 8 bits.\nInstead, we write bit patterns as base-16, or hexadecimal numbers.\nbinary values associated with the 16 hexadecimal digits.\nthe value of a single byte can range from 0016 to FF16.\nIn C, numeric constants starting with 0x or 0X are interpreted as being in\nthe C notation for representing hexadecimal values in this book.\nvert between decimal, binary, and hexadecimal representations of bit patterns.\n2, giving hexadecimal representation 0x800.\n(C)\n(C)\nA single byte can be represented by 2 hexadecimal digits.\nentries in the following table, giving the decimal, binary, and hexadecimal values\nof different byte patterns:\nis, for a machine with a w-bit word size, the virtual addresses can range from 0 to\n2w −1, giving the program access to at most 2w bytes.\nMost 64-bit machines can also run programs compiled for use on 32-bit ma-\nSo, for example, when a program prog.c\nthen this program will run correctly on either a 32-bit or a 64-bit machine.\nwill only run on a 64-bit machine.\nprogram is compiled, rather than the type of machine on which it runs.\nComputers and compilers support multiple data formats using different ways\nto encode data, such as integers and ﬂoating point, as well as different lengths.\nFor example, many machines have instructions for manipulating single bytes, as\nwell as integers represented as 2-, 4-, and 8-byte quantities.\nﬂoating-point numbers represented as 4- and 8-byte quantities.\nThe C language supports multiple data formats for both integer and ﬂoating-\nFigure 2.3 shows the number of bytes typically allocated for different C\ndata types.\nversus what is typical in Section 2.2.) The exact numbers of bytes for some data\ntypes depends on how the program is compiled.\nand 64-bit programs.\ntype char represents a single byte.\nData types short, int, and long are intended to provide a range of\nC declaration\nBytes\nTypical sizes (in bytes) of basic C data types.\nThe number of bytes allocated\nand 64-bit programs.\nFor any data type T , the declaration\nindicates that p is a pointer variable, pointing to an object of type T .\nis the declaration of a pointer to an object of type char.\nEven when compiled for 64-bit systems, data type int is usually just 4 bytes.\nData type long commonly has 4 bytes in 32-bit programs and 8 bytes in 64-bit\ntings, ISO C99 introduced a class of data types where the data sizes are ﬁxed\nAmong these are data types int32_t\nand int64_t, having exactly 4 and 8 bytes, respectively.\nMost of the data types encode signed values, unless preﬁxed by the keyword\nunsigned or using the speciﬁc unsigned declaration for ﬁxed-size data types.\nthese as signed data, the C standard does not guarantee this.\nto guarantee a 1-byte signed value.\nbehavior is insensitive to whether data type char is signed or unsigned.\ntype char *) uses the full word size of the program.\ntwo different ﬂoating-point formats: single precision, declared in C as float,\nThese formats use 4 and 8 bytes,\ntive to the exact sizes of the different data types.\non the numeric ranges of the different data types, as will be covered later, but there\nWith 32-bit machines and\nbit programs in Figure 2.3.\ndeclared as type int could be used to store a pointer.\nAddressing and Byte Ordering\nFor program objects that span multiple bytes, we must establish two conventions:\nwhat the address of the object will be, and how we will order the bytes in memory.\nIn virtually all machines, a multi-byte object is stored as a contiguous sequence\nFor example, suppose a variable x of type int has address 0x100; that is, the\nThen (assuming data type int has a\n32-bit representation) the 4 bytes of x would be stored in memory locations 0x100,\nFor ordering the bytes representing an object, there are two common conven-\nof 8, these bits can be grouped as bytes, with the most signiﬁcant byte having bits\n, xw−8], the least signiﬁcant byte having bits [x7, x6, .\nthe other bytes having bits from the middle.\nject in memory ordered from least signiﬁcant byte to most, while other machines\nbyte comes ﬁrst—is referred to as little endian.\nmost signiﬁcant byte comes ﬁrst—is referred to as big endian.\nSuppose the variable x of type int and at address 0x100 has a hexadecimal\nvalue of 0x01234567.\nThe ordering of the bytes within the address range 0x100\nthrough 0x103 depends on the type of machine:\nNote that in the word 0x01234567 the high-order byte has hexadecimal value\n0x01, while the low-order byte has value 0x67.\nIn practice, however, byte ordering becomes ﬁxed once a\nFor most application programmers, the byte orderings used by their machines\nAt times, however, byte ordering becomes an issue.\nbinary data are communicated over a network between different machines.\ncommon problem is for data produced by a little-endian machine to be sent to\na big-endian machine, or vice versa, leading to the bytes within the words being\nthe byte sequences representing integer data.\nmachine-level programs.\ngives a text representation of the machine-level code for an Intel x86-64 processor:\nsimply note that this line states that the hexadecimal byte sequence 01 05 43 0b\n20 00 is the byte-level representation of an instruction that adds a word of data\nis a common occurrence when reading machine-level program representations\nA third case where byte ordering becomes visible is when programs are\na different data type from which it was created.\nFigure 2.4 shows C code that uses casting to access and print the byte rep-\nWe use typedef to deﬁne data type\nbyte_pointer as a pointer to an object of type unsigned char.\nSuch a byte pointer\nThe ﬁrst routine show_bytes is given the address of a sequence of\nbytes, indicated by a byte pointer, and a byte count.\nIt prints the individual bytes in hexadecimal.\ntypedef unsigned char *byte_pointer;\nvoid show_bytes(byte_pointer start, size_t len) {\nshow_bytes((byte_pointer) &x, sizeof(int));\nshow_bytes((byte_pointer) &x, sizeof(float));\nshow_bytes((byte_pointer) &x, sizeof(void *));\nCode to print the byte representation of program objects.\ndata types.\nuse procedure show_bytes to print the byte representations of C program objects\nbytes a pointer &x to their argument x, casting the pointer to be of type unsigned\npointer to be to a sequence of bytes rather than to an object of the original data\nThis pointer will then be to the lowest byte address occupied by the object.\nThese procedures use the C sizeof operator to determine the number of bytes\nbytes required to store an object of type T .\nis one step toward writing code that is portable across different machine types.\ncode/data/show-bytes.c\ncode/data/show-bytes.c\nByte representation examples.\nThis code prints the byte representations\nBytes (hex)\nByte representations of different data values.\nare identical, except for byte ordering.\nPointer values are machine dependent.\nOur argument 12,345 has hexadecimal representation 0x00003039.\ndata, we get identical results for all machines, except for the byte ordering.\nparticular, we can see that the least signiﬁcant byte value of 0x39 is printed ﬁrst\nSimilarly, the bytes of the float data\nare identical, except for the byte ordering.\nLinux 32, Windows, and Sun machines use 4-byte addresses, while the Linux 64\nmachine uses 8-byte addresses.\nThe typedef declaration in C provides a way of giving a name to a data type.\nThus, the declaration of byte_pointer in Figure 2.4 has the same form as\ndecimal integer, %f to print a ﬂoating-point number, and %c to print a character having the character\nSpecifying the formatting of ﬁxed-size data types, such as int_32t, is a bit more involved, as is\nthe numeric value 12,345, they have very different byte patterns: 0x00003039\nIn function show_bytes (Figure 2.4), we see the close connection between pointers and arrays, as will\nWe see that this function has an argument start of type byte_\nIn C, we can dereference a pointer with array notation, and we can reference array\nIn lines 13, 17, and 21 of Figure 2.4 we see uses of two operations that give C (and therefore C++) its\ndepends on the type of x, and hence these three pointers are of type int *, float *, and void **,\n(Data type void * is a special kind of pointer with no associated type information.)\nThe cast operator converts from one data type to another.\nThus, the cast (byte_pointer) &x\nindicates that whatever type the pointer &x had before, the program will now reference a pointer to\ndata of type unsigned char.\nthe compiler to refer to the data being pointed to according to the new data type.\nshow_bytes(ap, 3); /* C.\nIndicate the values that will be printed by each call on a little-endian machine\nC. Little endian:\nA. Write the binary representations of these two hexadecimal values.\nx happens to be 0x3x, and that the terminating byte has the hex representation\ncharacter code, independent of the byte ordering and word size conventions.\nWhat would be printed as a result of the following call to show_bytes?\nthe following byte representations:\n81 c3 e0 08 90 02 00 09",
      "keywords": [
        "byte",
        "bytes",
        "data type",
        "data types",
        "data",
        "type",
        "pointer",
        "Hexadecimal",
        "data type int",
        "int",
        "Byte Ordering",
        "program",
        "type int",
        "machines",
        "Representations"
      ],
      "concepts": [
        "bytes",
        "byte",
        "machines",
        "machine",
        "programs",
        "program",
        "programming",
        "values",
        "value",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 11,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.894,
          "base_score": 0.744,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.697,
          "base_score": 0.697,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.604,
          "base_score": 0.604,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "byte",
          "type",
          "data",
          "bytes",
          "hexadecimal"
        ],
        "semantic": [],
        "merged": [
          "byte",
          "type",
          "data",
          "bytes",
          "hexadecimal"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46199177550840537,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712158+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 79-99)",
      "start_page": 79,
      "end_page": 99,
      "summary": "The base encoding, known as the “Universal Character Set” of Unicode, uses a 32-bit representa-\nencoding logic values true and false as binary values 1 and 0, he could formulate\nthese operations are chosen to match those used by the C bit-level operations,\nBinary values 1 and 0 encode logic values\nWe can extend the four Boolean operations to also operate on bit vectors,\nWe deﬁne the operations over bit\nLet a and b denote the bit vectors [aw−1, aw−2, .\nWe deﬁne a & b to also be a bit vector of length w, where the ith\nbit vectors in a similar fashion.\nFill in the following table showing the results of evaluating Boolean operations on\nbit vectors.\nThe Boolean operations |, &, and ~ operating on bit vectors of length w form a Boolean algebra,\nthe more general case there are 2w bit vectors of length w.\nc), Boolean operation & distributes over |, written a & (b | c) = (a & b) |\nBoolean operation | distributes over &, and so we can write a | (b & c) =\nWhen we consider operations ^, &, and ~ operating on bit vectors of length w, we get a different\nFor example, one property of integer arithmetic is that every value x has an additive\nThat is, a ^ a = 0 for any value a,\nwhere we use 0 here to represent a bit vector of all zeros.\n0 ^ 0 = 1 ^ 1 = 0, and it extends to bit vectors as well.\nOne useful application of bit vectors is to represent ﬁnite sets.\n, w −1} with a bit vector [aw−1, .\nright, bit vector a = [01101001]encodes the set A = {0, 3, 5, 6}, while bit vector b =\na & b yields bit vector [01000001], while A ∩B = {0, 6}.\nWe will see the encoding of sets by bit vectors in a number of practical\nenable or disable different signals by specifying a bit-vector mask, where a 1 in\nEach of these colors can be represented as a bit vector of length 3, and we can\nB. Describe the effect of applying Boolean operations on the following colors:\nYellow & Cyan\nBit-Level Operations in C\nOne useful feature of C is that it supports bitwise Boolean operations.\nsymbols we have used for the Boolean operations are exactly those used by C:\n~0x41\n~0x00\n0x41\nAs our examples show, the best way to determine the effect of a bit-level ex-\nAs an application of the property that a ^ a = 0 for any bit vector a, consider the\nthe values stored at the locations denoted by pointer variables x and y.\nStarting with values a and b in the locations pointed to by x and y, respectively,\n*x\nOne common use of bit-level operations is to implement masking operations,\nwhere a mask is a bit pattern that indicates a selected set of bits within a word.\nan example, the mask 0xFF (having ones for the least signiﬁcant 8 bits) indicates\nThe bit-level operation x & 0xFF yields a value\n32 bits, but it would not be as portable.\nWrite C expressions, in terms of variable x, for the following values.\nA. The least signiﬁcant byte of x, with all other bits set to 0.\n[0x00000021]\nB. All but the least signiﬁcant byte of x complemented, with the least signiﬁcant\nC. The least signiﬁcant byte set to all ones, and all other bytes of x left un-\nand or, it had instructions bis (bit set) and bic (bit clear).\nx modiﬁed according to the bits of m.\nTo see how these operations relate to the C bit-level operations, assume we\nhave functions bis and bic implementing the bit set and bit clear operations, and\nand ^, without using any other C operations.\nWrite C expressions for the operations bis and bic.\nLogical Operations in C\nC also provides a set of logical operators ||, &&, and !, which correspond to the\n!0x41\n0x00\n!0x00\n0x01\n!!0x41\nversus their bit-level counterparts ‘&’ and ‘|’ is that the logical operators do not\nSuppose that a and b have byte values 0x55 and 0x46, respectively.\nfollowing table indicating the byte values of the different C expressions:\nValue\nValue\nUsing only bit-level and logical operations, write a C expression that is equivalent\nShift Operations in C\nC also provides a set of shift operations for shifting bit patterns to the left and to\nFor an operand x having bit representation [xw−1, xw−2, .\n, x0], the C\nexpression x << k yields a value with bit representation [xw−k−1, xw−k−2, .\nThat is, x is shifted k bits to the left, dropping off the k most signiﬁcant\nbits and ﬁlling the right end with k zeros.\nThe shift amount should be a value\nShift operations associate from left to right, so x << j << k\nThere is a corresponding right shift operation, written in C as x >> k, but it has\nmost signiﬁcant bit, giving a result [xw−1, .\noperating on signed integer data.\nshift operations to two different values of an 8-bit argument x:\nValue 1\nValue 2\nSince its most signiﬁcant bit\nThe C standards do not precisely deﬁne which type of right shift should be\ncombinations use arithmetic right shifts for signed data, and many programmers\nFor unsigned data, on the other hand, right shifts must\nFill in the table below showing the effects of the different shift operations on single-\nConvert the initial values to binary, perform the shifts, and then\nFor a data type consisting of w bits, what should be the effect of shifting by some value k ≥w?\nexample, what should be the effect of computing the following expressions, assuming data type int has\nshift instructions consider only the lower log2 w bits of the shift amount when shifting a w-bit value, and\nﬁne and characterize how computers encode and operate on integer data.\nUnsigned to two’s complement\nTwo’s complement to unsigned\nMinimum two’s-complement value\nMaximum two’s-complement value\nMaximum unsigned value\nTerminology for integer data and arithmetic operations.\nw denotes the number of bits in the data representation.\nC supports a variety of integral data types—ones that represent ﬁnite ranges of\nThese are shown in Figures 2.9 and 2.10, along with the ranges of values\nthey can have for “typical” 32- and 64-bit programs.\n64 bits.\nvalues to be represented.\nMost 64-bit programs use an 8-byte representation, giving a much\nwider range of values than the 4-byte representation used with 32-bit programs.\nC data type\nTypical ranges for C integral data types for 32-bit programs.\nC data type\nTypical ranges for C integral data types for 64-bit programs.\nThe C standards deﬁne minimum ranges of values that each data type must\nSigned and unsigned numbers in C, C++, and Java\nBoth C and C++ support signed (the default) and unsigned numbers.\nC data type\nGuaranteed ranges for C integral data types.\nthat the data types have at least these ranges of values.\nto the days of 16-bit machines.\nwith 4-byte numbers, and it typically is for 32-bit programs.\ntypes guarantee that the ranges of values will be exactly those given by the typical\nLet us consider an integer data type of w bits.\nWe write a bit vector as either ⃗x, to\n, x0] to denote the individual bits\nIn this encoding, each bit xi has value 0 or 1, with the\nUnsigned number\nWhen bit i in the binary\nrepresentation has value 1,\nvalue.\ngiven by B2U, from bit vectors to integers for the following cases:\nIn the ﬁgure, we represent each bit position i by a rightward-pointing blue bar of\nThe numeric value associated with a bit vector then equals the sum of\nthe lengths of the bars for which the corresponding bit values are 1.\nLet us consider the range of values that can be represented using w bits.\nleast value is given by bit vector [00 .\nvalue is given by bit vector [11 .\n1] having integer value UMaxw .=\nUsing the 4-bit case as an example, we have UMax4 = B2U4([1111]) =\nnumber between 0 and 2w −1has a unique encoding as a w-bit value.\nthere is only one representation of decimal value 11 as an unsigned 4-bit number—\nit maps a value x to a value y where y = f (x), but it can also operate in reverse,\nsince for every y, there is a unique value x such that f (x) = y.\nthe inverse function f −1, where, for our example, x = f −1(y).\nmaps each bit vector of length w to a unique number between 0 and 2w −1, and\nnumber in the range 0 to 2w −1 to a unique pattern of w bits.\nmon computer representation of signed numbers is known as two’s-complement\nThis is deﬁned by interpreting the most signiﬁcant bit of the word to have\nThe most signiﬁcant bit xw−1 is also called the sign bit.\nWhen the sign bit is set\nto 1, the represented value is negative, and when set to 0, the value is nonnegative.\nAs examples, Figure 2.13 shows the mapping, given by B2T, from bit vectors to\nIn the ﬁgure, we indicate that the sign bit has negative weight by showing it as\nThe numeric value associated with a bit vector is\nBit 3 serves as a\nsign bit; when set to 1, it\nthe value.\nWe see that the bit patterns are identical for Figures 2.12 and 2.13 (as well as\nfor Equations 2.2 and 2.4), but the values differ when the most signiﬁcant bit is 1,\nLet us consider the range of values that can be represented as a w-bit two’s-\nThe least representable value is given by bit vector [10 .\n(set the bit with negative weight but clear all others), having integer value\nThe greatest value is given by bit vector [01 .\n1] (clear the bit\nwith negative weight but set all others), having integer value TMaxw .=\nUsing the 4-bit case as an example, we have TMin4 = B2T4([1000]) =\nWe can see that B2Tw is a mapping of bit patterns of length w to numbers be-\nrange has a unique encoding as a w-bit two’s-complement number.\na principle for two’s-complement numbers similar to that for unsigned numbers:\n(unique) w-bit pattern that encodes x.\nAssuming w = 4, we can assign a numeric value to each possible hexadecimal\ndigit, assuming either an unsigned or a two’s-complement interpretation.\nFigure 2.14 shows the bit patterns and numeric values for several important\nintegers in terms of the values of UMaxw, TMinw, and TMaxw.\n(those with the sign bit set to 1) represent negative numbers, while half (those\nwith the sign bit set to 0) represent nonnegative numbers.\nthe maximum unsigned value is just over twice the maximum two’s-complement\nAll of the bit patterns that denote negative numbers in\ntwo’s-complement notation become positive values in an unsigned representation.\nValue\nFor some programs, it is essential that data types be encoded using representations with speciﬁc sizes.\nWe have seen that some C data types, especially long, have different ranges on different machines,\nand in fact the C standards only specify the minimum ranges for any data type, not the exact ranges.\nWe have already encountered the 32- and 64-bit versions of ﬁxed-size integer types (Figure 2.3);\nuintN_t, specifying N-bit signed and unsigned integers, for different values of N.\nunambiguously declare an unsigned 16-bit variable by giving it type uint16_t, and a signed variable\nof 32 bits as int32_t.\nAlong with these data types are a set of macros deﬁning the minimum and maximum values for\nSo, for example, the values of variables x and y of type int32_t and\nWhen compiled as a 64-bit program, macro PRId32 expands to the string \"d\", while PRIu64 expands\nhas the same bit representation as UMax—a string of all ones.\nThe C standards do not require signed integers to be represented in two’s-\nparticular range of representable values, beyond the ranges indicated in Figure\nrepresentation of signed numbers, and the “typical” ranges shown in Figures 2.9\nThis is the same as two’s complement, except that the most signiﬁcant bit has\nThe value −0 can be represented\non ones’-complement representations were built in the past, almost all modern machines use two’s\n“two’s complement” arises from the fact that for nonnegative x we compute a w-bit representation\ndelimiting the ranges of the different integer data types for the particular machine\ntwo’s-complement machine in which data type int has w bits, these constants\nIt requires a two’s-complement representation with the exact ranges\nshown for the 64-bit case (Figure 2.10).\nBit\nValue\nBit\nValue\nBit\nValue\nTwo’s-complement representations of 12,345 and −12,345, and\nNote that the latter two have identical bit\nExpanding these into binary, we get bit patterns\nEquation 2.3 yields values 12,345 and −12,345 for these two bit patterns.\nﬁles contain many hexadecimal numbers, typically representing values in two’s-\nhexadecimal values (in 32-bit two’s-complement form) shown to the right of the\nC allows casting between different numeric data types.\nvariable x is declared as int and u as unsigned.\nThe expression (unsigned) x\nconverts the value of x to an unsigned value, and (int) u converts the value of u\nWhat should be the effect of casting signed value to unsigned,\nOn the other hand, converting a negative value to unsigned might yield\nConverting an unsigned value that is too large to be represented in two’s-\nWhat we see here is that the effect of casting is to keep the bit values identical\nbut change how these bits are interpreted.\nWe saw in Figure 2.15 that the 16-bit\ntwo’s-complement representation of −12,345 is identical to the 16-bit unsigned\nnumeric value, but not the bit representation.",
      "keywords": [
        "bit",
        "Boolean operations",
        "bit vector",
        "unsigned",
        "operations",
        "Boolean",
        "Data Types",
        "data type",
        "DATA",
        "Boolean Algebra",
        "bits",
        "operation",
        "int",
        "numbers",
        "representation"
      ],
      "concepts": [
        "values",
        "bit",
        "bits",
        "operating",
        "operations",
        "operation",
        "operate",
        "operators",
        "operator",
        "unsigned"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 10,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.617,
          "base_score": 0.467,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 7,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "bit",
          "value",
          "bit vector",
          "complement",
          "ranges"
        ],
        "semantic": [],
        "merged": [
          "bit",
          "value",
          "bit vector",
          "complement",
          "ranges"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3351499357150479,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712209+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 100-121)",
      "start_page": 100,
      "end_page": 121,
      "summary": "ing 4,294,967,295 (UMax32) in unsigned form and −1 in two’s-complement form\nIn casting from unsigned to int, the underlying bit representation\nbetween signed and unsigned numbers with the same word size—the numeric\nnumbers to their bit representations in either unsigned or two’s-complement form.\ngives the unique w-bit unsigned representation of x.\nrange TMinw ≤x ≤TMaxw, the function T2Bw(x) gives the unique w-bit two’s-\ncomplement representation of x.\nUMaxw, where the two numbers have identical bit representations, except that\nthe argument has a two’s-complement representation while the result is unsigned.\nB2Tw(U2Bw(x)), yields the number having the same two’s-complement represen-\ntation as the unsigned representation of x.\nand the unsigned representation of 53,191.\nmeric values (two’s complement and unsigned) represented by a given bit pat-\nsigned form as does −1 in two’s-complement form.\ncomplement number to its unsigned counterpart, while U2T converts in the op-\ncomplement and unsigned values for a given bit pattern can be expressed as a\nprinciple: Conversion from two’s complement to unsigned\n\u0006 x + 2w,\nx < 0\nx,\nderivation: Conversion from two’s complement to unsigned\nthe difference B2Uw(⃗x) −B2Tw(⃗x), the weighted sums for bits from 0 to w −2 will\nIn a two’s-complement representation of x, bit xw−1 determines whether or not x\nto bit patterns for w = 4.\nFor the two’s-complement case, the most signiﬁcant bit\nunsigned case, this bit has positive weight, which we show as a rightward-pointing\nIn going from two’s complement to unsigned, the most signiﬁcant bit\ntive in a two’s-complement representation increase by 24 = 16 with an unsigned\ncomplement to unsigned.\nUnsigned\nmapping a signed number to its unsigned counterpart, negative numbers are con-\nprinciple: Unsigned to two’s-complement conversion\nunsigned to two’s\nUnsigned\nderivation: Unsigned to two’s-complement conversion\nThis bit vector will also be the two’s-complement representation\nIn the unsigned representation of u, bit uw−1determines whether or not u is greater\n(≤TMaxw) numbers, the conversion from unsigned to signed preserves the nu-\nbetween unsigned and two’s-complement representations.\nFor values x in the\nbers in this range have identical unsigned and two’s-complement representations.\nexample, we have T2Uw(−1) = −1 + 2w = UMaxw—the negative number clos-\ntwo’s-complement numbers.\nSigned versus Unsigned in C\nAs indicated in Figures 2.9 and 2.10, C supports both signed and unsigned arith-\nify a particular representation of signed numbers, almost all machines use two’s\nC allows conversion between unsigned and signed.\nthe effect of applying the function U2Tw when converting from unsigned to signed,\nand T2Uw when converting from signed to unsigned, where w is the number of\nWhen printing numeric values with printf, the directives %d, %u, and %x\nare used to print a number as a signed decimal, an unsigned decimal, and in\n%u and a value of type unsigned with directive %d.\nint x = -1;\nx = 4294967295 = -1\nIn both cases, printf prints the word ﬁrst as if it represented an unsigned number\nimplicitly casts the signed argument to unsigned and performs the operations\nUnsigned\nUnsigned\nUnsigned\n(unsigned) -1\nUnsigned\nunsigned.\n32-bit two’s-complement representation.\nExpanding the Bit Representation of a Number\nTo convert an unsigned number to a larger data type, we can simply add\nFor converting a two’s-complement number to a larger data type, the rule\nprinciple: Expansion of a two’s-complement number by sign extension\n, x0] of width w and ⃗x′ = [xw−1, .\nWe see that, although the two’s-complement representation of −12,345 and the\nunsigned representation of 53,191 are identical for a 16-bit word size, they dif-\nBit vector [101]represents the value −4 + 1= −3.\nApplying sign extension gives bit vector [1101] representing the value −8 + 4 +\nbits, −8 + 4 = −4, matches the value of the sign bit for w = 3.\nof a two’s-complement number.\nbit for w = 3.\nderivation: Expansion of a two’s-complement number by sign extension\n, x0])\nby 1 bit preserves the numeric value, then this property will hold when sign\n, x0])\n, x0])\nShow that each of the following bit vectors is a two’s-complement representation\ndata size to another and between unsigned and signed can affect the behavior of\n0x00000076\n0x87654321\nSuppose that, rather than extending a value with extra bits, we reduce the number\nof bits representing a number.\nint x = 53191;\nCasting x to be short will truncate a 32-bit int to a 16-bit short.\nbefore, this 16-bit pattern is the two’s-complement representation of −12,345.\nones, yielding the 32-bit two’s-complement representation of −12,345.\nWhen truncating a w-bit number ⃗x = [xw−1, xw−2, .\n, x0] to a k-bit number,\n, x0].\nFor an unsigned\nprinciple: Truncation of an unsigned number\n, x0].\nderivation: Truncation of an unsigned number\n, x0])\nA similar property holds for truncating a two’s-complement number, except\nprinciple: Truncation of a two’s-complement number\n, x0].\nIn this formulation, x mod 2k will be a number between 0 and 2k −1.\nof converting value x = 53,191from int to short.\nBut when we convert this number to a 16-bit two’s-complement\nderivation: Truncation of a two’s-complement number\nUsing a similar argument to the one we used for truncation of an unsigned number\n, x0])\nThat is, x mod 2k can be represented by an unsigned number having bit-level rep-\n, x0].\nConverting this to a two’s-complement number\nSummarizing, the effect of truncation for unsigned numbers is\nwhile the effect for two’s-complement numbers is\nthe effect of this truncation for some cases, in terms of the unsigned and two’s-\nUnsigned\nunsigned.\nbers can yield a negative result, and that the comparison x < y can yield a different\nresult than the comparison x-y < 0.\nConsider two nonnegative integers x and y, such that 0 ≤x, y < 2w.\nthese values can be represented by a w-bit unsigned number.\nsum could require w + 1 bits.\ntion x + y when x and y have 4-bit representations.\nIf we were to maintain the sum as a (w + 1)-bit number and add it to\nanother value, we may require w + 2 bits, and so on.\nw for arguments x and y, where 0 ≤x, y < 2w,\nas the result of truncating the integer sum x + y to be w bits long and then\nviewing the result as an unsigned number.\nbits with weight greater than 2w−1 in the bit-level representation of x + y.\nexample, consider a 4-bit number representation with x = 9 and y = 12, having\nn is unsigned, memcpy will treat it as a very large positive number and attempt to copy that many bytes\nFor x and y such that 0 ≤x, y < 2w:\nx +u\nw y =\n\u0006 x + y,\nx + y < 2w\nx + y −2w,\nsum x + y on the left mapping to the unsigned w-bit sum x +u\nnormal case preserves the value of x + y, while the overﬂow case has the effect of\nIn general, we can see that if x + y < 2w, the leading bit in the (w + 1)-bit represen-\nOn the other hand, if 2w ≤x + y < 2w+1, the leading bit in the (w + 1)-bit\nx + y\nWhen x + y\nis greater than 2w −1, the sum overﬂows.\nunsigned addition function for word size w = 4.\nWhen x + y < 16, there is no overﬂow, and x +u\nFor x and y in the range 0 ≤x, y ≤UMaxw, let s .= x +u\nw y.\nof s overﬂowed if and only if s < x (or equivalently, s < y).\nObserve that x + y ≥x, and hence if s did not overﬂow, we will surely have s ≥x.\nOn the other hand, if s did overﬂow, we have s = x + y −2w.\nint uadd_ok(unsigned x, unsigned y);\nThis function should return 1 if arguments x and y can be added without\nconsider the set of w-bit unsigned numbers with addition operation +u\nvalue x, there must be some value -u\nw x such that -u\nw x +u\nw x = 0.\nFor any number x such that 0 ≤x < 2w, its w-bit unsigned negation -u\nw x is given\nw x =\nx = 0\nx > 0\nFor x > 0, consider the value 2w −x.\nHence it is the inverse of x under +u\nWe can represent a bit pattern of length w = 4 with a single hex digit.\ntable giving the values and the bit representations (in hex) of the unsigned additive\n4 x\nvalues x and y in the range −2w−1 ≤x, y ≤2w−1 −1, their sum is in the range\n−2w ≤x + y ≤2w −2, potentially requiring w + 1 bits to represent exactly.\nw y to be the result of truncating the integer sum x + y to be w\nbits long and then viewing the result as a two’s-complement number.\nFor integer values x and y in the range −2w−1 ≤x, y ≤2w−1 −1:\nx +t\nw y =\nx + y −2w,\nx + y,\nx + y + 2w,\nx + y < −2w−1\nThis principle is illustrated in Figure 2.24, where the sum x + y is shown on the\nleft, having a value in the range −2w ≤x + y ≤2w −2, and the result of truncating\nthe sum to a w-bit two’s-complement number is shown on the right.\nof the principle.) When the sum x + y exceeds TMaxw (case 4), we say that positive\nWhen the sum x + y is less than TMinw (case 1), we say that negative\nThe w-bit two’s-complement sum of two numbers has the exact same bit-level\nrepresentation as the unsigned sum.\nSince two’s-complement addition has the exact same bit-level representation as\nWhen x + y is\nx + y\nx +t\nw y = U2Tw(T2Uw(x) +u\nx +t\nw y = U2Tw(T2Uw(x) +u\n= U2Tw[(xw−12w + x + yw−12w + y) mod 2w]\n= U2Tw[(x + y) mod 2w]\nTo better understand this quantity, let us deﬁne z as the integer sum z .= x + y,\nw y.\nnumbers x and y (that’s the only way we can have z < −2w−1) and obtained\na nonnegative result z′′ = x + y + 2w.\ncomplement sum z′′ equals the integer sum x + y.\nAgain, the two’s-complement sum z′′ equals the integer sum x + y.\nWe have added two positive numbers x and y (that’s the only way\nwe can have z ≥2w−1) and obtained a negative result z′′ = x + y −2w.\nx + y\nx +t\nthe 4-bit two’s-complement sum can be obtained by performing binary addition of the\nFigure 2.26 illustrates two’s-complement addition for word size w = 4.\nWhen x + y < −8, two’s-complement addition\nWhen x + y ≥8, the addition has a positive\ntation of s has had positive overﬂow if and only if x > 0 and y > 0 but s ≤0.\ncomputation has had negative overﬂow if and only if x < 0 and y < 0 but s ≥0.",
      "keywords": [
        "unsigned",
        "unsigned number",
        "number",
        "bit",
        "two’s-complement",
        "Unsigned Addition",
        "two’s-complement number",
        "representation",
        "unsigned representation",
        "sum",
        "numbers",
        "two’s-complement representation",
        "bits",
        "Addition",
        "signed"
      ],
      "concepts": [
        "unsigned",
        "bit",
        "bits",
        "numbers",
        "number",
        "values",
        "value",
        "signed",
        "sign",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 5,
          "title": "",
          "score": 0.724,
          "base_score": 0.574,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 7,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 10,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "unsigned",
          "complement",
          "2w",
          "bit",
          "representation"
        ],
        "semantic": [],
        "merged": [
          "unsigned",
          "complement",
          "2w",
          "bit",
          "representation"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3176074752275843,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712270+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 122-139)",
      "start_page": 122,
      "end_page": 139,
      "summary": "negative overﬂow when x + y < −8 and a positive overﬂow when x + y ≥8.\nIf both x > 0 and y > 0 but s ≤0,\nthe 5-bit arguments, the values of both their integer and two’s-complement sums,\nthe bit-level representation of the two’s-complement sum, and the case from the\nx + y\nx +t\nx + y\nx +t\nint tadd_ok(int x, int y);\nThis function should return 1 if arguments x and y can be added without\nint tadd_ok(int x, int y) {\nint sum = x+y;\nx and y, that will return 1 if computing x-y does not cause overﬂow.\nint tsub_ok(int x, int y) {\nreturn tadd_ok(x, -y);\nFor what values of x and y will this function give incorrect results?\nWe can see that every number x in the range TMinw ≤x ≤TMaxw has an additive\nw x as follows:\nFor x in the range TMinw ≤x ≤TMaxw, its two’s-complement negation -t\nw x is\nw x =\nx = TMinw\nx > TMinw\nThat is, for w-bit two’s-complement addition, TMinw is its own additive in-\nverse, while any other value x has −x as its additive inverse.\nFor values of x such\nthat x > TMinw, the value −x can also be represented as a w-bit two’s-complement\nnumber, and their sum will be −x + x = 0.\n4 x\nWhat do you observe about the bit patterns generated by two’s-complement\nBit-level representation of two’s-complement negation\nThere are several clever ways to determine the two’s-complement negation of a value represented\nOne technique for performing two’s-complement negation at the bit level is to complement the bits\nIn C, we can state that for any integer value x, computing the expressions\nis 0x5, and so 0xfffffffa is the two’s-complement representation of −6.\nA second way to perform two’s-complement negation of a number x is based on splitting the bit\nLet k be the position of the rightmost 1, so the bit-level representation of x has the\nIntegers x and y in the range 0 ≤x, y ≤2w −1 can be represented as w-bit un-\nsigned numbers, but their product x .\nThis could require as many as 2w bits to represent.\nsigned multiplication in C is deﬁned to yield the w-bit value given by the low-order\nLet us denote this value as x *u\nTruncating an unsigned number to w bits is equivalent to computing its value\nw y = (x .\nIntegers x and y in the range −2w−1 ≤x, y ≤2w−1 −1 can be represented as w-bit\ntwo’s-complement numbers, but their product x .\nmany as 2w bits to represent in two’s-complement form.\nWe denote this value as x *t\nTruncating a two’s-complement number to w bits\nunsigned to two’s complement, giving the following:\nx *t\nw y = U2Tw((x .\nfor both unsigned and two’s-complement multiplication, as stated by the following\nprinciple: Bit-level equivalence of unsigned and two’s-complement multipli-\nDeﬁne integers x and y as the values repre-\nsented by these bits in two’s-complement form: x = B2Tw(⃗x) and y = B2Tw(⃗y).\nDeﬁne nonnegative integers x′ and y′ as the values represented by these bits in\ntwo’s-complement multiplication, yielding 6-bit products, and then truncate these\nand two’s-complement multiplication, even though the full 6-bit representations\nx .\nThree-bit unsigned and two’s-complement multiplication examples.\nderivation: Bit-level equivalence of unsigned and two’s-complement multipli-\nFrom Equation 2.6, we have x′ = x + xw−12w and y′ = y + yw−12w.\n= [x .\n= (x .\nw y = U2Tw((x .\nw y) = T2Uw(U2Tw((x .\ny) mod 2w)) = (x .\nw y)) = T2Bw(x *t\nx .\nx .\nint tmult_ok(int x, int y) {\nint p = x*y;\n/* Either x is zero, or dividing p by x gives y */\nreturn !x || p/x == y;\nYou test this code for a number of values of x and y, and it seems to work\nw-bit numbers x (x ̸= 0), y, p, and q, where p is the result of performing two’s-\ncomplement multiplication on x and y, and q is the result of dividing p by x.\n1. Show that x .\ny, the integer product of x and y, can be written in the form\nx .\nLet x be the unsigned integer represented by bit pattern [xw−1, xw−2, .\n, x0].\nThen for any k ≥0, the w + k-bit unsigned representation of x2k is given by\n, x0, 0, .\nby k = 2 yields the 6-bit vector [101100], which encodes the unsigned number\n, x0, 0, .\nWhen shifting left by k for a ﬁxed word size, the high-order k bits are discarded,\n, x0, 0, .\ncan therefore see that shifting a value left is equivalent to performing unsigned\nFor C variables x and k with unsigned values x and k, such that 0 ≤k < w, the C\nexpression x << k yields the value x *u\nSince the bit-level operation of ﬁxed-size two’s-complement arithmetic is\nthe relationship between left shifts and multiplication by a power of 2 for two’s-\nprinciple: Two’s-complement multiplication by a power of 2\nFor C variables x and k with two’s-complement value x and unsigned value k, such\nthat 0 ≤k < w, the C expression x << k yields the value x *t\nwill yield the same result, regardless of whether x is unsigned or two’s comple-\n(x<<4) - (x<<1), requiring only two shifts and a subtraction.\nthe expression x * K, for some constant K.\n+ (x<<m)\nForm B: (x<<(n + 1)) - (x<<m)\nBy adding together the results for each run, we are able to compute x * K with-\nFor each of the following values of K, ﬁnd ways to express x * K using only the\nFor x ≥0 and y > 0, integer\ndivision should yield ⌊x/y⌋, while for x < 0 and y > 0, it should yield ⌈x/y⌉.\nbecause right shifting is guaranteed to be performed logically for unsigned values.\nFor C variables x and k with unsigned values x and k, such that 0 ≤k < w, the C\nexpression x >> k yields the value ⌊x/2k⌋.\nLet x be the unsigned integer represented by bit pattern [xw−1, xw−2, .\n, x0], and\nLet x′ be the unsigned number with w −k-bit\n, xk], and let x′′ be the unsigned number with k-bit\n, x0].\nPerforming a logical right shift of bit vector [xw−1, xw−2, .\n, x0] by k yields\nresult by computing the expression x >> k.\nThe case for dividing by a power of 2 with two’s-complement arithmetic is\nprinciple: Two’s-complement division by a power of 2, rounding down\nLet C variables x and k have two’s-complement value x and unsigned value\nThe C expression x >> k, when the shift is\nFor x ≥0, variable x has 0 as the most signiﬁcant bit, and so the effect of an\nnegative number, Figure 2.29 shows the effect of applying arithmetic right shift to\na 16-bit representation of −12,340 for different shift amounts.\nno rounding is required (k = 1), the result will be x/2k.\nour strategy to handle division for negative values of x.\nderivation: Two’s-complement division by a power of 2, rounding down\nLet x be the two’s-complement integer represented by bit pattern [xw−1, xw−2,\n. , x0], and let k be in the range 0 ≤k < w.\nnumber represented by the w −k bits [xw−1, xw−2, .\nunsigned number represented by the low-order k bits [xk−1, .\n, x0].\nanalysis as the unsigned case, we have x = 2kx′ + x′′ and 0 ≤x′′ < 2k, giving x′ =\n, x0] right\narithmetically by k yields the bit vector\nthis shifted bit vector is the two’s-complement representation of ⌊x/2k⌋.\nDividing two’s-complement numbers by powers of 2.\nprinciple: Two’s-complement division by a power of 2, rounding up\nLet C variables x and k have two’s-complement value x and unsigned value k,\nThe C expression (x + (1 << k) - 1) >> k, when\nthe arithmetic right shift causes the result to be correctly rounded.\nbits (those that will be shifted off to the right) shown in italics.\nrounding is required (k = 1), adding the bias only affects bits that are shifted off.\nThe biasing technique exploits the property that ⌈x/y⌉= ⌊(x + y −1)/y⌋for\nintegers x and y such that y > 0.\nAs examples, when x = −30 and y = 4, we have\nWhen x = −32 and y = 4, we have\nderivation: Two’s-complement division by a power of 2, rounding up\na bias of y −1 to x and then rounding the division downward, we will get q when\ny divides x and q + 1 otherwise.\nReturning to the case where y = 2k, the C expression x + (1 << k) - 1 yields\nthe value x + 2k −1.\n(x<0 ?\nwill compute the value x/2k.\nWrite a function div16 that returns the value x/16 for integer argument x.\nthat data type int is 32 bits long and uses a two’s-complement representation, and\nint arith(int x, int y) {\nresult = x*M + y/N; /* M and N are mystery numbers.\nint t = x;\nx <<= 5;\nx -= t;\nreturn x+y;\nway to represent both negative and positive values, while using the same bit-level\nvery similar bit-level behaviors, whether the operands are in unsigned or two’s-\nAssume data type int is 32 bits long and uses a two’s-complement representation\nRight shifts are performed arithmetically for signed values and\nunsigned ux = x;\nates to 1) for all values of x and y, or (2) give values of x and y for which it is false\nx+y == uy+ux\nA ﬂoating-point representation encodes rational numbers of the form V = x × 2y.",
      "keywords": [
        "Two’s-complement",
        "unsigned",
        "Practice Problem",
        "Problem",
        "Two’s-Complement Multiplication",
        "Multiplication",
        "bits",
        "bit",
        "number",
        "int",
        "Two’s-Complement Negation",
        "unsigned number",
        "numbers",
        "two’s-complement representation",
        "integer"
      ],
      "concepts": [
        "bit",
        "bits",
        "value",
        "number",
        "numbers",
        "code",
        "integers",
        "unsigned",
        "complement",
        "shifting"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 10,
          "title": "",
          "score": 0.965,
          "base_score": 0.815,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.749,
          "base_score": 0.599,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 5,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "complement",
          "unsigned",
          "bit",
          "xw",
          "int"
        ],
        "semantic": [],
        "merged": [
          "complement",
          "unsigned",
          "bit",
          "xw",
          "int"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3324274393227884,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712324+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 140-157)",
      "start_page": 140,
      "end_page": 157,
      "summary": "represent such values.\ncan only represent numbers that can be written x × 2y.\nOther values can only be\nValue\nFractional value\nprogram would multiply the value of this counter by a 24-bit quantity that was\nThe program approximated 0.1, as a value x, by\nconsidering just the ﬁrst 23 bits of the sequence to the right of the binary point:\nwould like to represent numbers in a form x × 2y by giving the values of x and y.\nThe IEEE ﬂoating-point standard represents a number in a form V = (−1)s ×\n(s = 0), where the interpretation of the sign bit for numeric value 0 is handled\n. The signiﬁcand M is a fractional binary number that ranges either between 1\n. The exponent E weights the value by a (possibly negative) power of 2.\nFloating-point numbers are represented\nThe bit representation of a ﬂoating-point number is divided into three ﬁelds to\nencode these values:\nthe value encoded also depends on whether or not the exponent ﬁeld equals\nIn the single-precision ﬂoating-point format (a float\nin C), ﬁelds s, exp, and frac are 1, k = 8, and n = 23 bits each, yielding a 32-\nbit representation.\nIn the double-precision ﬂoating-point format (a double in C),\nThe value encoded by a given bit representation can be divided into three\nCase 1: Normalized Values\nall zeros (numeric value 0) nor all ones (numeric value 255 for single precision,\nThat is, the exponent value is E = e −Bias, where\ne is the unsigned number having bit representation ek−1 .\nvalue equal to 2k−1 −1 (127 for single precision and 1023 for double).\nThe fraction ﬁeld frac is interpreted as representing the fractional value f ,\nWhy set the bias this way for denormalized values?\nsee shortly that it provides for smooth transition from denormalized to normalized values.\nCategories of single-precision ﬂoating-point values.\nThe value of the\nexponent determines whether the number is (1) normalized, (2) denormalized, or (3) a\nspecial value.\nbinary point to the left of the most signiﬁcant bit.\nCase 2: Denormalized Values\nWhen the exponent ﬁeld is all zeros, the represented number is in denormalized\nIn this case, the exponent value is E = 1 −Bias, and the signiﬁcand value is\nM = f , that is, the value of the fraction ﬁeld without an implied leading 1.\nrepresent numeric value 0, since with a normalized number we must always have\nIn fact, the ﬂoating-point representation\nof +0.0 has a bit pattern of all zeros: the sign bit is 0, the exponent ﬁeld is all\nzeros (indicating a denormalized value), and the fraction ﬁeld is all zeros, giving\nget the value −0.0.\nWith IEEE ﬂoating-point format, the values −0.0 and +0.0\nA ﬁnal category of values occurs when the exponent ﬁeld is all ones.\nfraction ﬁeld is all zeros, the resulting values represent inﬁnity, either +∞when\nﬁeld is nonzero, the resulting value is called a NaN, short for “not a number.” Such\nvalues are returned as the result of an operation where the result cannot be given\nFigure 2.34 shows the set of values that can be represented in a hypothetical 6-bit\nformat having k = 3 exponent bits and n = 2 fraction bits.\nPart (a) of the ﬁgure shows all representable values (other than NaN).\nFigure 2.35 shows some examples for a hypothetical 8-bit ﬂoating-point for-\n(b) Values between \u00021.0 and \u00031.0\nRepresentable values for 6-bit ﬂoating-point format.\nValue\nBit representation\nExample nonnegative values for 8-bit ﬂoating-point format.\nrepresented value V = 2E × M.\nDenormalized numbers in this format have E = 1 −7 = −6, giv-\nThe fractions f and signiﬁcands M range over the values\nThe smallest normalized numbers in this format also have E = 1 −7 = −6,\nand the fractions also range over the values 0, 1\nof E for denormalized values.\nAs we increase the exponent, we get successively larger normalized values,\nThus, the numeric value is V = 240.\nrepresentations of the values in Figure 2.35 as unsigned integers, they occur in\nascending order, as do the values they represent as ﬂoating-point numbers.\nno accident—the IEEE format was designed so that ﬂoating-point numbers could\nConsider a 5-bit ﬂoating-point representation based on the IEEE ﬂoating-point\nformat, with one sign bit, two exponent bits (k = 2), and two fraction bits (n = 2).\nﬂoating-point representation.\ne: The value represented by considering the exponent ﬁeld to be an unsigned\nE: The value of the exponent after biasing\nf : The value of the fraction\nM: The value of the signiﬁcand\n2E × M: The (unreduced) fractional value of the number\nV : The reduced fractional value of the number\nExpress the values of 2E, f , M, 2E × M, and V either as integers (when\nFigure 2.36 shows the representations and numeric values of some important\nsingle- and double-precision ﬂoating-point numbers.\nrepresentation with a k-bit exponent and an n-bit fraction:\n. The value +0.0 always has a bit representation of all zeros.\n. The smallest positive denormalized value has a bit representation consisting of\n(and signiﬁcand) value M = f = 2−n and an exponent value E = −2k−1 + 2.\n. The largest denormalized value has a bit representation consisting of an\n(and signiﬁcand) value M = f = 1 −2−n (which we have written 1 −ϵ) and\nan exponent value E = −2k−1 + 2.\nThe numeric value is therefore V = (1 −\nvalue.\n. The smallest positive normalized value has a bit representation with a 1 in\nthe least signiﬁcant bit of the exponent ﬁeld and otherwise all zeros.\nValue\nValue\nExamples of nonnegative ﬂoating-point numbers.\nsigniﬁcand value M = 1 and an exponent value E = −2k−1 + 2.\n. The value 1.0 has a bit representation with all but the most signiﬁcant bit of\nIts signiﬁcand value\nis M = 1 and its exponent value is E = 0.\n. The largest normalized value has a bit representation with a sign bit of 0, the\nhas a fraction value of f = 1 −2−n, giving a signiﬁcand M = 2 −2−n (which we\nhave written 2 −ϵ.) It has an exponent value E = 2k−1 −1, giving a numeric\nOne useful exercise for understanding ﬂoating-point representations is to con-\nvert sample integer values into ﬂoating-point form.\nﬂoating-point representation in binary of [01000110010000001110010000000000].\nlevel representations of the integer value 12345 (0x3039) and the single-precision\nﬂoating-point value 12345.0 (0x4640E400):\nthe ﬂoating-point representation.\ntation 0x00359141, while the single-precision ﬂoating-point number 3,510,593.0\ntation and explain the correlation between the bits of the integer and ﬂoating-point\nA. For a ﬂoating-point format with an n-bit fraction, give a formula for the\nB. What is the numeric value of this integer for single-precision format (n =\nThus, for a value x, we generally want\nOne key problem is to deﬁne the direction to round a value that is\ncould determine representable values x−and x+ such that the value x is guaran-\nvalues.\nThe only design decision is to determine the effect of rounding values\npositive numbers downward and negative numbers upward, giving a value ˆx such\npositive and negative numbers upward, giving a value x+ such that x ≤x+.\nWhy not consistently round values halfway\nbetween two representable values upward?\nis that one can easily imagine scenarios in which rounding a set of data values\nvalues.\nFor example, suppose we want to round decimal numbers to the nearest\nWe consider least signiﬁcant bit value 0 to be even and 1 to be odd.\ngeneral, the rounding mode is only signiﬁcant when we have a bit pattern of the\n. ., where X and Y denote arbitrary bit values with\nof this form denote values that are halfway between two possible results.\namples, consider the problem of rounding values to the nearest quarter (i.e., 2 bits\n4), because these values are not halfway\nbetween two possible values.\n2), since these values are halfway between two\nShow how the following binary fractional values would be rounded to the nearest\nhalf (1 bit to the right of the binary point), according to the round-to-even rule.\nIn each case, show the numeric values, both before and after rounding.\nB. What is the approximate decimal value of x′ −0.1?\nConsider the following two 7-bit ﬂoating-point representations based on the IEEE\nthem to the closest value in format B.\nIn addition, give the values of numbers given by the format A\nValue\nValue\nViewing ﬂoating-point values x\nThis operation is deﬁned for all values of x and y,\nThe operation is commutative, with x +f y = y +f x for all values of x and\nprecision ﬂoating point the expression (3.14+1e10)-1e10 evaluates to 0.0—the\nvalue 3.14 is lost due to rounding.\nunder ﬂoating-point addition, that is, x +f −x = 0.\nHowever, this computation might yield a different value for x than would the\nproperty: if a ≥b, then x +f a ≥x +f b for any values of a, b, and x other than NaN.\nFor example, with single-precision ﬂoating point, the expression\nFor example, with single-precision ﬂoating point, the expression 1e20*(1e20-\ntonicity properties for any values of a, b, and c other than NaN:\nAll versions of C provide two different ﬂoating-point data types: float and dou-\nto single- and double-precision ﬂoating point.\nchange the rounding mode or to get special values such as −0, +∞, −∞, or NaN.\nFill in the following macro deﬁnitions to generate the double-precision values +∞,\nWhen casting values between int, float, and double formats, the program\nchanges the numeric values and the bit representations as follows (assuming data\n. From int to float, the number cannot overﬂow, but it may be rounded.\n. From int or float to double, the exact numeric value can be preserved be-\ncause double has both greater range (i.e., the range of representable values),\nas well as greater precision (i.e., the number of signiﬁcant bits).\n. From double to float, the value can overﬂow to +∞or −∞, since the range\n. From float or double to int, the value will be rounded toward zero.\n00] (TMinw for word size w) as an integer indeﬁnite value.\ninteger approximation yields this value.\nTheir values are arbitrary, except that neither f nor d equals +∞, −∞, or NaN.\nDifferent encodings are used for representing integers, real numbers, and charac-\ntion and encode ﬂoating-point numbers using IEEE Standard 754.\noperate correctly over the full range of numeric values.\nfunctions T2Uw and U2Tw, for a w-bit value.\nFloating-point values can also underﬂow, when they are so close to 0.0 that they\nConverting large ﬂoating-point numbers to integers is a common source of programming errors.\nconversion of a 64-bit ﬂoating-point number to a 16-bit signed integer.\nwould never overﬂow a 16-bit number.\nbit pattern has numeric value 2k −1.\nFloating-point representations approximate real numbers by encoding num-\nﬂoating point also has representations for special values representing plus and\nTry running the code for show_bytes for different sample values.\nSuppose we number the bytes in a w-bit word from 0 (least signiﬁcant) to w/8 −1\nunsigned value in which byte i of argument x has been replaced by byte b:",
      "keywords": [
        "numbers",
        "bit",
        "bit representation",
        "number",
        "bits",
        "ﬂoating-point",
        "exponent",
        "IEEE ﬂoating-point format",
        "representation",
        "exponent bits",
        "exponent ﬁeld",
        "ﬂoating-point numbers",
        "fraction bits",
        "ﬂoating-point format",
        "Problem"
      ],
      "concepts": [
        "values",
        "value",
        "valued",
        "bit",
        "bits",
        "number",
        "numbers",
        "rounding",
        "round",
        "rounds"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "",
          "score": 0.752,
          "base_score": 0.602,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "",
          "score": 0.697,
          "base_score": 0.697,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 10,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 7,
          "title": "",
          "score": 0.637,
          "base_score": 0.487,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "value",
          "exponent",
          "values",
          "ﬂoating point",
          "ﬂoating"
        ],
        "semantic": [],
        "merged": [
          "value",
          "exponent",
          "values",
          "ﬂoating point",
          "ﬂoating"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3491816546478962,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712375+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 158-179)",
      "start_page": 158,
      "end_page": 179,
      "summary": "Data type int is w bits long.\nAll bit-level and logic operations.\nfrom integer argument x:\nint get_msb(int x) {\nint xright = x >> shift_val;\nAssume x is of type int.\nA. Any bit of x equals 1.\nB. Any bit of x equals 0.\nC. Any bit in the least signiﬁcant byte of x equals 1.\nD. Any bit in the most signiﬁcant byte of x equals 0.\nYour code should follow the bit-level integer coding rules (page 164), with the\nWrite a function int_shifts_are_arithmetic() that yields 1 when run on a\nmachine that uses arithmetic right shifts for data type int and yields 0 otherwise.\nFill in code for the following C functions.\nshift using an arithmetic right shift (given by value xsra), followed by other oper-\n8*sizeof(int) to determine w, the number of bits in data type int.\nunsigned srl(unsigned x, int k) {\nunsigned xsra = (int) x >> k;\nint sra(int x, int k) {\nint xsrl = (unsigned) x >> k;\nWrite code to implement the following function:\n/* Return 1 when any odd bit of x equals 1; 0 otherwise.\nint any_odd_one(unsigned x);\nYour function should follow the bit-level integer coding rules (page 164),\nexcept that you may assume that data type int has w = 32 bits.\nWrite code to implement the following function:\n/* Return 1 when x contains an odd number of 1s; 0 otherwise.\nint odd_ones(unsigned x);\nYour function should follow the bit-level integer coding rules (page 164),\nexcept that you may assume that data type int has w = 32 bits.\nWrite code to implement the following function:\n* If x = 0, then return 0.\nint leftmost_one(unsigned x);\nYour function should follow the bit-level integer coding rules (page 164),\nexcept that you may assume that data type int has w = 32 bits.\nHint: First transform x into a bit vector of the form [0 .\nwhen run on a machine for which an int is 32 bits, and yields 0 otherwise.\nB. Modify the code to run properly on any machine for which data type int is\nat least 32 bits.\nC. Modify the code to run properly on any machine for which data type int is\nat least 16 bits.\nYour function should follow the bit-level integer coding rules (page 164).\nunsigned rotate_left(unsigned x, int n);\nYour function should follow the bit-level integer coding rules (page 164).\n* Return 1 when x can be represented as an n-bit, 2’s-complement\nint fits_bits(int x, int n);\nYour function should follow the bit-level integer coding rules (page 164).\nto operate on a data structure where 4 signed bytes are packed into a 32-bit\na 32-bit int.\nAlthough its use is a bit\nsizeof operator returns a value of type size_t.\nint saturating_add(int x, int y);\nYour function should follow the bit-level integer coding rules (page 164).\nint tsub_ok(int x, int y);\nThis function should return 1 if the computation x-y does not overﬂow.\nSuppose we want to compute the complete 2w-bit representation of x .\nboth x and y are unsigned, on a machine for which data type unsigned is w bits.\nThe low-order w bits of the product can be computed with the expression x*y, so\nthat computes the high-order w bits of x .\nint signed_high_prod(int x, int y);\nthat computes the high-order w bits of x .\nof the number of bits used to represent data of type size_t.\nSuppose we are given the task of generating code to multiply integer variable x\nFor the following values of K, write C expressions to\nint divide_power2(int x, int k);\nThe function should compute x/2k with correct rounding, and it should follow\nthe bit-level integer coding rules (page 164).\nWrite code for a function mul3div4 that, for integer argument x, computes 3 ∗\nx/4 but follows the bit-level integer coding rules (page 164).\nWrite code for a function threefourths that, for integer argument x, computes\nfollow the bit-level integer coding rules (page 164).\nWrite C expressions to generate the bit patterns that follow, where ak represents\nAssume a w-bit data type.\nWe are running programs where values of type int are 32 bits.\ntype unsigned are also 32 bits.\nWe generate arbitrary values x and y, and convert them to unsigned values as\nint x = random();\nC. ~x+~y+1 == ~(x+y)\nD. (ux-uy) == -(unsigned)(y-x)\n, where y is a k-bit sequence.\nB. What is the numeric value of the string for the following values of y?\nunsigned 32-bit number having the same bit representation as its ﬂoating-point\nint float_le(float x, float y) {\n/* Get the sign bits */\nGiven a ﬂoating-point format with a k-bit exponent and an n-bit fraction, write\nIn addition, describe the bit representation.\nformat with an 80-bit word divided into a sign bit, k = 15 exponent bits, a single\nimplied bit in the IEEE ﬂoating-point representation.\nwhere x is an integer and y is an integral power of 2.\nConsider the following two 9-bit ﬂoating-point representations based on the IEEE\nThere is 1 sign bit.\nThere is 1 sign bit.\nBits\nBits\nWe are running programs on a machine where values of type int have a 32-\nbit two’s-complement representation.\nValues of type float use the 32-bit IEEE\nformat, and values of type double use the 64-bit IEEE format.\nWe generate arbitrary integer values x, y, and z, and convert them to values\nint x = random();\nB. dx - dy == (double) (x-y)\npoint representation of 2x.\nWhen x is too\nfunction u2f returns a ﬂoating-point value having an identical bit representation\nfloat fpwr2(int x)\nif (x <\n} else if (x <\n} else if (x <\nA. What is the fractional binary number denoted by this ﬂoating-point value?\nC. At what bit position (relative to the binary point) do these two approxima-\nBit-Level Floating-Point Coding Rules\nIn the following problems, you will write code to implement ﬂoating-point func-\ntions, operating directly on bit-level representations of ﬂoating-point numbers.\nTo this end, we deﬁne data type float_bits to be equivalent to unsigned:\n/* Access bit-level representation floating-point number */\ntypedef unsigned float_bits;\nRather than using data type float in your code, you will use float_bits.\nYou may use both int and unsigned data types, including unsigned and integer\nInstead, your code should perform the bit manipulations that implement\nThe following function illustrates the use of these coding rules.\nfloat_bits float_denorm_zero(float_bits f) {\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\nFor ﬂoating-point number f , this function computes −f .\nTest your function by evaluating it for all 232 values of argument f and com-\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\nFor ﬂoating-point number f , this function computes |f |.\nTest your function by evaluating it for all 232 values of argument f and com-\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\nFor ﬂoating-point number f , this function computes 2.0 .\nTest your function by evaluating it for all 232 values of argument f and com-\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\nFor ﬂoating-point number f , this function computes 0.5 .\nTest your function by evaluating it for all 232 values of argument f and com-\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\n* If conversion causes overflow or f is NaN, return 0x80000000\nint float_f2i(float_bits f);\nFor ﬂoating-point number f , this function computes (int) f .\nof range, or it is NaN), then the function should return 0x80000000.\nTest your function by evaluating it for all 232 values of argument f and com-\nFollowing the bit-level ﬂoating-point coding rules, implement the function with\nfloat_bits float_i2f(int i);\nFor argument i, this function computes the bit-level representation of\nTest your function by evaluating it for all 232 values of argument f and com-\n0x20\n0x800000\n0x8000\n0x2000\n0x1000\n0x40\n0x100\n0x00\n0x91\n0x75\nC. We ﬁnd all bits of the integer embedded in the ﬂoating-point number, except\nfor the most signiﬁcant bit having value 0.\nB. We perform Boolean operations based on a bit-vector representation of the\n*x\nB. In this case, arguments x and y to inplace_swap both point to the same\nWhen we compute *x ^ *y, we get 0.\nB. x ^ ~0xFF\nC. x | 0xFF\nbit operations.\nwork when data type int is 32 bits.\n/* Compute x|y using only calls to functions bis and bic */\nint bool_or(int x, int y) {\nint result = bis(x,y);\n/* Compute x^y using only calls to functions bis and bic */\nint bool_xor(int x, int y) {\nint result = bis(bic(x,y), bic(y,x));\nbit is set in x or it is set in m.\nwe want the result to equal 1 only when the corresponding bit of x is 1 and of m is\nThis problem highlights the relation between bit-level Boolean operations and\nA common programming error is to use a bit-level oper-\n0x44\n0x01\n0x57\n0x01\n0x00\n0x00\n0x01\nThe expression is !(x ^ y).\nThat is, x^y will be zero if and only if every bit of x matches the corresponding\nbit of y.\nThere is no real reason to use this expression rather than simply writing x ==\ny, but it demonstrates some of the nuances of bit-level and logical operations.\n0x50\n0x64\n0x90\n0x72\n0x44\n0x10\n0x08\ncomplement values, hex digits 0 through 7 have a most signiﬁcant bit of 0, yielding\nnonnegative values, while hex digits 8 through F have a most signiﬁcant bit of 1,\n0x1\n0x2\n0x7\nFor a 32-bit word, any value consisting of 8 hexadecimal digits beginning with one\nFor example, the number 0x8048337\n0x1\n0x8\nvalues from a word in which multiple bit ﬁelds have been packed.\n0x00000076\n0x00000076\n0x00000076\n0x87654321\n0x00000021\n0x00000021\n0x00000087\nB. Function fun1 extracts a value from the low-order 8 bits of the argument,",
      "keywords": [
        "int",
        "function",
        "problem",
        "bits",
        "Data type int",
        "unsigned",
        "Solution to Problem",
        "bit",
        "type int",
        "code",
        "integer coding rules",
        "ﬂoating-point",
        "bit-level integer coding",
        "coding rules",
        "float"
      ],
      "concepts": [
        "bit",
        "unsigned",
        "value",
        "values",
        "returns",
        "function",
        "functions",
        "operations",
        "operators",
        "operator"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 10,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.764,
          "base_score": 0.614,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 7,
          "title": "",
          "score": 0.749,
          "base_score": 0.599,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "",
          "score": 0.733,
          "base_score": 0.583,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 5,
          "title": "",
          "score": 0.696,
          "base_score": 0.546,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "int",
          "bit",
          "coding rules",
          "coding",
          "rules"
        ],
        "semantic": [],
        "merged": [
          "int",
          "bit",
          "coding rules",
          "coding",
          "rules"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3945406264702524,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712437+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 180-198)",
      "start_page": 180,
      "end_page": 198,
      "summary": "int uadd_ok(unsigned x, unsigned y) {\nunsigned sum = x+y;\nreturn sum >= x;\nvalues of x, we must have (-u\n4 x) + x = 16.\n4 x\nx + y\nx +t\nint tadd_ok(int x, int y) {\nint sum = x+y;\nint neg_over = x <\nint pos_over = x >= 0 && y >= 0 && sum <\ncomplement addition forms an abelian group, and so the expression (x+y)-x\n(x+y)-y will always evaluate to x.\noverﬂow when x is negative and no overﬂow when x is nonnegative.\nopposite is true: tsub_ok(x, TMin) should yield 0 when x is negative and 1 when\n4 x\nThe bit patterns are the same as for unsigned negation.\nx .\nTruncated x .\nIt is not realistic to test this function for all possible values of x and y.\ncombinations when data type int is 32 bits.\nyour code by writing the function with data type short or char and then testing\n1. We know that x .\ny can be written as a 2w-bit two’s-complement number.\nu denote the unsigned number represented by the lower w bits, and v denote\nthe two’s-complement number represented by the upper w bits.\non Equation 2.3, we can see that x .\nLetting t = v + pw−1, we have x .\nWhen t = 0, we have x .\nt ̸= 0, we have x .\n2. By deﬁnition of integer division, dividing p by nonzero x gives a quotient\nq and a remainder r such that p = x .\nq + r, and |r| < |x|.\nvalues here, because the signs of x and r may differ.\nThen we have x .\ny = x .\nThen we will have x .\ny = x .\nWhen x equals 0, multiplication does not overﬂow, and so we see that our code\nWith 64 bits, we can perform the multiplication without overﬂowing.\nwhether casting the product to 32 bits changes the value:\nint tmult_ok(int x, int y) {\nint64_t pll = (int64_t) x*y;\nint64_t pll = x*y;\nthe product would be computed as a 32-bit value (possibly overﬂowing) and then\nB. With malloc having a 32-bit unsigned number as its argument, it cannot\nThe expression simply becomes -(x<<m).\nForm B states that we should compute (x<<w) - (x<<m), but shifting\nx to the left by w will yield the value 0.\n(x<<3) - x\n(x<<5) - (x<<2)\nn = m, form A requires only a single shift, while form B requires two shifts\nWhen n > m + 1, form B requires only two shifts and one\nWe use the trick that the expression x >> 31 generates a word with all\nones if x is negative, and all zeros otherwise.\nint div16(int x) {\n/* Compute bias to be either 0 (x >= 0) or 15 (x < 0) */\nint bias = (x >> 31) & 0xF;\nreturn (x + bias) >> 4;\nWe can see that M is 31; x*M is computed as (x<<5)-x.\nWe can see that N is 8; a bias value of 7 is added when y is negative, and the\nA. (x > 0) || (x-1 < 0)\nLet x be −2,147,483,648 (TMin32).\nWe will then have x-1 equal to\nB. (x & 7) != 7 || (x<<29 < 0)\nIf (x & 7) != 7 evaluates to 0, then we must have bit x2 equal to 1.\nC. (x * x) >= 0\nD. x < 0 || -x <= 0\nE. x > 0 || -x >= 0\nLet x be −2,147,483,648 (TMin32).\nThen both x and -x are negative.\nx+y == uy+ux\nTwo’s-complement and unsigned addition have the same bit-level be-\nG. x*~y + uy*ux == -x\nuy*ux equals x*y.\nto x*-y-x+x*y.\nsent a number as a fraction of the form x\nbinary representation of x, with the binary point inserted k positions from the\nBits\nHexadecimal 0x359141 is equivalent to binary [1101011001000101000001].\n10, we see that the 2 bits to the\nThis problem tests a lot of concepts about ﬂoating-point representations, including\nBits\nBits\nThis code seems to work on a variety of machines, however.\nA. x == (int)(double) x\nB. x == (int)(float) x\nNo. For example, when x is TMax. C.\nYes, since a ﬂoating-point number is negated by simply inverting its sign bit.\nMachine-Level Representation\nCombining Control and Data in Machine-Level Programs\nFloating-Point Code\nomputers execute machine code, sequences of bytes encoding the low-level\nA compiler generates machine\ncode through a series of stages, based on the rules of the programming language,\nThe gcc C compiler generates its output in the form of assembly\ncode, a textual representation of the machine code giving the individual instruc-\nthe executable machine code from the assembly code.\na close look at machine code and its human-readable representation as assem-\nWhen programming in a high-level language such as C, and even more so\nin Java, we are shielded from the detailed machine-level implementation of our\nIn contrast, when writing programs in assembly code (as was done in the\nthe program uses to carry out a computation.\nWith modern optimizing compilers, the generated code is usually at least as\nBest of all, a program written in a high-level language can be compiled and\nexecuted on a number of different machines, whereas assembly code is highly\nSo why should we spend our time learning machine code?\npilers do most of the work in generating assembly code, being able to read and\nshowing its output in assembly-code form.\nvariations of the source code, each time compiling and examining the generated\nassembly code to get a sense of how efﬁciently the program will run.\ncovered in Chapter 12, it is important to understand how program data are shared\nSuch information is visible at the machine-code level.\nthe machine-level representation of programs.\nmachine code has shifted over the years from one of being able to write programs\ndirectly in assembly code to one of being able to read and understand the code\nand see how C programs get compiled into this form of machine code.\nthe assembly code generated by a compiler involves a different set of skills than\nwriting assembly code by hand.\ncompilers make in converting the constructs of C into machine code.\nthe computations expressed in the C code, optimizing compilers can rearrange\nstanding the relation between source code and the generated assembly can often\nIn this case, the system is a machine-generated assembly-\nlar patterns and we can run experiments, having the compiler generate code for\nOur presentation is based on x86-64, the machine language for most of the\nover a long history, starting with Intel Corporation’s ﬁrst 16-bit processor in 1978,\narcane features of x86-64.\ntween C, assembly code, and machine code.\nx86-64, starting with the representation and manipulation of data and the imple-\nIA32, the 32-bit predecessor to x86-64, was introduced by Intel in 1985.\nMost x86 microprocessors sold today, and most operating\nsystems installed on these machines, are designed to run x86-64.\nIn addition, many existing systems cannot execute x86-64, due to limitations of their hardware\nbackground in x86-64 will enable you to learn the IA32 machine language quite readily.\nare implemented at the machine level.\nbehavior of a machine-level program.\non machine-program representations of code involving ﬂoating-point data and\nbit machines.\nA 32-bit machine can only make use of around 4 gigabytes (232\nCurrent 64-bit machines can use up to 256 terabytes (248 bytes) of memory, and\n4 gigabytes seemed like an extreme amount of memory when 32-bit machines\nOur presentation focuses on the types of machine-level programs generated\nwhen compiling C and similar programming languages targeting modern oper-\nfeatures of x86-64 that arise out of its legacy support for the styles of programs\naddresses allowed by 16-bit machines.\nThe Intel processor line, colloquially referred to as x86, has followed a long evo-\nkey features, especially those affecting machine-level programming.\nnumber of transistors required to implement the processors as an indication of\nfor the x86 line, often referred to as “x87.”\ninstructions, rather than x87 instructions, to compile ﬂoating-point code.\nPentium 4E (2004, 125 M transistors).\nIntel’s implementation of a 64-bit extension to IA32 developed by Ad-\nvanced Micro Devices (AMD), which we refer to as x86-64.\nable to run code compiled for any earlier version.\nto as x86-64.\npatible with Intel processors, capable of running the exact same machine-level\nand introducing x86-64, the widely adopted 64-bit extension to Intel’s IA32.\nThe original x87 ﬂoating-point instructions became obsolete\nIf we plot the number of transistors in the different Intel processors versus the year of introduction, and\ntion of x86 in x86-64 programs, many of the most arcane features of x86 do not\nSuppose we write a C program as two ﬁles p1.c and p2.c. We can then compile\ninstructs the compiler to apply a level of optimization that yields machine code\nthat follows the overall structure of the original C code.\nbetween the generated machine code and the original source code is difﬁcult to\nassembler converts the assembly code into binary object-code ﬁles p1.o and p2.o. Object code is one form of machine code—it contains binary representations of all\nfunctions (e.g., printf) and generates the ﬁnal executable code ﬁle p (as speciﬁed\nExecutable code is the second form of\nmachine code we will consider—it is the exact form of code that is executed by\nThe relation between these different forms of machine code and\nMachine-Level Code\nTwo of these are especially important for machine-level program-\nFirst, the format and behavior of a machine-level program is deﬁned by the\nISAs, including x86-64, describe the behavior of a program as if each instruction is\nby a machine-level program are virtual addresses, providing a memory model that\nthe command-line ﬂag -O1) is probably the best choice for generating code that follows the original",
      "keywords": [
        "Solution to Problem",
        "code",
        "Problem",
        "machine code",
        "assembly code",
        "Solution",
        "Unsigned",
        "machine",
        "Intel",
        "Program",
        "Programs",
        "bits",
        "transistors",
        "Form",
        "data"
      ],
      "concepts": [
        "bit",
        "bits",
        "programs",
        "program",
        "programming",
        "code",
        "operation",
        "operations",
        "operating",
        "operate"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 7,
          "title": "",
          "score": 0.965,
          "base_score": 0.815,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.78,
          "base_score": 0.63,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 8,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 6,
          "title": "",
          "score": 0.629,
          "base_score": 0.479,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 5,
          "title": "",
          "score": 0.623,
          "base_score": 0.473,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "machine",
          "machine code",
          "assembly",
          "x86"
        ],
        "semantic": [],
        "merged": [
          "code",
          "machine",
          "machine code",
          "assembly",
          "x86"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35203736313026635,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712490+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 199-218)",
      "start_page": 199,
      "end_page": 218,
      "summary": "vided by C into the very elementary instructions that the processor executes.\nassembly-code representation is very close to machine code.\nmachine code.\noriginal C code is a key step in understanding how computers execute programs.\nThe machine code for x86-64 differs greatly from the original C code.\n64) indicates the address in memory of the next instruction to be executed.\n. The integer register ﬁle contains 16 named locations storing 64-bit values.\nThese registers can hold addresses (corresponding to C pointers) or integer\n. The condition code registers hold status information about the most recently\nWhereas C provides a model in which objects of different data types can be\ndeclared and allocated in memory, machine code views the memory as simply\nstructures are represented in machine code as contiguous collections of bytes.\nEven for scalar data types, assembly code makes no distinctions between signed or\nThe program memory contains the executable machine code for the program,\nupper 16 bits must be set to zero, and so an address can potentially specify a byte\nThe ever-changing forms of generated code\nusing a different compiler or a different version of gcc and hence will generate different code.\nassembly code and map it back to the constructs found in high-level programming languages.\nA single machine instruction performs only a very elementary operation.\nexample, it might add two numbers stored in registers, transfer data between\nmemory and a register, or conditionally branch to a new instruction address.\ncompiler must generate sequences of such instructions to implement program\nCode Examples\nSuppose we write a C code ﬁle mstore.c containing the following function deﬁ-\nTo see the assembly code generated by the C compiler, we can use the -S\nThe assembly-code ﬁle contains various declarations, including the following\nTo display the binary object code for a program (say, mstore), we use a disassembler (described below)\nto determine that the code for the procedure is 14 bytes long.\ntelling it to display (abbreviated ‘x’) 14 hex-formatted (also ‘x’) bytes (‘b’) starting at the address where\nEach indented line in the code corresponds to a single machine instruction.\nexample, the pushq instruction indicates that the contents of register %rbx should\nIf we use the -c command-line option, gcc will both compile and assemble\nthe code\nThis will generate an object-code ﬁle mstore.o that is in binary format and hence\nThis is the object code corresponding to the assembly instructions listed previously.\nsimply a sequence of bytes encoding a series of instructions.\nvery little information about the source code from which these instructions were\nTo inspect the contents of machine-code ﬁles, a class of programs known as\nassembly code from the machine code.\nis a single instruction, with the assembly-language equivalent shown on the right.\n. x86-64 instructions can range in length from 1 to 15 bytes.\nThe instruction\n. The instruction format is designed in such a way that from a given starting\nposition, there is a unique decoding of the bytes into machine instructions.\nFor example, only the instruction pushq %rbx can start with byte value 53.\n. The disassembler determines the assembly code based purely on the byte\nsequences in the machine-code ﬁle.\nassembly-code versions of the program.\ntions than does the assembly code generated by gcc.\nGenerating the actual executable code requires running a linker on the set\nThis code is almost identical to that generated by the disassembly of mstore.c. One important difference is that the addresses listed along the left are different—\nthe linker has shifted the location of this code to a different range of addresses.\nfor the linker is to match function calls with the locations of the executable code for\nThese instructions will have no effect on the program, since they occur after\nthe return instruction (line 7).\nfunction to 16 bytes, enabling a better placement of the next block of code in terms\nThe assembly code generated by gcc is difﬁcult for a human to read.\nremarks about what the instructions do or how they relate to the source code.\nTo provide a clearer presentation of assembly code, we will show it in a form\ntions of the original C code.\na brief presentation of ways to incorporate assembly code into C programs.\nassembly code and combine them with C functions during the linking stage.\nATT versus Intel assembly-code formats\nIn our presentation, we show assembly code in ATT format (named after AT&T, the company that\nfrom Intel, show assembly code in Intel format.\nexample, gcc can generate code in Intel format for the sum function using the following command line:\nThis gives the following assembly code:\n. The Intel code omits the ‘%’ character in front of register names, using rbx instead of %rbx.\n. The Intel code has a different way of describing locations in memory—for example, QWORD PTR\nsecond is to use gcc’s support for embedding assembly code directly within C\nshows the x86-64 representations used for the primitive data types of C.\nint values are stored as double words (32 bits).\nare stored as 8-byte quad words, as would be expected in a 64-bit machine.\nx86-64, data type long is implemented with 64 bits, allowing a very wide range\nMost of our code examples in this chapter use pointers and long data\nCombining assembly code with C programs\nmachine code, there are some features of a machine that cannot be accessed by a C program.\nexample, every time an x86-64 processor executes an arithmetic or logical operation, it sets a 1-bit\nprogram to determine the value of the PF condition code ﬂag.\nincorporating a small number of assembly-code instructions into the program.\nThere are two ways to incorporate assembly code into C programs.\nfunction as a separate assembly-code ﬁle and let the assembler and linker combine this with code we\ncode can be incorporated into a C program using the asm directive.\nOf course, including assembly code in a C program makes the code speciﬁc to a particular class of\nAssembly-code sufﬁx\nSizes of C data types in x86-64.\nWith a 64-bit machine, pointers are 8 bytes\nThe x86-64 instruction set includes\na full complement of instructions for bytes, words, and double words as well.\nbyte) values, corresponding to C data type float, and double-precision (8-byte)\nvalues, corresponding to C data type double.\nAs the table of Figure 3.1 indicates, most assembly-code instructions gener-\nexample, the data movement instruction has four variants: movb (move byte),\n“long words.” The assembly code uses the sufﬁx ‘l’ to denote a 4-byte integer as\nity, since ﬂoating-point code involves an entirely different set of instructions and\nregisters.\nregisters storing 64-bit values.\nThese registers are used to store integer data as well\nAs the nested boxes in Figure 3.2 indicate, instructions can operate on data\nof different sizes stored in the low-order bytes of the 16 registers.\nand 64-bit operations can access entire registers.\ngenerating 1-, 2-, 4-, and 8-byte values.\nWhen these instructions have registers as\nthe register for instructions that generate less than 8 bytes: Those that generate 1-\nbyte quantities set the upper 4 bytes of the register to zero.\nregisters serve different roles in typical programs.\ninstructions speciﬁcally read and write this register.\nThe other 15 registers have\ngoverns how the registers are to be used for managing the stack, passing function\n%rax\narguments, returning values from functions, and storing local and temporary data.\nMost instructions have one or more operands specifying the source values to use\nRegister\nRegister\nOperands can denote immediate (constant) values, register\nin either registers or memory.\nformat assembly code, these are written with a ‘$’ followed by an integer using\nDifferent instructions allow\ncontents of a register, one of the sixteen 8-, 4-, 2-, or 1-byte low-order portions of\nthe registers for operands having 64, 32, 16, or 8 bits, respectively.\nwe use the notation ra to denote an arbitrary register a and indicate its value with\nthe reference R[ra], viewing the set of registers as an array R indexed by register\nMb[Addr] to denote a reference to the b-byte value stored in memory starting at\nimmediate offset Imm, a base register rb, an index register ri, and a scale factor\nAssume the following values are stored at the indicated memory addresses and\nregisters:\nRegister\n%rax\n%rax\n(%rax)\n4(%rax)\nData Movement Instructions\nAmong the most heavily used instructions are those that copy data from one lo-\nmovement instruction to express a range of possibilities that in many machines\nwould require a number of different instructions.\nent data movement instructions, differing in their source and destination types,\nthe instructions in a class perform the same operation but with different operand\nThese instructions copy data from a source location to a destination location,\nAll four of these instructions have similar effects; they differ\nprimarily in that they operate on data of different sizes: 1, 2, 4, and 8 bytes,\nInstruction\nSimple data movement instructions.\nThe source operand designates a value that is immediate, stored in a register,\nregister or a memory address.\nsource value into a register, and the second to write this register value to the des-\nReferring to Figure 3.2, register operands for these instructions can be\nFor most cases, the mov instructions will only update the speciﬁc register bytes\nthat when movl has a register as the destination, it will also set the high-order 4\nbytes of the register to 0.\nx86-64, that any instruction that generates a 32-bit value for a register also sets the\nThe following mov instruction examples show the ﬁve possible combinations\nImmediate--Register, 4 bytes\nMemory--Register,\nRegister--Memory,\nA ﬁnal instruction documented in Figure 3.4 is for dealing with 64-bit imme-\nThe regular movq instruction can only have immediate source operands\ncan only have a register as a destination.\nFigures 3.5 and 3.6 document two classes of data movement instructions for\ninstructions copy data from a source, which can be either a register or stored\nUnderstanding how data movement changes a destination register\ntions modify the upper bytes of a destination register.\nmovabsq $0x0011223344556677, %rax\n%rax = 0011223344556677\n$-1, %rax\nIn the example, the instruction on line 1\ninitializes register %rax to the pattern 0011223344556677.\nThe movb instruction (line 2)\ntherefore sets the low-order byte of %rax to FF, while the movw instruction (line 3) sets the low-order\nThe movl instruction (line 4) sets the low-order\nInstruction\nMove zero-extended byte to double word\nZero-extending data movement instructions.\nThese instructions have a\nregister or memory location as the source and a register as the destination.\nin memory, to a register destination.\nclasses, covering all cases of 1- and 2-byte source sizes and 2- and 4-byte destination\nInstruction\nMove sign-extended byte to double word\nMove sign-extended byte to quad word\nSign-extending data movement instructions.\nThe movs instructions have\na register or memory location as the source and a register as the destination.\ninstruction is speciﬁc to registers %eax and %rax.\nNote the absence of an explicit instruction to zero-extend a 4-byte source\nvalue to an 8-byte destination in Figure 3.5.\nmovement can be implemented using a movl instruction having a register as the\ngenerating a 4-byte value with a register as the destination will ﬁll the upper 4\nFigure 3.6 also documents the cltq instruction.\nThis instruction has no\noperands—it always uses register %eax as its source and %rax as the destination for\ninstruction sufﬁx based on the operands.\n$0xFF, %bl\nComparing byte movement instructions\nThe following example illustrates how different data movement instructions either do or do not change\nObserve that the three byte-movement instructions movb,\nmovabsq $0x0011223344556677, %rax\n%rax = 0011223344556677\nof the code initialize registers %rax and %dl to 0011223344556677 and AA, respectively.\ninstructions all copy the low-order byte of %rdx to the low-order byte of %rax.\nThe movb instruction\nThe movsbq instruction (line 4) sets the other 7 bytes to\nrepresents binary value 1010, sign extension causes the higher-order bytes to each be set to FF.\nmovzbq instruction (line 5) always sets the other 7 bytes to zero.\nEach of the following lines of code generates an error message when we invoke\nmovq %rax,$0x123\nAs an example of code that uses data movement instructions, consider the data\nexchange routine shown in Figure 3.7, both as C code and as assembly code\ninstructions: two data movements (movq) plus an instruction to return back to\nOur annotated assembly code documents\nA function returns a value by storing it in register %rax, or in one of the\n(a) C code\n(b) Assembly code\nC and assembly code for exchange routine.\nRegisters %rdi and %rsi\nstored in registers %rdi and %rsi, respectively.\nInstruction 2 then reads x from\nmemory and stores the value in register %rax, a direct implementation of the\noperation x = *xp in the C program.\nLater, register %rax will be used to return\na value from the function, and so the return value will be x.\nInstruction 3 writes y\nto the memory location designated by xp in register %rdi, a direct implementation\nThis example illustrates how the mov instructions can be\nused to read from memory to a register (line 2), and to write from a register to\nTwo features about this assembly code are worth noting.\ncopying that pointer into a register, and then using this register in a memory\nthe appropriate pair of data movement instructions to implement the operation\nFunction exchange (Figure 3.7(a)) provides a good illustration of the use of pointers in C.\nAssume that the values of sp and dp are stored in registers %rdi and %rsi,\nregister %rax.\nof %rax to memory.\nInstruction\nis compiled into assembly code, yielding the following:\nParameters xp, yp, and zp are stored in registers %rdi, %rsi, and %rdx, respec-\nWrite C code for decode1 that will have an effect equivalent to the assembly\nInstruction\n%rax\n%rax\n%rax\nlower addresses, so pushing involves decrementing the stack pointer (register %rsp) and\nThe pushq instruction provides the ability to push data onto the stack, while\nPushing a quad word value onto the stack involves ﬁrst decrementing the\nstack pointer by 8 and then writing the value at the new top-of-stack address.",
      "keywords": [
        "code",
        "assembly code",
        "instructions",
        "register",
        "Data Movement Instructions",
        "rax",
        "data",
        "registers",
        "memory",
        "bytes",
        "long",
        "machine code",
        "data movement",
        "byte",
        "Movement Instructions"
      ],
      "concepts": [
        "instructions",
        "instruction",
        "code",
        "register",
        "registers",
        "byte",
        "bytes",
        "data",
        "different",
        "difference"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.905,
          "base_score": 0.755,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "",
          "score": 0.896,
          "base_score": 0.746,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.699,
          "base_score": 0.699,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.677,
          "base_score": 0.677,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "register",
          "assembly",
          "assembly code",
          "instructions"
        ],
        "semantic": [],
        "merged": [
          "code",
          "register",
          "assembly",
          "assembly code",
          "instructions"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4733066982155535,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712552+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 219-238)",
      "start_page": 219,
      "end_page": 238,
      "summary": "pair of instructions\nexcept that the pushq instruction is encoded in the machine code as a single byte,\nwhereas the pair of instructions shown above requires a total of 8 bytes.\ntwo columns in Figure 3.9 illustrate the effect of executing the instruction pushq\nTherefore, the instruction popq %rax\nis equivalent to the following pair of instructions:\nThe third column of Figure 3.9 illustrates the effect of executing the instruction\nRegister %rsp is incremented back to 0x108.\ntopmost element of the stack is a quad word, the instruction movq 8(%rsp),%rdx\nFigure 3.10 lists some of the x86-64 integer and logic operations.\noperations are given as instruction classes, as they can have different variants with\neach of the instruction classes shown has instructions for operating on these four\nThe load effective address instruction leaq is actually a variant of the movq in-\nIt has the form of an instruction that reads from memory to a register,\nInstruction\nS, D\nS, D\nS, D\nS, D\nS, D\nS, D\nS, D\ninstruction is commonly used to perform simple arithmetic.\noperands with ATT-format assembly code.\nory reference, but instead of reading from the designated location, the instruction\nFigure 3.10 using the C address operator &S.\nThis instruction can be used to gener-\nvalue x, then the instruction leaq 7(%rdx,%rdx,4), %rax will set register %rax\nwith formulas indicating the value that will be stored in register %rax for each of\nthe given assembly-code instructions:\nInstruction\nAs an illustration of the use of leaq in compiled code, consider the following\nlong t = x + 4 * y + 12 * z;\nx in %rdi, y in %rsi, z in %rdx\nThe ability of the leaq instruction to perform addition and limited forms of\nx in %rdi, y in %rsi, z in %rdx\nFor example, the instruction incq (%rsp) causes the 8-byte\nassignment operators, such as x -= y.\nFor example, the instruction subq %rax,%rdx decrements register\n%rdx by the value in %rax.\n(It helps to read the instruction as “Subtract %rax from\n%rdx.”) The ﬁrst operand can be either an immediate value, a register, or a memory\nmov instructions, the two operands cannot both be memory locations.\nFill in the following table showing the effects of the following instructions,\nInstruction\nThe different shift instructions can specify the shift amount either as\n(These instructions are\nWith x86-64, a shift instruction operating on data values that are\nregister %cl has hexadecimal value 0xFF, then instruction salb would shift by\nAs Figure 3.10 indicates, there are two names for the left shift instruction: sal\nshift instructions differ in that sar performs an arithmetic shift (ﬁll with copies of\noperand of a shift operation can be either a register or a memory location.\ndenote the two different right shift operations in Figure 3.10 as >>A (arithmetic)\nSuppose we want to generate assembly code for the following C function:\nactual shifts and leaves the ﬁnal value in register %rax.\nTwo key instructions\nParameters x and n are stored in registers %rdi and %rsi,\nFill in the missing instructions, following the annotations on the right.\n(a) C code\nlong t1 = x ^ y;\n(b) Assembly code\nx in %rdi, y in %rsi, z in %rdx\nC and assembly code for arithmetic function.\nWe see that most of the instructions shown in Figure 3.10 can be used for either\nOnly right shifting requires instructions\nstored in registers %rdi, %rsi, and %rdx, respectively.\nshift instructions.\nIn the assembly code of Figure 3.11, the sequence of values in register %rax\ncompilers generate code that uses individual registers for multiple program values\nx in %rdi, y in %rsi, z in %rdx\nin code that was generated from C where no exclusive-or operations were\nA. Explain the effect of this particular exclusive-or instruction and what useful\nThe x86-64 instruction set\nInstruction\n%rdx and %rax are viewed as forming a single 128-bit oct word.\ndescribes instructions that support generating the full 128-bit product of two 64-bit\nThe imulq instruction has two different forms One form, shown in Figure 3.10,\nis as a member of the imul instruction class.\noperand” multiply instruction, generating a 64-bit product from two 64-bit oper-\nAdditionally, the x86-64 instruction set includes two different “one-operand”\nmultiply instructions to compute the full 128-bit product of two 64-bit values—\nFor both of these instructions, one argument must be in register %rax, and the\nother is given as the instruction source operand.\nregisters %rdx (high-order 64 bits) and %rax (low-order 64 bits).\nAs an example, the following C code demonstrates the generation of a 128-bit\nproduct of two unsigned 64-bit numbers x and y:\nIn this program, we explicitly declare x and y to be 64-bit numbers, using deﬁ-\nThe assembly code generated by gcc for this function is as follows:\ndest in %rdi, x in %rsi, y in %rdx\nObserve that storing the product requires two movq instructions: one for the\ncode is generated for a little-endian machine, the high-order bytes are stored at\noperand divide instructions similar to the single-operand multiply instructions.\nThe signed division instruction idivl takes as its dividend the 128-bit quantity\nin registers %rdx (high-order 64 bits) and %rax (low-order 64 bits).\ngiven as the instruction operand.\nThe instruction stores the quotient in register\n%rax and the remainder in register %rdx.\nThis value should be stored in register %rax.\neither all zeros (unsigned arithmetic) or the sign bit of %rax (signed arithmetic).\nThe latter operation can be performed using the instruction cqto.2 This instruction\nlong q = x/y;\nlong r = x%y;\nname for an instruction does not match the Intel name.\nThis compiles to the following assembly code:\nx in %rdi, y in %rsi, qp in %rdx, rp in %rcx\nIn this code, argument rp must ﬁrst be saved in a different register (line 2),\nsince argument register %rdx is required for the division operation.\nUnsigned division makes use of the divq instruction.\nunsigned long q = x/y;\nunsigned long r = x%y;\nModify the assembly code shown for signed division to implement this function.\nboth statements in C and instructions in machine code are executed sequentially,\ncode instructions can be altered with a jump instruction, indicating that control\nThe compiler must generate instruction sequences that build upon\nCondition Codes\nIn addition to the integer registers, the CPU maintains a set of single-bit condition\ncode registers describing attributes of the most recent arithmetic or logical oper-\ncondition codes are the most useful:\nFor example, suppose we used one of the add instructions to perform the\nThen the condition codes would be set according to the following C expressions:\nThe leaq instruction does not alter any condition codes, since it is intended\nOtherwise, all of the instructions listed in\nFigure 3.10 cause the condition codes to be set.\nreasons that we will not delve into, the inc and dec instructions set the overﬂow\nIn addition to the setting of condition codes by the instructions of Figure 3.10,\nthere are two instruction classes (having 8-, 16-, 32-, and 64-bit forms) that set\ncondition codes without altering any other registers; these are listed in Figure 3.13.\nThe cmp instructions set the condition codes according to the differences of their\nThey behave in the same way as the sub instructions, except that\nthey set the condition codes without updating their destinations.\nInstruction\nComparison and test instructions.\nThese instructions set the condition\ninstructions set the zero ﬂag if the two operands are equal.\nset the condition codes without altering their destinations.\nof using the condition codes: (1) we can set a single byte to 0 or 1 depending\non some combination of the condition codes, (2) we can conditionally jump to\nﬁrst case, the instructions described in Figure 3.14 set a single byte to 0 or to 1\nclass of instructions as the set instructions; they differ from one another based on\nwhich combinations of condition codes they consider, as indicated by the different\nsufﬁxes for the instruction names.\nthese instructions denote different conditions and not different operand sizes.\nexample, instructions setl and setb denote “set less” and “set below,” not “set\nA set instruction has either one of the low-order single-byte register elements\nA typical instruction sequence to compute the C expression a < b, where a\nInstruction\nThe set instructions.\nEach instruction sets a single byte to 0 or 1 based on\nsome combination of the condition codes.\nSome instructions have “synonyms,” that is,\nalternate names for the same machine instruction.\nNote the comparison order of the cmpq instruction (line 2).\ninstruction (line 4) clears not just the high-order 3 bytes of %eax, but the upper 4\nbytes of the entire register, %rax, as well.\nFor some of the underlying machine instructions, there are multiple possible\nand setnle (for “set not less or equal”) refer to the same machine instruction.\nAlthough all arithmetic and logical operations set the condition codes, the de-\nscriptions of the different set instructions apply to the case where a comparison\ninstruction has been executed, setting the condition codes according to the com-\nConsider the sete, or “set when equal,” instruction.\nfor signed comparison with the setl, or “set when less,” instruction.\nt = a-b, the carry ﬂag will be set by the cmp instruction when a −b < 0, and so the\nInstead, it mostly uses the same instructions for the two\ninstructions to handle signed and unsigned operations, such as using differ-\nent versions of right shifts, division and multiplication instructions, and different\ncombinations of condition codes.\nThe C code\neach of the following instruction sequences, determine which data types data_t\nD.\nThe C code\ninstruction sequences implement the comparison, where a is held in some portion\ncomparisons TEST could cause the compiler to generate this code.\nD.\nJump Instructions\nUnder normal execution, instructions follow each other in the order they are\nA jump instruction can cause the execution to switch to a completely\nInstruction\nThe jump instructions.\nThese instructions jump to a labeled destination\nfor the same machine instruction.\nThe instruction jmp .L1 will cause the program to skip over the movq instruc-\ntions and encodes the jump targets (the addresses of the destination instructions)\nas part of the jump instructions.\nFigure 3.15 shows the different jump instructions.\nThe jmp instruction jumps\nas part of the instruction, or an indirect jump, where the jump target is read from\nDirect jumps are written in assembly code by\ngiving a label as the jump target, for example, the label .L1 in the code shown.\nAs examples, the instruction\nuses the value in register %rax as the jump target, and the instruction\nreads the jump target from memory, using the value in %rax as the read address.\nThe remaining jump instructions in the table are conditional—they either\njump or continue executing at the next instruction in the code sequence, depending\nThe names of these instructions\nand the conditions under which they jump match those of the set instructions\nAs with the set instructions, some of the underlying machine\ninstructions have multiple names.\nJump Instruction Encodings\nOn the other hand, understanding how the targets of jump instructions\nis, they encode the difference between the address of the target instruction and\nthe address of the instruction immediately following the jump.\nAs an example of PC-relative addressing, the following assembly code for a\njmp instruction on line 2 jumps forward to a higher address, while the jg instruction\nd:\nare indicated as 0x8 for the jump instruction on line 2 and 0x5 for the jump\ninstruction on line 5 (the disassembler lists all numbers in hexadecimal).\nat the byte encodings of the instructions, however, we see that the target of the ﬁrst\njump instruction is encoded (in the second byte) as 0x03.\nWhat do the instructions rep and repz do?\nLine 8 of the assembly code shown on page 243 contains the instruction combination rep; ret.\nrep instruction, we ﬁnd that it is normally used to implement a repeating string operation [3, 51].\nthe ret instruction the destination of a conditional jump instruction.\nWithout the rep instruction, the\njg instruction (line 7 of the assembly code) would proceed to the ret instruction when the branch is not\nAccording to AMD, their processors cannot properly predict the destination of a ret instruction\nwhen it is reached from a jump instruction.\nThe rep instruction serves as a form of no-operation here,\naddress of the following instruction, we get jump target address 0x8, the address\nof the instruction on line 4.\nSimilarly, the target of the second jump instruction is encoded as 0xf8 (deci-\n(decimal 13), the address of the instruction on line 6, we get 0x5, the address of\nthe instruction on line 3.\ning PC-relative addressing is the address of the instruction following the jump, not\ninstruction.\n4004d0:\n4004d3:\n4004d8 <loop+0x8>\n4004d5:\n4004d8:\n4004d5 <loop+0x5>\nThe instructions have been relocated to different addresses, but the encodings\nencoding of the jump targets, the instructions can be compactly encoded (requiring\njust 2 bytes), and the object code can be shifted to different positions in memory\nAnswer the following questions about these instructions.\nA. What is the target of the je instruction below?\nanything about the callq instruction here.)\nB. What is the target of the je instruction below?\n400431: 5d\nC. What is the address of the ja and pop instructions?\nD. In the code that follows, the jump target is encoded in PC-relative form as a 4-\nThe jump instructions provide a means to implement conditional execution\nC into machine code is to use combinations of conditional and unconditional\nexample, Figure 3.16(a) shows the C code for a function that computes the absolute\nGcc generates the assembly code shown as Figure 3.16(c).\nthe machine code into C is shown as the function gotodiff_se (Figure 3.16(b)).\n(c) Generated assembly code\nx in %rdi, y in %rsi\nThe generated assembly code is shown (c), along with\n(b) a C procedure gotodiff_se that mimics the control ﬂow of the assembly code.\nIn the goto code (Figure 3.16(b)), the statement goto x_ge_y on line 5 causes\na jump to the label x_ge_y (since it occurs when x ≥y) on line 9.",
      "keywords": [
        "instruction",
        "rax",
        "code",
        "assembly code",
        "long",
        "Condition Codes",
        "rdx",
        "jump",
        "jump instruction",
        "Operations",
        "register",
        "Codes",
        "unsigned",
        "shift",
        "set instructions"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "code",
        "codes",
        "codings",
        "operation",
        "operations",
        "operating",
        "operator",
        "operators"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.877,
          "base_score": 0.727,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.794,
          "base_score": 0.644,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "instructions",
          "jump",
          "code",
          "codes"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "instructions",
          "jump",
          "code",
          "codes"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45049751881749367,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712610+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 239-261)",
      "start_page": 239,
      "end_page": 261,
      "summary": "Describing machine code with C code\nFigure 3.16 shows an example of how we will demonstrate the translation of C language control\nconstructs into machine code.\nof the assembly code generated by gcc (c).\nstructure of the assembly code (b).\nthe actual assembly code.\nOn the other hand, if the test x >= y\nThe assembly-code implementation (Figure 3.16(c)) ﬁrst compares the two\noperands (line 2), setting the condition codes.\nthat x is greater than or equal to y, it then jumps to a block of code starting at\nline 8 that increments global variable ge_cnt, computes x-y as the return value,\nOtherwise, it continues with the execution of code beginning at line\n4 that increments global variable lt_cnt, computes y-x as the return value, and\nWe can see, then, that the control ﬂow of the assembly code generated for\nabsdiff_se closely follows the goto code of gotodiff_se.\nThat is, the compiler generates separate blocks of code for then-statement and\nWhen given the C code\ngcc generates the following assembly code:\nA. Write a goto version in C that performs the same computation and mimics\nthe control ﬂow of the assembly code, in the style shown in Figure 3.16(b).\nYou might ﬁnd it helpful to ﬁrst annotate the assembly code as we have done\nB. Explain why the assembly code contains two conditional branches, even\nthough the C code has only one if statement.\nAn alternate rule for translating if statements into goto code is as follows:\nStarting with C code of the form\nshort test(short x, short y, short z) {\ngcc generates the following assembly code:\nshort test(short x, short y, short z)\nx in %rdi, y in %rsi, z in %rdx\nFill in the missing expressions in the C code.\ncases, but it can then be implemented by a simple conditional move instruction\nFigure 3.17(a) shows an example of code that can be compiled using a condi-\nThe function computes the absolute value of its arguments x and y,\nsimply computes the value to be returned by the function.\n(a) Original C code\nresult = y - x;\nresult = x - y;\nlong rval = y-x;\nlong eval = x-y;\n(c) Generated assembly code\nx in %rdi, y in %rsi\nThe generated assembly code is shown (c), along with (b) a\nC function cmovdiff that mimics the operation of the assembly code.\nFor this function, gcc generates the assembly code shown in Figure 3.17(c),\nStudying the C version, we can see that it computes both y-x and x-y,\nIt then tests whether x is greater than\ncode in Figure 3.17(c) follows the same logic.\ninstruction (line 7) of the assembly code implements the conditional assignment\ndestination, only if the cmpq instruction of line 6 indicates that one value is greater\nTo understand why code based on conditional data transfers can outperform\ncode based on conditional control transfers (as in Figure 3.16), we must understand\nprocessor using both methods of implementing the conditional operation.\nof the two code sequences require only a single clock cycle.\nx86-64 code with conditional jumps, we found that the function requires around 8\nAssume the probability of misprediction is p, the time to execute the code without misprediction is\nThen the average time to execute the code as a function of\nOn the other hand, the code compiled using conditional moves requires\nRunning on a new processor model, our code required around 45 cycles when the\nFigure 3.18 illustrates some of the conditional move instructions available with\non the values of the condition codes.\nreads the source value (possibly from memory), checks the condition code, and\ntional data transfers, consider the following general form of conditional expression\n~(SF ^ OF) & ~ZF\n(SF ^ OF) | ZF\n~CF & ~ZF\nCF | ZF\nThe conditional move instructions.\nThis code contains two code sequences—one evaluating then-expr and one evalu-\nFor the code based on a conditional move, both the then-expr and the else-\nThis can be described by the following abstract code:\nvalue ve is copied to v only if test condition t does not hold.\nMost signiﬁcantly, the abstract code we have shown evaluates both then-expr and\ncode:\nTest x\nmovq instruction (line 2) occurs even when the test fails, causing a null pointer\nInstead, this code must be compiled using branching code.\nUsing conditional moves also does not always improve code efﬁciency.\nstrategy to conditional control transfers for implementing conditional operations.\nIn the following C function, we have left the deﬁnition of operation OP incomplete:\nWhen compiled, gcc generates the following assembly code:\nx in %rdi\nB. Annotate the code to explain how it works.\nStarting with C code of the form\nshort test(short x, short y) {\ngcc generates the following assembly code:\nshort test(short x, short y)\nx in %rdi, y in %rsi\nFill in the missing expressions in the C code.\nLoops\nC provides several looping constructs—namely, do-while, while, and for.\ncorresponding instructions exist in machine code.\nditional tests and jumps are used to implement the effect of loops.\ncompilers generate loop code based on the two basic loop patterns.\nDo-While Loops\nThe effect of the loop is to repeatedly execute body-statement, evaluate test-expr,\nThis general form can be translated into conditionals and goto statements as\nloop:\ngoto loop;\n(a) C code\nloop:\ngoto loop;\n(c) Corresponding assembly-language code\nloop:\nIf >, goto loop\nCode for do–while version of factorial program.\nThe goto code shown in Figure 3.19(b) shows how the loop gets turned into\na lower-level combination of tests and conditional jumps.\nloop, consisting here of updates to variables result and n.\nn > 1, and, if so, it jumps back to the beginning of the loop.\nA key to understanding how the generated assembly code relates to the original source code is to ﬁnd a\nthe computations, so that some variables in the C code have no counterpart in the machine code, and\nnew values are introduced into the machine code that do not exist in the source code.\nLook at how registers are initialized before the loop, updated and tested within the loop, and used\nafter the loop.\nthe code, and others where it is hard to explain why the compiler chose that particular strategy.\nthe assembly code from which the goto code was generated.\ninstruction jg (line 7) is the key instruction in implementing a loop.\nReverse engineering assembly code, such as that of Figure 3.19(c), requires\nFor the C code\nshort dw_loop(short x) {\nshort n = 4*x;\ngcc generates the following assembly code:\nshort dw_loop(short x)\nA. Which registers are used to hold program values x, y, and n?\nC. Add annotations to the assembly code describing the operation of the pro-\nWhile Loops\nIt differs from do-while in that test-expr is evaluated and the loop is potentially\nways to translate a while loop into machine code, two of which are used in code\nloop.\nwhile loop form to goto code:\nloop:\ngoto loop;\n(a) C code\nloop:\ngoto loop;\n(c) Corresponding assembly-language code\nloop:\nIf >, goto loop\nC and assembly code for while version of factorial using jump-to-\nThe C function fact_while_jm_goto illustrates the operation of\nthe assembly-code version.\nfunction fact_while_jm_goto (Figure 3.20(b)) is a C rendition of the assembly\ncode generated by gcc when optimization is speciﬁed with the command-line op-\ntion -Og. Comparing the goto code generated for fact_while (Figure 3.20(b)) to\nthe statement goto test before the loop causes the program to ﬁrst perform the\ntest of n before modifying the values of result or n.\nﬁgure (Figure 3.20(c)) shows the actual assembly code generated.\nFor C code having the general form\ngcc, run with command-line option -Og, produces the following code:\nloop_while:\ninstruction on line 3 to jump to the test starting with label .L2.\nparts of the C code.\nforms the code into a do-while loop by using a conditional branch to skip over the\nloop if the initial test fails.\nThis, in turn, can be transformed into goto code as\nloop:\ngoto loop;\ntest, for example, determining that the test condition will always hold.\nAs an example, Figure 3.21 shows the same C code for a factorial function\nFigure 3.21(c) shows the actual assembly code\ngenerated, while Figure 3.21(b) renders this assembly code in a more readable C\nReferring to this goto code, we see that the loop will be skipped\ninteresting feature, however, is that the loop test (line 9 of the assembly code)\nhas been changed from n > 1 in the original C code to n ̸= 1.\ndetermined that the loop can only be entered when n > 1, and that decrementing\nFor C code having the general form\ngcc, run with command-line option -O1, produces the following code:\nloop_while2:\n(a) C code\nloop:\ngoto loop;\n(c) Corresponding assembly-language code\nloop:\nIf !=, goto loop\nC and assembly code for while version of factorial using guarded-\nassembly-code version.\ninstruction on line 3 to skip over the loop code when the initial test fails.\nthe missing parts of the C code.\nC code according to our translation rules.\nthe C code in a way that it will have equivalent behavior to the assembly code.\nshort test_one(unsigned short x) {\nThe gcc C compiler generates the following assembly code:\nshort test_one(unsigned short x)\nx in %rdi\nReverse engineer the operation of this code and then do the following:\nB. Use the assembly-code version to ﬁll in the missing parts of the C code.\nFor Loops\nthat the behavior of such a loop is identical to the following code using a while\nloop:\nloop where it ﬁrst evaluates the test condition test-expr, exiting if the test fails, then\nThe code generated by gcc for a for loop then follows one of our two trans-\njump-to-middle strategy yields the goto code\nloop:\ngoto loop;\nloop:\ngoto loop;\ncode we showed using either a while or a do-while loop.\nWe can identify the different components of the for loop in this code as\nfollowing version in goto code:\nloop:\ngoto loop;\nIndeed, a close examination of the assembly code produced by gcc with\nloop:\nIf <=, goto loop\nWrite goto code for a function called fibonacci to print ﬁbonacci numbers using\na while loop.\nwhile, and for—can be translated by a simple strategy, generating code that con-\nthe basic mechanism for translating loops into machine code.\nshort test_two(unsigned short x) {\nThe gcc C compiler generates the following assembly code:\ntest fun_b(unsigned test x)\nx in %rdi\nReverse engineer the operation of this code and then do the following:\nA. Use the assembly-code version to ﬁll in the missing parts of the C code.\nB. Explain why there is neither an initial test before the loop nor an initial jump\nto the test portion of the loop.\nExecuting a continue statement in C causes the program to jump to the end of\nconsider the following code:\n/* Example of for loop containing a continue statement */\ninto a while loop?\nWhat would be wrong with this code?\ncode more readable, but they also allow an efﬁcient implementation using a data\na code segment implementing the action the program should take when the switch\nThe code performs an array reference into the jump table using the\nFigure 3.22(a) shows an example of a C switch statement.\nFigure 3.23 shows the assembly code generated when compiling switch_eg.\nThe behavior of this code is shown in C as the procedure switch_eg_impl in\nThis code makes use of support provided by gcc for jump tables,\na pointer for a code location.) We recommend that you study the C procedure\nswitch_eg_impl and how it relates to the assembly-code version.\nOur original C code has cases for values 100, 102–104, and 106, but the switch\nIn the C and assembly code, there are\n(identiﬁed in the assembly code as .L3), loc_B (.L5), loc_C (.L6), loc_D (.L7),\nof these labels identiﬁes a block of code implementing one of the case branches.\nIn both the C and the assembly code, the program compares index to 6 and jumps\nto the code for the default case if it is greater.\nThe key step in executing a switch statement is to access a code location\nThis occurs in line 16 in the C code, with a goto statement\nIn our assembly-code version, a similar operation",
      "keywords": [
        "code",
        "assembly code",
        "loop",
        "long",
        "rdi",
        "goto",
        "goto code",
        "conditional",
        "goto loop",
        "result",
        "code long fact",
        "short",
        "code long",
        "assembly",
        "assembly code generated"
      ],
      "concepts": [
        "code",
        "codes",
        "loops",
        "looping",
        "loop",
        "instruction",
        "instructions",
        "condition",
        "conditional",
        "tested"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 18,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 35,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "loop",
          "goto",
          "assembly",
          "assembly code"
        ],
        "semantic": [],
        "merged": [
          "code",
          "loop",
          "goto",
          "assembly",
          "assembly code"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4608879400480469,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712670+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 262-284)",
      "start_page": 262,
      "end_page": 284,
      "summary": "void switch_eg(long x, long n,\nlong val = x;\nvoid switch_eg_impl(long x, long n,\n/* Table of code pointers */\nval = x * 13;\nval = x + 11;\nOur C code declares the jump table as an array of seven elements, each\nvoid switch_eg(long x, long n, long *dest)\nx in %rdi, n in %rsi, dest in %rdx\n3*x\nval = 13*x\nval = x + 11\nAssembly code for switch statement example in Figure 3.22.\nduplicate cases by simply having the same code label (loc_D) for entries 4 and 6,\nIn the assembly code, the jump table is indicated by the following declarations,\nbyte) words, where the value of each word is given by the instruction address\nThe different code blocks (C labels loc_A through loc_D and loc_def) im-\ncompute a value for val and then go to the end of the function.\nassembly-code blocks compute a value for register %rdi and jump to the position\nOnly the code for case label 102\ndoes not follow this pattern, to account for the way the code for this case falls\nthrough to the block with label 103 in the original C code.\nassembly-code block starting with label .L5, by omitting the jmp instruction at\nIn the C function that follows, we have omitted the body of the switch statement.\nIn the C code, the case labels did not span a contiguous range, and some cases had\nswitch (x) {\nIn compiling the function, gcc generates the assembly code that follows for\nthe initial part of the procedure, with variable x in %rdi:\nx in %rdi\nIt generates the following code for the jump table:\nA. What were the values of the case labels in the switch statement?\nB. What cases had multiple labels in the C code?\nFor a C function switcher with the general structure\ngcc generates the assembly code and jump table shown in Figure 3.24.\na in %rsi, b in %rdi, c in %rdx, d in %rcx\nthat implements some functionality with a designated set of arguments and an\ndure P calls procedure Q, and Q then executes and returns back to P.\ncode for Q upon entry and then set to the instruction in P following the\ncall to Q upon return.\nbe able to return a value back to P.\nUsing our example of procedure P calling\nprocedure Q, we can see that while Q is executing, P, along with any of the proce-\nOn the other hand, when Q returns, any local storage it\nby its procedures using a stack, where the stack and the program registers store\nAs P calls Q, control and data information are added to the end of the stack.\nAs described in Section 3.4.4, the x86-64 stack grows toward lower addresses\nand the stack pointer %rsp points to the top element of the stack.\nstored on and retrieved from the stack using the pushq and popq instructions.\nSpace for data with no speciﬁed initial value can be allocated on the stack by simply\ndeallocated by incrementing the stack pointer.\nWhen an x86-64 procedure requires storage beyond what it can hold in reg-\nGeneral stack frame\nThe stack\nReturn address\nStack pointer\nfunction P\nfunction Q\nstack frame.\ncurrently executing procedure is always at the top of the stack.\ncalls procedure Q, it will push the return address onto the stack, indicating where\nwithin P the program should resume execution once Q returns.\nreturn address to be part of P’s stack frame, since it holds state relevant to P.\ncode for Q allocates the space required for its stack frame by extending the cur-\nWithin that space, it can save the values of registers, allocate\nspace for local variables, and set up arguments for the procedures it calls.\nstack frames for most procedures are of ﬁxed size, allocated at the beginning of\nProcedure P can pass up to six integral values (i.e.,\npointers and integers) on the stack, but if Q requires more arguments, these can\nbe stored by P within its stack frame prior to the call.\nIn the interest of space and time efﬁciency, x86-64 procedures allocate only\nthe portions of stack frames they require.\nsix or fewer arguments, and so all of their parameters can be passed in registers.\nThus, parts of the stack frame diagrammed in Figure 3.25 may be omitted.\nmany functions do not even require a stack frame.\nvariables can be held in registers and the function does not call any other functions\nrequired stack frames.\nPassing control from function P to function Q involves simply setting the program\ncounter (PC) to the starting address of the code for Q.\ncomes time for Q to return, the processor must have some record of the code\nin x86-64 machines by invoking procedure Q with the instruction call Q.\ninstruction pushes an address A onto the stack and sets the PC to the beginning\ncounterpart instruction ret pops an address A off the stack and sets the PC to A.\nthese are x86-64 versions of call and return instructions, not IA32.\nIn x86-64\n0x400563\n0x400540\n0x400568\n0x400568\nfunction, while the ret instruction returns back to the instruction following the call.\nexcerpts of the disassembled code for the two functions:\nReturn from function multstore\n0x8(%rsp),%rdx\nIn this code, we can see that the call instruction with address 0x400563 in\nmain calls function multstore.\nindicated values for the stack pointer %rsp and the program counter %rip.\neffect of the call is to push the return address 0x400568 onto the stack and to jump\nto the ﬁrst instruction in function multstore, at address 0x0400540 (3.26(b)).\nThis instruction pops the value 0x400568 from the stack\nAs a more detailed example of passing control to and from procedures, Figure\n3.27(a) shows the disassembled code for two functions, top and leaf, as well as\nthe portion of code in function main where top gets called.\nFunction leaf returns 97 to top, which\n(a) Disassembled code for demonstrating procedure calls and returns\n0x2(%rdi),%rax\nDisassembly of top(long x)\nx in %rdi\n$0x5,%rdi\nT1: x-5\n(b) Execution trace of example code\n0x400545\n0x400560\n0x400549\n0x400560\n0x400540\n0x400544\n0x400560\n0x400551\n0x400560\n0x400560\nDetailed execution of program involving procedure calls and returns.\nUsing the stack to\nstore return addresses makes it possible to return to the right point in the procedures.\nincluding the contents of registers %rdi, %rax, and %rsp, as well as the value at\nthe top of the stack.\nneeded to support procedure calls and returns.\nInstruction L1 of leaf sets %rax to 97, the value to be returned.\nInstruction T3 sets %rax to 194, the value to be returned from top.\nIt pops 0x4000560 from the stack, thereby setting the PC to\nWe see that the stack pointer has also been restored to\nthe stack makes it possible for the function to later return to the proper point\nThe standard call/return mechanism of C (and of most program-\nThe disassembled code for two functions first and last is shown below, along\nwith the code for a call of first by function main:\nDisassembly of last(long x)\nx in %rdi\n0x1(%rdi),%rsi\n$0x1,%rdi\ninstruction execution through to the point where the program returns back to\n0x400560\nwhen the procedure returns, procedure calls may involve passing data as argu-\nments, and returning from a procedure may also involve returning a value.\nx86-64, most of these data passing to and from procedures take place via regis-\narguments are passed in registers %rdi, %rsi, and others, and where values are re-\nWhen procedure P calls procedure Q, the code for P must\nto P, the code for P can access the returned value in register %rax.\nWith x86-64, up to six integral (i.e., integer and pointer) arguments can be\nArguments are allocated to these registers according to their\nRegisters for passing function arguments.\npassed on the stack.\nAssume that procedure P calls procedure Q with n integral\nThen the code for P must allocate a stack frame with\nonto the stack, with argument 7 at the top of the stack.\nProcedure Q can access its arguments via registers and\nIf Q, in turn, calls some function that has more than six\narguments, it can allocate space within its stack frame for these, as is illustrated\nAs an example of argument passing, consider the C function proc shown in\nThis function has eight arguments, including integers with different\nThe assembly code generated for proc is shown in Figure 3.29(b).\nsix arguments are passed in registers.\nThe last two are passed on the stack, as\nstack during the execution of proc.\nonto the stack as part of the procedure call.\nA C function procprob has four arguments u, a, v, and b.\nIt compiles to the following x86-64 code:\n(a) C code\nExample of function with multiple arguments of different types.\nArguments 1–6 are passed in registers, while arguments 7–8 are passed on the stack.\nStack frame structure for\nthe stack.\nReturn address\nStack pointer\nLocal Storage on the Stack\nTypically, a procedure allocates space on the stack frame by decrementing the\nstack pointer.\nThis results in the portion of the stack frame labeled “Local vari-\nThe function swap_add swaps the two values\ndesignated by pointers xp and yp and also returns the sum of the two values.\nfunction caller creates pointers to local variables arg1 and arg2 and passes these\nFigure 3.31(b) shows how caller uses a stack frame to implement\nThe code for caller starts by decrementing the stack pointer\nby 16; this effectively allocates 16 bytes on the stack.\nthe stack pointer, we can see that the code computes &arg2 as S + 8 (line 5), &arg1\nwithin the stack frame at offsets 0 and 8 relative to the stack pointer.\nto swap_add completes, the code for caller then retrieves the two values from\nthe stack (lines 8–9), computes their difference, and multiplies this by the value\nreturned by swap_add in register %rax (line 10).\nits stack frame by incrementing the stack pointer by 16 (line 11.) We can see with\nthis example that the run-time stack provides a simple mechanism for allocating\nAs a more complex example, the function call_proc, shown in Figure 3.32,\nillustrates many aspects of the x86-64 stack discipline.\nstorage on the stack for local variables, as well as to pass values to the 8-argument\nfunction proc (Figure 3.29).\nThe function creates a stack frame, diagrammed in\nLooking at the assembly code for call_proc (Figure 3.32(b)), we can see\nthat a large portion of the code (lines 2–15) involves preparing to call function\n(a) Code for swap_add and calling function\nlong x = *xp;\nreturn x + y;\n(b) Generated assembly code for calling function\nAllocate 16 bytes for stack frame\nDeallocate stack frame\nThe calling code must allocate\na stack frame due to the presence of address operators.\nThis includes setting up the stack frame for the local variables and function\nparameters, and for loading function arguments into registers.\nshows, local variables x1–x4 are allocated on the stack and have different sizes.\nArguments 7 (with value\n4) and 8 (a pointer to the location of x4) are stored on the stack at offsets 0 and 8\nrelative to the stack pointer.\n(a) C code for calling function\nx2 = 2;\nAllocate 32-byte stack frame\nStore 1 in &x1\nStore 2 in &x2\nStore 3 in &x3\nStore &x4 as argument 8\nPass &x3 as argument 6\nPass &x2 as argument 4\nPass &x1 as argument 2\nGet x2 and convert to long\nDeallocate stack frame\nExample of code to call function proc, deﬁned in Figure 3.29.\ncreates a stack frame.\nStack frame for function\nThe stack\nStack pointer\nArgument 8 = &x4\nReturn address\nx1\nx4\nx3\nx2\nx4\nWhen procedure proc is called, the program will begin executing the code\nat offsets 8 and 16 relative to the stack pointer, because the return address was\npushed onto the stack.\nWhen the program returns to call_proc, the code retrieves the values of the\nby incrementing the stack pointer by 32 to deallocate the stack frame.\nWhen procedure P calls procedure Q, Q must preserve the values\nof these registers, ensuring that they have the same values when Q returns to P as\nProcedure Q can preserve a register value by either not\nchanging it at all or by pushing the original value on the stack, altering it, and then\npopping the old value from the stack before returning.\nvalues has the effect of creating the portion of the stack frame labeled “Saved\nvalue in a callee-saved register (after saving the previous value on the stack, of\ncourse), call Q, and then use the value in the register without risk of it having been\nAll other registers, except for the stack pointer %rsp, are classiﬁed as caller-\ndata in such a register and calling procedure Q.\nAs an example, consider the function P shown in Figure 3.34(a).\nDuring the ﬁrst call, it must retain the value of x for use later.\n(a) Calling function\nlong P(long x, long y)\nlong v = Q(x);\n(b) Generated assembly code for the calling function\nlong P(long x, long y)\nx in %rdi, y in %rsi\nAlign stack frame\nMove x to first argument\nCall Q(x)\nAdd saved Q(y) to Q(x)\nDeallocate last part of stack\nCode demonstrating use of callee-saved registers.\nValue x must be\nwe can see that the code generated by gcc uses two callee-saved registers: %rbp\nto hold x, and %rbx to hold the computed value of Q(y).\nfunction, it saves the values of these two registers on the stack (lines 2–3).\nargument x to %rbp before the ﬁrst call to Q (line 5).\n14), it restores the values of the two callee-saved registers by popping them off the\nstack.\nConsider a function P, which generates local values, named a0–a8.\nfunction Q using these generated values as arguments.\nlong P(long x)\nx in %rdi\nA. Identify which local values get stored in callee-saved registers.\nB. Identify which local values get stored on the stack.\nC. Explain why the program could not store all of the local values in callee-\nThe conventions we have described for using the registers and the stack allow\nx86-64 procedures to call themselves recursively.\nprovides the proper policy for allocating local storage when the procedure is called\nFigure 3.35 shows both the C code and the generated assembly code for a\nWe can see that the assembly code uses register %rbx\nto hold the parameter n, after ﬁrst saving the existing value on the stack (line 2)\nand later restoring the value before returning (line 11).\nto rfact(n-1) returns (line 9) that (1) the result of the call will be held in register\n(a) C code\nSet return value = 1\n%rax, and (2) the value of argument n will held in register %rbx.\n(saved values of the return location and callee-saved registers).\nThe stack discipline of allocation and\ndeallocation naturally matches the call-return ordering of functions.\nof implementing function calls and returns even works for more complex patterns,\nincluding mutual recursion (e.g., when procedure P calls Q, which in turn calls P).\nFor a C function having the general structure\nx in %rdi\nA. What value does rfun store in the callee-saved register %rbx?\nB. Fill in the missing expressions in the C code shown above.\nThese are translated into address computations in machine code.\nThe memory referencing instructions of x86-64 are designed to simplify array",
      "keywords": [
        "Stack",
        "call",
        "stack frame",
        "code",
        "long",
        "function",
        "rdi",
        "stack pointer",
        "procedure",
        "case",
        "rsp",
        "registers",
        "instruction",
        "calls",
        "arguments"
      ],
      "concepts": [
        "stack",
        "code",
        "returns",
        "returned",
        "returning",
        "function",
        "functionality",
        "functions",
        "procedure",
        "procedures"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 18,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stack",
          "stack frame",
          "frame",
          "procedure",
          "code"
        ],
        "semantic": [],
        "merged": [
          "stack",
          "stack frame",
          "frame",
          "procedure",
          "code"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44212348655048234,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712729+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 285-307)",
      "start_page": 285,
      "end_page": 307,
      "summary": "Array\nC allows arithmetic on pointers, where the computed value is scaled according to\nthe size of the data type referenced by the pointer.\nThat is, if p is a pointer to data\nof type T , and the value of p is xp, then the expression p+i has value xp + L .\nwhere L is the size of data type T .\npointers.\narrays and pointers.\nIt computes the address of the ith array element and then accesses this memory\narray E and integer index i are stored in registers %rdx and %rcx, respectively.\n%eax (for data) or register %rax (for pointers).\nIn these examples, we see that operations that return array values have type\nint, and hence involve 4-byte operations (e.g., movl) and registers (e.g., %eax).\nThose that return pointers have type int *, and hence involve 8-byte operations\ncompute the difference of two pointers within the same data structure, with the\nresult being data having type long and value equal to the difference of the two\naddresses divided by the size of the data type.\nSuppose xP, the address of short integer array P, and long integer index i are stored\nits type, a formula for its value, and an assembly-code implementation.\nshould be stored in register %rax if it is a pointer and register element %ax if it has\ndata type short.\nData type row3_t is deﬁned to be an array of three integers.\nsuch elements, each requiring 12 bytes to store the three integers.\nThe array elements are\narray of ﬁve elements, each of which is an array of three int’s, we ﬁrst have A[0],\nTo access elements of multidimensional arrays, the compiler generates code to\nwith the start of the array as the base address and the (possibly scaled) offset as\nT D[R][C];\narray element D[i][j] is at memory address\n&D[i][j] = xD + L(C .\nElements of array in\nwhere L is the size of data type T in bytes.\nSuppose xA, i, and j are in registers %rdi, %rsi, and %rdx,\nThen array element A[i][j] can be copied to register %eax by the\nfollowing code:\nA in %rdi, i in %rsi, and j in %rdx\nAs can be seen, this code computes the element’s address as xA + 12i + 4j = xA +\nConsider the following source code, where M and N are constants declared with\nlong P[M][N];\nIn compiling this program, gcc generates the following assembly code:\ni in %rdi, j in %rsi\nThe C compiler is able to make many optimizations for code operating on multi-\ndeclare data type fix_matrix to be 16 × 16 arrays of integers as follows:\n3.37(a) computes element i, k of the product of arrays A and B—that is, the\nC, shown as function fix_prod_ele_opt in Figure 3.37(b).\narray references to pointer dereferences.\n(2) generating a pointer, which we have named Bptr, that points to successive\nelements in column k of B, and (3) generating a pointer, which we have named\nThe initial value for Aptr is the address of the ﬁrst element of row i of A, given\nelement of column k of B, given by the C expression &B[0][k].\nis the index of what would be the (n + 1)st element in column j of B, given by the\nC expression &B[N][k].\n(a) Original C code\nint fix_prod_ele (fix_matrix A, fix_matrix B, long i, long k) {\n(b) Optimized C code\nint fix_prod_ele_opt(fix_matrix A, fix_matrix B, long i, long k) {\nint *Bend = &B[N][k];\nOriginal and optimized code to compute element i, k of matrix product\nThe following is the actual assembly code generated by gcc for function fix_\nWe see that four registers are used as follows: %eax holds result, %rdi\nint fix_prod_ele_opt(fix_matrix A, fix_matrix B, long i, long k)\nA in %rdi, B in %rsi, i in %rdx, k in %rcx\nBptr, and Bend in the C code of Figure 3.37(b) (lines 3–5) correctly describe their\ncomputations in the assembly code generated for fix_prod_ele (lines 3–5).\nThe following C code sets the diagonal elements of one of our ﬁxed-size arrays to\nCreate a C code program fix_set_diag_opt that uses optimizations similar\nHistorically, C only supported multidimensional arrays where the sizes (with the\nProgrammers requiring variable-size arrays had to allocate storage for these arrays\narray dimension expressions that are computed as the array is being allocated.\nIn the C version of variable-size arrays, we can declare an array\naccess element i, j of an n × n array as follows:\nint var_ele(long n, int A[n][n], long i, long j) {\nGcc generates code for this referencing function as\nint var_ele(long n, int A[n][n], long i, long j)\nn in %rdi, A in %rsi, i in %rdx, j in %rcx\nAs the annotations show, this code computes the address of element i, j as xA +\nWhen variable-size arrays are referenced within a loop, the compiler can often\nFor example, Figure 3.38(a) shows C code to compute element i, k of the product\nof two n × n arrays A and B.\nThis code follows a different style from the optimized\ncode for the ﬁxed-size array (Figure 3.37), but that is more an artifact of the choices\nThe code of Figure 3.38(b) retains loop variable j, both to detect when\n(a) Original C code\nint var_prod_ele(long n, int A[n][n], int B[n][n], long i, long k) {\n(b) Optimized C code\nint var_prod_ele_opt(long n, int A[n][n], int B[n][n], long i, long k) {\nOriginal and optimized code to compute element i, k of matrix product for variable-size\narrays.\nThe following is the assembly code for the loop of var_prod_ele:\nRegisters: n in %rdi, Arow in %rsi, Bptr in %rcx\n4n in %r9, result in %eax, j in %edx\nWe see that the program makes use of both a scaled value 4n (register %r9) for\nincrementing Bptr as well as the value of n (register %rdi) to check the loop\nThe need for two values does not show up in the C code, due to the scaling\narray.\nIt can then generate code that avoids the multiplication that would result\ncode of Figure 3.37(b) or the array-based code of Figure 3.38(b), these optimiza-\nC provides two mechanisms for creating data types by combining objects of dif-\nferent types: structures, declared using the keyword struct, aggregate multiple\nThe C struct declaration creates a data type that groups objects of possibly\nThe implementation of structures is similar to that of arrays\nmemory and a pointer to a structure is the address of its ﬁrst byte.\nmaintains information about each structure type indicating the byte offset of\nIt generates references to structure elements using these offsets as\nAs an example, consider the following structure declaration:\nint j;\nThis structure contains four ﬁelds: two 4-byte values of type int, a two-element\narray of type int, and an 8-byte integer pointer, giving a total of 24 bytes:\nObserve that array a is embedded within the structure.\nTo access the ﬁelds of a structure, the compiler generates code that adds the\nappropriate offset to the address of the structure.\nThe struct data type constructor is the closest thing C provides to the objects of C++ and Java.\nWe can declare a variable r of type struct rect and set its ﬁeld values as follows:\nwhere the expression r.llx selects ﬁeld llx of structure r.\nIt is common to pass pointers to structures from one place to another rather than copying them.\nFor example, the following function computes the area of a rectangle, where a pointer to the rectangle\nThe expression (*rp).width dereferences the pointer and selects the width ﬁeld of the resulting\nof type struct rec * is in register %rdi.\nThen the following code copies element\nr->i to element r->j:\nSince the offset of ﬁeld i is 0, the address of this ﬁeld is simply the value of r.\nstore into ﬁeld j, the code adds offset 4 to the address of r.\nTo generate a pointer to an object within a structure, we can simply add the\nﬁeld’s offset to the structure address.\nFor example, we can generate the pointer\nFor pointer r in register %rdi and long\ninteger variable i in register %rsi, we can generate the pointer value &(r->a[i])\nRegisters: r in %rdi, i %rsi\nAs a ﬁnal example, the following code implements the statement\nAs these examples show, the selection of the different ﬁelds of a structure is\nConsider the following structure declaration:\nwithin arrays.\nA. What are the offsets (in bytes) of the following ﬁelds?\nB. How many total bytes does the structure require?\nC. The compiler generates the following assembly code for st_init:\nThe following code shows the declaration of a structure of type ACE and the\nWhen the code for fun is compiled, gcc generates the following assembly\nA. Use your reverse engineering skills to write C code for test.\nB. Describe the data structure that this structure implements and the operation\nUnions provide a way to circumvent the type system of C, allowing a single object\nthe total size of data types S3 and U3, are as shown in the following table:\nrather than 9 or 12.) For pointer p of type union U3 *, references p->c, p->i[0],\nand p->v would all reference the beginning of the data structure.\nis when we know in advance that the use of two different ﬁelds in a data structure\nFor example, suppose we want to implement a binary tree data structure\npointers to two children but no data.\nIf n is a pointer to a node of type\nunion node_u *, we would reference the data of a leaf node as n->data[0]\nThis structure requires a total of 24 bytes: 4 for type, and either 8 each for\ntype and the union elements, bringing the total structure size to 4 + 4 + 16 = 24.\nFor data structures with more ﬁelds, the savings can be more\nUnions can also be used to access the bit patterns of different data types.\nexample, suppose we use a simple cast to convert a value d of type double to a\nvalue u of type unsigned long:\nfollowing code to generate a value of type unsigned long from a double:\nIn this code, we store the argument in the union using one data type and access it\nWhen using unions to combine data types of different sizes, byte-ordering\ncode for structure and union access.\nYou write the following structure declaration:\nwith different access expressions expr and with destination data type type set\nFill in the following table with data type type and sequences of\nprimitive data types, requiring that the address for some objects must be a multiple\nwill be aligned to have its address be a multiple of 8, then the value can be read\nobject of K bytes must have an address that is a multiple of K.\nFor example, the assembly-code declaration of\nFor code involving structures, the compiler may need to insert gaps in the\nﬁeld allocation to ensure that each structure element satisﬁes its alignment re-\nThe structure will then have some required alignment for its starting\nFor example, consider the structure declaration\nAs a result, j has offset 8, and the overall structure size is 12 bytes.\nthe compiler must ensure that any pointer p of type struct S1* satisﬁes a 4-byte\nUsing our earlier notation, let pointer p have value xp.\nso that each element in an array of structures will satisfy its alignment requirement.\nFor example, consider the following structure declaration:\nIf we pack this structure into 9 bytes, we can still satisfy the alignment requirements\nfor ﬁelds i and j by making sure that the starting address of the structure satisﬁes\nInstead, the compiler allocates 12 bytes for structure S2,\nFor each of the following structure declarations, determine the offset of each ﬁeld,\nthe total size of the structure, and its alignment requirement for x86-64:\nA. struct P1 { short i; int c; int *j; short *d; };\nB. struct P2 { int i[2]; char c[8]; short s[4]; long *j; };\nC. struct P3 { long w[2]; int *c[2] };\nD. struct P4 { char w[16]; char *c[2] };\nAnswer the following for the structure declaration\nallocated to hold a data structure that may be read from or stored into an SSE register must satisfy a\n. The starting address for any block generated by a memory allocation function (alloca, malloc,\nA. What are the byte offsets of all the ﬁelds in the structure?\nB. What is the total size of the structure?\nshow the byte offsets and total size for the rearranged structure.\ncontrol aspects of a program and how it implements different data structures.\nPointers are a central feature of the C programming language.\nuniform way to generate references to elements within different data structures.\n. Every pointer has an associated type.\nUsing the following pointer declarations as illustrations\nvariable ip is a pointer to an object of type int, while cpp is a pointer to an\nobject that itself is a pointer to an object of type char.\nhas type T , then the pointer has type *T .\nFor example, the malloc function returns a generic pointer,\nwhich is converted to a typed pointer via either an explicit cast or by the\nPointer types are not part of\nmachine code; they are an abstraction provided by C to help programmers\n. Every pointer has a value.\nThis value is an address of some object of the\nelements of structures, unions, and arrays.\ncode realization of the ‘&’ operator often uses the leaq instruction to compute\nthe expression value, since this instruction is designed to compute the address\ntype associated with the pointer.\n. Arrays and pointers are closely related.\nBoth array referencing and pointer arithmetic require scaling the\nWhen we write an expression p+i for pointer p with\nvalue p, the resulting address is computed as p + L .\nthe data type associated with p.\n. Casting from one type of pointer to another changes its type but not its value.\nexample, if p is a pointer of type char * having value p, then the expression\n. Pointers can also point to functions.\nthen we can declare and assign a pointer fp to this function by the following\nThe value of a function pointer is the address of the ﬁrst instruction in the\nFunction pointers\nThe syntax for declaring function pointers is especially difﬁcult for novice programmers to understand.\nIt is a pointer to a function that has a single int * as an argument,\nFinally, we see that it is a pointer to a function that takes an int * as an\nThese can be set to just after the entry of a function or at a program address.\nWe have seen that C does not perform any bounds checking for array references,\nas saved register values and return addresses.\nout-of-bounds array element.\nby the following program example:\nint c;",
      "keywords": [
        "int",
        "code",
        "type",
        "structure",
        "Array",
        "Pointer",
        "long",
        "data",
        "data type",
        "rdi",
        "struct",
        "arrays",
        "address",
        "Bptr",
        "Element"
      ],
      "concepts": [
        "types",
        "typed",
        "code",
        "coding",
        "array",
        "arrays",
        "byte",
        "bytes",
        "long",
        "pointer"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "",
          "score": 0.822,
          "base_score": 0.672,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 11,
          "title": "",
          "score": 0.816,
          "base_score": 0.666,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.606,
          "base_score": 0.606,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 9,
          "title": "",
          "score": 0.58,
          "base_score": 0.58,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "structure",
          "type",
          "pointer",
          "code",
          "element"
        ],
        "semantic": [],
        "merged": [
          "structure",
          "type",
          "pointer",
          "code",
          "element"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4537080282855232,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712790+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 308-330)",
      "start_page": 308,
      "end_page": 330,
      "summary": "Run your program (give command-line arguments here)\nExecute one instruction\nExecute four instructions\nRun until current function returns\nDisassemble function around address 0x400544\nDisassemble code within speciﬁed address range\nPrint long integer at address %rsp + 8\nValues of all the registers\nStack frame\nStack frame\nThe preceding code shows an implementation of the library function gets\ngets in the function echo, which simply reads a line from standard input and echos\nAllocate 24 bytes on stack\nFigure 3.40 illustrates the stack organization during the execution of echo.\nprogram allocates 24 bytes on the stack by subtracting 24 from the stack pointer\nThe 16 bytes between buf and the stored return pointer are not\nstored on the stack.\nUnused stack space\nthat, the value of the return pointer, and possibly additional saved state, will\nIf the stored value of the return address is corrupted, then the\nret instruction (line 8) will cause the program to jump to a totally unexpected\nunderstood by studying the program at the machine-code level.\nFigure 3.41 shows a (low-quality) implementation of a function that reads a line\naddress equal to 0x400776 and register %rbx equal to 0x0123456789ABCDEF.\n(a) C code\nDiagram stack at this point\nModify diagram to show stack contents at this point\nthat the error occurs during the execution of the ret instruction of get_line.\njust after executing the instruction at line 3 in the disassembly.\nC. To what address does the program attempt to return?\nD. What register(s) have corrupted value(s) when get_line returns?\nwith the code for get_line?\nexecutable code, called the exploit code, plus some extra bytes that overwrite the\nreturn address with a pointer to the exploit code.\ninstruction is then to jump to the exploit code.\nIn one form of attack, the exploit code then uses a system call to start up a\nshell program, providing the attacker with a range of operating system functions.\ncould make the daemon at a remote site have a buffer overﬂow and execute code\nthe code as well as a pointer to this code as part of the attack string.\nA virus is a piece of code that adds itself to other programs, including\nthis pointer requires knowing the stack address where the string will be located.\nHistorically, the stack addresses for a program were highly predictable.\nan attacker could determine the stack addresses used by a common Web server,\ncode, they would all be using different stack addresses.\nallocating a random amount of space between 0 and n bytes on the stack at the\nstart of a program, for example, by using the allocation function alloca, which\nallocates space for a speciﬁed number of bytes on the stack.\nnot used by the program, but it causes all subsequent stack locations to vary from\nThe following code shows a simple way to determine a “typical” stack address:\nThis code simply prints the address of a local variable in the main function.\nRunning the code 10,000 times on a Linux machine in 32-bit mode, the addresses\ncode, library code, stack, global variables, and heap data, are loaded into different\ninstructions before the actual exploit code.\nfect, other than incrementing the program counter to the next instruction.\nRunning our stack-checking code 10,000 times on a system running Linux ver-\ncanary value4 in the stack frame between any local buffer and the rest of the stack\nguard value, is generated randomly each time the program runs, and so there is no\nStack frame\nStack frame\nStack organization for echo function with stack protector enabled.\nThe code\nchecks the canary value to determine whether or not the stack state has been corrupted.\nand returning from the function, the program checks if the canary has been altered\noption -fno-stack-protector to prevent gcc from inserting this code.\nthe function echo without this option, and hence with the stack protector enabled,\nAllocate 24 bytes on stack\nStore on stack\nWe see that this version of the function retrieves a value from memory (line 3)\nand stores it on the stack at offset 8 from %rsp, just beyond the region allocated for\nThe instruction argument %fs:40 is an indication that the canary value is read\nthat an attacker cannot overwrite the stored canary value.\nregister state and returning, the function compares the value stored at the stack\nlocation with the canary value (via the xorq instruction on line 11).\nidentical, the xorq instruction will yield zero, and the function will complete in the\nA nonzero value indicates that the canary on the stack has been\ncorrupting state stored on the program stack.\nof an executing program, but reducing the vulnerability of the stack thwarts many\nA. For both versions: What are the positions in the stack frame for buf, v, and\ncode provide greater security against a buffer overrun attack?\nOne method is to limit which memory regions hold executable code.\nIn typical programs, only the portion of memory holding the code generated by\ntherefore the bytes on the stack were also executable.\nSupporting Variable-Size Stack Frames\nWe have examined the machine-level code for a variety of functions so far, but\nof space that must be allocated for their stack frames.\nThe code of Figure 3.43(a) gives an example of a function containing a\nThe function declares local array p of n pointers, where n is\nThis requires allocating 8n bytes on the stack, where\nthe value of n may vary from one call of the function to another.\ntherefore cannot determine how much space it must allocate for the function’s\nstack frame.\nIn addition, the program generates a reference to the address of local\np. On returning, the function must deallocate the stack frame and set the stack\npointer to the position of the stored return address.\nTo manage a variable-size stack frame, x86-64 code uses register %rbp to serve\nWe see that the code must save\nthe previous version of %rbp on the stack, since it is a callee-saved register.\nkeeps %rbp pointing to this position throughout the execution of the function, and\n(a) C code\ni in %rax and on stack, n in %rdi, p in %rcx, q in %rdx\nStore on stack\nCode for function exit\nFunction requiring the use of a frame pointer.\nthe stack frame cannot be determined at compile time.\nThe function uses register\nStack pointer\nFigure 3.43(b) shows portions of the code gcc generates for function vframe.\nAt the beginning of the function, we see code that sets up the stack frame and\nThe code starts by pushing the current value of %rbp\nonto the stack and setting %rbp to point to this stack position (lines 2–3).\nallocates 16 bytes on the stack, the ﬁrst 8 of which are used to store local variable\nthe program reaches line 11, it has (1) allocated at least 8n bytes on the stack and\nThe code for the initialization loop shows examples of how local variables\ninstruction uses the value in register %rcx as the address for the start of p.\nAt the end of the function, the frame pointer is restored to its previous value\nSet stack pointer to beginning of frame\nRestore saved %rbp and set stack ptr\nThat is, the stack pointer is ﬁrst set to the position of the saved value of %rbp, and\nthen this value is popped from the stack into %rbp.\nIn earlier versions of x86 code, the frame pointer was used with every function\nWith x86-64 code, it is used only in cases where the stack frame may be of\nframe pointers when generating IA32 code.\nObserve that it is acceptable to mix code that uses frame pointers\nwith code that does not, as long as all functions treat %rbp as a callee-saved register.\nof the code indicate, let us let s1 denote the address of the stack pointer after exe-\nThis instruction allocates the space for local\nLet s2 denote the value of the stack pointer after executing the subq\ninstruction of line 7.\nThis instruction allocates the storage for local array p.\nlet p denote the value assigned to registers %r8 and %rcx in the instructions of lines\nBoth of these registers are used to reference array p.\nthe andq instruction of line 6.\nC. For the following values of n and s1, trace the execution of the code to\ndetermine what the resulting values would be for s2, p, e1, and e2.\nD. What alignment properties does this code guarantee for the values of s2\nFloating-Point Code\nthat affect how programs operating on ﬂoating-point data are mapped onto the\n. How ﬂoating-point values are stored and accessed.\n. The instructions that operate on ﬂoating-point data.\n. The conventions used for passing ﬂoating-point values as arguments to func-\n. The conventions for how registers are preserved during function calls—for\nTo understand the x86-64 ﬂoating-point architecture, it is helpful to have a\nthe same operation is performed on a number of different data values in parallel.\nstructions have included ones to operate on scalar ﬂoating-point data, using single\nvalues in the low-order 32 or 64 bits of XMM or YMM registers.\nprovides a set of registers and instructions that are more typical of the way other\nAll processors capable of executing x86-64 code\nsupport SSE2 or higher, and hence x86-64 ﬂoating point is based on SSE or AVX,\nincluding conventions for passing procedure arguments and return values [77].\nGcc will generate AVX2 code when\narise in compiling ﬂoating-point programs with gcc.\nThese registers are used to hold ﬂoating-point data.\nEach YMM register holds 32 bytes.\nregister.\nWhen operating on scalar data, these registers only\nhold ﬂoating-point data, and only the low-order 32 bits (for float) or 64 bits (for\nThe assembly code refers to the registers by their SSE XMM\nregister names %xmm0–%xmm15, where each XMM register is the low-order 128 bits\nInstruction\nFloating-point movement instructions.\n(X: XMM register\nFigure 3.46 shows a set of instructions for transferring ﬂoating-point data between\nmemory and XMM registers, as well as from one XMM register to another without\nheld either in memory (indicated in the table as M32 and M64) or in XMM registers\nto an XMM register or from an XMM register to memory.\nbetween two XMM registers, it uses one of two different instructions for copying\nthe entire contents of one XMM register to another—namely, vmovaps for single-\nprecision and vmovapd for double-precision values.\nprogram copies the entire register or just the low-order value affects neither the\nprogram functionality nor the execution speed, and so using these instructions\nAs an example of the different ﬂoating-point move operations, consider the\nInstruction\nThese convert ﬂoating-point data to\n(X: XMM register (e.g., %xmm3); R32: 32-bit general-purpose register (e.g., %eax); R64: 64-bit\ngeneral-purpose register (e.g., %rax); M32: 32-bit memory range; M64: 64-bit memory range)\nInstruction\n(X: XMM register (e.g., %xmm3); M32: 32-bit memory range; M64: 64-bit memory\nand its associated x86-64 assembly code\nReturn v2 in %xmm0\nWe can see in this example the use of the vmovaps instruction to copy data from\none register to another and the use of the vmovss instruction to copy data\nfrom memory to an XMM register and from an XMM register to memory.\nFigures 3.47 and 3.48 show sets of instructions for converting between ﬂoating-\npoint and integer data types, as well as between different ﬂoating-point formats.\nThese are all scalar instructions operating on individual data values.\nFigure 3.47 convert from a ﬂoating-point value read from either an XMM register\nor memory and write the result to a general-purpose register (e.g., %rax, %ebx,\nWhen converting ﬂoating-point values to integers, they perform truncation,\nrounding values toward zero, as is required by C and most other programming\nThe instructions in Figure 3.48 convert from integer to ﬂoating point.\nﬁrst operand is read from memory or from a general-purpose register.\nThe destination must be an XMM register.\ninstruction\nThis instruction reads a long integer from register %rax, converts it to data type\ndouble, and stores the result in the lower bytes of XMM register %xmm1.\nFinally, for converting between two different ﬂoating-point formats, current\nversions of gcc generate code that requires separate documentation.\nthe low-order 4 bytes of %xmm0 hold a single-precision value; then it would seem\nto convert this to a double-precision value and store the result in the lower 8 bytes\nof register %xmm0.\nInstead, we ﬁnd the following code generated by gcc:\nThe vunpcklps instruction is normally used to interleave the values in two\nXMM registers and store them in a third.\nIn the code above, we see the\nheld values [x3, x2, x1, x0], then the instruction will update the register to hold\nprecision values in the source XMM register to be the two double-precision values\nin the destination XMM register.\nvunpcklps instruction would give values [dx0, dx0], where dx0 is the result of\nconverting x to double precision.\nto convert the original single-precision value in the low-order 4 bytes of %xmm0 to\ndouble precision and store two copies of it in %xmm0.\nthis code.\nthe XMM register.\nGcc generates similar code for converting from double precision to single\nSuppose these instructions start with register %xmm0 holding two double-precision\nvalues [x1, x0].\nThen the vmovddup instruction will set it to [x0, x0].\ninstruction will convert these values to single precision, pack them into the\n[0.0, 0.0, x0, x0] (recall that ﬂoating-point value 0.0 is represented by a bit pat-\none precision to another this way, rather than by using the single instruction\nAs an example of the different ﬂoating-point conversion operations, consider\ndouble fcvt(int i, float *fp, double *dp, long *lp)\nfloat f = *fp; double d = *dp; long l = *lp;\nand its associated x86-64 assembly code\ndouble fcvt(int i, float *fp, double *dp, long *lp)\nThe following two instructions convert f to double\nThe result is returned in register %xmm0.\nAs is documented in Figure 3.45, this is the designated return register for float\nor double values.\nFor the following C code, the expressions val1–val4 all map to the program values\ndouble fcvt2(int *ip, float *fp, double *dp, long l)\nint i = *ip; float f = *fp; double d = *dp;\nDetermine the mapping, based on the following x86-64 code for the function:\ndouble fcvt2(int *ip, float *fp, double *dp, long l)\nResult returned in %xmm0\nThe following C function converts an argument of type src_t to a return value of\nFor execution on x86-64, assume that argument x is either in %xmm0 or in\nvalue to the appropriately named portion of register %rax (integer result) or\n%xmm0 (ﬂoating-point result).\nInstruction(s)\nFloating-Point Code in Procedures\nWith x86-64, the XMM registers are used for passing ﬂoating-point arguments to\nfunctions and for returning ﬂoating-point values from them.\n. Up to eight ﬂoating-point arguments can be passed in XMM registers %xmm0–\ntional ﬂoating-point arguments can be passed on the stack.\n. A function that returns a ﬂoating-point value does so in register %xmm0.\n. All XMM registers are caller saved.\nWhen a function contains a combination of pointer, integer, and ﬂoating-\npoint arguments, the pointers and integers are passed in general-purpose registers,\nwhile the ﬂoating-point values are passed in XMM registers.\ndouble f1(int x, double y, long z);\nThis function would have x in %edi, y in %xmm0, and z in %rsi.\ndouble f2(double y, int x, long z);\nThis function would have the same register assignment as function f1.\ndouble f1(float x, double *y, long *z);\nThis function would have x in %xmm0, y in %rdi, and z in %rsi.\nFor each of the following function declarations, determine the register assignments\nA. double g1(double a, long b, float c, int d);\nB. double g2(int a, double *b, float *c, long d);\nFigure 3.49 documents a set of scalar AVX2 ﬂoating-point instructions that per-\nXMM register or a memory location.\nnation operands must be XMM registers.\nEach operation has an instruction for\nsingle precision and an instruction for double precision.\nAs an example, consider the following ﬂoating-point function:\ndouble funct(double a, float x, double b, int i)\nreturn a*x - b/i;\nThe x86-64 code is as follows:\ndouble funct(double a, float x, double b, int i)\nThe following two instructions convert x to double",
      "keywords": [
        "stack",
        "XMM register",
        "code",
        "register",
        "function",
        "double",
        "stack frame",
        "XMM",
        "instruction",
        "registers",
        "rsp",
        "program",
        "long",
        "double precision",
        "float"
      ],
      "concepts": [
        "code",
        "codes",
        "stack",
        "instruction",
        "instructions",
        "program",
        "programs",
        "programming",
        "registers",
        "register"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.698,
          "base_score": 0.698,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.639,
          "base_score": 0.639,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.628,
          "base_score": 0.628,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.6,
          "base_score": 0.6,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.595,
          "base_score": 0.595,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stack",
          "xmm",
          "register",
          "double",
          "xmm register"
        ],
        "semantic": [],
        "merged": [
          "stack",
          "xmm",
          "register",
          "double",
          "xmm register"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4666040580154057,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.712849+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 331-351)",
      "start_page": 331,
      "end_page": 351,
      "summary": "The three ﬂoating-point arguments a, x, and b are passed in XMM registers\ntwo-instruction sequence is used to convert argument x to double (lines 2–3).\nThe function value is returned in register %xmm0.\nFor the following C function, the types of the four arguments are deﬁned by\nreturn p/(q+r) - s;\nWhen compiled, gcc generates the following code:\ndouble funct2(double w, int x, float y, long z);\nGcc generates the following code for the function:\ndouble funct2(double w, int x, float y, long z)\nw in %xmm0, x in %edi, y in %xmm1, z in %rsi\nThe code then reads the values from memory.\nThe relevant parts of the x86-64 assembly code are as follows:\nWe see that the function reads the value 1.8 from the memory location labeled\nas ﬂoating-point values?\nthe machine uses little-endian byte ordering, the ﬁrst value gives the low-order 4\nUsing Bitwise Operations in Floating-Point Code\nAt times, we ﬁnd gcc generating code that performs bitwise operations on XMM\nregisters to implement useful ﬂoating-point results.\nentire destination XMM register, applying the bitwise operation to all the data in\nConsider the following C function, where EXPR is a macro deﬁned with #define:\nreturn EXPR(x);\nBelow, we show the AVX2 code generated for different deﬁnitions of EXPR,\nwhere value x is held in %xmm0.\nﬂoating-point values.\nC.\nAVX2 provides two instructions for comparing ﬂoating-point values:\nset the condition codes to indicate their relative values.\nThe ﬂoating-point comparison instructions set three condition codes: the zero\nﬂag in Section 3.6.1, because it is not commonly found in gcc-generated x86 code.\noperation yielded a value where the least signiﬁcant byte has even parity (i.e.,\nThe condition codes are set as follows:\nconditionally jump when a ﬂoating-point comparison yields an unordered result.\nExcept for this case, the values of the carry and zero ﬂags are the same as those\n(a) C code\nif (x < 0)\nelse if (x == 0)\nelse if (x > 0)\n(b) Generated assembly code\nx in %xmm0\nCompare 0:x\nCompare x:0\nCompare x:0\nIllustration of conditional branching in ﬂoating-point code.\nAs an example of ﬂoating-point comparisons, the C function of Figure 3.51(a)\nclassiﬁes argument x according to its relation to 0.0, returning an enumerated type\nfunction values are: 0 (NEG), 1 (ZERO), 2 (POS), and 3 (OTHER).\noccurs when the value of x is NaN.\nGcc generates the code shown in Figure 3.51(b) for find_range.\nThe code\npoint constant 0.0 twice—once using vxorps, and once by reading the value from\nx < 0.0\nvalue of 0.\nx = 0.0\nx > 0.0\nreturn value of 2.\nx = NaN\ninstruction (line 11) and the following instruction will set %eax to 1.\ngets incremented by the addl instruction (line 13) to give a return value\ndouble funct3(int *ap, double b, long c, float *dp);\nFor this function, gcc generates the following code:\ndouble funct3(int *ap, double b, long c, float *dp)\nap in %rdi, b in %xmm0, c in %rsi, dp in %rdx\nObservations about Floating-Point Code\nWe see that the general style of machine code generated for operating on ﬂoating-\nBoth use a collection of registers to hold and operate on values, and they use these\nAVX2 code involves many more different instructions and formats than is usually\nC language to get a view of machine-level programming.\ngenerate an assembly-code representation of the machine-level program, we gain\nMachine-level programs, and their representation by assembly code, differ\nmust use multiple instructions to generate and operate on different data structures\nWe have only examined the mapping of C onto x86-64, but much of what we\nto C and generated object code by running a C compiler on the result.\npointers to the code implementing the methods.\nsentation known as Java byte code.\nThis code can be viewed as a machine-level\nInstead, software interpreters process the byte code,\nas just-in-time compilation dynamically translates byte code sequences into ma-\nThis approach provides faster execution when code is executed\nThe advantage of using byte code as the low-level\nrepresentation of a program is that the same code can be “executed” on many\ndifferent machines, whereas the machine code we have considered runs only on\nx86-64 machines.\n(1) all documentation is based on the Intel assembly-code format, (2) there are\nbinary interface (ABI) for x86-64 code running on Linux systems [77].\nother features that are required for machine-code programs to execute properly.\nsive reference on code-optimization techniques.\nwealth of information about buffer overﬂow and other attacks on code generated\nby C compilers.\nlong decode2(long x, long y, long z);\ngcc generates the following assembly code:\n%rdx, %rsi\n%rsi, %rdi\n%rsi, %rax\n%rdi, %rax\nParameters x, y, and z are passed in registers %rdi, %rsi, and %rdx.\nThe code\nstores the return value in register %rax.\nWrite C code for decode2 that will have an effect equivalent to the assembly\ncode shown.\nThe following code computes the 128-bit product of two 64-bit signed values x and\n*dest = x * (int128_t) y;\nGcc generates the following assembly code implementing the computation:\n%rsi, %rdx\nThis code uses three multiplications for the multiprecision arithmetic required\nto compute the product, and annotate the assembly code to show how it realizes\nHint: When extending arguments of x and y to 128 bits, they can\nShow how the code computes the values of ph and pl\nConsider the following assembly code:\nlong loop(long x, int n)\nx in %rdi, n in %esi\nThe preceding code was generated by compiling C code that had the following\nlong loop(long x, long n)\nlong result =\nYour task is to ﬁll in the missing parts of the C code to get a program equivalent\nto the generated assembly code.\nRecall that the result of the function is returned\nYou will ﬁnd it helpful to examine the assembly code before,\nA. Which registers hold program values x, n, result, and mask?\nB. What are the initial values of result and mask?\nFill in all the missing parts of the C code.\nIn Section 3.6.6, we examined the following code as a candidate for the use of\ngenerated code should use a conditional move instruction rather than one of the\nThe code that follows shows an example of branching on an enumerated type\nIn our code, the actions associated\ntypedef enum {MODE_A, MODE_B, MODE_C, MODE_D, MODE_E} mode_t;\nlong result = 0;\ncase MODE_C:\nThe part of the generated assembly code implementing the different actions is\nvalues, and the case labels for the different jump destinations.\nFill in the missing parts of the C code.\ndisassembled machine code.\nlong switch_prob(long x, long n) {\nlong result = x;\n/* Fill in code here */\np1 in %rdi, p2 in %rsi, action in %edx\n(%rsi), %rax\n%rdx, (%rsi)\n(%rdi), %rax\n(%rsi), %rax\nMODE_C\n(%rsi), %rax\n(%rsi), %rax\nAssembly code for Problem 3.62.\nThis code implements the different\nFigure 3.53 shows the disassembled machine code for the procedure.\n0x400708:\n0x400718:\nFill in the body of the switch statement with C code that will have the same\nbehavior as the machine code.\nlong switch_prob(long x, long n)\nx in %rdi, n in %rsi\n$0x5,%rsi\n4005c3 <switch_prob+0x33>\n0x0(,%rdi,8),%rax\nc3\n%rdi,%rax\n48 c1 f8 03\n$0x3,%rax\nc3\n48 c1 e0 04\n$0x4,%rax\n48 89 c7\n4005c3:\n4005c7:\nc3\nDisassembled code for Problem 3.63.\nConsider the following source code, where R, S, and T are constants declared with\nlong A[R][S][T];\nIn compiling this program, gcc generates the following assembly code:\ni in %rdi, j in %rsi, k in %rdx, dest in %rcx\n%rdi, %rsi\n%rsi, %rdi\nB. Use your reverse engineering skills to determine the values of R, S, and T\nbased on the assembly code.\nThe following code transposes the elements of an M × M array, where M is a\nWhen compiled with optimization level -O1, gcc generates the following code\n(%rax), %rsi\n%rsi, (%rdx)\nWe can see that gcc has converted the array indexing to pointer code.\nC. What is the value of M?\nConsider the following source code, where NR and NC are macro expressions de-\nThis code computes the sum of the elements of column j of the array.\nlong result = 0;\nIn compiling this program, gcc generates the following assembly code:\nn in %rdi, A in %rsi, j in %rdx\n(%rsi,%rdx,8), %rcx\nFor this exercise, we will examine the code generated by gcc for functions that have\nstructures as arguments and return values, and from this see how these language\nThe following C code has a function process having structures as argument\nand return values, and a function eval that calls process:\nlong eval(long x, long y, long z) {\ns.a[0] = x;\nGcc generates the following code for these two functions:\nlong eval(long x, long y, long z)\nx in %rdi, y in %rsi, z in %rdx\nA. We can see on line 2 of function eval that it allocates 104 bytes on the stack.\nB. What value does eval pass in its call to process?\nC. How does the code for process access the elements of structure argument s?\nD. How does the code for process set the ﬁelds of result structure r?\naccesses the elements of structure r following the return from process.\npassed as function arguments and how they are returned as function results?\nIn the following code, A and B are constants deﬁned with #define:\nint x[A][B]; /* Unknown constants A and B */\nGcc generates the following code for setVal:\np in %rdi, q in %rsi\n8(%rsi), %rax\n32(%rsi), %rax\nWhat are the values of A and B?\nfollowing code:\n0x120(%rsi),%ecx\nc:\n48 8d 04 c6\n0x8(%rax),%rdx\n48 63 c9\n48 89 4c d0 10\n%rcx,0x10(%rax,%rdx,8)\n1c:\nc3\nin this structure are idx and x, and that both of these contain signed values.\nlong x;\nThe following function (with some expressions omitted) operates on a linked\ne2.x\nC. The compiler generates the following assembly code for proc:\nYour code should also check for error conditions\nFigure 3.54(a) shows the code for a function that is similar to function vfunct\n(a) C code\n(b) Portions of generated assembly code\nn in %rdi, idx in %rsi, q in %rdx\nCode for Problem 3.72.",
      "keywords": [
        "code",
        "long",
        "rax",
        "rdi",
        "rsi",
        "movq",
        "assembly code",
        "rdx",
        "function",
        "MODE",
        "result",
        "ret",
        "double",
        "rsp",
        "Generated assembly code"
      ],
      "concepts": [
        "code",
        "codes",
        "long",
        "values",
        "functions",
        "instruction",
        "instructions",
        "programming",
        "program",
        "programs"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 18,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.813,
          "base_score": 0.663,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.717,
          "base_score": 0.717,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "rsi",
          "long",
          "rdi",
          "assembly code"
        ],
        "semantic": [],
        "merged": [
          "code",
          "rsi",
          "long",
          "rdi",
          "assembly code"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4720241797534822,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712905+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 352-373)",
      "start_page": 352,
      "end_page": 373,
      "summary": "Figure 3.54(b) shows the part of the assembly code that sets up the frame\nD. What alignment properties does this code guarantee for the values of s2\nWrite a function in assembly code that matches the behavior of the function find_\nTest your code on all 232 possible argument values.\non page 214 describes how to incorporate functions written in assembly code into\nWrite a function in assembly code that matches the behavior of the function find_\nyour code on all 232 possible argument values.\ndescribes how to incorporate functions written in assembly code into C programs.\ndouble c_imag(double complex x) {\nreturn cimag(x);\ndouble c_real(double complex x) {\nreturn creal(x);\ndouble complex c_sub(double complex x, double complex y) {\nreturn x - y;\nWhen compiled, gcc generates the following assembly code for these func-\ndouble c_imag(double complex x)\ndouble c_real(double complex x)\ndouble complex c_sub(double complex x, double complex y)\nB. How are complex values returned from a function?\n0x100\n0x104\n$0x108\n0x108\nAddress 0x100\nAddress 0x104\n0x11\n0x13\nAddress 0x108\nAddress 0x100\n0x11\nAs we have seen, the assembly code generated by gcc includes sufﬁxes on the\nreferences in x86-64 are always given with quad word registers, such as %rax, even\nSince we will rely on gcc to generate most of our assembly code, being able to\nmovl %eax,$0x123\nto reverse the effect of the C compiler to determine what C code gave rise to this\nassembly code.\nThe best way is to run a “simulation,” starting with values x, y, and\nxp in %rdi, yp in %rsi, zp in %rdx\nGet x = *xp\nStore x at yp\n%rax, (%rdi)\nFrom this, we can generate the following C code:\nlong x = *xp;\n*yp = x;\nbetween C code and the generated assembly code.\nshort scale3(short x, short y, short z)\nx in %rdi, y in %rsi, z in %rdx\n10 * y + z + y * x\nshort t = 10 * y + z + y * x;\n0x100\n0x100\n0x108\n0x118\n0x110\n0x110\n0x14\n0x0\nThis exercise gives you a chance to generate a little bit of assembly code.\ncan then use byte register %cl to specify the shift amount for the sarq instruction.\nIt might seem odd to use a movl instruction, given that n is eight bytes long, but\nlong shift_left4_rightn(long x, long n)\nx in %rdi, n in %rsi\n%rdi, %rax\nGet x\nx <<= 4\nx >>= n\nThis problem is fairly straightforward, since the assembly code follows the struc-\nture of the C code closely.\nshort p1 = y | z;\nA. This instruction is used to set register %rcx to zero, exploiting the property\nIt corresponds to the C statement x = 0.\nB. A more direct way of setting register %rcx to zero is with the instruction\nC. Assembling and disassembling this code, however, we ﬁnd that the version\nfollowing code:\nvoid uremdiv(unsigned long x, unsigned long y,\nx in %rdi, y in %rsi, qp in %rdx, rp in %rcx\n%rdi, %rax\nMove x to lower 8 bytes of dividend\nIt is important to understand that assembly code does not keep track of the type\nsequences back to C code, we must do a bit of detective work to infer the data\nB. The sufﬁx ‘w’ and the register identiﬁers indicate 16-bit operands, while the\nC. The sufﬁx ‘b’ and the register identiﬁers indicate 8-bit operands, while\nB. The sufﬁx ‘w’ and the register identiﬁer indicate a 16-bit operand, while the\nC. The sufﬁx ‘b’ and the register identiﬁer indicate an 8-bit operand, while the\nA. The je instruction has as its target 0x4003fc + 0x02.\ncode shows, this is 0x400425:\nis at absolute address 0x400547.\nbe at an address 0x2 bytes beyond that of the pop instruction.\nthese gives address 0x400545.\nrequires 2 bytes, it must be located at address 0x400543.\nthe nop instruction) gives address 0x400560:\nAnnotating assembly code and writing C code that mimics its control ﬂow are good\nA. Here is the C code:\nof the code:\nlong gotodiff_se_alt(long x, long y) {\nif (x < y)\ngoto x_lt_y;\nresult = x - y;\nx_lt_y:\ny - x;\nwhole, the machine code is a straightforward translation of the C code.\nshort test(short x, short y, short z) {\nshort val = z+y-x;\nval = x/z;\nval = x/y;\nB. Here is an annotated version of the assembly code:\nshort arith(short x)\nx in %rdi\ntemp = x+15\nText x\nresult = temp >> 4 (= x/16)\n4 to generate x/16.\nto ﬁt this code into the framework of the original C code, you will ﬁnd that it follows\nshort test(short x, short y) {\nif (x < 0) {\nif (x < y)\nval = x * y;\nval = x | y;\nval = x / y;\nin Problem 2.35, when we get value x while attempting to compute n!, we\ncan test for overﬂow by computing x/n and seeing whether it equals (n −1)!\nB. Doing the computation with data type long lets us go up to 20!, thus the 14!\nThe code generated when compiling loops can be tricky to analyze, because the\ncompiler can perform many different optimizations on loop code, and because it\ndemonstrates several places where the assembly code is not just a direct translation\nof the C code.\nA. Although parameter x is passed to the function in register %rdi, we can see\ncan see that registers %rbx, %rcx, and %rdx are initialized in lines 2–5 to x,\nB. The compiler determines that pointer p always points to x, and hence the\nexpression (*p)+=5 simply increments x.\nC. The annotated code is as follows:\nshort dw_loop(short x)\nx initially in %rdi\nCopy x to %rbx\nCompute y = x/9\n(,%rdi,4), %rdx\nCompute n = 4*x\nCompute y += x + 5\nThis assembly code is a fairly straightforward translation of the loop using the\nThe full C code is as follows:\ntranslation, we can see that it is equivalent to the following C code:\nlong result = b;\nBeing able to work backward from assembly code to C code is a prime example\nA. We can see that the code uses the jump-to-middle translation, using the jmp\nB. Here is the original C code:\nshort test_one(unsigned short x) {\nwhile (x) {\nval ^= x;\nx >>= 1;\nC. This code computes the parity of argument x.\nThis problem is trickier than Problem 3.26, since the code within the loop is more\nA. Here is the original C code:\nlong fun_b(unsigned long x) {\nx >>= 1;\nB. The code was generated using the guarded-do transformation, but the com-\nC. This code reverses the bits in x, creating a mirror image.\nA. Applying our translation rule would yield the following code:\nseveral places in the assembly code.\n. Line 2 of the assembly code adds 2 to x to set the lower range of the cases to\nvalue 5) have the same destination (.L2) as the jump instruction on line 4,\nB. The case with destination .L5 has labels −1 and 6.\nC. The case with destination .L7 has labels 1 and 3.\ninformation from the assembly code and the jump table to sort out the different\nWe can see from the ja instruction (line 3) that the code for the default case\n.L5, and so this must be the code for the cases C and D.\nWe can see that the code\nThe original C code is as follows:\nvoid switcher(long a, long b, long c, long *dest)\nc = b ^ 15;\nval = (c + b) << 2;\nhow results are returned via register %rax.\n0x400560\n0x400548\n0x400565\n0x400565\n0x400550\n0x400565\n0x400540\n0x400555\n0x400543\n0x400555\n0x400547\n0x400555\n0x400555\n0x400565\n0x400565\nsee that the low-order byte of argument b is added to the byte pointed to by %rcx.\nbe 1, 2, 4, or 8 bytes long.\nint procprobl(int a, short b, long *u, char *v)\na in %edi, b in %si, u in %rdx, v in %rcx\n%rdi, (%rdx)\ntwo sums were computed in the assembly code in the opposite ordering as they are\nin the C code.\nint procprob(int b, short a, long *v, char *u);\nThis problem provides a chance to examine the code for a recursive function.\nA. Register %rbx holds the value of parameter x, so that it can be used to\nB. The assembly code was generated from the following C code:\nlong rfun(unsigned long x) {\nif (x == 0)\nunsigned long nx = x>>2;\nreturn x + rv;\nData type short requires 2 bytes, while\ndata type short requires 2 bytes, all of the array indices are scaled by a factor of\nAssembly Code\nleaq 6(%rdx,%rcx,2),%rax\nleaq 4(%rdx,%rcx,2),%rax\nﬁrst step is to annotate the assembly code to determine how the address references\n0(,%rdi,8), %rdx\n%rdi, %rdx\n%rax, %rdi\nQ(,%rdi,8), %rax\nP(,%rdx,8), %rax\n. For L = 4, C = 16, and j = 0, pointer Aptr is computed as xA + 4 .\n. For L = 4, C = 16, i = 0, and j = k, Bptr is computed as xB + 4 .\n. For L = 4, C = 16, i = 16, and j = k, Bend is computed as xB + 4 .\nThis exercise requires that you be able to study compiler-generated assembly code\nLet us ﬁrst study the following C code, and then see how it is derived from the\nassembly code generated for the original function.\nThe actual assembly code follows this general form, but now the pointer\nWe label register %rax as holding a value\nA in %rdi, val in %rsi\nThis problem gets you to think about structure layout and the code used to access\ns.x\nB. It uses 20 bytes.\nC. As always, we start by annotating the assembly code:\nGet st->s.x\n10(%rdi), %rax\n%rax, (%rdi)\nFrom this, we can generate C code as follows:\n= st->s.x;\nassembly code, recognizing that the two ﬁelds of the structure are at offsets 0\n(%rdi), %rax\nA. Based on the annotated code, we can generate a C version:\nCode\nmovl (%rdi,%rax,4), %eax\ncode generated by the compiler for accessing structures.\nA. struct P1 { short i; int c; int *j; short *d; };\nB. struct P2 { int i[2]; char c[8]; short [4]; long *j; };\nC. struct P3 { long w[2]; int *c[2] };\nB. The structure is a total of 56 bytes long.",
      "keywords": [
        "Solution to Problem",
        "Problem",
        "code",
        "assembly code",
        "long",
        "rax",
        "rdi",
        "Solution",
        "short",
        "assembly",
        "bytes",
        "instruction",
        "rdx",
        "val",
        "rsi"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "long",
        "code",
        "solutions",
        "solution",
        "short",
        "byte",
        "bytes",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.694,
          "base_score": 0.544,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.628,
          "base_score": 0.478,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 35,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "short",
          "assembly",
          "assembly code",
          "rdi"
        ],
        "semantic": [],
        "merged": [
          "code",
          "short",
          "assembly",
          "assembly code",
          "rdi"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37659453862235137,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.712996+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 374-394)",
      "start_page": 374,
      "end_page": 394,
      "summary": "C. The program is attempting to return to address 0x040034.\nD. The saved value of register %rbx was set to 0x3332313039383736.\nThis problem gives you another chance to see how x86-64 code manages the stack,\nA. The leaq instruction of line 5 computes the value 8n + 22, which is then\nrounded down to the nearest multiple of 16 by the andq instruction of line 6.\nB. The three instructions in this sequence round s2 up to the nearest multiple\nwhich conversion and data movement instructions are used.\nThe two instructions at lines 10–11\nconvert this to double precision as the value returned in register %xmm0.\nInstruction(s)\nRegisters: a in %xmm0, b in %rdi c in %xmm1, d in %esi\nRegisters: a in %edi, b in %rsi, c in %rdx, d in %rcx\nRegisters: a in %rdi, b in %xmm0, c in %esi, d in %xmm1\nRegisters: a in %xmm0, b in %rdi, c in %xmm1, d in %xmm2\nFrom this we see that the code computes the value i1/(i2+f1)-f2.\ndouble funct2(double w, int x, float y, long z)\nw in %xmm0, x in %edi, y in %xmm1, z in %rsi\nConvert x*y to double\nWe see that the two values are 0 and 1077936128 (0x40400000).\nB. We see that the vxorpd instruction sets the entire register to zero, and so this\nThe Y86-64 Instruction Set Architecture\ninstructions, where each instruction performs some primitive operation, such as\nAn instruction is encoded in binary form as a sequence of\nThe instructions supported by a particular processor and their\nbyte-level encodings are known as its instruction set architecture (ISA).\nand the ARM processor family, have different ISAs. A program compiled for one\ntions are permitted and how they are encoded, and processor designers, who must\nbuild machines that execute those instructions.\nstudy the way a hardware system can execute the instructions of a particular ISA.\nto imply sequential instruction execution, where each instruction is fetched and\nof multiple instructions simultaneously, the processor can achieve higher perfor-\nmance than if it executed just one instruction at a time.\n. Although few people design processors, many design hardware systems that\n. You just might work on a processor design.\nIn this chapter, we start by deﬁning a simple instruction set that we use as a\ninstruction set, because it was inspired by the x86-64 instruction set.\nwith x86-64, the Y86-64 instruction set has fewer data types, instructions, and\ncode less compact than the comparable x86-64 code, but also much easier to design\nEven though the Y86-64 instruction set is very simple,\nDesigning a processor to implement Y86-64 requires us to deal with many of the\nAs a ﬁrst step in designing a processor, we present a functionally correct,\nbut somewhat impractical, Y86-64 processor based on sequential operation.\nprocessor executes a complete Y86-64 instruction on every clock cycle.\nInstructions progress through the stages of the pipeline, with one in-\nbe executing the different steps of up to ﬁve instructions simultaneously.\ning this processor preserve the sequential behavior of the Y86-64 ISA requires\ninstruction depend on those of other instructions that are still in the pipeline.\nprocessor designs.\nY86-64 programs on your machine, and simulators for two sequential and one\npipelined processor design.\ninvolve implementing new instructions and modifying how the machine processes\ninstructions.\nby processor designers.\nY86-64 processor in the Verilog hardware description language.\nThe Y86-64 Instruction Set Architecture\nDeﬁning an instruction set architecture, such as Y86-64, includes deﬁning the\ndifferent components of its state, the set of instructions and their encodings, a\nAs Figure 4.1 illustrates, each instruction in a Y86-64 program can read and modify\nin assembly code or a compiler generating machine-level code.\nThe state for Y86-64 is similar to that for x86-64.\nThere are 15 program registers:\nthe x86-64 register %r15 to simplify the instruction encoding.) Each of these stores\nreturn instructions.\nx86-64, programs for Y86-\nthe program registers,\ncodes\nabout the effect of the most recent arithmetic or logical instruction.\ncounter (PC) holds the address of the instruction currently being executed.\nThe memory is conceptually a large array of bytes, holding both program\nY86-64 programs reference memory locations using virtual addresses.\nwe can think of the virtual memory system as providing Y86-64 programs with an\nA ﬁnal part of the program state is a status code Stat, indicating the overall\nsort of exception has occurred, such as when an instruction attempts to read\nY86-64 Instructions\nFigure 4.2 gives a concise description of the individual instructions in the Y86-64\nWe use this instruction set as a target for our processor implementations.\nset of Y86-64 instructions is largely a subset of the x86-64 instruction set.\nof the instructions on the left and the byte encodings on the right.\nfurther details of some of the instructions.\nHere are some details about the Y86-64 instructions.\n. The x86-64 movq instruction is split into four different instructions: irmovq,\nIt is designated by the ﬁrst character in the instruction name.\nin the instruction name.\nThe memory references for the two memory movement instructions have\nregister or any scaling of a register’s value in the address computation.\n. There are four integer operation instructions, shown in Figure 4.2 as OPq. These are addq, subq, andq, and xorq.\nwhereas x86-64 also allows operations on memory data.\nThese instructions\nY86-64 instruction set.\nInstruction encodings range between 1 and 10\nAn instruction consists of a 1-byte instruction speciﬁer, possibly a 1-byte register\n. The seven jump instructions (shown in Figure 4.2 as jXX) are jmp, jle, jl, je,\njne, jge, and jg.\nsettings of the condition codes.\n. There are six conditional move instructions (shown in Figure 4.2 as cmovXX):\ncmovle, cmovl, cmove, cmovne, cmovge, and cmovg.\nformat as the register–register move instruction rrmovq, but the destination\nregister is updated only if the condition codes satisfy the required constraints.\n. The call instruction pushes the return address on the stack and jumps to the\nThe ret instruction returns from such a call.\n. The pushq and popq instructions implement push and pop, just as they do in\n. The halt instruction stops instruction execution.\ninstruction, called hlt.\nthis instruction, since it causes the entire system to suspend operation.\nY86-64, executing the halt instruction causes the processor to stop, with the\nInstruction Encoding\nFigure 4.2 also shows the byte-level encoding of the instructions.\nEach instruction\ninstruction has an initial byte identifying the instruction type.\nvalues are signiﬁcant only for the cases where a group of related instructions share\nthe integer operation, branch, and conditional move instructions.\nrrmovq has the same instruction code as the conditional moves.\nThe numbering of registers in Y86-\nThe program registers are stored within the\nID value 0xF is used in the instruction encodings and within our\nhardware designs when we need to indicate that no register should be accessed.\nSome instructions are just 1 byte long, but those that require operands have\nAs the assembly-code versions of the instructions show, they can specify the\nan address computation, depending on the instruction type.\nInstructions that have\nFunction codes for Y86-64 instruction set.\nThese instructions are\nY86-64 program register identiﬁers.\nEach of the 15 program registers\ninstruction indicates the absence of a register operand.\nthe other register speciﬁer set to value 0xF.\nSome instructions require an additional 8-byte constant word.\ncompact encodings of branch instructions and to allow code to be shifted from\nWhen the instruction is written in disassembled form, these bytes appear in reverse\nAs an example, let us generate the byte encoding of the instruction rmmovq\nUsing the register numbers in Figure 4.4, we get a register speciﬁer byte of 42.\nCombining these, we get an instruction encoding of 4042cdab896745230100.\nOne important property of any instruction set is that the byte encodings must\nunique instruction sequence or is not a legal byte sequence.\nY86-64, because every instruction has a unique combination of code and function\ncode program without any ambiguity about the meaning of the code.\ncode is embedded within other bytes in the program, we can readily determine\nComparing x86-64 to Y86-64 instruction encodings\nCompared with the instruction encodings used in x86-64, the encoding of Y86-64 is much simpler but\nThe register ﬁelds occur only in ﬁxed positions in all Y86-64 instructions, whereas\nthey are packed into various positions in the different x86-64 instructions.\nAn x86-64 instruction can\nencode constant values in 1, 2, 4, or 8 bytes, whereas Y86-64 always requires 8 bytes.\nthe instruction sequence as long as we start from the ﬁrst byte in the sequence.\nmachine-level programs directly from object-code byte sequences.\nDetermine the byte encoding of the Y86-64 instruction sequence that follows.\nline .pos 0x100 indicates that the starting address of the object code should be\n# Start code at address 0x100\nFor each byte sequence listed, determine the Y86-64 instruction sequence it en-\ncodes.\nIf there is some invalid byte in the sequence, show the instruction sequence\nRISC and CISC instruction sets\nx86-64 is sometimes labeled as a “complex instruction set computer” (CISC—pronounced “sisk”),\nand is deemed to be the opposite of ISAs that are classiﬁed as “reduced instruction set computers”\nBy the early 1980s, instruction sets for mainframe and minicomputers had grown quite large,\nas machine designers incorporated new instructions to support high-level tasks, such as manipulating\nappeared in the early 1970s and had limited instruction sets, because the integrated-circuit technology\nquickly and, by the early 1980s, were following the same path of increasing instruction set complexity\nThe x86 line continues to evolve as new classes of instructions are\nCocke, recognized that they could generate efﬁcient code for a much simpler form of instruction set.\nfact, many of the high-level instructions that were being added to instruction sets were very difﬁcult to\nA simpler instruction set could be implemented with\nhave a special designation for a nearly universal form of instruction set.\nWhen comparing CISC with the original RISC instruction sets, we ﬁnd the following general\nA large number of instructions.\ninstructions [51] is over 1,200 pages long.\nSome instructions with long execution times.\nThese include instructions that copy an entire\nNo instruction with a long execution time.\ninteger multiply instruction, requiring compilers\nx86-64 instructions can\nTypically all instructions\nRISC and CISC instruction sets (continued)\nallowed by load instructions, reading from\nmemory into a register, and store instructions,\nparticular instruction sequences and have\ninstruction is executed.\nside effect of instructions and then used for\ninstructions store the test results in normal\nThe Y86-64 instruction set includes attributes of both CISC and RISC instruction sets.\nCISC side, it has condition codes and variable-length instructions, and it uses the stack to store return\nOn the RISC side, it uses a load/store architecture and a regular instruction encoding, and it\nIt can be viewed as taking a CISC instruction set (x86)\nversus CISC instruction sets.\na given amount of hardware through a combination of streamlined instruction set design, advanced\nCISC instructions were required to perform a given task, and so their machines could achieve higher\nand introduced more instructions, many of which take multiple cycles to execute.\nset machine.” The idea of exposing implementation artifacts to machine-level programs proved to be\nmany of these artifacts became irrelevant, but they still remained part of the instruction set.\ncore of RISC design is an instruction set that is well suited to execution on a pipelined machine.\nwill discuss in Section 5.7, they fetch the CISC instructions and dynamically translate them into a\nFor example, an instruction that adds a register to memory\nin advance of the actual instruction execution, the processor can sustain a very high execution rate.\nsuccess of different instruction sets.\nx86 made it easy to keep moving from one generation of processor to the next.\ntechnology improved, Intel and other x86 processor manufacturers could overcome the inefﬁciencies\ncreated by the original 8086 instruction set design, using RISC techniques to produce performance\nThe programmer-visible state for Y86-64 (Figure 4.1) includes a status code Stat\nCode value 1, named AOK, indicates that the program\nhalt instruction encountered\nInvalid instruction encountered\nY86-64 status codes.\nIn our design, the processor halts for any code other\nCode 2, named HLT, indicates that the processor has executed a halt\ninstruction.\nCode 3, named ADR, indicates that the processor attempted to read\nfrom or write to an invalid memory address, either while fetching an instruction\nCode 4, named INS, indicates that an invalid instruction\nFor Y86-64, we will simply have the processor stop executing instructions\nY86-64 Programs\nFigure 4.6 shows x86-64 and Y86-64 assembly code for the following C function:\nThe x86-64 code was generated by the gcc compiler.\nThe Y86-64 code is\n. The Y86-64 code loads constants into registers (lines 2–3), since it cannot use\nimmediate data in arithmetic instructions.\nx86-64 code\nY86-64 code\nComparison of Y86-64 and x86-64 assembly programs.\nThe Y86-64 code follows the same general pattern\nas the x86-64 code.\n. The Y86-64 code requires two instructions (lines 8–9) to read a value from\nmemory and add it to a register, whereas the x86-64 code can do this with a\nsingle addq instruction (line 5).\n. Our hand-coded Y86-64 implementation takes advantage of the property that\nthe subq instruction (line 11) also sets the condition codes, and so the testq\ninstruction of the gcc-generated code (line 9) is not required.\nthough, the Y86-64 code must set the condition codes prior to entering the\nloop with an andq instruction (line 5).\nThe program contains both data and instructions.\nbegin generating code starting at address 0.\nY86-64 programs.\nThe next instruction (line 3) initializes the stack pointer.\nindicate address 0x200 using a .pos directive (line 39).\nstack does not grow so large that it overwrites the code or other program data.\nAs this example shows, since our only tool for creating Y86-64 code is an\nOn lines of the assembly ﬁle that contain instructions or data, the object\ncode contains an address, followed by the values of between 1 and 10 bytes.\nWe have implemented an instruction set simulator we call yis, the purpose\nof which is to model the execution of a Y86-64 machine-code program without",
      "keywords": [
        "instruction",
        "Instruction Set",
        "code",
        "CISC instruction sets",
        "Processor",
        "register",
        "RISC",
        "Instruction Set Architecture",
        "program",
        "memory",
        "CISC instruction",
        "double",
        "registers",
        "instruction set design",
        "Problem"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "code",
        "codes",
        "processor",
        "processors",
        "bytes",
        "byte",
        "program",
        "programs"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.821,
          "base_score": 0.671,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.717,
          "base_score": 0.717,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.701,
          "base_score": 0.551,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "y86",
          "y86 64",
          "instructions",
          "64"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "y86",
          "y86 64",
          "instructions",
          "64"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44859747106229375,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713067+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 395-412)",
      "start_page": 395,
      "end_page": 412,
      "summary": ".quad 0xa000a000a000\nSample program written in Y86-64 assembly code.\n.quad 0xa000a000a000\nIn printing register and memory\nvalues, it only prints out words that change during simulation, either in registers\nWe can see in this output that register\nIn addition, we can see that the stack, which starts at address 0x200\n0x090, and so the pushing and popping of values on the stack did not corrupt the\nOne common pattern in machine-level programs is to add a constant value to a\nregister.\nirmovq instruction to set a register to the constant, and then an addq instruction to\nadd this value to the destination register.\nThis instruction adds the constant value V to register rB.\nRewrite the Y86-64 sum function of Figure 4.6 to make use of the iaddq\nWrite Y86-64 code to implement a recursive product function rproduct, based\nUse the same argument passing and register saving conventions as x86-64 code\nYou might ﬁnd it helpful to compile the C code on an x86-64 machine and\nModify the Y86-64 code for the sum function (Figure 4.6) to implement a function\nModify the Y86-64 code for the sum function (Figure 4.6) to implement a function\nMost Y86-64 instructions transform the program state in a straightforward man-\nThe pushq instruction both decrements the stack pointer by 8 and writes a\nregister value to memory.\ndo when executing the instruction pushq %rsp, since the register being pushed is\nLet us determine the behavior of the instruction pushq %rsp for an x86-64 pro-\nasm:easm on page 214 describes how to write programs that combine C code with\ndoes this imply about the behavior of the instruction pushq %rsp under x86-64?\n%rsp to the value read from memory or to the incremented stack pointer.\nhandle this instruction, and then design our Y86-64 machine to follow the same\nFor IA-32 processors from the Intel 286 on, the PUSH ESP instruction pushes the value of the ESP\nregister as it existed before the instruction was executed.\nPUSH SP instruction pushes the new value of the SP register (that is the value after it has been\ndepending on what mode an x86 processor operates under, it will do different things when instructed to\npush the stack pointer register.\nLogic Design and the Hardware Control Language HCL\nIn hardware design, electronic circuits are used to compute functions on bits and\ntechnology represents different bit values as high or low voltages on signal wires.\nto compute functions on the bits, memory elements to store bits, and clock signals\nwe use to describe the control logic of the different processor designs.\nAt one time, hardware designers created circuit designs by drawing schematic diagrams of logic circuits\nprograms that could generate efﬁcient circuit designs from HDL descriptions.\nwriting programs in assembly code to writing them in a high-level language and having a compiler\nOur HCL language expresses only the control portions of a hardware design, with only a limited set\nfunction of its inputs.\nLogic gates are the basic computing elements for digital circuits.\noutput equal to some Boolean function of the bit values at their inputs.\nexpressions are shown below the gates for the operators in C (Section 2.1.8): &&\nWe use these instead of the bit-level C operators\n&, |, and ~, because logic gates operate on single-bit quantities, not entire words.\nthese in HCL using binary operators, though, so the operation of a three-input\nand gate with inputs a, b, and c is described with the HCL expression a && b && c.\nCombinational Circuits and HCL Boolean Expressions\n. Every logic gate input must be connected to exactly one of the following:\nconnection of some memory element, or (3) the output of some logic gate.\n. The outputs of two or more logic gates cannot be connected together.\nIt has two inputs, a and b.\nThis code simply deﬁnes the bit-level (denoted by data type bool) signal eq as a\nfunction of inputs a and b.\nWrite an HCL expression for a signal xor, equal to the exclusive-or of inputs a\nequal input a if the control\ninput b when s is 0.\nselects a value from among a set of different data signals, depending on the value\nof a control input signal.\nIn this single-bit multiplexor, the two data signals are the\ninput bits a and b, while the control signal is the input bit s.\nThe upper and gate passes signal b when s is 0 (since the other input to the gate\nHCL expression for the output signal, using the same operations as are present in\nlogic circuits and logical expressions in C.\ncompute functions over their inputs.\n. Since a combinational circuit consists of a series of logic gates, it has the\nsome input to the circuit changes, then after some delay, the outputs will\n. Logical expressions in C allow arguments to be arbitrary integers, interpreting\nover the bit values 0 and 1.\n. Logical expressions in C have the property that they might only be partially\nWord-level equality test circuit.\nThe output will equal 1 when each bit\nWord-level equality is one of the\nWord-Level Combinational Circuits and HCL Integer Expressions\nthat operate on data words.\nintegers, addresses, instruction codes, and register identiﬁers.\nCombinational circuits that perform word-level computations are constructed\nusing logic gates to compute the individual bits of the output word, based on the\nindividual bits of the input words.\ncircuit that tests whether two 64-bit words A and B are equal.\ncircuit is implemented using 64 of the single-bit equality circuits shown in Figure\nThe outputs of these single-bit circuits are combined with an and gate to\nIn HCL, we will declare any word-level signal as an int, without specifying\nwords to be compared for equality, and so the functionality of the circuit shown\nin Figure 4.12 can be expressed at the word level as\nAs is shown on the right side of Figure 4.12, we will draw word-level circuits\nbits of the word, and we will show a single-bit signal as a dashed line.\nSuppose you want to implement a word-level equality circuit using the exclusive-\nor circuits from Problem 4.9 rather than from bit-level equality circuits.\nsuch a circuit for a 64-bit word consisting of 64 bit-level exclusive-or circuits and\nFigure 4.13 shows the circuit for a word-level multiplexor.\nates a 64-bit word Out equal to one of the two input words, A or B, depending on\nthe control input bit s.\nreplicating the bit-level multiplexor 64 times, the word-level version reduces the\nWord-level multiplexor circuit.\nThe output will equal input word A when\nthe control signal s is 1, and it will equal B otherwise.\nMultiplexing functions are described in HCL using case expressions.\nLogically, the selection expressions are eval-\nthe word-level multiplexor of Figure 4.13 can be described in HCL as\nIn this code, the second selection expression is simply 1, indicating that this\nAllowing nonexclusive selection expressions makes the HCL code more read-\ntrolling which input word should be passed to the output, such as the signals s and\nTo translate an HCL case expression into hardware, a logic syn-\ncircuit selects from among the four input words A, B, C, and D based on the control\nthis in HCL using Boolean expressions to describe the different combinations of\npossibility having s1 equal to 0 was given as the ﬁrst selection expression.\nAs a ﬁnal example, suppose we want to design a logic circuit that ﬁnds the\nminimum value among a set of words A, B, and C, diagrammed as follows:\nThe HCL code given for computing the minimum of three words contains four\ncomparison expressions of the form X <= Y.\ninput, the circuit will perform one of four different arithmetic and logical operations.\nWrite HCL code describing a circuit that for word inputs A, B, and C selects the\nThat is, the output equals the word lying between the\nCombinational logic circuits can be designed to perform many different types\nof operations on word-level data.\nIn our version, the circuit has three inputs: two data inputs labeled A and B and\na control input.\nDepending on the setting of the control input, the circuit will\nperform different arithmetic or logical operations on the data inputs.\ninteger operations supported by the Y86-64 instruction set, and the control values\nmatch the function codes for these instructions (Figure 4.3).\nbits from a 2-bit signal code, as follows:\nIn this circuit, the 2-bit signal code would then control the selection among the\nfour data words A, B, C, and D.\nWe can express the generation of signals s1 and s0\nusing equality tests based on the possible values of code:\nthey simply react to the signals at their inputs, generating outputs equal to some\nfunction of the inputs.\nClocked registers (or simply registers) store individual bits or words.\nsignal controls the loading of the register with the value at its input.\nan address to select which word should be read or written.\na processor, where a combination of hardware and operating system\nIn a Y86-64 processor, the register ﬁle holds the\n15 program registers (%rax through %r14).\nregister is directly connected to the rest of the circuit by its input and output\nIn machine-level programming, the registers represent a small collection\nof addressable words in the CPU, where the addresses consist of register IDs. These words are generally stored in the register ﬁle, although we will see that the\nhardware can sometimes pass a word directly from one instruction to another to\nInput = y\nOutput = x\nRegister operation.\nWhen the clock rises, the values at the register inputs are\navoid the delay of ﬁrst writing and then reading the register ﬁle.\nx), generating an output equal to its current state.\ncombinational logic preceding the register, creating a new value for the register\ninput (shown as y), but the register output remains ﬁxed as long as the clock is low.\nAs the clock rises, the input signals are loaded into the register as its next state\n(y), and this becomes the new register output until the next rising clock edge.\nkey point is that the registers serve as barriers between the combinational logic\nValues only propagate from a register input to its\nuse clocked registers to hold the program counter (PC), the condition codes (CC),\nRegister\nThis register ﬁle has two read ports, named A and B, and one write port, named\nIn the register ﬁle diagrammed, the circuit\ncan read the values of two program registers and update the state of a third.\nport has an address input, indicating which program register should be selected,\nand a data output or input giving a value for that program register.\nThe register ﬁle is not a combinational circuit, since it has internal storage.\nour implementation, however, data can be read from the register ﬁle as if it were\na block of combinational logic having addresses as inputs and the data as outputs.\nvalue stored in the corresponding program register will appear on either valA or\nFor example, setting srcA to 3 will cause the value of program register %rbx\nto be read, and this value will appear on output valA.\nThe writing of words to the register ﬁle is controlled by the clock signal in\na manner similar to the loading of values into a clocked register.\nclock rises, the value on input valW is written to the program register indicated by\nthe register ID on input dstW.\nprogram register is written.\nSince the register ﬁle can be both read and written,\nsame register ID is used for both a read port and the write port, then, as the clock\nrises, there will be a transition on the read port’s data output from the old value to\nWhen we incorporate the register ﬁle into our processor design, we will\nOur processor has a random access memory for storing program data, as\nThis memory has a single address input, a data input for writing, and a data output\nLike the register ﬁle, reading from our memory operates in a manner\nsimilar to combinational logic: If we provide an address on the address input and\nset the write control signal to 0, then after some delay, the value stored at that\nclock: We set address to the desired address, data in to the desired value, and\nThe fetch stage reads the bytes of an instruction from memory, using\nto as icode (the instruction code) and ifun (the instruction function).\nIt computes valP to be the address of the instruction following",
      "keywords": [
        "register",
        "HCL",
        "input",
        "instruction",
        "circuit",
        "code",
        "Output",
        "Logic",
        "circuits",
        "inputs",
        "register ﬁle",
        "memory",
        "Logic Gates",
        "signal",
        "Combinational circuit"
      ],
      "concepts": [
        "registers",
        "register",
        "values",
        "value",
        "instructions",
        "instruction",
        "instructed",
        "inputs",
        "input",
        "circuits"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.681,
          "base_score": 0.681,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.612,
          "base_score": 0.612,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.59,
          "base_score": 0.59,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.584,
          "base_score": 0.584,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.581,
          "base_score": 0.581,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "register",
          "input",
          "circuit",
          "circuits",
          "inputs"
        ],
        "semantic": [],
        "merged": [
          "register",
          "input",
          "circuit",
          "circuits",
          "inputs"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41282340610473933,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.713124+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 413-432)",
      "start_page": 413,
      "end_page": 432,
      "summary": "The decode stage reads up to two operands from the register ﬁle, giving\ninstruction ﬁelds rA and rB, but for some instructions it reads register %rsp.\nIn the execute stage, the arithmetic/logic unit (ALU) either performs\nthe operation speciﬁed by the instruction (according to the value of ifun),\ncomputes the effective address of a memory reference, or increments or\nFor a conditional move instruction, the\nstage will evaluate the condition codes and move condition (given by ifun)\nSimilarly, for a jump instruction, it determines whether or not the\nThe memory stage may write data to memory, or it may read data\nThe write-back stage writes up to two results to the register ﬁle.\nThe PC is set to the address of the next instruction.\nexecutes a halt or invalid instruction, or it attempts to read or write an invalid ad-\nprocessing required to execute a single instruction.\nthe stated operation of the instruction, we must also compute addresses, update\nstack pointers, and determine the next instruction address.\nﬂow can be similar for every instruction.\nhave the different instructions share as much of the hardware as possible.\nthat is used in different ways depending on the type of instruction being exe-\ninstructions to ﬁt within this general framework.\nFigure 4.17 to illustrate the processing of different Y86-64 instructions.\n4.18 through 4.21 contain tables describing how the different Y86-64 instructions\n0x014: 6123\n0x020: 40436400000000000000 |\n0x037: 804100000000000000\n0x040:\n0x040: 00\n0x041:\n0x041: 90\nSample Y86-64 instruction sequence.\nWe will trace the processing of these instructions through\nFigure 4.18 shows the processing required for instruction types OPq (integer\nand logical operations), rrmovq (register-register move), and irmovq (immediate-\nwe can see that we have carefully chosen an encoding of instructions so that the\nALU computation must be set according to the particular instruction operation,\nThe processing of an integer-operation instruction follows the general pattern\nifun, so that valE becomes the instruction result.\nFor example, the instruction subq %rax,%rdx is supposed\nfor these instructions, but valE is written to register rB in the write-back stage, and\nthe PC is set to valP to complete the instruction execution.\nExecuting an rrmovq instruction proceeds much like an arithmetic operation.\nComputations in sequential implementation of Y86-64 instructions OPq, rrmovq, and\nThese instructions compute a value and store the result in a register.\nindicates the two components of the instruction byte, while rA:rB indicates the two components of the\nThe notation M1[x] indicates accessing (either reading or writing) 1 byte at memory\nthe program counter by 10 for irmovq due to the long instruction format.\nof these instructions changes the condition codes.\nthe irmovq instruction on line 4 of the object code in Figure 4.17:\nTracing the execution of a subq instruction\nAs an example, let us follow the processing of the subq instruction on line 3 of the object code shown\nWe can see that the previous two instructions initialize registers %rdx and %rbx to 9 and\nWe can also see that the instruction is located at address 0x014 and consists of 2 bytes,\nhaving values 0x61 and 0x23.\ngeneric rule for processing an OPq instruction (Figure 4.18) on the left, and the computations for this\nspeciﬁc instruction on the right.\nrA :rB ←M1[0x015] = 2:3\ncondition codes to zero, and incrementing the PC by 2.\nHow does this instruction execution modify the registers and the PC?\nFigure 4.19 shows the processing required for the memory write and read in-\nand the base register value) for the memory operation.\neither write the register value valA to memory or read valM from memory.\nComputations in sequential implementation of Y86-64 instructions\nThese instructions read or write memory.\nFigure 4.20 shows the steps required to process pushq and popq instructions.\nThese are among the most difﬁcult Y86-64 instructions to implement, because\nAlthough the two instructions have similar ﬂows, they have important\nThe pushq instruction starts much like our previous instructions, but in the\ndecode stage we use %rsp as the identiﬁer for the second register operand, giving\nThis decremented value is used for the memory write\naddress and is also stored back to %rsp in the write-back stage.\nas the address for the write operation, we adhere to the Y86-64 (and x86-64)\nThe popq instruction proceeds much like pushq, except that we read two\nﬂow more similar to that of other instructions, enhancing the overall uniformity\nstage, but use the unincremented value as the address for the memory operation.\nIn the write-back stage, we update both the stack pointer register with the incre-\nmented stack pointer and register rA with the value read from memory.\nunincremented stack pointer as the memory read address preserves the Y86-64\nTracing the execution of an rmmovq instruction\nLet us trace the processing of the rmmovq instruction on line 5 of the object code shown in Figure 4.17.\nWe can see that the previous instruction initialized register %rsp to 128, while %rbx still holds 12, as\ncomputed by the subq instruction (line 3).\nWe can also see that the instruction is located at address\nrA :rB ←M1[0x021] = 4:3\nAs this trace shows, the instruction has the effect of writing 128 to memory address 112 and\n(and x86-64) convention that popq should ﬁrst read memory and then increment\nthe popq instruction on line 7 of the object code in Figure 4.17.\nComputations in sequential implementation of Y86-64 instructions\nThese instructions push and pop the stack.\nWhat effect does this instruction execution have on the registers and the PC?\nWhat would be the effect of the instruction pushq %rsp according to the steps\nTracing the execution of a pushq instruction\nLet us trace the processing of the pushq instruction on line 6 of the object code shown in Figure 4.17.\nWe can also see that the instruction is\nAs this trace shows, the instruction has the effect of setting %rsp to 120, writing 9 to address 120,\nAssume the two register writes in the write-back stage for popq occur in the order\nFigure 4.21 indicates the processing of our three control transfer instructions:\nA jump instruction proceeds through fetch and decode much like\nthe previous instructions, except that it does not require a register speciﬁer byte.\nIn the execute stage, we check the condition codes and the jump condition to de-\nPC update stage, we test this ﬂag and set the PC to valC (the jump target) if the\nﬂag is 1 and to valP (the address of the following instruction) if the ﬂag is 0.\nComputations in sequential implementation of Y86-64 instructions jXX, call, and ret.\nThese instructions cause control transfers.\nWe can see by the instruction encodings (Figures 4.2 and 4.3) that the rrmovq\nrrmovq instruction below to also handle the six conditional move instructions.\nYou may ﬁnd it useful to see how the implementation of the jXX instructions\nTracing the execution of a je instruction\nLet us trace the processing of the je instruction on line 8 of the object code shown in Figure 4.17.\ncondition codes were all set to zero by the subq instruction (line 3), and so the branch will not be taken.\nThe instruction is located at address 0x02e and consists of 9 bytes.\nThe ﬁrst has value 0x73, while the\nAs this trace shows, the instruction has the effect of incrementing the PC by 9.\nInstructions call and ret bear some similarity to instructions pushq and popq,\nWith instruction call, we\npush valP, the address of the instruction that follows the call instruction.\nthe PC update stage, we set the PC to valC, the call destination.\nWith instruction\nret, we assign valM, the value popped from the stack, to the PC in the PC update\nthe call instruction on line 9 of the object code in Figure 4.17:\ncall 0x041\nTracing the execution of a ret instruction\nLet us trace the processing of the ret instruction on line 13 of the object code shown in Figure 4.17.\nThe instruction address is 0x041 and is encoded by a single byte 0x90.\nThe previous call instruction\nset %rsp to 120 and stored the return address 0x040 at memory address 120.\nAs this trace shows, the instruction has the effect of setting the PC to 0x040, the address of the\nhalt instruction.\ncall 0x041\nWhat effect would this instruction execution have on the registers, the PC,\nY86-64 instructions.\nEven though the instructions have widely varying behavior,\nThe computations required to implement all of the Y86-64 instructions can be or-\nganized as a series of six basic stages: fetch, decode, execute, memory, write back,\nThe program counter is stored in a register, shown\nupdated values to write to the register ﬁle and the updated program counter.\nSEQ, all of the processing by the hardware units occurs within a single clock cycle,\nThe hardware units are associated with the different processing stages:\nUsing the program counter register as an address, the instruction mem-\nory reads the bytes of an instruction.\nThe PC incrementer computes valP,\nThe register ﬁle has two read ports, A and B, via which register values\nThe execute stage uses the arithmetic/logic (ALU) unit for different\npurposes according to the instruction type.\nFor other instructions, it serves as an adder\nThe condition code register (CC) holds the three condition code bits.\nNew values for the condition codes are computed by the ALU.\nexecuting a conditional move instruction, the decision as to whether or\nnot to update the destination register is computed based on the condition\nSimilarly, when executing a jump instruction,\nthe branch signal Cnd is computed based on the condition codes and the\nThe data memory reads or writes a word of memory when executing a\nmemory instruction.\nThe instruction and data memories access the same\ncomputed by the ALU, while port M is used to write values read from the\nInstruction\nprocessed during execution of an instruction follows a clockwise ﬂow starting with an\ninstruction fetch using the program counter (PC), shown in the lower left-hand corner\nvalP, the address of the next instruction, valC, the destination address\nspeciﬁed by a call or jump instruction, or valM, the return address read\n. Clocked registers are shown as white rectangles.The program counter PC is the\nsrcA, the source of valA; srcB, the source of valB; dstE, the register to which valE\nOPq and mrmovq instructions to illustrate the values being computed.\ncomputations into hardware, we want to implement control logic that will transfer\nthe data between the different hardware units and operate these units in such a way\nthat the speciﬁed operations are performed for each of the different instruction\nInstruction\nThe second column identiﬁes the value being computed or the operation\nThe computations for instructions OPq and mrmovq\ninstruction.\nof memory devices: clocked registers (the program counter and condition code\nregister) and random access memories (the register ﬁle, the instruction memory,\naccess memory operates much like combinational logic, with the output word\nSince our instruction memory is only used to read\ninstructions, we can therefore treat this unit as if it were combinational logic.\nover their sequencing—the program counter, the condition code register, the data\nmemory, and the register ﬁle.\ntriggers the loading of new values into the registers and the writing of values to the\nThe program counter is loaded with a new instruction\nThe condition code register is loaded only when an\ninteger operation instruction is executed.\nan rmmovq, pushq, or call instruction is executed.\nuse the special register ID 0xF as a port address to indicate that no write should\nThis clocking of the registers and memories is all that is required to control the\nof the nature of the Y86-64 instruction set, and because we have organized the\nThe processor never needs to read back the state updated by an instruction in\norder to complete the processing of this instruction.\ntion, suppose we implemented the pushq instruction by ﬁrst decrementing %rsp\nby 8 and then using the updated value of %rsp as the address of a write operation.\nthe updated stack pointer from the register ﬁle in order to perform the memory\nvalue of the stack pointer as the signal valE and then uses this signal both as the\ndata for the register write and the address for the memory write.\ncan perform the register and memory writes simultaneously as the clock rises to\nAs another illustration of this principle, we can see that some instructions (the\ninteger operations) set the condition codes, and some instructions (the conditional\nmove and jump instructions) read these condition codes, but no instruction must\nboth set and then read the condition codes.\nbefore any instruction attempts to read them.\nFigure 4.25 shows how the SEQ hardware would process the instructions at\ninstruction addresses listed on the left:\n0x000:\n0x014:\n0x016:\nthe combinational logic as being wrapped around the condition code register,\nto the condition code register, while other parts (such as the branch computation\nand the PC selection logic) have the condition code register as input.\nregister ﬁle and the data memory as having separate connections for reading and\ncombinational logic, while the write operations are controlled by the clock.\ndifferent instructions being executed.\nirmovq instruction (line 2 of the listing), shown in light gray.\nThe clock cycle begins with address 0x014 loaded into the program\nThis causes the addq instruction (line 3 of the listing), shown in blue, to\nValues ﬂow through the combinational logic, including\nthe combinational logic has generated new values (000) for the condition codes,\nan update for program register %rbx, and a new value (0x016) for the program\naddq instruction (shown in blue), but the state still holds the values set by the\nsecond irmovq instruction (shown in light gray).\ncounter, the register ﬁle, and the condition code register occur, and so we show\nIn this cycle, the je instruction (line 4 in the listing), shown\nthe je instruction (shown in dark gray), but the state still holds the values set by\nthe addq instruction (shown in blue) until the next cycle begins.\nlogic, sufﬁces to control the computations performed for each instruction in our\nprocessor begins executing a new instruction.\n0x300\n0x016\n0x014\n0x016\n0x014\n0x016\nelements (program counter, condition code register, register ﬁle, and data memory)\nset according to the previous instruction.\nof the different instructions.\nwell as constant values for the different instruction codes, function codes, register\nCode for halt instruction\nCode for nop instruction\nCode for rrmovq instruction\nCode for irmovq instruction\nCode for rmmovq instruction\nCode for mrmovq instruction\nCode for integer operation instructions\nCode for jump instructions\nCode for call instruction\nCode for ret instruction\nCode for pushq instruction\nCode for popq instruction\nStatus code for illegal instruction exception\nencodings of the instructions, function codes, register IDs, ALU operations, and status",
      "keywords": [
        "instruction",
        "memory",
        "register",
        "valE",
        "valP",
        "stage",
        "code",
        "write",
        "condition code register",
        "rsp",
        "valB",
        "valA",
        "ALU",
        "ifun",
        "code register"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "codes",
        "code",
        "coding",
        "register",
        "registers",
        "values",
        "value",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.877,
          "base_score": 0.727,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.876,
          "base_score": 0.726,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.871,
          "base_score": 0.721,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "register",
          "instructions",
          "condition",
          "stage"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "register",
          "instructions",
          "condition",
          "stage"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.47519212686366225,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713182+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 433-453)",
      "start_page": 433,
      "end_page": 453,
      "summary": "SEQ fetch stage.\ninstruction memory using\ninstruction ﬁelds.\nInstruction\nIn addition to the instructions shown in Figures 4.18 to 4.21, we include the\nthrough stages without much processing, except to increment the PC by 1.\nFetch Stage\nAs shown in Figure 4.27, the fetch stage includes the instruction memory hardware\nThis unit reads 10 bytes from memory at a time, using the PC as the address of\n“icode” and “ifun” then compute the instruction and function codes as equaling\neither the values read from memory or, in the event that the instruction address\na nop instruction.\nBased on the value of icode, we can compute three 1-bit signals\nDoes this instruction include a register speciﬁer byte?\nDoes this instruction include a constant word?\nThe signals instr_valid and imem_error (generated when the instruction address\nis out of bounds) are used to generate the status code in the memory stage.\nwhether the value of icode is one of the instructions that has a register speci-\nWrite HCL code for the signal need_valC in the SEQ implementation.\nAs Figure 4.27 shows, the remaining 9 bytes read from the instruction memory\nThese bytes are processed by the hardware unit labeled “Align” into the register\nare set to 0xF (RNONE), indicating there are no registers speciﬁed by this instruction.\nRecall also (Figure 4.2) that for any instruction having only one register operand,\nThe PC incrementer hardware unit generates the signal valP, based on the\ncurrent value of the PC, and the two signals need_regids and need_valC.\nDecode and Write-Back Stages\nand write-back stages in SEQ.\nThese two stages are combined because they both\noutput word (for a read port) or an input word (for a write port) of the register\nregister IDs for the register ﬁle, based on the instruction code icode, the register\nspeciﬁers rA and rB, and possibly the condition signal Cnd computed in the execute\nstage.\nRegister ID srcA indicates which register should be read to generate valA.\nstage.\nThe instruction ﬁelds are\nRegister\nThe desired value depends on the instruction type, as shown in the ﬁrst row for the\ndecode stage in Figures 4.18 to 4.21.\nThe register signal srcB indicates which register should be read to generate the\nThe desired value is shown as the second step in the decode stage in\nstep in the write-back stage.\ninstructions, then we can combine the destination registers for all of the different\ninstructions to give the following HCL description of dstE:\nexamine the execute stage.\nRegister ID dstM indicates the destination register for write port M, where valM,\nsecond step in the write-back stage.\nOnly the popq instruction uses both register ﬁle write ports simultaneously.\nExecute Stage\nThe execute stage includes the arithmetic/logic unit (ALU).\nIn Figures 4.18 to 4.21, the ALU computation for each instruction is shown as\nthe ﬁrst step in the execute stage.\ninstruction type.\nSEQ execute stage.\noperation instruction or\n# Other instructions don’t need ALU\nBased on the ﬁrst operand of the ﬁrst step of the execute stage in Figures 4.18 to\nLooking at the operations performed by the ALU in the execute stage, we\nThe execute stage also includes the condition code register.\ncodes when an OPq instruction is executed.\nthat controls whether or not the condition code register should be updated:\nFor other instructions, the Cnd signal may be set to either 1 or 0, depending on\nthe instruction’s function code and the setting of the condition codes, but it will\nAs Figure 4.28 shows, we can implement these instructions by making\nuse of the Cnd signal, generated in the execute stage.\ndstE to implement these instructions.\nMemory Stage\nThe memory stage has the task of either reading or writing program data.\nshown in Figure 4.30, two control blocks generate the values for the memory\nSEQ memory stage.\ngenerate the control signals indicating whether to perform a read or a write\nThe desired memory operation for each instruction type is shown in the\nmemory stage of Figures 4.18 to 4.21.\n# Other instructions don’t need address\nLooking at the memory operations for the different instructions shown in Fig-\nWrite HCL code for the signal mem_data in SEQ.\nWe want to set the control signal mem_read only for instructions that read\nWe want to set the control signal mem_write only for instructions that write data\nWrite HCL code for the signal mem_write in SEQ.\nSEQ PC update stage.\ninstruction code and the branch\nA ﬁnal function for the memory stage is to compute the status code Stat\nresulting from the instruction execution according to the values of icode, imem_\nerror, and instr_valid generated in the fetch stage and the signal dmem_error\nPC Update Stage\nThe ﬁnal stage in SEQ generates the new value of the program counter (see Figure\nor valP, depending on the instruction type and whether or not a branch should be\ninstructions into a uniform ﬂow, we can implement the entire processor with a\nthese units and generate the proper control signals based on the instruction types\nenough so that signals can propagate through all of the stages within a single\nan updated program counter at the beginning of the clock cycle, the instruction\nmust be read from the instruction memory, the stack pointer must be read from\nBefore attempting to design a pipelined Y86-64 processor, let us consider some\na pipelined system who attempts to go directly to the dessert stage risks incurring\nComputational Pipelines\nShifting our focus to computational pipelines, the “customers” are instructions\nand the stages perform some portion of the instruction execution.\nlogic that performs a computation, followed by a register to hold the results of this\nA clock signal controls the loading of the register at some regular\nregister.\nFigure 4.32 shows a form of timing diagram known as a pipeline diagram.\nduring which these instructions are executed.\ncomplete one instruction before beginning the next.\n1 instruction\nWe express throughput in units of giga-instructions per second (abbreviated\nstages, A, B, and C, where each requires 100 ps, as illustrated in Figure 4.33.\nwe could put pipeline registers between the stages so that each instruction moves\nAs the pipeline diagram in Figure 4.33 illustrates, we could allow\nthree stages would be active, with one instruction leaving and a new one entering\npipeline diagram where I1 is in stage C, I2 is in stage B, and I3 is in stage A.\n(a) Hardware: Three-stage pipeline\nThree-stage pipelined computation hardware.\nOn each 120 ps cycle, each instruction progresses through one\nstage.\nThree-stage pipeline\nthe clock signal controls the\nfrom one pipeline stage to\n3 clock cycles, the latency of this pipeline is 3 × 120 = 360 ps.\nincreased latency is due to the time overhead of the added pipeline registers.\nand operation of pipeline computations.\nFigure 4.34 shows the pipeline diagram\nfor the three-stage pipeline we have already looked at (Figure 4.33).\nof the instructions between pipeline stages is controlled by a clock signal, as shown\nthe next set of pipeline stage evaluations.\nOne clock cycle of pipeline operation.\ntime 240 (point 1), instructions I1 (shown in dark gray) and I2 (shown in blue) have\ncompleted stages B and A.\nAfter the clock rises, these instructions begin propagating\nthrough stages C and B, while instruction I3 (shown in light gray) begins propagating\nthrough stage A (points 2 and 3).\ninstructions have propagated to the inputs of the pipeline registers (point 4).\nA. Just before the rising clock at time 240 (point 1), the values computed in stage A\nfor instruction I2 have reached the input of the ﬁrst pipeline register, but its state\nand output remain set to those computed during stage A for instruction I1.\nvalues computed in stage B for instruction I1 have reached the input of the sec-\nond pipeline register.\nAs the clock rises, these inputs are loaded into the pipeline\nA is set to initiate the computation of instruction I3.\nthrough the combinational logic for the different stages (point 3).\nof the pipeline registers (point 4).\ninstructions will have progressed through one pipeline stage.\nthe clock would not change the pipeline behavior.\npipeline register inputs, but no change in the register states will occur until the\ncombinational logic sufﬁces to control the ﬂow of instructions in the pipeline.\nthe clock rises and falls repeatedly, the different instructions ﬂow through the\nstages of the pipeline without interfering with one another.\nThe example of Figure 4.33 shows an ideal pipelined system in which we are able\nto divide the computation into three independent stages, each requiring one-third\nFigure 4.36 shows a system in which we divide the computation into three stages\nas before, but the delays through the stages range from 50 to 150 ps.\nthe delays through all of the stages remains 300 ps.\ncan operate the clock is limited by the delay of the slowest stage.\nAs the pipeline\n100 ps every clock cycle, while stage C will be idle for 50 ps every clock cycle.\nDevising a partitioning of the system computation into a series of stages\n(a) Hardware: Three-stage pipeline, nonuniform stage delays\nLimitations of pipelining due to nonuniform stage delays.\nthroughput is limited by the speed of the slowest stage.\nWe can create pipelined versions of this design by inserting pipeline registers\nmany stages) and maximum throughput arise, depending on where we insert the\npipeline registers.\nAssume that a pipeline register has a delay of 20 ps.\nA. Inserting a single register gives a two-stage pipeline.\nthree-stage pipeline?\n4-stage pipeline?\nD. What is the minimum number of stages that would yield a design with the\nFigure 4.37 illustrates another limitation of pipelining.\ndivided the computation into six stages, each requiring 50 ps.\nregister between each pair of stages yields a six-stage pipeline.\nThus, in doubling the number of pipeline stages, we improve the\nthe throughput, due to the delay through the pipeline registers.\na limiting factor in the throughput of the pipeline.\nModern processors employ very deep pipelines (15 or more stages) in an\ninstruction execution into a large number of very simple steps so that each stage\nnumber of pipeline stages k, each having a delay of 300/k, and with each pipeline\nregister having a delay of 20 ps.\ninstructions.\nFor example, consider the following Y86-64 instruction sequence:\nIn this three-instruction sequence, there is a data dependency between each\nsuccessive pair of instructions, as indicated by the circled register names and the\nThe irmovq instruction (line 1) stores its result in %rax,\nits result in %rbx, which must then be read by the mrmovq instruction (line 3).\nThe jne instruction (line 3) creates a control dependency since the outcome of\nthe conditional test determines whether the next instruction to execute will be the\nregister ﬁle and the new PC value down to the PC register.\nFigure 4.38 illustrates the perils of introducing pipelining into a system con-\n(c) Hardware: Three-stage pipeline with feedback\nLimitations of pipelining due to logical dependencies.\nunpipelined system with feedback (a) to a pipelined one (c), we change its computational\npipeline diagram (Figure 4.38(b)), where the result of I1 becomes an input to\nIf we attempt to convert this to a three-stage pipeline in the most\nWhen we introduce pipelining into a Y86-64 processor, we must deal with\nwith the data and control dependencies between instructions so that the resulting\nSEQ to shift the computation of the PC into the fetch stage.\nregisters between the stages.\nSEQ+: Rearranging the Computation Stages\norder of the ﬁve stages in SEQ so that the PC update stage comes at the beginning\nthe sequencing of activities within the pipeline stages.\nWe can move the PC update stage so that its logic is active at the beginning of\nthe clock cycle by making it compute the PC value for the current instruction.\nFigure 4.39 shows how SEQ and SEQ+ differ in their PC computation.\nSEQ (Figure 4.39(a)), the PC computation takes place at the end of the clock\ncycle, computing the new value for the PC register based on the values of signals\nstate registers to hold the signals computed during an instruction.\nnew clock cycle begins, the values propagate through the exact same logic to\ncompute the PC for the now-current instruction.\nthe value of the program counter for the current state as the ﬁrst step in instruction\nthe PC is computed dynamically based on some state information stored from the previous instruction.\ninstructions in a completely different order than they occur in the machine-level program.\nbalance the delays between the different stages of a pipelined system.\nInserting Pipeline Registers\nregisters between the stages of SEQ+ and rearrange signals somewhat, yielding\nThe pipeline registers are shown in this ﬁgure\nindicated by the multiple ﬁelds, each pipeline register holds multiple bytes and\ntial design SEQ (Figure 4.40), but with the pipeline registers separating the stages.\nThe pipeline registers are labeled as follows:\nrecently fetched instruction for processing by the decode stage.\nRegister\nInstruction\nthe clock cycle to the beginning makes it more suitable for pipelining.\nRegister\nInstruction\ninserting pipeline registers into SEQ+ (Figure 4.40), we create a ﬁve-stage pipeline.\nE sits between the decode and execute stages.\nmost recently decoded instruction and the values read from the register\nﬁle for processing by the execute stage.\nM sits between the execute and memory stages.\nmost recently executed instruction for processing by the memory stage.\ncomputed results to the register ﬁle for writing and the return address\nto the PC selection logic when completing a ret instruction.\nﬁve-stage pipeline, where the comments identify the instructions as I1 to I5 for\nExample of instruction ﬂow through pipeline.",
      "keywords": [
        "instruction",
        "stage",
        "pipeline",
        "pipeline registers",
        "register",
        "SEQ",
        "stages",
        "clock",
        "logic",
        "Comb. logic",
        "ALU",
        "registers",
        "memory",
        "Write HCL code",
        "memory stage"
      ],
      "concepts": [
        "instructions",
        "stage",
        "stages",
        "register",
        "registers",
        "pipeline",
        "pipelines",
        "figures",
        "signals",
        "logic"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.871,
          "base_score": 0.721,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.803,
          "base_score": 0.653,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.794,
          "base_score": 0.644,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stage",
          "pipeline",
          "stages",
          "instruction",
          "register"
        ],
        "semantic": [],
        "merged": [
          "stage",
          "pipeline",
          "stages",
          "instruction",
          "register"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4445493161610376,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713290+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 454-475)",
      "start_page": 454,
      "end_page": 475,
      "summary": "The right side of the ﬁgure shows a pipeline diagram for this instruction\nthe pipeline stages, with time increasing from left to right.\ncycle 1, instruction I1 is fetched, and it then proceeds through the pipeline stages,\nInstruction\nthis point, there is an instruction in each of the pipeline stages.\nso that the instructions ﬂow from bottom to top.\nshows the pipeline stages with the fetch stage on the bottom and the write-back\nstage on the top, just as do our diagrams of the pipeline hardware (Figure 4.41).\nIf we look at the ordering of instructions in the pipeline stages, we see that they\nwith the different instructions ﬂowing through the system.\nthe status codes for four different instructions (see Figure 4.41).\nhave serious errors, such as storing the result computed for one instruction at the\ndestination register speciﬁed by another instruction.\nthe status value in pipeline register W.\nand memory stages and are directed to the register ﬁle only once they reach\nWith our naming system, the uppercase preﬁxes ‘D’, ‘E’, ‘M’, and ‘W’ refer to pipeline registers, and so\nM_stat refers to the status code ﬁeld of pipeline register M.\n‘w’ refer to the pipeline stages, and so m_stat refers to the status signal generated in the memory stage\nsame instruction.\ninstruction in the write-back stage, but with register IDs from the instruction in\nabout a particular instruction contained within a single pipeline stage.\nthe value valA for the pipeline register E by choosing either valP from pipeline\nOnly the jump instructions require the value of valP in the\nNone of these instructions\nof pipeline register state by merging these two signals and carrying them through\nAs shown in Figure 4.41, our pipeline registers include a ﬁeld for the status\nis to associate a status code with each instruction as it passes through the pipeline,\nOur goal in the pipelined design is to issue a new instruction on\nyield a throughput of one instruction per cycle.\nUnfortunately, if the fetched instruction is a conditional branch, we will not\nthe instruction has passed through the execute stage.\nWith the exception of conditional jump instructions and ret, we can deter-\nnext instruction.\ntherefore we have fetched and partially executed the wrong instructions.\nthe instruction fetch unit that holds the return address generated by procedure call instructions.\ntime a procedure call instruction is executed, its return address is pushed onto the stack.\ninstruction is fetched, the top value is popped from this stack and used as the predicted return address.\ninstruction passes through the write-back stage.\nthe instruction fetch.\nvalP (as computed by the PC incrementer) or valC (from the fetched instruction).\nThis value is stored in pipeline register F as the predicted value of the program\nthe address for the instruction memory: the predicted PC, the value of valP for\na not-taken branch instruction that reaches pipeline register M (stored in regis-\nter M_valA), or the value of the return address when a ret instruction reaches\nbetween successive instructions.\nwhere the results computed by one instruction are used as the data for a follow-\nthe location of the following instruction, such as when executing a jump, call, or\n6, the second irmovq writes its result to program register %rax.\nThe addq instruction\nreads its source operands in cycle 7, so it gets correct values for both %rdx and %rax.\n10 and 3 into program registers %rdx and %rax, executes three nop instructions,\nhazards resulting from the data dependencies between the two irmovq instructions\nand the addq instruction.\nThe pipeline stages for cycles 6 and 7 are\nthe start of cycle 7, both of the irmovq instructions have passed through the write-\nback stage, and so the register ﬁle holds the updated values of %rdx and %rax.\nAs the addq instruction passes through the decode stage during cycle 7, it will\nwrite to program register %rax does not occur until the start of cycle 7, and so the addq\ninstruction gets the incorrect value for this register in the decode stage.\ntwo nop instructions between the two irmovq instructions generating values for\nregisters %rdx and %rax and the addq instruction having these two registers as\ninstruction has passed through the write-back stage, and so program register %rdx\nThe second irmovq instruction is in the write-\nback stage during this cycle, and so the write to program register %rax only occurs\nbetween the irmovq instructions and the addq instruction, yielding a program\naddq instruction passes through the decode stage.\n5, the addq instruction reads its source operands from the register ﬁle.\nwrite to register %rdx is still in the write-back stage, and the pending write to register\n%rax is still in the memory stage.\nwrite to register %rdx is still in the write-back stage, and the pending write to\n%rax is still in the memory stage.\nTherefore, the addq instruction would get the\nbetween the irmovq instructions and the addq instruction, yielding a program\naddq instruction passes through the decode stage.\nwrite to register %rdx is still in the memory stage, and the new value for %rax\nTherefore, the addq instruction would\nThese examples illustrate that a data hazard can arise for an instruction\nThese hazards occur because our pipelined processor reads the operands for an\ninstruction from the register ﬁle in the decode stage but does not write the results\nfor the instruction to the register ﬁle until three cycles later, after the instruction\n4, the addq instruction reads its source operands from the register ﬁle.\nwrite to register %rdx is still in the memory stage, and the new value for register %rax\nprocessor holds back one or more instructions in the pipeline until the hazard\nan instruction in the decode stage until the instructions generating its source op-\nto the other two examples.) When the addq instruction is in the decode stage,\nthe pipeline control logic detects that at least one of the instructions in the exe-\ncute, memory, or write-back stage will update either register %rdx or register %rax.\nRather than letting the addq instruction pass through the stage with the incorrect\nresults, it stalls the instruction, holding it back in the decode stage for either one\ninstruction in cycle 6, the stall control logic detects a data hazard due to the pending\nwrite to register %rax in the write-back stage.\nand repeats the decoding of the addq instruction in cycle 7.\ninstruction in cycle 4, the stall control logic detects data hazards for both source registers.\nIt injects a bubble into the execute stage and repeats the decoding of the addq instruction\nexecute stage, and repeats the decoding of the addq instruction on cycle 6.\ndetects a hazard for source register %rax, injects a bubble into the execute stage, and\nrepeats the decoding of the addq instruction on cycle 7.\nIn holding back the addq instruction in the decode stage, we must also hold\nback the halt instruction following it in the fetch stage.\nthe program counter at a ﬁxed value, so that the halt instruction will be fetched\nStalling involves holding back one group of instructions in their stages while\nallowing other instructions to continue ﬂowing through the pipeline.\nshould we do in the stages that would normally be processing the addq instruction?\nan instruction back in the decode stage.\nHazards can potentially occur when one instruction updates part of the program state that will be\nread by a later instruction.\nbetween different instructions.\nNo hazard arises when our fetch-stage logic correctly predicts the new value of\nthe program counter before fetching the next instruction.\ninstructions require special handling, as will be discussed in Section 4.5.5.\nWrites and reads of the data memory both occur in the memory stage.\ninstruction reading memory reaches this stage, any preceding instructions writing memory\nwriting data in the memory stage and the reading of instructions in the fetch stage, since the\ninstruction and data memories reference a single address space.\nprograms containing self-modifying code, where instructions write to a portion of memory\nfrom which instructions are later fetched.\nThe program status can be affected by instructions as they ﬂow through the pipeline.\nOur mechanism of associating a status code with each instruction in the pipeline enables\nthe pipeline bubbles indicates that a bubble was injected into the execute stage in\nplace of the addq instruction that would normally have passed from the decode to\nIn using stalling to handle data hazards, we effectively execute programs\naddq instruction.\nin which one instruction updates a register and a closely following instruction uses\nstage, but there can also be a pending write to one of these source registers in\nsimply pass the value that is about to be written to pipeline register E as the\nThe decode-stage logic detects that register\nstage logic detects the presence of a pending write to register %rax in the write-back\nstage.\nstage logic detects a pending write to register %rdx in the write-back stage and to\nregister %rax in the memory stage.\ntechnique of passing a result value directly from one pipeline stage to an earlier\nIt allows the instructions of prog2 to proceed through the pipeline\na pending write to a register in the memory stage, avoiding the need to stall\nIn cycle 5, the decode-stage logic detects a pending write to\nregister %rdx on port E in the write-back stage, as well as a pending write to register\nuntil the writes have occurred, it can use the value in the write-back stage (signal\nW_valE) for operand valA and the value in the memory stage (signal M_valE) for\nstage logic detects a pending write to register %rdx in the memory stage.\nthat a new value is being computed for register %rax in the execute stage.\nvalues from the execute stage to the decode stage, avoiding the need to stall for\nIn cycle 4, the decode-stage logic\ndetects a pending write to register %rdx in the memory stage, and also that the\nvalue being computed by the ALU in the execute stage will later be written to\nIt can use the value in the memory stage (signal M_valE) for operand\npipeline register E can be loaded with the results from the decode stage as the\nFrom the memory stage, we can forward the value that has just been\nstage logic can determine whether to use a value from the register ﬁle or to use\nIt allows valA for pipeline register E to be\nhazard, where one instruction (the mrmovq at address 0x028) reads a value from\nmemory for register %rax while the next instruction (the addq at address 0x032)\nThe addq instruction requires the value of the register in cycle 7, but it is\nnot generated by the mrmovq instruction until cycle 8.\nwith the value being generated by the irmovq instruction at address 0x01e and\nused by the addq instruction at address 0x032, can be handled by forwarding.)\nthrough the execute stage, the pipeline control logic detects that the instruction\nin the decode stage (the addq) requires the result read from memory.\ninstruction in the decode stage for one cycle, causing a bubble to be injected into\nmemory can then be forwarded from the memory stage to the addq instruction\nThe value for register %rbx is also forwarded from the write-\nInstruction\ninstructions.\nThe addq instruction requires the value\nof register %rax during the decode stage in cycle 7.\nvalue for this register during the memory stage in cycle 8, which is too late for the addq\ninstruction.\nachieve our throughput goal of issuing one new instruction on every clock cycle.\nof the next instruction based on the current instruction in the fetch stage.\nwas discussed in Section 4.5.4, control hazards can only occur in our pipelined\nprocessor for ret and jump instructions.\nFor the ret instruction, consider the following example program.\ngram is shown in assembly code, but with the addresses of the different instructions\nBy stalling the addq instruction for one cycle in the\ndecode stage, the value for valB can be forwarded from the mrmovq instruction in the memory stage to the\naddq instruction in the decode stage.\nFigure 4.55 shows how we want the pipeline to process the ret instruction.\nthrough the decode, execute, and memory stages, injecting three bubbles in the process.\nlogic will choose the return address as the instruction fetch address once the ret reaches the write-back stage\nwhere instructions are not executed in a linear sequence.\ninstruction addresses to identify the different instructions in the program.\nAs this diagram shows, the ret instruction is fetched during cycle 3 and\nproceeds down the pipeline, reaching the write-back stage in cycle 7.\npasses through the decode, execute, and memory stages, the pipeline cannot do\nthe ret instruction reaches the write-back stage, the PC selection logic will set the\nprogram counter to the return address, and therefore the fetch stage will fetch the\nirmovq instruction at the return point (address 0x013).\nassembly code but with the instruction addresses shown on the left for reference:\nSince the jump instruction is predicted as being taken, the instruc-\ntion at the jump target will be fetched in cycle 3, and the instruction following this\nshould not be taken during cycle 4, two instructions have been fetched that should\nThat can only occur when an instruction\nbranches will be taken and so starts fetching instructions at the jump target.\ninstructions are fetched before the misprediction is detected in cycle 4 when the jump\ninstruction ﬂows through the execute stage.\ntarget instructions by injecting bubbles into the decode and execute stages, and it also\nfetches the instruction following the jump.\nthis point, the pipeline can simply cancel (sometimes called instruction squashing)\nthe two misfetched instructions by injecting bubbles into the decode and execute\nstages on the following cycle while also fetching the instruction following the jump\ninstruction.\nthe basic clocked register design will enable us to stall stages and to inject bubbles\ninto pipeline registers as part of the pipeline control logic.\nthree different internally generated exceptions, caused by (1) a halt instruction,\nof instructions through the processor pipeline.\nLet us refer to the instruction causing the exception as the excepting instruc-\nIn the case of an invalid instruction address, there is no actual excepting\nexample, during one cycle of pipeline operation, we could have a halt instruction\naddress for the instruction in the memory stage.\nby the instruction in the memory stage.\nthe instruction in the memory stage should appear to execute before one in the\n# Invalid instruction code\nIn this program, the pipeline will predict that the branch should be taken,\nand so it will fetch and attempt to use a byte with value 0xFF as an instruction\ntherefore detect an invalid instruction exception.\nthat the branch should not be taken, and so the instruction at address 0x016\ninstruction, but we want to avoid raising an exception.\ninstruction completes.\nThe pushq instruction causes an address exception, because decrementing the\nOn the same cycle, the addq instruction is in\nthe execute stage, and it will cause the condition codes to be set to new values.\nexcepting instruction should have had any effect on the system state.\navoid raising exceptions for instructions that are fetched due to mispredicted\nis the motivation for us to include a status code stat in each of our pipeline registers\nIf an instruction generates an exception at some stage in\nfor that instruction, until it reaches the write-back stage.\nbeyond the excepting instruction, the pipeline control logic must disable any\nupdating of the condition code register or the data memory when an instruction in\nthe memory or write-back stages has caused an exception.\nabove, the control logic will detect that the pushq in the memory stage has caused\naddq instruction in the execute stage will be disabled.\nThe event has no effect on the ﬂow of instructions in the pipeline until an\nexcepting instruction reaches the ﬁnal pipeline stage, except to disable any updat-\nby later instructions in the pipeline.\nSince instructions reach the write-back stage\nthe write-back stage, at which point program execution can stop and the status\ncode in pipeline register W can be recorded as the program status.\nexception status together with all other information about an instruction through\ndesigns, with the addition of pipeline registers, some reconﬁgured logic blocks, and\nvalues, to indicate that the signals come from pipeline register D, and d_ for the\nresult value, to indicate that it is generated in the decode stage.\nearlier, this stage must also select a current value for the program counter and",
      "keywords": [
        "instruction",
        "stage",
        "addq instruction",
        "pipeline",
        "register",
        "memory stage",
        "rax",
        "decode stage",
        "cycle",
        "execute stage",
        "pipeline register",
        "write-back stage",
        "rdx",
        "memory",
        "addq"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "pipeline",
        "register",
        "registers",
        "exceptional",
        "exception",
        "exceptions",
        "excepting",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 25,
          "title": "",
          "score": 0.751,
          "base_score": 0.601,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.707,
          "base_score": 0.557,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "stage",
          "instruction",
          "pipeline",
          "addq",
          "addq instruction"
        ],
        "semantic": [],
        "merged": [
          "stage",
          "instruction",
          "pipeline",
          "addq",
          "addq instruction"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.40654295825700937,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713348+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 476-493)",
      "start_page": 476,
      "end_page": 493,
      "summary": "Instruction\nmispredicted branch enters the memory stage, the value of valP for this instruction\n(indicating the address of the following instruction) is read from pipeline register\nWhen a ret instruction enters the write-back stage, the return\naddress is read from pipeline register W (signal W_valM).\npredicted value of the PC, stored in pipeline register F (signal F_predPC):\n# Completion of RET instruction\nThe PC prediction logic chooses valC for the fetched instruction when it is\nfetched instruction.\nDecode and Write-Back Stages\nwrite ports come from the write-back stage (signals W_dstE and W_dstM), rather\ndestination registers speciﬁed by the instruction in the write-back stage.\nThe block labeled “dstE” in the decode stage generates the register ID for the E\nport of the register ﬁle, based on ﬁelds from the fetched instruction in pipeline\nof state in the pipeline register.\njump instructions need the value of valP in later stages, and these instructions\nPIPE decode and write-back stage logic.\nNo instruction requires both valP and the value read\nfrom register port A, and so these two can be merged to form the signal valA for later stages.\nspeciﬁed by the dstE and dstM signals from the write-back stage rather than from the decode stage, since it\nis writing the results of the instruction currently in the write-back stage.\ncontrolled by the icode signal for this stage.\ninstruction code for either call or jXX, this block should select D_valP as its\nPending write to port E in memory stage\nvalue of valA for pipeline register E:\nthe forwarding sources in the execute and memory stages.\nﬁrst two instructions write to register %rdx, while the third uses this register as its\nWhen the rrmovq instruction reaches the decode stage in cycle\nbehavior of the machine-language program when it is executed one instruction\nThe ﬁrst irmovq instruction would set register %rdx to 10, the second\nwould set the register to 3, and then the rrmovq instruction would read 3 from\npriority to the forwarding source in the earliest pipeline stage, since it holds the\nlatest instruction in the program sequence setting the register.\nHCL code above ﬁrst tests the forwarding source in the execute stage, then those in\nthe memory stage, and ﬁnally the sources in the write-back stage.\npriority between the two sources in either the memory or the write-back stages\nthe one in the execute stage, since it represents the most recently generated value for\nfrom the memory stage) in the HCL code for d_valA were reversed.\nthe write-back stage) in the HCL code for d_valA were reversed.\nWrite HCL code for the signal d_valB, giving the value for source operand valB\nsupplied to pipeline register E.\npipeline register W.\nregister W holds the state of the most recently completed instruction, it is natural\ncase to consider is when there is a bubble in the write-back stage.\nFigure 4.60 shows the execute stage logic for PIPE.\nWe can see the signals e_valE and e_dstE directed toward the decode stage as\nThese signals are used to detect cases where an instruction\nPIPE execute stage logic.\nPIPE memory stage logic.\nMany of the signals from pipeline registers\nM and W are passed down to earlier stages to provide write-back results, instruction\ncausing an exception is passing through later pipeline stages, and therefore any\nOur second case in the HCL code for d_valA uses signal e_dstE to see whether\nwe use signal E_dstE, the destination register ID in pipeline register E for this\nMemory Stage\nFigure 4.61 shows the memory stage logic for PIPE.\nsources valP (for call instructions) and valA, but this selection is now performed\nforwarding and pipeline control logic.\nPipeline Control Logic\nThe pipeline must stall for one cycle between an instruction\nthat reads a value from memory and an instruction that uses this value.\nThe pipeline must stall until the ret instruction reaches the\nwrite-back stage.\nexecution once the excepting instruction reaches the write-back stage.\ninstructions read data from memory.\nstage and (2) an instruction requiring the destination register is in the decode\nstage, we want to hold back the second instruction in the decode stage and inject a\nbubble into the execute stage on the next cycle.\nThe pipeline can hold back an instruction in the decode\nstage by keeping pipeline register D in a ﬁxed state.\nkeep pipeline register F in a ﬁxed state, so that the next instruction will be fetched\nhazard condition, keeping pipeline registers F and D ﬁxed, and injecting a bubble\nFor the processing of a ret instruction, we have described the desired pipeline\nThe pipeline should stall for three cycles until the\nreturn address is read as the ret instruction passes through the memory stage.\ninject a bubble into the fetch stage of our pipeline.\nreads some instruction from the instruction memory.\nret instruction, the new value of the PC is predicted to be valP, the address of the\nfollowing instruction.\nof the rrmovq instruction following the ret.\nFor three clock cycles, the fetch stage stalls,\ncausing the rrmovq instruction to be fetched but then replaced by a bubble in the\narrow leading down to the bubbles passing through the remaining pipeline stages.\nFinally, the irmovq instruction is fetched on cycle 7.\nfetches the rrmovq instruction following the ret instruction, but then the pipeline\ncontrol logic injects a bubble into the decode stage rather than allowing the rrmovq\nbe detected as the jump instruction reaches the execute stage.\nthen injects bubbles into the decode and execute stages on the next cycle, causing\npipeline reads the correct instruction into the fetch stage.\nFor an instruction that causes an exception, we must make the pipelined im-\ndetected during two different stages (fetch and memory) of program execution,\nand (2) the program state is updated in three different stages (execute, memory,\nOur stage designs include a status code stat in each pipeline register to track\nthe status of each instruction as it passes through the pipeline stages.\ncontinue fetching, decoding, and executing instructions as if nothing were amiss.\nAs the excepting instruction reaches the memory stage, we take steps to prevent\nthe setting of condition codes by instructions in the execute stage, (2) injecting\nbubbles into the memory stage to disable any writing to the data memory, and (3)\nstalling the write-back stage when it has an excepting instruction, thus bringing\ntion in the execute stage generates new values for the condition codes.\nthe setting of condition codes when an excepting instruction is in the memory or\nwrite-back stage (by examining the signals m_stat and W_stat and then setting the\nmemory stage and stalling the excepting instruction in the write-back stage in the\nexample of Figure 4.63—the pushq instruction remains stalled in the write-back\nstage, and none of the subsequent instructions get past the execute stage.\nBy this combination of pipelining the status signals, controlling the setting of\ncondition codes, and controlling the pipeline stages, we achieve the desired behav-\nFigure 4.64 summarizes the conditions requiring special pipeline control.\nmemory reference by the pushq instruction causes the updating of the condition codes\nThe pipeline starts injecting bubbles into the memory stage and stalling\nthe excepting instruction in the write-back stage.\nDetection conditions for pipeline control logic.\nexecuted instructions.\nthe action of the pipeline registers as the clock rises to start the next cycle.\na clock cycle, pipeline registers D, E, and M hold the states of the instructions\nthat are in the decode, execute, and memory pipeline stages, respectively.\nthe register IDs of the source operands for the instruction in the decode stage.\nDetecting a ret instruction as it passes through the pipeline simply involves\nchecking the instruction codes of the instructions in the decode, execute, and\nmemory stages.\nDetecting a load/use hazard involves checking the instruction\ntype (mrmovq or popq) of the instruction in the execute stage and comparing its\ndestination register with the source registers of the instruction in the decode stage.\nThe pipeline control logic should detect a mispredicted branch while the jump\ninstruction is in the execute stage, so that it can set up the conditions required to\nrecover from the misprediction as the instruction enters the memory stage.\njump instruction is in the execute stage, the signal e_Cnd indicates whether or not\ninstruction status values in the memory and write-back stages.\nstage, we use the signal m_stat, computed within the stage, rather than M_stat\nfrom the pipeline register.\nFigure 4.65 shows low-level mechanisms that allow the pipeline control logic to\nhold back an instruction in a pipeline register or to inject a bubble into the pipeline.\nPipeline register\nActions for pipeline control logic.\nthe pipeline ﬂow by either stalling the pipeline or canceling partially executed instructions.\nSuppose that each pipeline register has two control inputs stall\nThe settings of these signals determine how the pipeline register is\nan instruction in some pipeline stage.\nthe pipeline register.\nFor example, to inject a bubble into pipeline register D, we\na bubble into pipeline register E, we want the icode ﬁeld to be set to INOP and\na pipeline register.\nThe table in Figure 4.66 shows the actions the different pipeline stages should\nnormal, stall, and bubble operations for the pipeline registers.\nthe stall and bubble control signals for the pipeline registers are generated by\neach of the pipeline registers to either load, stall, or bubble as the next clock cycle\nWith this small extension to the pipeline register designs, we can implement\ncombinational logic, clocked registers, and random access memories.\nour exception-handling mechanism to consider other instructions in the pipeline.\nFigure 4.67 diagrams the pipeline states that cause the other three special control\nstages.\nA load/use hazard requires that the instruction in the\nexecute stage reads a value from memory into a register, and that the instruction\nin the decode stage has this register as a source operand.\nrequires the instruction in the execute stage to have a jump instruction.\nthree possible cases for ret—the instruction can be in either the decode, execute,\nor memory stage.\nAs the ret instruction moves through the pipeline, the earlier\npipeline stages will have bubbles.\nCombination A involves a not-taken jump instruction in the execute stage and\na ret instruction in the decode stage.\nThe pipeline control logic should detect\nwe get the following pipeline control actions (assuming that either a bubble or a\nPipeline register\naddress of the instruction following the jump, rather than the predicted program\nCombination B involves a load/use hazard, where the loading instruction sets\nregister %rsp and the ret instruction then uses this register as a source operand,\nThe pipeline control logic\nshould hold back the ret instruction in the decode stage.\nand completes with a halt instruction if the pipeline operates correctly.\nPipeline register\nIf both sets of actions were triggered, the control logic would try to stall the ret\ninstruction to avoid the load/use hazard but also inject a bubble into the decode\nstage due to the ret instruction.\nhaving combination B was executed, the control logic would set both the bubble\nand the stall signals for pipeline register D to 1.\nFigure 4.68 shows the overall structure of the pipeline control logic.\nsignals from the pipeline registers and pipeline stages, the control logic generates\nPIPE pipeline control logic.\nThis logic overrides the normal ﬂow of instructions through the\npipeline to handle special conditions such as procedure returns, mispredicted branches, load/use hazards,\nstall and bubble control signals for the pipeline registers and also determines\ndescriptions for the different pipeline control signals.\nPipeline register F must be stalled for either a load/use hazard or a ret\ninstruction:\nWrite HCL code for the signal D_stall in the PIPE implementation.\nPipeline register D must be set to bubble for a mispredicted branch or a ret\ninstruction.\ninstruction:\nWrite HCL code for the signal E_bubble in the PIPE implementation.\nWrite HCL code for the signal set_cc in the PIPE implementation.\nWrite HCL code for the signals M_bubble and W_stall in the PIPE implemen-\nThis covers all of the special pipeline control signal values.\nHCL code for PIPE, all other pipeline control signals are set to zero.\nWe can see that the conditions requiring special action by the pipeline control\nlogic all cause our pipeline to fall short of the goal of issuing a new instruction on\nreturn instruction generates three bubbles, a load/use hazard generates one, and\nof clock cycles PIPE would require per instruction it executes, a measure known\nWith pipelining, there are many subtle interactions between the instructions at different pipeline stages.\nstack pointer) or unusual instruction combinations (such as a not-taken jump followed by a ret).\nand memory values to those produced by our yis instruction set simulator.\nRuns 49 tests of different Y86-64 instructions with different source and destination registers\nRuns 64 tests of the different jump and call instructions, with different combinations of whether\nRuns 28 tests of the different conditional move instructions, with different control combi-\nTests 12 different combinations where an instruction causes an exception and the instructions\nstage.\nOn each cycle, the execute stage either (1) processes an instruction and this\ninstruction continues through the remaining stages to completion, or (2) processes\ntotal clock cycles to execute Ci instructions.",
      "keywords": [
        "Instruction",
        "pipeline",
        "stage",
        "pipeline control logic",
        "pipeline register",
        "pipeline control",
        "register",
        "ret instruction",
        "memory stage",
        "execute stage",
        "control logic",
        "bubble",
        "pipeline stages",
        "logic",
        "write-back stage"
      ],
      "concepts": [
        "instructions",
        "signal",
        "signals",
        "stage",
        "stages",
        "registers",
        "logic",
        "conditional",
        "conditions",
        "condition"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.876,
          "base_score": 0.726,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.803,
          "base_score": 0.653,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.728,
          "base_score": 0.578,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "pipeline",
          "stage",
          "instruction",
          "pipeline register",
          "register"
        ],
        "semantic": [],
        "merged": [
          "pipeline",
          "stage",
          "instruction",
          "pipeline register",
          "register"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45658989630906893,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713406+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 494-513)",
      "start_page": 494,
      "end_page": 513,
      "summary": "Even when a design passes an extensive set of tests, we cannot be certain that it will operate correctly for\nWe set up a framework to compare the behavior of the pipelined design PIPE to the unpipelined\nprocessor models, and so we could catch any bugs in the HCL code.\nand PIPE are identical does not guarantee that either of them faithfully implements the instruction set\ninstructions, modify the hardware capabilities, or use different branch prediction strategies.\ningly, we found only one bug in all of our designs, involving control combination B (described in Section\nthe cycles required to start the instructions ﬂowing through the pipeline.\nof bubbles injected per instruction executed.\nSince only three different instruction\nVerilog implementation of a pipelined Y86-64 processor\nAs we have mentioned, modern logic design involves writing textual representations of hardware\nThe design can then be tested by both simulation and a\nOnce we have conﬁdence in the design, we can use logic synthesis\nWe have developed models of our Y86-64 processor designs in the Verilog hardware description\nThese designs combine modules implementing the basic building blocks of the processor,\nalong with control logic generated directly from the HCL descriptions.\n(FPGA) hardware, and run the processors on actual programs.\nfrequency with which bubbles are injected while stalling for ret instructions.\nreason (some portion of Cb) divided by the total number of instructions that were\nrelevant instructions (load, conditional branch, and return) occur, and for each of\n. Load instructions (mrmovq and popq) account for 25% of all instructions\n. Conditional branches account for 20% of all instructions executed.\n. Return instructions account for 2% of all instructions executed.\nquency of the instruction type, the frequency the condition arises, and the number\nInstruction\nOur goal was to design a pipeline that can issue one instruction per cycle,\nwe cancel two instructions for every misprediction.\nvalues of a very long array, and so the overall performance is determined largely by\nAssume that our jump instructions\nA. On average, how many instructions are executed in the inner loops of the\nC. What is the average number of clock cycles required per array element for\nWe have created a structure for the PIPE pipelined microprocessor, designed the\ncontrol logic blocks, and implemented pipeline control logic to handle special\nMulticycle Instructions\nAll of the instructions in the Y86-64 instruction set involve simple operations such\nIn a more complete instruction set, we would also need to implement\ninstructions requiring more complex operations such as integer multiplication and\ninstructions, we require both additional hardware to perform the computations\nand a mechanism to coordinate the processing of these instructions with the rest\nOne simple approach to implementing multicycle instructions is to simply\nAn instruction remains in the execute stage for as many clock\ncycles as it requires, causing the fetch and decode stages to stall.\nsimple to implement, but the resulting performance is not very good.\nan instruction enters the decode stage, it can be issued to the special unit.\nunit performs the operation, the pipeline continues processing other instructions.\ncan execute concurrently in the main pipeline and in the different units.\noperations being handled by different units, the control logic may need to stall\nIn our presentation of PIPE, we assumed that both the instruction fetch unit\nand the data memory could read or write any memory location in one clock\none instruction writes to the region of memory from which later instructions are\nread or write operation can be performed.\nmay reside on disk, requiring millions of clock cycles to read into the processor\nAs will be discussed in Chapters 6 and 9, the memory system of a processor\nuses a combination of multiple hardware memories and operating system soft-\nsible to read instructions and read or write data in a single clock cycle most of\nretrieved from a higher-level cache or from the main memory of the processor,\ninstruction in the fetch or memory stage until the cache can perform the read\nIn terms of our pipeline design, this can be implemented by\nadding more stall conditions to the pipeline control logic.\nreturn to the original program, where the instruction causing the page fault will be\nWe have seen that the instruction set architecture, or ISA, provides a layer of\nabstraction between the behavior of a processor—in terms of the set of instructions\na very sequential view of program execution, with one instruction executed to\nA ﬁve-stage pipeline, such as we have shown with the PIPE processor, represented the state of the art in\nprocessor design in the mid-1980s.\nThe Intel i486 processor also uses a ﬁve-stage pipeline, although with a different partitioning of\nThese pipelined designs are limited to a throughput of at most one instruction per clock cycle.\ndifferent stages can only process one instruction at a time.\nmultiple instructions in parallel.\nperformance measure has shifted from CPI to its reciprocal—the average number of instructions\nuse a technique known as out-of-order execution to execute multiple instructions in parallel, possibly\nPipelined processors are not just historical artifacts, however.\nprocessors [10].\nWe deﬁned the Y86-64 instruction set by starting with the x86-64 instructions\nand simplifying the data types, address modes, and instruction encoding consider-\nThe resulting ISA has attributes of both RISC and CISC instruction sets.\nthen organized the processing required for the different instructions into a series\nof ﬁve stages, where the operations at each stage vary according to the instruction\ninstruction is executed every clock cycle by having it ﬂow through all ﬁve stages.\nHCL descriptions of Y86-64 processors\nIn this chapter, we have looked at portions of the HCL code for several simple logic designs and for\nthe control logic for Y86-64 processors SEQ and PIPE.\nthe HCL language and complete HCL descriptions for the control logic of the two processors.\nthese descriptions requires only ﬁve to seven pages of HCL code, and it is worthwhile to study them in\nWe enhanced the pipeline performance by adding forwarding logic to speed the\nsending of a result from one instruction to another.\nadditional pipeline control logic to stall or cancel some of the pipeline stages.\nwe make sure that only instructions up to the excepting instruction affect the\ndesign:\ndifferent instruction types.\nunits among the logic for processing the different instruction types.\nThis led to the use of a pipelined design.\nprocessors.\n. The GUI (graphic user interface) version displays the memory, program code,\nthe instructions ﬂow through the processors.\nThe control logic for the simulators is generated by translating the HCL\ndeclarations of the logic blocks into C code.\nalso available that thoroughly exercise the different instructions and the different\nlogic design textbook [58] is a standard introductory text, emphasizing the use of\ntextbook [46] provides extensive coverage of processor design, including both\nsimple pipelines, such as the one we have presented here, and advanced processors\nthat execute more instructions in parallel.\nIn Section 3.4.2, the x86-64 pushq instruction was described as decrementing the\nhad an instruction of the form pushq REG, for some register REG, it would be\ncorrectly describe the behavior of the instruction pushq %rsp?\nIn Section 3.4.2, the x86-64 popq instruction was described as copying the result\nSo, if we had an instruction of the form popq REG, it would be\ncorrectly describe the behavior of the instruction popq %rsp?\nA. Write and test a C version that references the array elements with pointers,\nB. Write and test a Y86-64 program consisting of the function and test code.\nYou may ﬁnd it useful to pattern your implementation after x86-64 code\nModify the code you wrote for Problem 4.47 to implement the test and swap in\nModify the code you wrote for Problem 4.47 to implement the test and swap in the\n/* Testing Code */\nthe C code shown in Figure 4.69 for a function switchv, along with associated\ntest code.\nstruction set does not include an indirect jump instruction, you can get the same\ninstruction.\nImplement test code similar to what is shown in C to demonstrate that\nPractice Problem 4.3 introduced the iaddq instruction to add immediate data to a\nDescribe the computations performed to implement this instruction.\ndeclaration of a constant IIADDQ having hexadecimal value C, the instruction code\nModify the HCL descriptions of the control logic blocks to implement\nthe iaddq instruction, as described in Practice Problem 4.3 and Problem 4.51.\nThis design would\nhandle all data dependencies by stalling until the instruction generating a needed\nThe ﬁle pipe-stall.hcl contains a modiﬁed version of the HCL code for\n# Use value read from register file\nModify the pipeline control logic at the end of this ﬁle so that it correctly han-\nanalyze the different combinations of control cases, as we did in the design of the\npipeline control logic for PIPE.\ncan occur, since many more conditions require the pipeline to stall.\ninstruction, as described in Practice Problem 4.3 and Problem 4.51.\nThe ﬁle pipe-nt.hcl contains a copy of the HCL code for PIPE, plus a declaration\nof the constant J_YES with value 0, the function code for an unconditional jump\ninstruction.\naddress, to pipeline register M to recover from mispredicted branches.\nThe ﬁle pipe-btfnt.hcl contains a copy of the HCL code for PIPE, plus a decla-\nration of the constant J_YES with value 0, the function code for an unconditional\njump instruction.\nto devise a way to get both valC and valP to pipeline register M to recover from\nIn our design of PIPE, we generate a stall whenever one instruction performs a\nload, reading a value from memory into a register, and the next instruction has this\nFor cases where the second instruction\nstores the source operand to memory, such as with an rmmovq or pushq instruction,\nIn lines 1 and 2, the mrmovq instruction reads a value from memory into\n%rdx, and the pushq instruction then pushes this value onto the stack.\nOur design\nfor PIPE would stall the pushq instruction to avoid a load/use hazard.\nhowever, that the value of %rdx is not required by the pushq instruction until it\nin Figure 4.70, to forward the memory output (signal m_valM) to the valA ﬁeld\nin pipeline register M.\nThe value loaded by the popq instruction is\nExecute and memory stages capable of load forwarding.\nbypass path from the memory output to the source of valA in pipeline register M, we can\nused as part of the address computation by the next instruction, and this value is\nrequired in the execute stage rather than the memory stage.\nA. Write a logic formula describing the detection condition for a load/use haz-\nB. The ﬁle pipe-lf.hcl contains a modiﬁed version of the control logic for\nard in the pipeline control logic set to zero, and so the pipeline control logic\nOur pipelined design is a bit unrealistic in that we have two write ports for the\nregister ﬁle, but only the popq instruction requires two simultaneous writes to the\nThe other instructions could therefore use a single write port, sharing\nof the write-back logic, in which we merge the write-back register IDs (W_dstE\nand W_dstM) into a single signal w_dstE and the write-back values (W_valE and\nfollowing HCL code:\nto use the control logic to dynamically process the instruction popq rA so that it\nhas the same effect as the two-instruction sequence\n(See Practice Problem 4.3 for a description of the iaddq instruction.) Note the\nordering of the two instructions to make sure popq %rsp works properly.\nOn the next cycle, the popq instruction is refetched, but the instruction code\nThis is treated as a special instruction that\nhas the same behavior as the mrmovq instruction listed above.\nThe ﬁle pipe-1w.hcl contains the modiﬁed write port logic described above.\npipeline register D.\nThis deﬁnition can be modiﬁed to insert the instruction code\nIPOP2 the second time the popq instruction is fetched.\na declaration of the signal f_pc, the value of the program counter generated in the\nModify the control logic in this ﬁle to process popq instructions in the manner\n0x100:\n# Start code at address\n0x100\n0x116: 6031\n. The code starts at address 0x100.\nThe ﬁrst instruction requires 10 bytes, while\nprocessor.\nIt must read byte sequences and determine what instructions are to\nand byte sequence for each instruction.\nrmmovq %rsi,0x800(%rbx)\n0x114: 00\n0x216: 90\nC. Code containing illegal instruction speciﬁer byte 0xf0:\n0x300: 50540700000000000000 |\n# Invalid instruction code\nD. Code containing a jump operation:\n0x400:\n0x400: 6113\n0x402: 730004000000000000\nE. Code containing an invalid second byte in a pushq instruction:\n0x500: 6362\n0x502: a0\n# pushq instruction\ncode\nUsing the iaddq instruction, we can rewrite the sum function as\n# Set condition codes\nGcc, running on an x86-64 machine, produces the following code for rproduct:\n# Set condition codes\nThis problem gives you a chance to try your hand at writing assembly code.\n# Set condition codes\n# -x\nThis problem gives you a chance to try your hand at writing assembly code with\nWe show only the code for the loop.\n# -x\nAlthough it is hard to imagine any practical use for this particular instruction, it is\nWe want to determine a reasonable convention for the instruction’s behavior and\nThe subq instruction in this test compares the starting value of %rsp to the\nThis code\nSince the result equals 0xabcd, we can deduce that popq %rsp sets the stack\npointer to the value read from memory.\nIt is therefore equivalent to the instruction\nthe object code that this instruction is located at address 0x016.\nThis instruction sets register %rsp to 128 and increments the PC by 10.",
      "keywords": [
        "instruction",
        "code",
        "Problem",
        "control logic",
        "HCL code",
        "pipeline control logic",
        "PIPE",
        "Practice Problem",
        "instruction code",
        "logic",
        "Memory",
        "processor",
        "design",
        "HCL",
        "instruction set"
      ],
      "concepts": [
        "instruction",
        "instructions",
        "code",
        "codes",
        "processors",
        "processor",
        "memory",
        "memories",
        "pipeline",
        "pipelines"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.751,
          "base_score": 0.601,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.653,
          "base_score": 0.503,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.579,
          "base_score": 0.429,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "pipeline",
          "logic",
          "hcl",
          "control logic"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "pipeline",
          "logic",
          "hcl",
          "control logic"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32282785643111156,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713463+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 514-536)",
      "start_page": 514,
      "end_page": 536,
      "summary": "Register %rsp was set to 120 by the pushq instruction\nThe instruction sets %rax to 9, sets %rsp to 128, and increments the PC by 2.\nthe memory stage the instruction will store valA, the original value of the stack\noccur last, the net effect of the instruction will be to write the value read from\nWe can see that this instruction is located at address 0x037 and is 9 bytes long.\nThe effect of this instruction is to set %rsp to 120, to store 0x040 (the return\naddress) at this memory address, and to set the PC to 0x041 (the call target).\nAll of the HCL code in this and other practice problems is straightforward, but\nThis code is similar to the code for dstE.\npriority over the write via the E port in order to store the value read from memory\nThis code is similar to the code for mem_read.\nbool mem_write = icode in { IRMMOVQ, IPUSHQ, ICALL };\nand C in the ﬁrst stage and D, E, and F in the second.\nThe second stage requires 90 ps, giving a total cycle time of 110 ps and a\nD. The optimal design would be a ﬁve-stage pipeline, with each block in its\nEach stage would have combinational logic requiring 300/k ps and a pipeline\nyet determine whether the data memory will generate an error signal for this\n# Determine status code for fetched instruction\nThis code simply involves preﬁxing the signal names in the code for SEQ with d_\nThe rrmovq instruction (line 5) would stall for one cycle due to a load/use hazard\ninstruction would be in the memory stage, giving both M_dstE and M_dstM equal\ndevising test programs for a new processor.\nFor this example, we can use a slightly modiﬁed version of the program shown\nirmovq $0x100,%rsp\nThe two nop instructions will cause the popq instruction to be in the write-back\nsources in the write-back stage are given the wrong priority, then register %rax\nwill be set to the incremented program counter rather than the value read from\nthe condition, and therefore sets the dstE value to RNONE.\nThis code initializes register %rdx to 0x321.\nWith the altered design, however, the conditional move source value 0x321\nThis code completes the computation of the status code for this instruction.\nThe following test program is designed to set up control combination A (Figure\n# Code to generate a combination of not-taken branch and ret\n# Set Z condition code\nthe ret instruction is actually executed), then the program will execute one of the\nThis code illustrates the care required to\nimplement a test program.\nThe following test program is designed to set up control combination B (Figure\nfor a pipeline register are both set to zero, and so our test program need only set\nThis program uses two initialized words in memory.\nholds the address of the desired return point for the ret instruction.\nThe program\nloads the stack pointer into %rsp and executes the ret instruction.\nFrom Figure 4.66, we can see that pipeline register D must be stalled for a load/use\nFrom Figure 4.66, we can see that pipeline register E must be set to bubble for a\nThis control requires examining the code of the executing instruction and checking\n## Should the condition codes be updated?\nbool set_cc = E_icode == IOPQ &&\nan exception in either the memory or the write-back stage during the current cycle.\nFor stalling the write-back stage, we check only the status of the instruction\nstage, then this instruction would not be able to enter the write-back stage.\nestimate program performance.\nA. The inner loop of the code using the conditional jump has 11 instructions, all\nThe inner loop of the code using the conditional move has 10 instructions,\nbased code is the conditional jump, depending on whether or not the array\ncode.\nC. Our conditional jump code requires an average of 10.5 + 1.0 = 11.5 cycles\nwhile our conditional move code requires 10.0 cycles in all cases.\nusing conditional moves does not affect program performance very much.\nOptimizing Program\nPerformance\nCapabilities and Limitations of Optimizing Compilers\nExpressing Program Performance\nProgram Example\nSummary of Results for Optimizing Combining Code\nUnderstanding Memory Performance\nhe primary objective in writing a program must be to make it work correctly\nA program that runs fast but gives incorrect\nProgrammers must write clear and concise code,\nunderstand the code during code reviews and when modiﬁcations are required\nOn the other hand, there are many occasions when making a program run\nnetwork packets in real time, then a slow-running program will not provide the\nIn this chapter, we will explore how to make programs run faster via\nseveral different types of program optimization.\nWriting an efﬁcient program requires several types of activities.\nmust write source code that the compiler can effectively optimize to turn into\nefﬁcient executable code.\ncapabilities and limitations of optimizing compilers.\nhow a program is written can make large differences in how well a compiler can\noptimize it.\nSome programming languages are more easily optimized than others.\nmake it challenging for a compiler to optimize.\nprograms in ways that make it easier for compilers to generate efﬁcient code.\nIn approaching program development and optimization, we must consider\nIn general, program-\nAt the coding level, many\nlow-level optimizations tend to reduce code readability and modularity, making\nFor code that will be executed repeatedly in a performance-critical environment,\nWe describe a number of techniques for improving code performance.\na compiler would be able to take whatever code we write and generate the most\nefﬁcient possible machine-level program having the speciﬁed behavior.\nEven the best compilers, however, can be thwarted by optimization\nProgrammers must assist the compiler by writing code that can\nThe ﬁrst step in optimizing a program is to eliminate unnecessary work, mak-\ning the code perform its intended task as efﬁciently as possible.\neliminating unnecessary function calls, conditional tests, and memory references.\nTo maximize the performance of a program, both the programmer and the\ncompiler require a model of the target machine, specifying how instructions are\nputers use sophisticated techniques to process a machine-level program, executing\nthe program.\nthe processor, with which we can predict program performance.\nprogram optimization, exploiting the capability of processors to provide instruc-\nseveral program transformations that reduce the data dependencies between dif-\nWe describe the use of code proﬁlers—tools that measure the performance\nof different parts of a program.\nand identify the parts of the program on which we should focus our optimization\nIn this presentation, we make code optimization look like a simple linear\nprocess of applying a series of transformations to the code in a particular order.\nparticular code sequence has a particular execution time.\nStudying the assembly-code representation of a program is one of the most\nerated code will run.\nStarting with the assembly code, we\ndo only as much rewriting of a program as is required to get it to the point where\nthe compiler can then generate efﬁcient code.\nmodify the code and analyze its performance both through measurements and by\nexamining the generated assembly code.\ncode in an attempt to coax the compiler into generating efﬁcient code, but this\nis indeed how many high-performance programs are written.\nalternative of writing code in assembly language, this indirect approach has the\nadvantage that the resulting code will still run on other machines, although per-\nCapabilities and Limitations of Optimizing Compilers\ncomputed in a program and how they are used.\nand to reduce the number of times a given computation must be performed.\nThese can further improve program performance,\nmostly consider code compiled with optimization level -O1, even though level\nof writing a function in C can affect the efﬁciency of the code generated by a\nWe will ﬁnd that we can write C code that, when compiled just with\nCompilers must be careful to apply only safe optimizations to a program,\nmeaning that the resulting program will have the exact same behavior as would\nthe compiler to perform only safe optimizations eliminates possible sources of\nmore of an effort to write programs in a way that the compiler can then transform\ninto efﬁcient machine-level code.\nprogram transformations are safe or not, consider the following two procedures:\nrequires only three memory references (read *xp, read *yp, write *xp), whereas\nit could generate more efﬁcient code based on the computations performed by\ntwiddle1 will perform the following computations:\nother hand, function twiddle2 will perform the following computation:\nIt therefore cannot generate code in the style\nIn performing only safe optimizations, the compiler\nprogram with pointer variables p and q, consider the following code sequence:\nof the major optimization blockers, aspects of programs that can severely limit\nthe opportunities for a compiler to generate optimized code.\ncase is possible, limiting the set of possible optimizations.\nprogram behavior.\n/* Swap value x at xp with value y at yp */\nA second optimization blocker is due to function calls.\nIt is tempting to generate code in\nConsider, however, the following code for f:\nChanging the number of times it gets called changes the program behavior.\nOptimizing function calls by inline substitution\nCode involving function calls can be optimized by a process known as inline substitution (or simply\n“inlining”), where the function call is replaced by the code for the body of the function.\nwe can expand the code for func1 by substituting four instantiations of function f:\nThis transformation both reduces the overhead of the function calls and allows further optimization of\nthe expanded code.\nin func1in to generate an optimized version of the function:\n/* Optimization of inlined code */\nThis code faithfully reproduces the behavior of func1 for this particular deﬁnition of function f.\nThere are times when it is best to prevent a compiler from performing inline substitution.\nThe second is when evaluating the performance of a program\nIt performs basic optimizations, but it does not per-\nform the radical transformations on programs that more “aggressive” compilers\nprograms in a way that simpliﬁes the compiler’s task of generating efﬁcient code.\nExpressing Program Performance\nWe introduce the metric cycles per element, abbreviated CPE, to express program\nperformance in a way that can guide us in improving the code.\nments help us understand the loop performance of an iterative program at a\nIt is appropriate for programs that perform a repetitive compu-\nthe measurements express how many instructions are being executed rather than\nFunction psum1 computes one element of the result vector per iteration.\ntion psum2 uses a technique known as loop unrolling to compute two elements per\nshows a plot of the number of clock cycles required by the two functions for a\nto the timing code and to initiate the procedure, set up the loop, and complete the\nprogram performance.\nPerformance of preﬁx-sum functions.\nnumber of clock cycles per element (CPE).\nFor a set of data points (x1, y1), .\nent variants that preserve the function’s behavior, but with different performance\nProgram Example\nmore efﬁcient code, we will use a running example based on the vector data\ncode/opt/vec.h\ncode/opt/vec.h\nThe declaration uses data_t to designate the data type of the underlying elements.\nIn our evaluation, we measured the performance of our code for integer (C int\nand long), and ﬂoating-point (C float and double) data.\nand running the program separately for different type declarations, such as the\nfollowing for data type long:\nWe allocate the data array block to store the vector elements as an array of len\nis that get_vec_element, the vector access routine, performs bounds checking for\nThis code is similar to the array representations used in\nprogram error, but it can also slow down program execution.\nAs an optimization example, consider the code shown in Figure 5.5, which\ncombines all of the elements in a vector into a single value according to some\nOP, the code can be recompiled to perform different operations on the data.\nthe code, writing different versions of the combining function.\ncode/opt/vec.c\nint get_vec_element(vec_ptr v, long index, data_t *dest)\ncode/opt/vec.c\nIn the actual program, data\nvoid combine1(vec_ptr v, data_t *dest)\nwe measured the CPE performance of the functions on a machine with an Intel\ncharacterize performance in terms of how the programs run on just one particular\nwith those for a number of different compiler/processor combinations, and we\npart of the “black art” of writing fast code.\nways to enable further optimizations by the compiler.\nedly attempting different approaches, performing measurements, and examining\nthe assembly-code representations to identify underlying performance bottle-\noperation (addition or multiplication) and data type (long integer and double-\nOur experiments with many different programs showed\nthe exception of code involving division operations.\nperformance for programs operating on single- or double-precision ﬂoating-point\nnumber of clock cycles required by some code sequence.\nwith optimization level -Og.) For the remainder of our measurements, we use\noptimization levels -O1 and -O2 when generating and measuring our programs.\nObserve that procedure combine1, as shown in Figure 5.5, calls function vec_\nto translate code containing loops into machine-level programs (Section 3.6.7)\ncould therefore compute the vector length only once and use this value in our test\nhas noticeable effect on the overall performance for some data types and oper-\nfurther optimizations.\ncode motion.",
      "keywords": [
        "code",
        "Solution to Problem",
        "Problem",
        "program",
        "data",
        "program performance",
        "performance",
        "Solution",
        "instruction",
        "Practice Problem",
        "compiler",
        "rsp",
        "long",
        "memory",
        "optimization"
      ],
      "concepts": [
        "code",
        "codes",
        "coding",
        "programs",
        "program",
        "programming",
        "programmed",
        "performance",
        "perform",
        "performed"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 19,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.809,
          "base_score": 0.659,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.805,
          "base_score": 0.655,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "code",
          "program",
          "performance",
          "optimization",
          "compiler"
        ],
        "semantic": [],
        "merged": [
          "code",
          "program",
          "performance",
          "optimization",
          "compiler"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4373790629264899,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713522+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 537-558)",
      "start_page": 537,
      "end_page": 558,
      "summary": "void combine2(vec_ptr v, data_t *dest)\nlength out of the loop test, we eliminate the need to execute it on every iteration.\ntimes, (e.g., within a loop), but such that the result of the computation will not\nThe library function strlen is called as part of the loop test of lower1.\nperformance.\nFor a string length of 1,048,576, the function requires just 2.0\nperformance.\nnumber of times each of the four functions is called in code fragments A–C:\nvoid combine3(vec_ptr v, data_t *dest)\nSuppose instead that we add a function get_vec_start to our abstract data\nThis function returns the starting address of the data array, as shown in\nEvidently, other operations\nin the inner loop are forming a bottleneck that limits the performance more\nThe code for combine3 accumulates the value being computed by the combining\noperation at the location designated by the pointer dest.\nWe show here the x86-64 code generated for data type double and with\nmultiplication as the combining operation:\ndest in %rbx, data+i in %rdx, data+length in %rax\nelement in register %rdx, shown in the annotations as data+i.\nacc that is used in the loop to accumulate the computed value.\nshows, the compiler can now use register %xmm0 to hold the accumulated value.\nCompared to the loop in combine3, we have reduced the memory operations per\nacc in %xmm0, data+i in %rdx, data+length in %rax\nWe see a signiﬁcant improvement in program performance, as shown in the\nvoid combine4(vec_ptr v, data_t *dest)\nand write back the updated value on every loop iteration.\nform the combine3 code shown in Figure 5.9 to accumulate the value in a register,\nas it does with the code for combine4 shown in Figure 5.10.\nexample, the case of integer data with multiplication as the operation and 1 as the\nWe achieve performance comparable to that for combine4, except for the case\ndest in %rbx, data+i in %rdx, data+length in %rax\ndest in %rbx, data+i in %rdx, data+length in %rax\nevery last bit of performance requires a detailed analysis of the program as well as\nfor other machines, but the general principles of operation and optimization apply\nresult is that their actual operation is far different from the view that is perceived\ntions are executed one at a time, where each instruction involves fetching values\nfrom registers or memory, performing an operation, and storing results back to\nbe executed in parallel, while presenting an operational view of simple sequential\noperate sufﬁces to understand how they achieve instruction-level parallelism.\nThe latency bound is encountered when a series of operations\nmust be performed in strict sequence, because the result of one operation is\nwhen the data dependencies in the code limit the ability of the processor to\ncomputing capacity of the processor’s functional units.\nultimate limit on program performance.\nOverall Operation\nmeans they can perform multiple operations on every clock cycle and out of order,\ninstructions from memory and generating from these a set of primitive operations\nto perform on program data, and the execution unit (EU), which then executes\nthese operations.\nenough time to decode these and send operations down to the EU.\nOperations\nOperation results\nData\nData\nData\noperations\noperations\nunit is responsible for reading instructions from memory and generating a sequence\nof primitive operations.\nThe execution unit then performs the operations and indicates\nat where it predicts the branch will go, and even begins executing these operations\nto that at the branch point and begins fetching and executing instructions in the\nto perform the task of determining which instructions to fetch.\noperations).\nEach of these operations performs some simple computational task\nFor machines with complex instructions, such as x86 processors, an instruction\ncan be decoded into multiple operations.\ncoded into sequences of operations varies between machines, and this information\nIn a typical x86 implementation, an instruction that only operates on registers,\nis converted into a single operation.\nyields multiple operations, separating the memory references from the arithmetic\noperations.\nThis particular instruction would be decoded as three operations: one\nvalue in register %eax, and one to store the result back to memory.\nThese units can then execute the different parts of multiple instructions in\nThe EU receives operations from the instruction fetch unit.\nThese operations are dispatched to\na set of functional units that perform the actual operations.\nThese functional units\nare specialized to handle different types of operations.\nload unit handles operations that read data from the memory into the processor.\nThis unit has an adder to perform address computations.\nhandles operations that write data from the processor to the memory.\nstore units access memory via a data cache, a high-speed memory containing the\nmost recently accessed data values.\nWith speculative execution, the operations are evaluated, but the ﬁnal results\nare not stored in the program registers or data memory until the processor can\noperations are sent to the EU, not to determine where the branch should go, but\nFigure 5.11 indicates that the different functional units are designed to per-\nform different operations.\nare typically specialized to perform different combinations of integer and ﬂoating-\npoint operations.\nhave increased the total number of functional units, the combinations of opera-\nmetic units are intentionally designed to be able to perform a variety of different\noperations, since the required operations vary widely across different programs.\nFor example, some programs might involve many integer operations, while others\nrequire many ﬂoating-point operations.\nperform integer operations while another could only perform ﬂoating-point oper-\nfunctional units.\nWe see that a store operation requires two functional units—one\nthe mechanics of store (and load) operations in Section 5.12.\nWe can see that this combination of functional units has the potential to\nperform multiple operations of the same type simultaneously.\ncapable of performing integer operations, two that can perform load operations,\nand two that can perform ﬂoating-point multiplication.\nFirst, once the operations for the\nOut-of-order processing was ﬁrst implemented in the Control Data Corporation 6600 processor in\nInstructions were processed by 10 different functional units, each of which could be operated\nthe result of the operation.\nassociation between program register r and tag t for an operation that will update\ndecoded, the operation sent to the execution unit will contain t as the source\nWhen some execution unit completes the ﬁrst operation,\nit generates a result (v, t), indicating that the operation with tag t produced\nvalue, a form of data forwarding.\ndirectly from one operation to another, rather than being written to and read from\nthe register ﬁle, enabling the second operation to begin as soon as the ﬁrst has\nwrite operations.\nWith register renaming, an entire sequence of operations can be performed\nOperation\noperations.\nLatency indicates the total number of clock cycles required to perform the\nactual operations, while issue time indicates the minimum number of cycles between\ntwo independent operations.\nThe capacity indicates how many of these operations can\nThe times for division depend on the data values.\nFunctional Unit Performance\nFigure 5.12 documents the performance of some of the arithmetic operations for\nEach operation is characterized by its latency, meaning the total time\nrequired to perform the operation, the issue time, meaning the minimum num-\nber of clock cycles between two independent operations of the same type, and\nthe capacity, indicating the number of functional units capable of performing that\noperation.\noperations.\nWe see also that the addition and multiplication operations all have\nnew one of these operations.\neach of which performs part of the operation.\noperations to be performed.\nFunctional units with issue times of 1 cycle are said\nto be fully pipelined: they can start a new operation every clock cycle.\nOperations\ncostly operation.\nfunctional unit has a maximum throughput of 1 operation per clock cycle, while\nFor an operation with\nC/I operations per clock cycle.\nperforming ﬂoating-point multiplication operations at a rate of 2 per clock cycle.\nCircuit designers can create functional units with wide ranges of performance\nand ﬂoating-point operations.\nthe number of functional units and their individual performance to achieve op-\nThe latencies, issue times, and capacities of these arithmetic operations can\naffect the performance of our combining functions.\nThe latency bound gives a minimum value for the CPE for any function that must\nperform the combining operation in a strict sequence.\nfunctional units can produce results.\nwith four functional units capable of performing integer addition, the processor\ntwo load units limit the processor to reading at most 2 data values per clock\nAn Abstract Model of Processor Operation\nAs a tool for analyzing the performance of a machine-level program executing on a\nmodern processor, we will use a data-ﬂow representation of programs, a graphical\nnotation showing how the data dependencies between the different operations\nrequired to execute a set of machine instructions.\nCPE measurements obtained for function combine4, our fastest code up to this\nthat the performance of these functions is dictated by the latency of the sum\ncombining operation and K represents the overhead of calling the function and\nFrom Machine-Level Code to Data-Flow Graphs\nvisualize how the data dependencies in a program dictate its performance.\nWe focus just on the computation performed by the loop, since this is the\ntype double with multiplication as the combining operation.\nof data type and operation yield similar code.\nThe compiled code for this loop\nacc in %xmm0, data+i in %rdx, data+length in %rax\nGraphical representation of inner-loop code for combine4.\nare dynamically translated into one or two operations, each of which receives values\nfrom other operations or from registers and produces values for other operations and for\nstructions are expanded by the instruction decoder into a series of ﬁve operations,\nwith the initial multiplication instruction being expanded into a load operation\nto read the source operand from memory, and a mul operation to perform the\nAs a step toward generating a data-ﬂow graph representation of the program,\nare used and updated by the different operations, with the boxes along the top\nrepresenting the register values at the beginning of the loop, and those along the\nas a source value by the cmp operation, and so the register has the same value at\noperations; its new value is generated by the add operation, which is then used\nby the cmp operation.\nRegister %xmm0 is also updated within the loop by the mul\noperation, which ﬁrst uses the initial value as a source value.\nSome of the operations in Figure 5.13 produce values that do not correspond\nThe load operation reads a value from memory and passes it directly to the\nmul operation.\ninstruction, there is no register associated with the intermediate value passing\nThe cmp operation updates the condition codes, and these are\nthen tested by the jne operation.\nFor a code segment forming a loop, we can classify the registers that are\noperations as a data-ﬂow\noperators of Figure 5.13\nthose operations that use\ndata[i]\nThese are used as source values, either as data or to compute mem-\nonly register for the loop in combine4 is %rax.\nThese are used as the destinations of data-movement operations.\nThere are no such registers in this loop.\nfor this loop: they are updated by the cmp operation and used by the jne\noperation, but this dependency is contained within individual iterations.\nsee that %rdx and %xmm0 are loop registers for combine4, corresponding\nto program values data+i and acc.\nAs we will see, the chains of operations between loop registers determine the\nperformance-limiting data dependencies.\nure 5.13, with a goal of showing only those operations and data dependencies that\nthe operators to show more clearly the ﬂow of data from the source registers at\nIn Figure 5.14(a), we also color operators white if they are not part of some\n(cmp) and branch (jne) operations do not directly affect the ﬂow of data in the\nWe assume that the instruction control unit predicts that branch will be\nIn Figure 5.14(b), we have eliminated the operators that were colored white\nabstract template showing the data dependencies that form among loop registers\ndependencies between successive values of program value acc, stored in register\nThe loop computes a new value for acc by multiplying the old value by a\ndata element, generated by the load operation.\ndependencies between successive values of the pointer to the ith data element.\nOn each iteration, the old value is used as the address for the load operation, and\nit is also incremented by the add operation to compute its new value.\nFigure 5.15 shows the data-ﬂow representation of n iterations by the inner loop\nof function combine4.\nWe can see that the program has two chains of data\nloop of combine4.\noperations forms a critical\nperformance.\ndata[0]\ndata[1]\ndependencies, corresponding to the updating of program values acc and data+i\nlimit the program performance.\nof 5 cycles for combine4, when performing ﬂoating-point multiplication.\nThe other operations required during the loop—manipulating and testing pointer\nvalue data+i and reading data from memory—proceed in parallel with the mul-\nThe ﬂow for other combinations of data type and operation are identical to\nthose shown in Figure 5.15, but with a different data operation forming the chain of\nFor all of the cases where the operation has\nof functional units available and the number of data values that can be passed\nthe combining operation, the data operation is sufﬁciently fast that the rest of the\noperations cannot supply data fast enough.\nTo summarize our performance analysis of combine4: our abstract data-ﬂow\nrepresentation of program operation showed that combine4 has a critical path of\nour combining operations can be performed.\nthe operations to enhance instruction-level parallelism.\ncode perform?\nB. On our reference machine, with arithmetic operations having the latencies\niterations due to the operations implementing lines 7–8 of the function.",
      "keywords": [
        "operations",
        "data",
        "operation",
        "loop",
        "functional units",
        "performance",
        "length",
        "code",
        "units",
        "program",
        "instruction",
        "unit",
        "function",
        "register",
        "rdx"
      ],
      "concepts": [
        "operations",
        "operational",
        "operate",
        "operates",
        "operated",
        "operators",
        "data",
        "perform",
        "performing",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 29,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 28,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.58,
          "base_score": 0.58,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.515,
          "base_score": 0.515,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "operations",
          "operation",
          "data",
          "functional",
          "units"
        ],
        "semantic": [],
        "merged": [
          "operations",
          "operation",
          "data",
          "functional",
          "units"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3994036287899745,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713580+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 559-577)",
      "start_page": 559,
      "end_page": 577,
      "summary": "code perform?\ntween iterations due to the operations implementing line 7 of the function.\nthough it requires more operations.\nLoop Unrolling\nLoop unrolling is a program transformation that reduces the number of iterations\nfor a loop by increasing the number of elements computed on each iteration.\nLoop unrolling can improve performance in two ways.\nit reduces the number of operations that do not contribute directly to the program\nresult, such as loop indexing and conditional branching.\nin which we can further transform the code to reduce the number of operations\nsimple loop unrolling, without any further transformations.\nFigure 5.16 shows a version of our combining code using what we will refer\nto as “2 × 1 loop unrolling.” The ﬁrst loop steps through the array two elements\ncombining operation is applied to array elements i and i + 1 in a single iteration.\nFor a vector of length n, we set the loop limit to be n −1.\nWe can generalize this idea to unroll a loop by any factor k, yielding k × 1\nloop unrolling.\nloop apply the combining operation to elements i through i + k −1.\n/* 2 x 1 loop unrolling */\n/* Combine 2 elements at a time */\nApplying 2 × 1 loop unrolling.\n“k × 1 loop unrolling,” since we unroll by a factor of k but accumulate values in a\nModify the code for combine5 to unroll the loop by a factor k = 5.\nWhen we measure the performance of unrolled code for unrolling factors\nk = 2 (combine5) and k = 3, we get the following results:\nk × 1 loop unrolling.\nUnrolling factor k\nto the number of additions required to compute the vector sum, we can reach\nthe point where the 1-cycle latency of integer addition becomes the performance-\nunrolling the loop by up to a factor of 10.\nTo understand why k × 1 unrolling cannot improve performance beyond\nthe latency bound, let us examine the machine-level code for the inner loop of\ncombine5, having k = 2.\nThe following code gets generated when type data_t is\ndouble, and the operation is multiplication:\nInner loop of combine5.\ni in %rdx, data %rax, limit in %rbx, acc in %xmm0\nloop:\nLoop index i is held in register %rdx, and the address of data is held in register\nAs before, the accumulated value acc is held in vector register %xmm0.\nloop unrolling leads to two vmulsd instructions—one to add data[i] to acc, and\n2. The gcc optimizer operates by generating multiple variants of a function and then choosing one that\npointer-based or array-based code has no impact on the performance of programs running on our\nGraphical representation of inner-loop code for combine5.\nmul operation.\noperations as a data-\nThe vmulsd instructions each get translated into two operations:\ncomputation for a vector of length n, obtaining the data-ﬂow representation\nof combine5 operating\nn. Even though the loop\nmul operations along the\ndata[n-2]\ndata[n-1]\ntwo multiplication operations in sequence.\nfactor for the performance of the code without loop unrolling, it remains so with\nk × 1 loop unrolling.\nGetting the compiler to unroll loops\nLoop unrolling can easily be performed by a compiler.\ngcc will perform some forms of loop unrolling when invoked with optimization level 3\noperations every clock cycle, and some of the operations can be performed by\nthis capability, even with loop unrolling, since we are accumulating the value as a\nvalue for acc can start a new operation every clock cycle, it will only start one\nevery L cycles, where L is the latency of the combining operation.\nFor a combining operation that is associative and commutative, such as integer\naddition or multiplication, we can improve performance by splitting the set of\ncombining operations into two or more parts and combining the results at the\nunrolling, to combine more elements per iteration, and two-way parallelism,\nWe therefore refer to this as “2 × 2 loop unrolling.” As\nbefore, we include a second loop to accumulate any remaining array elements for\nthe case where the vector length is not a multiple of 2.\noperation to acc0 and acc1 to compute the ﬁnal result.\nComparing loop unrolling alone to loop unrolling with two-way parallelism,\n/* 2 x 2 loop unrolling */\nApplying 2 × 2 loop unrolling.\nWe see that we have improved the performance for all cases, with integer\nproduct, ﬂoating-point addition, and ﬂoating-point multiplication improving by\nTo understand the performance of combine6, we start with the code and\noperation sequence shown in Figure 5.22.\nGraphical representation of inner-loop code for combine6.\ninstructions, each of which is translated into a load and a mul operation.\nAbstracting combine6 operations as a data-ﬂow graph.\nthe representation of Figure 5.22 to show the data dependencies between successive iterations (a).\nthat there is no dependency between the two mul operations (b).\ndata dependencies between iterations through the process shown in Figure 5.23.\nAs with combine5, the inner loop contains two vmulsd operations, but these\ninstructions translate into mul operations that read and write separate registers,\nwith no data dependency between them (Figure 5.23(b)).\ntemplate n/2 times (Figure 5.24), modeling the execution of the function on a\nto computing the product of even-numbered elements (program value acc0) and\nof combine6 operating\noperations.\ndata[n-2]\ndata[n-1]\none for the odd-numbered elements (program value acc1).\npaths contains only n/2 operations, thus leading to a CPE of around 5.00/2 = 2.50.\nlatency L for the different combinations of data type and combining operation.\nOperationally, the programs are exploiting the capabilities of the functional units\nWe can generalize the multiple accumulator transformation to unroll the loop\nby a factor of k and accumulate k values in parallel, yielding k × k loop unrolling.\nUnrolling factor k\nCPE performance of k × k loop unrolling.\nInteger multiplication and ﬂoating-point addition achieve CPEs of 1.01\nwhen k ≥3, approaching the throughput bound of 1.00 set by their functional units.\nFloating-point multiplication achieves a CPE of 0.51 for k ≥10, approaching the\neven though multiplication is a more complex operation.\nIn general, a program can achieve the throughput bound for an operation\nperforming that operation.\nIn performing the k × k unrolling transformation, we must consider whether it\nHence, for an integer data type, the result computed by combine6 will be\noptimizing compiler could potentially convert the code shown in combine4 ﬁrst\nto a two-way unrolled variant of combine5 by loop unrolling, and then to that\ntransformations to improve performance for integer data.\nimprove performance beyond the latency bound.\nWe saw that the k × 1 loop un-\nrolling of combine5 did not change the set of operations performed in combining\ncode, however, we can fundamentally change the way the combining is performed,\nFigure 5.26 shows a function combine7 that differs from the unrolled code of\ncombine5 (Figure 5.16) only in the way the elements are combined in the inner\nloop.\nIn combine5, the combining is performed by the statement\nwhile in combine7 it is performed by the statement\nare combined with the accumulated value acc, yielding a form of loop unrolling\n/* 2 x 1a loop unrolling */\nincreases the number of operations that can be performed in parallel.\nThe integer addition case matches the performance of k × 1 unrolling\n(combine5), while the other three cases match the performance of the versions\nwith parallel accumulators (combine6), doubling the performance relative to k × 1\nFigure 5.27 illustrates how the code for the inner loop of combine7 (for the\ncase of multiplication as the combining operation and double as data type) gets\ndecoded into operations and the resulting data dependencies.\noperations resulting from the vmovsd and the ﬁrst vmulsd instructions load vector\nelements i and i + 1 from memory, and the ﬁrst mul operation multiplies them\nThe second mul operation then multiples this result by the accumulated\nforms a data-dependency chain between loop registers.\nthis template n/2 times to show the computations performed in multiplying n vec-\ntor elements (Figure 5.29), we see that we only have n/2 operations along the\nThe ﬁrst multiplication within each iteration can be performed with-\nGraphical representation of inner-loop code for combine7.\niteration gets decoded into similar operations as for combine5 or combine6, but with\nAbstracting combine7 operations as a data-ﬂow graph.\nsimplify, and abstract the representation of Figure 5.27 to show the data dependencies\nThe upper mul operation multiplies two 2-vector elements\nwith each other, while the lower one multiplies the result by loop variable acc.\nof combine7 operating\nn/2 operations.\ndata[n-2]\ndata[n-1]\ntion to achieve what we refer to as k × 1a loop unrolling for values up to k = 10.\nWe can see that this transformation yields performance results similar to what is\nachieved by maintaining k separate accumulators with k × k unrolling.\norder in which the vector elements will be combined together.\nand multiplication, the fact that these operations are associative implies that\nUnrolling factor k\nCPE performance for k × 1a loop unrolling.\ntions along the critical path in a computation, resulting in better performance by\ncompilers will not attempt any reassociations of ﬂoating-point operations, since\nperform reassociations of integer operations, but not always with good effects.\ngeneral, we have found that unrolling a loop and accumulating multiple values in\nparallel is a more reliable way to achieve improved program performance.\nConsider the following function for computing the product of an array of n double-\nWe have unrolled the loop by a factor of 3.\nAssume we run these functions on a machine where ﬂoating-point multiplication\nthe data dependencies of the multiplication.\ninstruction, multiple data.” The SSE capability has gone through multiple generations, with more\noperating on entire vectors of data within single instructions.\nAVX instructions can then perform vector operations on these registers, such\nwill read the eight values from memory and perform eight multiplications in parallel, computing\nthat a single instruction is able to generate a computation over multiple data values, hence the term\nvector operations that can be compiled into the vector instructions of AVX (as well as code based\nlanguage, since gcc can also generate code for the vector instructions found on other processors.\nUsing a combination of gcc instructions, loop unrolling, and multiple accumulators, we are able to\nachieve the following performance for our combining functions:\nIn this chart, the ﬁrst set of numbers is for conventional, scalar code written in the style of combine6,\nwritten in a form that gcc can compile into AVX vector code.\nIn addition to using vector operations,\nthis version unrolls the main loop by a factor of 8 and maintains eight separate vector accumulators.\nshow results for both 32-bit and 64-bit numbers, since the vector instructions achieve 8-way parallelism\nWe can see that the vector code achieves almost an eightfold improvement on the four 32-bit cases,\nOnly the long integer multiplication code\ndoes not perform well when we attempt to express it in vector code.\ninclude one to do parallel multiplication of 64-bit integers, and so gcc cannot generate vector code\nUsing vector instructions creates a new throughput bound for the combining operations.\nOur code comes close to achieving these bounds for several combinations of data type\nand operation.\nSummary of Results for Optimizing Combining Code\nto take advantage of the newer SIMD instructions yields additional performance\nthat a program requires a total of N computations of some operation, that the\nmicroprocessor has C functional units capable of performing that operation, and\nof extending the multiple accumulator scheme of combine6 to the cases of k = 10\nof loop variables exceeds the number of available registers, the program must\nAs an example, the following snippet of code shows how accumulator acc0 is\nupdated in the inner loop of the code with 10 × 10 unrolling:\nThe comparable part of the code for 20 × 20 unrolling has a much different\nUpdating of accumulator acc0 in 20 x 20 unrolling\nregisters that most loops will become throughput limited before this occurs.\noperations to perform on what operands.",
      "keywords": [
        "Loop Unrolling",
        "data",
        "Loop",
        "load mul",
        "Unrolling",
        "load mul mul",
        "mul",
        "load load mul",
        "operations",
        "code",
        "load mul add",
        "load",
        "mul load mul",
        "mul add data",
        "load mul load"
      ],
      "concepts": [
        "data",
        "code",
        "coding",
        "operations",
        "operation",
        "operates",
        "operating",
        "operationally",
        "operate",
        "loop"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 27,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 29,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.438,
          "base_score": 0.438,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "",
          "score": 0.397,
          "base_score": 0.397,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.395,
          "base_score": 0.395,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "unrolling",
          "loop",
          "loop unrolling",
          "mul",
          "combine5"
        ],
        "semantic": [],
        "merged": [
          "unrolling",
          "loop",
          "loop unrolling",
          "mul",
          "combine5"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2881910980589514,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713644+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 578-600)",
      "start_page": 578,
      "end_page": 600,
      "summary": "processors capable of executing x86-64 programs, have conditional move instruc-\ngcc can generate code that uses these instructions when compiling condi-\ncombine2 to combine3, when we took the function get_vec_element out of the\ninner loop of the function, as is reproduced below:\nFunction\nthis function, the checks always succeed, and hence they are highly predictable.\nthe following combining code, where we have modiﬁed the inner loop of combine4\nby replacing the access to the data element with the result of performing an\nThis code performs bounds checking and also references the vector\nWe can then directly compare the CPE for the functions with and without bounds\nFunction\nThe additional computation required to perform bounds checking can take place\nprogram execution.\nWrite Code Suitable for Implementation with Conditional Moves\nFor inherently unpredictable cases, program performance can\nin a more “functional” style, where we use conditional operations to compute\nOur measurements for this function show a CPE of around 13.5 for random data\nOur measurements for this function show a CPE of around 4.0 regardless of\nassembly code to make sure that it indeed uses conditional moves.)\nwith conditional data transfers, and so there are inevitably cases where program-\nmers cannot avoid writing code that will lead to conditional branches for which\nof experimentation, writing different versions of the function and then examining\nthe generated assembly code and measuring performance.\nbranch, yielding a CPE (where the number of elements is 2n) of around 15.0 when\nfurther investigate the performance of programs that involve load (reading from\nmemory into registers) and store (writing from registers to memory) operations,\nand how to write code that makes best use of caches.\nAs Figure 5.11 shows, modern processors have dedicated functional units to\nperform load and store operations, and these units have internal buffers to hold\nLoad Performance\nThe performance of a program containing load operations depends on both the\ncombining operations using our reference machine, we saw that the CPE never\ngot below 0.50 for any combination of data type and combining operation, except\nthat they all require reading one value from memory for each element computed.\nWith two load units, each able to initiate at most 1 load operation every clock\nlatency of load operations.\nThe addresses for our load operations depended only\non the loop index i, and so the load operations did not form part of a performance-\nTo determine the latency of the load operation on a machine, we can set up\na computation with a sequence of load operations, where the outcome of one\nAs an example, consider the function list_\nfunction, each successive value of variable ls depends on the value read by the\nOur measurements show that function list_len has\nlong data;\nLinked list function.\nIts performance is limited by the latency of the load\noperation.\noperation.\nTo see this, consider the assembly code for the loop:\nsuccessive value of register %rdi depends on the result of a load operation having\nThus, the load operation for one iteration cannot\nfor this function is determined by the latency of the load operation.\nStore Performance\nIn all of our examples thus far, we analyzed only functions that reference mem-\nory mostly with load operations, reading from a memory location into a register.\nIts counterpart, the store operation, writes a register value to memory.\nAs with the load operation, in most cases, the store operation can operate in a\nthe function shown in Figure 5.32 that sets the elements of an array dest of length\non a machine with a single store functional unit.\nUnlike the other operations we have considered so far, the store operation\noperations cannot create a data dependency.\nOnly a load operation is affected by\nthe result of a store operation, since only a load can read back the memory value\nThe function write_read shown in Figure 5.33\nFunction to set array elements to 0.\nvoid write_read(long *src, long *dst, long n)\nExample B: write_read(&a[0],&a[0],3)\nCode to write and read memory locations, along with illustrative\nThis function highlights the interactions between stores and loads when\nshows two example executions of this function, when it is called for a two-element\nexecutions illustrate some subtleties of the load and store operations.\nIn Example B of Figure 5.33, both arguments src and dest are pointers to\nyield the value stored by the previous execution of the pointer reference *dest.\nDetail of load and store\nData\nData\nData\nData\ngeneral, if function write_read is called with arguments src and dest pointing\nto the same memory location, and with argument cnt having some value n > 0, the\nOur performance measurements show that Example B has\ncontaining the addresses and data of the store operations that have been issued\nThis buffer is provided so that a series of store operations\na load operation occurs, it must check the entries in the store buffer for matching\nentry as the result of the load operation.\ngcc generates the following code for the inner loop of write_read:\nInner loop of write_read\nof inner-loop code\noperations to compute the\nthe data to memory.\ns_data\nFigure 5.35 shows a data-ﬂow representation of this loop code.\nmovq %rax,(%rsi) is translated into two operations: The s_addr instruction com-\nputes the address for the store operation, creates an entry in the store buffer, and\nThe s_data operation sets the data ﬁeld for the\npendently can be important to program performance.\nfunctional units for these operations in the reference machine.\nIn addition to the data dependencies between the operations caused by the\nwriting and reading of registers, the arcs on the right of the operators denote\ncomputation of the s_addr operation must clearly precede the s_data operation.\naddition, the load operation generated by decoding the instruction movq (%rdi),\n%rax must check the addresses of any pending store operations, creating a data\nbetween the s_data and load operations.\ntwo addresses match, the load operation must wait until the s_data has deposited\nits result into the store buffer, but if the two addresses differ, the two operations\nFigure 5.36 illustrates the data dependencies between the operations for the\ninner loop of write_read.\ndependencies involving the load and store operations for special attention.\nbefore the data can be stored.\nload operation to compare its address with that for any pending store operations.\ndata values being stored, loaded, and incremented (only for the case of matching\noperations for write_\noperators of Figure 5.35(a)\noperations that use values\ns_data\ns_data\nWe can now understand the performance characteristics of function write_\nFigure 5.37 illustrates the data dependencies formed by multiple iterations of\ndestination addresses, the load and store operations can proceed independently,\nand destination addresses, the data dependency between the s_data and load\ninstructions causes a critical path to form involving data being stored, loaded, and\nAs these two examples show, the implementation of memory operations in-\nWith memory operations, on the other hand, the processor cannot predict\nEfﬁcient handling of memory operations is critical to the performance of\nmany programs.\nAs another example of code with potential load-store interactions, consider the\nof function write_read.\ndata being stored, loaded,\ns_data\ns_data\ns_data\ns_data\ns_data\ns_data\nC. Our performance measurements indicate that the call of part A has a CPE\nWe saw that our measurements of the preﬁx-sum function psum1 (Figure 5.1) yield\na CPE of 9.00 on a machine where the basic operation to be performed, ﬂoating-\nour function performs so poorly.\nThe following is the assembly code for the inner loop of the function:\nwrite_read (Figure 5.36) to diagram the data dependencies created by this loop,\nWe measured the resulting code to have a CPE of 3.00, limited by the latency of\nof basic strategies for optimizing program performance:\nEliminate excessive function calls.\nRewrite conditional operations in a functional style to enable compi-\neach version of a function as it is being optimized, to ensure no bugs are introduced\nChecking code applies a series of tests to the new versions of\na function and makes sure they yield the same results as the original.\nFor example, checking code that uses loop unrolling\nUp to this point, we have only considered optimizing small programs, where there\nis some clear place in the program that limits its performance and therefore should\nwe describe how to use code proﬁlers, analysis tools that collect performance\ndata about a program as it executes.\nProgram proﬁling involves running a version of a program in which instrumenta-\ntion code has been incorporated to determine how much time the different parts\ncan be performed while running the actual program on realistic benchmark data.\nfor each of the functions in the program.\nmany times each function gets called, categorized by which function performs the\nthe relative importance of the different functions in determining the overall run\nprogram.\nProﬁling with gprof requires three steps, as shown for a C program prog.c,\nperform any optimizations via inline substitution, or else the calls to functions\nThe ﬁrst part of the proﬁle report lists the times spent executing the different\nfunctions, sorted in descending order.\nthis part of the report for the three most time-consuming functions in a program:\nEach row represents the time spent for all calls to some function.\ncolumn indicates the percentage of the overall time spent on the function.\nsecond shows the cumulative time spent by the functions up to and including\nThe third shows the time spent on this particular function,\nIn our example, the function sort_words was called only once, but this single\ncall required 203.66 seconds, while the function find_ele_rec was called 965,027\ntimes (not including recursive calls), requiring a total of 4.85 seconds.\nFunction\nStrlen computes the length of a string by calling the library function strlen.\nLibrary function calls are normally not shown in the results by gprof.\nare usually reported as part of the function calling them.\ncalled 12,511,031 times but only requiring a total of 0.30 seconds.\nThe second part of the proﬁle report shows the calling history of the functions.\nThis history shows both the functions that called find_ele_rec, as well as\nthe functions that it called.\nThe ﬁrst two lines show the calls to the function:\n158,655,725 calls by itself recursively, and 965,027 calls by function insert_string\nFunction find_ele_rec, in turn, called two\nother functions, save_string and new_ele, each a total of 363,039 times.\nFrom these call data, we can often infer useful information about the program\nFor example, the function find_ele_rec is a recursive procedure that\nfunction, comparing the number of recursive calls with the number of top-level\nin which the compiled program maintains a counter for each function record-\ning the time spent executing that function.\nprogram to be interrupted at some regular time interval δ.\nIt then determines what function\nSome other function may run\nery function should be charged according to the relative time spent executing\nFor programs that run for less than around 1 second, however, the numbers\n. By default, the timings for library functions are not shown.\ntimes are incorporated into the times for the calling functions.\nAs an example of using a proﬁler to guide program optimization, we created an ap-\nFor a given value of n, our program\nreads a text ﬁle, creates a table of unique n-grams and how many times each one\nused the function lower1 (Figure 5.7), which we know to have quadratic run\n2. A hash function is applied to the string to create a number between 0 and\nOur initial function simply summed the\nperformed this operation recursively, inserting new elements at the end of the\nFigure 5.38 shows the proﬁle results for six different versions of our n-gram-\nScanning the linked list for a matching n-gram, inserting a new element if\nProﬁle results for different versions of bigram-frequency counting program.\naccording to the different major operations in the program.\nComputing the hash function\nThe sum of all other functions\nrun time and the program sorted 363,039 values.\nIn our next version, we performed sorting using the library function qsort,\nbetween the two list functions.\nBy inserting new n-grams at the end, the ﬁrst function tended to order n-\ngrams in descending order of frequency, while the second function tended to do\nWe therefore created a third list-scanning function that uses\nthat converting recursive code to iterative code would improve its performance\nThat explains why so much of the time is spent performing\nparticular, the maximum code value for a letter is 122, and so a string of n char-\nIn addition, a commutative hash function,\nWe switched to a hash function that uses shift and exclusive-or operations.\nWith this version, shown as “Better hash,” the time drops to 0.6 seconds.\nWe have already seen that function lower1 has quadratic performance,\nattention on the most time-consuming parts of the program and also provides\nFor example, if we had run the original function on data\nhelp us optimize for typical cases, assuming we run the program on representative\ndata, but we should also make sure the program will have respectable performance\nn-gram code, we saw the total execution time drop from 209.0 to 5.4 seconds when\nseconds performing insertion sort, giving α = 0.974, the fraction of time subject\nthe program.\nlatencies, and issue times of the functional units establishes a baseline for predict-\ning program performance.\nin the program, especially between the different iterations of a loop.\nperform those operations.\nloop programs we ﬁrst considered.\nload operations.\ndown the execution time below the procedure level to estimate the performance\nAn abstract version of the function has a CPE of 14–18 with x86-\nOur measurements show that this function has CPEs of 1.50 for integer data\nFor data type double, the x86-64 assembly code\nAssume that the functional units have the characteristics listed in Figure 5.12.\nB. For data type double, what lower bound on the CPE is determined by the\nlower bound on the CPE is determined by the critical path for integer data?\nB. Explain why the performance for ﬂoating-point data did not improve with\nOur measurements for this function with x86-64 give a\nthis function give a CPE of 1.10 for integer data and 1.05 for ﬂoating-point data.\nThis function ﬁlls n bytes of the memory area starting at s with copies of the low-\nImplement a more efﬁcient version of the function by using a word of data\nThat is, the program is able to write\nvalue of sizeof(unsigned long) for the machine on which you run your program.\n. You may not call any library functions.\n. Your code should work for arbitrary values of n, including when it is not a\n. You should write your code so that it will compile and run correctly on any\nyour code so that it starts with byte-level writes until the destination address\noperator, the testing may be performed with unsigned arithmetic.",
      "keywords": [
        "data",
        "function",
        "load operation",
        "operations",
        "program",
        "load",
        "code",
        "CPE",
        "performance",
        "data load",
        "data load add",
        "operation",
        "loop",
        "time",
        "store"
      ],
      "concepts": [
        "programs",
        "program",
        "programming",
        "function",
        "functions",
        "functional",
        "data",
        "performance",
        "performing",
        "performs"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 27,
          "title": "",
          "score": 0.829,
          "base_score": 0.679,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 28,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.577,
          "base_score": 0.577,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.554,
          "base_score": 0.554,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.552,
          "base_score": 0.552,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "load",
          "function",
          "s_data",
          "operations",
          "load operation"
        ],
        "semantic": [],
        "merged": [
          "load",
          "function",
          "s_data",
          "operations",
          "load operation"
        ]
      },
      "topic_id": 6,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4257738574015785,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713706+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 601-619)",
      "start_page": 601,
      "end_page": 619,
      "summary": "This problem illustrates some of the subtle effects of memory aliasing.\nmemory aliasing.\nbecause, with the exception of the ﬁrst iteration, the value read from dest at\nA. The function performs n multiplications and n additions, half the number of\npath, each iteration will require a minimum of 5P clock cycles and will compute\nData dependencies among multiplication operations for cases in Problem 5.8.\nof a store (from the previous iteration), a load, and a ﬂoating-point addition.\nCPE of write_read when there is a data dependency, since write_read involves\nThis code holds last_val in %xmm0, avoiding the need to read p[i-1] from\nmemory and thus eliminating the write/read dependency seen in psum1.\nThe Memory Hierarchy\nThe Memory Hierarchy\nCache Memories\ncomputer system as a CPU that executes instructions and a memory system\nthat holds instructions and data for the CPU.\nIn our simple model, the memory\nsystem is a linear array of bytes, and the CPU can access each memory location in\nIn practice, a memory system is a hierarchy of storage devices with different\nSmall, fast cache memories nearby the CPU act as staging areas for a subset\nof the data and instructions stored in the relatively slow main memory.\nmemory stages data stored on large, slow disks, which in turn often serve as\nstaging areas for data stored on the disks or tapes of other machines connected by\nMemory hierarchies work because well-written programs tend to access the\nAs a programmer, you need to understand the memory hierarchy because it\nIf the data your program\nneeds are stored in a CPU register, then they can be accessed in 0 cycles during\nmain memory, hundreds of cycles.\nunderstand how the system moves data up and down the memory hierarchy, then\nyou can write your application programs so that their data items are stored higher\nPrograms with good locality tend to access more data items from the upper levels\nof the memory hierarchy than programs with poor locality, and thus run faster.\nory, DRAM memory, ROM memory, and rotating and solid state disks—and\ncache memories that act as staging areas between the CPU and main memory, be-\ncharacterize the performance of the memory hierarchy on a particular machine\nas a “memory mountain” that shows read access times as a function of locality.\nmemory.\n2015, typical machines had 300,000 times as much disk storage, and the amount of\nRandom Access Memory\nRandom access memory (RAM)comes in two varieties—static and dynamic.\n(DRAM).\nSRAM is used for cache memories, both on and off the CPU chip.\nDRAM is used for the main memory plus the frame buffer of a graphics system.\nSRAM stores each bit in a bistable memory cell.\nSuch a memory cell is analogous to the inverted pendulum illustrated\nDue to its bistable nature, an SRAM memory cell will retain its value indef-\nCache memory\nDRAM\nMain memory, frame buffers\nCharacteristics of DRAM and SRAM memory.\nDRAM stores each bit as charge on a capacitor.\nDRAM storage can be made very dense—\nhowever, a DRAM memory cell is very sensitive to any disturbance.\nThe memory system must periodically refresh every bit of memory by\nFigure 6.2 summarizes the characteristics of SRAM and DRAM memory.\nSRAM can be accessed faster than DRAM.\nuse more transistors than DRAM cells and thus have lower densities, are more\nConventional DRAMs\nThe cells (bits) in a DRAM chip are partitioned into d supercells, each consisting\nof w DRAM cells.\nA d × w DRAM stores a total of dw bits of information.\nEach supercell has an address of the form (i, j), where i denotes the row\nFor example, Figure 6.3 shows the organization of a 16 × 8 DRAM chip with\nd = 16 supercells, w = 8 bits per supercell, r = 4 rows, and c = 4 columns.\nThe storage community has never settled on a standard name for a DRAM array element.\narchitects tend to refer to it as a “cell,” overloading the term with the DRAM storage cell.\ndesigners tend to refer to it as a “word,” overloading the term with a word of main memory.\n128-bit 16 × 8 DRAM\nMemory\nDRAM chip\nin or out of the chip, and two addr pins that carry two-bit row and column supercell\nEach DRAM chip is connected to some circuitry, known as the memory\ncontroller, that can transfer w bits at a time to and from each DRAM chip.\nthe contents of supercell (i, j), the memory controller sends the row address i to\nthe DRAM, followed by the column address j.\nDRAM address pins.\nFor example, to read supercell (2, 1) from the 16 × 8 DRAM in Figure 6.3, the\nmemory controller sends row address 2, as shown in Figure 6.4(a).\nThe DRAM\nthe memory controller sends column address 1, as shown in Figure 6.4(b).\nDRAM responds by copying the 8 bits in supercell (2, 1) from the row buffer and\nsending them to the memory controller.\nOne reason circuit designers organize DRAMs as two-dimensional arrays\nexample, if our example 128-bit DRAM were organized as a linear array of 16\nMemory\nDRAM chip\nMemory\nDRAM chip\nReading the contents of a DRAM supercell.\nMemory Modules\nDRAM chips are packaged in memory modules that plug into expansion slots on\nmemory module (DIMM),which transfers data to and from the memory controller\nFigure 6.5 shows the basic idea of a memory module.\nstores a total of 64 MB (megabytes) using eight 64-Mbit 8M × 8 DRAM chips,\nEach supercell stores 1 byte of main memory, and each 64-bit\nword at byte address A in main memory is represented by the eight supercells\nDRAM 0 stores the ﬁrst (lower-order) byte, DRAM 1 stores the next byte, and\nTo retrieve the word at memory address A, the memory controller converts\nA to a supercell address (i, j) and sends it to the memory module, which then\nIn response, each DRAM outputs the 8-bit\nforms them into a 64-bit word, which it returns to the memory controller.\nMain memory can be aggregated by connecting multiple memory modules to\nthe memory controller.\nIn the following, let r be the number of rows in a DRAM array, c the number of\ncolumns, br the number of bits needed to address the rows, and bc the number of\nof bits needed to address the rows or columns of the array.\nmemory module.\nDRAM 7\nDRAM 0\nmemory module\nMemory\n64-bit word to CPU chip\n64-bit word at main memory address A\nThere are many kinds of DRAM memories, and new kinds appear on the market\nEach is based on the conventional DRAM cell, with optimizations\nthat improve the speed with which the basic DRAM cells can be accessed.\nexample, to read four supercells from row i of a conventional DRAM, the\nmemory controller must send four RAS/CAS requests, even though the\nrow of an FPM DRAM, the memory controller sends an initial RAS/CAS\ncopies row i into the row buffer and returns the supercell addressed by the\nExtended data out DRAM (EDO DRAM).\nnal that drives the memory controller.\nDouble Data-Rate Synchronous DRAM (DDR SDRAM).\nthe memory.\nNonvolatile Memory\nmemories.\nmemories (ROMs), even though some types of ROMs can be written to as well as\nFlash memories are everywhere,\nAccessing Main Memory\nData ﬂows back and forth between the processor and the DRAM main memory\nCPU and memory is accomplished with a series of steps called a bus transaction.\nA read transaction transfers data from the main memory to the CPU.\ntransaction transfers data from the CPU to the main memory.\nA bus is a collection of parallel wires that carry address, data, and control\nDepending on the particular bus design, data and address signals can share\nis this transaction of interest to the main memory, or to some other I/O device\non the bus an address or a data item?\nincludes the memory controller), and the DRAM memory modules that make up\nmain memory.\nthat connects the CPU to the I/O bridge, and a memory bus that connects the I/O\nchipsets known as the northbridge and the southbridge to connect the CPU to memory and I/O devices,\nand main memory.\nMemory bus\nmemory\nbridge to the main memory.\nsystem bus into the electrical signals of the memory bus.\nbridge also connects the system bus and memory bus to an I/O bus that is shared\nthe memory bus.\nCPU chip called the bus interface initiates a read transaction on the bus.\nThe I/O bridge passes the signal along to the memory bus\nNext, the main memory senses the address signal on the memory\nbus, reads the address from the memory bus, fetches the data from the DRAM,\nand writes the data to the memory bus.\nThe I/O bridge translates the memory bus\nFinally, the CPU senses the data on the system bus, reads the data from the bus,\nand copies the data to register %rax (Figure 6.7(c)).\nConversely, when the CPU performs a store operation such as\nMemory read transaction\n(a) CPU places address A on the memory bus.\nmemory\n(b) Main memory reads A from the bus, retrieves word x, and places it on the bus.\nmemory\n(c) CPU reads word x from the bus, and copies it into register %rax.\nmemory\nwhere the contents of register %rax are written to address A, the CPU initiates\naddress on the system bus.\nThe memory reads the address from the memory bus\n%rax to the system bus (Figure 6.8(b)).\nFinally, the main memory reads the data\nfrom the memory bus and stores the bits in the DRAM (Figure 6.8(c)).\nDisks are workhorse storage devices that hold enormous amounts of data, on\nthousands of megabytes in a RAM-based memory.\nof milliseconds to read information from a disk, a hundred thousand times longer\nthan from DRAM and a million times longer than from SRAM.\n(a) CPU places address A on the memory bus.\nMain memory reads it and waits for the data word.\nmemory\n(b) CPU places data word y on the bus.\nmemory\n(c) Main memory reads data word y from the bus and stores it at address A.\nmemory\nMemory write transaction for a store operation: movq %rax,A.\nEach sector contains an equal number of data bits\nseparated by gaps where no data bits are stored.\nThe maximum number of bits that can be recorded by a disk is known as its max-",
      "keywords": [
        "memory",
        "DRAM",
        "main memory",
        "memory bus",
        "DRAM memory",
        "Memory controller",
        "CPU",
        "DRAM main memory",
        "data",
        "bus",
        "Main memory reads",
        "DRAM chip",
        "DRAM memory cell",
        "FPM DRAM",
        "DRAM memory modules"
      ],
      "concepts": [
        "dram",
        "drams",
        "memory",
        "memories",
        "data",
        "disks",
        "disk",
        "code",
        "coding",
        "codes"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 32,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.579,
          "base_score": 0.579,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.578,
          "base_score": 0.578,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "dram",
          "memory",
          "bus",
          "main memory",
          "memory bus"
        ],
        "semantic": [],
        "merged": [
          "dram",
          "memory",
          "bus",
          "main memory",
          "memory bus"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44390542978220643,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713768+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 620-638)",
      "start_page": 620,
      "end_page": 638,
      "summary": "For measures related to the capacity of I/O devices such as disks and\nThus, modern high-capacity disks use a technique known as multiple zone\nThe capacity of a disk is given by the following formula:\ndisk\nFor example, suppose we have a disk with ﬁve platters, 512 bytes per sector, 20,000\nthe disk is\ndisk\nNotice that manufacturers express disk capacity in units of gigabytes (GB) or\nWhat is the capacity of a disk with 3 platters, 15,000 cylinders, an average of 500\nDisk Operation\nDisks read and write bits stored on the magnetic surface using a read/write head\nThe disk surface\nThe read/write head\nthe disk surface on\nRead/write heads\nDisk dynamics.\nDisks with multiple platters have a separate\nread/write head for each surface, as shown in Figure 6.10(b).\nAt any point in time, all heads are positioned\nThe read/write head at the end of the arm ﬂies (literally) on a thin cushion of\nair over the disk surface at a height of about 0.1 microns and a speed of about 80\nDisks read and write data in sector-size blocks.\nThe access time for a sector\nhas three main components: seek time, rotational latency, and transfer time:\nTo read the contents of some target sector, the arm ﬁrst positions the\nThe average seek time in modern drives, Tavg seek, measured by\nhead arrives at the target track and the rotational speed of the disk.\nworst case, the head just misses the target sector and waits for the disk to\ncan begin to read or write the contents of the sector.\nWe can estimate the average time to access the contents of a disk sector as\nthe sum of the average seek time, the average rotational latency, and the average\nFor example, consider a disk with the following parameters:\nFor this disk, the average rotational latency (in ms) is\nTavg transfer = 60/7,200 RPM × 1/400 sectors/track × 1,000 ms/sec\n. The time to access the 512 bytes in a disk sector is dominated by the seek time\n. Since the seek time and rotational latency are roughly the same, twice the\nseek time is a simple and reasonable rule for estimating disk access time.\n. The access time for a 64-bit word stored in SRAM is roughly 4 ns, and 60 ns\nThus, the time to read a 512-byte sector-size block from memory\nThe disk access time,\nEstimate the average time (in ms) to access a sector on the following disk:\nLogical Disk Blocks\nAs we have seen, modern disks have complex geometries, with multiple surfaces\na sequence of B sector-size logical blocks, numbered 0, 1, .\nhardware/ﬁrmware device in the disk package, called the disk controller, maintains\nthe mapping between logical block numbers and actual (physical) disk sectors.\na disk sector into main memory, it sends a command to the disk controller asking\nit to read a particular logical block number.\na fast table lookup that translates the logical block number into a (surface, track,\nSuppose that a 1 MB ﬁle consisting of 512-byte logical blocks is stored on a disk\nFormatted disk capacity\nBefore a disk can be used to store data, it must be formatted by the disk controller.\ndisk.\nFor each case below, suppose that a program reads the logical blocks of the\nA. Best case: Estimate the optimal time (in ms) required to read the ﬁle given\nthe best possible mapping of logical blocks to disk sectors (i.e., sequential).\nB. Random case: Estimate the time (in ms) required to read the ﬁle if blocks\nare mapped randomly to disk sectors.\ndisks are connected to the CPU and main memory using an I/O bus.\nsystem bus and memory buses, which are CPU-speciﬁc, I/O buses are designed\nbus structure that connects the CPU, main memory, and I/O devices.\nAlthough the I/O bus is slower than the system and memory buses, it can\ngame controllers, printers, external disk drives, and solid state disks.\nmain memory, and I/O\nMemory bus\nDisk drive\nmemory\nDisk\ndisk\n. A host bus adapter that connects one or more disks to the I/O bus using\nSCSI disks are typically faster and more\ncontroller) can support multiple disk drives, as opposed to SATA adapters,\nAdditional devices such as network adapters can be attached to the I/O bus by\nAccessing Disks\nFigure 6.12 summarizes the steps that take place when a CPU reads data from a\ndisk.\nand only one device at a time can access these wires.\nThe CPU issues commands to I/O devices using a technique called memory-\nIn a system with memory-mapped I/O, a block of\nAs a simple example, suppose that the disk controller is mapped to port 0xa0.\nThen the CPU might initiate a disk read by executing three store instructions to\ndisk to initiate a read, along with other parameters such as whether to interrupt\nsecond instruction indicates the logical block number that should be read.\nthe disk sector should be stored.\ndisk is performing the read.\ndisk.\nAfter the disk controller receives the read command from the CPU, it trans-\nlates the logical block number to a sector address, reads the contents of the sector,\nthe CPU (Figure 6.12(b)).\nThis process, whereby a device performs a read or write\nmemory access (DMA).\nAfter the DMA transfer is complete and the contents of the disk sector are\nsafely stored in main memory, the disk controller notiﬁes the CPU by sending an\nReading a disk sector.\n(a) The CPU initiates a disk read by writing a command, logical block number, and\ndestination memory address to the memory-mapped address associated with the disk.\nDisk\nmemory\nDisk\nDisk\nmemory\nDisk\n(b) The disk controller reads the sector and performs a DMA transfer into main memory.\nDisk\nmemory\nDisk\n(c) When the DMA transfer is complete, the disk controller notiﬁes the CPU with an interrupt.\nCharacteristics of a commercial disk drive\nDisk manufacturers publish a lot of useful high-level technical information on their Web sites.\nAverage seek time\nSolid state disk (SSD).\nSolid state disk (SSD)\nwrite logical disk blocks\nSolid State Disks\nA solid state disk (SSD) is a storage technology, based on ﬂash memory (Sec-\nrotating disk.\ndard disk slot on the I/O bus (typically USB or SATA) and behaves like any other\ndisk, processing requests from the CPU to read and write logical disk blocks.\nchanical drive in a conventional rotating disk, and a ﬂash translation layer, which\nis a hardware/ﬁrmware device that plays the same role as a disk controller, trans-\nlating requests for logical blocks into accesses of the underlying physical device.\nAs shown in Figure 6.13, a ﬂash memory consists of a sequence of B\nAvg. sequential read access time\nAvg. sequential write access time\nPerformance characteristics of a commercial solid state disk.\nThroughput numbers are based on reads and writes of\nthat random writing will ever perform as well as reading.\nSSDs have a number of advantages over rotating disks.\naccess times than rotating disks, use less power, and are more rugged.\nSSDs are about 30 times more expensive per byte than rotating disks, and thus the\ntypical storage capacities are signiﬁcantly less than rotating disks.\nSSDs have completely replaced rotating disks in portable music devices, are\nWhile rotating disks are here to stay, it is clear that SSDs are an\nof 470 MB/s (the average sequential write throughput of the device).\nof 303 MB/s (the average random write throughput of the device).\nSRAM is somewhat faster than DRAM, and DRAM is much faster than disk.\nDRAM costs much more than disk.\nthe difference between DRAM and rotating disk.\nAccess times and cost per megabyte have de-\nDRAM access times have decreased by only a factor of 10 (Figure 6.15(b)).\nDisk\nWhile the cost of a megabyte of disk storage has plummeted by a factor\nThese startling long-term trends highlight a basic truth of memory and disk tech-\ndecrease access time.\nDRAM and disk performance are lagging behind CPU performance.As we see\nin Figure 6.15(d), CPU cycle times improved by a factor of 500 between 1985 and\nMin. seek time (ms)\n(c) Rotating disk trends\nthe gap between DRAM and disk performance and CPU performance is actually\ngap was a function of latency, with DRAM and disk access times decreasing\nDisk seek time\nSSD access time\nDRAM access time\nSRAM access time\nCPU cycle time\nEffective CPU cycle time \nThe gap between disk, DRAM, and CPU speeds.\nthroughput, with multiple processor cores issuing requests to the DRAM and disk\naccess and cycle times from Figure 6.15 on a semi-log scale.\nwhen you will be able to buy a petabyte (1015 bytes) of rotating disk storage for\nLocality\nWell-written computer programs tend to exhibit good locality.\nIn a program with good temporal locality, a memory location\nIn a program with good spatial locality, if a memory location is referenced\nWhen cycle time stood still: The advent of multi-core processors\nprocessors (dual-core in 2004 and quad-core in 2007), the effective cycle time continues to decrease at\nprinciple of locality allows computer designers to speed up main memory accesses\nby introducing small fast memories known as cache memories that hold blocks of\nlevel, the principle of locality allows the system to use the main memory as a cache\noperating system uses main memory to cache the most recently used disk blocks in\nthe disk ﬁle system.\ncaching recently referenced documents on a local disk.\nhold recently requested documents in front-end disk caches that satisfy requests\n(a) A function with good locality.\nthe vector elements are accessed in the same order that they are stored in memory.\nLocality of References to Program Data\nDoes this function have good locality?\nAs we see in Figure 6.17(b), the elements of vector v are read sequentially, one\nof spatial locality in programs.\nThe sumarrayrows function enjoys good spatial locality because it\nlocality.\n(a) Another function with good locality.\n(b) Reference pattern for array a (M = 2, N = 3).\nThere is good spatial locality because the array is accessed in the same row-major order in which it is stored\nin memory.\n(b) Reference pattern for array a (M = 2, N = 3).\nThe function has poor spatial locality because it scans memory with a stride-N reference pattern.\nhave on its locality?\nrow-wise, the result is a stride-N reference pattern, as shown in Figure 6.19(b).\nSince program instructions are stored in memory and must be fetched (read)\nby the CPU, we can also evaluate the locality of a program with respect to its\nreads its instructions from memory.\nlocality.\n. Loops have good temporal and spatial locality with respect to instruction\nIt will also become clear to you why programs with good locality\ntypically run faster than programs with poor locality.\nRank-order the functions with respect to the spatial local-\ntimes.\nThe gap between CPU and main memory speed is widening.\nWell-written programs tend to exhibit good locality.\nMain memory holds disk blocks \nretrieved from local disks.\nLocal disks hold files\nretrieved from disks on\n(local disks)\nmore small to moderate-size SRAM-based cache memories that can be accessed\nbut enormous local disks.\nof disks on remote servers that can be accessed over a network.",
      "keywords": [
        "disk",
        "CPU",
        "time",
        "memory",
        "locality",
        "disks",
        "disk controller",
        "CPU cycle time",
        "main memory",
        "disk sector",
        "bus",
        "seek time",
        "spatial locality",
        "disk access time",
        "read"
      ],
      "concepts": [
        "disks",
        "disk",
        "memory",
        "memories",
        "time",
        "times",
        "locality",
        "local",
        "cpu",
        "bus"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "",
          "score": 0.564,
          "base_score": 0.414,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.552,
          "base_score": 0.402,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.517,
          "base_score": 0.367,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "disk",
          "sector",
          "disks",
          "locality",
          "cpu"
        ],
        "semantic": [],
        "merged": [
          "disk",
          "sector",
          "disks",
          "locality",
          "cpu"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2828661688632297,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713820+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 639-661)",
      "start_page": 639,
      "end_page": 661,
      "summary": "device at level k caches a\nThe basic principle of caching in a memory hierarchy.\nIn other words, each level in the hierarchy caches data objects from\nFor example, the local disk serves as a cache for ﬁles (such\nserves as a cache for data on the local disks, and so on, until we get to the smallest\ncache of all, the set of CPU registers.\nFigure 6.22 shows the general concept of caching in a memory hierarchy.\nFigure 6.22, the cache at level k has room for four blocks and currently contains\nCache Hits\nat level k, then we have what is called a cache hit.\na data object from block 14, resulting in a cache hit from level k.\nCache Misses\nIf, on the other hand, the data object d is not cached at level k, then we have what\nis called a cache miss.\nWhen there is a miss, the cache at level k fetches the block\ncontaining d from the cache at level k + 1, possibly overwriting an existing block\nif the level k cache is already full.\nThe decision about which block to replace is governed by the cache’s replacement\nAfter the cache at level k has fetched the block from level k + 1, the program\nfrom block 12 in the level k cache would result in a cache miss because block 12 is\nnot currently stored in the level k cache.\nKinds of Cache Misses\nthe cache at level k is empty, then any access of any data object will miss.\nempty cache is sometimes referred to as a cold cache, and misses of this kind are\nWhenever there is a miss, the cache at level k must implement some placement\nFor caches high in the memory hierarchy (close to\nNotice that our example cache in Figure 6.22\nobjects, but because they map to the same cache block, the cache keeps missing.\nmiss in the cache at level k, even though this cache can hold a total of four blocks.\naccesses some reasonably constant set of cache blocks.\nIn other words, the cache is just too small to handle this particular working set.\nCache Management\nat each level is a cache for the next lower level.\nmust manage the cache.\nthe cache hierarchy.\nThe caches at levels L1, L2, and\nwith virtual memory, the DRAM main memory serves as a cache for data blocks\nWhat cached\nWhere cached\nL1 cache\nOn-chip L1 cache\nL2 cache\nOn-chip L2 cache\nL3 cache\nOn-chip L3 cache\nBuffer cache\nDisk cache\nNetwork cache\nBrowser cache\nWeb cache\nTo summarize, memory hierarchies based on caching work because slower storage\ninto the cache on the ﬁrst miss, we can expect a number of subsequent\nCache Memories\ncache memories.\nCache\na small SRAM cache memory, called an L1 cache (level 1 cache) between the\nThe L1 cache can be\ncalled an L2 cache, between the L1 cache and main memory, that can be accessed\ncache between the CPU and main memory.\nGeneric Cache Memory Organization\nAs illustrated in Figure 6.25(a), a cache for such a\nmachine is organized as an array of S = 2s cache sets.\nEach set consists of E cache\nidentify the block stored in the cache line.\nA of main memory, it sends address A to the cache.\ndoes the cache know whether it contains a copy of the word at address A?\nof cache (S, E, B, m).\n(a) A cache is an array\n(b) The cache\nCache size:   C = B × E × S data bytes\nper cache block\nthe tag in the set identiﬁed by the set index, then the b block offset bits give us the\neach cache, determine the number of cache sets (S), tag bits (t), set index bits (s),\nCache\nCache size (bytes), not including overhead such as the valid and tag bits\nDirect-mapped cache\nCache block\nCache block\nCache block\nDirect-Mapped Caches\nA cache with exactly one line per set (E = 1) is known as a direct-mapped\ncache (see Figure 6.27).\nhow caches work.\nSuppose we have a system with a CPU, a register ﬁle, an L1 cache, and a main\nit requests the word from the L1 cache.\nOtherwise, we have a cache miss, and the CPU must wait while the L1\ncache requests a copy of the block containing w from the main memory.\nthe requested block ﬁnally arrives from memory, the L1 cache stores the block in\none of its cache lines, extracts word w from the stored block, and returns it to the\nmapped cache.\nCache block\nCache block\nCache block\nmapped cache.\ncache block, w0 denotes\ncache line must \ncache hit, and\nSet Selection in Direct-Mapped Caches\nIn this step, the cache extracts the s set index bits from the middle of the address\nset selection works for a direct-mapped cache.\nLine Matching in Direct-Mapped Caches\ndetermine if a copy of the word w is stored in one of the cache lines contained in\nand the tag in the cache line matches the tag in the address of w.\nFigure 6.29 shows how line matching works in a direct-mapped cache.\nexample, there is exactly one cache line in the selected set.\nline is set, so we know that the bits in the tag and block are meaningful.\ntag bits in the cache line match the tag bits in the address, we know that a copy of\nIn other words, we have a cache hit.\nthen we would have had a cache miss.\nWord Selection in Direct-Mapped Caches\nSimilar to our view of a cache as an array of lines, we can think of a block as an\nLine Replacement on Misses in Direct-Mapped Caches\nIf the cache misses, then it needs to retrieve the requested block from the next\nlevel in the memory hierarchy and store the new block in one of the cache lines of\nIn general, if the set is full of valid cache lines,\nFor a direct-mapped cache, where\nThe mechanisms that a cache uses to select sets and identify lines are extremely\nIn other words, the cache has four sets, one line per set, 2 bytes per block, and 4-\n. Since there are eight memory blocks but only four cache sets, multiple blocks\nmap to the same cache set (i.e., they have the same set index).\n. Blocks that map to the same cache set are uniquely identiﬁed by the tag.\n4-bit address space for example direct-mapped cache.\nInitially, the cache is empty (i.e., each valid bit is 0):\nis not really part of the cache.\neach cache line.\nSince the valid bit for set 0 is 0, this is a cache miss.\nThe cache fetches block 0 from memory (or a lower-level cache) and stores the\nThen the cache returns m[0] (the contents of memory location\n0) from block[0] of the newly fetched cache line.\nThis is a cache hit.\nm[1] from block[1] of the cache line.\nSince the cache line in set 2 is not valid, this is a\ncache miss.\nThe cache loads block 6 into set 2 and returns m[13] from block[1]\nof the new cache line.\nThe cache line in set 0 is indeed valid,\nThe cache loads block 4 into set 0 (replacing the\nof the new cache line.\nreferences to blocks that map to the same set, is an example of a conﬂict miss.\nConﬂict Misses in Direct-Mapped Caches\nConﬂict misses in direct-mapped caches typically occur when programs\nand that the cache consists of two sets, for a total cache size of 32 bytes.\nmap to the identical cache set:\nthe same sets of cache blocks.\nYou may be wondering why caches use the middle bits for the set index instead of the high-order bits.\nused as an index, then some contiguous memory blocks will map to the same cache set.\nthe ﬁgure, the ﬁrst four blocks map to the ﬁrst cache set, the second four blocks map to the second set,\nthe cache can only hold a block-size chunk of the array at any point in time.\nthe cache.\nContrast this with middle-bit indexing, where adjacent blocks always map to different cache\nFour-set cache\nWhy caches index with the middle bits.\nand we have room in the cache to hold the blocks for both x[i] and y[i], each\nreference results in a conﬂict miss because the blocks map to the same cache set.\nImagine a hypothetical cache that uses the high-order s bits of an address as the\nFor such a cache, contiguous chunks of memory blocks are mapped to\nthe same cache set.\nWhat is the maximum number of array blocks that are stored in the cache\nSet Associative Caches\nThe problem with conﬂict misses in direct-mapped caches stems from the con-\nassociative cache relaxes this constraint so that each set holds more than one cache\nA cache with 1 < E < C/B is often called an E-way set associative cache.\nSet associative cache\nassociative cache, each\nset associative cache.\nCache block\nCache block\nCache block\nCache block\nCache block\nCache block\nassociative cache.\nCache block\nCache block\nCache block\nCache block\nCache block\nCache block\nthe organization of a two-way set associative cache.\nSet Selection in Set Associative Caches\nSet selection is identical to a direct-mapped cache, with the set index bits identi-\nLine Matching and Word Selection in Set Associative Caches\nLine matching is more involved in a set associative cache than in a direct-mapped\ncache because it must check the tags and valid bits of multiple lines in order to\nThus, we can think of each set in a set associative cache as\nassociative cache.\nof the cache lines \ncache hit, and\nFigure 6.34 shows the basic idea of line matching in an associative cache.\nimportant idea here is that any line in the set can contain any of the memory blocks\nSo the cache must search each line in the set for a valid line\nIf the cache ﬁnds such a line, then we\nLine Replacement on Misses in Set Associative Caches\nwe have a cache miss, and the cache must fetch the block that contains the word\nHowever, once the cache has retrieved the block, which line should\nFully Associative Caches\nA fully associative cache consists of a single set (i.e., E = C/B) that contains all of\nthe cache lines.\nFully associative cache\nassociative cache, a single\nCache block\nCache block\nCache block\nassociative cache.\nCache block\nCache block\nCache block\nThe entire cache is one set, so\nassociative cache.\nEntire cache\nof the cache lines \ncache hit, and\nSet Selection in Fully Associative Caches\nSet selection in a fully associative cache is trivial because there is only one set,\nLine Matching and Word Selection in Fully Associative Caches\nLine matching and word selection in a fully associative cache work the same as\nwith a set associative cache, as we show in Figure 6.37.\n. The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4)\n2-way set associative cache\nThe cache block offset\nThe cache set index\nThe cache tag\nIndicate whether a cache miss occurs.\nCache block offset (CO)\nCache set index (CI)\nCache tag (CT)\nCache hit?\nCache byte returned\nCache block offset (CO)\nCache set index (CI)\nCache tag (CT)\nCache hit?\nCache byte returned\nCache block offset (CO)\nCache set index (CI)\nCache tag (CT)\nCache hit?\nCache byte returned\nFor the cache in Problem 6.12, list all of the hexadecimal memory addresses that\nlower level of the memory hierarchy, store the block in some cache line (possibly\nw that is already cached (a write hit).\ndirty bit for each cache line that indicates whether or not the cache block has been\nallocate, loads the corresponding block from the next lower level into the cache\nand then updates the cache block.\nfrom the next lower level to the cache.\nbypasses the cache and writes the word directly to the next lower level.\nthrough caches are typically no-write-allocate.\nWrite-back caches are typically\na rule, caches at lower levels of the memory hierarchy are more likely to use write-\nvirtual memory systems (which use main memory as a cache for the blocks stored\nwrite-back caches at all levels of modern systems.\nBut, in fact, caches\nan i-cache.\nA cache that\nfeature of this hierarchy is that all of the SRAM cache memories are contained in\nThe time to deliver a word in the cache to the CPU, including the time\nd-cache\ni-cache\nd-cache\ni-cache\nCache type\nCache size (C)\nL1 i-cache\nL1 d-cache\nHowever, for a given cache size, larger blocks imply a smaller number of cache\ni7 compromise with cache blocks that contain 64 bytes.\ncache lines per set.\nthe clock rates would opt for smaller associativity for L1 caches (where the miss\nworks independently of the cache to update memory.\nIn general, caches further down the hierarchy are more",
      "keywords": [
        "cache",
        "cache block",
        "set associative cache",
        "block",
        "caches",
        "associative cache",
        "cache set",
        "cache line",
        "bits",
        "set index bits",
        "blocks",
        "memory",
        "set index",
        "tag",
        "cache set index"
      ],
      "concepts": [
        "caches",
        "caching",
        "cache",
        "cached",
        "blocks",
        "block",
        "memory",
        "memories",
        "sets",
        "misses"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "",
          "score": 0.748,
          "base_score": 0.598,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.466,
          "base_score": 0.466,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.453,
          "base_score": 0.453,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cache",
          "block",
          "cache block",
          "cache cache",
          "block cache"
        ],
        "semantic": [],
        "merged": [
          "cache",
          "block",
          "cache block",
          "cache cache",
          "block cache"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3571977401057637,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713875+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 662-682)",
      "start_page": 662,
      "end_page": 682,
      "summary": "Cache lines, sets, and blocks: What’s the difference?\nIt is easy to confuse the distinction between cache lines, sets, and blocks.\n. A block is a ﬁxed-size packet of information that moves back and forth between a cache and main\nmemory (or a lower-level cache).\n. A line is a container in a cache that stores a block, as well as other information such as the valid\nSets in direct-mapped caches consist of a single line.\nin set associative and fully associative caches consist of multiple lines.\nIn direct-mapped caches, sets and lines are indeed equivalent.\nHowever, in associative caches, sets and\nFor example, systems professionals usually refer to the “line size” of a cache, when what they\nwrite code that is cache friendly, in the sense that it has good locality.\n2. Minimize the number of cache misses in each inner loop.All other things being\nequal, such as the total number of loads and stores, loops with better miss rates\nIs this function cache friendly?\nare local variables, any reasonable optimizing compiler will cache them in the\nIn general, if a cache has a block size of B bytes, then a\nmin (1, (word size × k)/B) misses per loop iteration.\nso the stride-1 references to v are indeed cache friendly.\nthat v is block aligned, words are 4 bytes, cache blocks are 4 words, and the cache\nis initially empty (a cold cache).\nThen, regardless of the cache organization, the\nreferences to v will result in the following pattern of hits and misses:\nIn this example, the reference to v[0] misses and the corresponding block,\nwhich contains v[0]–v[3], is loaded into the cache from memory.\nblock is loaded into the cache, the next three references are hits, and so on.\ncase with a cold cache.\nabout writing cache-friendly code:\ncache them in the register ﬁle (temporal locality).\n. Stride-1 reference patterns are good because caches at all levels of the memory\nhierarchy store data as contiguous blocks (spatial locality).\nSince C stores arrays in row-major order, the inner loop of this function has\nmake the same assumptions about the cache as for sumvec.\nto the array a will result in the following pattern of hits and misses:\nIf we are lucky and the entire array ﬁts in the cache, then we will enjoy the same\nHowever, if the array is larger than the cache (the more likely\ncase), then each and every access of a[i][j] will miss!\n. There is a single L1 data cache that is direct-mapped, write-through, and write-\nallocate, with a block size of 8 bytes.\n. The cache has a total size of 16 data bytes and the cache is initially empty.\n. Accesses to the src and dst arrays are the only sources of read and write\nB. Repeat the problem for a cache with 32 data bytes.\nYou are evaluating its cache performance on a\nmachine with a 2,048-byte direct-mapped data cache with 32-byte blocks (B = 32).\n. The cache is initially empty.\n. The only memory accesses are to the entries of the array grid.\nDetermine the cache performance for the following code:\nB. What is the total number of reads that miss in the cache?\nC. What is the miss rate?\nGiven the assumptions of Practice Problem 6.18, determine the cache perfor-\nB. What is the total number of reads that hit in the cache?\nD. What would the miss hit be if the cache were twice as big?\nGiven the assumptions of Practice Problem 6.18, determine the cache perfor-\nB. What is the total number of reads that hit in the cache?\nD. What would the hit rate be if the cache were twice as big?\nPutting It Together: The Impact of Caches\npact that caches have on the performance of programs running on real machines.\nThe Memory Mountain\nThe rate that a program reads data from the memory system is called the read\nIf a program reads n bytes over a\na tight program loop, then the measured read throughput would give us some\nto the test function in line 37 warms the cache.\nThe size and stride arguments to the run function allow us to control the\ndegree of temporal and spatial locality in the resulting read sequence.\nvalues of size result in a smaller working set size, and thus better temporal\nSmaller values of stride result in better spatial locality.\nfunction repeatedly with different values of size and stride, then we can recover\nThis function is called a memory mountain [112].\nFor example, Figure 6.41 shows the memory\n/* run - Run test(elems, stride) and return read throughput (MB/s).\n\"size\" is in bytes, \"stride\" is in array elements, and Mhz is\n/* Warm up the cache */\nfor a particular computer by calling the run function with different values of size (which corresponds to\ntemporal locality) and stride (which corresponds to spatial locality).\n32 KB L1 d-cache\n256 KB L2 cache\n8 MB L3 cache\n64 B block size\nA memory mountain.\nShows read throughput as a function of temporal and spatial locality.\nwhere the working set ﬁts entirely in the L1 cache, L2 cache, L3 cache, and\nOn each of the L2, L3, and main memory ridges, there is a slope of spatial\nNotice that even when the working set is too large to ﬁt in any of the caches, the\nattempts to fetch those blocks into the cache before they are accessed.\nWorking set size (bytes)\nL1 cache\nL2 cache\nL3 cache\nRidges of temporal locality in the memory mountain.\nure 6.42, we can see the impact of cache size and temporal locality on performance.\nFor sizes up to 32 KB, the working set ﬁts entirely in the L1 d-cache, and thus\nthe working set ﬁts entirely in the uniﬁed L2 cache, and for sizes up to 8 MB, the\nworking set ﬁts entirely in the uniﬁed L3 cache.\nThe dips in read throughputs at the leftmost edges of the L2 and L3 cache\nrespective cache sizes—are interesting.\nThe only way to be sure is to perform a detailed cache simulation, but it is likely\nworking set size constant, gives us some insight into the impact of spatial locality on\nset ﬁts entirely in the L3 cache but is too large for the L2 cache.\nNotice how the read throughput decreases steadily as the stride increases from\nIn this region of the mountain, a read miss in L2 causes a\nper cache line\nthis system equals the block size of 64 bytes, every read request misses in L2 and\nThus, the read throughput for strides of at least eight is\na constant rate determined by the rate that cache blocks can be transferred from\nTo summarize our discussion of the memory mountain, the performance of the\nmemory system is not characterized by a single number.\nheavily used words are fetched from the L1 cache, and to exploit spatial locality\nso that as many words as possible are accessed from a single L1 cache line.\nUse the memory mountain in Figure 6.41 to estimate the time, in CPU cycles, to\nread a 16-byte word from the L1 d-cache.\nof A and B is read n times.\n. There is a single cache with a 32-byte block size (B = 32).\n. The array size n is so large that a single matrix row does not ﬁt in the L1 cache.\nmembers of class AB because they reference arrays A and B (but not C) in their\nstores (writes) in each inner-loop iteration, the number of references to A, B, and\nC that will miss in the cache in each loop iteration, and the total number of cache\nThe inner loops of the class AB routines (Figure 6.44(a) and (b)) scan a row\nSince each cache block holds four 8-byte words, the\na column of B with a stride of n.\nSince n is large, each access of array B results in\nThe inner loops in the class AC routines (Figure 6.44(c) and (d)) have some\nC[i][j] += r*B[k][j];\nC[i][j] += r*B[k][j];\nB misses\nC misses\nArray size (n)\nloop scans the columns of A and C with a stride of n.\ntwo loads and a store, they require one more memory operation than the AB\nOn the other hand, since the inner loop scans both B and C row-wise\nwith a stride-1 access pattern, the miss rate on each array is only 0.25 misses per\nper inner-loop iteration as a function of array size (n).\n. For large values of n, the fastest version runs almost 40 times faster than the\n. Pairs of versions with the same number of memory references and misses per\n. The two versions with the worst memory behavior, in terms of the number of\nnumber of memory accesses.\nmisses per iteration, perform much better than the class AB routines, with\n1.25 misses per iteration, even though the class BC routines perform more\nUsing blocking to increase temporal locality\nThere is an interesting technique called blocking that can improve the temporal locality of inner loops.\n(In this context, “block” refers to an application-level chunk of data, not to a cache block.) The\nprogram is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it\nUnlike the simple loop transformations for improving spatial locality, blocking makes the code\nmemory references in the inner loop (two loads and one store) than the class\n. For large values of n, the performance of the fastest pair of versions (kij and\ncache memories, the prefetching hardware is smart enough to recognize the\nstride-1 access pattern, and fast enough to keep up with memory accesses\ncan access memory locations is not characterized by a single number.\na wildly varying function of program locality (what we have dubbed the memory\naccess most of their data from fast cache memories.\naccess most of their data from the relatively slow DRAM main memory.\nand memory accesses occur.\n. Try to maximize the spatial locality in your programs by reading data objects\nsequentially, with stride 1, in the order they are stored in memory.\nas often as possible once it has been read from memory.\nThe basic storage technologies are random access memories (RAMs), nonvolatile\nmemories (ROMs), and disks.\n(SRAM) is faster and more expensive and is used for cache memories.\na memory system that runs at the rate of the higher levels, but at the cost and\nby writing programs with good spatial and temporal locality.\nbased cache memories is especially important.\nfrom cache memories can run much faster than programs that fetch data primarily\nfrom memory.\nWilkes wrote the ﬁrst paper on cache memories [117].\nand colleagues have proposed a cache-aware memory controller [17].\nto use caches and locality to improve the performance of disk accesses [12, 21].\nSuppose that a 2 MB ﬁle consisting of 512-byte logical blocks is stored on a disk\nFor each case below, suppose that a program reads the logical blocks of the\nB. Random case: Estimate the time (in ms) required to read the ﬁle if blocks\nThe following table gives the parameters for a number of different caches.\neach cache, ﬁll in the missing ﬁelds in the table.\nphysical address bits, C is the cache size (number of data bytes), B is the block\nsize in bytes, E is the associativity, S is the number of cache sets, t is the number of\ntag bits, s is the number of set index bits, and b is the number of block offset bits.\nCache\nThe following table gives the parameters for a number of different caches.\naddress bits, C is the cache size (number of data bytes), B is the block size in bytes,\nE is the associativity, S is the number of cache sets, t is the number of tag bits, s is\nthe number of set index bits, and b is the number of block offset bits.\nCache\nA. List all of the hex memory addresses that will hit in set 1.\nB. List all of the hex memory addresses that will hit in set 6.\nA. List all of the hex memory addresses that will hit in set 2.\nB. List all of the hex memory addresses that will hit in set 4.\nC. List all of the hex memory addresses that will hit in set 5.\nD. List all of the hex memory addresses that will hit in set 7.\n. The memory is byte addressable.\n. Memory accesses are to 1-byte words (not to 4-byte words).\n. The cache is two-way set associative (E = 2), with a 4-byte block size (B = 4)\nThe contents of the cache are as follows, with all addresses, tags, and values given\nThe cache block offset\nThe cache set index\nThe cache tag\nB. For each of the following memory accesses, indicate if it will be a cache hit\nif it can be inferred from the information in the cache.\n. The memory is byte addressable.\n. Memory accesses are to 1-byte words (not to 4-byte words).\n. The cache is 4-way set associative (E = 4), with a 4-byte block size (B = 4)\nConsider the following cache state.\n4-way set associative cache\nA. What is the size (C) of this cache in bytes?\nB. The box that follows shows the format of an address (1 bit per box).\nThe cache block offset\nThe cache set index\nThe cache tag\nSuppose that a program using the cache in Problem 6.30 references the 1-byte\nIndicate the cache entry accessed and the cache byte\nIndicate whether a cache miss occurs.\nIf there is a cache\nB. Memory reference:\nCache tag (CT)\nCache hit?\nCache byte returned\nB. Memory reference:\nCache offset (CO)\nCache index (CI)\nCache tag (CT)\nCache hit?\nCache byte returned\nFor the cache in Problem 6.30, list the eight memory addresses (in hex) that will\n. There is a single L1 data cache that is direct-mapped, write-through, write-\nallocate, with a block size of 16 bytes.\n. The cache has a total size of 32 data bytes, and the cache is initially empty.\n. Accesses to the src and dst arrays are the only sources of read and write\nRepeat Problem 6.34 for a cache with a total size of 128 data bytes.\nThis problem tests your ability to predict the cache behavior of C code.",
      "keywords": [
        "Cache",
        "memory",
        "locality",
        "size",
        "number",
        "read",
        "spatial locality",
        "Memory Mountain",
        "array",
        "stride",
        "read throughput",
        "block size",
        "block",
        "memory system",
        "bytes"
      ],
      "concepts": [
        "cache",
        "caches",
        "memory",
        "memories",
        "read",
        "reading",
        "reads",
        "stride",
        "strides",
        "size"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 32,
          "title": "",
          "score": 0.784,
          "base_score": 0.634,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "",
          "score": 0.594,
          "base_score": 0.444,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.452,
          "base_score": 0.452,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 11,
          "title": "",
          "score": 0.415,
          "base_score": 0.415,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cache",
          "locality",
          "size",
          "stride",
          "memory"
        ],
        "semantic": [],
        "merged": [
          "cache",
          "locality",
          "size",
          "stride",
          "memory"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30051481520512313,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713935+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 683-702)",
      "start_page": 683,
      "end_page": 702,
      "summary": "int sum = 0;\n. Array x begins at memory address 0x0 and is stored in row-major order.\n. In each case below, the cache is initially empty.\n. The only memory accesses are to the entries of the array x.\nA. Case 1: Assume the cache is 512 bytes, direct-mapped, with 16-byte cache\nB. Case 2: What is the miss rate if we double the cache size to 1,024 bytes?\nC. Case 3: Now assume the cache is 512 bytes, two-way set associative using\nan LRU replacement policy, with 16-byte cache blocks.\nWhat is the cache\nD. For case 3, will a larger cache size help to reduce the miss rate?\nE. For case 3, will a larger block size help to reduce the miss rate?\nThis is another problem that tests your ability to analyze the cache behavior of C\n. The machine has a 4 KB direct-mapped cache with a 16-byte block size.\n. Within the two loops, the code uses memory accesses only for the array data.\n. Array a is stored starting at memory address 0x08000000.\nFill in the table for the approximate cache miss rate for the two cases N = 64\ntypedef int array_t[N][N];\nint sumA(array_t a)\nint i, j;\nint sum = 0;\nint sumB(array_t a)\nint i, j;\nint sum = 0;\nint sumC(array_t a)\nint i, j;\nint sum = 0;\nof the following algorithms on a machine with a 1,024-byte direct-mapped data\ncache with 16-byte blocks.\nint c;\nint i, j;\n. The cache is initially empty.\n. The only memory accesses are to the entries of the array square.\nDetermine the cache performance of the following code:\nsquare[i][j].c = 0;\nB. What is the total number of writes that hit in the cache?\nGiven the assumptions in Problem 6.38, determine the cache performance of the\nsquare[j][i].c = 0;\nB. What is the total number of writes that hit in the cache?\nGiven the assumptions in Problem 6.38, determine the cache performance of the\nsquare[i][j].c = 0;\nB. What is the total number of writes that hit in the cache?\nmachine you are working on has a 32 KB direct-mapped cache with 8-byte lines.\nint i, j;\n. buffer begins at memory address 0.\n. The cache is initially empty.\n. The only memory accesses are to the entries of the array buffer.\nWhat percentage of writes in the following code will hit in the cache?\nbuffer[i][j].g = 0;\ncode will hit in the cache?\ncode will hit in the cache?\nto the problem of optimizing code for a memory-intensive application.\na procedure to copy and transpose the elements of an N × N matrix of type int.\nint i, j;\nint i, j;\nThe ﬁle consists of 10,000 512-byte logical blocks.\nSo the total time to read the ﬁle is Tavg seek + Tavg rotation + 2 × Tmax rotation =\ntotal time to read the ﬁle is (Tavg seek + Tavg rotation) × 10,000 = 83,000 ms\nSince these are occurring every 18 months, we might expect\nint i, j, k, product = 1;\nThe key to solving this problem is to visualize how the array is laid out in memory\nthe cache organization induces these partitions in the address bits before you can\nreally understand how caches work.\nCache\ncapacity is 512 32-byte blocks with t = 18 tag bits in each cache line.\nﬁrst 218 blocks in the array would map to set 0, the next 218 blocks to set 1.\nSince our array consists of only (4,096 × 4)/32 = 512 blocks, all of the blocks\nThus, the cache will hold at most 1 array block at\ncache.\nClearly, using high-order bit indexing makes poor use of the cache.\nCache block offset (CO)\nCache hit?\nCache byte returned\nCache block offset (CO)\nCache hit?\nCache byte returned\nCache block offset\nCache hit?\nCache byte returned\nNotice that each cache line holds exactly one row of the array, that the cache\ndst maps to the same cache line.\narrays, references to one array keep evicting useful lines from the other array.\nB. When the cache is 32 bytes, it is large enough to hold both arrays.\nCache\nEach 32-byte cache line holds two contiguous algae_position structures.\nB. What is the total number of read accesses that miss in the cache?\nThe key to this problem is noticing that the cache can only hold 1/2 of the ar-\nB. What is the total number of read accesses that hit in the cache?\nIf the cache were\nB. What is the total number of read accesses that hit in the cache?\ncache size by any amount would not change the miss rate, since cold misses\nRunning Programs\nat the systems software that builds and runs application programs.\ngle ﬁle that can be loaded into memory and executed by the processor.\nmain memory, when in reality multiple programs are running on the sys-\nLoading Executable Object Files\ndata into a single ﬁle that can be loaded (copied) into memory and executed.\nLinking can be performed at compile time, when the source code is translated\ninto machine code; at load time, when the program is loaded into memory and\nexecuted by the loader; and even at run time, by application programs.\nis performed automatically by programs called linkers.\n. Understanding linkers will help you build large programs.\nbuild large programs often encounter linker errors caused by missing modules,\n. Understanding linkers will help you avoid dangerous programming errors.The\ndecisions that Linux linkers make when they resolve symbol references can\nThe resulting programs can exhibit bafﬂing run-time\ncepts.The executable object ﬁles produced by linkers play key roles in impor-\ntant systems functions such as loading and running programs, virtual memory,\ncode/link/main.c\nint array[2] = {1, 2};\nint main()\nint val = sum(array, 2);\ncode/link/main.c\ncode/link/sum.c\nint i, s = 0;\ncode/link/sum.c\nThe example program consists of two source ﬁles, main.c and sum.c. The\nmain function initializes an array of ints, and then calls the sum function to sum the array elements.\ntraditional static linking, to dynamic linking of shared libraries at load time,\nto dynamic linking of shared libraries at run time.\nELF) object ﬁle format.\nConsider the C program in Figure 7.1.\nlinux> gcc -Og -o prog main.c sum.c\nprogram from an ASCII source ﬁle into an executable object ﬁle.\nthe C preprocessor (cpp),1 which translates the C source ﬁle main.c into an ASCII\nexecutable object ﬁle\nNext, the driver runs the C compiler (cc1), which translates main.i into an ASCII\nrelocatable object ﬁle main.o:\nlinker program ld, which combines main.o and sum.o, along with the necessary\nsystem object ﬁles, to create the binary executable object ﬁle prog:\nld -o prog [system object ﬁles and args] /tmp/main.o /tmp/sum.o\nthe code and data in the executable ﬁle prog into memory, and then transfers\nStatic linkers such as the Linux ld program take as input a collection of relocatable\nobject ﬁles and command-line arguments and generate as output a fully linked\nexecutable object ﬁle that can be loaded and run.\nﬁles consist of various code and data sections, where each section is a contiguous\nInstructions are in one section, initialized global variables are\nTo build the executable, the linker must perform two main tasks:\nObject ﬁlesdeﬁneand reference symbols, whereeach\nsymbol corresponds to a function, a global variable, or a static variable\nCompilers and assemblers generate code and data sections\nthe references to those symbols so that they point to this memory location.\nin mind some basic facts about linkers: Object ﬁles are merely collections of blocks\nSome of these blocks contain program code, others contain program\nblocks, and modiﬁes various locations within the code and data blocks.\nRelocatable object ﬁle.\ncombined with other relocatable object ﬁles at compile time to create an\nexecutable object ﬁle.\nExecutable object ﬁle.\nShared object ﬁle.\nA special type of relocatable object ﬁle that can be loaded\ninto memory and linked dynamically, at either load time or run time.\nCompilers and assemblers generate relocatable object ﬁles (including shared\nis a sequence of bytes, and an object ﬁle is an object module stored on disk in a\nObject ﬁles are organized according to speciﬁc object ﬁle formats, which vary\nobject ﬁle.\nFigure 7.3 shows the format of a typical ELF relocatable object ﬁle.\ninformation that allows a linker to parse and interpret the object ﬁle.\nthe size of the ELF header, the object ﬁle type (e.g., relocatable, executable, or\nshared), the machine type (e.g., x86-64), the ﬁle offset of the section header table,\ncontains a ﬁxed-size entry for each section in the object ﬁle.\nA typical ELF relocatable object ﬁle contains the following\n.text The machine code of the compiled program.\n.data Initialized global and static C variables.\nin the object ﬁle; it is merely a placeholder.\nObject ﬁle formats distinguish\ntialized variables do not have to occupy any actual disk space in the object\nAt run time, these variables are allocated in memory with an initial\n.symtab A symbol table with information about functions and global variables\ntakenly believe that a program must be compiled with the -g option to\nIn fact, every relocatable object ﬁle has\ninside a compiler, the .symtab symbol table does not contain entries for\nwhen the linker combines this object ﬁle with others.\ninstruction that calls an external function or references a global variable\nis not needed in executable object ﬁles, and is usually omitted unless the\ninitial value is the address of a global variable or externally deﬁned func-\n.debug A debugging symbol table with entries for local variables and typedefs\ndeﬁned in the program, global variables deﬁned and referenced in the\nprogram, and the original C source ﬁle.\n.line A mapping between line numbers in the original C source program and\nEach relocatable object module, m, has a symbol table that contains information",
      "keywords": [
        "object ﬁle",
        "cache",
        "problem",
        "Object",
        "executable object ﬁle",
        "Solution to Problem",
        "relocatable object ﬁle",
        "int",
        "ﬁle",
        "Relocatable Object",
        "Executable Object",
        "Array",
        "Object Files",
        "memory",
        "code"
      ],
      "concepts": [
        "program",
        "programs",
        "programming",
        "cache",
        "caches",
        "array",
        "arrays",
        "object",
        "disk",
        "disks"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 32,
          "title": "",
          "score": 0.8,
          "base_score": 0.65,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "",
          "score": 0.759,
          "base_score": 0.609,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.641,
          "base_score": 0.641,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.52,
          "base_score": 0.52,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "cache",
          "object",
          "object ﬁle",
          "ﬁle",
          "relocatable"
        ],
        "semantic": [],
        "merged": [
          "cache",
          "object",
          "object ﬁle",
          "ﬁle",
          "relocatable"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37406856755012574,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.713992+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 703-721)",
      "start_page": 703,
      "end_page": 721,
      "summary": ". Global symbols that are deﬁned by module m and that can be referenced by\nGlobal linker symbols correspond to nonstatic C functions and\n. Global symbols that are referenced by module m but deﬁned by some other\nSuch symbols are called externals and correspond to nonstatic C\nfunctions and global variables that are deﬁned in other modules.\n. Local symbols that are deﬁned and referenced exclusively by module m.\ncorrespond to static C functions and global variables that are deﬁned with the\nThese symbols are visible anywhere within module m, but\nIt is important to realize that local linker symbols are not the same as local\nInterestingly, local procedure variables that are deﬁned with the C static\n.data or .bss for each deﬁnition and creates a local linker symbol in the symbol\nmodule deﬁne a static local variable x:\nstatic int x = 0;\nstatic int x = 1;\nIn this case, the compiler exports a pair of local linker symbols with different names\nFor example, it might use x.1 for the deﬁnition in function f and\nThe value is the symbol’s address.\nmodules, the value is an offset from the beginning of the section where the object\nFor executable object ﬁles, the value is an absolute run-time address.\nThe symbol table can also contain entries for the individual sections\nC programmers use the static attribute to hide variable and function declarations inside modules,\nELF symbol table entry.\nEach symbol is assigned to some section of the object ﬁle, denoted by the sec-\nbols that are referenced in this object module but deﬁned elsewhere.\nFor COMMON symbols,\ngcc assign symbols in relocatable object ﬁles to COMMON and .bss using the\nFor example, here are the last three symbol table entries for the relocatable\nobject ﬁle main.o, from the example program in Figure 7.1.\nwhich are not shown, are local symbols that the linker uses internally.\nIn this example, we see an entry for the deﬁnition of global symbol main, a 24-\nbyte function located at an offset (i.e., value) of zero in the .text section.\nis followed by the deﬁnition of the global symbol array, an 8-byte object located\nto the external symbol sum.\nsymbol that is deﬁned or referenced in swap.o, indicate whether or not it will\nhave a symbol table entry in the .symtab section in module swap.o. If so, indicate\nthe module that deﬁnes the symbol (swap.o or m.o), the symbol type (local, global,\nSymbol\nSymbol type\nThe linker resolves symbol references by associating each reference with exactly\none symbol deﬁnition from the symbol tables of its input relocatable object ﬁles.\nSymbol resolution is straightforward for references to local symbols that are de-\nof each local symbol per module.\nables, which get local linker symbols, have unique names.\nResolving references to global symbols, however, is trickier.\npiler encounters a symbol (either a variable or function name) that is not deﬁned\nates a linker symbol table entry, and leaves it for the linker to handle.\nis unable to ﬁnd a deﬁnition for the referenced symbol in any of its input modules,\n/tmp/ccSz5uti.o(.text+0x7): undefined reference to ‘foo’\nSymbol resolution for global symbols is also tricky because multiple object\nmodules might deﬁne global symbols with the same name.\nMangling of linker symbols in C++ and Java\nHow Linkers Resolve Duplicate Symbol Names\nThe input to the linker is a collection of relocatable object modules.\nmodules deﬁnes a set of symbols, some of which are local (visible only to the\nWhat happens if multiple modules deﬁne global symbols with the same name?\nAt compile time, the compiler exports each global symbol to the assembler\nin the symbol table of the relocatable object ﬁle.\nUninitialized global variables get weak symbols.\nGiven this notion of strong and weak symbols, Linux linkers use the following\nFor example, suppose we attempt to compile and link the following two C modules:\nIn this case, the linker will generate an error message because the strong symbol\nbar1.c:(.text+0x0): multiple definition of ‘main’\nSimilarly, the linker will generate an error message for the following modules\nbecause the strong symbol x is deﬁned twice (rule 1):\nint x = 15213;\nint x = 15213;\nHowever, if x is uninitialized in one module, then the linker will quietly choose\nthe strong symbol deﬁned in the other (rule 2):\nint x = 15213;\nint x;\nx = 15212;\nAt run time, function f changes the value of x from 15213 to 15212, which\nof x:\nx = 15212\nint x;\nx = 15213;\nint x;\nx = 15212;\nsymbol deﬁnitions have different types.\nx is inadvertently deﬁned as an int in one module and a double in another:\nint x = 15213;\nx = -0.0;\nthe assignment x = -0.0 in line 6 of bar5.c will overwrite the memory locations\nfor x and y (lines 5 and 6 in foo5.c) with the double-precision ﬂoating-point\n/usr/bin/ld: Warning: alignment 4 of symbol ‘x’ in /tmp/cclUFK5g.o\ndeﬁned global symbols.\nIn Section 7.5, we saw how the compiler assigns symbols to COMMON and\nthe fact that in some cases the linker allows multiple modules to deﬁne global\nsymbols with the same name.\nencounters a weak global symbol, say, x, it does not know if other modules also\ndeﬁne x, and if so, it cannot predict which of the multiple instances of x the linker\nSo the compiler defers the decision to the linker by assigning x to\nOn the other hand, if x is initialized to zero, then it is a strong symbol\nSimilarly, static symbols are unique by construction, so the compiler can\narbitrary reference to symbol x in module i to the deﬁnition of x in module k.\nresolve references to the multiply-deﬁned symbol in each module.\nC. /* Module 1 */\nint x;\nﬁle called a static library, which can then be supplied as input to the linker.\nit builds the output executable, the linker copies only the object modules in the\nnumber of standard functions deﬁned by the C standard.\nrelocatable object module, say, libc.o, that application programmers could link\nlinux> gcc main.c /usr/lib/libc.o\nappropriate object modules into their executables, a process that would be error\nmodules and then packaged in a single static library ﬁle.\ncan then use any of the functions deﬁned in the library by specifying a single\nthe C standard library and the math library could be compiled and linked with a\nAt link time, the linker will only copy the object modules that are referenced\n(In fact, C compiler drivers always pass libc.a to the linker,\nEach routine, deﬁned in its own object module, performs a\nTo create a static library of these functions, we would use the ar tool as follows:\nTo use the library, we might write an application such as main2.c in Figure 7.7,\nTo build the executable, we would compile and link the input ﬁles main2.o\ncode/link/main2.c\nint x[2] = {1, 2};\ncode/link/main2.c\ncompiler driver that the linker should build a fully linked executable object ﬁle\nWhen the linker runs, it determines that the addvec symbol deﬁned by\nSince the program doesn’t reference any symbols deﬁned by multvec.o, the linker\nmodule from libc.a, along with a number of other modules from the C run-time\nHow Linkers Use Static Libraries to Resolve References\nmers because of the way the Linux linker uses them to resolve external references.\nDuring the symbol resolution phase, the linker scans the relocatable object ﬁles\non the command line into .o ﬁles.) During this scan, the linker maintains a set E\nunresolved symbols (i.e., symbols referred to but not yet deﬁned), and a set D of\nsymbols that have been deﬁned in previous input ﬁles.\n. For each input ﬁle f on the command line, the linker determines if f is an\nIf f is an object ﬁle, the linker adds f to E, updates\nU and D to reﬂect the symbol deﬁnitions and references in f , and proceeds\n. If f is an archive, the linker attempts to match the unresolved symbols in U\nagainst the symbols deﬁned by the members of the archive.\nmember m deﬁnes a symbol that resolves a reference in U, then m is added\nto E, and the linker updates U and D to reﬂect the symbol deﬁnitions and\nrelocates the object ﬁles in E to build the output executable ﬁle.\nbecause the ordering of libraries and object ﬁles on the command line is signiﬁcant.\nIf the library that deﬁnes a symbol appears on the command line before the object\nﬁle that references that symbol, then the reference will not be resolved and linking\nlinux> gcc -static ./libvector.a main2.c\n/tmp/cc9XH6Rp.o(.text+0x18): undefined reference to ‘addvec’\nreferences a symbol deﬁned by another member, then the libraries can be placed\nFor example, suppose foo.c calls a function in libx.a\nLet a and b denote object modules or static libraries in the current directory, and\nline (i.e., one with the least number of object ﬁle and library arguments) that will\nallow the static linker to resolve all symbol references.\nOnce the linker has completed the symbol resolution step, it has associated each\nsymbol reference in the code with exactly one symbol deﬁnition (i.e., a symbol\ntable entry in one of its input object modules).\nthe exact sizes of the code and data sections in its input object modules.\nrun-time addresses to each symbol.\n1. Relocating sections and symbol deﬁnitions.\nexample, the .data sections from the input modules are all merged into one\nThe linker then assigns run-time memory addresses to the new aggregate\nsections, to each section deﬁned by the input modules, and to each symbol\nand global variable in the program has a unique run-time memory address.\n2. Relocating symbol references within sections.\nevery symbol reference in the bodies of the code and data sections so that\nrelies on data structures in the relocatable object modules known as relocation\nany externally deﬁned functions or global variables that are referenced by the\nultimate location is unknown, it generates a relocation entry that tells the linker\nhow to modify the reference when it merges the object ﬁle into an executable.\nthe symbol that the modiﬁed reference should point to.\nRelocate a reference that uses a 32-bit PC-relative address.\n/* Offset of the reference to relocate */\nEach entry identiﬁes a reference that must be relocated\nRelocate a reference that uses a 32-bit absolute address.\nThese two relocation types support the x86-64 small code model, which as-\nRelocating Symbol References\neach relocation entry r is a struct of type Elf64_Rela, as deﬁned in Figure 7.9.\ntime addresses for each section (denoted ADDR(s)) and each symbol (denoted\nADDR(r.symbol)).\n13 <main+0x13>\nCode and relocation entries from main.o. The original C code is in Figure 7.1.\nLet’s see how the linker uses this algorithm to relocate the references in our\nThe main function references two global symbols, array and sum.\nthe following line.2 The relocation entries tell the linker that the reference to sum\nshould be relocated using a 32-bit PC-relative address, and the reference to array\nhow the linker relocates these references.\nIn line 6 in Figure 7.11, function main calls the sum function, which is deﬁned in\nmodule sum.o. The call instruction begins at section offset 0xe and consists of the\nr.symbol = sum\nThese ﬁelds tell the linker to modify the 32-bit PC-relative reference starting at\n2. Recall that relocation entries and instructions are actually stored in different sections of the object\nADDR(r.symbol) = ADDR(sum) = 0x4004e8\nIn the resulting executable object ﬁle, the call instruction has the following\nThe mov instruction begins at section offset 0x9 and consists of\nr.symbol = array\nRelocated .text and .data sections for the executable ﬁle prog.\nADDR(r.symbol) = ADDR(array) = 0x601018\nThe linker updates the reference using line 13 of the algorithm in Figure 7.10:\nIn the resulting executable object ﬁle, the reference has the following relocated",
      "keywords": [
        "symbol",
        "linker",
        "symbols",
        "object",
        "object ﬁles",
        "object modules",
        "module",
        "Global symbols",
        "int",
        "reference",
        "modules",
        "local linker symbols",
        "linker symbols",
        "section",
        "symbol table"
      ],
      "concepts": [
        "symbols",
        "symbol",
        "relocated",
        "relocates",
        "relocation",
        "relocating",
        "relocations",
        "relocate",
        "main",
        "module"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.664,
          "base_score": 0.514,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 18,
          "title": "",
          "score": 0.587,
          "base_score": 0.437,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "symbol",
          "linker",
          "symbols",
          "modules",
          "module"
        ],
        "semantic": [],
        "merged": [
          "symbol",
          "linker",
          "symbols",
          "modules",
          "module"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3590791586115472,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714049+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 722-740)",
      "start_page": 722,
      "end_page": 740,
      "summary": "Consider the call to function swap in object ﬁle m.o (Figure 7.5).\nWe have seen how the linker merges multiple object ﬁles into a single executable\ninformation needed to load the program into memory and run it.\nsections to run-time\nTypical ELF executable object ﬁle.\nProgram header table for the example executable prog.\nvaddr/paddr: memory address; align: alignment requirement; filesz: segment size in object ﬁle;\nmemsz: segment size in memory; flags: run-time permissions.\nprogram’s entry point, which is the address of the ﬁrst instruction to execute when\nthe program runs.\na relocatable object ﬁle, except that these sections have been relocated to their\neventual run-time memory addresses.\ncalled _init, that will be called by the program’s initialization code.\nous chunks of the executable ﬁle mapped to contiguous memory segments.\nprogram header table for our example executable prog, as displayed by objdump.\nFrom the program header table, we see that two memory segments will be\ninitialized with the contents of the executable object ﬁle.\nthat the ﬁrst segment (the code segment) has read/execute permissions, starts at\nmemory address 0x400000, has a total size in memory of 0x69c bytes, and is\ninitialized with the ﬁrst 0x69c bytes of the executable object ﬁle, which includes\npermissions, starts at memory address 0x600df8, has a total memory size of 0x230\ndata that will be initialized to zero at run time.\nFor any segment s, the linker must choose a starting address, vaddr, such that\nwhere off is the offset of the segment’s ﬁrst section in the object ﬁle, and align\nﬁle to be transferred efﬁciently to memory when the program executes.\nLoading Executable Object Files\nTo run an executable object ﬁle prog, we can type its name to the Linux shell’s\nprog is an executable object ﬁle, which it runs for us by invoking some memory-\nThe loader copies the code and data in the executable object ﬁle from\ndisk into memory and then runs the program by jumping to its ﬁrst instruction, or\nThis process of copying the program into memory and then running\nEvery running Linux program has a run-time memory image similar to the\nOn Linux x86-64 systems, the code segment starts at address\nThe run-time heap follows the data\nsegment and grows upward via calls to the malloc library.\ntime addresses to the stack, shared library, and heap segments.\nlocations of these regions change each time the program is run, their relative po-\nLinux x86-64 run-time\n(created at run time)\nshared libraries\nexecutable object ﬁle into the code and data segments.\nto the program’s entry point, which is always the address of the _start function.\nThis function is deﬁned in the system object ﬁle crt1.o and is the same for all C\nDynamic Linking with Shared Libraries\ntheir programs against the updated library.\nAnother issue is that almost every C program uses standard I/O functions such\nAt run time, the code for these functions is duplicated in the\ntext segment of each running process.\nand creates a new set of code, data, heap, and stack segments.\nThe new code and data segments are initialized to the contents of the executable\nﬁle by mapping pages in the virtual address space to page-size chunks of the executable ﬁle.\nfrom some header information, there is no copying of data from disk to memory during loading.\nShared libraries are modern innovations that address the disadvantages of\nA shared library is an object module that, at either run time or load\ntime, can be loaded at an arbitrary memory address and linked with a program in\nThis process is known as dynamic linking and is performed by a program\ncalled a dynamic linker.\nShared libraries are also referred to as shared objects, and\nmake heavy use of shared libraries, which they refer to as DLLs (dynamic link\nthis .so ﬁle are shared by all of the executable object ﬁles that reference the library,\na shared library in memory can be shared by different running processes.\nFigure 7.16 summarizes the dynamic linking process for the example program\nTo build a shared library libvector.so of our example vector\nlinux> gcc -shared -fpic -o libvector.so addvec.c multvec.c\nshared libraries.\nDynamic linker (ld-linux.so)\nOnce we have created the library, we would then link it into our example\nThis creates an executable object ﬁle prog2l in a form that can be linked with\nlibvector.so at run time.\nwhen the executable ﬁle is created, and then complete the linking process dynami-\nWhen the loader loads and runs the executable prog2l, it loads the partially\nthe dynamic linker, which is itself a shared object (e.g., ld-linux.so on Linux\nthe loader loads and runs the dynamic linker.\n. Relocating the text and data of libc.so into some memory segment\n. Relocating the text and data of libvector.so into another memory segment\nthe locations of the shared libraries are ﬁxed and do not change during execution\nLoading and Linking Shared Libraries from Applications\nand links shared libraries when an application is loaded, just before it executes.\nload and link arbitrary shared libraries while the application is running, without\nhaving to link in the applications against those libraries at compile time.\na new copy of a shared library, which users can then download and use as a\nThe next time they run their application,\nit will automatically link and load the new shared library.\na shared library.\ndynamically loads and links the appropriate function and then calls it directly,\nfunctions can be updated and new functions can be added at run time, without\napplication programs to load and link shared libraries at run time.\nThe dlopen function loads and links the shared library filename.\nto defer symbol resolution until code from the library is executed.\nThe dlsym function takes a handle to a previously opened shared library and\nThe dlclose function unloads the shared library if no other shared libraries are\nlibvector.so shared library at run time and then invoke its addvec routine.\n/* Dynamically load the shared library containing addvec() */\n/* Get a pointer to the addvec() function we just loaded */\nDynamically loads and links the shared library\nlibvector.so at run time.\nand C++ functions to be called from Java programs.\nfunction, say, foo, into a shared library, say, foo.so.\nfunction foo, the Java interpreter uses the dlopen interface (or something like it) to dynamically link\nA key purpose of shared libraries is to allow multiple running processes to share\nthe same library code in memory and thus save precious memory resources.\nbe to assign a priori a dedicated chunk of the address space to each shared library,\nand then require the loader to always load the shared library at that address.\nshared modules so that they can be loaded anywhere in memory without having to\ncode segment can be shared by an unlimited number of processes.\nShared libraries must always be compiled with\nOn x86-64 systems, references to symbols in the same executable object mod-\nPC-relative addressing and relocated by the static linker when it builds the object\ninteresting fact: no matter where we load an object module (including shared\nat run time\nobject modules) in memory, the data segment is always the same distance from\nand any variable in the data segment is a run-time constant, independent of the\nabsolute memory locations of the code and data segments.\nload time, the dynamic linker relocates each GOT entry so that it contains the\nFigure 7.18 shows the GOT from our example libvector.so shared module.\nin the PC-relative reference to GOT[3] is a run-time constant.\nSuppose that a program calls a function that is deﬁned by a shared library.\ncompiler has no way of predicting the run-time address of the function, since\nthe shared module that deﬁnes it could be loaded anywhere at run time.\nthe dynamic linker could then resolve when the program was loaded.\ncode segment of the calling module.\nshared library such as libc.so.\nthe ﬁrst time the function is called, but each call thereafter costs only a single\nIf an object module calls any functions that are deﬁned in shared libraries, then it\nThe GOT is part of the data segment.\nFigure 7.19 shows how the PLT and GOT work together to resolve the address\nof a function at run time.\nPLT[0] is a special entry that jumps into the dynamic linker.\nlibrary function called by the executable has its own PLT entry.\n# PLT[0]: call dynamic linker\n# PLT[0]: call dynamic linker\nUsing the PLT and GOT to call external functions.\nThe dynamic linker resolves the address of\nwhich initializes the execution environment, calls the main function, and\nEntries starting at PLT[2] invoke functions called\nGOT[1] contain information that the dynamic linker uses when it resolves\nfunction addresses.\nGOT[2] is the entry point for the dynamic linker in\na called function whose address needs to be resolved at run time.\nthe run-time address of function addvec the ﬁrst time it is called:\nInstead of directly calling addvec, the program calls into PLT[2], which\nGOT[1] and then jumps into the dynamic linker indirectly through GOT[2].\nThe dynamic linker uses the two stack entries to determine the run-\ntime location of addvec, overwrites GOT[4] with this address, and passes\nLinux linkers support a powerful technique, called library interpositioning, that\nallows you to intercept calls to shared library functions and execute your own code\nlibrary function is called, validate and trace its input and output values, or even\nexecutes its own logic, then calls the target function and passes its return value\nInterpositioning can occur at compile time, link time, or run time as the\nprogram is being loaded and executed.\nwe will use the example program in Figure 7.20(a) as a running example.\nthe malloc and free functions from the C standard library (libc.so).\nfree as the program runs.\nFigure 7.20 shows how to use the C preprocessor to interpose at compile time.\nEach wrapper function in mymalloc.c (Figure 7.20(c)) calls the target function,\nThe local malloc.h header ﬁle (Figure 7.20(b)) instructs\nHere is how to compile and link the program:\nThe Linux static linker supports link-time interpositioning with the --wrap f ﬂag.\ncode/link/interpose/malloc.h\ncode/link/interpose/malloc.h\n/* malloc wrapper function */\n/* malloc wrapper function */\nprintf(\"malloc(%d) = %p\\n\", (int)size, ptr);\nAnd here is how to link the object ﬁles into an executable:\nlinux> gcc -Wl,--wrap,malloc -Wl,--wrap,free -o intl int.o mymalloc.o\nRun-Time Interpositioning\nCompile-time interpositioning requires access to a program’s source ﬁles.\nexecutable object ﬁle.\nprogram, the dynamic linker (ld-linux.so) will search the LD_PRELOAD libraries\nﬁrst, before any other shared libraries, when it resolves undeﬁned references.\nthis mechanism, you can interpose on any function in any shared library, including\nHere is how to build the shared library that contains the wrapper functions:\n/* malloc wrapper function */\nprintf(\"malloc(%d) = %p\\n\", (int)size, ptr);\nRun-time interpositioning with LD_PRELOAD.\nLinux systems also provide the ldd program for manipulating shared libraries:\nldd: Lists the shared libraries that an executable needs at run time.\nLinking can be performed at compile time by static linkers and at load time and run\ntime by dynamic linkers.\nLinkers manipulate binary ﬁles called object ﬁles, which\nobject ﬁles are combined by static linkers into an executable object ﬁle that can\nbe loaded into memory and executed.\nShared object ﬁles (shared libraries) are\nlinked and loaded by dynamic linkers at run time, either implicitly when the calling\nprogram is loaded and begins executing, or on demand, when the program calls\nfunctions from the dlopen library.\nmemory address for each symbol is determined and where references to those\nmultiple relocatable object ﬁles into a single executable object ﬁle.\nuse libraries to resolve symbol references in other object modules.",
      "keywords": [
        "executable object ﬁle",
        "object ﬁle",
        "shared",
        "object",
        "shared library",
        "shared libraries",
        "PLT",
        "code",
        "Executable Object",
        "dynamic linker",
        "function",
        "program",
        "memory",
        "data segment",
        "code segment"
      ],
      "concepts": [
        "function",
        "functions",
        "functionality",
        "memory",
        "library",
        "libraries",
        "segments",
        "segment",
        "called",
        "calling"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.712,
          "base_score": 0.712,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.652,
          "base_score": 0.652,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.639,
          "base_score": 0.639,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.637,
          "base_score": 0.637,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.621,
          "base_score": 0.621,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "shared",
          "shared library",
          "library",
          "run time",
          "shared libraries"
        ],
        "semantic": [],
        "merged": [
          "shared",
          "shared library",
          "library",
          "run time",
          "shared libraries"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.48916999681341433,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.714136+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 741-758)",
      "start_page": 741,
      "end_page": 758,
      "summary": "and relocating the references in the program.\ning, linking, and running x86-64 programs, including the rules for relocation and\nof the swap.c function that counts the number of times it has been called:\nhave a symbol table entry in the .symtab section in module swap.o. If so, indicate\nthe module that deﬁnes the symbol (swap.o or m.o), the symbol type (local, global,\nWhen this program is compiled and executed on an x86-64 Linux system, it\nprints the string 0x48\\n and terminates normally, even though function p2 never\nThe program header in Figure 7.14 indicates that the data segment occupies 0x230\nConsider the call to function swap in object ﬁle m.o (Problem 7.6).\nA. Suppose that the linker relocates .text in m.o to address 0x4004e0 and swap\nB. Suppose that the linker relocates .text in m.o to address 0x4004d0 and swap\nB. This is an error, because each module deﬁnes a strong symbol main (rule 1).\nB. The hex value of the relocated reference in line 5 is 0x5.\nThus, in the resulting executable object ﬁle, the PC-relative reference to swap has\nExceptional Control Flow\nExceptions\nProcesses\nProcess Control\nSuch instructions are necessary mechanisms that allow programs\nthe execution of the program.\nIn general, we refer to these abrupt changes as exceptional control\nto exception handlers.\nAt the operating systems level, the kernel transfers control\nis the basic mechanism that operating systems use to implement I/O, processes,\nthe current process are all accomplished by application programs invoking\n. Understanding ECF will help you write interesting new application programs.\nThe operating system provides application programs with powerful ECF\nnotifying other processes of exceptional events in the system, and detecting\nthen you can use them to write interesting programs such as Unix shells and\nare all examples of concurrency in action: an exception handler that interrupts\nthe execution of an application program; processes and threads whose exe-\ncution overlap in time; and a signal handler that interrupts the execution of\nan application program.\n. Understanding ECF will help you understand how software exceptions work.\nLanguages such as C++ and Java provide software exception mechanisms via\nSoftware exceptions allow the program\ncalls, which are exceptions that provide applications with entry points into the\nExceptions\nExceptions are a form of exceptional control ﬂow that are implemented partly\ngeneral understanding of exceptions and exception handling and to help demystify\nAn exception is an abrupt change in the control ﬂow in response to some\nIn the ﬁgure, the processor is executing some current instruction Icurr when a\napplication-level ECF mechanism provided by C++ and Java in the form of catch, throw, and try\n(an exception) from the\napplication program to an\nexception handler.\nprogram or aborts.\nprogram\nException\nException\nException\nprocessing\nException\nThe event might be directly related to the execution of the current instruction.\nunrelated to the execution of the current instruction.\ntable, to an operating system subroutine (the exception handler) that is speciﬁcally\nWhen the exception handler\nthat caused the exception:\n1. The handler returns control to the current instruction Icurr, the instruction\n2. The handler returns control to Inext, the instruction that would have executed\nnext had the exception not occurred.\n3. The handler aborts the interrupted program.\nException Handling\nExceptions can be difﬁcult to understand because handling them involves close\nException table.\nexception table is a\nexception k.\nexception handler 0\nexception handler 1\nexception handler 2\nexception handler n \u0002 1\nException\nof an exception handler.\nThe exception number is\nException table\nfor exception # k\nException number\nException table\ninteger exception number.\ning system allocates and initializes a jump table called an exception table, so that\nentry k contains the address of the handler for exception k.\nAt run time (when the system is executing some program), the processor\ndetects that an event has occurred and determines the corresponding exception\ncedure call, through entry k of the exception table, to the corresponding handler.\nFigure 8.3 shows how the processor uses the exception table to form the address of\nthe appropriate exception handler.\ntion, the return address is either the current instruction (the instruction that\nwas executing when the event occurred) or the next instruction (the instruc-\ntion that would have executed after the current instruction had the event not\nwill be necessary to restart the interrupted program when the handler returns.\n. When control is being transferred from a user program to the kernel, all of\n. Exception handlers run in kernel mode (Section 8.2.4), which means they have\nsoftware by the exception handler.\nAfter the handler has processed the event, it\noptionally returns to the interrupted program by executing a special “return from\nexception interrupted a user program, and then returns control to the interrupted\nprogram.\nClasses of Exceptions\nExceptions can be divided into four classes: interrupts, traps, faults, and aborts.\nException\nhandlers for hardware interrupts are often called interrupt handlers.\na pin on the processor chip and placing onto the system bus the exception number\nAsync/sync\nAlways returns to next instruction\nAlways returns to next instruction\nMight return to current instruction\nClasses of exceptions.\nAsynchronous exceptions occur as a result of events in I/O devices that\nSynchronous exceptions occur as a direct result of executing an instruction.\napplication program’s\nhandler returns control\nthe application program’s\nAfter the current instruction ﬁnishes executing, the processor notices that the\ninterrupt pin has gone high, reads the exception number from the system bus, and\nthen calls the appropriate interrupt handler.\ncontrol to the next instruction (i.e., the instruction that would have followed the\ncurrent instruction in the control ﬂow had the interrupt not occurred).\nthat the program continues executing as though the interrupt had never happened.\nThe remaining classes of exceptions (traps, faults, and aborts) occur syn-\nchronously as a result of executing the current instruction.\nstruction as the faulting instruction.\nTraps are intentional exceptions that occur as a result of executing an instruction.\nLike interrupt handlers, trap handlers return control to the next instruction.\na ﬁle (read), creating a new process (fork), loading a new program (execve), and\nservices, processors provide a special syscall n instruction that user programs can\nExecuting the syscall instruction\ncauses a trap to an exception handler that decodes the argument and calls the\nnot, the fault handler either\nre-executes the faulting\n(4) Handler either re-executes\nprogram.\nrun in user mode, which restricts the types of instructions they can execute, and\nmode, which allows it to execute privileged instructions and access a stack deﬁned\na fault occurs, the processor transfers control to the fault handler.\nis able to correct the error condition, it returns control to the faulting instruction,\nkernel that terminates the application program that caused the fault.\nA classic example of a fault is the page fault exception, which occurs when\nan instruction references a virtual address whose corresponding page is not res-\npage fault handler loads the appropriate page from disk and then returns control\nto the instruction that caused the fault.\nWhen the instruction executes again, the\nhandlers never return control to the application program.\nthe handler returns control to an abort routine that terminates the application\nprogram.\nException number\nException class\nExamples of exceptions in x86-64 systems.\nExceptions in Linux/x86-64 Systems\nTo help make things more concrete, let’s look at some of the exceptions deﬁned\nLinux/x86-64 Faults and Aborts\nA divide error (exception 0) occurs when an application attempts\nThe infamous general protection fault (exception 13)\nﬁned area of virtual memory or because the program attempts to write to a\nA page fault (exception 14) is an example of an exception where\nA machine check (exception 18) occurs as a result of a fatal\nhardware error that is detected during the execution of the faulting in-\nMachine check handlers never return control to the application\nprogram.\nLinux provides hundreds of system calls that application programs use when they\nExecute a program\nExamples of popular system calls in Linux x86-64 systems.\n(Notice that this jump table is not the same as the exception\nC programs can invoke any system call directly by using the syscall function.\ninstruction, and then pass the return status of the system call back to the calling\nprogram.\nSystem calls are provided on x86-64 systems via a trapping instruction called\nIt is quite interesting to study how programs can use this instruction\nterm and distinguish between asynchronous exceptions (interrupts) and synchronous exceptions (traps,\nImplementing the hello program directly with Linux system calls.\nsyscall instruction to invoke the write and exit system calls directly.\nProcesses\nExceptions are the basic building blocks that allow the operating system kernel\nillusion that our program is the only one currently running in the system.\nprogram appears to have exclusive use of both the processor and the memory.\nThe processor appears to execute the instructions in our program, one after the\nThe classic deﬁnition of a process is an instance of a program in execution.\nEach program in the system runs in the context of some process.\nprogram’s code and data stored in memory, its stack, the contents of its general-\nEach time a user runs a program by typing the name of an executable object\nﬁle to the shell, the shell creates a new process and then runs the executable object\nApplication programs can also create new\nprocesses and run either their own code or other applications in the context of the\nA process provides each program with the illusion that it has exclusive use of the\nprocessor, even though many other programs are typically running concurrently\ncorresponded exclusively to instructions contained in our program’s executable\nobject ﬁle or in shared objects linked into our program dynamically at run time.\none for each process.",
      "keywords": [
        "system",
        "exception",
        "System Call",
        "exception handler",
        "program",
        "handler",
        "instruction",
        "Control",
        "Exceptions",
        "handler returns control",
        "Linux system calls",
        "exception table",
        "handler returns",
        "ECF",
        "operating system"
      ],
      "concepts": [
        "program",
        "programs",
        "programming",
        "exceptional",
        "exceptions",
        "exception",
        "processes",
        "process",
        "processing",
        "processed"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 35,
          "title": "",
          "score": 0.83,
          "base_score": 0.68,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 14,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 17,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.712,
          "base_score": 0.712,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 13,
          "title": "",
          "score": 0.67,
          "base_score": 0.52,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "exception",
          "handler",
          "exceptions",
          "exception handler",
          "instruction"
        ],
        "semantic": [],
        "merged": [
          "exception",
          "handler",
          "exceptions",
          "exception handler",
          "instruction"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4589149989086442,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714198+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 759-776)",
      "start_page": 759,
      "end_page": 776,
      "summary": "Processes provide each\na process.\nProcess A\nProcess B\nProcess C\na process.\nProcess A runs for a while, followed by B, which runs to completion.\nProcess C\nThe key point in Figure 8.12 is that processes take turns using the processor.\nEach process executes a portion of its ﬂow and then is preempted (temporarily\ncontext of one of these processes, it appears to have exclusive use of the proces-\nprocesses, signal handlers, threads, and Java processes are all examples of logical\nFor example, in Figure 8.12, processes A and B run concurrently, as do\nEach time period that a process executes a portion of its\nexample, in Figure 8.12, the ﬂow for process A consists of two time slices.\nConsider three processes with the following starting and ending times:\nProcess\nFor each pair of processes, indicate whether they run concurrently (Y) or\nProcess pair\nA process provides each program with the illusion that it has exclusive use of the\nA process provides each program\nread or written by any other process.\nLinux process.\ncontains the code, data, and stack that the kernel uses when it executes instructions\non behalf of the process (e.g., when the application program executes a system\nIn order for the operating system kernel to provide an airtight process abstraction,\nProcess address space.\nthe mode bit is set, the process is running in kernel mode (sometimes called\nA process running in kernel mode can execute any instruction\nWhen the mode bit is not set, the process is running in user mode.\nA process\nA process running application code is initially in user mode.\nthe process to change from user mode to kernel mode is via an exception such as\nuser mode processes to access the contents of kernel data structures.\nThe kernel maintains a context for each process.\nthat the kernel needs to restart a preempted process.\ndata structures such as a page table that characterizes the address space, a process\nAt certain points during the execution of a process, the kernel can decide\nWhen the kernel selects a new process to run, we say that the kernel\nhas scheduled that process.\nAfter the kernel has scheduled a new process to run,\nmechanism called a context switch that (1) saves the context of the current process,\nthen the kernel can put the current process to sleep and switch to another process.\nperform a context switch and run another process instead of waiting for the data\nexplicit request to put the calling process to sleep.\nreturn control to the calling process.\nthe current process has run long enough and switch to a new process.\nFigure 8.14 shows an example of context switching between a pair of processes\nIn this example, initially process A is running in user mode until it traps to\nAnatomy of a process\nProcess A\nProcess B\nperforms a context switch from process A to B.\nkernel is executing instructions in user mode on behalf of process A (i.e., there\nis no separate kernel process).\nexecuting instructions in kernel mode on behalf of process A.\nit begins executing instructions (still in kernel mode) on behalf of process B.\nafter the switch, the kernel is executing instructions in user mode on behalf of\nprocess B.\nProcess B then runs for a while in user mode until the disk sends an interrupt\nthat process B has run long enough and performs a context switch from process B\nto A, returning control in process A to the instruction immediately following the\nProcess A continues to run until the next exception occurs, and\nThe wrapper calls the base function, checks for errors, and terminates\nProcess Control\nUnix provides a number of system calls for manipulating processes from C pro-\nfunction returns the PID of the calling process.\nPID of its parent (i.e., the process that created the calling process).\nReturns: PID of either the caller or the parent\nCreating and Terminating Processes\nThe process is either executing on the CPU or waiting to be executed\nThe execution of the process is suspended and will not be scheduled.\nThe process is stopped permanently.\nA process becomes termi-\nis to terminate the process, (2) returning from the main routine, or (3)\nThe exit function terminates the process with an exit status of status.\nway to set the exit status is to return an integer value from the main routine.)\nA parent process creates a new running child process by calling the fork\nReturns: 0 to child, PID of child to parent, −1 on error\nThe newly created child process is almost, but not quite, identical to the parent.\nand the newly created child is that they have different PIDs. The fork function is interesting (and often confusing) because it is called once\nbut it returns twice: once in the calling process (the parent), and once in the newly\ncreated child process.\nIn the parent, fork returns the PID of the child.\nSince the PID of the child is always nonzero, the return\nthe parent or the child.\nFigure 8.15 shows a simple example of a parent process that uses fork to create\na child process.\nparent and child.\nThe fork function is called once by the parent, but it\nreturns twice: once to the parent and once to the newly created child.\nThe parent and the child are separate processes that\non our system, the parent process completes its printf statement ﬁrst,\nthe interleaving of the instructions in different processes.\nUsing fork to create a new process.\nchild immediately after the fork function returned in each process, we\nwould see that the address space of each process is identical.\nEach process\nprogram, local variable x has a value of 1 in both the parent and the child\nwhen the fork function returns in line 6.\nthe child are separate processes, they each have their own private address\nprivate and are not reﬂected in the memory of the other process.\nwhy the variable x has different values in the parent and child when they\nWhen we run the example program, we notice that both parent and\nProcess graph for the\nProcess graph for a nested fork.\ncorresponds to the parent process calling main.\nThe sequence of vertices for each process ends with a vertex\nFor example, Figure 8.16 shows the process graph for the example program in\ncreates a child process that runs concurrently with the parent in its own private\nvertices in the corresponding process graph represents a feasible total ordering\nthe printf statements in the parent and child can occur in either order because\nThe process graph can be especially helpful in understanding programs with\nFor example, Figure 8.17 shows a program with two calls to fork\nThe corresponding process graph helps us see that this program\nruns four processes, each of which makes a call to printf and which can execute\nA. What is the output of the child process?\nB. What is the output of the parent process?\nReaping Child Processes\nWhen a process terminates for any reason, the kernel does not remove it from\nInstead, the process is kept around in a terminated state\nWhen the parent reaps the terminated child, the\nkernel passes the child’s exit status to the parent and then discards the terminated\nA terminated process that has not yet\nWhen a parent process terminates, the kernel arranges for the init process\nThe init process, which\nhas a PID of 1, is created by the kernel during system start-up, never terminates,\nand is the ancestor of every process.\nIf a parent process terminates without reaping\nits zombie children, then the kernel arranges for the init process to reap them.\nA process waits for its children to terminate or stop by calling the waitpid\nReturns: PID of child if OK, 0 (if WNOHANG), or −1 on error\nA zombie process is\nwaitpid suspends execution of the calling process until a child process in its wait\nIf a process in the wait set has already terminated at the time of the\nthe terminated child that caused waitpid to return.\n. If pid > 0, then the wait set is the singleton child process whose process ID is\n. If pid = -1, then the wait set consists of all of the parent’s child processes.\nchild processes in the wait set has terminated yet.\nsuspends the calling process until a child terminates; this option is useful\nSuspend execution of the calling process until a process in the\nterminated or stopped child that caused the return.\nSuspend execution of the calling process until a running\nprocess in the wait set is terminated or until a stopped process in the wait\nreturn value equal to the PID of one of the stopped or terminated children.\nabout the child that caused the return in status, which is the value pointed to\nReturns true if the child terminated normally, via a call\nReturns the exit status of a normally terminated\nReturns true if the child process terminated be-\nReturns the number of the signal that caused the child\nprocess to terminate.\nReturns true if the child process was restarted by\nIf the calling process has no children, then waitpid returns −1 and sets errno to\nIf the waitpid function was interrupted by a signal, then it returns −1\nReturns: PID of child if OK or −1 on error\neach of the N children, and in line 12, each child exits with a unique exit status.\nprintf(\"child %d terminated normally with exit status=%d\\n\",\nprintf(\"child %d terminated abnormally\\n\", pid);\nIn line 15, the parent waits for all of its children to terminate by using waitpid\nthe call to waitpid returns with the nonzero PID of that child.\nLine 24 checks that the waitpid function terminated\nchild 22966 terminated normally with exit status=100\nchild 22967 terminated normally with exit status=101\nwaits for each child in this same order by calling waitpid with the appropriate\nprintf(\"child %d terminated normally with exit status=%d\\n\",\nprintf(\"child %d terminated abnormally\\n\", retpid);\nPutting Processes to Sleep\nThe sleep function suspends a process for a speciﬁed period of time.\ncalling function to sleep until a signal is received by the process.\nThe execve function loads and runs a new program in the context of the current\nprocess.",
      "keywords": [
        "process",
        "child",
        "pid",
        "child process",
        "parent",
        "fork",
        "kernel",
        "parent process",
        "exit",
        "status",
        "function",
        "returns",
        "code",
        "fork fork fork",
        "program"
      ],
      "concepts": [
        "processes",
        "process",
        "returns",
        "returning",
        "returned",
        "kernel",
        "functions",
        "function",
        "child",
        "status"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 39,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.729,
          "base_score": 0.579,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "process",
          "child",
          "parent",
          "terminated",
          "pid"
        ],
        "semantic": [],
        "merged": [
          "process",
          "child",
          "parent",
          "terminated",
          "pid"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3864759117728293,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714253+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 777-794)",
      "start_page": 777,
      "end_page": 794,
      "summary": "int main(int argc, char **argv, char **envp);\nint main(int argc, char *argv[], char *envp[]);\nWrite a program called myecho that prints its command-line arguments and envi-\nPrograms versus processes\na process.\nA process is a speciﬁc instance of a program in execution; a program\nalways runs in the context of some process.\nThe fork function runs the same program in a new child\nThe execve function loads and runs a new program in the\nint builtin_command(char **argv);\nAfter parsing the command line, the eval function calls the builtin_command\nfunction, which checks whether the ﬁrst command-line argument is a built-in shell\nIf builtin_command returns 0, then the shell creates a child process and\nprogram to run in the background, then the shell returns to the top of the loop and\nint builtin_command(char **argv)\nSignals\nas a Linux signal, that allows processes and the kernel to interrupt other processes.\nAbort signal from abort function\nUser-deﬁned signal 1\nUser-deﬁned signal 2\nTimer signal from alarm function\nSoftware termination signal\nA child process has stopped or terminated\nStop signal not from terminal\nStop signal from terminal\nBackground process read from terminal\nBackground process wrote to terminal\nLinux signals.\n(b) This signal can be neither caught nor ignored.\n(Source: man 7 signal.\nA signal is a small message that notiﬁes a process that an event of some type\nFigure 8.26 shows the 30 different types of signals that\nEach signal type corresponds to some kind of system event.\nware exceptions are processed by the kernel’s exception handlers and would not\nthe occurrence of such exceptions to user processes.\nFor example, if a process at-\ntempts to divide by zero, then the kernel sends it a SIGFPE signal (number 8).\nIf a process executes an illegal instruction, the kernel sends it a SIGILL signal\nIf a process makes an illegal memory reference, the kernel sends it a\nSIGSEGV signal (number 11).\nevents in the kernel or in other user processes.\nin the foreground, then the kernel sends a SIGINT (number 2) to each process in\nthe foreground process group.\nby sending it a SIGKILL signal (number 9).\nWhen a child process terminates or\nstops, the kernel sends a SIGCHLD signal (number 17) to the parent.\nSignal Terminology\nThe transfer of a signal to a destination process occurs in two distinct steps:\nSending a signal.\nThe kernel sends (delivers) a signal to a destination process by\nThe signal\nevent such as a divide-by-zero error or the termination of a child process.\n(2) A process has invoked thekill function (discussed in the next section)\nto explicitly request the kernel to send a signal to the destination process.\nA process can send a signal to itself.\nReceiving a signal.\nA destination process receives a signal when it is forced by\nthe kernel to react in some way to the delivery of the signal.\nThe process\ncan either ignore the signal, terminate, or catch the signal by executing\na user-level function called a signal handler.\nidea of a handler catching a signal.\nA signal that has been sent but not yet received is called a pending signal.\nany point in time, there can be at most one pending signal of a particular type.\nIf a process has a pending signal of type k, then any subsequent signals of type\nA process can\nselectively block the receipt of certain signals.\nWhen a signal is blocked, it can be\nSignal handling.\nof a signal triggers a\ncontrol transfer to a signal\nprocessing, the handler\nto signal handler\n(3) Signal\n(4) Signal handler \n(1) Signal received\nby process\ndelivered, but the resulting pending signal will not be received until the process\nunblocks the signal.\nA pending signal is received at most once.\nFor each process, the kernel main-\ntains the set of pending signals in the pending bit vector, and the set of blocked\nsignals in the blocked bit vector.1 The kernel sets bit k in pending whenever a\nSending Signals\nUnix systems provide a number of mechanisms for sending signals to processes.\nProcess Groups\nThe getpgrp function returns the process group\nReturns: process group ID of calling process\nThe setpgid function changes the process group of process pid to pgid.\nspeciﬁed by pid is used for the process group ID.\nFor example, if process 15213 is\nthe calling process, then\n1. Also known as the signal mask.\nSending Signals with the /bin/kill Program\nThe /bin/kill program sends an arbitrary signal to another process.\nsends signal 9 (SIGKILL) to process 15213.\nA negative PID causes the signal to\nbe sent to every process in process group PID.\nsends a SIGKILL signal to every process in process group 15213.\nSending Signals from the Keyboard\nUnix shells use the abstraction of a job to represent the processes that are created\ncreates a foreground job consisting of two processes connected by a Unix pipe: one\na separate process group for each job.\nfrom one of the parent processes in the job.\nforeground job has a PID of 20 and a process group ID of 20.\nbackground process\nprocess group 32\nprocess group 20\nprocess group 40\nTyping Ctrl+C at the keyboard causes the kernel to send a SIGINT signal to\nSIGTSTP signal to every process in the foreground process group.\nSending Signals with the kill Function\nProcesses send signals to other processes (including themselves) by calling the\n#include <signal.h>\nIf pid is greater than zero, then the kill function sends signal number sig to\nprocess pid.\nIf pid is equal to zero, then kill sends signal sig to every process\npid is less than zero, then kill sends signal sig to every process in process group\nuses the kill function to send a SIGKILL signal to its child.\n/* Child sleeps until SIGKILL signal received, then dies */\n/* Wait for a signal to arrive */\n/* Parent sends a SIGKILL signal to a child */\nUsing the kill function to send a signal to a child.\nSending Signals with the alarm Function\nA process can send SIGALRM signals to itself by calling the alarm function.\nThe alarm function arranges for the kernel to send a SIGALRM signal to the\nReceiving Signals\nunblocked pending signals (pending & ~blocked) for p.\nsome signal k in the set (typically the smallest k) and forces p to receive signal\nk. The receipt of the signal triggers some action by the process.\nOnce the process\nEach signal type has a predeﬁned default action, which\n. The process terminates.\n. The process terminates and dumps core.\n. The process stops (suspends) until restarted by a SIGCONT signal.\n. The process ignores the signal.\nFigure 8.26 shows the default actions associated with each type of signal.\nthe receiving process.\na SIGCHLD is to ignore the signal.\nassociated with a signal by using the signal function.\n#include <signal.h>\nsighandler_t signal(int signum, sighandler_t handler);\nReturns: pointer to previous handler if OK, SIG_ERR on error (does not set errno)\nThe signal function can change the action associated with a signal signum in\n. If handler is SIG_IGN, then signals of type signum are ignored.\n. If handler is SIG_DFL, then the action for signals of type signum reverts to\n. Otherwise, handler is the address of a user-deﬁned function, called a signal\nhandler, that will be called whenever the process receives a signal of type\nthe signal function is known as installing the handler.\nhandler is called catching the signal.\nto as handling the signal.\nWhen a process catches a signal of type k, the handler installed for signal k is\nhandler function to catch different types of signals.\nto the instruction in the control ﬂow where the process was interrupted by the\nreceipt of the signal.\nFigure 8.30 shows a program that catches the SIGINT signal that is sent\nif (signal(SIGINT, sigint_handler) == SIG_ERR)\nunix_error(\"signal error\");\npause(); /* Wait for the receipt of a signal */\nA program that uses a signal handler to catch a SIGINT signal.\ncatches signal s\ncatches signal t\nis to immediately terminate the process.\nbehavior to catch the signal, print a message, and then terminate the process.\nSignal handlers can be interrupted by other handlers, as shown in Figure 8.31.\nIn this example, the main program catches signal s, which interrupts the main\ncatches signal t ̸= s, which interrupts S and transfers control to handler T .\nWrite a program called snooze that takes a single command-line argument, calls\nWrite your program so that the user can interrupt the snooze function by typing\nBlocking and Unblocking Signals\nLinux provides implicit and explicit mechanisms for blocking signals:\nnals of the type currently being processed by a handler.\nFigure 8.31, suppose the program has caught signal s and is currently run-\nIf another signal s is sent to the process, then s will become\nselected signals using the sigprocmask function and its helpers.\n#include <signal.h>\nThe sigprocmask function changes the set of currently blocked signals (the\nAdd the signals in set to blocked (blocked = blocked | set).\nRemove the signals in set from blocked (blocked =\nSignal sets such as set are manipulated using the following functions: The\nsignal to set.\nrarily block the receipt of SIGINT signals.\nTemporarily blocking a signal from being received.\nWriting Signal Handlers\nSignal handling is one of the thornier aspects of Linux system-level programming.\nwriting safe, correct, and portable signal handlers.\nSafe Signal Handling\nSignal handlers are tricky because they can run concurrently with the main pro-\nmight simply set a global ﬂag and return immediately; all processing\nassociated with the receipt of the signal is performed by the main program,\nCall only async-signal-safe functions in your handlers.\ncalled from a signal handler, either because it is reentrant (e.g., ac-\nbe interrupted by a signal handler.\nThe only safe way to generate output from a signal handler is to use\nyou can use to print simple messages from signal handlers.\nsignal\nAsync-signal-safe functions.\n(Source: man 7 signal.\nThe _exit function in line 17 is an async-signal-\nMany of the Linux async-signal-safe functions set\nThe Sio (Safe I/O) package for signal handlers.",
      "keywords": [
        "signal",
        "process",
        "process group",
        "signal handler",
        "signals",
        "handler",
        "program",
        "function",
        "PID",
        "argv",
        "SIGKILL signal",
        "command",
        "Returns",
        "int",
        "terminate"
      ],
      "concepts": [
        "signals",
        "signal",
        "function",
        "functions",
        "terminates",
        "terminate",
        "terminal",
        "termination",
        "terminated",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.73,
          "base_score": 0.58,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.654,
          "base_score": 0.504,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 40,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "signal",
          "process",
          "handler",
          "signals",
          "process group"
        ],
        "semantic": [],
        "merged": [
          "signal",
          "process",
          "handler",
          "signals",
          "process group"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39479223589209694,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714309+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 795-814)",
      "start_page": 795,
      "end_page": 814,
      "summary": "void sigint_handler(int sig) /* Safe SIGINT handler */\nand restore it before the handler returns.\nif the handler returns.\nProtect accesses to shared global data structures by blocking all signals.\na handler shares a global data structure with the main program or with\nother handlers, then your handlers and main program should temporarily\nblock all signals while accessing (reading or writing) that data structure.\nrarily blocking signals while you access d guarantees that a handler will\nthe main function would never see the updated values from the handler.\nby temporarily blocking signals.\nhandler records the receipt of the signal by writing to a global ﬂag.\nmain program periodically reads the ﬂag, responds to the signal, and\nsig_atomic_t variables without temporarily blocking signals.\nand follow the guidelines by keeping your handlers as simple as possible, calling\nCorrect Signal Handling\nthere can be at most one pending signal of any particular type.\nThus, if two signals\nof type k are sent to a destination process while signal k is blocked because the\ndestination process is currently executing a handler for signal k, then the second\nstructure is that a parent process creates some children that run independently for\nthe kernel sends a SIGCHLD signal to the parent whenever one of its children\nThe parent installs a SIGCHLD handler\nsending it a SIGCHLD signal.\nThe parent catches the SIGCHLD, reaps one child,\ncode/ecf/signal1.c\nvoid handler1(int sig)\nSio_puts(\"Handler reaped child\\n\");\nif (signal(SIGCHLD, handler1) == SIG_ERR)\nunix_error(\"signal error\");\n/* Parent waits for terminal input and then processes it */\nprintf(\"Parent processing input\\n\");\ncode/ecf/signal1.c\nFigure 8.36 signal1.\nThis program is ﬂawed because it assumes that signals are\nThe signal1 program in Figure 8.36 seems fairly straightforward.\nlinux> ./signal1\nHandler reaped child\nHandler reaped child\nFrom the output, we note that although three SIGCHLD signals were sent to the\nparent, only two of these signals were received, and thus the parent only reaped\nIf we suspend the parent process, we see that, indeed, child process\n0:02 ./signal1\n0:00 [signal1] <defunct>\nthat signals are not queued.\nWhile the handler is still processing the ﬁrst signal, the\nSIGCHLD signals are blocked by the SIGCHLD handler, the second signal is not\nShortly thereafter, while the handler is still processing the ﬁrst signal,\nthe third signal arrives.\nSIGCHLD signal is discarded.\nSometime later, after the handler has returned,\nthe kernel notices that there is a pending SIGCHLD signal and forces the parent\nto receive the signal.\nThe parent catches the signal and executes the handler a\nAfter the handler ﬁnishes processing the second signal, there are no\nmore pending SIGCHLD signals, and there never will be, because all knowledge\nTo ﬁx the problem, we must recall that the existence of a pending signal only\nimplies that at least one signal has been delivered since the last time the process\nreceived a signal of that type.\nSo we must modify the SIGCHLD handler to reap\ncode/ecf/signal2.c\nvoid handler2(int sig)\nSio_puts(\"Handler reaped child\\n\");\ncode/ecf/signal2.c\nsignal2.\nmodiﬁed SIGCHLD handler.\nWhen we run signal2 on our Linux system, it now correctly reaps all of the\nlinux> ./signal2\nHandler reaped child\nHandler reaped child\nHandler reaped child\nvoid handler1(int sig)\nsignal(SIGUSR1, handler1);\nPortable Signal Handling\nsignal-handling semantics.\n. The semantics of the signal function varies.Some older Unix systems restore\nthe action for signal k to its default after signal k has been caught by a handler.\nOn these systems, the handler must explicitly reinstall itself, by calling signal,\nthat can potentially block the process for a long period of time are called\ninterrupted when a handler catches a signal do not resume when the signal\nhandler returns but instead return immediately to the user with an error\nhandler_t *Signal(int signum, handler_t *handler)\nunix_error(\"Signal error\");\nreturn (old_action.sa_handler);\nFigure 8.38 Signal.\n#include <signal.h>\nStevens [110], is to deﬁne a wrapper function, called Signal, that calls sigaction\nFigure 8.38 shows the deﬁnition of Signal, which is invoked in the same\nway as the signal function.\nThe Signal wrapper installs a signal handler with the following signal-\n. Only signals of the type currently being processed by the handler are blocked.\n. Once the signal handler is installed, it remains installed until Signal is called\nWe will use the Signal wrapper in all of our code.\nAfter the parent creates a new child process, it adds the child to the job\nWhen the parent reaps a terminated (zombie) child in the SIGCHLD signal\nhandler, it deletes the child from the job list.\nzombie, causing the kernel to deliver a SIGCHLD signal to the parent.\nthe signal handler in the parent.\n4. The signal handler reaps the terminated child and calls deletejob, which does\n5. After the handler completes, the kernel then runs the parent, which returns\nThus, for some interleavings of the parent’s main routine and signal-handling\nwhen the fork call returns instead of the child, then the parent will correctly add\nthe child to the job list before the child terminates and the signal handler removes\nvoid handler(int sig)\nSignal(SIGCHLD, handler);\nif ((pid = Fork()) == 0) { /* Child process */\nSigprocmask(SIG_BLOCK, &mask_all, &prev_all); /* Parent process */\nSIGCHLD signals before the call to fork and then unblocking them only after we\nbe careful to unblock the SIGCHLD signal in the child before calling execve.\nExplicitly Waiting for Signals\nSometimes a main program needs to explicitly wait for a certain signal handler to\nthe job to terminate and be reaped by the SIGCHLD handler before accepting\nThe parent installs handlers for SIGINT and\nchild, it resets pid to zero, unblocks SIGCHLD, and then waits in a spin loop for\nAfter the child terminates, the handler reaps it and assigns\nIf the signal is received after the while\nvoid handler(int sig)\nSignal(SIGCHLD, handler);\nif ((pid = Fork()) == 0) { /* Child process */\nSigprocmask(SIG_BLOCK, &mask_all, NULL); /* Parent process */\nvoid sigchld_handler(int s)\nvoid sigint_handler(int s)\nSignal(SIGCHLD, sigchld_handler);\nSignal(SIGINT, sigint_handler);\nWaiting for a signal with a spin loop.\n#include <signal.h>\nand then suspends the process until the receipt of a signal whose action is either\nto run a handler or to terminate the process.\nprocess terminates without returning from sigsuspend.\nhandler, then sigsuspend returns after the handler returns, restoring the blocked\nrace where a signal is received after the call to sigprocmask and before the call\nsigsuspend temporarily unblocks SIGCHLD, and then sleeps until the parent\ncatches a signal.\nC provides a form of user-level exceptional control ﬂow, called a nonlocal jump,\nvoid sigchld_handler(int s)\nvoid sigint_handler(int s)\nSignal(SIGCHLD, sigchld_handler);\nSignal(SIGINT, sigint_handler);\nWaiting for a signal with sigsuspend.\nThe setjmp function saves the current calling environment in the env buffer, for\nThe longjmp function restores the calling environment from the env buffer and\nThe setjmp function is called once but returns multiple times: once when the\nfunction is called once but never returns.\nuse a nonlocal jump to return directly to a common localized error handler instead\ncalls setjmp to save the current calling environment, and then calls function foo,\nvoid handler(int sig)\nSignal(SIGINT, handler);\nAnother important application of nonlocal jumps is to branch out of a signal\nhandler to a speciﬁc code location, rather than returning to the instruction that was\ninterrupted by the arrival of the signal.\nThe program uses signals and nonlocal jumps to\nby signal handlers.\nthe user types Ctrl+C, the kernel sends a SIGINT signal to the process, which\nInstead of returning from the signal handler, which would pass control\nback to the interrupted processing loop, the handler performs a nonlocal jump\nC setjmp and longjmp functions.\nrisk of the handler running before the initial call to sigsetjmp sets up the calling\nand siglongjmp functions are not on the list of async-signal-safe functions in\nhandler, which does some processing and then returns control to the interrupted\nstruction, while abort handlers never return control to the interrupted ﬂow.\ncan create child processes, wait for their child processes to stop or terminate, run\nnew programs, and catch signals from other processes.\nThe semantics of signal\nsignal-handling semantics.\n106, 113] contain additional information on exceptions, processes, and signals.\ndescription of how to work with processes and signals from application programs.\nincluding details of the process and signal implementations.\nA. Called once, returns only if there is an error\nC. Called once, returns either a pointer or NULL",
      "keywords": [
        "SIGCHLD",
        "signal",
        "handler",
        "SIGCHLD signal",
        "signal handler",
        "SIGCHLD handler",
        "SIGCHLD signal handler",
        "sig",
        "child",
        "parent",
        "signals",
        "Handler reaped child",
        "int",
        "code",
        "mask"
      ],
      "concepts": [
        "signals",
        "signal",
        "handler",
        "handlers",
        "process",
        "processes",
        "processing",
        "processed",
        "code",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.677,
          "base_score": 0.527,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 39,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.568,
          "base_score": 0.418,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "handler",
          "signal",
          "sigchld",
          "parent",
          "child"
        ],
        "semantic": [],
        "merged": [
          "handler",
          "signal",
          "sigchld",
          "parent",
          "child"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3368199262609624,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714420+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 815-837)",
      "start_page": 815,
      "end_page": 837,
      "summary": "fork(); printf(\"Example\\n\");\nHow many lines of output does the following function print if the value of n\nUsing the example in Figure 8.23 as a starting point, write a shell program that\n. If the command line ends with an ampersand, then the shell runs the job in\nIn our example program in Figure 8.15, the parent and child execute disjoint sets\nthe fork returns, it executes the printf in line 6.\nprintf(\"Command-line arguments:\\n\");\nVirtual Memory\nPhysical and Virtual Addressing\nMemory Mapping\nDynamic Memory Allocation\nCommon Memory-Related Bugs in C Programs\nrocesses in a system share the CPU and main memory with other processes.\nif too many processes need too much memory, then some of them will simply\nMemory is\nIf some process inadvertently writes to the memory\nsystems provide an abstraction of main memory known as virtual memory (VM).\nVirtual memory is an elegant interaction of hardware exceptions, hardware ad-\ndress translation, main memory, disk ﬁles, and kernel software that provides each\nprocess with a large, uniform, and private address space.\nanism, virtual memory provides three important capabilities: (1) It uses main\nmemory efﬁciently by treating it as a cache for an address space stored on disk,\nkeeping only the active areas in main memory and transferring data back and\nforth between disk and memory as needed.\n(2) It simpliﬁes memory management\nby providing each process with a uniform address space.\nVirtual memory is one of the great ideas in computer systems.\nSince virtual memory works so well behind the\n. Virtual memory is central.\nVirtual memory pervades all levels of computer\n. Virtual memory is powerful.\nVirtual memory gives applications powerful ca-\nportions of disk ﬁles, and share memory with other processes.\nand writing memory locations?\nUnderstanding virtual memory\n. Virtual memory is dangerous.\nApplications interact with virtual memory ev-\nIf virtual memory is used improp-\nstanding virtual memory, and the allocation packages such as malloc that\nThis chapter looks at virtual memory from two angles.\nchapter describes how virtual memory works.\nvirtual memory is used and managed by applications.\nmemory mechanism of a small system by hand, and the virtual memory idea will\nmanage virtual memory in your programs.\nPhysical and Virtual Addressing\nThe main memory of a computer system is organized as an array of M contiguous\naccess memory would be to use physical addresses.\nFigure 9.1 shows an example of physical addressing in the context of\na load instruction that reads the 4-byte word starting at physical address 4.\nthe CPU executes the load instruction, it generates an effective physical address\nThe main memory fetches the\n4-byte word starting at physical address 4 and returns it to the CPU, which stores\nHowever, modern processors use a form of addressing known as virtual address-\nphysical addressing.\nMain memory\nMain memory\nVirtual\nWith virtual addressing, the CPU accesses main memory by generating a vir-\nbeing sent to main memory.\nThe task of converting a virtual address to a physical\nDedicated hardware on the CPU chip called the memory management unit\n(MMU) translates virtual addresses on the ﬂy, using a lookup table stored in main\nmemory whose contents are managed by the operating system.\nIn a system with virtual memory, the CPU generates virtual addresses from\nan address space of N = 2n addresses called the virtual address space:\nFor example, a virtual address space\nwith N = 2n addresses is called an n-bit address space.\nsupport either 32-bit or 64-bit virtual address spaces.\nA system also has a physical address space that corresponds to the M bytes of\nphysical memory in the system:\nThis is the basic idea of virtual memory.\nEach byte of main memory has a virtual\naddress chosen from the virtual address space, and a physical address chosen from\nthe physical address space.\nvirtual address bits (n)\nvirtual addresses (N)\nLargest possible virtual address\nConceptually, a virtual memory is organized as an array of N contiguous byte-size\nEach byte has a unique virtual address that serves as an index\nThe contents of the array on disk are cached in main memory.\nwith any other cache in the memory hierarchy, the data on disk (the lower level)\nvirtual memory into ﬁxed-size blocks called virtual pages (VPs).\nEach virtual page\nSimilarly, physical memory is partitioned into physical pages\nAt any point in time, the set of virtual pages is partitioned into three disjoint\nAllocated pages that are currently cached in physical memory.\nAllocated pages that are not cached in physical memory.\nThe example in Figure 9.3 shows a small virtual memory with eight virtual\npages.\nVirtual pages 0 and 3 have not been allocated yet, and thus do not yet exist\nmain memory as a cache.\nVirtual memory\nPhysical memory\nVirtual pages (VPs)\nPhysical pages (PPs)\nVirtual pages 1, 4, and 6 are cached in physical memory.\nPages 2, 5, and 7\nare allocated but are not currently cached in physical memory.\nTo help us keep the different caches in the memory hierarchy straight, we will use\nCPU and main memory, and the term DRAM cache to denote the VM system’s\ncache that caches virtual pages in main memory.\nThe position of the DRAM cache in the memory hierarchy has a big impact\nusually served from DRAM-based main memory.\nvirtual pages tend to be large—typically 4 KB to 2 MB.\npenalty, DRAM caches are fully associative; that is, any virtual page can be placed\nin any physical page.\nimportance, because the penalty associated with replacing the wrong virtual page\nPage Tables\nAs with any cache, the VM system must have some way to determine if a virtual\npage is cached somewhere in DRAM.\nphysical page it is cached in.\nPage table.\nPhysical page\nMemory-resident\npage table\nVirtual memory\nPhysical memory\nwhere the virtual page is stored on disk, select a victim page in physical memory,\nand copy the virtual page from disk to DRAM, replacing the victim page.\nware, address translation hardware in the MMU (memory management unit), and\na data structure stored in physical memory known as a page table that maps vir-\nThe address translation hardware reads the page table\neach time it converts a virtual address to a physical address.\nFigure 9.4 shows the basic organization of a page table.\nEach page in the virtual address space has a PTE at\nthe virtual page is currently cached in DRAM.\nﬁeld indicates the start of the corresponding physical page in DRAM where the\nvirtual page is cached.\nthe virtual page has not yet been allocated.\nstart of the virtual page on disk.\nThe example in Figure 9.4 shows a page table for a system with eight virtual\npages and four physical pages.\nFour virtual pages (VP 1, VP 2, VP 4, and VP 7)\ncache is fully associative, any physical page can contain any virtual page.\nfollowing combinations of virtual address size (n) and page size (P):\nConsider what happens when the CPU reads a word of virtual memory contained\nin detail in Section 9.6, the address translation hardware uses the virtual address\naddress translation hardware knows that VP 2 is cached in memory.\nphysical memory address in the PTE (which points to the start of the cached page\nPage Faults\nIn virtual memory parlance, a DRAM cache miss is known as a page fault.\nure 9.6 shows the state of our example page table before the fault.\ntion hardware reads PTE 3 from memory, infers from the valid bit that VP 3 is\nnot cached, and triggers a page fault exception.\nthe fact that VP 4 is no longer cached in main memory.\nPhysical page\nMemory-resident\npage table\nVirtual memory\nPhysical memory\nVirtual address\na page fault.\nPhysical page\nMemory-resident\npage table\nVirtual memory\nPhysical memory\nVirtual address\nThe page fault handler\nmemory normally, without\nPhysical page\nMemory-resident\npage table\nVirtual memory\nPhysical memory\nVirtual address\nNext, the kernel copies VP 3 from disk to PP 3 in memory, updates PTE 3,\nwhich resends the faulting virtual address to the address translation hardware.\nBut now, VP 3 is cached in main memory, and the page hit is handled normally by\nFigure 9.7 shows the state of our example page\ntable after the page fault.\nVirtual memory was invented in the early 1960s, long before the widening\nCPU-memory gap spawned SRAM caches.\nAs a result, virtual memory systems\nIn virtual memory parlance, blocks are known as pages.\nof transferring a page between disk and memory is known as swapping or paging.\npage.\nPhysical page\nMemory-resident\npage table\nVirtual memory\nPhysical memory\nAllocating Pages\nFigure 9.8 shows the effect on our example page table when the operating system\nallocates a new page of virtual memory—for example, as a result of calling malloc.\nWhen many of us learn about the idea of virtual memory, our ﬁrst impression is\nthat paging will destroy program performance.\nIn practice, virtual memory works\nAlthough the total number of distinct pages that programs reference during an\nentire run might exceed the total size of physical memory, the principle of locality\nthe working set is paged into memory, subsequent references to the working set\nAs long as our programs have good temporal locality, virtual memory systems\nthe working set size exceeds the size of physical memory, then the program can\nAlthough virtual memory is usually efﬁcient, if a program’s\nVirtual address spaces\nPhysical memory\nIn the last section, we saw how virtual memory provides a mechanism for using the\nDRAM to cache pages from a typically larger virtual address space.\nsome early systems such as the DEC PDP-11/70 supported a virtual address space\nthat was smaller than the available physical memory.\nYet virtual memory was\nThus far, we have assumed a single page table that maps a single virtual\na separate page table, and thus a separate virtual address space, for each process.\nIn the example, the page table for process i maps\nSimilarly, the page table for process j maps VP 1\nNotice that multiple virtual pages can be mapped to\nthe same shared physical page.\nThe combination of demand paging and separate virtual address spaces has\nallocating memory to applications.\nA separate address space allows each process to use the\nactually reside in physical memory.\nery process on a given Linux system has a similar memory format.\naddress spaces, the code segment always starts at virtual address 0x400000.\nThe stack occupies the highest portion of the user process address space and\nindependent of the ultimate location of the code and data in physical memory.\nVirtual memory also makes it easy to load executable\nan object ﬁle into a newly created process, the Linux loader allocates virtual\npages for the code and data segments, marks them as invalid (i.e., not cached),\nfrom disk into memory.\nby the virtual memory system the ﬁrst time each page is referenced, either by\nreferences a memory location.\nThis notion of mapping a set of contiguous virtual pages to an arbitrary\nmemory mapping.\nvirtual pages to disjoint physical pages.\nappropriate virtual pages in different processes to the same physical pages,\n. Simplifying memory allocation.Virtual memory provides a simple mechanism\nfor allocating additional memory to user processes.\ncontiguous virtual memory pages, and maps them to k arbitrary physical pages\nlocated anywhere in physical memory.\nphysical memory.\nThe pages can be scattered randomly in physical memory.",
      "keywords": [
        "Virtual Memory",
        "Memory",
        "Virtual",
        "virtual address",
        "physical memory",
        "Address",
        "virtual pages",
        "virtual address space",
        "main memory",
        "printf printf printf",
        "Page table",
        "address space",
        "printf printf",
        "printf",
        "DRAM"
      ],
      "concepts": [
        "memory",
        "memories",
        "pages",
        "paging",
        "paged",
        "virtual",
        "addressing",
        "address",
        "addresses",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.878,
          "base_score": 0.728,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 39,
          "title": "",
          "score": 0.802,
          "base_score": 0.652,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.741,
          "base_score": 0.591,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "virtual",
          "physical",
          "memory",
          "page",
          "virtual memory"
        ],
        "semantic": [],
        "merged": [
          "virtual",
          "physical",
          "memory",
          "page",
          "virtual memory"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.46071763988792114,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714492+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 838-857)",
      "start_page": 838,
      "end_page": 857,
      "summary": "page-level memory\nPage tables with permission bits\nmodify any virtual pages that are shared with other processes, unless all parties\nSince the address translation hardware reads a PTE each time the CPU generates\nan address, it is straightforward to control access to the contents of a virtual page\nthe page.\nProcesses running in kernel mode can access any page, but processes\nREAD and WRITE bits control read and write access to the page.\nNumber of addresses in virtual address space\nVirtual page offset (bytes)\nVirtual page number\nPhysical page offset (bytes)\nPhysical page number\nelement virtual address space (VAS) and an M-element physical address space\nA′ in PAS\nFigure 9.12 shows how the MMU uses the page table to perform this mapping.\nA control register in the CPU, the page table base register (PTBR) points to the\ncurrent page table.\nThe n-bit virtual address has two components: a p-bit virtual\npage offset (VPO) and an (n −p)-bit virtual page number (VPN).\nof the physical page number (PPN) from the page table entry and the VPO from\nthe virtual address.\nNotice that since the physical and virtual pages are both P\nbytes, the physical page offset (PPO) is identical to the VPO.\nPage table\nPhysical address\nVirtual address\nVirtual page number (VPN)\nVirtual page offset (VPO)\nPage\nPhysical page number (PPN)\nthe page table\nthen page\n(page fault)\nPhysical page number (PPN)\nPhysical page offset (PPO)\nAddress translation with a page table.\nis a page hit.\nThe processor generates a virtual address and sends it to the MMU.\nThe MMU generates the PTE address and requests it from the cache/\nThe MMU constructs the physical address and sends it to the cache/main\nThe fault handler identiﬁes a victim page in physical memory, and if that\nThe fault handler pages in the new page and updates the PTE in memory.\n(a) Page hit\nNew page\nPage fault exception handler\n(b) Page fault\nVA: virtual address.\npage table entry address.\nPTE: page table entry.\nBecause the virtual page is now cached in physical memory,\nGiven a 64-bit virtual address space and a 32-bit physical address, determine the\nnumber of bits in the VPN, VPO, PPN, and PPO for the following page sizes P:\nIntegrating VM with a physically addressed cache.\nVA: virtual address.\nPTEA: page table entry address.\nPTE: page table entry.\nIn any system that uses both virtual memory and SRAM caches, there is the\nissue of whether to use virtual or physical addresses to access the SRAM cache.\nblocks from the same virtual pages.\nFigure 9.14 shows how a physically addressed cache might be integrated with\nNotice that page table entries can be cached, just like any other\nAs we have seen, every time the CPU generates a virtual address, the MMU must\nrefer to a PTE in order to translate the virtual address into a physical address.\nA TLB is a small, virtually addressed cache where each line holds a block\nmatching are extracted from the virtual page number in the virtual address.\nThe CPU generates a virtual address.\nThe MMU translates the virtual address to a physical address and sends\nMulti-Level Page Tables\nThus far, we have assumed that the system uses a single page table to do address\nBut if we had a 32-bit address space, 4 KB pages, and a 4-byte PTE,\nthen we would need a 4 MB page table resident in memory at all times, even if\nof page tables instead.\nConsider a 32-bit virtual address space partitioned into 4 KB pages, with page\naddress space has the following form: The ﬁrst 2 K pages of memory are allocated\nhow we might construct a two-level page table hierarchy for this virtual address\nvirtual address space, where each chunk consists of 1,024 contiguous pages.\nIf every page in chunk i is unallocated, then level 1 PTE i is null.\nHowever, if at least one page in chunk\ni is allocated, then level 1 PTE i points to the base of a level 2 page table.\nlevel 1 PTEs point to level 2 page tables.\nEach PTE in a level 2 page table is responsible for mapping a 4-KB page of\nvirtual memory, just as before when we looked at single-level page tables.\nthat with 4-byte PTEs, each level 1 and level 2 page table is 4 kilobytes, which\nlevel 1 table is null, then the corresponding level 2 page table does not even have\nvirtual address space for a typical program is unallocated.\nThe level 2 page tables can be\nOnly the most heavily used level 2 page tables need to\npages\n1,023 unallocated pages\n1 allocated VM page\npage tables\npage table\nA two-level page table hierarchy.\nVirtual address\nPhysical address\npage table\npage table\npage table\nAddress translation with a k-level page table.\nFigure 9.18 summarizes address translation with a k-level page table hierarchy.\nis an index into a page table at level i.\npoints to the base of some page table at level j + 1.\ncontains either the PPN of some physical page or the address of a disk block.\nTo construct the physical address, the MMU must access k PTEs before it can\never, the TLB comes to the rescue here by caching PTEs from the page tables at\nIn practice, address translation with multi-level page tables is\nnot signiﬁcantly slower than with single-level page tables.\naddress translation on a small system with a TLB and L1 d-cache.\n. The memory is byte addressable.\n. Virtual addresses are 14 bits wide (n = 14).\n. Physical addresses are 12 bits wide (m = 12).\n. The L1 d-cache is physically addressed and direct mapped, with a 4-byte line\nFigure 9.19 shows the formats of the virtual and physical addresses.\npage is 26 = 64 bytes, the low-order 6 bits of the virtual and physical addresses serve\nThe high-order 8 bits of the virtual address\nThe high-order 6 bits of the physical address serve as the PPN.\n(Figure 9.20(a)), a portion of the page table (Figure 9.20(b)), and the L1 cache\nhow the bits of the virtual and physical addresses are partitioned by the hardware\n(Virtual page number)\n(Virtual page offset)\n(Physical page number)\n(Physical page offset)\nAssume 14-bit virtual addresses\n(n = 14), 12-bit physical addresses (m = 12), and 64-byte pages (P = 64).\n(b) Page table: Only the first 16 PTEs are shown\nTLB, page table, and cache for small memory system.\nTLB, page table, and cache are in hexadecimal notation.\nThe TLB is virtually addressed using the bits of the VPN.\nPage table.\nThe page table is a single-level design with a total of 28 = 256 page\nThe direct-mapped cache is addressed by the ﬁelds in the physical\nsimulation, we ﬁnd it helpful to write down the bits in the virtual address, identify\nTo begin, the MMU extracts the VPN (0x0F) from the virtual address and\nthe PPN (0x0D) from the PTE with the VPO (0x14) from the virtual address, which\nNext, the MMU sends the physical address to the cache, which extracts the\nfrom the physical address.\nthe TLB misses, then the MMU must fetch the PPN from a PTE in the page table.\nIf the resulting PTE is invalid, then there is a page fault and the kernel must page\nShow how the example memory system in Section 9.6.4 translates a virtual address\ninto a physical address and accesses the cache.\nFor the given virtual address,\nindicate the TLB entry accessed, physical address, and cache byte value returned.\nIndicate whether the TLB misses, whether a page fault occurs, and whether a cache\nVirtual address: 0x03d7\nA. Virtual address format\nPage fault?\nmicroarchitecture allows for full 64-bit virtual and physical address spaces, the\n48-bit (256 TB) virtual address space and a 52-bit (4 PB) physical address space,\nVirtual address (VA)\nPage tables\nThe TLBs are virtually addressed, and 4-way set associative.\nThe L1, L2, and L3 caches are physically addressed, with a block size of 64 bytes.\nThe page\npages.\nFigure 9.22 summarizes the entire Core i7 address translation process, from the\ntime the CPU generates a virtual address until a data word arrives from memory.\nThe Core i7 uses a four-level page table hierarchy.\npage table hierarchy.\nWhen a Linux process is running, the page tables associated\nwith allocated pages are all memory-resident, although the Core i7 architecture\nallows these page tables to be swapped in and out.\ncontains the physical address of the beginning of the level 1 (L1) page table.\nPage table physical base addr Unused\nChild page table present in physical memory (1) or not (0).\nWrite-through or write-back cache policy for the child page table.\nCaching disabled or enabled for the child page table.\n40 most signiﬁcant bits of physical base address of child page table.\nFormat of level 1, level 2, and level 3 page table entries.\nreferences a 4 KB child page table.\npage table.\ncontains a 40-bit physical page number (PPN) that points to the beginning of the\nappropriate page table.\non page tables.\nFigure 9.24 shows the format of an entry in a level 4 page table.\nthe address ﬁeld contains a 40-bit PPN that points to the base of some page in\npages.\nThe PTE has three permission bits that control access to the page.\nbit, which determines whether the page can be accessed in user mode, protects\ninstruction fetches from individual memory pages.\nAs the MMU translates each virtual address, it also updates two other bits that\ncan be used by the kernel’s page fault handler.\nis known as a reference bit, each time a page is accessed.\nPage physical base addr\nChild page present in physical memory (1) or not (0).\nWrite-through or write-back cache policy for the child page.\n40 most signiﬁcant bits of physical base address of child page.\nFormat of level 4 page table entries.\npage.\nFigure 9.25 shows how the Core i7 MMU uses the four levels of page tables\nto translate a virtual address to a physical address.\ninto four 9-bit chunks, each of which is used as an offset into a page table.\nCR3 register contains the physical address of the L1 page table.\nan offset to an L1 PTE, which contains the base address of the L2 page table.\norganizes virtual memory and how it handles page faults.\nLinux maintains a separate virtual address space for each process of the form\nThe kernel virtual memory contains the code and data structures in the kernel.\nSome regions of the kernel virtual memory are mapped to physical pages that\nPage\nof page\nPhysical address\nVirtual address\nvirtual page\nCore i7 page table translation.\nPT: page table; PTE: page table entry; VPN: virtual page number;\nVPO: virtual page offset; PPN: physical page number; PPO: physical page offset.\nlevels of page tables are also shown.\n(e.g., page tables,\nMMU (1) translates the virtual address to a physical address and then (2) passes the physical address\nFor example, a virtual address on\na Core i7 with 4 KB pages has 12 bits of VPO, and these bits are identical to the 12 bits of PPO in the\nSince the 8-way set associative physically addressed L1 caches have\n64 sets and 64-byte cache blocks, each physical address has 6 (log2 64) cache offset bits and 6 (log2 64)\nThese 12 bits ﬁt exactly in the 12-bit VPO of a virtual address, which is no accident!\nthe CPU needs a virtual address translated, it sends the VPN to the MMU and the VPO to the L1\nWhile the MMU is requesting a page table entry from the TLB, the L1 cache is busy using the\nvirtual pages (equal in size to the total amount of DRAM in the system) to the\ncorresponding set of contiguous physical pages.\nwhen it needs to access page tables or to perform memory-mapped I/O operations\nOther regions of kernel virtual memory contain data that differ for each\nExamples include page tables, the stack that the kernel uses when it is\nLinux Virtual Memory Areas\nAn area is a contiguous chunk of existing (allocated) virtual memory whose pages\npage is contained in some area, and any virtual page that is not part of some area\nimportant because it allows the virtual address space to have gaps.\nnot keep track of virtual pages that do not exist, and such pages do not consume\nProcess virtual memory\nare pgd, which points to the base of the level 1 table (the page global directory),\ncharacterizes an area of the current virtual address space.\nDescribes (among other things) whether the pages in the area are\nProcess virtual memory\nwriting to a read-only page)\nSuppose the MMU triggers a page fault while trying to translate some virtual\nThe exception results in a transfer of control to the kernel’s page fault\n1. Is virtual address A legal?\npermission to read, write, or execute the pages in this area?\nwas the page fault the result of a store instruction trying to write to a read-\nonly page in the code segment?\nIs the page fault the result of a process\n3. At this point, the kernel knows that the page fault resulted from a legal\noperation on a legal virtual address.",
      "keywords": [
        "page table",
        "virtual address",
        "Address",
        "PTE",
        "physical address",
        "virtual",
        "TLB",
        "virtual page",
        "virtual address space",
        "virtual memory",
        "Physical",
        "memory",
        "VPN",
        "MMU",
        "Physical page"
      ],
      "concepts": [
        "paged",
        "memories",
        "bits",
        "bit",
        "address",
        "addresses",
        "addressing",
        "addressable",
        "cache",
        "cached"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.719,
          "base_score": 0.569,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.663,
          "base_score": 0.513,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.571,
          "base_score": 0.421,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "page",
          "page table",
          "virtual",
          "physical",
          "virtual address"
        ],
        "semantic": [],
        "merged": [
          "page",
          "page table",
          "virtual",
          "physical",
          "virtual address"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.39429693776742775,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714551+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 858-879)",
      "start_page": 858,
      "end_page": 879,
      "summary": "object on disk, a process known as memory mapping.\nallocated by the currently running processes.\nAn object can be mapped into an area of virtual memory as either a shared\nIf a process maps a shared object into an area of its virtual\nany other processes that have also mapped the shared object into their virtual\nA virtual memory area into which a\nSuppose that process 1 maps a shared object into an area of its virtual memory,\nPrivate objects are mapped into virtual memory using a clever technique\nprivate object into different areas of their virtual memories but share the same\nFor each process that maps the private object, the page\nthe process trying to write to a page in a private copy-on-write area, it creates a\nthe virtual memory for the new process, it creates exact copies of the current\nLinux processes can use the mmap function to create new areas of virtual memory\nThe mmap function asks the kernel to create a new virtual memory area, preferably\nnewly mapped virtual memory area (i.e., the vm_prot bits in the corresponding\nDynamic Memory Allocation\nconvenient and more portable to use a dynamic memory allocator when they need\nA dynamic memory allocator maintains an area of a process’s virtual memory\nAn allocator maintains the heap as a collection of various-size blocks.\nblock is a contiguous chunk of virtual memory that is either allocated or free.\nallocated block has been explicitly reserved for use by the application.\nA free block\nA free block remains free until it is explicitly allocated\nAn allocated block remains allocated until it is freed, either\nexplicitly by the application or implicitly by the memory allocator itself.\nexplicitly allocate blocks.\nallocated blocks.\n. Explicit allocators require the application to explicitly free any allocated\nblocks.\nC programs allocate a block by calling the malloc\nfunction, and free a block by calling the free function.\nan allocated block is no longer being used by the program and then free\nthe block.\nprocess of automatically freeing unused allocated blocks is known as garbage\non garbage collection to free allocated blocks.\nness, our discussion focuses on allocators that manage heap memory.\nyou should be aware that memory allocation is a general idea that arises in a vari-\nwill often use the standard allocator to acquire a large block of virtual memory\nand then use an application-speciﬁc allocator to manage the memory within that\nPrograms allocate blocks from the heap by calling the malloc function.\nReturns: pointer to allocated block if OK, NULL on error\nThe malloc function returns a pointer to a block of memory of at least size bytes\nblock.\nreturns a block whose address is always a multiple of 8.\nIf malloc encounters a problem (e.g., the program requests a block of memory\nfunction that initializes the allocated memory to zero.\nchange the size of a previously allocated block can use the realloc function.\nDynamic memory allocators such as malloc can allocate or deallocate heap\nPrograms free allocated heap blocks by calling the free function.\nThe ptr argument must point to the beginning of an allocated block that was\nAllocating and freeing\nblocks with malloc\nAllocated blocks are\nallocated blocks are shaded\nThe heavy-lined rectangles correspond to allocated blocks (shaded) and\nfree blocks (unshaded).\nword-aligned free block.1\nThe program asks for a four-word block.\ncarving out a four-word block from the front of the free block and return-\nThe program requests a ﬁve-word block.\nallocating a six-word block from the front of the free block.\nple, malloc pads the block with an extra word in order to keep the free\nblock aligned on a double-word boundary.\nThe program requests a six-word block and malloc responds by\ncarving out a six-word block from the free block.\nThe program frees the six-word block that was allocated in\n1. Throughout this section, we will assume that the allocator returns blocks aligned to 8-byte double-\nThe program requests a two-word block.\nallocates a portion of the block that was freed in the previous step and\nreturns a pointer to this new block.\nWhy Dynamic Memory Allocation?\nThe most important reason that programs use dynamic memory allocation is that\nDynamic memory allocation is a useful and important programming tech-\nquence of allocate and free requests, subject to the constraint that each\nfree request must correspond to a currently allocated block obtained from\na previous allocate request.\ntions about the ordering of allocate and free requests.\nmatching free request, or that matching allocate and free requests are\nThe allocator must align blocks in\nNot modifying allocated blocks.\nAllocators can only manipulate or change free\nblocks.\nonce they are allocated.\nGiven some sequence of n allocate and free\ntor completes 500 allocate requests and 500 free requests in 1 second, then its\nput by minimizing the average time to satisfy allocate and free requests.\nof free blocks and the running time of a free request is constant.\nof virtual memory allocated by all of the processes in a system is limited by the\nmemory allocator that might be asked to allocate and free large blocks of memory.\nwe are given some sequence of n allocate and free requests\nIf an application requests a block of p bytes, then the resulting allocated block has\ndenoted Pk, be the sum of the payloads of the currently allocated blocks, and let\nallocate requests.\nInternal fragmentation occurs when an allocated block is larger than the pay-\nof an allocator might impose a minimum size on allocated blocks that is greater\nthe differences between the sizes of the allocated blocks and their payloads.\nto satisfy an allocate request, but no single free block is large enough to handle\nadditional virtual memory from the kernel, even though there are eight free words\nover two free blocks.\nsuppose that after k requests all of the free blocks are exactly four words in size.\nIf all of the future allocate requests are for blocks that\nOn the other hand, if one or more requests ask for blocks larger than four words,\nlarger free blocks rather than large numbers of smaller free blocks.\nTo allocate\nHowever, since the allocator never reuses any blocks, memory utilization\nFree block organization.\nHow do we keep track of free blocks?\nHow do we choose an appropriate free block in which to place a\nnewly allocated block?\nAfter we place a newly allocated block in some free block, what do\nwe do with the remainder of the free block?\nblock organizations, we will introduce them in the context of a simple free block\nblock boundaries and to distinguish between allocated and free blocks.\nallocators embed this information in the blocks themselves.\nIn this case, a block consists of a one-word header, the payload, and possibly\nThe header encodes the block size (including the header\nand any padding) as well as whether the block is allocated or free.\ndouble-word alignment constraint, then the block size is always a multiple of 8 and\nthe 3 low-order bits of the block size are always zero.\nthe 29 high-order bits of the block size, freeing the remaining 3 bits to encode\nblock.\nBlock size\n(allocated block only)\nThe block size includes\na = 1: Allocated\nAllocated blocks are shaded.\nFree blocks are\nHeaders are labeled with (size (bytes)/allocated bit).\n(the allocated bit) to indicate whether the block is allocated or free.\nsuppose we have an allocated block with a block size of 24 (0x18) bytes.\nSimilarly, a free block with a block size of 40 (0x28) bytes would have a header of\nGiven the block format in Figure 9.35, we can organize the heap as a sequence\nof contiguous allocated and free blocks, as shown in Figure 9.36.\nWe call this organization an implicit free list because the free blocks are linked\nthe entire set of free blocks by traversing all of the blocks in the heap.\nheader with the allocated bit set and a size of zero.\nsetting the allocated bit simpliﬁes the coalescing of free blocks.)\nallocated blocks, will be linear in the total number of allocated and free blocks in\nallocator’s choice of block format impose a minimum block size on the allocator.\nNo allocated or free block may be smaller than this minimum.\nwe assume a double-word alignment requirement, then the size of each block\ninduces a minimum block size of two words: one word for the header and another\nsingle byte, the allocator would still create a two-word block.\ndouble-word alignment and uses an implicit free list with the block format from\nBlock size (decimal bytes)\nPlacing Allocated Blocks\nWhen an application requests a block of k bytes, the allocator searches the free\nlist for a free block that is large enough to hold the requested block.\nBest ﬁt examines every free block and chooses the free block with the smallest size\nAn advantage of ﬁrst ﬁt is that it tends to retain large free blocks at the end\nA disadvantage is that it tends to leave “splinters” of small free blocks\nblocks.\nmotivated by the idea that if we found a ﬁt in some free block the last time, there\nSplitting Free Blocks\nOnce the allocator has located a free block that ﬁts, it must make another policy\ndecision about how much of the free block to allocate.\nthe entire free block.\nSplitting a free block to satisfy a three-word allocation request.\nAllocated blocks are shaded.\nFree blocks are unshaded.\nHeaders are labeled with (size (bytes)/allocated bit).\nthe free block into two parts.\nThe ﬁrst part becomes the allocated block, and the\nremainder becomes a new free block.\nsplit the eight-word free block in Figure 9.36 to satisfy an application’s request for\nWhat happens if the allocator is unable to ﬁnd a ﬁt for the requested block?\noption is to try to create some larger free blocks by merging (coalescing) free\nblocks that are physically adjacent in memory (next section).\ndoes not yield a sufﬁciently large block, or if the free blocks are already maximally\ncoalesced, then the allocator asks the kernel for additional heap memory by calling\nThe allocator transforms the additional memory into one large\nfree block, inserts the block into the free list, and then places the requested block\nin this new free block.\nCoalescing Free Blocks\nWhen the allocator frees an allocated block, there might be other free blocks\nSuch adjacent free blocks can cause\nmemory chopped up into small, unusable free blocks.\nshows the result of freeing the block that was allocated in Figure 9.37.\nis two adjacent free blocks with payloads of three words each.\naggregate size of the two free blocks is large enough to satisfy the request.\nfree blocks in a process known as coalescing.\nfor deferred coalescing by waiting to coalesce free blocks at some later time.\nand then scan the entire heap, coalescing all free blocks.\nAllocated blocks are shaded.\nFree blocks are unshaded.\nHeaders are labeled with (size (bytes)/allocated bit).\nure 9.38, a repeated pattern of allocating and freeing a three-word block would\nto free as the current block.\nThen coalescing the next free block (in memory) is\nof the next block, which can be checked to determine if the next block is free.\nBut how would we coalesce the previous block?\ncurrent block.\nConsider all the cases that can exist when the allocator frees the current block:\n1. The previous and next blocks are both allocated.\n2. The previous block is allocated and the next block is free.\n3. The previous block is free and the next block is allocated.\n4. The previous and next blocks are both free.\nFormat of heap block that\nBlock size\n(allocated block only)\na = 001: Allocated\nBlock size\nIn case 1, both adjacent blocks are allocated and thus no coalescing is possible.\nSo the status of the current block is simply changed from allocated to free.\nThe header of the current block\nand the footer of the next block are updated with the combined sizes of the current\nand next blocks.\nwith the combined sizes of the two blocks.\nto form a single free block, with the header of the previous block and the footer of\nmany different types of allocators and free list organizations.\nwill consume half of each allocated block.\nthe need for a footer in allocated blocks.\nthe current block with the previous and next blocks in memory, the size ﬁeld in\nthe footer of the previous block is only needed if the previous block is free.\nwere to store the allocated/free bit of the previous block in one of the excess low-\norder bits of the current block, then allocated blocks would not need footers, and\nNote, however, that free blocks would\nallocated.\nAllocated block\nFree block\nmerous alternatives for block format and free list format, as well as placement,\nallocator based on an implicit free list with immediate boundary-tag coalescing.\nOur allocator uses a model of the memory system provided by the memlib.c\nbrk represent allocated virtual memory.\nThe allocator requests additional heap memory by\nThe allocator uses the block format",
      "keywords": [
        "Free blocks",
        "block",
        "free",
        "blocks",
        "virtual memory",
        "allocated blocks",
        "Memory",
        "block size",
        "allocated",
        "allocator",
        "free list",
        "virtual",
        "current block",
        "area",
        "virtual memory area"
      ],
      "concepts": [
        "allocated",
        "allocation",
        "allocator",
        "allocators",
        "allocate",
        "allocating",
        "allocates",
        "free",
        "freeing",
        "frees"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "",
          "score": 0.831,
          "base_score": 0.681,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.72,
          "base_score": 0.57,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "free",
          "block",
          "allocated",
          "blocks",
          "free blocks"
        ],
        "semantic": [],
        "merged": [
          "free",
          "block",
          "allocated",
          "blocks",
          "free blocks"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41264847854322273,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714608+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 880-902)",
      "start_page": 880,
      "end_page": 902,
      "summary": "The minimum block size is 16 bytes.\nThe padding is followed by a special prologue block, which is an 8-byte allocated\nblock consisting of only a header and a footer.\nor more regular blocks that are created by calls to malloc or free.\nalways ends with a special epilogue block, which is a zero-size allocated block\nblock\nblock 1\nblock 2\nblock n\nblock hdr\n(static) global variable (heap_listp) that always points to the prologue block.\nprologue block.)\n(WSIZE) and double words (DSIZE), and the size of the initial free block and\nThe PACK macro (line 9) combines a size and an allocate bit and\nThe GET_SIZE and GET_ALLOC macros (lines 16–17) return the size and\nmacros operate on block pointers (denoted bp) that point to the ﬁrst payload\nGiven a block pointer bp, the HDRP and FTRP macros (lines 20–21) return\npointers to the block header and footer, respectively.\nPREV_BLKP macros (lines 24–25) return the block pointers of the next and\nexample, given a pointer bp to the current block, we could use the following line\nof code to determine the size of the next block in memory:\n/* Pack a size and allocated bit into a word */\n((size) | (alloc))\n/* Read the size and allocated fields from address p */\n/* Given block ptr bp, compute address of its header and footer */\n/* Given block ptr bp, compute address of next and previous blocks */\nBefore calling mm_malloc or mm_free, the application must initialize the heap by\nthe initial free block.\nallocate and free requests from the application.\n/* Extend the empty heap with a free block of CHUNKSIZE bytes */\nFigure 9.44 mm_init creates a heap with an initial free block.\nstatic void *extend_heap(size_t words)\n/* Initialize free block header/footer and the epilogue header */\n/* Free block header */\n/* Free block footer */\n/* Coalesce if the previous block was free */\nFigure 9.45 extend_heap extends the heap with a new free block.\nheap returns a block whose size is an integral number of double words.\nfollowing the header of the epilogue block.\nthe new free block (line 12), and the last word of the chunk becomes the new\nepilogue block header (line 14).\nwas terminated by a free block, we call the coalesce function to merge the two\nfree blocks and return the block pointer of the merged blocks (line 17).\nFreeing and Coalescing Blocks\nAn application frees a previously allocated block by calling the mm_free function\n(Figure 9.46), which frees the requested block (bp) and then merges adjacent\nfree blocks using the boundary-tags coalescing technique described in Section\nfree list format we have chosen—with its prologue and epilogue blocks that are\nconditions where the requested block bp is at the beginning or end of the heap.\nAllocating Blocks\nAn application requests a block of size bytes of memory by calling the mm_malloc\nadjust the requested block size to allow room for the header and the footer, and to\nblock size of 16 bytes: 8 bytes to satisfy the alignment requirement and 8 more\nOnce the allocator has adjusted the requested size, it searches the free list for a\nsuitable free block (line 18).\nblock and optionally splits the excess (line 19) and then returns the address of the\nnewly allocated block.\nIf the allocator cannot ﬁnd a ﬁt, it extends the heap with a new free block\n(lines 24–26), places the requested block in the new free block, optionally splitting\nthe block (line 27), and then returns a pointer to the newly allocated block.\nsize_t next_alloc = GET_ALLOC(HDRP(NEXT_BLKP(bp)));\nmm_free frees a block and uses boundary-tag coalescing to merge it\nwith any adjacent free blocks in constant time.\n/* Adjusted block size */\n/* Adjust block size to include overhead and alignment reqs.\nGet more memory and place the block */\nFigure 9.47 mm_malloc allocates a block from the free list.\nYour solution should place the requested block at the beginning of the free\nblock, splitting only if the size of the remainder would equal or exceed the mini-\nmum block size.\nHowever, because block allocation time is linear in the total\nnumber of heap blocks, the implicit free list is not appropriate for a general-\nthe number of heap blocks is known beforehand to be small).\nA better approach is to organize the free blocks into some form of explicit\nSince by deﬁnition the body of a free block is not needed by the\nbodies of the free blocks.\neach free block, as shown in Figure 9.48.\nallocation time from linear in the total number of blocks to linear in the number\nof free blocks.\nHowever, the time to free a block can be either linear or constant,\ndepending on the policy we choose for ordering the blocks in the free list.\nBlock size\n(a) Allocated block\nBlock size\nBlock size\n(b) Free block\nBlock size\nFormat of heap blocks that use doubly linked free lists.\nblocks ﬁrst.\nIn this case, freeing a block can be performed in constant time.\nof each block in the list is less than the address of its successor.\nA disadvantage of explicit lists in general is that free blocks must be large\nThis results in a larger minimum block size and increases the potential\nAs we have seen, an allocator that uses a single linked list of free blocks requires\ntime linear in the number of free blocks to allocate a block.\nmultiple free lists, where each list holds blocks that are roughly the same size.\ngeneral idea is to partition the set of all possible block sizes into equivalence classes\nmight partition the block sizes by powers of 2:\nOr we might assign small blocks to their own size classes and partition large blocks\nThe allocator maintains an array of free lists, with one free list per size class,\nWhen the allocator needs a block of size n, it searches\nIf it cannot ﬁnd a block that ﬁts, it searches the next list,\nWith simple segregated storage, the free list for each size class contains same-size\nblocks, each the size of the largest element of the size class.\nsize class is deﬁned as {17–32}, then the free list for that class consists entirely of\nblocks of size 32.\nTo allocate a block of some given size, we check the appropriate free list.\nlist is not empty, we simply allocate the ﬁrst block in its entirety.\nFree blocks are\nmultiple of the page size), divides the chunk into equal-size blocks, and links the\nblocks together to form the new free list.\nTo free a block, the allocator simply\ninserts the block at the front of the appropriate free list.\nfreeing blocks are both fast constant-time operations.\nof the same-size blocks in each chunk, no splitting, and no coalescing means that\nthere is very little per-block memory overhead.\nsize blocks, the size of an allocated block can be inferred from its address.\nthere is no coalescing, allocated blocks do not need an allocated/free ﬂag in the\nThus, allocated blocks require no headers, and since there is no coalescing,\nSince allocate and free operations insert\nand delete blocks at the beginning of the free list, the list need only be singly\nany block is a one-word succ pointer in each free block, and thus the minimum\nblock size is only one word.\nfree blocks are never split.\nexternal fragmentation because free blocks are never coalesced (Practice Prob-\nWith this approach, the allocator maintains an array of free lists.\nEach list contains potentially different-size blocks whose sizes are members of the\nTo allocate a block, we determine the size class of the request and do a ﬁrst-\nﬁt search of the appropriate free list for a block that ﬁts.\nﬁnd a block that ﬁts, then we search the free list for the next larger size class.\nIf none of the free lists yields a block that ﬁts,\nthen we request additional heap memory from the operating system, allocate the\nblock out of this new heap memory, and place the remainder in the appropriate\nTo free a block, we coalesce and place the result on the appropriate\nlist for each block size 2k, where 0 ≤k ≤m.\nRequested block sizes are rounded up\nOriginally, there is one free block of size 2m words.\nTo allocate a block of size 2k, we ﬁnd the ﬁrst available block of size 2j, such\nTo free a block of size 2k, we continue\nA key fact about buddy systems is that, given the address and size of a block,\nFor example, a block of size 32 bytes\nIn other words, the addresses of a block and its buddy differ in exactly one bit\nblock size can cause signiﬁcant internal fragmentation.\nfor certain application-speciﬁc workloads, where the block sizes are known in\nand frees heap blocks by making calls to malloc and free.\nresponsibility to free any allocated blocks that it no longer needs.\nFailing to free allocated blocks is a common programming error.\nconsider the following C function that allocates a block of temporary storage as\nUnfortunately, the programmer has forgotten to free the block.\nA garbage collector is a dynamic storage allocator that automatically frees al-\nlocated blocks that are no longer needed by the program.\nSuch blocks are known\ngarbage collection, applications explicitly allocate heap blocks but never explic-\nblocks and makes the appropriate calls to free to place those blocks back on the\nEach heap node corresponds to an allocated block\nA directed edge p →q means that some location in block p points to\nThey are conservative in the sense that each reachable block is correctly\nIf malloc is unable to ﬁnd a free block that ﬁts, then it calls the garbage col-\nthe garbage blocks and returns them to the heap by calling the free function.\nto the collector returns, malloc tries again to ﬁnd a free block that ﬁts.\nreturns a pointer to the requested block (if successful) or the NULL pointer (if\nwhich frees each unmarked allocated block.\nbits in the block header is used to indicate whether a block is marked or not.\nIf p points to some word in an allocated block, it returns a\npointer b to the beginning of that block.\nReturns true if block b is already marked.\nReturns true if block b is allocated.\nMarks block b.\nblock b.\nChanges the status of block b from marked to un-\nReturns the successor of block b in the heap.\nan allocated and unmarked heap block.\nOtherwise, it marks the block and calls\nitself recursively on each word in block.\nphase, any allocated block that is not marked is guaranteed to be unreachable and,\nThe sweep function iterates over each block in the heap, freeing any unmarked\nallocated blocks (i.e., garbage) that it encounters.\nEach block has a one-word header, which is either marked or\nUnmarked block\nMarked block\nallocated blocks.\nRemainder of block\nAllocated block header\nInitially, the heap in Figure 9.52 consists of six allocated blocks, each of which\nBlock 3 contains a pointer to block 1.\nBlock 4 contains pointers to\nblocks 3 and 6.\nThe root points to block 4.\nAfter the mark phase, blocks 1, 3, 4, and 6\nBlocks 2 and 5 are unmarked\nan allocated block.\nOne solution to the latter problem is to maintain the set of allocated blocks\nﬁelds (left and right) in the header of each allocated block.\nthe header of some allocated block.\nperform a binary search of the allocated blocks.\nas application users would certainly not appreciate having their allocated blocks\nexample, suppose that some reachable allocated block contains an int in its\nother allocated block b.\nblock b as reachable, when in fact it might not be.\nblock, we may not discover the error until we free the block much later in the\nAlthough p still points to a valid memory address, it\nReferencing Data in Free Heap Blocks\nA similar error is to reference data in heap blocks that have already been freed.\nprematurely frees block x in line 10, and then later references it in line 14:\nx[i] is a word in a free block */\nsome other allocated heap block and may have been overwritten.\ncreate garbage in the heap by forgetting to free allocated blocks.\nfollowing function allocates a heap block x and then returns without freeing it:\nThe block in a virtual memory cache is known as a page.\nbetween processes, the allocation of memory for processes, and program loading.\nmemory allocator such as malloc, which manages memory in an area of the virtual\nrequire applications to explicitly free their memory blocks.\n(garbage collectors) free any unused and unreachable blocks automatically.\non an explicit free list with a block size and successor pointer in each free block.",
      "keywords": [
        "block",
        "free block",
        "free list",
        "size",
        "free",
        "block size",
        "blocks",
        "heap",
        "allocated block",
        "memory",
        "list",
        "int",
        "heap blocks",
        "free allocated blocks",
        "free block header"
      ],
      "concepts": [
        "blocks",
        "allocated",
        "allocator",
        "allocate",
        "allocating",
        "allocates",
        "allocation",
        "allocators",
        "sizes",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 46,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.613,
          "base_score": 0.613,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.551,
          "base_score": 0.551,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 4,
          "title": "",
          "score": 0.55,
          "base_score": 0.55,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "block",
          "free",
          "blocks",
          "free block",
          "size"
        ],
        "semantic": [],
        "merged": [
          "block",
          "free",
          "blocks",
          "free block",
          "size"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.440311666987741,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714703+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 903-922)",
      "start_page": 903,
      "end_page": 922,
      "summary": "Cache byte returned\nGiven an input ﬁle hello.txt that consists of the string Hello, world!\\n, write\nDetermine the block sizes and header values that would result from the fol-\n(2) Block sizes are rounded up to the nearest multiple of 8 bytes.\nBlock size (decimal bytes)\nDetermine the minimum block size for each of the following combinations of\npred and succ pointers in each free block, zero-size payloads are not allowed, and\nsize (bytes)\ndecreasing block sizes results in low performance for allocations, but\nThis problem gives you some appreciation for the sizes of different address spaces.\naddress bits (n)\nvoid mmapcopy(int fd, int size)\nbufp = Mmap(NULL, size, PROT_READ, MAP_PRIVATE, fd, 0);\nWrite(1, bufp, size);\nfd = Open(argv[1], O_RDONLY, 0);\nimum block sizes, and header encodings.\nthe block size is to round the sum of the requested payload and the header size\nexample, the block size for the malloc(2) request is 4 + 2 = 6 rounded up to 8.\nThe block size for the malloc(20) request is 20 + 4 = 24 rounded up to 24.\nBlock size (decimal bytes)\nThe minimum block size can have a signiﬁcant effect on internal fragmentation.\nThus, it is good to understand the minimum block sizes associated with different\nsame block can be allocated or free at different points in time.\nblock size is the maximum of the minimum allocated block size and the minimum\nfree block size.\nfree block size is a 4-byte header and 4-byte footer, which is already a multiple of\nSo the minimum block size for this allocator is\nsize (bytes)\nif (!GET_ALLOC(HDRP(bp)) && (asize <= GET_SIZE(HDRP(bp)))) {\nNotice that for this allocator the minimum block size is 16 bytes.\nsize, then we go ahead and split the block (lines 6–10).\nis to realize that you need to place the new allocated block (lines 6 and 7) before\nnumerous allocation and free requests to the ﬁrst size class, followed by numer-\nous allocation and free requests to the second size class, followed by numerous\nallocation and free requests to the third size class, and so on.\ndoesn’t coalesce, and because the application never requests blocks from that size\nReading and Writing Files\nRobust Reading and Writing with the Rio Package\nPutting It Together: Which I/O Functions Should I Use?\nnput/output (I/O) is the process of copying data between main memory and ex-\ncopies data from an I/O device to main memory, and an output operation copies\nI/O.\nFor example, ANSI C provides the standard I/O library, with functions such as\nLinux systems, these higher-level I/O functions are implemented using system-\nlevel Unix I/O functions provided by the kernel.\nI/O functions work quite well and there is no need to use Unix I/O directly.\ninto I/O in more detail.\nFor example, the standard I/O library provides no way to access ﬁle\nmetadata such as ﬁle size or ﬁle creation time.\nI/O and shows you how to use them reliably from your C programs.\nA Linux ﬁle is a sequence of m bytes:\nall input and output is performed by reading and writing the appropriate ﬁles.\nby asking the kernel to open the corresponding ﬁle.\ninformation about the open ﬁle.\nChanging the current ﬁle position.\n0, for each open ﬁle.\nThe ﬁle position is a byte offset from the beginning\nof a ﬁle.\nAn application can set the current ﬁle position k explicitly by\nReading and writing ﬁles.\nA read operation copies n > 0 bytes from a ﬁle to\nmemory, starting at the current ﬁle position k and then incrementing k\nGiven a ﬁle with a size of m bytes, performing a read operation\nthe end of a ﬁle.\nSimilarly, a write operation copies n > 0 bytes from memory to a ﬁle,\nWhen an application has ﬁnished accessing a ﬁle, it informs the\nkernel by asking it to close the ﬁle.\nthe data structures it created when the ﬁle was opened and restoring the\nfor any reason, the kernel closes all open ﬁles and frees their memory\nEach Linux ﬁle has a type that indicates its role in the system:\nA Linux text ﬁle consists of a sequence of text lines, where each line is a\nﬁlename to a ﬁle, which may be another directory.\n. A socket is a ﬁle that is used to communicate with another process across a\nOther ﬁle types include named pipes, symbolic links, and character and block\nA process opens an existing ﬁle or creates a new ﬁle by calling the open function.\nReturns: new ﬁle descriptor if OK, −1 on error\nThe open function converts a filename to a ﬁle descriptor and returns the de-\nReading only\nReading and writing\nFor example, here is how to open an existing ﬁle for reading:\nfd = Open(\"foo.txt\", O_RDONLY, 0);\nBefore each write operation, set the ﬁle position to the end of\nthe ﬁle.\nUser (owner) can read this ﬁle\nUser (owner) can write this ﬁle\nMembers of the owner’s group can read this ﬁle\nMembers of the owner’s group can write this ﬁle\nOthers (anyone) can read this ﬁle\nOthers (anyone) can write this ﬁle\nOthers (anyone) can execute this ﬁle\nDeﬁned in sys/stat.h. For example, here is how you might open an existing ﬁle with the intent of\nfd = Open(\"foo.txt\", O_WRONLY|O_APPEND, 0);\nWhen a process creates a new ﬁle by calling the open function\nwith some mode argument, then the access permission bits of the ﬁle are set to\nS_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP|S_IROTH|S_IWOTH\nS_IWGRP|S_IWOTH\nThen the following code fragment creates a new ﬁle in which the owner of the ﬁle\nfd = Open(\"foo.txt\", O_CREAT|O_TRUNC|O_WRONLY, DEF_MODE);\nFinally, a process closes an open ﬁle by calling the close function.\nfd1 = Open(\"foo.txt\", O_RDONLY, 0);\nfd2 = Open(\"baz.txt\", O_RDONLY, 0);\nReading and Writing Files\nApplications perform input and output by calling the read and write functions,\nssize_t read(int fd, void *buf, size_t n);\nReturns: number of bytes read if OK, 0 on EOF, −1 on error\nssize_t write(int fd, const void *buf, size_t n);\nReturns: number of bytes written if OK, −1 on error\nThe read function copies at most n bytes from the current ﬁle position of descriptor\nThe write function copies at most n bytes from memory location buf to the\ncurrent ﬁle position of descriptor fd.\nFigure 10.3 shows a program that uses read\nand write calls to copy the standard input to the standard output, 1 byte at a time.\nApplications can explicitly modify the current ﬁle position by calling the\nIn some situations, read and write transfer fewer bytes than the application\nYou might have noticed that the read function has a size_t input argument and an ssize_t return\nThe read function returns a signed\nsize rather than an unsigned size because it must return a −1 on error.\nreturning a single −1 reduces the maximum size of a read by a factor of 2.\nUsing read and write to copy standard input to standard output 1 byte\nSuppose that we are ready to read from a ﬁle that\ncontains only 20 more bytes from the current ﬁle position and that we are\nreading the ﬁle in 50-byte chunks.\nThen the next read will return a short\ncount of 20, and the read after that will signal EOF by returning a short\nReading text lines from a terminal.\nIf the open ﬁle is associated with a terminal\ntext line at a time, returning a short count equal to the size of the text line.\nReading and writing network sockets.\nIf the open ﬁle corresponds to a network\nwork delays can cause read and write to return short counts.\nread and write until all requested bytes have been transferred.\nRobust Reading and Writing with the Rio Package\nbetween memory and a ﬁle, with no application-level buffering.\nespecially useful for reading and writing binary data to and from networks.\nThese functions allow you to efﬁciently read text lines\nand binary data from a ﬁle whose contents are cached in an application-\nlevel buffer, similar to the one provided for standard I/O functions such as\nRio Unbuffered Input and Output Functions\nApplications can transfer data directly between memory and a ﬁle by calling the\nssize_t rio_readn(int fd, void *usrbuf, size_t n);\nssize_t rio_writen(int fd, void *usrbuf, size_t n);\nReturns: number of bytes transferred if OK, 0 on EOF (rio_readn only), −1 on error\nThe rio_readn function transfers up to n bytes from the current ﬁle position\ntransfers n bytes from location usrbuf to descriptor fd.\nfunction manually restarts the read or write function if it is interrupted by the\nRio Buffered Input Functions\nSuppose we wanted to write a program that counts the number of lines in a text ﬁle.\nOne approach is to use the read function to transfer 1 byte\nat a time from the ﬁle to the user’s memory, checking each byte for the newline\ntrap to the kernel to read each byte in the ﬁle.\nbytes from the same read buffer as rio_readlineb.\nssize_t rio_readlineb(rio_t *rp, void *usrbuf, size_t maxlen);\nssize_t rio_readnb(rio_t *rp, void *usrbuf, size_t n);\nReturns: number of bytes read if OK, 0 on EOF, −1 on error\nThe rio_readinitb function is called once per open descriptor.\ndescriptor fd with a read buffer of type rio_t at address rp.\nThe rio_readlineb function reads the next text line from ﬁle rp (including\nfunction reads at most maxlen-1 bytes, leaving room for the terminating NULL\nThe rio_readnb function reads up to n bytes from ﬁle rp to memory location\nYou will encounter numerous examples of the Rio functions in the remainder\nFigure 10.5 shows how to use the Rio functions to copy a text ﬁle from\nan empty read buffer and associates an open ﬁle descriptor with that buffer.\nssize_t rio_readn(int fd, void *usrbuf, size_t n)\nsize_t nleft = n;\nif ((nread = read(fd, bufp, nleft)) < 0) {\n/* and call read() again */\nssize_t rio_writen(int fd, void *usrbuf, size_t n)\nsize_t nleft = n;",
      "keywords": [
        "ﬁle",
        "Block size",
        "size",
        "Rio",
        "block",
        "read",
        "minimum block size",
        "Free block size",
        "bytes",
        "ﬁles",
        "function",
        "ﬁle position",
        "open ﬁle",
        "open",
        "Problem"
      ],
      "concepts": [
        "size",
        "byte",
        "bytes",
        "functions",
        "functionality",
        "function",
        "read",
        "reading",
        "reads",
        "returned"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 44,
          "title": "",
          "score": 0.857,
          "base_score": 0.707,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 46,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "",
          "score": 0.641,
          "base_score": 0.641,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.591,
          "base_score": 0.591,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 15,
          "title": "",
          "score": 0.559,
          "base_score": 0.559,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ﬁle",
          "open",
          "size",
          "fd",
          "block"
        ],
        "semantic": [],
        "merged": [
          "ﬁle",
          "open",
          "size",
          "fd",
          "block"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.45147427431023923,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714766+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 923-940)",
      "start_page": 923,
      "end_page": 940,
      "summary": "Copying a text ﬁle from standard input to standard output.\nint rio_fd;\nint rio_cnt;\n} rio_t;\nvoid rio_readinitb(rio_t *rp, int fd)\nrp->rio_fd = fd;\nA read buffer of type rio_t and the rio_readinitb function that initializes it.\nThe heart of the Rio read routines is the rio_read function shown in Fig-\nThe rio_read function is a buffered version of the Linux read function.\nWhen rio_read is called with a request to read n bytes, there are rp->rio_cnt\nstatic ssize_t rio_read(rio_t *rp, char *usrbuf, size_t n)\nrp->rio_cnt = read(rp->rio_fd, rp->rio_buf,\n/* Copy min(n, rp->rio_cnt) bytes from internal buf to user buf */\nif (rp->rio_cnt < n)\nThe internal rio_read function.\nnonempty, rio_read copies the minimum of n and rp->rio_cnt bytes from the\nread buffer to the user buffer and returns the number of bytes copied.\nTo an application program, the rio_read function has the same semantics as\nthe Linux read function.\nthe number of unread bytes in the read buffer.\nmakes it easy to build different kinds of buffered read functions by substituting\nrio_read for read.\nFor example, the rio_readnb function in Figure 10.8 has the\nsame structure as rio_readn, with rio_read substituted for read.\nrio_readlineb routine in Figure 10.8 calls rio_read at most maxlen-1 times.\nEach call returns 1 byte from the read buffer, which is then checked for being the\nif ((rc = rio_read(rp, &c, 1)) == 1) {\nssize_t rio_readnb(rio_t *rp, void *usrbuf, size_t n)\nif ((nread = rio_read(rp, bufp, nleft)) < 0)\nThe rio_readlineb and rio_readnb functions.\nThe Rio functions are inspired by the readline, readn, and writen functions described by W.\nThe rio_readn and rio_writen functions are\nfunctions cannot be used together on the same descriptor.\nrio_readnb functions, which are mutually compatible and thread-safe.\nmetadata) by calling the stat and fstat functions.\nThe stat function takes as input a ﬁlename and ﬁlls in the members of a stat\nThe fstat function is similar, but it takes a ﬁle\nThe st_size member contains the ﬁle size in bytes.\nLinux deﬁnes macro predicates in sys/stat.h for determining the ﬁle type from\nFigure 10.10 shows how we might use these macros and the stat function to read\nand interpret a ﬁle’s st_mode bits.\n/* Metadata returned by the stat and fstat functions */\nif ((stat.st_mode & S_IRUSR)) /* Check read access */\nprintf(\"type: %s, read: %s\\n\", type, readok);\nfunctions.\nThe opendir function takes a pathname and returns a pointer to a directory stream.\nure 10.11 shows how we might use readdir to read the contents of a directory.\npicture of how the kernel represents open ﬁles, the idea of ﬁle sharing can be quite\nDescriptor table.\ntries are indexed by the process’s open ﬁle descriptors.\nThe set of open ﬁles is represented by a ﬁle table that is shared by all\nﬁle position, a reference count of the number of descriptor entries that\na descriptor decrements the reference count in the associated ﬁle table\nThe kernel will not delete the ﬁle table entry until its reference\nLike the ﬁle table, the v-node table is shared by all processes.\nDescriptor table\nOpen file table\nthrough two open ﬁle table\nDescriptor table\nOpen file table\ndifferent ﬁles through distinct open ﬁle table entries.\nwhere ﬁles are not shared and where each descriptor corresponds to a distinct ﬁle.\nMultiple descriptors can also reference the same ﬁle through different ﬁle\neach descriptor has its own distinct ﬁle position, so different reads on different\ndescriptors can fetch data from different locations in the ﬁle.\nand child share the same set of open ﬁle tables and thus share the same ﬁle pos-\ndescriptors before the kernel will delete the corresponding ﬁle table entry.\nDescriptor tables\nOpen file table\nfd1 = Open(\"foobar.txt\", O_RDONLY, 0);\nfd2 = Open(\"foobar.txt\", O_RDONLY, 0);\nRead(fd1, &c, 1);\nRead(fd2, &c, 1);\nfd = Open(\"foobar.txt\", O_RDONLY, 0);\nRead(fd, &c, 1);\nRead(fd, &c, 1);\nOne way is to use the dup2 function.\nThe dup2 function copies descriptor table entry oldfd to descriptor table entry\nwhere descriptor 1 (standard output) corresponds to ﬁle A (say, a terminal)\nBoth descriptors now point to ﬁle B; ﬁle A has been closed and its\nﬁle table and v-node table entries deleted; and the reference count for ﬁle B has\nHow would you use dup2 to redirect standard input to descriptor 5?\nDescriptor table\nOpen file table\nfd1 = Open(\"foobar.txt\", O_RDONLY, 0);\nfd2 = Open(\"foobar.txt\", O_RDONLY, 0);\nRead(fd2, &c, 1);\nRead(fd1, &c, 1);\nStandard I/O\nThe C language deﬁnes a set of higher-level input and output functions, called the\nThe library (libc) provides functions for opening and closing ﬁles\n(fopen and fclose), reading and writing bytes (fread and fwrite), reading and\nThe standard I/O library models an open ﬁle as a stream.\n/* Standard input (descriptor 0) */\n/* Standard output (descriptor 1) */\n/* Standard error (descriptor 2) */\nA stream of type FILE is an abstraction for a ﬁle descriptor and a stream\nThe purpose of the stream buffer is the same as the Rio read buffer: to\nwe have a program that makes repeated calls to the standard I/O getc function,\nthe ﬁrst time, thelibraryﬁllsthestream bufferwith asinglecalltothe read function\nPutting It Together: Which I/O Functions Should I Use?\nStandard I/O\nfunctions\nRio\nfunctions\nUnix I/O functions\nRelationship between Unix I/O, standard I/O, and Rio.\nable to applications through functions such as open, close, lseek, read, write,\nThe higher-level Rio and standard I/O functions are implemented “on\ntop of” (using) the Unix I/O functions.\nThe Rio functions are robust wrappers for\nThe standard I/O functions provide a more complete buffered alterna-\ntive to the Unix I/O functions, including formatted I/O routines such as printf\nSo which of these functions should you use in your programs?\nG1: Use the standard I/O functions whenever possible.\nC programmers use standard I/O exclusively throughout their careers,\nnever bothering with the lower-level Unix I/O functions (except possibly\nstat, which has no counterpart in the standard I/O library).\nG2: Don’t use scanf or rio_readlineb to read binary ﬁles.Functions like scanf\nand rio_readlineb are designed speciﬁcally for reading text ﬁles.\ncommon error that students make is to use these functions to read binary\nG3: Use the Rio functions for I/O on network sockets.\ntion for a network is a type of ﬁle called a socket.\nother computers by reading and writing socket descriptors.\nStandard I/O streams are full duplex in the sense that programs can perform\nRestriction 1: Input functions following output functions.\nAn input function\nThe latter three functions use the Unix I/O lseek\nfunction to reset the current ﬁle position.\nRestriction 2: Output functions following input functions.\nAn output function\nfsetpos, or rewind, unless the input function encounters an end-of-ﬁle.\nto use the lseek function on a socket.\nopen two streams on the same open socket descriptor, one for reading and one\nThus, we recommend that you not use the standard I/O functions for input\nUse the robust Rio functions instead.\nformatted output, use the sprintf function to format a string in memory, and then\nIf you need formatted input, use rio_\nLinux provides a small number of system-level functions, based on the Unix I/O\nmodel, that allow applications to open, close, read, and write ﬁles, to fetch ﬁle\nInstead of calling the Unix I/O functions directly, applications should use the Rio\nThe Linux kernel uses three related data structures to represent open ﬁles.\nEntries in a descriptor table point to entries in the open ﬁle table, which point\nwhile all processes share the same open ﬁle and v-node tables.\nFor most applications, standard I/O is the\nincompatible restrictions on standard I/O and network ﬁles, Unix I/O, rather than\nstandard I/O, should be used for network applications.\nKerrisk gives a comprehensive treatment of Unix I/O and the Linux ﬁle sys-\nStevens wrote the original standard reference text for Unix I/O [111].\nfunctions [61].\nfd1 = Open(\"foo.txt\", O_RDONLY, 0);\nfd2 = Open(\"bar.txt\", O_RDONLY, 0);\nfd2 = Open(\"baz.txt\", O_RDONLY, 0);\nModify the cpfile program in Figure 10.5 so that it uses the Rio functions to copy\nThe open function always re-\nThe call to the close function frees up descriptor 3.\ndescriptor 3, and thus the output of the program is fd2 = 3.\nThe descriptors fd1 and fd2 each have their own open ﬁle table entry, so each\ndescriptor has its own ﬁle position for foobar.txt.\nshared the same open ﬁle table.\nchild points to the same open ﬁle table entry.\nTo redirect standard input (descriptor 0) to descriptor 5, we would call dup2(5,0),",
      "keywords": [
        "rio",
        "ﬁle",
        "File",
        "open ﬁle table",
        "read",
        "ﬁle table",
        "open",
        "functions",
        "Open file table",
        "Descriptor",
        "standard",
        "open ﬁle",
        "Rio functions",
        "function",
        "Descriptor table"
      ],
      "concepts": [
        "function",
        "functions",
        "descriptor",
        "descriptors",
        "read",
        "reading",
        "reads",
        "returns",
        "returned",
        "type"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.703,
          "base_score": 0.553,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 44,
          "title": "",
          "score": 0.655,
          "base_score": 0.505,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "",
          "score": 0.482,
          "base_score": 0.482,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.479,
          "base_score": 0.479,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.47,
          "base_score": 0.47,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "open",
          "descriptor",
          "ﬁle",
          "functions",
          "ﬁle table"
        ],
        "semantic": [],
        "merged": [
          "open",
          "descriptor",
          "ﬁle",
          "functions",
          "ﬁle table"
        ]
      },
      "topic_id": 7,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3701554470911253,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714823+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 941-960)",
      "start_page": 941,
      "end_page": 960,
      "summary": "server programming model and how to write client-server programs that use the\nThe Client-Server Programming Model\nEvery network application is based on the client-server model.\napplication consists of a server process and one or more client processes.\n3. The server sends a response to the client and then waits for the next request.\nFor example, a Web server sends the ﬁle back to a client.\nA client-server transaction.\nThe client-server model is the same, regardless\nof the mapping of clients and servers to hosts.\nClients and servers often run on separate hosts and communicate using the hard-\nof a network host.\nHost\nHost\nHost\nHowever, if host A\nsends a frame to host C on a different segment, then bridge X will copy the frame\nconnected to host C’s segment.\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nHost\nEach router has an adapter (port) for each network that it is connected\nample, Figure 11.6 shows an example internet with a pair of LANs and WANs\nHost\nHost\nHost\nHost\nHost\nHost\nEach host is\nways of assigning addresses to hosts.\ndifferences by deﬁning a uniform format for host addresses.\nEach host\nis then assigned at least one of these internet addresses that uniquely\nFigure 11.7 shows an example of how hosts and routers use the internet\nprotocol to transfer data across incompatible LANs. The example internet consists\nA client running on host A, which is attached\nto LAN1, sends a sequence of data bytes to a server running on host B, which is\n1. The client on host A invokes a system call that copies the data from the client’s\n2. The protocol software on host A creates a LAN1 frame by appending an\ninternet header and a LAN1 frame header to the data.\nis addressed to internet host B.\nThe LAN1 frame header is addressed to the\nLAN1 frame is an internet packet, whose payload is the actual user data.\nHost A\nHow data travel from one host to another on an internet.\n5. The router fetches the destination internet address from the internet packet\n6. The router’s LAN2 adapter copies the frame to the network.\n8. Finally, the protocol software on host B strips off the packet header and frame\nInternet client host\nInternet server host\nGlobal IP Internet\nThe Global IP Internet\ninternet.\nclient-server application.\nEach Internet host runs software that implements the TCP/IP protocol\nInternet clients and servers communicate using\nsockets interface in Section 11.4.) The sockets functions are typically implemented\nmechanism that can send packets, known as datagrams, from one Internet host to\nany other host.\n. The set of hosts is mapped to a set of 32-bit IP addresses.\nThe original Internet protocol, with its 32-bit addresses, is known as Internet Protocol Version 4 (IPv4).\nProtocol Version 6 (IPv6), that uses 128-bit addresses and that was intended as the successor to IPv4.\n. The set of IP addresses is mapped to a set of identiﬁers called Internet domain\n. A process on one Internet host can communicate with a process on any other\nInternet host over a connection.\nIP Addresses\nAn IP address is an unsigned 32-bit integer.\nNetwork programs store IP addresses\nin the IP address structure shown in Figure 11.9.\nBecause Internet hosts can have different host byte orders, TCP/IP deﬁnes a\nas an IP address, that is carried across the network in a packet header.\nAddresses in\nIP address structures are always stored in (big-endian) network byte order, even\nconverting between network and host byte order.\n/* IP address structure */\ns_addr; /* Address in network byte order (big-endian) */\nIP address structure.\nThe htonl function converts an unsigned 32-bit integer from host byte order to\nnetwork byte order to host byte order.\nown host:\nApplication programs can convert back and forth between IP addresses and\naddress in network byte order (dst).\ninet_ntop function converts a binary IP address in network byte order (src) to\nInternet Domain Names\nInternet clients and servers use IP addresses when they communicate with each\nmechanism that maps the set of domain names to the set of IP addresses.\nThe Internet deﬁnes a mapping between the set of domain names and the\nset of IP addresses.\nthe mapping between a set of domain names and a set of IP addresses.\nand IP addresses.\nwith the Linux nslookup program, which displays the IP addresses associated with\nEach Internet host has the locally deﬁned domain name localhost, which\nAddress: 127.0.0.1\nand an IP address:\nAddress: 128.2.210.175\naddress:\nAddress: 18.62.1.6\nAddress: 18.62.1.6\nmultiple IP addresses:\nAddress: 199.16.156.6\nAddress: 199.16.156.70\nAddress: 199.16.156.102\nAddress: 199.16.156.230\nAddress: 199.16.156.102\nAddress: 199.16.156.230\nAddress: 199.16.156.6\nAddress: 199.16.156.70\naddress:\nInternet Connections\nInternet clients and servers communicate by sending and receiving streams of\nHow many Internet hosts are there?\nsurvey, which estimates the number of Internet hosts by counting the number of IP addresses that\n20,000 Internet hosts, the number of hosts has been increasing exponentially.\n1,000,000,000 Internet hosts!\nA socket is an end point of a connection.\nsocket address that consists of an Internet address and a 16-bit integer port2 and\nThe port in the client’s socket address is assigned automatically by the kernel\nwhen the client makes a connection request and is known as an ephemeral port.\nHowever, the port in the server’s socket address is typically some well-known\nA connection is uniquely identiﬁed by the socket addresses of its two end\nwhere cliaddr is the client’s IP address, cliport is the client’s port, servaddr is the\nserver’s IP address, and servport is the server’s port.\nshows a connection between a Web client and a Web server.\nIn this example, the Web client’s socket address is\nsocket address is\nwere more than 50,000 hosts.\nwere almost 10,000,000 Internet hosts, NSF retired NSFNET and replaced it with the modern Internet\nClient host address\nConnection socket pair\nServer host address\nClient socket address\nServer socket address\nclient and server socket addresses, the connection between the client and server\nThe sockets interface is a set of functions that are used in conjunction with the Unix\ntypical client-server transaction.\nOverview of network applications based on the sockets interface.\n/* IP socket address structure */\n/* IP address in network byte order */\n/* Generic socket address structure (for connect, bind, and accept) */\nSocket address structures.\nSocket Address Structures\nInternet socket addresses are stored in 16-byte structures having the type\nﬁeld contains a 32-bit IP address.\nThe IP address and port number are always\nThe connect, bind, and accept functions require a pointer to a protocol-\nspeciﬁc socket address structure.\ninterface was how to deﬁne these functions to accept any kind of socket address\nThe socket Function\nClients and servers use the socket function to create a socket descriptor.\nIf we wanted the socket to be the end point for a connection, then we could call\nwhere AF_INET indicates that we are using 32-bit IP addresses and SOCK_\nSTREAM indicates that the socket will be an end point for a connection.\nshow you how to use getaddrinfo with the socket function in Section 11.4.8.\ndepends on whether we are a client or a server.\nwe ﬁnish opening the socket if we are a client.\nThe connect Function\nA client establishes a connection with a server by calling the connect function.\nThe connect function attempts to establish an Internet connection with the server\nat socket address addr, where addrlen is sizeof(sockaddr_in).\nand the resulting connection is characterized by the socket pair\nwhere x is the client’s IP address and y is the ephemeral port that uniquely\nidentiﬁes the client process on the client host.\nto establish connections with clients.\nThe bind function asks the kernel to associate the server’s socket address in addr\nAs with socket and connect, the best practice is to use getaddrinfo to\nClients are active entities that initiate connection requests.\nentities that wait for connection requests from clients.\nsocket that will live on the client end of a connection.\nthat can accept connection requests from clients.\n2. Client makes connection request by\nClient returns from connect.\nServers wait for connection requests from clients by calling the accept function.\nThe accept function waits for a connection request from a client to arrive on\nthe listening descriptor listenfd, then ﬁlls in the client’s socket address in addr,\nand returns a connected descriptor that can be used to communicate with the client\nestablished between the client and the server.\naccepts a connection request and exists only as long as it takes the server to service\nstep 1, the server calls accept, which waits for a connection request to arrive on\nIn step 2, the client calls the connect function, which sends a connection\nIn step 3, the accept function opens a new connected de-\nYou might wonder why the sockets interface makes a distinction between listening and connected\nprocess many client connections simultaneously.\nclient also returns from the connect, and from this point, the client and server\nfor converting back and forth between binary socket address structures and the\nstring representations of hostnames, host addresses, service names, and port\naddresses, service names, and port numbers into socket address structures.",
      "keywords": [
        "Internet",
        "host",
        "Internet hosts",
        "socket address",
        "address",
        "client",
        "server",
        "socket",
        "internet protocol",
        "network",
        "Internet client host",
        "Internet server host",
        "Web server",
        "network byte order",
        "protocol"
      ],
      "concepts": [
        "internet",
        "internets",
        "hosts",
        "host",
        "functional",
        "functions",
        "function",
        "network",
        "networks",
        "networking"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 48,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.448,
          "base_score": 0.448,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.403,
          "base_score": 0.403,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.384,
          "base_score": 0.384,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.352,
          "base_score": 0.352,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "host",
          "internet",
          "client",
          "socket",
          "host host"
        ],
        "semantic": [],
        "merged": [
          "host",
          "internet",
          "client",
          "socket",
          "host host"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.26341366021582713,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.714867+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 961-980)",
      "start_page": 961,
      "end_page": 980,
      "summary": "Socket address structs\nGiven host and service (the two components of a socket address), getaddrinfo\nwhich points to a socket address structure that corresponds to host and service\nAfter a client calls getaddrinfo, it walks this list, trying each socket address\nin turn until the calls to socket and connect succeed and the connection is\nSimilarly, a server tries each socket address on the list until the calls\nprovides ﬁner control over the list of socket addresses that getaddrinfo re-\n. By default, getaddrinfo can return both IPv4 and IPv6 socket addresses.\nSetting ai_family to AF_INET restricts the list to IPv4 addresses.\n/* Ptr to socket address structure */\nfunction can return up to three addrinfo structures, each with a different ai_\none whose socket address can be used as the end point of a connection.\nIt asks getaddrinfo to return IPv4 addresses only if the\nBy default, getaddrinfo returns socket addresses that can\nbe used by clients as active sockets in calls to connect.\ninstructs it to return socket addresses that can be used by servers as\nThe address ﬁeld in the resulting socket address structure(s) will be\nthe wildcard address, which tells the kernel that this server will accept\nrequests to any of the IP addresses for this host.\nThe ai_addr ﬁeld points to a socket address\nstructure, the ai_addrlen ﬁeld gives the size of this socket address structure, and\nthe ai_next ﬁeld points to the next addrinfo structure in the list.\nThis powerful property allows us to write clients and servers that are\nThe sa argument points to a socket address structure of size salen bytes, host\nThe getnameinfo function converts the socket address structure sa to the corre-\nhints.ai_socktype = SOCK_STREAM; /* Connections only */\nGetnameinfo(p->ai_addr, p->ai_addrlen, buf, MAXLINE, NULL, 0, flags);\nFigure 11.17 shows a simple program, called hostinfo, that uses getaddrinfo\nthat can be used as end points of connections (line 17).\nlevel helper functions, called open_clientfd and open_listenfd, that clients and\nA client establishes a connection with a server by calling open_clientfd.\nThe open_clientfd function establishes a connection with a server running on\nhost hostname and listening for connection requests on port number port.\nreturns an open socket descriptor that is ready for input and output using the\nWe call getaddrinfo, which returns a list of addrinfo structures, each of\nwhich points to a socket address structure that is suitable for establishing a con-\n/* Get a list of potential server addresses */\nif ((clientfd = socket(p->ai_family, p->ai_socktype, p->ai_protocol)) < 0)\n/* Connect to the server */\nif (connect(clientfd, p->ai_addr, p->ai_addrlen) != -1)\nopen_clientfd: Helper function that establishes a connection with a server.\nnection with a server running on hostname and listening on port.\nthe list, trying each list entry in turn, until the calls to socket and connect suc-\nIf the connect fails, we are careful to close the socket descriptor before\nThe arguments to socket and connect are generated for us automat-\nA server creates a listening descriptor that is ready to receive connection requests\nint open_listenfd(char *port);\nThe open_listenfd function returns a listening descriptor that is ready to receive\nconnection requests on portport.\nwe use the setsockopt function (not described here) to conﬁgure the server so\nBy default, a restarted server will deny connection requests from\nhost argument, the address ﬁeld in each socket address structure is set to the\nwildcard address, which tells the kernel that this server will accept requests to any\nExample Echo Client and Server\nAfter establishing a connection with the server,\nthe client enters a loop that repeatedly reads a text line from standard input, sends\nthe text line to the server, reads the echo line from the server, and prints the result\nEOF notiﬁcation being sent to the server, which it detects when it receives a return\nFigure 11.21 shows the main routine for the echo server.\nrequest from a client, prints the domain name and port of the connected client, and\nthen calls the echo function that services the client.\nint open_listenfd(char *port)\n/* Get a list of potential server addresses */\nif ((listenfd = socket(p->ai_family, p->ai_socktype, p->ai_protocol)) < 0)\n/* Make it a listening socket ready to accept connection requests */\nopen_listenfd: Helper function that opens and returns a listening descriptor.\nOnce the client and server have\nThe clientaddr variable in line 9 is a socket address structure that is passed\nBefore accept returns, it ﬁlls in clientaddr with the socket address of\nthe client on the other end of the connection.\nNotice that our simple echo server can only handle one client at a time.\nA server of this type that iterates through clients, one at a time, is called an iterative\nserver.\nservers that can handle multiple clients simultaneously.\nreads and writes lines of text until the rio_readlineb function encounters EOF\nprintf(\"Connected to (%s, %s)\\n\", client_hostname, client_port);\nprintf(\"server received %d bytes\\n\", (int)n);\nreturn code from the read function.\nWeb Servers\nserver.\nprogramming to build your own small, but quite functional, Web server.\nWeb clients and servers interact using a text-based application-level protocol\nclient (known as a browser) opens an Internet connection to a server and requests\nThe server responds with the requested content and then closes the\nserver.\ncorresponding HTML ﬁle from the CMU server and displays it.\nTo Web clients and servers, content is a sequence of bytes with an associated MIME\nWeb servers provide content to clients in two different ways:\n. Fetch a disk ﬁle and return its contents to the client.\nas static content and the process of returning the ﬁle to the client is known as\n. Run an executable ﬁle and return its output to the client.\nrunning the program and returning its output to the client is known as serving\nEvery piece of content returned by a Web server is associated with some ﬁle\nthat is managed by a Web server listening on port 80.\nClients and servers use different parts of the URL\nFor example, a server might be conﬁgured so that all static content is stored\nThe browser appends the missing ‘/’ to the URL and passes it to the server,\nSince HTTP is based on text lines transmitted over Internet connections, we can\nuse the Linux telnet program to conduct transactions with any Web server on\nlogin tool, but it is very handy for debugging servers that talk to clients with text\nlines over connections.\npage from the AOL Web server.\nClient: open connection to server\nClient: request line\nClient: empty line terminates headers\nServer: response line\nServer: followed by five response headers\nServer:\nContent-Type: text/html\nServer: expect HTML in the response body\nServer: expect 42,092 bytes in the response body\nServer: empty line terminates response headers\nServer: first HTML line in response body\nServer: 766 lines of HTML not shown\nServer: last HTML line in response body\nServer: closes connection\nClient: closes connection and terminates\nExample of an HTTP transaction that serves static content.\nIn line 1, we run telnet from a Linux shell and ask it to open a connection to\nthe AOL Web server.\nthe connection, and then waits for us to enter text (line 5).\nand line feed characters (‘\\r\\n’ in C notation), and sends the line to the server.\nenter an HTTP request (lines 5–7).\nThe server replies with an HTTP response\n(lines 8–17) and then closes the connection (line 18).\nHTTP Requests\nAn HTTP request consists of a request line (line 5), followed by zero or more\nrequest headers (line 6), followed by an empty text line that terminates the list of\ninstructs the server to generate and return the content identiﬁed by the URI\nThe version ﬁeld in the request line indicates the HTTP version to which the\nas a mechanism that allows a client and server to perform multiple transactions\nbecause HTTP/1.0 clients and servers simply ignore unknown HTTP/1.1 headers.\nTo summarize, the request line in line 5 asks the server to fetch and return\nrequest will be in HTTP/1.1 format.\nRequest headers provide additional information to the server, such as the\na browser and the origin server that manages the requested ﬁle.\ncan exist between a client and an origin server in a so-called proxy chain.\nin the Host header, which identiﬁes the domain name of the origin server, allow a\nthe server to send the requested HTML ﬁle.\nHTTP responses are similar to HTTP requests.\nIf a proxy server requests content, then\nPassing arguments in HTTP POST requests\nArguments for HTTP POST requests are passed in the request body rather than in the URI.\nRequest could not be understood by the server.\nServer lacks permission to access the requested ﬁle.\nServer could not ﬁnd the requested ﬁle.\nServer does not support the request method.\nServer does not support version in request.\n(line 12), which tells the client the MIME type of the content in the response body,\nby the response body, which contains the requested content.\nIf we stop to think for a moment how a server might provide dynamic content\nprogram arguments to the server?\nHow does the server pass these arguments\nHow Does the Client Pass Program Arguments to the Server?\nHow Does the Server Pass Arguments to the Child?\nAfter a server receives a request such as\nSERVER_PORT\nA CGI program sends its dynamic content to the standard output.\nchild process loads and runs the CGI program, it uses the Linux dup2 function\nContent-length response headers, as well as the empty line that terminates the\nreturns an HTML ﬁle with the result to the client.\nAssume that a CGI program needs to send dynamic content to the client.\ntypically done by making the CGI program send its content to the standard output.\nPassing arguments in HTTP POST requests to CGI programs\nFor POST requests, the child would also need to redirect standard input to the connected descriptor.\nThe CGI program would then read the arguments in the request body from standard input.\nprintf(\"Content-type: text/html\\r\\n\\r\\n\");\nClient: open connection\nClient: request line\nClient: empty line terminates headers\nServer: response line\nServer: Tiny Web Server\nContent-type: text/html\nAdder: third HTML line in response body\nServer: closes connection\nClient: closes connection and terminates\nAn HTTP transaction that serves dynamic HTML content.\nPutting It Together: The Tiny Web Server\nfunctioning Web server called Tiny.\nthe sockets interface, and HTTP, in only 250 lines of code.\nfor connection requests on the port that is passed in the command line.\nopening a listening socket by calling the open_listenfd function, Tiny executes\nthe typical inﬁnite server loop, repeatedly accepting a connection request (line 32),\nperforming a transaction (line 36), and closing its end of the connection (line 37).\nreadlineb function from Figure 10.8 to read the request line.\nIf the client requests another method\n* tiny.c - A simple, iterative HTTP/1.0 Web server that uses the\nprintf(\"Accepted connection from (%s, %s)\\n\", hostname, port);\nThe Tiny Web server.",
      "keywords": [
        "server",
        "Web server",
        "Socket address",
        "line",
        "socket address structure",
        "Socket",
        "client",
        "HTTP POST requests",
        "Tiny Web Server",
        "Web",
        "content",
        "address",
        "HTML",
        "connection",
        "HTML line"
      ],
      "concepts": [
        "server",
        "servers",
        "line",
        "lines",
        "content",
        "contents",
        "clients",
        "char",
        "web",
        "function"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 47,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.404,
          "base_score": 0.404,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.389,
          "base_score": 0.389,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 50,
          "title": "",
          "score": 0.389,
          "base_score": 0.389,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.362,
          "base_score": 0.362,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "server",
          "socket",
          "client",
          "connection",
          "content"
        ],
        "semantic": [],
        "merged": [
          "server",
          "socket",
          "client",
          "connection",
          "content"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2732801086767686,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.714910+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 981-999)",
      "start_page": 981,
      "end_page": 999,
      "summary": "/* Read request line and headers */\nprintf(\"Request headers:\\n\");\nserve_static(fd, filename, sbuf.st_size);\n\"Tiny couldn’t run the CGI program\");\nserve_dynamic(fd, filename, cgiargs);\nstring, and we set a ﬂag that indicates whether the request is for static or dynamic\ncontent (line 23).\nFinally, if the request is for static content, we verify that the ﬁle is a regular\n(line 36) to the client.\nSimilarly, if the request is for dynamic content, we verify\nthat the ﬁle is executable (line 39), and, if so, we go ahead and serve the dynamic\ncontent (line 44).\nTiny lacks many of the error-handling features of a real server.\nfunction in Figure 11.31 sends an HTTP response to the client with the appropriate\nsprintf(body, \"%s<hr><em>The Tiny Web server</em>\\r\\n\", body);\nsprintf(buf, \"HTTP/1.0 %s %s\\r\\n\", errnum, shortmsg);\nsprintf(buf, \"Content-type: text/html\\r\\n\");\nsprintf(buf, \"Content-length: %d\\r\\n\\r\\n\", (int)strlen(body));\nTiny clienterror sends an error message to the client.\nTiny read_requesthdrs reads and ignores request headers.\nstatus code and status message in the response line, along with an HTML ﬁle in\nTiny does not use any of the information in the request headers.\nthat the empty text line that terminates the request headers consists of a carriage\nstring cgi-bin is assumed to denote a request for dynamic content.\nfor static content (line 5), we clear the CGI argument string (line 6) and then\nOn the other hand, if the request is for dynamic content (line 13), we\nint parse_uri(char *uri, char *filename, char *cgiargs)\nTiny serves ﬁve common types of static content: HTML ﬁles, unformatted text\nThe serve_static function in Figure 11.34 sends an HTTP response whose\nresponse headers to the client (lines 8–13).\nNext, we send the response body by copying the contents of the requested ﬁle\nto the connected descriptor fd.\nLine 18 opens filename for reading and gets its descriptor.\nline 19, the Linux mmap function maps the requested ﬁle to a virtual memory area.\nvoid serve_static(int fd, char *filename, int filesize)\n/* Send response headers to client */\nsprintf(buf, \"HTTP/1.0 200 OK\\r\\n\");\nsprintf(buf, \"%sServer: Tiny Web Server\\r\\n\", buf);\nsprintf(buf, \"%sContent-type: %s\\r\\n\\r\\n\", buf, filetype);\n/* Send response body to client */\nTiny serve_static serves static content to a client.\nso we close the ﬁle (line 20).\nLine 21 performs the actual transfer of the ﬁle to the client.\nof course is mapped to the requested ﬁle) to the client’s connected descriptor.\nTiny serves any type of dynamic content by forking a child process and then\nrunning a CGI program in the context of the child.\nThe serve_dynamic function in Figure 11.35 begins by sending a response line\nindicating success to the client, along with an informational Server header.\nAfter sending the ﬁrst part of the response, we fork a new child process\nthe CGI arguments from the request URI (line 13).\nvoid serve_dynamic(int fd, char *filename, char *cgiargs)\nsprintf(buf, \"HTTP/1.0 200 OK\\r\\n\");\nsprintf(buf, \"Server: Tiny Web Server\\r\\n\");\n/* Real server would set all CGI vars here */\nExecve(filename, emptylist, environ); /* Run CGI program */\nTiny serve_dynamic serves dynamic content to a client.\nAlthough the basic functions of a Web server are quite simple, we don’t want to give you the false\nFor example, if a server writes to a connection that has already\nline is that a robust server must catch these SIGPIPE signals and check write function calls for EPIPE\nNext, the child redirects the child’s standard output to the connected ﬁle\ndescriptor (line 14) and then loads and runs the CGI program (line 15).\nthe CGI program runs in the context of the child, it has access to the same open\nthe client process, without any intervention from the parent process.\nEvery network application is based on the client-server model.\nan application consists of a server and one or more clients.\nThe basic operation in the client-server model is a client-server transaction,\nwhich consists of a request from a client, followed by a response from the server.\nClients and servers communicate over a global network known as the Internet.\nClients and servers establish connections by using the sockets interface.\nClients and servers communicate with each other by\nWeb servers and their clients (such as browsers) communicate with each other\nA browser requests either static or dynamic content\nfrom the server.\nA request for static content is served by fetching a ﬁle from the\nserver’s disk and returning it to the client.\nA request for dynamic content is served\nby running a program in the context of a child process on the server and returning\nthe client passes program arguments to the server, how the server passes these\narguments and other information to the child process, and how the child sends\nA simple but functioning Web server that serves\nboth static and dynamic content can be implemented in a few hundred lines of\nA. Modify Tiny so that it echoes every request line and request header.\nB. Use your favorite browser to make a request to Tiny for static content.\neach header in the HTTP request from your browser.\nModify Tiny so that when it serves static content, it copies the requested ﬁle to the\nA. Write an HTML form for the CGI adder function in Figure 11.27.\nYour form should request content using the GET method.\nB. Check your work by using a real browser to request the form from Tiny,\nExtend Tiny so that it serves dynamic content requested by the HTTP POST\nBefore the process that runs the CGI program is loaded, a Linux dup2 function\nwith the client.\nConcurrent Programming\nConcurrent Programming with Processes\nConcurrent Programming with I/O Multiplexing\nConcurrent Programming with Threads\nating system kernel uses to run multiple application programs.\nsimilar way by overlapping useful work with I/O requests.\n. Servicing multiple network clients.The iterative network servers that we stud-\nreal server that might be expected to service hundreds or thousands of clients\nA better approach is to build a concurrent server that creates a separate\nThis allows the server to service multiple clients\nconcurrently and precludes slow clients from monopolizing the server.\nconcurrent programs:\nSince the program is a single process, all ﬂows share the same address space.\nThreads are logical ﬂows that run in the context of a single process\napplication throughout—a concurrent version of the iterative echo server from\nConcurrent Programming with Processes\nThe simplest way to build a concurrent program is with processes, using familiar\nbuilding a concurrent server is to accept client connection requests in the parent\nand then create a new child process to service each new client.\nTo see how this might work, suppose we have two clients and a server that is\nlistening for connection requests on a listening descriptor (say, 3).\nthat the server accepts a connection request from client 1 and returns a connected\nAfter accepting the connection request,\nthe server forks a child, which gets a complete copy of the server’s descriptor table.\nshown in Figure 12.2, where the child process is busy servicing the client.\nSince the connected descriptors in the parent and child each point to the\nsame ﬁle table entry, it is crucial for the parent to close its copy of the connected\nconnection request from\nclient.\nClient 1\nClient 2\nServer\nchild process to service\nthe client.\nClient 1\nClient 2\nServer\nClient 1\nClient 2\nOtherwise, the ﬁle table entry for connected descriptor 4 will never\nNow suppose that after the parent creates the child for client 1, it accepts\na new connection request from client 2 and returns a new connected descriptor\nservicing its client using connected descriptor 5, as shown in Figure 12.4.\npoint, the parent is waiting for the next connection request and the two children\nare servicing their respective clients concurrently.\nA Concurrent Server Based on Processes\nFigure 12.5 shows the code for a concurrent echo server based on processes.\nThe echo function called in line 29 comes from Figure 11.22.\nClient 1\nClient 2\nportant for the parent, which must close its copy of the connected descriptor\nconnection to the client will not be terminated until both the parent’s and\nFigure 12.5 demonstrates a concurrent server in which the parent process creates\na child process to handle each new connection request.\nIf we were to delete line 33 of Figure12.5, which closes the connected descriptor,\n/* Child services client */\n/* Child closes connection with client */\nConcurrent echo server based on processes.\nconnection request.\nfrom Chapter 8 are primitive IPC mechanisms that allow processes to send tiny messages to processes\nConcurrent Programming with I/O Multiplexing\nrespond to two independent I/O events: (1) a network client making a connection\nrequest, and (2) a user typing a command line at the keyboard.\nidea is to use the select function to ask the kernel to suspend the process, return-\nint select(int n, fd_set *fdset, NULL, NULL, NULL);",
      "keywords": [
        "tiny",
        "Tiny Web server",
        "buf",
        "server",
        "client",
        "request",
        "CGI program",
        "child",
        "line",
        "CGI",
        "Web server",
        "connection request",
        "uri",
        "dynamic content",
        "MAXLINE"
      ],
      "concepts": [
        "lines",
        "server",
        "servers",
        "client",
        "clients",
        "function",
        "functions",
        "functioning",
        "tiny",
        "concurrent"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 54,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.409,
          "base_score": 0.409,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.395,
          "base_score": 0.395,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "",
          "score": 0.38,
          "base_score": 0.38,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.376,
          "base_score": 0.376,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "client",
          "server",
          "content",
          "request",
          "tiny"
        ],
        "semantic": [],
        "merged": [
          "client",
          "server",
          "content",
          "request",
          "tiny"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2598323236101398,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.714970+00:00"
      }
    },
    {
      "chapter_number": 50,
      "title": "Segment 50 (pages 1000-1022)",
      "start_page": 1000,
      "end_page": 1022,
      "summary": "least one descriptor in the read set is ready for reading.\nto indicate a subset of the read set called the ready set, consisting of the descriptors\nlistenfd function from Figure 11.19 to open a listening descriptor (line 16), and\nthen using FD_ZERO to create an empty read set (line 18):\nNext, in lines 19 and 20, we deﬁne the read set to consist of descriptor 0 (standard\nready for reading, the server executes the transition for the corresponding state\nserver opens the connection (line 37) and calls the add_client function to add the\necho a single text line from each ready connected descriptor (line 42).\nInitially, the set of connected descriptors is empty (lines 5–7), and\nthe listening descriptor is the only descriptor in the select read set (lines 10–12).\nthe connected descriptor to the select read set (line 12), and we update some\nfd_set ready_set; /* Subset of descriptors ready for reading\n/* Echo a text line from each ready connected descriptor */\n/* If the descriptor is ready, echo a text line from it */\ninput events, and the add_client function creates a new logical ﬂow (state ma-\nsigniﬁcant performance advantage compared to processes and threads.\nConcurrent Programming with Threads\nA thread is a logical ﬂow that runs in the context of a process.\nin this book, our programs have consisted of a single thread per process.\nmodern systems also allow us to write programs that have multiple threads running\nThe threads are scheduled automatically by the\nAll threads running in a process share the entire virtual address\nLogical ﬂows based on threads combine qualities of ﬂows based on processes\nLike processes, threads are scheduled automatically by the\nmultiplexing, multiple threads run in the context of a single process, and thus they\nThread Execution Model\nAt some point, the main thread\ncreates a peer thread, and from this point in time the two threads run concurrently.\nEventually, control passes to the peer thread via a context switch, either because\nthe main thread executes a slow system call such as read or sleep or because it\nThe peer thread executes for a while\nbefore control passes back to the main thread, and so on.\nThread execution differs from processes in some important ways.\nThe threads associated\nConcurrent thread\nThread 1\n(main thread)\nThread 2\n(peer thread)\nThread context switch\nThread context switch\nThread context switch\nwith a process form a pool of peers, independent of which threads were created\nby which other threads.\nin the sense that it is always the ﬁrst thread to run in the process.\nof this notion of a pool of peers is that a thread can kill any of its peers or wait\nPosix Threads\nPosix threads (Pthreads) is a standard interface for manipulating threads from C\ndeﬁnes about 60 functions that allow programs to create, kill, and reap threads,\nto share data safely with peer threads, and to notify peers about changes in the\nThe main thread creates a peer\nthread and then waits for it to terminate.\nThe peer thread prints Hello, world!\\n\nWhen the main thread detects that the peer thread has terminated,\nThis is the ﬁrst threaded program we\nThe code and local data for a thread are\nencapsulated in a thread routine.\nAs shown by the prototype in line 2, each thread\nvoid *thread(void *vargp);\nPthread_create(&tid, NULL, thread, NULL);\nLine 4 marks the beginning of the code for the main thread.\nThe main thread\ndeclares a single local variable tid, which will be used to store the thread ID of\nthe peer thread (line 6).\nThe main thread creates a new peer thread by calling the\nmain thread and the newly created peer thread are running concurrently, and tid\ncontains the ID of the new thread.\nThe main thread waits for the peer thread to\nFinally, the main thread calls\nexit (line 9), which terminates all threads (in this case, just the main thread)\nLines 12–16 deﬁne the thread routine for the peer thread.\nstring and then terminates the peer thread by executing the return statement in\nCreating Threads\nThreads create other threads by calling the pthread_create function.\nThe pthread_create function creates a new thread and runs the thread routine f\nin the context of the new thread and with an input argument of arg.\ncreated thread.\nReturns: thread ID of caller\nTerminating Threads\nA thread terminates in one of the following ways:\n. The thread terminates explicitly by calling the pthread_exit function.\nthe main thread calls pthread_exit, it waits for all other peer threads to\nterminate and then terminates the main thread and the entire process with\na return value of thread_return.\nvoid pthread_exit(void *thread_return);\n. Some peer thread calls the Linux exit function, which terminates the process\nand all threads associated with the process.\n. Another peer thread terminates the current thread by calling the pthread_\ncancel function with the ID of the current thread.\nReaping Terminated Threads\nThreads wait for other threads to terminate by calling the pthread_join function.\nint pthread_join(pthread_t tid, void **thread_return);\nThe pthread_join function blocks until thread tid terminates, assigns the generic\n(void *) pointer returned by the thread routine to the location pointed to by\nthread_return, and then reaps any memory resources held by the terminated\nthread.\nonly wait for a speciﬁc thread to terminate.\njoin to wait for an arbitrary thread to terminate.\nDetaching Threads\nAt any point in time, a thread is joinable or detached.\nA joinable thread can be\nIn contrast, a detached thread cannot\nBy default, threads are created joinable.\nThe pthread_detach function detaches the joinable thread tid.\nThreads can\nAlthough some of our examples will use joinable threads, there are good rea-\nsons to use detached threads in real programs.\nWeb server might create a new peer thread each time it receives a connection re-\nplicitly wait for each peer thread to terminate.\nIn this case, each peer thread should\nInitializing Threads\nthread routine.\nmultiple threads.\nA Concurrent Server Based on Threads\nFigure 12.14 shows the code for a concurrent echo server based on threads.\nThe main thread repeat-\nedly waits for a connection request and then creates a peer thread to handle the\nvoid *thread(void *vargp);\nPthread_create(&tid, NULL, thread, connfdp);\n/* Thread routine */\nvoid *thread(void *vargp)\nConcurrent echo server based on threads.\nthe connected descriptor to the peer thread when we call pthread_create.\nPthread_create(&tid, NULL, thread, &connfd);\nThen we have the peer thread dereference the pointer and assign it to a local\nvoid *thread(void *vargp) {\nvariable in the peer thread gets the correct descriptor value.\nthread gets the descriptor number of the next connection.\nthat two threads are now performing input and output on the same descriptor.\nAnother issue is avoiding memory leaks in the thread routine.\ncareful to free the memory block that was allocated by the main thread (line 32).\nIn the threads-\nShared Variables in Threaded Programs\nease with which multiple threads can share the same program variables.\nIn order to write correctly threaded programs, we must\nmemory model for threads?\nvoid *thread(void *vargp);\nPthread_create(&tid, NULL, thread, (void *)i);\nvoid *thread(void *vargp)\nThe variable is shared if and only if multiple threads reference some\nexample program consists of a main thread that creates two peer threads.\nmain thread passes a unique ID to each peer thread, which uses the ID to print\nthread routine has been invoked.\nThreads Memory Model\nA pool of concurrent threads runs in the context of a process.\nEach thread has\nthread shares the rest of the process context with the other threads.\nThe threads\nIn an operational sense, it is impossible for one thread to read or write the\nregister values of another thread.\nIf some thread modiﬁes a memory location,\nthen every other thread will eventually see the change if it reads that location.\nThe memory model for the separate thread stacks is not as clean.\nline 26, where the peer threads reference the contents of the main thread’s stack\nVariables in threaded C programs are mapped to virtual memory according to\none instance of each global variable that can be referenced by any thread.\nFor example, the global ptr variable declared in line 5 has one run-time\nAt run time, each thread’s\non the stack of the main thread.\none instance on the stack of peer thread 0 and the other on the stack of\npeer thread 1.\nthread in our example program declares cnt in line 25, at run time there is\nEach peer thread reads and writes this instance.\nby more than one thread.\nboth peer threads.\ninstances is referenced by exactly one thread.\nlocal stack for thread t, where t is either m (main thread), p0 (peer thread 0),\nor p1 (peer thread 1).\nmain thread?\npeer thread 0?\npeer thread 1?\nSynchronizing Threads with Semaphores\ncreates two threads, each of which increments a global shared counter variable\nvoid *thread(void *vargp);\n/* Thread routine prototype */\n/* Create threads and wait for them to finish */\nPthread_create(&tid1, NULL, thread, &niters);\nPthread_create(&tid2, NULL, thread, &niters);\n/* Thread routine */\nvoid *thread(void *vargp)\nWhen the two peer threads in badcnt.c run concurrently on a uniprocessor,\nstructions in the two threads.\nC code for thread i\nAsm code for thread i\nThread\nThread\nthe operating system will choose a correct ordering for your threads.\nAfter each thread has updated the shared variable cnt, its value in memory is 2,\nThe problem occurs because thread 2 loads cnt in step 5, after thread 1\nloads cnt in step 2 but before thread 1 stores its updated value in step 6.\nThread\nThread\nA progress graph models the execution of n concurrent threads as a trajectory\nof thread k.\n, In) represents the state where thread k (k =\ninitial state where none of the threads has yet completed an instruction.\nPoint (L1, S2) corresponds to the state where thread\n1 has completed L1 and thread 2 has completed S2.\nLegal transitions move to the right (an instruction in thread 1\ncompletes) or up (an instruction in thread 2 completes).\nbadcnt.c. Thread 2\nThread 1\nThread 2\nThread 1\ncnt) that should not be interleaved with the critical section of the other thread.\nguarantee correct execution of our example threaded program—and indeed any\nchronize the threads so that they always have a safe trajectory.",
      "keywords": [
        "thread",
        "peer thread",
        "threads",
        "main thread",
        "descriptor",
        "pthread",
        "thread routine",
        "read set",
        "line",
        "read",
        "variable",
        "server",
        "NULL",
        "peer",
        "ready"
      ],
      "concepts": [
        "threads",
        "thread",
        "threaded",
        "line",
        "lines",
        "void",
        "server",
        "servers",
        "function",
        "functions"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.593,
          "base_score": 0.443,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.585,
          "base_score": 0.435,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 22,
          "title": "",
          "score": 0.565,
          "base_score": 0.415,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 21,
          "title": "",
          "score": 0.515,
          "base_score": 0.365,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 26,
          "title": "",
          "score": 0.492,
          "base_score": 0.342,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "threads",
          "peer",
          "peer thread",
          "main thread"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "threads",
          "peer",
          "peer thread",
          "main thread"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31097449123206605,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.715029+00:00"
      }
    },
    {
      "chapter_number": 51,
      "title": "Segment 51 (pages 1023-1040)",
      "start_page": 1023,
      "end_page": 1040,
      "summary": "Thread 2\nThread 1\nto the problem of synchronizing different execution threads based on a special\nIf there are any threads blocked at a P\nexactly one of these threads, which then completes its P operation by\nnition of V does not deﬁne the order in which waiting threads are restarted.\nonly requirement is that the V must restart exactly one waiting thread.\nseveral threads are waiting at a semaphore, you cannot predict which one will be\nThe deﬁnitions of P and V ensure that a running program can never enter a\nPrograms perform P and V operations by calling the sem_wait and\nThread 2\nThread 1\ncorresponding critical section with P(s) and V (s) operations.\nA semaphore that is used in this way to protect shared variables is called a\nP operation on a mutex is called locking the mutex.\nV operation is called unlocking the mutex.\nA thread that has locked but not yet\nIn an operational sense, the forbidden region created by the P and V op-\nin Figure 12.16 using semaphores, we ﬁrst declare a semaphore called mutex:\nFinally, we protect the update of the shared cnt variable in the thread routine by\nP(&mutex);\nWhen we run the properly synchronized program, it now produces the correct\nIn this scenario, a thread uses a semaphore\nthread\nthread\nThe consumer removes items from the buffer and then\noperation to notify another thread that some condition in the program state has\nsumer thread share a bounded buffer with n slots.\nThe producer thread repeatedly\nproduces new items and inserts them in the buffer.\nThe consumer thread repeat-\nedly removes items from the buffer and then consumes (uses) them.\nSince inserting and removing items involves updating shared variables, we\nbuffer is full (there are no empty slots), then the producer must wait until a slot\nSimilarly, if the buffer is empty (there are no available items),\nthen the consumer must wait until an item becomes available.\nbounded buffers of type sbuf_t (Figure 12.24).\nThe mutex semaphore provides mutually exclusive buffer access.\nSemaphores slots and items are counting semaphores that count the number of\nan empty buffer, and assigns initial values to the three semaphores.\nsbuf_insert function waits for an available slot, locks the mutex, adds the item,\nunlocks the mutex, and then announces the availability of a new item.\nAfter waiting for an available buffer item, it locks\nthe mutex, removes the item from the front of the buffer, unlocks the mutex, and\nLet p denote the number of producers, c the number of consumers, and n the\nthe mutex semaphore in sbuf_insert and sbuf_remove is necessary or not.\nA collection of concurrent threads is accessing a shared object such as a data\nSome threads only read the\nThreads that modify the object are called writers.\nThreads that only read it are called readers.\n/* Create an empty, bounded, shared FIFO buffer with n slots */\nvoid sbuf_init(sbuf_t *sp, int n)\n/* Buffer holds max of n items */\nSem_init(&sp->mutex, 0, 1);\nSem_init(&sp->slots, 0, n);\n/* Insert item onto the rear of shared buffer sp */\nvoid sbuf_insert(sbuf_t *sp, int item)\nP(&sp->mutex);\nsp->buf[(++sp->rear)%(sp->n)] = item;\nV(&sp->mutex);\nV(&sp->items);\n/* Remove and return the first item from buffer sp */\nP(&sp->items);\nP(&sp->mutex);\nitem = sp->buf[(++sp->front)%(sp->n)];\nV(&sp->mutex);\nSbuf: A package for synchronizing concurrent access to bounded buffers.\npages from the shared page cache, but any thread that writes a new page to the\nFigure 12.26 shows a solution to the ﬁrst readers-writers problem.\nThe w semaphore controls access to the critical sections that access the shared\nThe mutex semaphore protects access to the shared readcnt variable,\nThe solution to the ﬁrst readers-writers problem in Figure 12.26 gives priority to\nWe have seen how semaphores can be used to access shared variables and to\nP(&mutex);\nP(&mutex);\nIn the concurrent server in Figure 12.14, we created a new thread for each\nof creating a new thread for each new client.\nThe server consists of a main thread and a set of worker threads.\nThe main thread repeatedly accepts connection requests from clients and places\nWe have shown you how to synchronize threads using semaphores, mainly because they are simple, clas-\nFor example, Java threads are synchronized with a mechanism called a Java monitor [48],\ninterface deﬁnes a set of synchronization operations on mutex and condition variables.\nresources, such as the bounded buffer in a producer-consumer program.\nthread\nthread\nPool of worker threads\nthread\nthreads repeatedly remove and process connected descriptors from a bounded buffer.\nEach worker thread\nAfter initializing buffer sbuf (line 24), the\nmain thread creates the set of worker threads (lines 25–26).\nfrom thread routines.\nand the mutex semaphore.\npackages, is to require the main thread to explicitly call an initialization function.\nvoid *thread(void *vargp);\nsbuf_t sbuf; /* Shared buffer of connected descriptors */\n/* Create worker threads */\nPthread_create(&tid, NULL, thread, NULL);\nvoid *thread(void *vargp)\nint connfd = sbuf_remove(&sbuf); /* Remove connfd from buffer */\nP(&mutex);\nthe initialization function the ﬁrst time some thread calls the echo_cnt function.\nNotice that the accesses to the shared byte_cnt variable in lines 23–25\nEvent-driven programs based on threads\nwith simple state machines for the main and worker threads.\nThe main thread has two states (“waiting\nSimilarly, each worker thread has one state (“waiting for available buffer item”),\nUsing Threads for Parallelism\noperating system kernel schedules the concurrent threads in parallel on multi-\nA parallel program is a concurrent program running on multiple processors.\nthreads to work on its own region.\nthat multiple threads might work on their assigned regions in parallel.\nThe simplest and most straightforward option is to have the threads sum into\na shared global variable that is protected by a mutex.\nIn lines 28–33, the main thread creates the peer threads\nNotice that the main thread passes a small\nEach peer thread\nwill use its thread ID to determine which portion of the sequence it should work\nAfter the peer threads have\nThe main thread then\nFigure 12.32 shows the function that each peer thread executes.\nthread extracts the thread ID from the thread argument and then uses this ID to\nthe thread iterates over its portion of the sequence, updating the shared global\nwith P and V mutex operations.\nWhen we run psum-mutex on a system with four cores on a sequence of size\nn = 231 and measure its running time (in seconds) as a function of the number of\nNumber of threads\nthread, it is nearly an order of magnitude slower when it runs in parallel as\nmultiple threads.\npeer thread compute its partial sum in a private variable that is not shared with\nany other thread, as shown in Figure 12.33.\nThe main thread (not shown) deﬁnes\na global array called psum, and each peer thread i accumulates its partial sum in\nSince we are careful to give each peer thread a unique memory location\nnecessary synchronization is that the main thread must wait for all of the children\nAfter the peer threads have terminated, the main thread sums up the\nvoid *sum_mutex(void *vargp); /* Thread routine */\nlong nelems_per_thread;\nnelems_per_thread = nelems / nthreads;\n/* Create peer threads and wait for them to finish */\nUses multiple threads to sum the elements\nof a sequence into a shared global variable protected by a mutex.\n/* Thread routine for psum-mutex.c */\nlong start = myid * nelems_per_thread; /* Start element index */\nlong end = start + nelems_per_thread;\nP(&mutex);\nThread routine for psum-mutex.\nEach peer thread sums into a shared global variable protected\n/* Thread routine for psum-array.c */\nlong start = myid * nelems_per_thread; /* Start element index */\nlong end = start + nelems_per_thread;\nThread routine for psum-array.\nEach peer thread accumulates its partial sum in a private\narray element that is not shared with any other peer thread.\nNumber of threads\neach peer thread accumulate its partial sum into a local variable rather than\nNumber of threads\n/* Thread routine for psum-local.c */\nlong start = myid * nelems_per_thread; /* Start element index */\nlong end = start + nelems_per_thread;\nThread routine for psum-local.\nEach peer thread accumulates its partial sum in a local\nThreads\nFigure 12.35 plots the total elapsed running time of the psum-local program in\nFigure 12.34 as a function of the number of threads.\nWe see that running time decreases as we increase the number of threads,\ntime we double the number of threads.\nthe point (t > 4) where each of the four cores is busy running at least one thread.\nRunning time actually increases a bit as we increase the number of threads because\nof the overhead of context switching multiple threads on the same core.\nthread.",
      "keywords": [
        "Thread",
        "threads",
        "buffer",
        "mutex",
        "Sbuf",
        "main thread",
        "peer thread",
        "semaphore",
        "shared",
        "sem",
        "item",
        "concurrent",
        "programs",
        "program",
        "thread routine"
      ],
      "concepts": [
        "thread",
        "threads",
        "threaded",
        "buffer",
        "buffers",
        "buffered",
        "void",
        "items",
        "item",
        "programming"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.702,
          "base_score": 0.552,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 40,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.607,
          "base_score": 0.457,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.582,
          "base_score": 0.432,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "mutex",
          "threads",
          "sp",
          "peer"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "mutex",
          "threads",
          "sp",
          "peer"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31464555821102236,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.715077+00:00"
      }
    },
    {
      "chapter_number": 52,
      "title": "Segment 52 (pages 1041-1063)",
      "start_page": 1041,
      "end_page": 1063,
      "summary": "Threads (t)\ntime of a sequential version of the program, then Sp is called the absolute speedup.\nWhen T1 is the execution time of the parallel version of the program running on\nThreads (t)\ndiscussion in terms of threads.\nThread Safety\nWhen we program with threads, we must be careful to write functions that have a\nproperty called thread safety.\nA function is said to be thread-safe if and only if it will\nthreads.\nIf a function is not thread-safe, then we say it is thread-unsafe.\nWe can identify four (nondisjoint) classes of thread-unsafe functions:\nClass 1: Functions that do not protect shared variables.\ncountered this problem with the thread function in Figure 12.16, which\nA thread-unsafe pseudorandom number generator.\nThis class of thread-\nunsafe functions is relatively easy to make thread-safe: protect the shared\nnumber generator is a simple example of this class of thread-unsafe func-\nThe rand function is thread-unsafe because the result of the current\nWhen we call rand repeatedly from a single thread after seeding it with a\nthis assumption no longer holds if multiple threads are calling rand.\nThe only way to make a function such as rand thread-safe is to rewrite\nClass 3: Functions that return a pointer to a static variable.\nSome functions,\nIf we call such functions from\nThread-safe wrapper function for the C standard library ctime function.\nuses the lock-and-copy technique to call a class 3 thread-unsafe function.\nconcurrent threads, then disaster is likely, as results being used by one\nThere are two ways to deal with this class of thread-unsafe func-\nshared data, but it requires the programmer to have access to the function\nIf the thread-unsafe function is difﬁcult or impossible to modify (e.g.,\nassociate a mutex with the thread-unsafe function.\nthe mutex, call the thread-unsafe function, copy the result returned by\nTo minimize changes to the caller, you should deﬁne a thread-safe wrap-\nper function that performs the lock-and-copy and then replace all calls\nto the thread-unsafe function with calls to the wrapper.\nFigure 12.38 shows a thread-safe wrapper for ctime that uses the lock-\nClass 4: Functions that call thread-unsafe functions.If a function f calls a thread-\nunsafe function g, is f thread-unsafe?\nclass 1 or class 3 function, then f can still be thread-safe if you protect\nthread-safe function that calls a thread-unsafe function.\nthread-safe, and thread-\nunsafe functions.\nAll functions\nThread-safe\nfunctions\nThread-unsafe\nfunctions\nfunctions\nFigure 12.40 rand_r: A reentrant version of the rand function from Figure 12.37.\nThere is an important class of thread-safe functions, known as reentrant functions,\nwhen they are called by multiple threads.\nAlthough the terms thread-safe and\ntween reentrant, thread-safe, and thread-unsafe functions.\nis partitioned into the disjoint sets of thread-safe and thread-unsafe functions.\nset of reentrant functions is a proper subset of the thread-safe functions.\nReentrant functions are typically more efﬁcient than non-reentrant thread-\nsafe functions because they require no synchronization operations.\nthe only way to convert a class 2 thread-unsafe function into a thread-safe one is\nversion of the rand function from Figure 12.37.\nno references to static or global variables), then the function is explicitly reentrant,\nour otherwise explicitly reentrant function to be passed by reference (i.e., we\nallow them to pass pointers), then we have an implicitly reentrant function, in\nthe sense that it is only reentrant if the calling threads are careful to pass pointers\nFor example, the rand_r function in Figure 12.40 is implicitly\nThe rand_r function in Figure 12.40 is implicitly reentrant.\nUsing Existing Library Functions in Threaded Programs\n(such as malloc, free, realloc, printf, and scanf), are thread-safe, with only\nare obsolete network programming functions that have been replaced by the\na threaded program, the least disruptive approach to the caller is to lock and copy.\nSecond, functions that return\nwill not work for a class 2 thread-unsafe function such as rand that relies on static\nThread-unsafe function\nThread-unsafe class\nLinux thread-safe version\nCommon thread-unsafe library functions.\nTherefore, Linux systems provide reentrant versions of most thread-unsafe\nfunctions.\nA race occurs when the correctness of a program depends on one thread reaching\nsimple program in Figure 12.42.\nThe main thread creates four peer threads and\nEach peer thread copies the\nPthread_create(&tid[i], NULL, thread, &i);\n/* Thread routine */\nprintf(\"Hello from thread %d\\n\", myid);\nA program with a race.\nHello from thread 1\nHello from thread 3\nHello from thread 2\nHello from thread 3\nThe problem is caused by a race between each peer thread and the main\nthread.\nWhen the main thread\ncreates a peer thread in line 13, it passes a pointer to the local stack variable\nIf the peer thread\nexecutes line 22 before the main thread increments i in line 12, then the myid\nOtherwise, it will contain the ID of some other thread.\nkernel schedules the execution of the threads.\ninteger ID and pass the thread routine a pointer to this block, as shown in Fig-\nNotice that the thread routine must free the block in order\nHello from thread 0\nHello from thread 1\nHello from thread 2\nHello from thread 3\nately after line 14 in the main thread, instead of freeing it in the peer thread.\nor free functions.\nPthread_create(&tid[i], NULL, thread, ptr);\n/* Thread routine */\nprintf(\"Hello from thread %d\\n\", myid);\nA correct version of the program in Figure 12.42 without a race.\ndeadlock, where a collection of threads is blocked, waiting for a condition that\nFor example, Figure 12.44 shows the progress graph for a pair of threads\nThread 2\nThread 1\nProgress graph for a program that can deadlock.\nthread is waiting for the other to do a V operation that will never occur.\nOr the program\nThread 2\nThread 1\nProgress graph for a deadlock-free program.\ndeadlock-free if each thread acquires its mutexes in order and releases them in\nin each thread.\nThread 1:\nThread 2:\nD. Draw the progress graph for the resulting deadlock-free program.\nrent programs: processes, I/O multiplexing, and threads.\nBecause the program runs in a single process, sharing data between\nThreads are a hybrid of these approaches.\non processes, threads are scheduled automatically by the kernel.\non I/O multiplexing, threads run in the context of a single process, and thus can\nFunctions that are called\nfour classes of thread-unsafe functions, along with suggestions for making them\nthread-safe.\nReentrant functions are the proper subset of thread-safe functions\nthreads programming and its pitfalls.\nlibrary that simpliﬁes the design and implementation of threaded programs.\nthreads, where n is a command-line argument.\nA. The program in Figure 12.46 has a bug.\nThe thread is supposed to sleep for\ndifferent Pthreads function calls.\nPthread_create(&tid, NULL, thread, NULL);\n/* Thread routine */\nThe functions in the Rio I/O package (Section 10.5) are thread-safe.\nIn the prethreaded concurrent echo server in Figure 12.28, each thread calls the\necho_cnt function (Figure 12.29).\nIs echo_cnt thread-safe?\nUse the lock-and-copy technique to implement a thread-safe non-reentrant ver-\nHowever, if you try this approach in a concurrent server based on threads,\non whether or not the program deadlocks?\nThread 1\nThread 2\nThread 1\nThread 2\nThread 1\nThread 2\nThread 1\nThread 2\nCan the following program deadlock?\nThread 1:\nThread 2:\nConsider the following program that deadlocks.\nThread 1:\nThread 2:\nThread 3:\nB. If a < b < c, which threads violate the mutex lock ordering rule?\nC. For these threads, show a new lock ordering that guarantees freedom from\nImplement a version of the standard I/O fgets function, called tfgets, that times\nYour function should be implemented in a package called tfgets-\nTest your solution using the driver program in Figure 12.47.\nImplement a version of the tfgets function from Problem 12.31 that uses the\nYour function should be implemented in a package called\ntfgets-select.c. Test your solution using the driver program from Problem\nImplement a threaded version of the tfgets function from Problem 12.31.\nfunction should be implemented in a package called tfgets-thread.c. Test your\nsolution using the driver program from Problem 12.31.\nWrite a parallel threaded version of an N × M matrix multiplication kernel.\nImplement a concurrent version of the Tiny Web server based on threads.\nsolution should create a new thread for each new connection request.\ntion should dynamically increase or decrease the number of threads in response to\nOne strategy is to double the number of threads when the buffer\nbecomes full, and halve the number of threads when the buffer becomes empty.\nRecall that the echo function from Figure 11.22 echoes each line from the client\nit is important to free the memory block that was allocated by the main thread.\nsharing is limited to the functions within their scope—in this case, the thread\nmain thread?\npeer thread 0?\npeer thread 1?\nptr A global variable that is written by the main thread and read by the\npeer threads.\nwritten by the two peer threads.\ni.m A local automatic variable stored on the stack of the main thread.\nEven though its value is passed to the peer threads, the peer threads\nmsgs.m A local automatic variable stored on the main thread’s stack and\nreferenced indirectly through ptr by both peer threads.\nthe stacks of peer threads 0 and 1, respectively.\nB. Variables ptr, cnt, and msgs are referenced by more than one thread and\nordering that the kernel chooses when it schedules your threads.\nThread\nB. p = 1, c = 1, n = 1: No, the mutex semaphore is not necessary in this case,\nSo at any point in time, only a single thread can access\nSuppose that a particular semaphore implementation uses a LIFO stack of threads\nWhen a thread blocks on a semaphore in a P operation, its ID\nSimilarly, the V operation pops the top thread ID from\nthe stack and restarts that thread.\nThreads (t)\nThe rand_r function is implicitly reentrant function, because it passes the param-\nwe will introduce a new race, this time between the call to free in the main thread\nand the assignment statement in line 24 of the thread routine.\nPthread_create(&tid[i], NULL, thread, (void *)i);\nIn the thread routine, we cast the argument back to an int and assign it to\nB. The program always deadlocks, since any feasible trajectory is eventually\nD. The progress graph for the corrected program is shown in Figure 12.49.\nThread 2\nThread 1\nProgress graph for a program that deadlocks.\nThread 2\nThread 1\nProgress graph for the corrected deadlock-free program.\nThe wrapper calls the base function and checks for errors.",
      "keywords": [
        "Thread",
        "function",
        "functions",
        "problem",
        "Threads",
        "program",
        "thread-unsafe function",
        "main thread",
        "Reentrant functions",
        "solution",
        "peer threads",
        "reentrant",
        "Solution to Problem",
        "code",
        "Thread routine"
      ],
      "concepts": [
        "threads",
        "thread",
        "threaded",
        "functions",
        "function",
        "program",
        "programs",
        "programming",
        "deadlocks",
        "deadlock"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "",
          "score": 0.727,
          "base_score": 0.577,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 40,
          "title": "",
          "score": 0.518,
          "base_score": 0.368,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "",
          "score": 0.482,
          "base_score": 0.332,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "thread",
          "thread unsafe",
          "unsafe",
          "threads",
          "thread safe"
        ],
        "semantic": [],
        "merged": [
          "thread",
          "thread unsafe",
          "unsafe",
          "threads",
          "thread safe"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27242629186500794,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.715120+00:00"
      }
    },
    {
      "chapter_number": 53,
      "title": "Segment 53 (pages 1064-1083)",
      "start_page": 1064,
      "end_page": 1083,
      "summary": "encounters an error (e.g., there is no child process to reap), it returns −1 and sets\nUnix-style error-handling code is typically of the following form:\nSummary of Error-Reporting Functions\nError-reporting functions.\nWrapper for Unix-style wait function.\nSIGOPS Operating Systems Review 41(2):88–\n(SPAA), pages 355–366.\nAutomation Conference, pages 746–749.\npages 31–44.\n(SIGCSE), pages 90–94.\nAddison-Wesley, 1997.\nProgramming Languages and Systems 9(1):25–\nJ. B.\n(HPCA), pages 70–79.\npages 196–259.\nSurveys 26(2):145–185, June 1994.\nData, pages 235–246.\npages 1–12.\n3(2):67–78, June 1971.\nIEEE Computer 14(10):48–54, October 1981.\nP. J.\n14(10):667–668, 1971.\nand Expo (DISCEX), volume 2, pages 119–129,\nJ. H.\n10(1):27–36, February 1990.\nArchitecture (ISCA), pages 222–233, ACM,\nComputer Science, pages 26–31.\nand Implementation (PLDI), pages 229–241.\nM. W.\nPrivacy, pages 326–343.\npages 285–297.\npages 271–280.\npages 92–103.\nACM 43(11):37–45, November 2000.\nCommunications of the ACM 31(5):532–533,\nMorgan Kaufmann, 2011.\nMorgan Kaufmann,\nACM 17(10):549–557, October 1974.\n43–45, October 1992.\n(SOSP), pages 52–65.\nThe C Program-\nThe C Program-\n11:223–235, April 1962.\nOperating Systems (ASPLOS), pages 63–74.\nAlgorithmica 6(1–6),\nJ. R.\npages 157–168.\nConference on Supercomputing, pages 808–817.\nM. Matz, J.\nJ. Morris, M.\nCommunications of the ACM, pages 184–201,\nOperating Systems (ASPLOS), pages 62–73.\nMorgan Kaufmann, 1997.\nProceedings of VLDB, pages 970–983.\nData, pages 109–116.\nIEEE Security and Privacy 2(4):20–27, 2004.\nMorgan Kaufmann, 1990.\n35(8):102–114, August 1992.\nGrande, pages 89–98.\nTechnical Journal 63(6 Part 2):1577–1593,\nLanguages, pages 201–208.\n17(7):365–367, July 1974.\nM. Satyanarayanan, J.\non Computers 39(4):447–459, April 1990.\nSecurity and Privacy 7(1):14–17, January 2009.\nR. C.\npages 298–307.\n[100] J.\nArchitecture (HPCA), pages 168–179.\n[116] J.\n[117] M.\nNotes in Computer Science, pages 1–116.\n[119] M.\nImplementation (PLDI), pages 30–44, June\n[121] J.\n33:61–68, August 2000.\npages 451–461.\nC standard library function\nUnix program, function, variable, or constant\nx86-64 machine-language instruction\n727, 729–730\n561–567\n972–973\ndisks, 633–636\nIA32 registers, 215–216\nmain memory, 623–625\ndata movement, 218–225\noperand speciﬁers, 216–218\naccess time for disks, 629, 629–631\naccumulators, multiple, 572–577\nﬂoating point, 158–160, 338\ntwo’s complement, 126, 126–131\nunsigned, 120–126, 121\n651–652\n(ASLR), 321, 321–322\nvirtual, 840–841\nCore i7, 862–864\nend-to-end, 857–861\nmulti-level page tables, 855–857\noverview, 849–852\nTLBs for, 853–855\nbyte ordering, 78–85\nIP, 960, 961–963\nmachine-level programming, 206–\nvirtual, 839–840\nsegmented, 323–324\nsockets, 966, 969–970\nstructures, 301–303\nsymbol relocation, 726–727\ninstructions, 330, 582–583\nalgebra, Boolean, 86–89, 88\ndata, 309, 309–312\nfree list creation, 893–895\nfree list manipulation, 892–893\ngeneral design, 890–892\npractice problems, 897–898\nrequirements and goals, 880–881\nstyles, 875–876\n444–445\nAmdahl’s law, 58, 58–60, 598, 604\nBoolean, 87–88\nHCL expressions, 410–411\nlogical, 92–93\nshared libraries from, 737–739\nﬂoating-point, 329, 329–332\nWeb servers, 989–990\ndiscussion, 232–233\nﬂoating-point code, 338–340\nload effective address, 227–229\npointers, 293–294, 909\nshift operations, 94, 140–142, 228,\n230–232\nspecial, 233–236\nunary and binary, 230–232\n444–445\nbasic principles, 291–293\ndeclarations, 291–292, 299\nﬁxed-size, 296–298\nnested, 294–296\npointer arithmetic, 293–294\nvariable-size, 298–301\nrandomization), 321, 321–322\nwith C programs, 325–326\nformatting, 211–213\nassociative caches, 660–662\nﬂoating-point addition, 159–160\ncondition codes, 237–238\ninstructions, 312, 330, 582–583\nbackground processes, 789, 789–792\n906–907\nsynchronized program, 1031–\nbiased number encoding, 149, 149–153\n78–80\nbinary points, 146, 146–147\nwith hexadecimal, 72–73\nsigned and unsigned, 106–112\n108–109, 133\nto unsigned, 98–99\nfractional, 145–148\nbinary tree structure, 306–307\nbit-level operations, 90–92\nbit representation expansion, 112–116\nbit vectors, 87, 87–88\nunion access to, 307–308\nbitwise operations, 341–342\nblocked signals, 794, 795, 800–801\nsignals, 800–801\ncaches, 647, 647–648, 651, 669\ncoalescing, 886–887, 896\nfree lists, 883–885\nlogical disk, 631, 631–632, 637\nreferencing data in, 910–911\nsplitting, 885–886\nHCL, 410–411\nworking with, 86–89\nproﬁlers, 601–604\nprogram proﬁling, 598–600\nboundary tags, 887, 887–890, 895\nbounded buffers, 1040, 1041–\nmisprediction handling, 479–480\nperformance, 585–589\ncondition codes, 237–238\ncondition control, 245–249\nmoves, 250–256, 586–589\nswitch, 268–274\nbreakpoints, 315–316\nbubbles, pipeline, 470, 470–471,\n495–496\n325–326\noverview, 315–320\n322–325\nstack randomization for, 320–322\nbuffered I/O functions, 934–938\nbounded, 1040, 1041–1042\nread, 934, 936–937\nstore, 593–594\nbypassing for data hazards, 472–475\nbyte order, 78–85\nY86 encoding, 395–396\nbit-level operations, 90–92\n160–162\nlogical operations, 92–93\nshift operations, 93–95\nstatic libraries, 720–724\nobjects, 302–303\nsoftware exceptions, 759–760, 822\nC standard library, 40–41, 42\ncache-friendly code, 669–675, 670\ncache-friendly code, 669–675, 670\nfully associative, 663–664\nimportance, 47–50\nlocality in, 641, 679–683, 846\nmemory mountains, 675–679\nmisses, 506, 648, 648–649\norganization, 651–653\noverview, 646–648\npage faults, 844, 844–845\npage tables, 842–844, 843\nperformance, 569, 667–669, 675–683\npractice problems, 664–666\nset associative, 660, 660–662\nvirtual memory with, 841–847, 853\nwrite issues, 666–667\nY86-64 pipelining, 505–506\ncall [x86-64] procedure call, 277–278,\ncallee-save registers, 287, 287–288\ncaller-save registers, 287, 287–288\ncalloc function [C Stdlib] memory\nsecurity vulnerability, 136–137\ncalls, 53, 763–764\nerror handling, 773–774\nLinux/x86-64 systems, 766–767\nin performance, 548–549\ncanary values, 322–323\ndisks, 627, 627–628\nsigned values, 106–107\n45–46\nmulti-core, 52, 60–61, 204, 641, 1008\noverview, 388–390\ntrends, 638–639\nprogram, 989, 989–991\ncreating, 777–779\nerror conditions, 781–782\nreaping, 779, 779–785\nwaitpid function, 782–785\ncombinational, 410, 410–416\ncomputers), 397, 397–399\nexceptions, 762–764\nstorage, 1030–1031\nclient-server model, 954, 954–955\nfunction, 995–996\nclocked registers, 437–438\nclocking in logic design, 417–420\nclose [Unix] close ﬁle, 930, 930–931\nclose operations for ﬁles, 927, 930–931\nwith boundary tags, 887–890\nperformance strategies, 597–598\nproﬁlers, 598–600\nrepresenting, 85–86\nY86 instructions, 394, 395–396\ncode segments, 732, 733–734\nbasics, 902–903\nconservative, 903, 905–906\nMark&Sweep, 903–906\ncombinational circuits, 410, 410–416\ncombinational pipelines, 448–450,\n496–498\nprogram, 989, 989–991\npoint code, 342–345\ncompilation systems, 42, 42–43\ncompile-time interpositioning, 744–\ncompiler drivers, 40, 707–708\nlimitations, 534–538\nprocess, 205–206\n(CISC), 397, 397–399\n457–458\nﬂow synchronizing, 812–814\nthread-level, 60–62\nconcurrent ﬂow, 769, 769–770\nconcurrent programming, 1008–1009\ndeadlocks, 1063–1066\nwith I/O multiplexing, 1014–1021\nlibrary functions in, 1060–1061\nwith processes, 1009–1013\nraces, 1061–1063\nreentrancy issues, 1059–1060\nshared variables, 1028–1031\nthreads, 1021–1028\nfor parallelism, 1049–1054\nsafety issues, 1056–1058\nbased on prethreading, 1041–1049\nbased on processes, 1010–1011\nbased on threads, 1027–1028\nSEQ timing, 437–438\ncondition codes, 237, 237–238\naccessing, 238–241\nY86-64, 391–393\ncondition codes, 237–238\ncondition control, 245–249\nmoves, 250–256, 586–589\nswitch, 268–274\nconﬂict misses, 649, 658–660\nwith server, 970, 970–971\nconnected descriptors, 972, 972–973\nInternet, 961, 965–967\nI/O devices, 632–633\n905–906\nﬂoating-point code, 340–341\nfree lists, 892–893\nmultiplication, 137–139\nfor ranges, 103–104\ndynamic, 989–990\nWeb, 984, 985–986\ncontext switches, 52, 772–773\nlogical, 768, 768–769\n496–498\ncontrol mechanisms, 495–496\nimplementation, 498–500\nspecial cases, 491–493\nspecial conditions, 493–495\ncontrol structures, 236–237\ncondition codes, 236–241\nconditional branches, 245–249\n250–256\njumps, 241–245\nswitch statements, 268–274\ncontrol transfer, 277–281, 758\ndisk, 631, 631–632\nconventional DRAMs, 618–620\nwith hexadecimal, 72–73\nsigned and unsigned, 106–112\n108–109, 133\nto unsigned, 98–99\nﬂoating point, 161, 332–337\nlowercase, 545–547\nnumber systems, 72–75\nfunction, 973, 973–976\n976–978\ncopy_from_kernel function, 122–123\ncopy-on-write technique, 871, 871–872\nabstract operation model, 561–567\naddress translation, 862–864\npage table entries, 862–864\nvirtual memory, 861–864\ncorrect signal handling, 806–810\n540, 543–544\nin performance analysis, 500–504\n777–779\n540, 543–544\nin performance analysis, 500–504\nconditional transfers, 250–256\nforwarding, 472–475, 473\nsizes, 75–78\ndata alignment, 309, 309–312\n465–467\ndata-ﬂow graphs, 561–566\nprogramming, 213–215\navoiding, 477–480\nforwarding for, 472–475\nload/use, 475–477\nstalling, 469–472\nY86-64 pipelining, 465–469\ndata movement instructions, 218–225\nlocality, 642–643\nPIC, 740–741\ndata alignment, 309–312\nstructures, 301–305\nunions, 305–309\ndata transfer, procedures, 281–284\ndeadlocks, 1063, 1063–1066\ndebugging, 315–316\ndecimal system conversions, 73–75\narrays, 291–292, 299\nstructures, 301–305\nunions, 305–309\ninstruction processing, 421, 423–433\nPIPE processor, 485–489\nY86-64 implementation, 442–444\ndeep pipelining, 454–455\n150, 150–152\n465–467\nwrite/read, 593–595\n313, 906–907\n972–973\ndetaching threads, 1025–1026\nDijkstra, Edsger, 1037–1038\nconﬂict misses, 658–660\nexample, 655–657\ndescription, 927, 927–928\nreading contents, 941–942\ndisassemblers, 80, 105, 209, 209–210\naccessing, 633–636\ncapacity, 627, 627–628\nconnecting, 632–633\ncontrollers, 631, 631–632\ngeometry, 626–627\nlogical blocks, 631–632\noperation, 628–631\ninstructions, 234–236\nby powers of 2, 139–143\ndo [C] variant of while loop, 256–259\n992, 994, 994–995\ndomain names, 961, 963–965\ndouble [C] integer data type, 77\nC, 77, 160–162\ndrivers, compiler, 40, 707–708\nduplicate symbol names, 716–720\ndynamic content, 737, 989–990\ndynamic linking, 735, 735–737\nallocator design, 890–892\n880–881\ncoalescing free blocks, 886–887\n887–890\nexplicit free lists, 898–899\nimplementation issues, 882–883\nimplicit free lists, 883–885\nmalloc and free functions, 876–\noverview, 875–876\npurpose, 879–880\nsegregated free lists, 899–901\nsplitting free blocks, 885–886\ndynamic memory allocators, 875–876\ncaches, 842, 844, 844–845\nconventional, 618–620\nenhanced, 621–622\ntrends, 638–639\nE-way set associative caches, 660–661\nECHILD return code, 782–783\necho function, 317–318, 323\n980–981\necho server, 972–973, 983\nprogramming, 205–206\ncode examples, 208–211\ncode overview, 206–207\nformatting, 211–213\nY86-64 instructions, 394–396\nentry points, 732, 733–734\nenvironment variables lists, 787–788\nerror-correcting codes for memory,\nsystem calls, 773–774\nUnix systems, 1078–1079\nwrappers, 774, 1077, 1079–1081\nerror-reporting functions, 773\nchild processes, 781–782\nrace, 812, 812–814\nfunctions, 970, 970–971, 978–980\nbased on I/O multiplexing, 1016–1021\nY86-64, 399–400, 480–483\nexceptions, 759–767\nimportance, 758–759\nnonlocal jumps, 817–822\nsystem call error handling, 773–774\nanatomy, 759–760\nclasses, 762–764\nhandling, 760–762\nLinux/x86-64 systems, 765–767\nexecutable object ﬁles, 731–732\nheaders, 710–711\nsymbol tables, 711–715\nloading, 733–734\nrunning, 43–44\ninstruction processing, 421, 423–433\nPIPE processor, 489–490\n444–445\nspeculative, 555, 555, 585–586\ntracing, 423, 430–431, 439\nexecution code regions, 325–326\nvariables, 786–788\nrunning programs, 789–792\nvirtual memory, 872–873\nexpanding bit representation, 112–116\ngoals, 880–881\n875–876\nexplicit free lists, 898–899\nexplicit waiting for, signals, 814–817\nLinux/x86-64 systems, 765, 868–869\nfeedback in pipelining, 455–457, 461\ninstruction processing, 420, 423–433\nPIPE processor, 483–485\nSEQ, 440–442\nfetches, locality, 643–644\nmetadata, 939–940\nregister, 46, 207, 394–395, 418–419,\nsharing, 942–944\ntypes, 927–929\nUnix, 926, 926–927\nﬁts, segregated, 899, 900–901\nﬁxed-size arrays, 296–298\nﬂash translation layers, 636–637\narchitecture, 329, 329–332\narithmetic operations, 338–340\nbitwise operations, 341–342\ncomparison operations, 342–345\nconstants, 340–341\noperations, 332–337\nin procedures, 337–338\nprograms, 144–145\nC, 160–162\ndenormalized values, 150, 150–152\nfractional binary numbers, 145–148\nIEEE, 148–150\nnormalized value, 149–150\noperations, 158–160\nrounding, 156, 156–158\nconcurrent, 769, 769–770\nlogical, 768, 768–769\nsynchronizing, 812–814",
      "keywords": [
        "ACM",
        "Instruction",
        "function",
        "Unix",
        "Error",
        "code",
        "Proceedings",
        "memory",
        "ACM Conference",
        "Computer",
        "Systems",
        "Data",
        "app",
        "Programming",
        "instruction set computers"
      ],
      "concepts": [
        "function",
        "functions",
        "functional",
        "instructions",
        "instruction",
        "codes",
        "code",
        "coding",
        "programming",
        "programs"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 54,
          "title": "",
          "score": 0.444,
          "base_score": 0.444,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "",
          "score": 0.43,
          "base_score": 0.43,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.421,
          "base_score": 0.421,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 23,
          "title": "",
          "score": 0.42,
          "base_score": 0.42,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 24,
          "title": "",
          "score": 0.385,
          "base_score": 0.385,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "631",
          "acm",
          "free",
          "769",
          "309"
        ],
        "semantic": [],
        "merged": [
          "631",
          "acm",
          "free",
          "769",
          "309"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2849698993504448,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.715164+00:00"
      }
    },
    {
      "chapter_number": 54,
      "title": "Segment 54 (pages 1084-1102)",
      "start_page": 1084,
      "end_page": 1102,
      "summary": "264–268\nexample, 777–779\nrunning programs, 789–792\n213–215\nmachine-level code, 211–213\nfor data hazards, 472–475\ntion, 148–156, 173\ncoalescing, 886–887\nsplitting, 885–886\nstorage, 877, 877–879\n910–911\ncreating, 893–895\ndynamic memory allocation, 883–\nexplicit, 898–899\nmanipulating, 892–893\nsegregated, 899–901\nsystem, 122–123\n663–664\nPIC, 741–743\nfunctional units, 556–557, 559–560\nstatic libraries, 720–724\n1056, 1056–1058\n1078–1079\nbasics, 902–903\nconservative, 903, 905–\nMark&Sweep, 903–906\noverview, 901–902\ncode formatting, 211–212\nworking with, 204–205\ngeometry of disks, 626–627\nnames, 976, 976–978\nsecurity vulnerability, 122–123\ngets function, 315, 317–318\n741–743\n741–743\ngprof Unix proﬁler, 598, 598–599\ndata-ﬂow, 561–566\ncode for, 440–441\nexceptions, 400, 480–483\nblocking and unblocking, 800–801\nportable, 810–811\nprocessors, 45–46\nhardware registers, 417–420\n432–436\nhardware units, 432–434, 437\navoiding, 477–480\nforwarding for, 472–475\noverview, 465–469\nstalling for, 469–472\nheap, 54, 54–55, 875\ndynamic memory allocation, 875–\nreferencing data in, 910–911\n46–48\n978–980\ndata alignment, 309–312\nstructures, 301–305\nunions, 305–309\nsockets interface, 973–978\n984, 984–985\n984, 984–985\nmethods, 987–988\nrequests, 987, 987–988\nresponses, 988, 988–989\ntransactions, 986–987\nmachine language, 201–202\nregisters, 215–216\ncall instruction, 440\nprocesses, 775–776\nregister, 394–395\nif [C] conditional statement, 247–249\ninstruction, 392\n658–660\nrepresentation, 150–151\ninformation, 38–40\nregisters, 215–216\ndata movement, 218–225\naddressing and byte ordering, 78–85\nbit-level operations, 90–92\ncode, 85–86\ndata sizes, 75–78\nhexadecimal, 72–75\nlogical operations, 92–93\nshift operations, 93–95\nprograms, 148–150\nnormalized, 149–150\ninstructions\nfetch locality, 643–644\nissuing, 463–464\njump, 46, 241–245\nmove, 250–256, 586–589\npipelining, 504–505, 585\nupdate, 45–46\noverview, 143–144\n133–137\ninteger registers in x86-64, 215–216\nintegers, 68, 95–96\nbit-level operations, 90–92\nbyte order, 79–80\ndata types, 96–98\nshift operations, 93–95\n106–112\n119–120\ntruncating, 117–118\n100–106\nevolution, 203–204\nPentium III, 203–204\n957, 957–958\nconnections, 965–967\nIP addresses, 961–963\norganization, 960–961\n957, 957–958\ncompile-time, 744–745\nrun-time, 746–748\ninterrupts, 762, 762–763\nredirection, 945, 945–946\nUnix, 55, 926, 926–927\nconnecting, 632–633\n1014–1021\n1016–1021\nIP addresses, 960, 961–963\nstatic libraries, 720–724\nissuing instructions, 463–464\nobjects, 302–303\nsoftware exceptions, 759–760, 822\ninstruction, 242\ninstruction, 242\ninstruction, 242\ninstruction, 242\ninstruction, 242\njump instructions, 46, 241–245, 440\nnonlocal, 759, 817, 817–822\njump tables, 269, 270–271, 761\nprocesses, 770–772, 771\nvirtual memory, 866–867\nkeyboard, signals from, 796–797\n956–958\ninstruction, 449\nload operations, 590–591\n746–748\n227, 227–228, 313\nstorage, 645–646\nin concurrent programming, 1060–\ninterpositioning, 743, 743–748\nshared, 55, 735, 735–737\nstatic, 720, 720–724\ndeclarations, 103–104, 113\nset associative caches, 661–662\ncompiler drivers, 707–708\ndynamic, 735, 735–737\nobject ﬁles, 709, 709–710\nexecutable, 731–734\nloading, 733–734\nrelocatable, 710–711\noverview, 706–707\nrelocation, 725–731\n737–739\nsummary, 749–750\nsymbol tables, 711–715\nvirtual memory for, 847–848\ncode segments, 733–734\nexceptions, 765–767\nﬁles, 927–929\nstatic libraries, 721–722\nvirtual memory, 866–869\n78–80\n227–229, 313\nprocess, 555–556\nload performance of memory, 590–591\nexecutable object ﬁles, 733–734\nprograms, 786–788\n737–739\n956–958\nregisters, 287–289\nstack, 284–287\nlocality, 49, 616, 640–641\ncaches, 679–683, 846\ninstruction fetches, 643–644\nprogram data references, 642–643\nsummary, 644–645\nmemory and clocking, 417–420\nset membership, 416–417\ndisks, 631, 631–632\nlogical operations, 92–93, 227\ndiscussion, 232–233\nload effective address, 227–229\nshift, 94, 140, 228, 230–232\nspecial, 233–236\nlong [C] integer data type, 76–77,\n97–98\noverview, 567–571\n577–579\ndo-while, 256–259\nfor, 264–268\ninefﬁciencies, 544–548\nsegments, 562–563\nwhile, 259–264\nlseek [Unix] function, 932–933\ndata formats, 213–215\ndata movement instructions, 218–\nencodings, 205–213\ninstructions, 40\noverview, 200–202\naccessing, 623–625\nstorage, 71, 360, 733, 875–876,\ndeclaration, 170–171\ndynamic memory allocation, 876–\nvariables, 1030–1031\n903–906\nmultiplying, 679–683\nmembership, set, 416–417\naccessing, 623–625\ndata alignment in, 309–312\nhierarchy, 50, 50, 645–650\nload performance, 590–591\nin logic design, 397–400\nmain, 45, 620, 623–625\nperformance, 589–597\npipelining, 505–506\nprotecting, 325, 848–849\nthreads, 1029–1030\ntrends, 638–640\nexecve function, 872–873\nobjects, 869–872\nuser-level, 873–875\noverview, 675–679\nin performance, 550–553\ninstruction processing, 421, 423–433\nPIPE processor, 490–491\n445–447\nmemset function, declaration, 170–171\nmetadata, 939, 939–940\nhandling, 479–480\n585–589\nkinds, 648–649\nmemory, 873, 873–875\n554–567\nprocesses, 770–772, 771\nimplementation, 457–458\n218–219\ninstruction, 332\nmove data instructions, 218–225\nmove if less instruction, 253, 393\ninstruction, 253\nmove if not less instruction, 253\ninstruction, 253\ninstruction, 253\ninstruction, 253\ninstruction, 253\nmove instructions, conditional, 250–\n256, 586–589\ninstruction, 221\ninstruction, 220\ncode, 332–337\nmulti-level page tables, 855–857\nmulti-threading, 53–54, 61\nmulticycle instructions, 504–505\n572–577\n1014–1021\n1016–1021\nmultiplexors, 410, 410–411\nword-level, 414–416\nconstants, 137–139\ninstructions, 234\nmatrices, 679–683\ntwo’s complement, 133–137\nunsigned, 132–133, 234, 234\nsemaphores for, 1038–1040\ndomain, 961, 963–965\nnetworks, 955–959\nsummary, 1000–1001\nWeb servers, 984–992\nnetworks, 56–57\nLANs, 956, 956–958\nWANs, 957, 957–958\npipelining, 466–467\nnonlocal jumps, 759, 817, 817–822\npipelining, 466–467\n149–150\nBoolean, 87–88\nC operators, 92–93\nrelocatable, 41, 708, 709–711\nmemory-mapped, 869–872\nshared, 735, 869–872\nas struct, 302–303\noct word, 233, 233–234\nGOTs, 741, 741–743\nopen [Unix] open ﬁle, 927, 929–931\n978–980\nopen operations for ﬁles, 927, 929–931\nprocesses, 51–53\nthreads, 53–54\nvirtual memory, 54–55\nbit-level, 90–92\nlogical, 92–93\nshift, 93–95\nBoolean, 87–88\nC operators, 92–93\norder, bytes, 78–85\narithmetic, 123, 123–125, 170\nidentifying, 128–129\nnegative, 126, 126–127\npositive, 126, 126–127\nP semaphore operation, 1037, 1037–\nvirtual, 839–840\nalignment, 310–311\nDRAM caches, 844, 844–845\nLinux/x86-64 systems, 765, 869–\n843–844\nCore i7, 862–864\nTLBs for, 853–857, 859\ncaches, 842–844, 843\nphysical, 841, 841–842\nvirtual, 325, 841, 841–842\n577–582\nSIMD, 62, 582–583\nthreads for, 1049–1054\nparent processes, 775, 775–776\naddresses, 651–652\njumps, 243, 243–245\nsymbol references, 726, 728–729\n483–485\ninstruction processing, 421, 423–431\nPentium III microprocessor, 203–204\nbottlenecks, 598–604\ncaches, 589, 667–669, 675–683\nlimitations, 534–538\nexpressing, 538–540\nmemory, 589–597\nmemory references, 550–553\nmodern processors, 554–567\noverview, 532–534\nprogram example, 540–544\nprogram proﬁling, 598–600\nregister spilling, 584–585\nsummary, 604–605\nY86-64 pipelining, 500–504\nvirtual, 839–840\ndata references, 740–741\nfunction calls, 741–743\nPIPE−processor, 457, 458, 462–466\nPIPE processor stages, 475–476, 483\nexecute, 489–490\nmemory, 490–491\ncombinational, 448–450\ndeep, 454–455\nfunctional units, 559–560\ninstruction, 585\nlimitations, 452–454\noperation, 450–452\nstore operation, 591–592\n742–743\narithmetic, 293–294, 909\n906–907\nvirtual memory, 906–909\n608–609\ndata references, 740–741\nfunction calls, 741–743\nPosix threads, 1023, 1023–1024\nPOST method, 987–989\nY86-64 pipelining, 458, 463–465\nprethreading, 1041–1049, 1044\n487–488\n742–743\nprocedures, 274–275\ncall performance, 548–549\ndata transfer, 281–284\nﬂoating-point code in, 337–338\nrecursive, 289–291\n1009–1013\n1010–1011\nerror conditions, 781–782\nIDs, 775–776\nloading programs, 735, 786–788\noverview, 51–53\nreaping, 779, 779–785\nrunning programs, 786–792\nsleeping, 785–786\ntools, 822–823\nwaitpid function, 782–785\n1041–1042\nproﬁling, program, 598–600\nY86-64 pipelining, 459, 463–465\n642–643\nclocked, 417–420\nY86-64, 391–392\nforms, 40–41\nloading and running, 786–788\nproﬁling, 598–600\nY86-64, 400–406\nprogress graphs, 1035, 1035–1037\nprotection, memory, 848–849\n843–844\nCore i7, 862–864\nTLBs for, 853–857, 859\nPthreads, 1023, 1023–1024, 1046\nprocessing steps, 406–407, 428\n1061–1063\nsignals, 812–814\nconstants for, 103–104\nread [Unix] read ﬁle, 931, 931–933\nbuffered, 934, 936–937\ndisk sectors, 633–635\nﬁles, 927, 931–933\nunbuffered, 933–934\nexample of, 624–625\ndirectory contents, 941–942\nchild processes, 779, 779–785\n462–463\n577–582, 606\nreceiving signals, 794, 798–800\nredirection of I/O, 945, 945–946\nCISC, 397–399\ndata in free heap blocks, 910–911\ncontents, 418–419, 557\npurpose, 394–395\nregister identiﬁer (ID), 394–395\ninstruction, 394\ndata transfer, 281–284\nhardware, 417–420\nlocal storage, 287–289\nprogram, 391–392, 417–420, 471\nspilling, 584–585\nx86-64 integer, 215, 215–216\nY86-64, 395, 458–462\nrelabeling signals, 462–463\nrelocation, 709, 725–726\nentries, 726, 726–727\nHTTP, 987, 987–988\nshared, 1040–1044\nHTTP, 988, 988–989\ncall, 244, 277–278\nY86-64 pipelining, 464–465, 491–\n493, 497–498\nright shift operations, 93–94, 228\nbuffered functions, 934–938\nunbuffered functions, 933–934\nunbuffered read, 933, 933–935,\nCISC, 397–399\nbuffered functions, 934–938\nunbuffered functions, 933–934\n933–935\n933–935\nround-to-even mode, 156, 156–157,\nin division, 141–142\n156–158\nrounding modes, 156, 156–158\ninterpositioning, 746–748\nstacks, 207, 275–277\nprograms, 46–48, 786–792\n583–584\n1055–1056\nscanf function, 906–907\nshared resources, 1040–1044\nsectors, disk, 626, 626–628\naccess time, 629–631\nreading, 633–635\ngetpeername function, 122–123\nsegmented addressing, 323–324\ncode, 732, 733–734\nloops, 562–563\nsemaphores, 1037, 1037–1038\n1040–1044\nsending signals, 771, 795–798\n457, 457–458\nsequential execution, 236–237\n442–444\nexecute stage, 444–445\nhardware structure, 432–436\n420–431\nmemory stage, 445–447\n457–458\ntiming, 436–439\nhelper function, 999–1000\nfunction, 997–999\ninterface, 973–978\n661–662\nset index bits, 651, 651–652\ninstruction, 239\ninstruction, 239\ninstruction, 239\nmembership, 416–417\ndynamic linking with, 735–737\napplications, 737–739\nshared objects, 735, 869–872, 870\nshared variables, 1028–1031, 1029\nﬁles, 942–944\nshift operations, 93, 93–95\nfor division, 139–143\nmachine language, 230–232\nfor multiplication, 137–139\nsign extension, 113, 113, 219–220\nwriting, 802–811\nsignals, 758, 792–794\nblocking and unblocking, 800–801\nreceiving, 798, 798–800\nsending, 794, 795–798\nterminology, 794–795\nwaiting for, 814–817\n462–463\nsigned integers, 68, 76, 97–98, 103\nunsigned conversions, 106–112\nguidelines, 119–120\n148, 148–149\ninstructions, 312\n899–900\n582–583\ncaches, 668–669\ndata, 75–78\ndesignating sizes, 80, 119–120,\n165–167, 169\nsockets interface, 968, 968–969\naccept function, 972–973\naddress structures, 969–970\nconnect function, 970–971\nexample, 980–983\nhelper functions, 978–980\n973–978\nopen_clientfd function, 970–971\nECF for, 759–760\noperation, 636–638\ncaches, 679–683\ndetecting, 493–495\nhandling, 491–493\n585–586\n1054–1055\nspilling, register, 584–585\nfree blocks, 885–886\ntrends, 638–639\noperation, 636–638\ninstructions, 203–204, 330\nparallelism, 582–583\nstack frames, 276, 276–277\nvariable-size, 326–329\nstacks, 55, 225, 225–227\nwith execve function, 787–788\nlocal storage, 284–287\nrun time, 275–277\nexecute, 444–445\nfetch, 440–442\nmemory stage, 445–447\nstandard C library, 40, 40–41\n939–940\nstatic libraries, 720, 720–724\ntrends, 638–639\nstatic variables, 1030, 1030–1031\nY86-64, 399–400, 400\nstdlib, 40, 40–41\nregisters, 287–289\nstack, 284–287\nstore performance of memory, 591–\nstraight-line code, 236–237\ninstructions, 203–204, 330\nparallelism, 582–583\nfunction, 119, 545–547\naddress, 969–970\n671–672\nsupercells, 618, 618–619\nstatement, 268–274\nstatic libraries, 720–724\nsymbol tables, 711, 711–715\nrelocation, 725–731\nﬂow, 812–814\nthreads, 1031–1035\nprogress graphs, 1035–1037\nsystem calls, 53, 763, 763–764\nerror handling, 773–774\nLinux/x86-64 systems, 766–767\nclosing ﬁles, 930–931\nI/O redirection, 945–946\nopening ﬁles, 929–931\nreading ﬁles, 931–933\nsharing ﬁles, 942–944\nsummary, 949–950\nUnix I/O, 926–927\nwriting ﬁles, 932–933\nconversion), 96, 107, 107–109\nGOTs, 741, 741–743\nhash, 603–604\njump, 269, 270–271, 761\npage, 772, 842–844, 843, 855–857,\nsymbol, 711, 711–715\ntargets, jump, 242, 242–245\n986–987\nprocesses, 775–779\nthreads, 1024–1025\nthread-level concurrency, 60–62\nthread-safe functions, 1056, 1056–1058\nthread-unsafe functions, 1056, 1056–\nthreads, 53, 54, 1009, 1021–1022\ndetaching, 1025–1026\nlibrary functions for, 1060–1061\nmapping variables in, 1030–1031\nmemory models, 1029–1030\nfor parallelism, 1049–1054\nPosix, 1023–1024\nraces, 1061–1063\nshared variables with, 1028–1031, 1029\nsynchronizing, 1031–1035\nprogress graphs, 1035–1037\nterminating, 1024–1025\nthree-stage pipelines, 450–452\ntiming, SEQ, 436–439\n992–1000",
      "keywords": [
        "instruction",
        "Unix",
        "APP",
        "equal instruction",
        "instruction code",
        "word instruction",
        "function",
        "memory",
        "move",
        "quad word instruction",
        "unsigned greater instruction",
        "instruction set",
        "greater instruction",
        "jump",
        "register"
      ],
      "concepts": [
        "instructions",
        "instruction",
        "function",
        "functional",
        "functions",
        "unix",
        "memory",
        "programs",
        "program",
        "programming"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "",
          "score": 0.726,
          "base_score": 0.726,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.587,
          "base_score": 0.587,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 16,
          "title": "",
          "score": 0.552,
          "base_score": 0.552,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 49,
          "title": "",
          "score": 0.551,
          "base_score": 0.401,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.55,
          "base_score": 0.55,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "743",
          "933",
          "93",
          "741"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "743",
          "933",
          "93",
          "741"
        ]
      },
      "topic_id": 8,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4313578698250554,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:01:48.715832+00:00"
      }
    },
    {
      "chapter_number": 55,
      "title": "Segment 55 (pages 1103-1105)",
      "start_page": 1103,
      "end_page": 1105,
      "summary": "506, 853, 853–861\nbus, 623, 624–625\nHTTP, 986–989\ntransferring control, 277–281\n577–582, 606\ntranslating programs, 40–41\n506, 853, 853–861\ntraps, 763, 763–764\ntruncating numbers, 117–118\naddition, 126–131\nmultiplication, 133–137\n106–110\nsigned numbers, 100, 100–106\nﬂoating point, 160–162\nintegral, 96, 96–98\nmachine-level, 207, 213–214\numask function, 930–931\n99, 102–103\nunions, 80, 305–309\nI/O, 55, 926, 926–927\nloops, 538, 540, 567, 567–571, 608\nunsigned representations, 119–120\naddition, 120–126\nconversions, 106–112\nencodings, 68, 98–100\nmultiplication, 132–133, 234, 234\nupdate instructions, 45–46\nuser-level memory mapping, 873–875\nprocesses, 770–772, 771\nV semaphore operation, 1037, 1037–\nmapping, 1030–1031\nshared, 1028–1031, 1029\nvector data types, 62, 540–543\nvector sum function, 670, 671–672\nvectors, bit, 87, 87–88\nmachine-level programming, 206–\nphysical, 839–840\naddress spaces, 840–841\nbugs, 906–911\nfor caching, 841–847\ncharacteristics, 838–839\nCore i7, 861–864\nLinux, 866–869\nfor memory management, 847–848\nfor memory protection, 848–849\noverview, 54–55\n839–840\nsummary, 911–912\nvirtual pages (VPs), 325, 841, 841–842\nviruses, 321–322\n805–806\nVP (virtual pages), 325, 841, 841–842\nvulnerabilities, security, 122–123\n782–785\nfunction, 972, 972–973\nwaiting for signals, 814–817\n779, 782–785\n957–958\nbasics, 984–985\nWeb content, 985–986\nwhile [C] loop statement, 259–264\n957–958\nWNOHANG constant, 780–781\n412–416\nset associative caches, 661–662\nerror handling, 774, 1077, 1079–\ninstruction processing, 421, 423–433\n442–444\nwrite [Unix] write ﬁle, 931, 932–933\nwrite issues for caches, 666–667\nwrite operations for ﬁles, 927, 932–\nwrite/read dependencies, 593–595\nwrite transactions, 623, 624–625\nWUNTRACED constant, 780–781\n250–256\nexceptions, 765–767\nmachine language, 201–202\ndata movement, 218–225\nY86-64, 401–402\n389–390\ndetails, 406–408\ninstruction encoding, 394–396\ninstruction set, 392–394\nprograms, 400–406\ncomputation stages, 457–458\nmemory system interfacing, 505–\nmulticycle instructions, 504–505\npredicted values, 463–465\nregister insertions, 458–462\nsignals, 462–463\nzombie processes, 779, 779–780, 806",
      "keywords": [
        "packed double precision",
        "double precision",
        "packed single precision",
        "single precision",
        "precision",
        "convert double precision",
        "convert single precision",
        "packed double",
        "packed single",
        "virtual",
        "double",
        "Unix",
        "single",
        "packed",
        "integer"
      ],
      "concepts": [
        "instructions",
        "instruction",
        "write",
        "writing",
        "precision",
        "constant",
        "constants",
        "function",
        "functions",
        "unix"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 54,
          "title": "",
          "score": 0.726,
          "base_score": 0.726,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 37,
          "title": "",
          "score": 0.552,
          "base_score": 0.552,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "",
          "score": 0.496,
          "base_score": 0.496,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 36,
          "title": "",
          "score": 0.493,
          "base_score": 0.493,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 45,
          "title": "",
          "score": 0.484,
          "base_score": 0.484,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "841",
          "precision",
          "packed",
          "853",
          "861"
        ],
        "semantic": [],
        "merged": [
          "841",
          "precision",
          "packed",
          "853",
          "861"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.41427967420260725,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:01:48.715891+00:00"
      }
    }
  ],
  "total_chapters": 55,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "Computer Systems A Programmer’s Perspective_metadata.json",
    "enrichment_date": "2025-12-17T23:01:48.732799+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 5456.997252000292,
    "total_similar_chapters": 275
  }
}