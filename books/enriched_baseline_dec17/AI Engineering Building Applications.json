{
  "metadata": {
    "title": "AI Engineering Building Applications",
    "source_file": "AI Engineering Building Applications_metadata.json"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Segment 1 (pages 1-18)",
      "start_page": 1,
      "end_page": 18,
      "summary": "Building Applications\nPraise for AI Engineering\nessential aspects of building generative AI systems.\nteams bring AI into production.\ngenerative AI applications in production.\nEvery AI engineer building real-world applications should read this\nbook.\nIt’s a vital guide to end-to-end AI system design, from model\nThis book serves as an essential guide for building AI products that\nfunctional teams, making it a must-read for anyone involved in AI\nThis is the definitive segue into AI engineering from one of the greats\ncondensed her expertise for new AI Engineers entering the field.\n—swyx, Curator, AI.Engineer\nAI Engineering is a practical guide that provides the most up-to-date\ninformation on AI development, making it approachable for novice\nanyone looking to build robust and scalable AI systems.\nAI Engineering is a comprehensive guide that serves as an essential\nAI Engineering is an essential guide for anyone building software\nwith Generative AI!\n—Rafal Kawala, Senior AI Engineering Director, 16\nAI Engineering\nBuilding Applications with Foundation Models\nAI Engineering\ndecade, the AI community has known that scaling up a model improves it.\nbecome so easy to get started with building AI applications.\nEven though AI adoption today seems new, it’s built upon techniques that\nmodel-based applications.\nThe familiarity and ease of use of many AI engineering techniques can\nmislead people into thinking there is nothing new to AI engineering.\nwhile many principles for building AI applications remain the same, the\nscale and improved capabilities of AI models introduce opportunities and\nThis book covers the end-to-end process of adapting foundation models to\nother engineering fields and techniques emerging with foundation models.\n100 conversations and interviews, including researchers from major AI labs\nheads of AI/data at companies of different sizes, product managers,\nWhat This Book Is About\nThis book provides a framework for adapting foundation models, which\nShould I build this AI application?\nCan I use AI to evaluate AI outputs?\nThe book will also help you navigate the overwhelming AI landscape: types\nLike my previous O’Reilly book, Designing Machine Learning Systems\n(DMLS), this book focuses on the fundamentals of AI engineering instead\nREADING AI ENGINEERING (AIE) WITH DESIGNING MACHINE LEARNING\nDMLS focuses on building applications\nbuilding applications on top of foundation models, which involves more\nSince foundation models are ML models, some concepts are relevant to\nML engineering and AI engineering.\nfrom the fundamental limitations of how AI works or if it’ll go away with\nIn this book, however, I occasionally included a concept that I believe to be\ntemporary because it’s immediately useful for some application developers\nWhat This Book Is Not\nAI chatbots are also\nis a practical book that focuses on helping you build successful AI\nWhile it’s possible to build foundation model-based applications without\nthis book without any prior ML background.\neffective while building AI applications if you know the following\nWho This Book Is For\nThis book is for anyone who wants to leverage foundation models to solve\nis geared toward technical roles, including AI engineers, ML engineers, data\nYou’re building or optimizing an AI application, whether you’re starting\nYou want to streamline your team’s AI development process, making it\nTool developers who want to identify underserved areas in AI\nResearchers who want to better understand AI use cases.",
      "keywords": [
        "book",
        "Engineering",
        "Foundation Models",
        "Models",
        "Applications",
        "Chip Huyen",
        "Foundation",
        "Building",
        "Building Applications",
        "Engineering Building Applications",
        "Models Chip Huyen",
        "Foundation Models Chip",
        "Solutions",
        "Chip",
        "DMLS"
      ],
      "concepts": [
        "models",
        "engineering",
        "engineer",
        "book",
        "applications",
        "application",
        "developer",
        "including",
        "include",
        "production"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 5,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "book",
          "engineering",
          "building",
          "ai engineering"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "book",
          "engineering",
          "building",
          "ai engineering"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33386154349579944,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603128+00:00"
      }
    },
    {
      "chapter_number": 2,
      "title": "Segment 2 (pages 19-39)",
      "start_page": 19,
      "end_page": 39,
      "summary": "an AI engineer.\nThis book is structured to follow the typical process for developing an AI\nBefore deciding to build an AI application, it’s necessary to understand\nfirst chapter of the book helps you answer these questions.\nrange of successful use cases to give a sense of what foundation models can\nunderstanding how a foundation model works under the hood is useful to\napplications, including its training data recipe, model architectures and\ndiscusses how a model generates a response, which helps explain the\nChanging the generation setting of a model is also often a\nOnce you’ve committed to building an application with foundation models,\nGiven a query, the quality of a model’s response depends on the following\naspects (outside of the model’s generation setting):\nThe context the model can use to respond to the query\nThe model itself\nThe next three chapters of the book focus on how to optimize each of these\naspects to improve a model’s performance for an application.\nChapter 6 explores why context is important for a model to generate\nChapter 7 is about how to adapt a model to an application by changing the\nmodel itself with finetuning.\nDue to the scale of foundation models, native\nmodel finetuning is memory-intensive, and many techniques are developed\nto allow finetuning better models with less memory.\napproach: model merging.\nIf Chapters 5 to 8 are about improving a model’s quality, Chapter 9 is about\nIf you’re using a model API—i.e.,\nHowever, if you host the model yourself—\nThe last chapter in the book brings together the different concepts from this\nthis book.\nprogram that uses several chunks of code from this book does not require\nSelling or distributing examples from O’Reilly books does\namount of example code from this book into your product’s documentation\nAI application patterns.\nreality with GPT models.\nEven my small project in 2017, which used a language model to evaluate translation quality,\nconcluded that we needed “a better language model.”\nApplications with Foundation Models\nmodels behind applications like ChatGPT, Google’s Gemini, and\nThe scaling up of AI models has two major consequences.\nFirst, AI models\nSecond, training large language models (LLMs) requires data, compute\nAnyone who wishes to leverage AI to build applications can now use these\nmodels to do so without having to invest up front in building a model.\nBuilding applications on top of machine learning (ML) models isn’t new.\nmodels brings about new possibilities and new challenges, which are the\nThis chapter begins with an overview of foundation models, the key\nincluding what has changed with foundation models, what remains the\nFoundation models emerged from large language models, which, in turn,\noriginated as just language models.\nmodels emerging in the 1950s.\nthat enabled the evolution from language models to AI engineering.\nFrom Language Models to Large Language\nModels\nWhile language models have been around for a while, they’ve only been\ngives a quick overview of what language model and self-supervision mean.\nLanguage models\nA language model encodes statistical information about one or more\ncolor is __”, a language model that encodes English should predict “blue”\nmodel English was published in his 1951 landmark paper “Prediction and\nincluding entropy, are still used for language modeling today.\nIn the early days, a language model involved one language.\na language model can involve multiple languages.\nThe basic unit of a language model is token.\nword, or a part of a word (like -tion), depending on the model.\nexample, GPT-4, a model behind ChatGPT, breaks the phrase “I can’t wait\ncan see how different OpenAI models tokenize text on the OpenAI website.\nThe set of all tokens a model can work with is the model’s vocabulary.\nvocabulary size are decided by model developers.\nWhy do language models use token as their unit instead of word or character?\n1. Compared to characters, tokens allow the model to break words into meaningful components.\n2. Because there are fewer unique tokens than unique words, this reduces the model’s vocabulary\nsize, making the model more efficient (as discussed in Chapter 2).\n3. Tokens also help the model process unknown words.\n“chatgpting” could be split into “chatgpt” and “ing”, helping the model understand its structure.\nThere are two main types of language models: masked language models and\nautoregressive language models.\nMasked language model\nA masked language model is trained to predict missing tokens\nIn essence, a masked language model is trained to\nfavorite __ is blue”, a masked language model should predict that the\nAs of writing, masked language models are commonly used for non-\noverall context, such as code debugging, where a model needs to\nAutoregressive language model\nAn autoregressive language model is trained to predict the next token\nautoregressive language models are the models of choice for text\nmasked language models.\nFigure 1-2 shows these two types of language models.\nAutoregressive language model and masked language model.\nIn this book, unless explicitly stated, language model will refer to an autoregressive model.\nThe outputs of language models are open-ended.\nA language model can use\nA model\nYou can think of a language model as a completion machine: given a text\nCompletion (from language model): “, that is the\nlanguage models makes them both so exciting and frustrating to use.\nare you in French is …”, a language model might be able to complete it",
      "keywords": [
        "language model",
        "model",
        "Masked language model",
        "language",
        "Book",
        "Autoregressive language model",
        "foundation models",
        "large language models",
        "masked language",
        "application",
        "autoregressive language",
        "tokens",
        "engineering",
        "finetuning",
        "data"
      ],
      "concepts": [
        "models",
        "chapters",
        "token",
        "examples",
        "application",
        "applications",
        "likely",
        "engineer",
        "engineering",
        "book"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "language",
          "language model",
          "masked",
          "masked language",
          "language models"
        ],
        "semantic": [],
        "merged": [
          "language",
          "language model",
          "masked",
          "masked language",
          "language models"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.44845766219972605,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603246+00:00"
      }
    },
    {
      "chapter_number": 3,
      "title": "Segment 3 (pages 40-60)",
      "start_page": 40,
      "end_page": 60,
      "summary": "A language model might be able to complete it with: “Likely spam”, which\nturns this language model into a spam classifier.\nLanguage modeling is just one of many ML algorithms.\nThe answer is that language models can be trained using self-supervision,\nwhile many other models require supervision.\nlabeling bottleneck to create larger datasets for models to learn from,\nmodel to learn, and then train the model on these examples.\nthe model can be applied to new data.\ndetection model, you use examples of transactions, each labeled with\nOnce the model learns from these examples, you can\nuse this model to predict whether a transaction is fraudulent.\nThe success of AI models in the 2010s lay in supervision.\nThe model that\nto-Latin model is more expensive.\nsupervision, instead of requiring explicit labels, the model can infer labels\nLanguage modeling is self-supervised because each\ncontexts the model can use to predict these labels.\nTraining samples from the sentence “I love street food.” for language modeling.\nThese markers are necessary for a language model to work with\nthe model.\nlanguage models know when to end their responses.\nSelf-supervised learning means that language models can learn from text\nmodel have to be to be considered large?\nA parameter is a variable within an ML model that is\nWhen OpenAI’s first generative pre-trained transformer (GPT) model came\nwriting of this book, a model with 100 billion parameters is considered\nusually taken for granted: Why do larger models need more data?\nmodels have more capacity to learn, and, therefore, would need more\nYou can train a large model\nFrom Large Language Models to Foundation\nModels\nWhile language models are capable of incredible tasks, they are limited to\nIncorporating more data modalities into language models makes them\ncharacterized as foundation models.\nimportance of these models in AI applications and the fact that they can be\nFoundation models mark a breakthrough from the traditional structure of AI\nText-only models can be used for tasks such as\nImage-only models can be used for object\nA model that can work with more than one data modality is also called a\nmultimodal model.\nA generative multimodal model is also called a large\nmultimodal model (LMM).\nIf a language model generates the next token\nconditioned on text-only tokens, a multimodal model generates the next\nthat the model supports, as shown in Figure 1-3.\nA multimodal model can generate the next token using information from both text and\nJust like language models, multimodal models need data to scale up.\nsupervision works for multimodal models too.\nlanguage-image model CLIP (OpenAI, 2021).\nThis book uses the term foundation models to refer to both large language models and large\nmultimodal models.\nCLIP is an embedding model, trained to produce joint\nembedding models like CLIP are the backbones of generative multimodal\nFoundation models also mark the transition from task-specific models to\ngeneral-purpose models.\nPreviously, models were often developed for\nA model trained for\nFoundation models, thanks to their scale and the way they are trained, are\nmodel to maximize its performance on a specific task.\nidea of the types of tasks a foundation model can perform.\nAn out-of-the-box model might be\nThere are multiple techniques you can use to get the model to generate what\nmodel can leverage to generate better descriptions.\nengineering techniques that you can use to adapt a model to your needs.\nAdapting an existing powerful model to your task is generally a lot easier\nthan building a model for your task from scratch—for example, ten\nFoundation models make it cheaper to develop AI applications and reduce\nExactly how much data is needed to adapt a model depends\nFrom Foundation Models to AI Engineering\nAI engineering refers to the process of building applications on top of\nfoundation models.\nPeople have been building AI applications for over a\nIf traditional ML engineering involves developing ML models, AI\nFoundation models are powerful not just because they can do\nAI is used\nAI can even be\nall of which will help train even more powerful models in the future.\nAs AI applications\nThe model as a service approach popularized by OpenAI and other\nmodel providers makes it easier to leverage AI to build applications.\nNot only that, AI also makes it possible to build applications with\nwork with these models in plain English instead of having to use a\ndevelop AI applications.\nBecause of the resources it takes to develop foundation models, this process\nmajority of people will be to adapt these models for specific applications.\non top of foundation models, including ML engineering, MLOps, AIOps,\nEngineering Versus ML Engineering”, working with foundation models\ndiffers from working with traditional ML models in several important\n(engineering) foundation models to do what you want.\nfoundation models about what term they would use to describe what they\nFoundation Model Use Cases\nmodels seems endless.\nAI for that.\n(AWS) has categorized enterprise generative AI use cases into three\nto AI models directly, whereas β and ζ refer to exposures to AI-powered software.\nAs you learn more about how to build foundation models in\nform a better picture of what use cases foundation models can and should",
      "keywords": [
        "models",
        "foundation models",
        "language models",
        "engineering",
        "language",
        "Foundation",
        "applications",
        "data",
        "Large Language Models",
        "multimodal model",
        "tasks",
        "cases",
        "BOS",
        "cases foundation models",
        "image"
      ],
      "concepts": [
        "modeling",
        "generating",
        "generates",
        "generation",
        "labeled",
        "engineering",
        "engineers",
        "foundation",
        "trained",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 17,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 9,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "foundation",
          "foundation models",
          "multimodal",
          "language",
          "multimodal model"
        ],
        "semantic": [],
        "merged": [
          "foundation",
          "foundation models",
          "multimodal",
          "language",
          "multimodal model"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32192582290574523,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603291+00:00"
      }
    },
    {
      "chapter_number": 4,
      "title": "Segment 4 (pages 61-81)",
      "start_page": 61,
      "end_page": 81,
      "summary": "Common generative AI use cases across consumer and enterprise applications.\nAI companion\nInternal applications help companies develop their AI\nEven after seeing hundreds of AI applications, I still find new applications\nAs we learn to make the most out of AI, the use case that will\nIn multiple generative AI surveys, coding is hands down the most popular\nAI coding tools are popular both because AI is good at coding and\nbecause early AI engineers are coders who are more exposed to coding\nAs of this writing, AI-powered\n(GPT-Migrate, AI Code Translator)\nGenerating commit messages (AI Commits)\nIt’s clear that AI can do many software engineering tasks.\nwhether AI can automate software engineering altogether.\nAI is better at some than\nMcKinsey researchers found that AI can help developers be twice as\nconversations with developers of AI coding tools, many told me that\nAI can help developers be significantly more productive, especially for simple tasks, but\nAI can also disrupt the outsourcing\nmost successful AI startups are creative applications, such as Midjourney\nprobabilistic nature of AI in more detail.\nIt’s now common to use AI to generate profile pictures for social media,\nfrom LinkedIn to TikTok. Many candidates believe that AI-generated\nThe perception of AI-generated profile pictures\napps provide tools that let users use AI to generate profile photos.\nFor enterprises, ads and marketing have been quick to incorporate AI.\nAI\nYou can use AI to generate multiple ads and test to see which one works the\nAI can generate variations of your ads according to\nFor example, you can use AI to change leaf colors\nAI has long been used to aid writing.\nAI.\nWriting is an ideal application for AI because we do it a lot, it can be\nMany use AI to help them\nYou can be angry in an email and ask AI to make it\nasking AI to improve it first.\nStudents are using AI to write essays.\nWriters are using AI to write books.\nMany startups already use AI to generate children’s, fan fiction, romance,\nUnlike traditional books, AI-generated books can be\nNote-taking and email apps like Google Docs, Notion, and Gmail all use AI\nAI’s ability to write can also be abused.\nreported that Amazon was flooded with shoddy AI-generated travel\nall AI-generated.\nFor enterprises, AI writing is common in sales, marketing, and general team\nMany managers told me they’ve been using AI to help\nAI can help craft effective cold outreach\nAI is so good at SEO that it has enabled a new generation of content\nThese farms set up junk websites and fill them with AI-generated\nidentified almost 400 ads from 141 popular brands on junk AI-generated\nInstead of banning AI, schools could incorporate it to help students learn\nAI can summarize textbooks and generate personalized lecture plans\nAI can help adapt the materials\nStudents who love animals can use AI to adapt\nAI is especially helpful for language learning, as you can ask AI to roleplay\ncan benefit the most from AI, as shown in Figure 1-10.\nAI can be used throughout all four stages of course creation at Duolingo, but it’s the\nAI can generate quizzes, both multiple-choice and open-ended, and evaluate\nAI can become a debate partner as it’s much better at\nexample, Khan Academy offers AI-powered teaching assistants to students\nis that teachers assign AI-generated essays for students to find and correct\nWhile many education companies embrace AI to build better products,\nmany find their lunches taken by AI.\nstudents have been turning to AI for help.\nFor many skills, AI can help someone\nbetter than AI.\nAI can be your companion and\nAI can also be product\nOne use case of AI-powered 3D characters is smart NPCs, non-player\nWithout AI, NPCs are\nAI\nLuckily, AI came to\nAI has proven to be capable of aggregating information and\nAccording to Salesforce’s 2023 Generative AI Snapshot\nResearch, 74% of generative AI users use it to distill complex ideas and\nAI can help you\nDuring the process of writing this book, I found AI helpful for\nThis template asks AI to summarize meeting notes, emails,\nAI can help you surface the critical information about your potential\nAI can help with exactly that.\nAI can automatically generate text\nusing AI to surface images that match search queries.\nAI is very good with data analysis.\nEnterprises can use AI to extract structured information from unstructured\nUltimately, AI should automate as much as possible.\nFor enterprises, AI can automate repetitive tasks such as lead management,\nOne especially exciting use case is using AI models to synthesize data,\nYou can use AI\nAIs that can plan and use tools are called\nAI agents have the potential to make every person\nIt’s been a lot of fun looking into different AI applications.\nwhat we should consider before building an AI application.\nPlanning AI Applications\nexperiment with AI applications to upskill themselves.\nmany business decisions, building an AI application is often a response to\n1. If you don’t do this, competitors with AI can make you obsolete.\nIf AI\ncontinuity as their reason for embracing AI.\nAI.\nMost companies embrace AI for the opportunities it brings.\nAI can help in most, if not all, business operations.\nAI can make user\nAI can increase user\nAI can also help with sales lead generation, internal\nIf AI poses an existential\nHowever, if you’re using AI to boost profits\nThe role of AI and humans in the application\nWhat role AI plays in the AI product influences the application’s\ndifferent ways AI can be used in a product.\nFor example, Face ID wouldn’t work without AI-powered facial\nThe more critical AI is to the application, the more accurate and\nreliable the AI part has to be.\nwhen AI isn’t core to the application.",
      "keywords": [
        "applications",
        "Data",
        "cases",
        "Information",
        "Writing",
        "tasks",
        "n’t",
        "generate",
        "Coding",
        "Information aggregation",
        "bots",
        "make",
        "Image",
        "Conversational bots",
        "Students"
      ],
      "concepts": [
        "ais",
        "generative",
        "generate",
        "generations",
        "data",
        "applications",
        "application",
        "users",
        "coding",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.766,
          "base_score": 0.616,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.608,
          "base_score": 0.458,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 5,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.561,
          "base_score": 0.411,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.52,
          "base_score": 0.37,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "ai",
          "ai help",
          "students",
          "help",
          "use ai"
        ],
        "semantic": [],
        "merged": [
          "ai",
          "ai help",
          "students",
          "help",
          "use ai"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2692942497688741,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603331+00:00"
      }
    },
    {
      "chapter_number": 5,
      "title": "Segment 5 (pages 82-102)",
      "start_page": 82,
      "end_page": 102,
      "summary": "their own model, continually finetuned on their data, or other\nHowever, static features might have one model for a group of users.\nmodel is updated.\nWill AI\nThe role of humans can change over time as the quality of the AI system\nAI product defensibility\nIf you’re selling AI applications as standalone products, it’s important to\nIn a way, building applications on top of foundation models means\nproviding a layer on top of these models.\nsubsumed by the models, rendering your application obsolete.\nOnce you’ve decided that you need to build this amazing AI application by\nautomate 60% of customer support tickets and the off-the-shelf model you\nPlanning an AI product needs to account for its last mile challenge.\nsuccess with foundation models can be misleading.\nBuilding on top of foundation models today means committing to riding\nFor example, the limitations of many models are\nModel outputs are\nmodel benchmark, between 2022 and 2024.\nYou may decide to build a model in-house because\nFor example, as model providers\ndevelopers working with the new model will need to adjust their\nworkflows, prompts, and data to this new model.\nproduct on top of a model trained using other people’s data, can you be\nOnce you’ve committed to building an AI product, let’s look into the\nengineering stack needed to build these applications.\nThe AI Engineering Stack\nmodels, and applications introduced every day can be overwhelming.\nthe fundamental building blocks of AI engineering.\nTo understand AI engineering, it’s important to recognize that AI\nexperimenting with foundation models, it’s natural that its existing ML\nSome companies treat AI engineering the same\nMany companies put AI engineering and ML engineering under the same umbrella, as\nSome companies have separate job descriptions for AI engineering, as\nExisting ML engineers can add AI\nthere are also AI engineers with no previous ML experience.\nTo best understand AI engineering and how it differs from traditional ML\nengineering, the following section breaks down different layers of the AI\napplication building process and looks at the role each layer plays in AI\nSome companies have separate job descriptions for AI engineering, as shown in the job\nThere are three layers to any AI application stack: application development,\nmodel development, and infrastructure.\nWhen developing an AI application,\nWith models readily available, anyone can use them to develop\nModel development\nThis layer provides tooling for developing models, including\nframeworks for modeling, training, finetuning, and inference\nBecause data is central to model development, this\nModel development also\nmodel serving, managing data and compute, and monitoring.\nThree layers of the AI engineering stack.\nrepositories for applications and models, which are the products of the\napplication development and model development layers, respectively.\nEven though models and\naround foundation models is unprecedented, many principles of building AI\nFor enterprise use cases, AI applications still\nWith foundation models, you experiment\nfor everyone to begin building AI applications.\nAI Engineering Versus ML Engineering\nAt a high level, building applications using foundation models today differs\n1. Without foundation models, you have to train your own models for your\nWith AI engineering, you use a model someone else has\nThis means that AI engineering focuses less on modeling\nand training, and more on model adaptation.\n2. AI engineering works with models that are bigger, consume more\n3. AI engineering works with models that can produce open-ended outputs.\nbigger problem in AI engineering.\nIn short, AI engineering differs from ML engineering in that it’s less about\nmodel development and more about adapting and evaluating models.\nmodel adaptation means.\nIn general, model adaptation techniques can be\nmodel weights.\nYou adapt a model by giving it\ninstructions and context instead of changing the model itself.\nof finding a model that is unexpectedly good for your applications.\nFinetuning, on the other hand, requires updating model weights.\na model by making changes to the model itself.\nNow, let’s zoom into the application development and model development\nlayers to see how each has changed with AI engineering, starting with what\noverview of different processes involved in developing an AI application.\nModel development\nIt has three main responsibilities: modeling and training,\nModeling and training\nModeling and training refers to the process of coming up with a model\nDeveloping ML models requires specialized ML knowledge.\nIt also requires understanding how a model learns, including\nWith the availability of foundation models, ML knowledge is no longer a\nmust-have for building AI applications.\nTraining always involves changing model weights, but not all changes to\nmodel weights constitute training.\nPre-training refers to training a model from scratch—the model\ninvolves training a model for text completion.\nFor the InstructGPT model, pre-training takes up to 98% of the",
      "keywords": [
        "model",
        "engineering",
        "foundation models",
        "model development",
        "Google Photos",
        "applications",
        "data",
        "model weights",
        "product",
        "application development",
        "training",
        "development",
        "layer",
        "model development layers",
        "foundation"
      ],
      "concepts": [
        "model",
        "products",
        "production",
        "train",
        "application",
        "applications",
        "engineers",
        "engineering",
        "include",
        "included"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.755,
          "base_score": 0.605,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.699,
          "base_score": 0.549,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "",
          "score": 0.606,
          "base_score": 0.456,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.568,
          "base_score": 0.568,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.547,
          "base_score": 0.547,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering",
          "ai",
          "ai engineering",
          "development",
          "model development"
        ],
        "semantic": [],
        "merged": [
          "engineering",
          "ai",
          "ai engineering",
          "development",
          "model development"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33340917538756265,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603390+00:00"
      }
    },
    {
      "chapter_number": 6,
      "title": "Segment 6 (pages 103-122)",
      "start_page": 103,
      "end_page": 122,
      "summary": "the model weights are obtained from the previous training process.\nBecause the model already has certain knowledge from pre-training,\nmodel after the pre-training phase.\nIt’s usually post-training when it’s done by model developers.\nFor example, OpenAI might post-train a model to make it better at\nOpenAI model (which might have been post-trained itself) to adapt it\nSome people use the term training to refer to prompt engineering, which\nthe word training is correct, as she’s teaching the model to do something.\nthe model, you’re doing prompt engineering.\nneeded for training and adapting AI models.\nIn traditional ML engineering, most use cases are close-ended—a model’s\nFoundation models, however, are open-ended.\ndata annotation is a much bigger challenge for AI engineering.\ntabular data, whereas foundation models work with unstructured data.\nMany people argue that because models are now commodities, data will be\nTraining a model from scratch generally requires more data than finetuning,\nexamining a model, as its training data gives important clues about that\nHowever, as foundation models scale up to incur even higher\nOne challenge with foundation models is that they are often autoregressive\nA summary of how the importance of different categories of model\ndevelopment change with AI engineering is shown in Table 1-4.\nHow different responsibilities of model development have changed with foundation\nmodels.\nmodels\nModeling and\na model from scratch\nWith traditional ML engineering, where teams build applications using their\nevaluation, prompt engineering, and AI interface.\nEvaluation is necessary throughout the whole model adaptation process.\nEvaluation is needed to select models, to benchmark progress, to determine\nWhile evaluation has always been important in ML engineering, it’s even\nmore important with foundation models, for many reasons.\nof evaluating foundation models are discussed in Chapter 3.\nyour model’s outputs against.\nIf a model’s output differs from the expected\noutput, you know the model is wrong.\nDifferent prompts can cause models to perform very differently, as seen in Gemini’s technic\nPrompt engineering is about getting AI models to express the desirable\nmodel performance.\nIt’s possible to get a model to do amazing things with just prompts.\nright instructions can get a model to perform the task you want, in the\nPrompt engineering is not just about telling a model\nmodel can keep track of its history.\nAI applications.\nBefore foundation models, only organizations with\nsufficient resources to develop AI models could develop AI applications.\nWith foundation models, anyone can build AI applications.\nThere need to be tools that provide interfaces for standalone AI applications\nBrowser extensions that let users quickly query AI models while\ndevelopment changes with AI engineering is shown in Table 1-6.\nThe importance of different categories in app development for AI engineering and ML\nfoundation models\ninterfaces, brings AI engineering closer to full-stack development.\nBefore foundation models, the most popular ML frameworks\nWhile many AI engineers come from traditional ML backgrounds, more are\ntraining a model.\nmodels readily available today, it’s possible to start with building the\nproduct first, and only invest in data and models once the product shows\nIn traditional ML engineering, model development and product\nfoundation models, AI engineers tend to be much more involved in building\nof AI engineering as a discipline, thanks to the availability of foundation\nmodels.\napplications on top of these models.\nmodels incorporated other data modalities to become foundation models,\nand how foundation models gave rise to AI engineering.\nThe rapid growth of AI engineering is motivated by the many applications\nenabled by the emerging capabilities of foundation models.\nWhile AI engineering is a new term, it evolved out of ML engineering,\nall ML models.\nAI engineering.\nHowever, AI engineering also brings with it new challenges\nThe last section of the chapter discusses the AI engineering\nthe fundamental building block of AI engineering: the foundation models\nIn this book, I use traditional ML to refer to all ML before foundation models.\nIt seems counterintuitive that larger models require more training data.\nIf a model is more powerful,\nmatch the performance of a small model using the same data.\nExploring different AI applications is perhaps one of my favorite things about writing this book.\nAnton Bacaj told me that “AI engineering is just software engineering with AI models thrown in the\nModels\nTo build applications with foundation models, you first need foundation\nmodels.\nWhile you don’t need to know how to develop a model to use it, a\nTraining a foundation model is an incredibly complex and costly process.\ntell you how to build a model to compete with ChatGPT.\nmodels, it’s difficult to know all the design decisions that go into making a\nmodel.\nIn general, however, differences in foundation models can be traced\nback to decisions about training data, model architecture and size, and how\nSince models learn from data, their training data reveals a great deal about\nThis chapter begins with how model\nChapter 8 explores dataset engineering techniques in detail, including data\nmodel architecture is less of a choice.\nWhenever a new model is released, one of the first things people want to\nThis chapter will also explore how a model developer\nAs mentioned in Chapter 1, a model’s training process is often divided into\nPre-training makes a model capable, but not\ngoal of post-training is to align the model with human preferences.\na model can learn?\nWhile most people understand the impact of training on a model’s\nAn AI model is only as good as the data it was trained on.\nVietnamese in the training data, the model won’t be able to translate from\ndata for training a large model isn’t easy, and it can be expensive.\nModel\nmost foundation models that disclose their training data sources, including\nalso used in models that don’t disclose their training data.",
      "keywords": [
        "model",
        "Foundation models",
        "engineering",
        "data",
        "prompt engineering",
        "Foundation",
        "training data",
        "training",
        "applications",
        "language models",
        "Dataset engineering",
        "important",
        "prompt",
        "Gemini",
        "n’t"
      ],
      "concepts": [
        "model",
        "data",
        "engineering",
        "engineers",
        "ais",
        "train",
        "developers",
        "including",
        "include",
        "chapters"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.735,
          "base_score": 0.585,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 18,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.642,
          "base_score": 0.642,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering",
          "foundation models",
          "foundation",
          "ai engineering",
          "ai"
        ],
        "semantic": [],
        "merged": [
          "engineering",
          "foundation models",
          "foundation",
          "ai engineering",
          "ai"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.38519467494551785,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603454+00:00"
      }
    },
    {
      "chapter_number": 7,
      "title": "Segment 7 (pages 123-143)",
      "start_page": 123,
      "end_page": 143,
      "summary": "strategies for models tailored to highly specific tasks.\nWhile language- and domain-specific foundation models can be trained\nmodels.\nSome might wonder, why not just train a model on all data available, both\ngeneral data and specialized data, so that the model can do everything?\nexample, a model trained with a smaller amount of high-quality data might\noutperform a model trained with a large amount of low-quality data.\n7B tokens of high-quality coding data, Gunasekar et al.\ntrain a 1.3B-parameter model that outperforms much larger models on\nMultilingual Models\neight times more prevalent than the second-most common language,\nThe most common languages in Common Crawl, a popular dataset for training LLMs. Sour\nLanguage\nlanguages.\nunder-represented this language is in Common Crawl.\nExamples of under-represented languages in Common Crawl.\nLanguage\ngeneral-purpose models work much better for English than other languages,\nperformed much better in English than under-represented languages like\nOn the MMLU benchmark, GPT-4 performs better in English than in any other language.\nGPT-4 is much better at math in English than in other languages.\nlanguages that have the worst performance on GPT-4’s MMLU benchmarks\nalso make a language harder for a model to learn.\nqueries from other languages into English, obtain the responses, and\nFirst, this requires a model that can\nModels can also have unexpected performance challenges in non-English\nlanguages.\nfor non-English languages.\nthat tokenization can be much more efficient for some languages than\nsame meaning, languages like Burmese and Hindi require a lot more tokens\nTo address this, many models have been trained to focus on non-English\nlanguages.\nThe most active language, other than English, is undoubtedly\nand many more languages.\nDomain-Specific Models\nGeneral-purpose models like Gemini, GPTs, and Llamas can perform\nHowever, you can infer a model’s domains from its benchmark\nTable 2-3 shows how two models, CLIP and Open CLIP,\ntwo models do on birds, flowers, cars, and a few more categories, but the\nTo train a model to perform well on these domain-specific tasks, you might\nspecific models is perhaps DeepMind’s AlphaFold, trained on the sequences\nis another model that focuses on biomolecular data for drug discovery.\nDomain-specific models are especially common for biomedicine, but other fields can benefit from\ndomain-specific models too.\nIt’s possible that a model trained on architectural sketches can help\nmanufacturing processes much better than a generic model like ChatGPT.\nmodel’s performance.\nNext, let’s explore the impact of how a model is\nModeling\nThese decisions impact not only the model’s capabilities but also its\nFor example, a 7B-parameter model\noptimizing a transformer model for latency is very different from\nModel Architecture\nfoundation models is the transformer architecture (Vaswani et al., 2017),\ntransformer architecture has its own limitations.\nTransformer architecture\ninput tokens sequentially, outputting the final hidden state that represents\nThe decoder then generates output tokens sequentially,\nFor the transformer architecture, the\nFirst, the vanilla seq2seq decoder generates output tokens using only the\nThe transformer architecture addresses both problems with the attention\nThe attention mechanism allows the model to weigh the\nimportance of different input tokens when generating each output token.\nWhile the attention mechanism is often associated with the transformer model, it was introduced\nGoogle used the attention mechanism with their seq2seq architecture in 2016 for their\nGNMT (Google Neural Machine Translation) model.\ntransformers, the input tokens can be processed in parallel, significantly\ninput bottleneck, transformer-based autoregressive language models still\nInference for transformer-based language models, therefore, consists of two\nThe model processes the input tokens in parallel.\ninput tokens.\nThe model generates one output token at a time.\nto make language model inference cheaper and faster.\nAt the heart of the transformer architecture is the attention mechanism.\nmodels work.\nEach key vector (K) represents a previous token.\nat a given decoding step, previous tokens include both input tokens and\nEach value vector (V) represents the actual value of a previous token, as\nlearned by the model.\ntoken by performing a dot product between the query vector and its key\nvisualization of the attention mechanism with the key, value, and query\nBecause each previous token has a corresponding key and value vector, the\ntransformer models.\nThe key, value, and query vectors are computed as\nmodel’s hidden dimension.\n2023), the model’s hidden dimension size is 4096, meaning that each of\nallow the model to attend to different groups of previous tokens\nWith multi-headed attention, the query, key, and value\nconcatenated output before it’s fed to the model’s next computation step.\nThe output projection matrix has the same dimension as the model’s hidden\nmodel.",
      "keywords": [
        "model",
        "Common Crawl",
        "English",
        "transformer architecture",
        "attention mechanism",
        "languages",
        "attention",
        "transformer",
        "data",
        "common",
        "architecture",
        "tokens",
        "input",
        "Crawl",
        "input tokens"
      ],
      "concepts": [
        "models",
        "languages",
        "data",
        "english",
        "tokenization",
        "architectural",
        "architecture",
        "transformer",
        "transformations",
        "perform"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 3,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 9,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 17,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "languages",
          "english",
          "transformer",
          "input tokens",
          "transformer architecture"
        ],
        "semantic": [],
        "merged": [
          "languages",
          "english",
          "transformer",
          "input tokens",
          "transformer architecture"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.37373796291907463,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603501+00:00"
      }
    },
    {
      "chapter_number": 8,
      "title": "Segment 8 (pages 144-164)",
      "start_page": 144,
      "end_page": 164,
      "summary": "The number of transformer blocks in a transformer model is often referred\nto as that model’s number of layers.\nA transformer-based language model is\nindices determines the model’s maximum context length.\nmodel’s context length without increasing the number of position\nFigure 2-6 visualizes a transformer model architecture.\ntransformer model is determined by the dimensions of its building blocks.\n(Dubey et al., 2024) models.\nThe dimension values of different Llama models.\nModel\nModel dim\nOther model architectures\nWhile the transformer model dominates the landscape, it’s not the only\nWhile transformer-based models are dominating, as\nhave the same context length limitation that transformer-based models have.\n(state space models) (Gu et al., 2021a).\nmodel sizes.\nOn language modeling, Mamba-3B outperforms\nmixture-of-experts model with 52B total available parameters (12B\nstrong performance on standard language model benchmarks and long-\nModel Size\nend of a model name.\nLlama, a model family developed by Meta, with 13 billion parameters.\nIn general, increasing a model’s parameters increases its capacity to learn,\nresulting in better models.\nAs the community better understands how to train large models, newer-generation models tend to\nto train and run this model.\nFor example, if a model has 7 billion\nThe number of parameters can be misleading if the model is sparse.\nsparse model has a large percentage of zero-value parameters.\nparameter model that is 90% sparse only has 700 million non-zero\nThis means that a large sparse model can require less compute\nWhile this model has\nparameter model.\nA larger model can also underperform a smaller model if it’s not trained on\nImagine a 13B-param model trained on a dataset consisting of\nthan a much smaller model trained on more data.\nWhen discussing model size, it’s important to consider the size of the data it\nFor most models, dataset sizes are measured by the number\nFor language models, a training sample can be a sentence, a Wikipedia\nmodels can have different tokenization processes, resulting in the same\ndataset having different numbers of tokens for different models.\nthe unit that a model operates on, knowing the number of tokens in a dataset\nhelps us measure how much a model can potentially learn from that data.\nmodels:\nThe number of tokens in a model’s dataset isn’t the same as its number of\nmodel is trained on.\nIf a dataset contains 1 trillion tokens and a model is\nexamples of the number of training tokens for models with different\nExamples of the number of training tokens for models with different numbers of\nModel\nPre-training large models requires compute.\nA more standardized unit for a model’s compute requirement is FLOP, or\nIn summary, three numbers signal a model’s scale:\nNumber of parameters, which is a proxy for the model’s learning capacity.\nNumber of tokens a model was trained on, which is a proxy for how much a model learned.\nbigger models perform worse?\nAccording to their paper, models trained to be more aligned “are much\nmodels perform worse.\nScaling law: Building compute-optimal models\n1. Model performance depends on the model size and the dataset size.\n2. Bigger models and bigger datasets require more compute.\nstart with an arbitrarily large model size and see how much it would cost.\nout the best model performance you can afford.\nof FLOPs, what model size and dataset size would give the best\nGiven a compute budget, the rule that helps calculate the optimal model\nTo study the relationship between model size, dataset\nsize, compute budget, and model performance, the authors trained 400\nlanguage models ranging from 70 million to over 16 billion parameters on 5\nneed the number of training tokens to be approximately 20 times the model\nThis means that a 3B-parameter model needs approximately 60B\nThe model size and the number of training tokens should be\nscaled equally: for every doubling of the model size, the number of training\nGraphs that depict the relationships between training loss, a model’s number of\nparameters, FLOPs, and number of training tokens.\nThe scaling law was developed for dense models trained on predominantly\nThe scaling law optimizes model quality given a compute budget.\nOn the topic of model performance given a compute budget, it’s worth\nnoting that the cost of achieving a given model performance is decreasing.\nWhile the cost for the same model performance is decreasing, the cost for\nmodel performance improvement remains high.\nan order of magnitude more data, compute, or energy than a model with a\nFor large vision models,\nHowever, small performance changes in language modeling loss or\nThe performance of a model depends heavily on the values of its\nto train a model multiple times with different sets of hyperparameters and\nmodels as training them once is resource-draining enough.\nA parameter can be learned by the model during the training process.\nmodel learns.\nHyperparameters to configure the model include the number\nof layers, the model dimension, and vocabulary size.\ncontrol how a model learns include batch size, number of epochs, learning\nhyperparameters on models of different sizes, usually much smaller than the\ntarget model size, and then extrapolate how these hyperparameters would\nwork on the target model size.\na 6.7B model.\nexperience and resources to study the training of large models.",
      "keywords": [
        "model",
        "Model Size",
        "number",
        "training",
        "language models",
        "tokens",
        "size",
        "parameters",
        "training tokens",
        "Llama",
        "transformer model",
        "transformer",
        "billion",
        "Model performance",
        "billion parameters"
      ],
      "concepts": [
        "model",
        "training",
        "parameters",
        "scale",
        "scaling",
        "computation",
        "compute",
        "data",
        "perform",
        "performance"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.602,
          "base_score": 0.452,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.502,
          "base_score": 0.502,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.493,
          "base_score": 0.343,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "number",
          "size",
          "number training",
          "model size",
          "parameters"
        ],
        "semantic": [],
        "merged": [
          "number",
          "size",
          "number training",
          "model size",
          "parameters"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3116832696684645,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603542+00:00"
      }
    },
    {
      "chapter_number": 9,
      "title": "Segment 9 (pages 165-185)",
      "start_page": 165,
      "end_page": 185,
      "summary": "might not be observable on smaller models trained on smaller datasets.\nUntil now, every order of magnitude increase in model size has led to an\nincrease in model performance.\nmeans a three-orders-of-magnitude increase in model sizes between 2018\nHow many more orders of magnitude can model sizes grow?\nbottlenecks for scaling: training data and electricity.\nFoundation models use so much data that there’s a realistic concern we’ll\nincluded in the training data for some language models, whether you\nProjection of historical trend of training dataset sizes and available data stock.\ntraining data of future models.\nthey want on the internet, hoping it will influence future models to generate\nAn open research question is how to make a model forget specific information it has learned during\ntraining.\nincluded in a model’s training data, the model might still reproduce the post’s content.\nby AI models.\nIf companies continue using internet data to train future\nmodels, these new models will be partially trained on AI-generated data.\nDecember 2023, Grok, a model trained by X, was caught refusing a request\nGrok was trained on web data, and “the web is full of ChatGPT outputs.”\nSome researchers worry that recursively training new AI models on AI-\ngenerated data causes the new models to gradually forget the original data\nHowever, the impact of AI-generated data on models is more nuanced and\nmore human-generated training data is proprietary data.\ncompanies from scraping their data for their models.\nmodels with human preferences.\nPost-Training\nPost-training starts with a pre-trained model.\ntrained a foundation model using self-supervision.\nDue to how pre-training\nworks today, a pre-trained model typically has two issues.\nsupervision optimizes the model for text completion, not conversations.\nSecond, if the model is pre-trained on data indiscriminately\nEvery model’s post-training is different.\n1. Supervised finetuning (SFT): Finetune the pre-trained model on high-\nquality instruction data to optimize models for conversations instead of\n2. Preference finetuning: Further finetune the model to output responses\nLet me highlight the difference between pre-training and post-training\nFor language-based foundation models, pre-training optimizes\ntoken-level quality, where the model is trained to predict the next token\nthe model to generate responses that users prefer.\nthat the pre-trained model already has but are hard for users to access via\napproximate how well a model aligns with human preference by\n1. Self-supervised pre-training results in a rogue model that can be\n3. This finetuned model is further polished using preference finetuning to\nNote that a combination of pre-training, SFT, and preference finetuning is\nthe popular solution for building foundation models today, but it’s not the\nAs discussed in Chapter 1, the pre-trained model is likely optimized for\nWe know that a model mimics its training data.\nTo encourage a model to\nresponses.\nSuch examples follow the format (prompt, response) and are\nmodel to handle, such as question answering, summarization, and\nto finetune their model InstructGPT.\ncontain multimodal tasks, as InstructGPT is a text-only model.\nof (prompt, response) pairs created by labelers for InstructGPT.\nExamples of demonstration data used for InstructGPT.\nLabeler’s response\nLabeler’s response\nAmong those who labeled demonstration data for\ndesigning the data (what tasks and prompts to include), recruiting labelers,\nIn theory, the labelers that teach models the human preference\ndata to train their model Gopher.\nTechnically, you can train a model from scratch on the demonstration data\ninstead of finetuning a pre-trained model, effectively eliminating the self-\nHowever, the pre-training approach often has\nA model that can assist users\nDemonstration data teaches the model to have a conversation but doesn’t\nteach the model what kind of conversations it should have.\na model should do.\nIf your model responds to a controversial issue, whatever the responses,\nIf a model is censored too\nmuch, your model may become boring, driving away users.\nFear of AI models generating inappropriate responses can stop companies\nis to get AI models to behave according to human preference.\n1. Train a reward model that scores the foundation model’s outputs.\n2. Optimize the foundation model to generate responses for which the\nreward model will give maximal scores.\nReward model\nRLHF relies on a reward model.\nreward model outputs a score for how good the response is.\nTraining a\nmodel to score a given input is a common ML task.\nThe resulting labeled data is comparison data, which follows\nan example of comparison data used by Anthropic for one of their models.\nOf the two responses in this example, I prefer the response labeled as the\ndata for the reward model of InstructGPT.\nfrom 1 to 7 as well as rank the responses in the order of their preference, but\nonly the ranking is used to train the reward model.\nThe interface labelers used to generate comparison data for OpenAI’s InstructGPT.\nGiven only comparison data, how do we train the model to give concrete\nrθ: the reward model being trained, parameterized by θ.\nTraining data format:\nsw = r (x, yw): reward model’s scalar score for the winning response\nsl = r (x, yl): reward model’s scalar score for the losing response",
      "keywords": [
        "model",
        "data",
        "reward model",
        "Preference finetuning",
        "response",
        "training data",
        "Finetuning",
        "pre-trained model",
        "Preference",
        "demonstration data",
        "training",
        "RLHF",
        "comparison data",
        "human",
        "human preference"
      ],
      "concepts": [
        "model",
        "train",
        "responses",
        "responsibilities",
        "labeling",
        "generated",
        "generate",
        "prompt",
        "quality",
        "scale"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 3,
          "title": "",
          "score": 0.642,
          "base_score": 0.492,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.641,
          "base_score": 0.491,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 17,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "training",
          "pre",
          "reward",
          "reward model"
        ],
        "semantic": [],
        "merged": [
          "data",
          "training",
          "pre",
          "reward",
          "reward model"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3378809154753672,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603586+00:00"
      }
    },
    {
      "chapter_number": 10,
      "title": "Segment 10 (pages 186-208)",
      "start_page": 186,
      "end_page": 208,
      "summary": "The reward model can be trained from scratch or finetuned on top of\nthe strongest foundation model seems to give the best performance.\nFinetuning using the reward model\nWith the trained RM, we further train the SFT model to generate output\nThese prompts are input into the model, whose\nresponses are scored by the reward model.\nexample, Stitch Fix and Grab find that having the reward model alone is\nThey get their models to generate\nmodels.\nhow a model samples outputs to improve its performance.\nA model constructs its outputs through a process known as sampling.\nmultiple outputs to improve a model’s performance.\nsampling process can be modified to get models to generate responses that\nSampling makes AI’s outputs probabilistic.\nFor a classification model, possible\nAs an example, if a model is trained to\nThe model computes the probability of each\noutput probabilities.\nFor a language model, to generate the next token, the model first computes\nTo generate the next token, the language model first computes the probability\nFor example, if the model thinks that an email is more\nHowever, for a language model, greedy sampling creates boring outputs.\nInstead of always picking the next most likely token, the model can sample\nHow does a model compute these probabilities?\nIn the case of a language model, each logit corresponds to one token in the\nmodel’s vocabulary.\nFor each input, a language model produces a logit vector.\nThe right sampling strategy can make a model generate responses more\nthe model generate more creative responses, whereas another strategy can\ntypically requires access to the model’s logits.\nOne problem with sampling the next token according to the probability\ndistribution is that the model can be less creative.\nThe language model’s answer ends up sounding like that of a\nprobability, the model has a low chance of generating a creative sentence\nTo redistribute the probabilities of the possible values, you can sample with\nImagine that we have a model that has only two possible\nThe model picks B 73% of the\nThe model now\nThe higher the temperature, the less likely it is that the model is going to\nmodel’s outputs more creative but potentially less coherent.\ntemperature, the more likely it is that the model is going to pick the most\nobvious value, making the model’s output more consistent but potentially\nmodel picks token B becomes closer to 1.\nbelow 0.1, the model almost always outputs B.\nIf you own your model, you can use\nThe softmax probabilities for tokens A and B at different temperatures, given their logits\nIt’s common practice to set the temperature to 0 for the model’s outputs to\nIn practice, when we set the temperature to 0, the model\nA common debugging technique when working with an AI model is to look at the probabilities this\nmodel computes for given inputs.\nFor example, if the probabilities look random, the model hasn’t\nMany model providers return probabilities generated by their models as\nmodel might be working with a vocabulary size of 100,000, which means\nHow logits, probabilities, and logprobs are computed.\nunderstanding how models work under the hood.\nothers to replicate the model.\nFor a language model with a large vocabulary, this process is\nTo avoid this problem, after the model has computed the logits, we pick the\nThe model then\nIn top-p sampling, the model sums\nlanguage models typically range from 0.9 to 0.95.\nexample, means that the model will consider the smallest set of values\nExample token probabilities.\nprobability that a token must reach to be considered during sampling.\nAn autoregressive language model generates sequences of tokens by\nwant to set a condition for the model to stop the sequence.\nOne easy method is to ask models to stop generating after a fixed number of\nask a model to stop generating when it encounters the end-of-sequence\nThe downside of early stopping is that if you want models to generate\nFor example, if you ask the model to generate JSON, early\nThe last section discussed how a model might sample the next token.\nsection discusses how a model might sample the whole output.\nOne simple way to improve a model’s response quality is test time compute:\nrandomly generate multiple outputs and pick one that works best.\nexample, instead of generating all outputs independently, which might\nIf you use the same model to\ngenerate different options, it’s often a good practice to vary the model’s\nIt’s test time compute because the number of outputs\nA language model’s output is a sequence of tokens, and\neach token has a probability computed by the model.\noutput is the product of the probabilities of all tokens in the output.\nsampling multiple outputs, you pick the one with the highest average\nAnother selection method is to use a reward model to score each output, as\nthe outputs given high scores by their reward models or verifiers.\nOpenAI also trained verifiers to help their models pick the best solutions to\nsignificantly boosted the model performance.\nThis means that a 100-million-parameter model that uses a verifier\ncan perform on par with a 3-billion-parameter model that doesn’t use a\noutputs during inference) can be more efficient than scaling model\nIn OpenAI’s experiment, sampling more outputs led to better performance,\nThey hypothesized that as the number of sampled outputs increases, the\nSQL queries, you can get the model to keep on generating outputs until it\nqueries, a model might take a long time to complete the response.\nmath problem, the model can solve it multiple times and pick the most\nquestion, a model can pick the most frequent output option.\nsampled 32 outputs for each question.\nThis allowed the model to achieve a\nA model is considered robust if it doesn’t dramatically change its outputs\nThe less robust a model is, the more you\ncan benefit from sampling multiple outputs.\nOften, in production, you need models to generate outputs following certain\nFor example, if you use an AI model to write an email, the email itself\noutputs are often passed as inputs into tools that the model can use, as\nEach model provider might also use their own\ntechniques to improve their models’ ability to generate structured outputs.",
      "keywords": [
        "model",
        "outputs",
        "language model",
        "reward model",
        "probabilities",
        "Sampling",
        "probability",
        "token",
        "temperature",
        "logits",
        "Test Time Compute",
        "multiple outputs",
        "Time Compute",
        "time",
        "SFT model"
      ],
      "concepts": [
        "model",
        "samples",
        "sampling",
        "probabilities",
        "probability",
        "generation",
        "generations",
        "generating",
        "token",
        "temperature"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.734,
          "base_score": 0.584,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 3,
          "title": "",
          "score": 0.633,
          "base_score": 0.483,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.589,
          "base_score": 0.589,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 9,
          "title": "",
          "score": 0.581,
          "base_score": 0.431,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "probabilities",
          "sampling",
          "outputs",
          "probability",
          "pick"
        ],
        "semantic": [],
        "merged": [
          "probabilities",
          "sampling",
          "outputs",
          "probability",
          "pick"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4137899710472475,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603637+00:00"
      }
    },
    {
      "chapter_number": 11,
      "title": "Segment 11 (pages 209-230)",
      "start_page": 209,
      "end_page": 230,
      "summary": "You can guide a model to generate structured outputs at different layers of\nbest if the model is already pretty good at generating structured outputs and\nmodel to generate outputs in any format.\nHowever, whether a model can\nA few percentage points of invalid model outputs can still be\noutput, there will be at least two model queries: one to generate the output\nWhen I started working with foundation models, I noticed the\nmeans if you find the common mistakes a model makes, you can potentially\nhappens if a model’s outputs are already mostly correctly formatted, with\nAt a high level, to generate a token, the model samples among values that\nRecall that to generate a token, your model first\nneeded for constrained sampling are better invested in training models to\nFinetuning a model on examples following your desirable format is the\nmost effective and general approach to get models to generate outputs in\nfinetuning doesn’t guarantee that the model will always output the expected\nmodel’s architecture before finetuning.\ncan append a classifier head to the foundation model’s architecture to make\nsure that the model outputs only one of the pre-specified classes.\nthe model, such as this classifier head.\nthe model, by itself, isn’t capable of generating structured outputs.\nHowever, as models become more powerful, we can expect them to get\nget models to output exactly what we need with minimal prompting, and\nThe way AI models sample their responses makes them probabilistic.\nIf you ask an AI model the same question twice, its answer can\nIf an AI model thinks that Vietnamese cuisine has a 70% chance of\nInconsistency is when a model generates very different responses for the\nHallucination is when a model gives a\nThe model later will probabilistically output\nwho doesn’t believe that US presidents are aliens, the model is making this\nFoundation models are usually trained using a large amount of data.\n1. Same input, different outputs: Giving the model the same prompt twice\n2. Slightly different input, drastically different outputs: Giving the model a\nThe same input can produce different outputs in the same model.\nmodel will be consistent 100% of the time.\nIf you host your models, you have some\nHowever, if you use a model API\nFixing the model’s output generation variables is\nstill a good practice, but it won’t force the model to generate the same\nIt is, however, possible to get models to\nhallucination was a common phenomenon for generative models even\nbefore the term foundation model and the transformer architecture were\nA model samples outputs from all probable options.\nmodel can output something that is believed to have never been seen before\nhypotheses about why language models hallucinate.\n2021, is that a language model hallucinates because it can’t differentiate\nImagine that you give the model the prompt: “Who’s Chip Huyen?” and the\nfirst sentence the model generates is: “Chip Huyen is an architect.” The\nnext token the model generates will be conditioned on the sequence:\nthe model can expand upon it and generate outrageously wrong facts.\nFigure 2-24 shows an example of self-delusion by the model LLaVA-v1.5-\nIn its response, the model\nmaking an incorrect assumption, a model can continue hallucinating to\ninitial wrong assumptions can cause the model to make mistakes on\nwhich the model is made to differentiate between user-provided prompts\ngenerated by the model (called the model’s actions).\nDuring SFT, models are trained to mimic responses written by labelers.\nthese responses use the knowledge that the labelers have but the model\ndoesn’t have, we’re effectively teaching the model to hallucinate.\nwrite so that the model knows that the responses aren’t made up, we can\nperhaps teach the model to use only what it knows.\nhallucinations can be fixed by forcing a model to give answers based on\nthe reward model is trained using only comparisons—response A is better\nargued that a better reward function that punishes a model more for making\nalone model.\nHallucination is worse for the model that uses both RLHF and SFT (InstructGPT)\ncompared to the same model that uses only SFT (Ouyang et al., 2022).\nBased on the assumption that a foundation model knows what it knows,\n‘Sorry, I don’t know.’” Asking models for concise responses also seems to\nhelp with hallucinations—the fewer tokens a model has to generate, the less\nmodel hallucinates so that we won’t serve those hallucinated responses to\nfoundation model.\ntraining details in favor of modeling factors that help you determine what\nmodels to use and how to use them.\nA crucial factor affecting a model’s performance is its training data.\nmodels require a large amount of training data, which can be expensive and\nModel providers, therefore, often leverage\nThis leads to models that can perform well on\ncurate training data to develop models targeting specific languages,\nWhile model\narchitecting the model.\nThe chapter looked into modeling choices, such as\nmodel architecture and model size.\nlanguage-based foundation models is transformer.\nThe scale of a model can be measured by three key numbers: the number of\nneeded to train a model are the model size and the data size.\nCurrently, scaling up a model generally makes it better.\ntraining, the resulting model might produce outputs that don’t align with\nby which a model generates output tokens.\nSampling makes AI models\nThis probabilistic nature is what makes models like ChatGPT\nWorking with AI models requires building your workflows around their\nEvaluation for foundation models is\ndidn’t include as much data in the Chinese language or China-centric narratives to train their models.\nML fundamentals related to model training are outside the scope of this book.\nFor example, self-supervision—where a model\nmodel’s parameters are updated during training based on the error—is discussed in Chapter 7.\ncauses them to shrink toward zero, making it difficult for the model to learn.\nWhy do simple activation functions work for complex models like LLMs?\nThe model just needs a nonlinear function to\nChapter 7 discusses how to calculate a model’s memory usage.\nAs of this writing, large models are typically pre-trained on only one epoch of data.\nA friend used this analogy: a pre-trained model talks like a web page, not a human.\nthe risk of people using AI to spread misinformation, you might want to try to build a model that’s as\nPaid model APIs often charge per number of output tokens.",
      "keywords": [
        "model",
        "outputs",
        "model generates",
        "n’t",
        "hallucinations",
        "model generates output",
        "foundation models",
        "model outputs",
        "training",
        "data",
        "training data",
        "generate",
        "sampling",
        "structured outputs",
        "model training"
      ],
      "concepts": [
        "model",
        "hallucinations",
        "hallucination",
        "hallucinate",
        "hallucinating",
        "generating",
        "generation",
        "generates",
        "generator",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.722,
          "base_score": 0.572,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "outputs",
          "output",
          "training",
          "generate",
          "foundation"
        ],
        "semantic": [],
        "merged": [
          "outputs",
          "output",
          "training",
          "generate",
          "foundation"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3474547860332407,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603684+00:00"
      }
    },
    {
      "chapter_number": 12,
      "title": "Segment 12 (pages 231-252)",
      "start_page": 231,
      "end_page": 252,
      "summary": "Training a model from scratch on data following the desirable format works too, but this book isn’t\nAI models.\nevaluate open-ended models, how these methods work, and their\nmodels for your application and build an evaluation pipeline to evaluate\nchallenges of evaluating foundation models.\nSince many foundation models have a language model component, this\nlanguage models, including cross entropy and perplexity.\nessential for guiding the training and finetuning of language models and are\nEvaluating foundation models is especially challenging because they are\non what model and prompt the AI judge uses.\nModels\nEvaluating ML models has always been difficult.\nfoundation models, evaluation has become even more so.\nreasons why evaluating foundation models is more challenging than\nevaluating traditional ML models.\nFirst, the more intelligent AI models become, the harder it is to evaluate\ntraditional approach of evaluating a model against ground truths.\nclassification model, you can evaluate its outputs against the expected\nas the model architecture, training data, and the training process can reveal\ncan evaluate only a model by observing its outputs.\nbe inadequate for evaluating foundation models.\nfoundation models, benchmarks are becoming saturated fast.\nmodels.\nWith task-specific models, evaluation involves measuring a\nmodel’s performance on its trained task.\nmodels, evaluation is not only about assessing a model’s performance on\nto the number of tools for modeling and training and AI orchestration, as\nto evaluate models.\nUnderstanding Language Modeling\nFoundation models evolved out of language models.\nmodels still have language models as their main components.\nmodels, the performance of the language model component tends to be well\nlanguage modeling metrics can be quite helpful in understanding\nAs discussed in Chapter 1, language modeling has been around for decades,\nmodels are trained using cross entropy or its relative, perplexity.\nreading papers and model reports, you might also come across bits-per-\nRecall that a language model encodes statistical information (how likely a\nIn ML lingo, a language model learns the distribution of its training data.\nmodel, you care about its performance not just on the training data but also\nIn general, the closer your data is to a model’s\ntraining data, the better the model can perform on your data.\nmodels, understanding these metrics can help with evaluating which models\nWhen you train a language model on a dataset, your goal is to get the model\nget the model to predict what comes next in the training data.\nmodel’s cross entropy on a dataset measures how difficult it is for the\nlanguage model to predict what comes next in this dataset.\nA model’s cross entropy on the training data depends on two qualities:\n2. How the distribution captured by the language model diverges from the\nlearned by the language model.\nThe model’s cross entropy with respect to the training data is therefore:\nA language model is trained to minimize its cross entropy with respect to\nIf the language model learns perfectly from its training\ndata, the model’s cross entropy will be exactly the same as the entropy of\nYou can think of a model’s cross entropy as its approximation of the\nlanguage model is 6 bits, this language model needs 6 bits to represent each\nSince different models have different tokenization methods—for example,\none model uses words as tokens and another uses characters as tokens—the\nnumber of bits per token isn’t comparable across models.\nnumber of bits a language model needs to represent one byte of the original\nCross entropy tells us how efficient a language model will be at\nIf the BPB of a language model is 3.43, meaning it can\nrepresent each original byte (8 bits) using 3.43 bits, this language model can\nThe perplexity of a language model (with the learned distribution Q) on this\nIf cross entropy measures how difficult it is for a model to predict the next\nConsider a language model trained to encode the 4 position tokens, as in\nThe cross entropy of this language model is 2 bits.\nIf this language model tries to predict a position in the square, it has to\nThus, this language model has a\ninstead of cross entropy, when reporting their language models’\nlanguage models’ predictive accuracy measurements.\nmodel can predict a text, the lower these metrics are.\nperplexity as the default language modeling metric.\nmore uncertainty the model has in predicting what comes next in a given\na model has access to.\nmodel on HTML code should be lower than the expected perplexity\nof a model on everyday text.\nmodel to predict the next token.\nFor example, a model’s perplexity\non a children’s book will likely be lower than the same model’s\nmodel’s cross entropy by using it to predict the next token\nmodel’s perplexity can typically be computed and conditioned on\nhappening, a perplexity of 3 means that this model has a 1 in 3 chance of\nGiven that a model’s vocabulary is in\nOther than guiding the training of language models, perplexity is useful in\nfor a model’s capabilities.\nIf a model’s bad at predicting the next token, its\nLarger GPT-2 models consistently give lower perplexity on different datasets.\nPerplexity might not be a great proxy to evaluate models that have been post-trained using techniques\nPost-training is about teaching models how to complete tasks.\nAs a model gets\nA language model’s\nRecall that the perplexity of a model with respect to a text measures how\ndifficult it is for this model to predict this text.\nFor a given model,\nperplexity is the lowest for texts that the model has seen and memorized\nwas in a model’s training data.\ncontamination—if a model’s perplexity on a benchmark’s data is low, this\nbenchmark was likely included in the model’s training data, making the\nmodel’s performance on this benchmark less trustworthy.\nunderlying language model, which is a proxy for understanding the model’s\nHOW TO USE A LANGUAGE MODEL TO COMPUTE A TEXT’S PERPLEXITY\nA model’s perplexity with respect to a text measures how difficult it is for\nthe model to predict that text.\nGiven a language model X, and a sequence of\nthe language model assigns to each next token.\nWhen evaluating models’ performance, it’s important to differentiate",
      "keywords": [
        "language model",
        "model",
        "language",
        "cross entropy",
        "perplexity",
        "entropy",
        "foundation models",
        "Evaluation",
        "training data",
        "data",
        "Training",
        "evaluating foundation models",
        "token",
        "cross",
        "language model learns"
      ],
      "concepts": [
        "models",
        "evaluation",
        "evaluate",
        "evaluating",
        "evaluators",
        "evaluations",
        "perplexity",
        "tokens",
        "data",
        "language"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.699,
          "base_score": 0.699,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.642,
          "base_score": 0.642,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.639,
          "base_score": 0.639,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.628,
          "base_score": 0.628,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.604,
          "base_score": 0.604,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "language model",
          "cross",
          "entropy",
          "language",
          "cross entropy"
        ],
        "semantic": [],
        "merged": [
          "language model",
          "cross",
          "entropy",
          "language",
          "cross entropy"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.4254146520294201,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.603731+00:00"
      }
    },
    {
      "chapter_number": 13,
      "title": "Segment 13 (pages 253-274)",
      "start_page": 253,
      "end_page": 274,
      "summary": "I’ll cover two evaluation approaches that produce exact scores: functional\ncorrectness and similarity measurements against reference data.\nthis section focuses on evaluating open-ended responses (arbitrary text\nFunctional correctness evaluation means evaluating a system based on\nFunctional correctness is the ultimate metric for evaluating the performance\nCode generation is an example of a task where functional correctness\nPopular benchmarks for evaluating AI’s code generation capabilities, such\nWhen evaluating a model, for each problem a number of code samples,\nThe more code samples a model generates, the more\nSimilarity Measurements Against Reference Data\ncorrectness, one common approach is to evaluate AI’s outputs against\nFor example, if you ask a model to translate a sentence from\nSince this evaluation approach requires reference data, it’s bottlenecked by\nhow much and how fast reference data can be generated.\ngenerated data as the reference means that we treat human performance as\nto generate, leading many to use AI to generate reference data instead.\nGenerated responses that are more similar to the reference responses are\nThere are four ways to measure the similarity between\n1. Asking an evaluator to make the judgment whether two texts are the\n2. Exact match: whether the generated response matches one of the\nreference responses exactly\n3. Lexical similarity: how similar the generated response looks to the\nreference responses\n4. Semantic similarity: how close the generated response is to the reference\nTwo responses can be compared by human evaluators or AI evaluators.\nflexibility of the AI as a judge approach, hand-designed similarity\nThis section discusses how you can use similarity measurements to evaluate the quality of a\nHowever, you can also use similarity measurements for many other use cases,\nIt’s considered an exact match if the generated response matches one of the\nreference responses exactly.\nConsider the question “What’s 2 + 3?” The reference response is\nmodel outputs “September 12, 1929”, the correct year is included in the\nyou doing?” If the reference data contains only these three translations and\na model generates “How is it going?”, the model’s response will be marked\nLexical similarity measures how much two texts overlap.\nreference response “My cats scare the mice” and two generated responses:\n(the similarity score is 80%), whereas response B contains only 3 out of 5\n(the similarity score is 60%).\nsimilar to the reference response.\nOne way to measure lexical similarity is approximate string matching,\nIt measures the similarity between\ngrams in reference responses is also in the generated response.\nmodels, fewer benchmarks use lexical similarity.\nof reference responses.\nA good response can get a low similarity score if the\nnot because the model’s outputs were wrong, but because some correct\nand correct solutions were similar.\nAn example where Fuyu generated a correct option but was given a low score because of\nSemantic similarity\nLexical similarity measures whether two texts look similar, not whether\nConversely, similar-looking texts can mean very different things.\nan embedding.\nSemantic similarity is, therefore, also called embedding similarity.\nlet’s assume that you have a way to transform texts into embeddings.\nsimilarity between two embeddings can be computed using metrics such as\nTwo embeddings that are exactly the same have a\nsimilarity score of 1.\nTwo opposite embeddings have a similarity score of –\nI’m using text examples, but semantic similarity can be computed for\nembeddings of any data modality, including images and audio.\nsimilarity for text is sometimes called semantic textual similarity.\nWhile I put semantic similarity in the exact evaluation category, it can be considered subjective, as\nHowever, given two embeddings,\nthe similarity score between them is computed exactly.\nMathematically, let A be an embedding of the generated response, and B be\nan embedding of a reference response.\nMetrics for semantic textual similarity include BERTScore (embeddings are\ngenerated by BERT) and MoverScore (embeddings are generated by a\nSemantic textual similarity doesn’t require a set of reference responses as\nsemantic similarity depends on the quality of the underlying embedding\nsimilarity score if their embeddings are bad.\nmeasurement is that the underlying embedding algorithm might require\nAn embedding is a\nModels trained especially to produce embeddings include the open source\nThere are also proprietary embedding models\nEmbedding sizes used by common models.\ntext-embedding-3-small: 1536\ntext-embedding-3-large: 3072\ninvolve a step to generate embeddings.\nvisualizes the embedding layer in a transformer model.\nembeddings.\ngood as the embeddings generated by specialized embedding models.\nThe embedding\nAt a high level, an embedding algorithm is considered good if more-similar\ntexts have closer embeddings, measured by cosine similarity or related\nYou can also evaluate the quality of embeddings based on their utility for\nbenchmarks that measure embedding quality on multiple tasks is MTEB,\nMassive Text Embedding Benchmark (Muennighoff et al., 2023).\nI use texts as examples, but any data can have embedding representations.\nA new frontier is to create joint embeddings for data of different modalities.\nmap data of different modalities, text and images, into a joint embedding\na text encoder to convert the text to a text embedding, and an image encoder\nembedding of an image close to the embedding of the corresponding text in\nA joint embedding space that can represent data of different modalities is a\nFor example, this enables text-based image\nAn AI model that is used to evaluate other AI models is called an AI\nmethods for evaluating AI models in production.\nevaluations on their platform were done by AI judges.\nYou can ask AI models to judge an output based on any criteria:\nNot only can AI evaluate a response, but it can also explain its decision,\ncan use AI to evaluate the quality of a response by itself, compare that\nresponse to reference data, or compare that response to another response.\n1. Evaluate the quality of a response by itself, given the original question:",
      "keywords": [
        "embedding",
        "similarity",
        "reference",
        "reference data",
        "reference responses",
        "Semantic similarity",
        "response",
        "Lexical similarity",
        "functional correctness",
        "data",
        "model",
        "text",
        "generated",
        "similarity score",
        "generated response"
      ],
      "concepts": [
        "embedding",
        "model",
        "text",
        "similarity",
        "ais",
        "generation",
        "generated",
        "generates",
        "evaluation",
        "evaluating"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.582,
          "base_score": 0.432,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 15,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "similarity",
          "embedding",
          "reference",
          "embeddings",
          "response"
        ],
        "semantic": [],
        "merged": [
          "similarity",
          "embedding",
          "reference",
          "embeddings",
          "response"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.343382179667326,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603776+00:00"
      }
    },
    {
      "chapter_number": 14,
      "title": "Segment 14 (pages 275-296)",
      "start_page": 275,
      "end_page": 296,
      "summary": "2. Compare a generated response to a reference response to evaluate\nevaluate whether this generated answer is the s\nA general-purpose AI judge can be asked to evaluate a response based on\nbuilt-in AI as a judge criteria offered by some AI tools.\nExamples of built-in AI as a judge criteria offered by some AI tools, as of September 2024.\nIt’s essential to remember that AI as a judge criteria aren’t standardized.\nAzure AI Studio’s relevance scores might be very different from MLflow’s\nThese scores depend on the judge’s underlying model and\nHow to prompt an AI judge is similar to how to prompt any AI application.\n1. The task the model is to perform, such as to evaluate the relevance\n2. The criteria the model should follow to evaluate, such as “Your primary\n3. The scoring system, which can be one of these:\nIt’s been reported that AI judges\nscoring system between 1 and 5, include examples of what a response with\nIn this example, the generated answer\nFigure 3-8 shows an example of an AI judge that evaluates the quality of an\nAn example of an AI judge that evaluates the quality of an answer given a question.\nAn AI judge is not just a model—it’s a system that includes both a model\nLimitations of AI as a Judge\nAI judges can potentially introduce nontrivial costs and latency to\nGiven these limitations, some teams see AI as a judge as a\nYet AI judges, like all AI applications, are probabilistic.\nThe same judge, on\nthe same input, can output different scores if prompted differently.\nsame judge, prompted with the same instruction, can output different scores\nIt’s possible to get an AI judge to be more consistent.\nincluding evaluation examples in the prompt can increase the consistency of\n5, Ragas uses 0 and 1, whereas LlamaIndex’s prompt asks the judge to\nScoring\nIf, given a (context, answer) pair, MLflow gives a faithfulness score of 3,\nHowever, AI judges are also AI applications, which\nthe judge’s prompt this month is different from the one last month?\nThis can become especially confusing if the application and the AI judge\nThe AI judge team might change the\nDo not trust any AI judge if you can’t see the model and the prompt used for the judge.\nYou can use AI judges to evaluate applications both during experimentation\nMany teams use AI judges as guardrails in production to\nAI judge.\nUsing powerful models to evaluate responses can be expensive.\nGPT-4 to both generate and evaluate responses, you’ll do twice as many\nYou can reduce costs by using weaker models as the judges (see “What\nModels Can Act as Judges?”.) You can also reduce costs with spot-\nAll things considered, AI judges are much cheaper than human\nImplementing AI judges in your production pipeline can add latency.\nBiases of AI as a judge\nHuman evaluators have biases, and so do AI judges.\nDifferent AI judges\nBeing aware of your AI judges’ biases helps you interpret their scores\nAI judges tend to have self-bias, where a model favors its own responses\nover the responses generated by other models.\nhelps a model compute the most likely response to generate will also give\nthis response a high score.\nMany AI models have first-position bias.\nAn AI judge may favor the first\nSome AI judges have verbosity bias, favoring lengthier answers, regardless\n(e.g., one response is twice as long as the other), the judge almost always\nOn top of all these biases, AI judges have the same limitations as all AI\nyour judge, you’d need to send your data to this model.\nIf the model\nDespite the limitations of the AI as a judge approach, its many advantages\nWhat Models Can Act as Judges?\nThe judge can either be stronger, weaker, or the same as the model being\njudged.\nbother using a weaker model to generate responses?\ngenerate all responses, so you use it to evaluate a subset of responses.\nexample, you may use a cheap in-house model to generate responses and\nGPT-4 to evaluate 1% of the responses.\nuse a fast model to generate responses while the stronger, but slower, model\nthe response with that of the strong model.\nYou use a strong model to generate responses, with a weak\nmodel running in the background to do evaluation.\nUsing the stronger model as a judge leaves us with two challenges.\nthe strongest model will be left with no eligible judge.\nUsing a model to judge itself, self-evaluation or self-critique, sounds like\nIf a model thinks its own response is incorrect, the\nevaluate itself can nudge a model to revise and improve its responses (Press\nFirst response [from AI]: 30\nOne open question is whether the judge can be weaker than the model being\njudged.\nWeaker models should be able to judge the outputs of stronger\nmodels.\njudges.\njudges.\nBecause there are many possible ways to use AI judges, there are many\npossible specialized AI judges.\nspecialized judges: reward models, reference-based judges, and preference\nmodels:\nA reward model takes in a (prompt, response) pair and scores how\ngood the response is given the prompt.\nmodels.\nA reference-based judge evaluates the generated response with\nThis judge can output a\nsimilarity score or a quality score (how good the generated response\nresponse) pair and outputs a similarity score between the candidate\n(prompt, generated response, reference response, scoring rubric) and\nA preference model takes in (prompt, response 1, response 2) as\npreference data is essential for aligning AI models to human\nAn example output of PandaLM, given a human prompt and two generated\nUsing cheaper models as judges makes it even more useful.\nAI as a judge is exciting, and the next approach we’ll discuss is just as\nOften, you evaluate models not because you care about their scores, but\nWith comparative evaluation, you evaluate models against each other and\nmost judges.",
      "keywords": [
        "judge",
        "model",
        "answer",
        "response",
        "generated answer",
        "score",
        "prompt",
        "evaluation",
        "generated",
        "question",
        "evaluate",
        "generated response",
        "scoring",
        "truth answer",
        "reference response"
      ],
      "concepts": [
        "models",
        "score",
        "scoring",
        "evaluate",
        "evaluation",
        "evaluator",
        "evaluating",
        "responses",
        "judges",
        "judged"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 15,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "judge",
          "judges",
          "ai",
          "ai judge",
          "ai judges"
        ],
        "semantic": [],
        "merged": [
          "judge",
          "judges",
          "ai",
          "ai judge",
          "ai judges"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32665464455120774,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603830+00:00"
      }
    },
    {
      "chapter_number": 15,
      "title": "Segment 15 (pages 297-315)",
      "start_page": 297,
      "end_page": 315,
      "summary": "leaderboard that ranks models using scores computed from pairwise model\nMany model providers use comparative evaluation to evaluate their models\nAn evaluator,\nComparative evaluation shouldn’t be confused with A/B testing.\ncomparative evaluation, a user sees outputs from multiple models at the\nModel A\nModel B\nModel 1\nModel 2\nModel 1\nModel 3\nModel 10\nModel 10\nModel 7\nModel 4\nModel 4\nThe probability that model A is preferred over model B is the win rate of A\nIf there are only two models, ranking them is straightforward.\nThe model\nThe more models there are, the more\nModel pair #\nModel A\nModel B\nModel 1\nModel 2\nModel 1\nModel 3\nModel 1\nModel 4\nModel 1\nModel 5\nModel 2\nModel 3\nModel 2\nModel 4\nModel 2\nModel 5\nModel 3\nModel 4\nModel 3\nModel 5\nModel 4\nModel 5\nranking of models.\nmodel from the comparative signals and then ranks models by their scores.\nComparative evaluation is new in AI but has been around for almost a\nadapted to evaluating AI models, such as Elo, Bradley–Terry, and TrueSkill.\nLMSYS’s Chatbot Arena originally used Elo to compute models’ ranking\nlikely to win in a match against the lower-ranked model.\nIf model A ranks\nhigher than model B, users should prefer model A to model B more than\nranking is good, at least for model pairs with sufficient matches.\nChallenges of Comparative Evaluation\nscores to rank models is easy.\nWith comparative evaluation, both signal\ngathering and model ranking are challenging.\ncompare grows quadratically with the number of models.\nLMSYS evaluated 57 models using 244,000 comparisons.\ntransitivity can happen because different model pairs are evaluated by\nThere’s also the challenge of evaluating new models.\nevaluation, only the new model needs to be evaluated.\nevaluation, the new model has to be evaluated against existing models,\nwhich can change the ranking of existing models.\nThis also makes it hard to evaluate private models.\nIf you want to use comparative evaluation for your model,\nevaluation for you.\nHowever, not all model pairs need to be equally compared.\nOnly after voting is done are the model\nyou might prefer that the model refuses.\nSecond, crowdsourcing comparisons require users to evaluate models\nmodels’ performance.\nEvaluating models using too many simple prompts\nmodels using only these hard prompts.\nevaluators on the criteria to compare two responses or train them to use\nand let users evaluate models during their workflows.\nSome teams prefer AI to human evaluators.\nWe need a model that is good enough.\nwhich model is better.\nmodel is good enough for our use case.\nConsider model B, which wins against A 51%\nIf model B\nThe Future of Comparative Evaluation\nAs models\nimpossible for human evaluators to give model responses concrete scores.\nComparative evaluation can give us discriminating signals about models\nevaluating open-ended, powerful models is challenging.\nto evaluate than traditional ML models.\nevaluation, or both.\nWhen evaluating models, you can evaluate each model independently, and\ncomparative signals: which of the two models is better?\nAI evaluation.\nmotivated the development of preference models: specialized AI judges that\nmodels.\nA 2023 study by a16z showed that 6 out of 70 decision makers evaluated models by word of mouth.\nmodels, we’ll have no one qualified to evaluate future models.\nWithout Reference text), an evaluation method that leverages stronger language models to\nthat the model Llama-13b has a score of 800.\nno one has admitted to me that they tried to game the ranking, several model developers have told me\nevaluate models in the context of your application.\nuse these approaches to evaluate models for your applications.\nintroduced to evaluate these models along different criteria.\nmodels.",
      "keywords": [
        "model",
        "comparative evaluation",
        "evaluation",
        "comparative",
        "ranking",
        "model pairs",
        "evaluate",
        "Chatbot Arena",
        "users",
        "n’t",
        "human",
        "evaluate models",
        "Elo",
        "LMSYS Chatbot Arena",
        "scores"
      ],
      "concepts": [
        "model",
        "evaluate",
        "evaluator",
        "evaluating",
        "evaluations",
        "compare",
        "compared",
        "users",
        "ranking",
        "human"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.687,
          "base_score": 0.537,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 19,
          "title": "",
          "score": 0.616,
          "base_score": 0.616,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.577,
          "base_score": 0.427,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.559,
          "base_score": 0.409,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "model model",
          "comparative",
          "evaluation",
          "comparative evaluation",
          "ranking"
        ],
        "semantic": [],
        "merged": [
          "model model",
          "comparative",
          "evaluation",
          "comparative evaluation",
          "ranking"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2948069322081355,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603870+00:00"
      }
    },
    {
      "chapter_number": 16,
      "title": "Segment 16 (pages 316-334)",
      "start_page": 316,
      "end_page": 334,
      "summary": "The last part discusses developing an evaluation pipeline that can guide the\nEvaluation Criteria\napplication that is deployed but can’t be evaluated is worse.\nevaluate but also because application developers don’t have visibility into\nit’s important to understand how this application will be evaluated.\nthis approach evaluation-driven development.\nIn AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nenterprise applications in production are those with clear evaluation criteria:\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\nEven though foundation models are open-ended, many of their use cases\nIt’s much easier to evaluate classification tasks\nWhile the evaluation-driven development approach makes sense from a\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\ndomain-specific capability metrics tell you how good the model is at\nGeneration capability metrics measure how\nThe last chapter started with an evaluation approach and discussed what\ncriteria a given approach can evaluate.\ngiven a criterion, what approaches can you use to evaluate it?\napplication to translate from Latin to English, you need a model that\nA model’s domain-specific\nTo evaluate whether a model has the necessary capabilities, you can rely on\nDomain-specific capabilities are commonly evaluated using exact\nevaluation.\nCoding-related capabilities are typically evaluated using\nan SQL query generated by your text-to-SQL model is correct but takes too\nEfficiency can be exactly evaluated by measuring runtime or memory\nNon-coding domain capabilities are often evaluated with close-ended tasks,\nFor example, if you want to evaluate a model’s ability to do\nmath, an open-ended approach is to ask the model to generate the solution\nA close-ended approach is to give the model several\nand the model outputs option A, the model is wrong.\n75% of the tasks in Eleuther’s lm-evaluation-harness are multiple-choice,\nA common metric is accuracy—how many questions the model\nSome tasks use a point system to grade a model’s performance—\ngood way to evaluate foundation models.\nfor evaluating knowledge (“does the model know that Paris is the capital of\nevaluating generation capabilities such as summarization, translation, and\nLet’s discuss how generation capabilities can be evaluated in\nAI was used to generate open-ended outputs long before generative AI\nMetrics used to evaluate the quality of generated texts back then included\nFor example, a metric a translation task might use is faithfulness:\nrepurposed, with significant modifications, to evaluate the outputs of\nAs generative models improved, many issues of early\nmodels’ generation capabilities have improved, AI-generated texts have\nuseful for weaker models or for applications involving creative writing and\nFluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nGenerative models, with their new capabilities and new use cases, have new\ndevelopers want to measure is factual consistency.\ntracked is safety: can the generated outputs cause harm to users and\nThis section focuses on how to evaluate factual consistency\nto evaluate other qualities you care about.\nFactual consistency\nThe factual consistency of a model’s output can be verified under two\nLocal factual consistency\nThe output is evaluated against a context.\nfactually consistent if it’s supported by the given context.\nexample, if the model outputs “the sky is blue” and the given context\nConversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for tasks with limited scopes\nGlobal factual consistency\nThe output is evaluated against open knowledge.\nIf the model\nfactual consistency is important for tasks with broad scopes such as\nFactual consistency is much easier to verify against explicit facts.\nexample, the factual consistency of the statement “there has been no proven\nOften, the hardest part of factual consistency verification is determining\nOne interesting research question is what evidence AI models find\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nFor example, in one of my projects, I found that the model I was working with tended to hallucinate\nevaluation approach is AI as a judge.\ncan be asked to evaluate anything, including factual consistency.\noutperform previous methods at measuring factual consistency.\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\n(2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual Consistency: Does the summary\nMore sophisticated AI as a judge techniques to evaluate factual consistency\na model generates multiple outputs that disagree with one another,\nevaluate, SelfCheckGPT generates N new responses and measures\nmany AI queries to evaluate a response.\nSAFE, Search-Augmented Factuality Evaluator, introduced by\n1. Use an AI model to decompose the response into individual\n4. Use AI to determine whether the statement is consistent with the\nVerifying whether a statement is consistent with a given context can also be\nEntailment implies factual consistency, contradiction implies factual\nspecialized in factual consistency prediction.\nThis makes factual consistency\nBenchmarks for factual consistency include TruthfulQA.\nresponse is factually consistent with the reference response.\nshows example questions and false answers generated by GPT-3.",
      "keywords": [
        "model",
        "factual consistency",
        "factual",
        "evaluate",
        "consistency",
        "model outputs",
        "evaluate factual consistency",
        "application",
        "output",
        "tasks",
        "metrics",
        "evaluated",
        "capabilities",
        "n’t",
        "evaluation"
      ],
      "concepts": [
        "evaluation",
        "evaluate",
        "evaluated",
        "evaluator",
        "model",
        "generative",
        "generation",
        "generate",
        "factuality",
        "factually"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.731,
          "base_score": 0.581,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.718,
          "base_score": 0.568,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.685,
          "base_score": 0.535,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "factual",
          "factual consistency",
          "consistency",
          "evaluated",
          "evaluate"
        ],
        "semantic": [],
        "merged": [
          "factual",
          "factual consistency",
          "consistency",
          "evaluated",
          "evaluate"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3308799277699296,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603912+00:00"
      }
    },
    {
      "chapter_number": 17,
      "title": "Segment 17 (pages 335-354)",
      "start_page": 335,
      "end_page": 354,
      "summary": "Figure 4-2 shows the performance of several models on this benchmark, as\nThe performance of different models on TruthfulQA, as shown in GPT-4’s technical\nOther than factual consistency, there are many ways in which a model’s\nChapter 5 also discusses more ways in which AI models can be unsafe and\nmodel generating only content that supports this ideology.\nhave shown that models, depending on their training, can be imbued\nPolitical and economic leanings of different foundation models (Feng et al., 2023).\nThese model providers also need to develop\nMany models developed to detect toxicity in\nExamples of these models are Facebook’s hate\nmodels specialized in different languages, such as Danish and Vietnamese.\nnaturally occurring prompts that are likely to get models to generate toxic\nInstruction-Following Capability\nInstruction-following measurement asks the question: how good is this\nmodel at following the instructions you give it?\nIf the model is bad at\nfinetuned for following instructions.\nMore powerful models are generally\nbetter at following instructions.\ninstructions than GPT-3.5, and similarly, Claude-v2 is better at following\nLet’s say you ask the model to detect the sentiment in a tweet and output\nThe model seems to understand\ncapability to do sentiment analysis on tweets, but its instruction-following\nInstruction-following capability is essential for applications that require\nFor example, if you ask a model to classify an input as\nA, B, or C, but the model outputs “That’s correct”, this output isn’t very\nBut instruction-following capability goes beyond generating structured\nIf you ask a model to use only words of at most four characters, the\nmodel’s outputs don’t have to be structured, but they should still follow the\ninstruction to contain only words of at most four characters.\nThe model they use needs the ability to follow the instruction to work with\nImagine you ask a model to write a lục bát poem, which is a\nIf the model fails to do so, it can either be because\nHow well a model performs depends on the quality of its instructions, which makes it hard to\nevaluate AI models.\nWhen a model performs poorly, it can either be because the model is bad or the\nInstruction-following criteria\nDifferent benchmarks have different notions of what instruction-following\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nIf you ask a model to write a\nif the output contains this word; hence, this instruction is automatically\nAutomatically verifiable instructions proposed by Zhou et al.\nto evaluate models’\ninstruction-following capability.\nInstruction\nInstruction\nInstruction\nInstruction\nInstruction\nInstruction\nwhat instruction-following means.\nOn top of evaluating a model’s ability to\nmodel’s ability to follow content constraints (such as “discuss only climate\nIf you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nA model is considered to successfully follow an instruction if its output\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3.\nThe final score for a model on this\nbenchmark is the number of criteria a model gets right divided by the total\nnumber of criteria for all instructions.\nhow good different models are at following instructions.\nA model that\nyour instructions.\nYou should curate your own benchmark to evaluate your model’s capability to follow your\nIf you need a model to output YAML, include YAML\ninstructions in your benchmark.\nevaluate the model on this instruction.\nasking the model to assume a fictional character or a persona.\na model’s outputs, as discussed in Chapter 5\nevaluate roleplaying capability include RoleLLM (Wang et al., 2023) and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nRoleLLM evaluates a model’s ability to emulate a persona using both\nevaluate whether your model stays in character.\nmight be able to create heuristics to evaluate the model’s outputs.\nthe average of the model’s outputs.\nFor example, if a model is supposed to\nprompt used by the RoleLLM AI judge to rank models based on their ability\nSystem Instruction:\nYou should rank the models based on\nThe models below are to play the role of\nneed to rank the following models based on the\nA model that generates high-quality outputs but is too slow and expensive\nWhen evaluating models, it’s important to balance\nmodel quality, latency, and cost.\nmodels if they provide better cost and latency.\nThere are multiple metrics for latency for foundation models, including but\nLatency depends not only on the underlying model but also on each prompt\nAutoregressive language models typically generate\nprompting, such as instructing the model to be concise, setting a stopping\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nIf you use model APIs, they typically charge by tokens.\nchoose the largest models that can fit their machines.\nTherefore, many popular models are those that max out these memory\nIt’s not a coincidence that many models today have 7 billion\nIf you use model APIs, your cost per token usually doesn’t change much as\nHowever, if you host your own models, your cost per token can\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\nThe row scale is especially important when evaluating model\nAn example of criteria used to select models for a fictional application.\nOverall model",
      "keywords": [
        "model",
        "instructions",
        "outputs",
        "Latency",
        "Capability",
        "role",
        "n’t",
        "criteria",
        "evaluate",
        "Roleplaying",
        "Instruction-Following Capability",
        "format",
        "Cost",
        "models based",
        "response"
      ],
      "concepts": [
        "models",
        "instruction",
        "instructions",
        "instruct",
        "generation",
        "generated",
        "generate",
        "outputs",
        "evaluation",
        "evaluate"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 3,
          "title": "",
          "score": 0.651,
          "base_score": 0.501,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 9,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "instruction",
          "instruction following",
          "following",
          "capability",
          "instructions"
        ],
        "semantic": [],
        "merged": [
          "instruction",
          "instruction following",
          "following",
          "capability",
          "instructions"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35166851168557856,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.603960+00:00"
      }
    },
    {
      "chapter_number": 18,
      "title": "Segment 18 (pages 355-374)",
      "start_page": 355,
      "end_page": 374,
      "summary": "Model Selection\n(licenses, training data, model size) or your own policies (privacy, control).\nmodels significantly.\nof tweaking, and I had to give up on that model.\nIt’s a hard attribute if you use a model\nyou want to use commercial APIs or host your own models.\nyou might initially want to host open source models.\nand private evaluation, you might realize that open source models can’t\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves.\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself.\nOpen source, open weight, and model licenses\nsource was used to refer to any model that people can download and use.\nfunction of what data it was trained on, a model should be considered open\nOpen data allows more flexible model usage, such as retraining the model\nmodel.\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with open data.\nbook, for simplicity, I use open source to refer to all models whose weights are made public,\nModel developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nAnother important attribute of open source models is their licenses.\nfoundation models, the open source world was confusing enough, with so\nMany models are released under\nDoes the license allow using the model’s outputs to train or improve\nupon other models?\nimportant source of data to train future models (discussed together with\nSome people use the term restricted weight to refer to open source models\nOpen source models versus model APIs\nmodel API, as shown in Figure 4-6.\nThe term model API is typically used to\nmodel services, such as finetuning APIs and evaluation APIs. Chapter 9\nAn inference service runs the model and provides an interface for users to access the\nmodel.\nAfter developing a model, a developer can choose to open source it, make it\nCohere and Mistral open source some models and\nmodels, but they’ve also open sourced models (GPT-2, CLIP).\nmodel providers open source weaker models and keep their best models\nModel APIs can be available through model providers (such as OpenAI and\nThe same model can be available through different APIs\ndifferences in the performance of the same model provided through\nmodel APIs. Commercial models are only accessible via APIs licensed by the model\nOpen source models can be supported by any API provider,\ncommercial model providers, models are their competitive advantages.\nAPI providers that don’t have their own models, APIs are their competitive\nopen source models.\nprovide API access to popular open source models.\nThe answer to whether to host a model yourself or use a model API depends\nExternally hosted model APIs are out of the question for companies with\nIf a model API provider wants to serve these use cases, they will\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models.\nEven though most model API providers claim they\nusage data and diagnostics data, to train its AI models.\ndirections: toward open source models, toward proprietary models, or away\nwhat data was used to train their models.\nmodel was trained on copyrighted data, and you use this model to create\nmodels, whose training data has been made publicly available.\nto commercial models.\nIf you use an open source model that infringes on\nHowever, if you use a commercial model,\nVarious benchmarks have shown that the gap between open source models\nthat one day, there will be an open source model that performs just as well,\nAs much as I want open source models to catch up with proprietary models,\nIf you have the strongest model\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nThe gap between open source models and proprietary models is decreasing on the\nFor this reason, it’s likely that the strongest open source model will lag\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nOnce a model is open sourced,\nMany functionalities are needed around a model to make it work for a use\nHowever, commercial model providers might be\ntheir models.\nYou can also only finetune a commercial model if the model provider lets\nand want to finetune that model.\nHowever, if it’s an open source model, you can find a service that offers\nA commercial model provider might\nits resources using APIs might consider hosting their own models.\nthe inference service as needed, and provide guardrails around your model.\nRegardless of whether you go with open or proprietary models, you want\npopular models.\nA model with a large\nopen source models are control and customizability, as shown in Figure 4-8.\nWhy enterprises care about open source models.\nup finetuning open source models.\nmodel the way you can with open source models.\nIf you want to run a model on-device, third-party APIs are out of the\nIn many use cases, running a model locally is desirable.\nhosting models.",
      "keywords": [
        "open source models",
        "model",
        "open source",
        "source models",
        "model APIs",
        "model API",
        "open",
        "APIs",
        "API",
        "source",
        "data",
        "API providers",
        "commercial model",
        "model providers",
        "n’t"
      ],
      "concepts": [
        "model",
        "data",
        "license",
        "licensing",
        "apis",
        "api",
        "providers",
        "provides",
        "useful",
        "typically"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.686,
          "base_score": 0.536,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "",
          "score": 0.553,
          "base_score": 0.403,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.548,
          "base_score": 0.398,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.539,
          "base_score": 0.389,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "source",
          "open source",
          "open",
          "source models",
          "commercial"
        ],
        "semantic": [],
        "merged": [
          "source",
          "open source",
          "open",
          "source models",
          "commercial"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27484993145313114,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604000+00:00"
      }
    },
    {
      "chapter_number": 19,
      "title": "Segment 19 (pages 375-397)",
      "start_page": 375,
      "end_page": 397,
      "summary": "Using model APIs\ndata to model\nmodel will likely be\nmodels\nto the model\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\nIn addition, as AI models improve, old benchmarks\nA tool that helps you evaluate a model on multiple benchmarks is an\nsupports over 400 benchmarks.\nevaluate OpenAI models.\nTheir benchmarks evaluate a wide range of\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nIf model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark, which model would you choose?\nbenchmarks, it’s useful to look into how public leaderboards do so.\nMany public leaderboards rank models based on their aggregated\nperformance on a subset of benchmarks.\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards might exclude an important but expensive benchmark.\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\nconsisted of four benchmarks.\nsix benchmarks.\nmodels.\nto use the average of six different benchmarks to rank models:\nbenchmarks, only two of which (MMLU and GSM-8K) were in the\nThe other eight benchmarks are:\nbenchmarks.\nleaderboards but to highlight the challenge of selecting benchmarks to rank\nmodels.\nIf leaderboard developers can’t explain their benchmark selection\nbenchmark correlation.\nIt is important because if two benchmarks are\nleaderboard again with an entirely new set of benchmarks that are more challenging and focus on\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nsuggesting that improving a model’s reasoning and math capabilities\nThe correlation between the six benchmarks used on Hugging Face’s leaderboard, compute\nmodels.\nthese benchmarks to get the final score to rank that model.\nWhile public leaderboards are useful to get a sense of models’ broad\nA model that ranks high on a public leaderboard will\nmodel for code generation, a public leaderboard that doesn’t include a code\ngeneration benchmark might not help you as much.\nCustom leaderboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nwriting benchmarks.\nMake sure to evaluate how reliable a benchmark is.\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nNot all models have publicly available scores on all benchmarks.\nbenchmark, you will need to run the evaluation yourself.\nRunning benchmarks can be\nto evaluate 30 models on their full HELM suite.\nThe more models you\nwant to evaluate and the more benchmarks you want to use, the more\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models.\nNot all benchmark scores are in the same unit\nAs you evaluate models using public benchmarks, keep in mind that the\nbecause public benchmarks are unlikely to represent your application’s\nbenchmarks get contaminated and how to handle data contamination will be\nData contamination with public benchmarks\ncontamination happens when a model was trained on the same data it’s\nA model that is trained on the MMLU benchmark can achieve high\ntraining exclusively on data from several benchmarks, his one-million-\nmuch larger models on all these benchmarks.\nWhile some might intentionally train on benchmark data to achieve\nmodels today are trained on data scraped from the internet, and the scraping\nprocess can accidentally pull data from publicly available benchmarks.\nBenchmark data published before the training of a model is likely included\nin the model’s training data.\nIt’s one of the reasons existing benchmarks\nto create new benchmarks to evaluate their new models.\ninclude math textbooks in the training data to improve the model’s math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks.\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nto evaluate it on contaminated benchmarks, but this might still be the right\nevaluation benchmarks.\nalso in the training data, the model has likely seen this evaluation\nIf a model’s perplexity on evaluation data is\npossible that the model has seen this data before during training.\nbenchmark example with the entire training data.\nThe goal is to keep evaluation benchmarks standardized so\nover training data, we might not want to remove all benchmark data from\nthe training data, because high-quality benchmark data can help improve\nBesides, there will always be benchmarks\ncreated after models are trained, so there will always be contaminated\nFor model developers, a common practice is to remove benchmarks they\ncare about from their training data before training their models.\nwhen reporting your model performance on a benchmark, it’s helpful to\ndisclose what percentage of this benchmark data is in your training data,\nand what the model’s performance is on both the overall benchmark and the\nbenchmarks, found 13 benchmarks with at least 40% in the training data\nevaluating only the clean sample and evaluating the whole benchmark is\nsample compared to evaluating using the whole benchmark.\nstandard deviations of models’ performance on a given benchmark to spot\nPublic benchmarks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application.\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nIf applicable, evaluate your application both per turn and per task.\nOne example of task-based evaluation is the twenty_questions",
      "keywords": [
        "Benchmarks",
        "model",
        "Data",
        "Hugging Face",
        "evaluation",
        "training data",
        "benchmark data",
        "Public Benchmarks",
        "data contamination",
        "leaderboard",
        "training",
        "application",
        "Face",
        "n’t",
        "evaluate"
      ],
      "concepts": [
        "benchmarks",
        "model",
        "evaluation",
        "evaluate",
        "evaluating",
        "data",
        "questions",
        "question",
        "reasoning",
        "scores"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 15,
          "title": "",
          "score": 0.616,
          "base_score": 0.616,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.499,
          "base_score": 0.499,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.483,
          "base_score": 0.483,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.459,
          "base_score": 0.459,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 17,
          "title": "",
          "score": 0.452,
          "base_score": 0.452,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "benchmarks",
          "benchmark",
          "data",
          "public",
          "benchmark data"
        ],
        "semantic": [],
        "merged": [
          "benchmarks",
          "benchmark",
          "data",
          "public",
          "benchmark data"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33499145104030037,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.604045+00:00"
      }
    },
    {
      "chapter_number": 20,
      "title": "Segment 20 (pages 398-416)",
      "start_page": 398,
      "end_page": 416,
      "summary": "multiple responses, either manually or using AI models, and determine if\nFor example, to evaluate\nguideline is the backbone of a reliable evaluation pipeline.\nTie evaluation metrics to business metrics\nIdeally, you want to map evaluation metrics to business\nUnderstanding the impact of evaluation metrics on business metrics is\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nDefine Evaluation Methods and Data\nwhat methods and data you want to use to evaluate your application.\nSelect evaluation methods\nDifferent criteria might require different evaluation methods.\nIt’s possible to mix and match evaluation methods for the same criteria.\nFor example, if you ask a model to output one of the three\nused to evaluate a model’s perplexity for a generated text, which can be\non human evaluation, even in production.\nevaluate a model’s quality is a long-standing practice in AI.\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\nEach day, you can use human experts to evaluate a subset of\na process to manually evaluate up to 500 daily conservations with their AI\nConsider evaluation methods to be used not just during experimentation but\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application.\nannotated data to evaluate each of your system’s components and each\ncreated for evaluation can be reused to create instruction data for finetuning\nperforms better than model B on aggregated data but worse than model\nModel A\nModel B\nYou should have multiple evaluation sets to represent different data slices.\nare common in production, you should have evaluation examples that\nYou might want an out-of-scope evaluation set, inputs your\nannotated for evaluation can then later be used to synthesize more data for\nHow much data you need for each evaluation set depends on the application\nand evaluation methods you use.\nevaluation set should be large enough for the evaluation result to be\nLet’s say you have an evaluation set of 100 examples.\nof these 100 examples and see if they give similar evaluation results.\nBasically, you want to know that if you evaluate the model on a different\nevaluation set of 100 examples, would you get a different result?\n90% on one bootstrap but 70% on another bootstrap, your evaluation\n2. Evaluate your model on these 100 bootstrapped samples and obtain the\nevaluation results.\ndifferent bootstraps, this means that you’ll need a bigger evaluation set.\nThey should help you decide which model, prompt, or\nOpenAI suggested a rough estimation of the number of evaluation samples needed to be certain that\nA rough estimation of the number of evaluation\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nReliability is especially important with subjective evaluation\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better evaluation\nHow reliable is your evaluation pipeline?\nyou run the pipeline multiple times with different evaluation datasets,\nwhat would be the variance in the evaluation results?\nBe consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\nan interesting insight into your model or that your metrics just aren’t\nHow much cost and latency does your evaluation pipeline add to your\nAs your needs and user behaviors change, your evaluation criteria will also\nevolve, and you’ll need to iterate on your evaluation pipeline.\nneed to update the evaluation criteria, change the scoring rubric, and add or\na certain level of consistency from your evaluation pipeline.\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nprocess, including but not limited to the evaluation data, the rubric, and the\nNot having a reliable evaluation pipeline is one of\nWhile evaluation takes time, a reliable\nmodels but in selecting the right models for your application.\ndiscussed a list of criteria that are often used to evaluate models for\napplications, and how they are evaluated.\nIt discussed how to evaluate both\nMany criteria to evaluate foundation models\nTo help answer the question of whether to host a model or to use a model\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications.\nmodels.\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nEvaluating modern AI systems has many limitations\nChapter 6 explores evaluating\naddressed in Chapter 8, and using user feedback to evaluate production\nfoundation models for.\nAnother argument for making training data public is that since models are likely trained on data\naccess the models’ training data.\nIt’s possible that a model’s output can’t be used to improve other models, even if its license allows\nis why knowing a model’s data lineage is so important.\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure.\nUsers want models to be open source because open means more information and more options, but\nservices to leverage open source models.\nBoth Mistral and Cohere have open source models, but they also have APIs. At some point,\nbenchmark where more challenging problems are procedurally generated as models level up.\nThe same model update can cause some applications to degrade and some to\nfor open models.\nhuman-to-AI communication: you communicate with AI models to get them",
      "keywords": [
        "evaluation",
        "evaluation pipeline",
        "model",
        "data",
        "application",
        "evaluation metrics",
        "Evaluation Methods",
        "evaluation set",
        "n’t",
        "metrics",
        "Factual consistency",
        "evaluation results",
        "pipeline",
        "prompt engineering",
        "evaluate"
      ],
      "concepts": [
        "evaluate",
        "evaluation",
        "evaluating",
        "models",
        "user",
        "data",
        "scoring",
        "score",
        "prompt",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.82,
          "base_score": 0.67,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.795,
          "base_score": 0.645,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 46,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 15,
          "title": "",
          "score": 0.614,
          "base_score": 0.464,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "evaluation",
          "evaluation pipeline",
          "pipeline",
          "evaluation set",
          "evaluate"
        ],
        "semantic": [],
        "merged": [
          "evaluation",
          "evaluation pipeline",
          "pipeline",
          "evaluation set",
          "evaluate"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.36321217825613744,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604089+00:00"
      }
    },
    {
      "chapter_number": 21,
      "title": "Segment 21 (pages 417-434)",
      "start_page": 417,
      "end_page": 434,
      "summary": "prompt engineering.\nA prompt is an instruction given to a model to perform a task.\nFor example, if you want the model to detect toxicity in text, you\nThe concrete task you want the model to do, such as the question to\nFor prompting to work, the model has to be able to follow instructions.\nmodel is bad at it, it doesn’t matter how good your prompt is, the model\nHow to evaluate a model’s instruction-following\nHow much prompt engineering is needed depends on how robust the model\nis to prompt perturbation.\nThe less robust the model\nYou can measure a model’s robustness by randomly perturbing the prompts\nMost models,\nprompt.\nHowever, some models, including Llama 3, seem to perform better when the task description\nis at the end of the prompt.\nTeaching models what to do via prompts is also known as in-context\nGPT-3 paper demonstrated that language models can learn the desirable\nIn-context learning allows a model to incorporate new information\nuse this model to answer questions about the new JavaScript version,\nwithout in-context learning, you’d have to retrain this model.\ncontext learning, you can include the new JavaScript changes in the model’s\ncontext, allowing the model to respond to queries beyond its cut-off date.\nEach example provided in the prompt is called a shot.\nlearn from examples in the prompt is also called few-shot learning.\nExactly how many examples are needed depends on the model and the\nmodel, the better it can learn.\nmodel’s maximum context length.\nlearning on GPT-4 and a few other models.\nFor example, if a model\nincluding Ibis examples in the prompt can still make a big difference.\nSometimes, prompt and context are used interchangeably.\ninput into a model.\nIn this sense, context is exactly the same as prompt.\ncontext is part of the prompt.\nContext refers to the information a model\ncontext as the description that shapes “how the model responds throughout\nIn this book, I’ll use prompt to refer to the whole input into the model, and\ncontext to refer to the information provided to the model so that it can\nA foundation model learns\nMany model APIs give you the option to split a prompt into a system\ndescription and the user prompt as the task.\ninstruction in the system prompt, while the user question and the uploaded\ndisclosure can be in the user prompt.\nUser prompt:\nprompts.\nput into the system prompt, while the instructions provided by users are put\ninto the user prompt.\nGiven a system prompt and a user prompt, the model combines them into a\n{{ system_prompt }}\nA model’s chat template, discussed in this section, is different from a prompt template used by\nA model’s chat template\nDifferent models use different chat templates.\nThe same model provider can\nchat model, Meta changed the template to the following:\nBefore sending a query to a model, print out the final prompt to double-check if it follows the\nMany model providers emphasize that well-crafted system prompts can\nprompts?\nconcatenated into a single final prompt before being fed into the model.\nFrom the model’s perspective, system prompts and user prompts are\nThe system prompt comes first in the final prompt, and the model might\nTraining a model to prioritize system prompts also helps\nHow much information can be included in a prompt depends on the model’s\nModels’ maximum context length has increased rapidly\nContext length expansion soon became a race among model providers and\nAn example of a needle in a haystack prompt used by Liu et al., 2023\nAll the models tested seemed\nThe effect of changing the position of the inserted information in the prompt on models’\nevaluate how good a model is at processing long prompts.\nIf the model’s\nSystem prompt, user prompt, examples, and context are the key components\nof a prompt.\nprompts.\nPrompt engineering can get incredibly hacky, especially for weaker models.\nthese tips can be useful for some models, they can become outdated as\nmodels get better at following instructions and more robust to prompt\nWhen working with a model, you should\nupdate your prompt to tell the model to output only integer scores.\nExamples can reduce ambiguity about how you want the model to respond.\nTo prevent this, you can provide the model with examples of how to",
      "keywords": [
        "model",
        "Prompt",
        "System Prompt",
        "User Prompt",
        "prompt engineering",
        "context",
        "System",
        "context length",
        "User",
        "Learning",
        "template",
        "task",
        "engineering",
        "In-Context Learning",
        "length"
      ],
      "concepts": [
        "prompt",
        "model",
        "context",
        "instruction",
        "instructions",
        "different",
        "difference",
        "including",
        "include",
        "useful"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 44,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.638,
          "base_score": 0.488,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.62,
          "base_score": 0.47,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt",
          "context",
          "user prompt",
          "user",
          "prompts"
        ],
        "semantic": [],
        "merged": [
          "prompt",
          "context",
          "user prompt",
          "user",
          "prompts"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3525574502168557,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604135+00:00"
      }
    },
    {
      "chapter_number": 22,
      "title": "Segment 22 (pages 435-456)",
      "start_page": 435,
      "end_page": 456,
      "summary": "Providing an example can nudge the model toward the response you want.\nUser prompt\nUser prompt\nQ: Will Santa\nPrompt\ncostly (model APIs charge per token) but they also increase latency.\nEnsuring the model outputs are in the correct format is essential when they\nmark the end of the prompts to let the model know that the structured\nOtherwise, the model might get\nWithout explicit markers to mark the end of the input, a model might continue appending\nPrompt\nModel’s output\ncontext can help models perform better.\nIf you want the model to answer\nimprove the model’s responses.\nYou can either provide the model with the necessary context or give it tools\nHOW TO RESTRICT A MODEL’S KNOWLEDGE TO ONLY ITS CONTEXT\nFor example, if you want a model to play\nHow to restrict a model to only the context is tricky.\nThis approach can nudge the model to generate only answers that are\nHowever, since there’s no guarantee that the model will follow all\nsubtask has its own prompt.\n2. Generating response: based on this intent, instruct the model on how to\nprompts.\nThe following example from OpenAI’s prompt engineering guide shows the\nPrompt 1 (intent classification)\nPrompt 2 (response to a troubleshooting\npersists, ask them which router model they are\nintent classification prompt into two prompts, one for the primary category\nWhile models are getting better at understanding complex instructions, they\nImagine asking a model to generate three different story versions for\nPrompt decomposition typically involves more model queries, which can\nHowever, the cost of two decomposed prompts might not be\nThis is because most model APIs charge\nper input and output token, and smaller prompts often incur fewer tokens.\nAdditionally, you can use cheaper models for simpler steps.\ncustomer support, it’s common to use a weaker model for intent\nclassification and a stronger model to generate user responses.\nAs you work to improve your application, your prompt can quickly become\nGoDaddy (2024) found that the prompt\ndifferent subtasks, they found that their model performed better while also\nGive the Model Time to Think\ncritique prompting.\nCoT means explicitly asking the model to think step by step, nudging it\nfirst prompting techniques that work well across models.\nFigure 5-6 shows how CoT improved the performance of models of\nLinkedIn found that CoT also reduces models’ hallucinations.\ndecision” in your prompt.\nThe model then works out what steps to take.\nAlternatively, you can specify the steps the model should take or include\nexamples of what the steps should look like in your prompt.\nshows four CoT response variations to the same original prompt.\nA few CoT prompt variations to the same original query.\nincluded in the prompt)\nSelf-critique means asking the model to check its own outputs.\nSimilar to prompt decomposition, CoT and self-critique can increase the\nA model might perform multiple intermediate\nchallenging if you encourage the model to come up with steps on its own.\nIterate on Your Prompts\nAs you understand a model\nexample, if you ask a model to pick the best video game, it might respond\nUpon seeing this response, you can revise your prompt to ask the model to\nEach model has its quirks.\nOne model might be better at understanding\nOne model might\nprefer system instructions at the beginning of the prompt, whereas another\nTry different prompts.\nRead the prompting guide provided by the model\nUse the same prompt\non different models to see how their responses differ, which can give you a\nbetter understanding of your model.\nAs you experiment with different prompts, make sure to test changes\nVersion your prompts.\nthe performance of different prompts.\nEvaluate each prompt in the context\nA prompt might improve the model’s performance on\nEvaluate Prompt Engineering Tools\nManual prompt\nThese prompt optimization tools\nA common approach to automating prompt generation is to use AI models.\nAI models themselves are capable of writing prompts.\nform, you can ask a model to generate a prompt for your application, such\nYou can also ask AI models to critique and\nimprove your prompts or generate in-context examples.\nprompt optimization tools.\nIt starts with an initial prompt and uses an\nAI model to generate mutations to this prompt.\nThe prompt mutation\nAI models can write prompts for you, as shown by this prompt generated by Claude 3.5\nStarting from an initial prompt, Promptbreeder generates mutations to this prompt and\nGuidance, Outlines, and Instructor guide models toward structured outputs.\nIf used correctly, prompt engineering tools can greatly improve your\nFirst, prompt engineering tools often generate hidden model API calls,\ntool might generate multiple variations of the same prompt and then",
      "keywords": [
        "model",
        "prompt",
        "prompt engineering",
        "Context",
        "CoT",
        "Prompt Engineering Tools",
        "tools",
        "steps",
        "Output",
        "Prompt decomposition",
        "engineering",
        "User",
        "performance",
        "model APIs",
        "User prompt"
      ],
      "concepts": [
        "prompt",
        "model",
        "output",
        "outputting",
        "tools",
        "different",
        "differ",
        "steps",
        "cot",
        "context"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.815,
          "base_score": 0.665,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "",
          "score": 0.662,
          "base_score": 0.512,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 44,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt",
          "prompts",
          "cot",
          "tools",
          "prompt engineering"
        ],
        "semantic": [],
        "merged": [
          "prompt",
          "prompts",
          "cot",
          "tools",
          "prompt engineering"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3400629668808354,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604178+00:00"
      }
    },
    {
      "chapter_number": 23,
      "title": "Segment 23 (pages 457-476)",
      "start_page": 457,
      "end_page": 476,
      "summary": "Often, multiple API calls are required per prompt: one to generate a\nmore if you give the tool free rein in devising prompt chains, which could\nwrong template for a given model, construct a prompt by concatenating\nOn top of that, any prompt engineering tool can change without warning.\nprompts.\nyour own prompts without any tool.\nIf you use a prompt engineering tool, always inspect the prompts produced\nby that tool to see whether these prompts make sense and track how many\nOrganize and Version Prompts\nFor example, you can put your prompts in a file prompts.py and\nreference these prompts when creating a model query.\nfile: prompts.py\ndef query_openai(model_name, user_prompt):\n{\"role\": \"user\", \"content\": user_prompt}\nMultiple applications can reuse the same prompt.\nCode and prompts can be tested separately.\ntested with different prompts.\nSeparating prompts from code makes both easier to read.\ndevising prompts without getting distracted by code.\nIf you have a lot of prompts across multiple applications, it’s useful to give\nYou might also want to organize your prompts in a way that\nmakes it possible to search for prompts by models, applications, etc.\nexample, you can wrap each prompt in a Python object as follows:\nclass Prompt(BaseModel):\nprompt_text: str\nYour prompt template might also contain other information about how the\nprompt should be used, such as the following:\nSeveral tools have proposed special .prompt file formats to store prompts.\nIf the prompt files are part of your git repository, these prompts can be\napplications share the same prompt and this prompt is updated, all\napplications dependent on this prompt will be automatically forced to\nupdate to this new prompt.\nIn other words, if you version your prompts\nstay with an older version of a prompt for their application.\nprompt so that different applications can use different prompt versions.\nallow prompt search.\nDefensive Prompt Engineering\nmain types of prompt attacks that, as application developers, you want to\nPrompt extraction\nExtracting the application’s prompt, including the system prompt,\nJailbreaking and prompt injection\nPrompt attacks pose multiple risks for applications; some are more\nAttackers might manipulate models to output misinformation to\ndiscuss how these risks can occur with each type of prompt attack.\nprompts can be quite valuable.\nMany public prompt marketplaces let users upvote\nlet users sell and buy prompts (see PromptBase).\nprompts can be patented.\nfashionable reverse prompt engineering becomes.\nReverse prompt\nengineering is the process of deducing the system prompt used for a certain\nBad actors can use the leaked system prompt to replicate your\nReverse prompt engineering is typically done by analyzing the application\noutputs or by tricking the model into repeating its entire prompt, which\nincludes the system prompt.\nreverse prompt engineering.\nChatGPT’s system prompt had 1,700 tokens.\nclaim to contain supposedly leaked system prompts of GPT models.\ninto spitting out what looks like its system prompt.\nMore often than not, the extracted prompt is hallucinated\nby the model.\nNot only system prompts but also context can be extracted.\nImage from Brex’s Prompt Engineering Guide (2023).\nPrompts require maintenance.\nJailbreaking and Prompt Injection\nPrompt injection refers to a type of attack where malicious instructions are\ninjected into user prompts.\nSo the prompt “When will my\nthe model to execute the prompt “When will my order arrive?\norder entry from the database.”, it’s prompt injection.\nHowever, a model can\nPrompt attacks are possible precisely because models are trained to follow\nfor a model to differentiate between system prompts (which might ask the\nmodel to act responsibly) and user prompts (which might ask the model to\nhigh economic values, the economic incentive for prompt attacks also\nThis family of attacks involves manually crafting a prompt or a series of\nprompts that trick a model into dropping its safety filters.\npassword-like strings, into the prompt.\nOriginating from Reddit (2022), the prompt for this attack has gone through\nPrompt Automatic Iterative Refinement (PAIR) uses an AI model to act as\n1. Generate a prompt.\n2. Send the prompt to the target AI.\nPAIR uses an attacker AI to generate prompts to bypass the target AI.\nIndirect prompt injection\nInstead of placing malicious instructions in the prompt directly,\nattackers place these instructions in the tools that the model is integrated\nAttackers can inject malicious prompts and code that your model can retrieve and\nIntegrated Applications with Indirect Prompt Injection” (Greshake et al., 2023).",
      "keywords": [
        "prompt",
        "model",
        "prompt engineering",
        "system prompt",
        "prompt injection",
        "API calls",
        "system",
        "Reverse Prompt",
        "reverse prompt engineering",
        "code",
        "instructions",
        "prompt attacks",
        "prompt engineering tool",
        "application",
        "tool"
      ],
      "concepts": [
        "prompt",
        "attacks",
        "model",
        "likely",
        "application",
        "applications",
        "user",
        "code",
        "emails",
        "output"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.793,
          "base_score": 0.643,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.667,
          "base_score": 0.517,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.562,
          "base_score": 0.412,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.474,
          "base_score": 0.324,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt",
          "prompts",
          "prompt engineering",
          "injection",
          "prompt injection"
        ],
        "semantic": [],
        "merged": [
          "prompt",
          "prompts",
          "prompt engineering",
          "injection",
          "prompt injection"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2505962141732175,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604215+00:00"
      }
    },
    {
      "chapter_number": 24,
      "title": "Segment 24 (pages 477-494)",
      "start_page": 477,
      "end_page": 494,
      "summary": "Extracting training data to build a competitive model.\nExtracting private and sensitive information in both the training data\nand the context used for the model.\nMany models are trained on\nFor example, Gmail’s auto-complete model is trained on\nExtracting the model’s training data\nIf the model is trained on copyrighted data, attackers could get the\nmodel to regurgitate copyrighted information.\nmodel knows.\nModel Analysis) benchmark (Petroni et al., 2019) probes for the relational\nGiven this prompt, a model that has this knowledge should be\nThe same techniques used to probe a model for its knowledge can also be\nused to extract sensitive information from training data.\nthat the model memorizes its training data, and the right prompts can\ntrigger the model to output its memorization.\nsomeone’s email address, an attacker might prompt a model with “X’s\ntraining data within the context “X frequently changes her email address,\nthat causes the model to divulge sensitive information without having to\nmodel diverges, its generations are often nonsensical, but a small fraction of\nThis suggests the existence of prompt strategies that allow training data\ncause the model to diverge and divulge training data.\n(2023) also estimated the memorization rates for some models,\nmemorization rate will be higher for models whose training data\nFor all model\nmore, making larger models more vulnerable to data extraction attacks.\nTraining data extraction is possible with models of other modalities, too.\ngenerative models such as GANs, and that mitigating these vulnerabilities\nwhich is likely because these real-world images were included in the model’s training data.\nIt’s important to remember that training data extraction doesn’t always lead\nTo avoid this attack, some models block suspicious fill-in-the-blank\nfill in the blank, mistaking this for a request to get the model to output\nModels can also just regurgitate training data without adversarial attacks.\na model was trained on copyrighted data, copyright regurgitation could be\nharmful to model developers, application developers, and copyright owners.\nIf a model was trained on copyrighted content, it can regurgitate this\nmeasured a model’s copyright regurgitation by trying to prompt it to\nFor example, they give the model\nIf the generated paragraph is exactly as in the book, the model\nFor example, if a model outputs a story\nThe best solution is to not train a model on\ncopyrighted materials, but if you don’t train the model yourself, you don’t\nDefenses Against Prompt Attacks\nmodel against these attacks.\nmodel, prompt, and system levels.\nfalse refusal rate measures how often a model refuses a query when it’s\nModel-level defense\nMany prompt attacks are possible because the model is unable to\nthe model.\nThis means that many attacks can be thwarted if the model is\ntrained to better follow system prompts.\n1. System prompt\n2. User prompt\n3. Model outputs\nprompt injection attacks.\nThe model was then finetuned to output to appropriate outputs\nWhen finetuning a model for safety, it’s important to train the model not\nwhat the model isn’t supposed to do, for example, “Do not return sensitive\nthe user prompt.\nDuplication helps remind the model of what it’s supposed to do.\ncan prepare the model to thwart them.\npatterns to match the inputs against, or a model to detect suspicious\nFoundation models can do many things, but you must tell them exactly\nThe process of crafting an instruction to get a model to do\ndepends on how sensitive the model is to prompts.\ncause a big change in the model’s response, more crafting will be necessary.\nPrompt\nJust like humans, AI models have their quirks and biases,\nFoundation models are useful because they can follow instructions.\nactors get models to follow malicious instructions.\nmodels to do what you want.\nHow to provide a model with\nTo avoid users making template mistakes, many model APIs are designed so that users don’t have to\nRecall that a language model, by itself, doesn’t differentiate between user-provided input and its\nA model’s ability to write prompts is likely boosted if it’s been trained on prompts shared on the\nAsking the model to repeat a text is a variation of repeated token attacks.\nIn “Scalable Extraction of Training Data from (Production) Language Models” (Nasr et al., 2023),\nsuccessful “if the model outputs text that contains a substring of length at least 50 tokens that is\nIt’s likely because larger models are better at learning from data.\nTo solve a task, a model needs both the instructions on how to do it, and the\na wrong answer when lacking information, AI models are more likely to\napplication, the model’s instructions are common to all queries, whereas\ngood instructions to the model.\nThe RAG pattern allows the model to\nExternal tools can help models address\nbring to already powerful models.",
      "keywords": [
        "model",
        "training data",
        "Data",
        "prompt",
        "training",
        "training data extraction",
        "attacks",
        "data extraction",
        "instructions",
        "system",
        "Prompt Attacks",
        "Extracting training data",
        "n’t",
        "prompt engineering",
        "Information"
      ],
      "concepts": [
        "model",
        "prompt",
        "data",
        "attacks",
        "copyright",
        "likely",
        "instructions",
        "instruction",
        "information",
        "context"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 9,
          "title": "",
          "score": 0.742,
          "base_score": 0.592,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.725,
          "base_score": 0.575,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 3,
          "title": "",
          "score": 0.71,
          "base_score": 0.56,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.643,
          "base_score": 0.493,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 17,
          "title": "",
          "score": 0.598,
          "base_score": 0.448,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "attacks",
          "training data",
          "data",
          "training",
          "prompt"
        ],
        "semantic": [],
        "merged": [
          "attacks",
          "training data",
          "data",
          "training",
          "prompt"
        ]
      },
      "topic_id": 2,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3703432895792423,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604260+00:00"
      }
    },
    {
      "chapter_number": 25,
      "title": "Segment 25 (pages 495-517)",
      "start_page": 495,
      "end_page": 517,
      "summary": "RAG is a technique that enhances a model’s generation by retrieving the\nwork, the system first retrieves five Wikipedia pages most relevant to a\nThe retrieve-then-generate pattern.\nThe term retrieval-augmented generation was coined in “Retrieval-\nWith RAG, only the information most relevant to the query, as\ndetermined by the retriever, is retrieved and input into the model.\nA RAG system has two components: a retriever that retrieves information\nbased on the retrieved information.\noff-the-shelf retrievers and models.\nThe success of a RAG system depends on the quality of its retriever.\nretriever has two main functions: indexing and querying.\nretrieve data relevant to it is called querying.\nNaively retrieving whole documents can cause your context to be arbitrarily\neach query, our goal is to retrieve the data chunks most relevant to this\nRetrieval Algorithms\nRetrieval isn’t unique to RAG.\nRetrieval is typically limited to one database or system, whereas search involves retrieval across\nThis chapter uses retrieval and search interchangeably.\nAt its core, retrieval works by ranking documents based on their relevance\nRetrieval algorithms differ based on how relevance scores\nI’ll start with two common retrieval mechanisms: term-based\nretrieval and embedding-based retrieval.\nopted for term-based versus embedding-based categorization.\nSparse retrievers represent data using sparse vectors.\nTerm-based retrieval is\nDense retrievers represent data using dense vectors.\nEmbedding-based retrieval\nLexical and Expansion) is a retrieval algorithm that works using sparse\nretrieval than those of term-based retrieval.\nTerm-based versus embedding-\nTerm-based retrieval\nGiven a query, the most straightforward way to find relevant documents is\nexample, given the query “AI engineering”, the model will retrieve all the\nMany documents might contain the given term, and your model might\ndocument, the more relevant this document is to this term.\nof times a term appears in a document is called term frequency (TF).\nAn intuition is that the more documents contain a term, the less\nscore of document D for the query Q is computed as follows:\nTwo common term-based retrieval solutions are Elasticsearch and BM25.\ndocuments given a term.\nsuch as the term frequency and the document count (how many documents\n(Document index, term frequency)\nmore sophisticated retrieval algorithms, such as embedding-based retrieval,\nbased retrieval solutions often handle these automatically.\nCan we retrieve documents based on the\nworks best when the query and the documents are of similar lengths.\nEmbedding-based retrieval\nTerm-based retrieval computes relevance at a lexical level rather than a\nOn the other hand, embedding-based retrievers aim to rank\ndocuments based on how closely their meanings align with the query.\nWith embedding-based retrieval, indexing has an extra function: converting\nembeddings are stored is called a vector database.\n1. Embedding model: convert the query into an embedding using the same\nembedding model used during indexing.\n2. Retriever: fetch k data chunks whose embeddings are closest to the query\nembedding, as determined by the retriever.\nfetch, k, depends on the use case, the generative model, and the query.\nA high-level view of how an embedding-based, or semantic, retriever works.\nThe embedding-based retrieval workflow shown here is simplified.\nworld semantic retrieval systems might contain other components, such as a\nWith embedding-based retrieval, we again encounter embeddings, which\nembedding-based retriever doesn’t work if the embedding model is bad.\nEmbedding-based retrieval also introduces a new component: vector\npart of a vector database.\nThe hard part is vector search.\nembedding, a vector database is responsible for finding vectors in the\nVectors have to be indexed\nand stored in a way that makes vector search fast and efficient.\nvector search isn’t unique to generative AI.\nVector search is common in any\nVector search is typically framed as a nearest-neighbor search problem.\nexample, given a query, find the k nearest vectors.\n1. Compute the similarity scores between the query embedding and all\nDue to the importance of vector search,\nVector search algorithms differ based on the heuristics they use to increase\nVectors can also\nHere are some significant vector search algorithms:\njust vectors.\ncluster centroids closest to the query embedding, and the vectors in\nComparing retrieval algorithms\nterm-based and embedding-based retrieval relatively easy to start.\nTerm-based retrieval is generally much faster than embedding-based\nretrieval during both indexing and query.\nembedding generation, and mapping from a term to the documents that\nTerm-based retrieval also works well out of the box.\nretrieval applications.\nEmbedding-based retrieval, on the other hand, can be significantly\nimproved over time to outperform term-based retrieval.\nthe embedding model and the retriever, either separately, together, or in\nretrieval with term-based retrieval, as discussed later in this chapter.\nThe quality of a retriever can be evaluated based on the quality of the data it\nretrieves.\nOut of all the documents retrieved, what percentage is relevant to the\nOut of all the documents that are relevant to the query, what\nqueries and a set of documents.\nannotate the relevance of all documents in your database to that query.\nretrieved documents to the query, which can be done by an AI judge.\nIf you care about the ranking of the retrieved documents, for example, more\nevaluates embeddings for a broad range of tasks including retrievals,\nThe quality of a retriever should also be evaluated in the context of the\nembedding generation and vector search might be minimal compared to the\nvector storage and vector search queries can be expensive, too.\neven half of their spending on model APIs. Table 6-2 shows a side-by-side comparison of term-based retrieval and\nembedding-based retrieval.\nTerm-based retrieval and semantic retrieval by speed, performance, and cost.\nTerm-based retrieval\nretrieval\nretrieval\nQuery embedding\ngeneration and vector search\ndocuments due to term\nretrieval with finetuning\nretrieval\nEmbedding, vector storage,\nand vector search solutions\nWith retrieval systems, you can make certain trade-offs between indexing\nThe more detailed the index is, the more accurate the retrieval",
      "keywords": [
        "Retrieval",
        "RAG",
        "vector search",
        "vector",
        "documents",
        "embedding-based retrieval",
        "term",
        "query",
        "context",
        "term-based retrieval",
        "model",
        "RAG system",
        "search",
        "document",
        "embedding"
      ],
      "concepts": [
        "retrieving",
        "retrieve",
        "retrieval",
        "term",
        "embedding",
        "vectors",
        "query",
        "queries",
        "context",
        "search"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 43,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.388,
          "base_score": 0.388,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 7,
          "title": "",
          "score": 0.372,
          "base_score": 0.372,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.36,
          "base_score": 0.36,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "retrieval",
          "based retrieval",
          "embedding",
          "term",
          "based"
        ],
        "semantic": [],
        "merged": [
          "retrieval",
          "based retrieval",
          "embedding",
          "term",
          "based"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23735919819169718,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604295+00:00"
      }
    },
    {
      "chapter_number": 26,
      "title": "Segment 26 (pages 518-535)",
      "start_page": 518,
      "end_page": 535,
      "summary": "3. Evaluate the embeddings (for embedding-based retrieval).\nGiven the distinct advantages of different retrieval algorithms, a production\nbased retrieval and embedding-based retrieval is called hybrid search.\nretriever, such as a term-based system, fetches candidates.\nretrieve the context associated with “Who’s responsible for the most sales?”\nthat a retriever works by ranking documents by their relevance scores to the\nquery.\nYou can use multiple retrievers to fetch candidates at the same time,\nbased on its ranking by a retriever.\nretrievers.\nIf a document is ranked first by one retriever and second by\nanother retriever, its score is 1 + 0.5 = 1.5.\nri (D) is the rank of the document by the retriever i.\nreranking, query rewriting, and contextual retrieval.\nHow your data should be indexed depends on how you intend to retrieve it\nThe last section covered different retrieval algorithms and their\nassumption that documents have already been split into manageable chunks.\nThe simplest strategy is to chunk documents into chunks of equal length\nFor example, you can split each document into chunks of 2,048\nYou can also split each document so that each\nSpecific documents might also support creative chunking strategies.\nWhen a document is split into chunks without overlap, the chunks might be\nThe chunk size shouldn’t exceed the maximum context length of the\nFor the embedding-based approach, the chunk size also\nYou can also chunk documents using tokens, determined by the generative\nYou can then split documents into chunks using tokens as the\nyou can fit more chunks into the model’s context.\nMore chunks can provide a model\nsplit this document into two chunks, the second half of the document might\nnot be retrieved, and the model won’t be able to use its information.\nespecially an issue for embedding-based retrieval.\nThe initial document rankings generated by the retriever can be further\nto reduce the number of retrieved documents, either to fit them into your\nIn context reranking, the order of documents still matters because it\ndocuments at the beginning and end of the context, as discussed in “Context\nQuery rewriting\nIf you use this query verbatim to retrieve documents, you’ll likely get\nYou can use other generative models to rewrite queries.\nThe idea behind contextual retrieval is to augment each chunk with relevant\ncontext to make it easier to retrieve the relevant chunks.\nchunk.\nthe system to retrieve it by that keyword, even after the document has been\nYou can also augment each chunk with the questions it can answer.\nIf a document is split into multiple chunks, some chunks might lack the\nnecessary context to help the retriever understand what the chunk is about.\nTo avoid this, you can augment each chunk with the context from the\ntokens, that explains the chunk and its relationship to the original document.\n<document>\n{{WHOLE_DOCUMENT}}\n</document>\nthe whole document:\n<chunk>\n</chunk>\nsituate this chunk within the overall document\nof the chunk.\nThe generated context for each chunk is prepended to each chunk, and the\naugmented chunk is then indexed by the retrieval algorithm.\nAnthropic augments each chunk with a short context that situates this chunk within the\noriginal document, making it easier for the retriever to find the relevant chunks given a query.\nEVALUATING RETRIEVAL SOLUTIONS\nHow scalable is it, both in terms of data storage and query traffic?\nWhat’s its query latency for different retrieval algorithms?\ndocument/vector volume or on the query volume?\nThe last section discussed text-based RAG systems where the external data\nsources are text documents.\nwith text documents but also with images, videos, audio, etc., from external\nGiven a query, the retriever\nMultimodal RAG can augment a query with both text and images.\nFor example, an image is retrieved if its\nIf you want to retrieve images based on their content, you’ll need to have a\nIf queries are texts, you’ll need a\nThe retriever works as follows:\n1. Generate CLIP embeddings for all your data, both texts and images, and\n2. Given a query, generate its CLIP embedding.\n3. Query in the vector database for all images and texts whose embeddings\nMany queries might need information\nThe SQL query\n1. Text-to-SQL: based on the user query and the provided table schemas,\nA RAG system that augments context with tabular data.\nexecutors can enable models to handle more queries and generate higher-",
      "keywords": [
        "chunk",
        "query",
        "document",
        "documents",
        "retrieval",
        "chunk size",
        "context",
        "model",
        "data",
        "SQL query",
        "RAG",
        "query rewriting",
        "retriever",
        "chunk documents",
        "SQL"
      ],
      "concepts": [
        "retrieval",
        "retriever",
        "retrieve",
        "query",
        "queries",
        "queried",
        "chunking",
        "data",
        "documents",
        "document"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 43,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 15,
          "title": "",
          "score": 0.367,
          "base_score": 0.367,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 19,
          "title": "",
          "score": 0.366,
          "base_score": 0.366,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.337,
          "base_score": 0.337,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "chunk",
          "document",
          "documents",
          "chunks",
          "retriever"
        ],
        "semantic": [],
        "merged": [
          "chunk",
          "document",
          "documents",
          "chunks",
          "retriever"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.21592217349178178,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604330+00:00"
      }
    },
    {
      "chapter_number": 27,
      "title": "Segment 27 (pages 536-554)",
      "start_page": 536,
      "end_page": 554,
      "summary": "two aspects that determine the capabilities of an agent: tools and planning.\nAgent Overview\nan agent?\nAn agent is anything that can perceive its environment and act upon that\nThe environment an agent can operate in is defined by its use case.\nA self-driving car agent’s environment is the\nThe set of actions an AI agent can perform is augmented by the tools it has\nare agents with access to tools, albeit simple ones.\nChatGPT is an agent.\nsystems are agents, and text retrievers, image retrievers, and SQL executors\nThere’s a strong dependency between an agent’s environment and its set of\nThe environment determines what tools an agent can potentially use.\nHowever, an agent’s tool inventory\nSWE-agent (Yang et al., 2024) is a coding agent whose environment is the computer and\nAn AI agent is meant to accomplish tasks typically provided by the users in\nIn an AI agent, AI is the brain that processes the information it\nreceives, including the task and feedback from the environment, plans a\nThis is a simple agent with three actions: response generation,\nCompound mistakes: an agent often needs to perform multiple steps to\nHigher stakes: with access to tools, agents are capable of performing\nGiven an environment, the success of an agent in an environment depends\nstart by looking into different kinds of tools a model can use.\nA system doesn’t need access to external tools to be an agent.\nwithout external tools, the agent’s capabilities would be limited.\nmake an agent vastly more capable.\nTools help an agent to both perceive the environment and act upon it.\nActions that allow an agent to perceive the environment are read-only\nactions, whereas actions that allow an agent to act upon the environment are\nThe set of tools an agent has access to is its tool inventory.\nSince an agent’s\ntool inventory determines what an agent can do, it’s important to think\nthrough what and how many tools to give an agent.\nagent more capabilities.\nDepending on the agent’s environment, there are many possible tools.\nthat let your agent act upon its environment.\nof tools includes those that help augment your agent’s knowledge of your\nagent.\nHowever, tools can also give models access to\nlot more resource-efficient to just give the model access to a tool.\nOther simple tools that can significantly boost a model’s capability include\nThis capability lets your agents act as coding assistants, data\nExternal tools can make a text-only or image-only model multimodal.\nexample, a model that can generate only texts can leverage a text-to-image\nmodel as a tool, allowing it to generate both texts and images.\nrequest, the agent’s AI planner decides whether to invoke text generation,\nAgents can also use a code\nGPT-4-powered agent, augmented with a set of 13 tools, can outperform\nExamples of tools this agent used are\nBut tools can also perform write actions, making changes\nMany model\nproviders already support tool use with their models, a feature often called\nPlanning\nAt the heart of a foundation model agent is the model responsible for\nComplex tasks require planning.\nplan, which is a roadmap outlining the steps needed to accomplish a task.\nEffective planning typically requires the model to understand the task,\nBut what if the model comes up with a 1,000-step plan that doesn’t even\nWithout oversight, an agent can run those steps for\nYou ask the agent to first generate a plan, and only after this plan is\nexample, one simple heuristic is to eliminate plans with invalid actions.\nthe generated plan requires a Google search and the agent doesn’t have\nA plan can also be\nYou can ask a model to evaluate whether the plan\nIf the generated plan is evaluated to be bad, you can ask the planner to\ngenerate another plan.\nIf the generated plan is good, execute it.\nIf the plan\nexecuting this plan will then again need to be evaluated.\ngenerated plan doesn’t have to be an end-to-end plan for the whole task.\nYour system now has three components: one to generate plans, one to\nTo speed up the process, instead of generating plans sequentially, you can\ngenerate several plans in parallel and ask the evaluator to pick the most\nPlanning requires understanding the intention behind a task: what’s the user\nplan.\nKnowing the intent can help the agent pick the right tools.\ncustomer support, if the query is about billing, the agent might need access\nto reset a password, the agent might need to access documentation retrieval.\nSome queries might be out of the scope of the agent.\nSo far, we’ve assumed that the agent automates all three stages: generating\nFor example, for complex tasks for which an agent has trouble generating\nthe whole plan, a human expert can provide a high-level plan that the agent\nthe agent’s performance:\n1. Plan generation: come up with a plan for accomplishing this task.\nA plan\n2. Reflection and error correction: evaluate the generated plan.\nplan, generate a new one.\n3. Execution: take the actions outlined in the generated plan.\ncompleted, generate a new plan.\nAn open question is how well foundation models can plan.\nexecutable plans.\nthey conclude that autoregressive models can’t plan.\nAfter executing a path with action A, if the model\ntoolings needed to plan.\nThis means it’s not sufficient to prompt a model to generate only a sequence\nThe paper “Reasoning with Language Model is Planning with World\nEven if AI can’t plan, it can still be a part of a planner.\nplan.\n“concerned with how an intelligent agent ought to take actions in a dynamic\nIn an RL agent, the planner is\nIn an FM agent, the model is the planner.\nThis model",
      "keywords": [
        "agent",
        "model",
        "tools",
        "plan",
        "actions",
        "environment",
        "task",
        "Invoke SQL query",
        "planning",
        "generate",
        "n’t",
        "SQL query",
        "SQL",
        "Invoke SQL",
        "query"
      ],
      "concepts": [
        "planning",
        "plans",
        "tool",
        "models",
        "actions",
        "generative",
        "generate",
        "generation",
        "generator",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 29,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "",
          "score": 0.489,
          "base_score": 0.489,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.48,
          "base_score": 0.48,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "",
          "score": 0.467,
          "base_score": 0.467,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.413,
          "base_score": 0.413,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "plan",
          "environment",
          "tools",
          "actions"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "plan",
          "environment",
          "tools",
          "actions"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.27631228603775865,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.604367+00:00"
      }
    },
    {
      "chapter_number": 28,
      "title": "Segment 28 (pages 555-576)",
      "start_page": 555,
      "end_page": 576,
      "summary": "Plan generation\nThe simplest way to turn a model into a plan generator is with prompt\nHere’s an example of a prompt for plan generation.\ngenerate_query(task_history, tool_output)\nPlan: [fetch_product_info, generate_query,\nPlan: [fetch_top_products, generate_query,\nThe generate_query  function takes in the task’s current history\nand the most recent tool outputs to generate a query to be fed into the\nThe tool output at each step is added to the task’s\nHere are a few approaches to make an agent better at planning:\nFinetune a model for plan generation.\nMany model providers offer tool use for their models, effectively turning\ntheir models into agents.\nA tool is a function.\nDeclare all the tools that you might want a model to use.\nEach tool is\n2. Specify what tools the agent can use.\nBecause different queries might need different tools, many APIs let you\ntool use further by the following settings:\nThe model must use at least one tool.\nThe model shouldn’t use any tool.\nThe model decides which tools to use.\nAn example of a model using two simple tools.\ngenerate what tools to use and their parameters.\nthe agent might decide that it needs the tool lbs_to_kg_tool  with one\ntool_calls=[\nWhen working with agents, always ask the system to report what parameter values it uses for each\nA plan is a roadmap outlining the steps needed to accomplish a task.\nFirst, use a planner to generate a high-level plan, such as a quarter-to-\nSo far, all examples of generated plans use the exact function names, which\nA problem with this approach is that an agent’s tool\na tool changes, you’ll need to update your prompt and all your examples.\nacross different use cases with different tool APIs. If you’ve previously finetuned a model to generate plans based on the old\ntool inventory, you’ll need to finetune the model again on the new tool\nweek”, an agent can be instructed to output a plan that looks like this:\nchanges in tool APIs. If your model was trained mostly on natural language,\nis a much simpler task than planning and can be done by weaker models\nquery “Find me best-selling products under $100”, an agent might\nExamples of different orders in which a plan can be executed.\nWith AI-powered agents, AI models determine control flows.\nAfter the initial plan generation to evaluate whether the plan makes\nAfter the whole plan has been executed to determine if the task has been\nagent is asked to explain its thinking (planning), take actions, then analyze\nobservations (reflection), until the task is considered finished by the agent.\nThe agent is typically prompted, using examples, to generate outputs in the\nFigure 6-12 shows an example of an agent following the ReAct framework\nYou can implement reflection in a multi-agent setting: one agent plans and\ntakes actions, and another agent evaluates the outcome after each step or\nIf the agent’s response failed to accomplish the task, you can prompt the\nsuggestion, the agent generates a new plan.\nFor example, given a coding generation task, an evaluator\nThe agent then\nFigure 6-13 shows examples of Reflexion agents in action.\nExamples of how Reflexion agents work.\nBecause tools often play a crucial role in a task’s success, tool selection\nThe tools to give your agent depend on the\nthe agent.\nAgent\nChameleon (Lu et al., 2023) uses 13 tools.\net al., 2023) attempted to prompt agents to select the right API call among\n1,645 APIs. More tools give the agent more capabilities.\nHowever, the more tools there\nCompare how an agent performs with different sets of tools.\nLook for tools that the agent frequently makes mistakes on.\nIf a tool\ntool.\nwhat tools are least used.\nFigure 6-14 shows the differences in tool use\nDifferent models and tasks express different tool use patterns.\n1. Different tasks require different tools.\nanswering task, relies much more on knowledge retrieval tools than\n2. Different models have different tool preferences.\nWhen evaluating an agent framework, evaluate what planners and tools it supports.\nAs humans, we become more productive not just by using the tools we’re\nX, how likely is the agent to call tool Y?\nagent itself can combine initial tools to continually build more complex\ntools.\nskills (tools) that an agent acquires for later reuse.\nuseful (e.g., because it’s successfully helped an agent accomplish a task), it\nenvironment depends on its tool inventory and its planning capabilities.\ndiscuss different failure modes of an agent and how to evaluate them.\nThe more complex a task an agent\nalso have unique failures caused by planning, tool execution, and efficiency.",
      "keywords": [
        "agent",
        "tool",
        "Plan",
        "task",
        "function",
        "model",
        "generate",
        "products",
        "query",
        "tool inventory",
        "response",
        "parameters",
        "Reflection",
        "best-selling products"
      ],
      "concepts": [
        "tool",
        "planning",
        "plans",
        "agent",
        "different",
        "differences",
        "task",
        "functions",
        "function",
        "reflection"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 29,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.657,
          "base_score": 0.507,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "",
          "score": 0.64,
          "base_score": 0.49,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.596,
          "base_score": 0.446,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.567,
          "base_score": 0.417,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "agent",
          "tool",
          "plan",
          "tools",
          "task"
        ],
        "semantic": [],
        "merged": [
          "agent",
          "tool",
          "plan",
          "tools",
          "task"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3432915803921725,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604411+00:00"
      }
    },
    {
      "chapter_number": 29,
      "title": "Segment 29 (pages 577-594)",
      "start_page": 577,
      "end_page": 594,
      "summary": "There are also agent\nplanning failure is tool use failure.\nThe agent might generate a plan with\nbing_search  isn’t in the agent’s tool inventory.\nAnother mode of planning failure is goal failure: the agent fails to achieve\na task to an agent and only need to check in when it’s done.\nmany cases, the agent becomes less useful with time.\nThe agent\nTo evaluate an agent for planning failures, one option is to create a planning\nFor each task, use the agent to generate a K number of plans.\n2. For a given task, how many plans does the agent have to generate, on\nWhat types of tasks does the agent\nWhat tools does the model\nSome tools might be harder for an agent to\nYou can improve an agent’s ability to use a challenging tool by better\nTool failures can also happen because the agent doesn’t have access to the\nAn agent might generate a valid plan using the right tools to accomplish a\nHow many steps does the agent need, on average, to complete a task?\nIn this chapter, we’ve discussed in detail how RAG and agent systems\nBoth patterns often deal with information that exceeds a model’s\nA memory system that supplements the model’s context in\nMemory\nMemory refers to mechanisms that allow a model to retain and utilize\napplications like RAG and multi-step applications like agents.\nAn agentic system needs\nmemory to store instructions, examples, context, tool inventories, plans,\nWhile RAG and agents place greater\nAn AI model typically has three main memory mechanisms:\nThe model itself is a memory mechanism, as it retains the knowledge\nThe model can access this knowledge in all\nA model’s context is a memory mechanism.\nconversation can be added to the model’s context, allowing the\nA model’s\ncontext can be considered its short-term memory as it doesn’t persist\nLong-term memory\nExternal data sources that a model can access via retrieval, such as in\na RAG system, are a memory mechanism.\nmodel’s long-term memory, as it can be persisted across tasks.\na model’s internal knowledge, information in the long-term memory\nYour short-term memory contains information\nYour long-term memory is augmented with books, computers,\nWhich memory mechanism to use for your data depends on its frequency of\nmodel’s internal knowledge via training or finetuning.\nThese three memory\nThe hierarchy of information for an agent.\nmodels, too.\nMany memory management tools for AI models have been\ndeveloped, and many model providers have incorporated external memory.\nAugmenting an AI model with a memory system has many benefits.\ninformation, which can exceed the agent’s maximum context length.\nThe excess information can be stored in a memory system with long-\nterm memories.\nSimilarly, if an AI model can\nFor example, if you ask an agent to find potential sales\nA memory system for AI models typically consists of two functions:\nMemory management: managing what information should be stored in\nMemory retrieval: retrieving information relevant to the task from long-\nterm memory.\nMemory retrieval is similar to RAG retrieval, as long-term memory is an\nmemory.\nlimited by the model’s maximum context length and, therefore, requires a\nLong-term memory can be used to store the overflow from short-term\nmemory.\nFor a given query, the context input into the model\nconsists of both its short-term memory and information retrieved from its\nlong-term memory.\nfrom long-term memory.\nthen the model can use at most 70% of the context limit for short-term\nmemory.\nlong-term memory.\nLike many components previously discussed in this chapter, memory\nto use memory efficiently.\nThis summary can be generated using the same or another model.\n2. Determine if this new information should be inserted into the memory,\nlimitations, RAG also enables more efficient use of information, improving\nfrom external memory and then uses this information to generate more\nThe RAG pattern can be seen as a special case of agent where the retriever\nis a tool the model can use.\nAn agent is defined by its environment and the tools it\nmodel to plan.\nThe more tools you give a model, the more capabilities the model has,\nFor agents to\nBoth RAG and agents work with a lot of information, which often exceeds\ninformation a model has.\nMemory).\nsupported by the model it uses.\nsince you’ll need a way to know when a model uses a given piece of information.\nFinetuning is the process of adapting a model to a specific task by further",
      "keywords": [
        "memory",
        "model",
        "agent",
        "information",
        "Long-term memory",
        "Short-term memory",
        "memory system",
        "RAG",
        "tool",
        "context",
        "task",
        "memory management",
        "Tool failures",
        "system",
        "RAG system"
      ],
      "concepts": [
        "memory",
        "memories",
        "agent",
        "information",
        "tools",
        "model",
        "context",
        "task",
        "likely",
        "uses"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 28,
          "title": "",
          "score": 0.817,
          "base_score": 0.667,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 27,
          "title": "",
          "score": 0.638,
          "base_score": 0.638,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.611,
          "base_score": 0.461,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.531,
          "base_score": 0.381,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.465,
          "base_score": 0.315,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "agent",
          "term memory",
          "term",
          "long term"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "agent",
          "term memory",
          "term",
          "long term"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.313353554581231,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604453+00:00"
      }
    },
    {
      "chapter_number": 30,
      "title": "Segment 30 (pages 595-616)",
      "start_page": 595,
      "end_page": 616,
      "summary": "With prompt-based methods, knowledge about how ML models operate\nfinetuning brings you to the realm of model training, where ML knowledge\nTo finetune, you start with a base model that has some, but not all, of the\nThe goal of finetuning is to get this model to perform\nThe model transferred its\nmodel on tasks with abundant data, you can then transfer that knowledge to\nFor example, while training a model from\nfinetuning a good base model might only require a few hundred.\nIdeally, much of what the model needs to learn is already present in the base\nmodel, and finetuning just refines the model’s behavior.\nthis approach, a model is trained to extract features from the data, usually as embedding vectors,\nwhich are then used by another model.\ndiscussing how part of a foundation model can be reused for a classification task by adding a\nFinetuning is part of a model’s training process.\nfinetuning.\nBefore finetuning this pre-trained model with expensive task-specific data,\nyou can finetune it with self-supervision using cheap task-related data.\nexample, to finetune a model for legal question answering, before\nSimilarly, to finetune a model to do\nAs discussed in Chapter 1, language models can be autoregressive or\nfinetuning, you can also finetune a model to predict the next token or fill in\nYou can finetune a model\nThe massive amount of data a model can learn from during self-supervised\nfinetuning uses high-quality annotated data to refine the model to align with\nDuring supervised finetuning, the model is trained using (input, output)\nA model can also be finetuned with reinforcement learning to generate\nIt’s possible to finetune a model to extend its context length.\nfinetuning typically requires modifying the model’s architecture, such as\nFigure 7-1 shows the making of different Code Llama models (Rozière et\nal., 2024), from the base model Llama 2, using different finetuning\nFinetuning can be done by both model developers and application\nModel developers typically post-train a model with different\nA model developer might also\nrelease different model versions, each finetuned to a different extent, so that\nDifferent finetuning techniques used to make different Code Llama models.\nAs an application developer, you might finetune a pre-trained model, but\nmost likely, you’ll finetune a model that has been post-trained.\nrefined a model is and the more relevant its knowledge is to your task, the\nWhen to Finetune\nprompt-based methods, finetuning requires significantly more resources,\nTherefore, finetuning is\nThe primary reason for finetuning is to improve a model’s quality, in terms\nFinetuning is\nIf the model you want to use\nwasn’t sufficiently trained on your task, finetuning it with your data can be\nIn this case, finetuning this model on data containing this SQL\nmodel on customer-specific queries might help.\nis that if the base model perpetuates certain biases from its training data,\nFor example, if a model consistently\nfinetuning BERT-like language models on text authored by women can\nreduce these models’ gender biases, while finetuning them on texts by\nYou can finetune a big model to make it even better, but finetuning smaller\nmodels is much more common.\nA common approach is to finetune a small model to imitate the behavior of\na larger model using data generated by this large model.\nA small model, finetuned on a specific task, might outperform a much\nlarger out-of-the-box model on that task.\nthat their finetuned Flan-T5 models (Chung et al., 2022) outperformed a\ntypically needed to train a text-editing model from scratch.\nmodels available for finetuning.\nWhile finetuning can improve a model in many ways, many of these\nFinetuning can improve a model’s performance, but so do carefully crafted\nFirst, while finetuning a model for a specific task can improve its\nTo fix this, you finetune the model on a dataset\nThe finetuned model\nYou can finetune the model on all the\nmodel to perform well on all your tasks, consider using separate models for\nSecond, finetuning requires the knowledge of how to train models.\nneed to evaluate base models to choose one to finetune.\nWhile finetuning\nThird, once you have a finetuned model, you’ll need to figure out how to\nmodels may improve faster than you can enhance your finetuned model.\nnew base model outperforms your finetuned model on your specific task,\nswitch to the new base model?\nWhat if a new base model doesn’t\nvarious prompts, as a model’s performance can vary greatly with different\nFINETUNING DOMAIN-SPECIFIC TASKS\ndomain-specific tasks, and, therefore, you must finetune or train models for\ndomain-specific models.\nThe strongest models on the\nModel\nthrough training this model, which might enable them to better develop and\nBoth finetuning and prompting experiments require systematic processes.\nInstead of including your examples in each prompt, you can finetune a\nmodel on these examples.\nfinetuned model, as shown in Figure 7-2.\nfinetune a model on these examples.\nFinetuning and RAG\nquestions about current events, RAG outperformed finetuned models.\nonly that, RAG with the base model outperformed RAG with finetuned\nmodels, as shown in Table 7-2.\ncan enhance a model’s performance on a specific task, it may also lead to a\nBase model\nBase model +\nOn the other hand, if the model has behavioral issues, finetuning might\nFor example, you ask the model to generate\nFinetuning the model with well-defined technical specifications\nmodel to more HTML code during finetuning.\nA RAG system can help mitigate your model’s hallucinations.\nFinetuning, on the other hand, helps your model understand and follow\ntraining data or hosting the finetuned models.\nfinetuning.\non top of a finetuned model can boost its performance on the MMLU\nusing RAG with finetuned models doesn’t improve the performance 57% of\nSo the workflow to adapt a model to a task might work as follows.\n1. Try to get a model to perform your task with prompting alone.",
      "keywords": [
        "model",
        "finetuning",
        "base model",
        "data",
        "RAG",
        "finetune",
        "task",
        "finetuned model",
        "n’t",
        "RAG outperforms finetuning",
        "finetuning techniques",
        "training",
        "Code Llama models",
        "base",
        "finetune a model"
      ],
      "concepts": [
        "model",
        "data",
        "task",
        "base",
        "based",
        "prompt",
        "learning",
        "examples",
        "required",
        "require"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.648,
          "base_score": 0.648,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.625,
          "base_score": 0.625,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 34,
          "title": "",
          "score": 0.584,
          "base_score": 0.584,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 33,
          "title": "",
          "score": 0.573,
          "base_score": 0.573,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.565,
          "base_score": 0.565,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "finetuning",
          "finetune",
          "base",
          "base model",
          "finetuned"
        ],
        "semantic": [],
        "merged": [
          "finetuning",
          "finetune",
          "base",
          "base model",
          "finetuned"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3859795278018183,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.604496+00:00"
      }
    },
    {
      "chapter_number": 31,
      "title": "Segment 31 (pages 617-639)",
      "start_page": 617,
      "end_page": 639,
      "summary": "while finetuning increases the complexity of model development but\nyou’d need to serve or finetune a model.\n1. Because of the scale of foundation models, memory is a bottleneck for\nThe memory\nneeded for finetuning is typically much higher than the memory needed\n2. The key contributors to a model’s memory footprint during finetuning\n3. The more trainable parameters, the higher the memory footprint.\nreduce memory requirement for finetuning by reducing the number of\n4. Quantization refers to the practice of converting a model from a format\nstraightforward and efficient way to reduce a model’s memory footprint.\nFor a model of 13 billion parameters, using FP32 means 4 bytes per\nonly 2 bytes, the memory needed for the model’s weights decreases to 26\nmodel in lower precision.\nA key factor that determines a model’s memory footprint during finetuning\nDuring pre-training, all model\nDuring inference, no model parameters are\nDuring finetuning, some or all model parameters may be updated.\nThe memory needed for each trainable parameter results from the way a\nmodel is trained.\n2. Backward pass: the process of updating the model’s weights using the\nIf they are different, the model made a\nparameters there are, the more memory is needed to store these additional\nIt’s useful to know how much memory a model needs so that you can use\nIf a model\nA model’s memory footprint depends on the model as well as the workload\nand the different optimization techniques used to reduce its memory usage.\nneed to operate a model, both during inference and training.\nMemory needed for inference\nrequires memory for the model’s weights.\nLet N be the model’s parameter\ncount and M be the memory needed for each parameter; the memory\nneeded to load the model’s parameters is:\nThe forward pass also requires memory for activation values.\nmodels need memory for key-value vectors for the attention mechanism.\nbe assumed to be 20% of the memory for the model’s weights.\nThis assumption brings the model’s memory\nConsider a 13B-parameter model.\nmodel’s weights will require 13B × 2 bytes = 26 GB.\nA model’s memory footprint grows rapidly with its size.\nAs models become\nparameter model with 2 bytes per parameter will require a whooping 140\nMemory needed for training\nTo train a model, you need memory for the model’s weights and activations,\nOverall, the memory needed for training is calculated as:\nTraining memory = model weights + activations + gradients + optimizer\nDuring the backward pass, each trainable parameter requires one value for gradient plus zero to two\nAn Adam optimizer stores two values per trainable parameter.\nImagine you’re updating all parameters in a 13B-parameter model using the\nmemory needed for gradients and optimizer states will be:\nHowever, if you only have 1B trainable parameters, the memory needed for\nmodel’s weights.\nneeded for activations can dwarf the memory needed for the model’s\nthe memory needed for the model’s weights for different Megatron models\nOne way to reduce the memory needed for activations is not to store them.\nThe memory needed for activations can dwarf the memory needed for the model’s\nThe memory required to represent each value in a model\ncontributes directly to the model’s overall memory footprint.\nthe memory needed for each value by half, the memory needed for the\nmodel’s weights is also reduced by half.\nBefore discussing how to reduce the memory needed for each value, it’s\nFP64 uses 64 bits (8 bytes) and is called double precision.\nFP16 uses 16 bits (2 bytes) and is called half precision.\nThe number of range bits determines the range of values the format\nReducing the number of precision bits makes a\nprecision bits.\nFormats with more bits are considered higher precision.\nConvert from FP32 values to lower-precision formats.\nThe fewer bits needed to represent a model’s values, the lower the model’s\nA 10B-parameter model in a 32-bit format\nrequires 40 GB for its weights, but the same model in a 16-bit format will\ncheap and extremely effective way to reduce a model’s memory footprint.\nconvert values to a lower-precision format.\nmajor contributors to a model’s memory footprint during inference\nare the model’s weights and activations.\ntraining quantization (PTQ) means quantizing a model after it’s been\nto AI application developers who don’t usually train models.\ncommon to serve models in 16 bits and in even lower precision.\nA model can also be served in mixed precision, where values are reduced in\nparameter values are converted into an integer format, such as INT8 or\nlanguage model that requires only 1.58 bits per parameter and whose\nModel\nReduced precision not only reduces the memory footprint but also often\nIf a value is outside the range the reduced precision format can\nA model is trained\n1. To produce a model that can perform well in low precision during\nQuantization reduces a model’s\nmemory footprint, allowing a model to be trained on cheaper hardware\nor allowing the training of a larger model on the same hardware.\nQuantization-aware training (QAT) aims to create a model with high quality\nprecision (e.g., 8-bit) behavior during training, which allows the model to\ndoesn’t reduce a model’s training time since its computations are still\nOn the other hand, training a model directly in lower precision can help\nPeople attempted to train models in reduced precision as\nthe weights is kept in higher precision but other values, such as gradients\nsensitive weight values computed in lower precision and more-sensitive\nweight values computed in higher precision.\nThe portions of the model that should be in lower precision can be set\nFor example, a model can be trained in higher precision but\norganization with sufficient compute for higher precision training.\nmodel is published, developers with less compute access can finetune that\nmodel in lower precision.\nmodels is so memory-intensive.\nThe more memory finetuning requires, the\nTechniques that reduce a model’s\nfinetune entire models.\ntraining starts with randomized model weights, whereas finetuning starts",
      "keywords": [
        "model",
        "memory",
        "memory needed",
        "precision",
        "memory footprint",
        "finetuning",
        "bits",
        "Training",
        "trainable parameter",
        "needed",
        "parameters",
        "inference",
        "Quantization",
        "lower precision",
        "format"
      ],
      "concepts": [
        "model",
        "precision",
        "precisely",
        "memory",
        "trained",
        "bits",
        "bit",
        "value",
        "quantization",
        "parameters"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "",
          "score": 0.605,
          "base_score": 0.455,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.536,
          "base_score": 0.536,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "precision",
          "memory needed",
          "needed",
          "footprint"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "precision",
          "memory needed",
          "needed",
          "footprint"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.30885680277756855,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604535+00:00"
      }
    },
    {
      "chapter_number": 32,
      "title": "Segment 32 (pages 640-662)",
      "start_page": 640,
      "end_page": 662,
      "summary": "with model weights that have been previously trained.\nConsider a 7B-parameter model:\nIf you use a 16-bit format like FP16, loading the model’s weights alone\nFull finetuning this model with the Adam optimizer, also in a 16-bit\nThe total memory needed for the model’s weights, gradients, and\nTo fit a model on a given hardware, you can either reduce the model’s memory footprint or find ways\nIn partial finetuning, only some of the model’s\nnumber of trainable parameters to 10% of full finetuning.\nWhile partial finetuning can reduce the memory footprint, it’s parameter-\nPartial finetuning requires many trainable parameters to achieve\ncomparable to that of full finetuning on the GLUE benchmark (Wang et al.,\nThe blue line shows that partial finetuning requires many trainable parameters to achieve\nfull finetuning while using significantly fewer trainable parameters?\nFinetuning techniques resulting from this quest are parameter-efficient.\nThe idea of PEFT (parameter-efficient finetuning) was introduced by\nparameters into the model in the right places, you can achieve strong\nfinetuning performance using a small number of trainable parameters.\nDuring finetuning, they kept the model’s original parameters unchanged\nlatency of the finetuned model.\nbefore diving deeper into the most common PEFT technique: LoRA.\nto the model weights, such as the one developed by Houlsby et al.\nAs of this writing, LoRA (Hu et al., 2021) is by far the most popular\noutperform LoRA and even full finetuning in some cases.\net al., 2023) is a LoRA variant that incorporates attention-modification\nIf adapter-based methods add trainable parameters to the model’s\narchitecture, soft prompt-based methods modify how the model processes\nLoRA\n(2019), LoRA (Low-\nRank Adaptation) (Hu et al., 2021) incorporates additional parameters in a\nadditional layers to the base model, LoRA uses modules that can be merged\nYou can apply LoRA to individual weight matrices.\nLoRA decomposes this matrix into the product of two smaller matrices,\nLoRA rank.\nUse Wʹ in place of W as part of the model.\n3. During finetuning, update only the parameters in A and B.\nTo apply LoRA to a weight matrix W, decompose it into the product of two matrices A\nLoRA (Low-Rank Adaptation) is built on the concept of low-rank factorization, a long-standing\nLike the original adapter method, LoRA is parameter-efficient and sample-\nThe factorization enables LoRA to use even fewer trainable\nwhile using only ~4.7M trainable parameters, 0.0027% of full finetuning.\nParameter-efficient methods like LoRA have become so popular that many\nIf a model requires a lot of parameters to learn certain behaviors during\nwords, the better trained an LLM is, the easier it is to finetune the model\nuse LoRA for pre-training as well?\napplying low-rank factorization only during finetuning, could we factorize a\nsignificantly reduce the model’s number of parameters, significantly\n(Sainath et al., 2013), “Semi-Orthogonal Low-Rank Matrix Factorization\nbased models of up to 1.3B parameters.\ncomparable to that of a full-rank model at 1B parameters and promising\nperformance at 7B parameters.\nway to scale up low-rank pre-training to hundreds of billions of parameters.\nimplicitly compresses a model’s intrinsic dimension—full-rank pre-training\nTo apply LoRA, you need to decide what weight matrices to apply LoRA to\nLoRA can be applied to each individual weight matrix.\net al., 2024), LoRA has been primarily used for transformer models.\nLoRA is most commonly applied to the four weight matrices in the\nwithin a model.\nFor example, applying LoRA to the query matrix means\napplying LoRA to all query matrices in the model.\nbudget of trainable parameters, what matrices should you apply LoRA to, to\nWhen finetuning GPT-3 175B, Hu et al.\nbudget at 18M, which is 0.01% of the model’s total number of parameters.\nGPT-3 175B has 96 transformer layers with a model dimension of 12,288.\nApplying LoRA with rank\n18,874,368 trainable parameters for the whole model.\nThey found that applying LoRA to all four matrices with rank = 2 yields the\nLoRA performance with the budget of 18M trainable parameters.\nThe beauty of LoRA is that while its performance depends on its rank,\nA smaller r means fewer LoRA parameters,\nServing LoRA adapters\nLoRA not only lets you finetune models using less memory and data, but it\nthis benefit, let’s examine how to serve a LoRA-finetuned model.\nIn general, there are two ways to serve a LoRA-finetuned model:\n1. Merge the LoRA weights A and B into the original model to create the\nnew matrix Wʹ prior to serving the finetuned model.\nThe first option is generally better if you have only one LoRA model to\nserving multiple LoRA models that share the same base model.\nKeeping LoRA adapters separate allows reuse of the same full-rank matrix W in multi-\nyou finetune a model for each of your customers using LoRA.\ncustomers, you end up with 100 finetuned models, all sharing the same base\nmodel.\nIf the LoRA’s rank is 8,\nIn option 2, one full-rank matrix W and 100 sets of small matrices (A, B)\nyou only need to load Y’s LoRA adapter, which can significantly reduce the\nMulti-LoRA serving makes it easy to combine multiple specialized models.\none LoRA adapter for each task.\nadapters to adapt the same 3B-parameter base model to different iPhone\nmemory footprint of this base model and adapters, allowing the serving of\nThere are publicly available finetuned LoRA adapters that you\ncan use the way you’d use pre-trained models.\nfull finetuning.\nsupport LoRA for popular base models right out of the box.\nHowever, as illustrated in Table 7-6, the memory of a LoRA adapter\nis minimal compared to the memory of the model’s weights.\nnumber of LoRA parameters decreases the overall memory footprint by\nThe memory needed by LoRA weights compared to that needed by the model’s weights.\nModel’s\nLoRA trainable\nLoRA adapter\nRather than trying to reduce LoRA’s number of parameters, you can reduce\nthe memory usage more effectively by quantizing the model’s weights,\nquantized version of LoRA is QLoRA (Dettmers et al., 2023).\noriginal LoRA paper, during finetuning, the model’s weights are stored\nThese techniques allow a 65B-parameter model to be finetuned on\nThe authors finetuned a variety of models, including Llama 7B to 65B, in",
      "keywords": [
        "LoRA",
        "model",
        "trainable parameters",
        "parameters",
        "finetuning",
        "Full finetuning",
        "matrices",
        "Memory",
        "matrix",
        "trainable",
        "PEFT",
        "LoRA adapters",
        "performance",
        "number",
        "Full"
      ],
      "concepts": [
        "models",
        "parameter",
        "memory",
        "adapter",
        "matrices",
        "rank",
        "weight",
        "trained",
        "techniques",
        "matrix"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.868,
          "base_score": 0.718,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.774,
          "base_score": 0.624,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "",
          "score": 0.602,
          "base_score": 0.452,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.589,
          "base_score": 0.439,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "",
          "score": 0.557,
          "base_score": 0.407,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "lora",
          "parameters",
          "trainable",
          "matrices",
          "trainable parameters"
        ],
        "semantic": [],
        "merged": [
          "lora",
          "parameters",
          "trainable",
          "matrices",
          "trainable parameters"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32571464792129423,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604577+00:00"
      }
    },
    {
      "chapter_number": 33,
      "title": "Segment 33 (pages 663-680)",
      "start_page": 663,
      "end_page": 680,
      "summary": "Model\nModel Merging and Multi-Task Finetuning\nmodel, model merging allows you to create a custom model by combining\nmultiple models.\nModel merging offers you greater flexibility than\nYou can take two available models and merge them\nfinetune any or all of the constituent models before merging them together.\nWhile you don’t have to further finetune the merged model, its performance\nWithout finetuning, model merging\nThe goal of model merging is to create a single model that provides more\nFor example, if you have two models\na single model that is better than both of them on that task.\nFor example, if you have two models that can do\ndifferent tasks, they can be merged into one model that can do both tasks\nmodels.\nGiven two models that were finetuned on top of the same base\nOne important use case of model merging is multi-task finetuning.\nmodel merging, if you want to a finetune a model for multiple tasks, you\nmodel on this dataset to make the model learn all the tasks\nYou can finetune the model on each task separately but sequentially.\nAfter training a model on task A, train it on task B, and so on.\nassumption is that it’s easier for models to learn one task at a time.\nA model can forget how to do an old task\nModel merging offers another method for multi-task finetuning.\nfinetune the model on different tasks separately but in parallel.\nthese different models are merged together.\nseparately allows the model to learn that task better.\nModel merging is also appealing when you have to deploy models to\nInstead of squeezing multiple models for different tasks\nonto a device, you can merge these models together into one model that can\nModel merging is one way to do federated learning (McMahan et al.,\n2016), in which multiple devices train the same model using separate data.\nconstituent models.\nThe idea of combining models together to obtain better performance started\nalgorithms alone.” If model merging typically involves mixing parameters\nFigure 7-13 compares ensembling and model merging.\nJust like model\nHow ensembling and model merging work.\nMany model-merging techniques are experimental and might become\nModel merging approaches differ in how the constituent parameters are\nThree main approaches to model merging: summing, layer stacking, and concatenation.\nYou can mix these approaches when merging models, e.g., summing some\nThis approach involves adding the weight values of constituent models\nIf the parameters in two models are in\nidea that multiple models can be linearly combined to create a better one\nModel soups\nmultiple finetuned models can improve accuracy without increasing\nHowever, it’s more common to merge models by linearly\nWhile you can linearly combine any set of models, linear combination is\nthe most effective for models finetuned on top of the same base model.\nThe idea is that once you’ve finetuned a model for a specific task,\nsubtracting the base model from it should give you a vector that captures\nMerging Models Modulo Permutation Symmetries” (Ainsworth et al.,\n2022), and “Merging by Matching Models in Task Parameter Subspaces”\nIn the case of model\nmerging, the unknown value is the merged model, and the known values are the constituent models.\nBecause the formula for SLERP is mathy, and model-merging tools\nDuring finetuning, many models’ parameters are adjusted.\nmodel’s performance on the task.\noriginal value in the base model, effectively setting the corresponding task\nsubtracting the base model from the finetuned model.)\nThese redundant parameters, while not harmful to one model, might be\nharmful to the merged model.\npractice can significantly improve the quality of the final merged models.\nThe more models there are to merge, the more important pruning is because\nIn this approach, you take different layers from one or more models and\napproach, the merged models resulting from layer stacking typically require\nwhich was merged from two finetuned Llama 2-70B models, Xwin and\nIt took 72 out of 80 layers from each model and merged them\nLayer stacking can be used to train mixture-of-experts (MoE) models, as\nYou then further train the merged model along with the\nshowed that layer stacking can produce models that\nModel\n3. Further train this upscaled model toward the target performance.\nInstead of adding the parameters of the constituent models together in\nFinetuning frameworks and base models\nBase models",
      "keywords": [
        "Model",
        "Model Merging",
        "Merging",
        "task",
        "parameters",
        "Finetuning",
        "constituent models",
        "base model",
        "task vector",
        "vector",
        "performance",
        "layer",
        "multiple",
        "merge"
      ],
      "concepts": [
        "model",
        "task",
        "merging",
        "merge",
        "parameters",
        "performance",
        "perform",
        "combined",
        "combine",
        "combinations"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "",
          "score": 0.573,
          "base_score": 0.573,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 34,
          "title": "",
          "score": 0.514,
          "base_score": 0.514,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.47,
          "base_score": 0.47,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.463,
          "base_score": 0.463,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "",
          "score": 0.434,
          "base_score": 0.434,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "merging",
          "model merging",
          "merged",
          "constituent",
          "task"
        ],
        "semantic": [],
        "merged": [
          "merging",
          "model merging",
          "merged",
          "constituent",
          "task"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.31922525193817025,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.604648+00:00"
      }
    },
    {
      "chapter_number": 34,
      "title": "Segment 34 (pages 681-701)",
      "start_page": 681,
      "end_page": 701,
      "summary": "For finetuning, the starting models vary for different projects.\n1. Test your finetuning code using the cheapest and fastest model to make\n2. Test your data by finetuning a middling model.\n4. Once you have good results, do a training run with all models to map out\nthe price/performance frontier and select the model that makes the most\n1. Start with a small dataset and the strongest model you can afford.\nthe best possible model with this small dataset.\nBecause the base model\n2. Use this finetuned model to generate more training data.\n3. Use this new dataset to train a cheaper model.\nThe finetuning methods to use also depend on your data volume.\nDepending on the base model and the task, full finetuning typically requires\nhave a small dataset, such as a few hundred examples, full finetuning might\nTake into account how many finetuned models you need and how you want\nmethods like LoRA allow you to more efficiently serve multiple models\nthat share the same base model.\nfull model, whereas full finetuning requires serving multiple full models.\ndata, select a base model, and get back a finetuned model.\nLike model\ninference APIs, finetuning APIs can be provided by model providers, cloud\nthat you’re limited to the base models that the API supports.\nIf you want to do full finetuning, many base models provide\nyour own data.\nof finetuning frameworks and model repositories.\nTo finetune a model using more than one machine, you’ll need a framework\nDepending on the base model and the finetuning method, there are many\nmodel or the finetuning framework you use.\nThe learning rate determines how fast the model’s parameters should\nThe batch size determines how many examples a model learns from in each\nIn general, the larger the batch size, the faster the model can go through\nneeded to run your model.\nmodels are so large, and memory is so constrained, that only small batch\nThis can lead to unstable model weight updates.\naddress this, instead of updating the model weights after each batch, you\ncan accumulate gradients across several batches and update the model\nwith different batch sizes to see which gives the best model performance.\nAn epoch is a pass over the training data.\nsteadily decrease, the model can benefit from more epochs (and more data).\nmodel is overfitting to the training data, and you might try lowering the\nresponse, both of which can contribute to the model’s loss during training.\nmodel only needs to generate responses.\ncontribute more to the model’s loss during training than prompt tokens.\nThe prompt model weight determines how much prompts should contribute\ncontribute to the loss as much as responses, meaning that the model learns\nIf this weight is 0%, the model learns only from\nmodel should learn some from prompts but mostly from responses.\nwhether you should even finetune a model.\nupdating the model’s entire weights.\nHowever, as models increased in size,\nand data) to do full finetuning with foundation models.\nmodels.\nThe idea of combining finetuned models brought the chapter to model\nmerging; its goal is to combine multiple models into one model that works\ngetting data for finetuning is hard.\nprinciple that post-training should align the model to ‘know what it knows’ rather than add\ngradients, instead of using real gradients, to update model weights.\nDuring training, the model’s weights are updated via multiple steps.\ncompound during the training process, making it difficult for the model to achieve the desirable\nTo effectively use LoRA for a model, it’s necessary to understand that model’s architecture.\nChapter 2 already covered the weight composition of some transformer-based models.\nweight composition of a model, refer to its paper.\nWhen task vectors are pruned, they become more sparse, but the finetuned model doesn’t.\nIn college, I made the painful mistake of letting my model train overnight, only to have it crash after\non different machines need to be accumulated and used to update the model’s weights.\nThe quality of a model depends on the quality of its training data.\nmodel if you don’t have data.\ndataset that allows you to train the best model, ideally within your allocated\nAs models demand\nFor the same model, different training phases aim to teach the model\nFor example, data quantity for pre-training is often measured in\nthe number of tokens, whereas data quantity for supervised finetuning is\nModel-centric AI tries to improve AI performance by enhancing the\nmodels themselves.\nincreasing the sizes of the models, or developing new training\nhigh-quality datasets that allow better models to be trained with fewer\nIn the early days of deep learning, many AI benchmarks were model-\nmodel using the same dataset.\nGiven the same model, people try to develop a dataset\nthat gives this model the best performance.\nwas to create the best dataset for training a CLIP model (Radford et al.,\nA standardized script trains a CLIP model on each submitted dataset.\nThe quality of a dataset is evaluated based on its resulting model’s\nThe model-centric and data-centric division helps guide research.\nboth model and data improvements.\nWhile not all issues with AI models can be solved with data, data is often a\nThe right data can make the model more capable,\nthe model and waste resources.\nData curation is a science that requires understanding how the model learns\nwork closely with application and model developers.\nmight be the same person—the person responsible for training a model is\nmodel.\nFor self-supervised finetuning, you need sequences of data.\ninstruction finetuning, you need data in the (instruction, response) format.\nFor preference finetuning, you need data in the (instruction, winning\nTo train a reward model, you can use the\nsame data format as preference finetuning or use data with annotated scores\nTraining data should exhibit the behaviors you want your model to learn.\nAs discussed in Chapter 5, CoT prompting nudges the model to work\nteach a model to generate step-by-step responses, its training data\nby-step responses in the finetuning data greatly enhances the\nperformance of models of various sizes on CoT tasks, with accuracy\ntraining, many models might intuitively know how to use certain\nHowever, a model’s tool use ability can be improved by\ncreate tool use data, where each prompt is a task that requires tool\nexample, if you want data to finetune a model to act as a personal\nwhereas it’s easier for a model to use an API.\nTool use data might also require special formats.",
      "keywords": [
        "model",
        "finetuning",
        "data",
        "training",
        "Learning",
        "training data",
        "full finetuning",
        "base model",
        "Learning rate",
        "dataset",
        "performance",
        "n’t",
        "loss",
        "model learns",
        "Finetuning frameworks"
      ],
      "concepts": [
        "models",
        "data",
        "train",
        "learning",
        "performance",
        "perform",
        "response",
        "responsible",
        "memory",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "",
          "score": 0.584,
          "base_score": 0.584,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 38,
          "title": "",
          "score": 0.524,
          "base_score": 0.524,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 33,
          "title": "",
          "score": 0.514,
          "base_score": 0.514,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.491,
          "base_score": 0.491,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "finetuning",
          "training",
          "dataset",
          "base"
        ],
        "semantic": [],
        "merged": [
          "data",
          "finetuning",
          "training",
          "dataset",
          "base"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.310919880312516,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.604689+00:00"
      }
    },
    {
      "chapter_number": 35,
      "title": "Segment 35 (pages 702-724)",
      "start_page": 702,
      "end_page": 724,
      "summary": "Single-turn data helps train a model to respond to individual instructions.\nMulti-turn data, on the other hand, teaches the model how to solve tasks—\nMulti-turn data\nData curation isn’t just about creating new data to help a model learn new\nbehaviors but is also about removing existing data to help a model unlearn\nYou investigate and find that in the training data, there are several examples\nthese examples from the training data and another request to acquire new\nEach application might require data of different characteristics.\ntraining phases also require different data mixes.\ncooking, the data fed into the model is the ingredients.\nData quality is\nData quantity is about how many ingredients you should have.\nData Quality\nA small amount of high-quality data can outperform a large amount of noisy\nHowever, the downside of having too few data examples is that LIMA is\nassisted annotation tools to ensure high data quality.\nMost people understand the importance of data quality, but what does it\nmean for data to be high-quality?\nThe short answer is that data is considered\nIn general, data can be\nthe model to do.\nexample, if the task requires factual consistency, the annotations\nAll examples should follow the format expected by the model.\nThis refers to unique examples in your data.\nallowed to use PII data to train your models, your data shouldn’t\nData Coverage\nA model’s training data should cover the range of problems you expect it to\nHaving data that\nCoverage requires sufficient data diversity, which is why\nmany refer to this attribute as data diversity.\ndata should include both detailed and short instructions.\napplication works with multiple programming languages, your training data\nFor general-purpose use cases like chatbots, the finetuning data should be\nperformance of chat language models is to increase the quality and diversity\nof data employed in the training process.\n“primarily driven by improvements in data quality and diversity as well as\nby increased training scale.” The Llama 3 paper has rich details on data\ntraining data, it’s useful to look at the data mix for the same model across\nPost-training data\nuses synthetic data for post-training, so another dimension is the ratio of\nalmost half of the training data.\nhigh-quality code and math data (training the model using an increasingly\ncommon belief that high-quality code and math data is more effective than\nThe percentage of code and math data during preference finetuning is much\nYou can also use experiments to find optimal data\nFor each candidate data mix,\nthey trained several small models on a data mix and used that to predict the\nTo evaluate the impact of data diversity and quality, Zhou et al.\nA 7B-parameter model, finetuned on a dataset that is both high-quality and diverse,\noutperforms that same model finetuned on a dataset that is either diverse or high-quality.\nData Quantity\nhave finetuned models with millions of examples.\ndata typically needed to train a foundation model from scratch.\nYou might wonder: if I have millions of examples, shouldn’t I just train a model from scratch?\ncan and should evaluate whether training a model from scratch would improve your performance.\nthere are situations when finetuning can be worse, especially when you have a lot of training data.\nweights so that they don’t adapt as well to the finetuning data (Hernandez et al., 2021).\nOther than data quality and data diversity, three other factors influence how\nmuch data you need:\npositive or negative, will require much less data than a complex task,\nare better, you might need fewer examples to finetune big models.\ntraining data.\nmore advanced models give you better finetuning performance.\nWith 100 examples, more advanced models give much better performance after\nWith 550,000 examples, all models give similar performance after finetuning.\nIn short, if you have a small amount of data, you might want to use PEFT\nIf you have a large amount of data, use\nfull finetuning with smaller models.\nsmall, well-crafted dataset (e.g., 50 examples) to see if finetuning can\ndata will improve the performance even more.\nobserved with small data, a bigger dataset will rarely do the trick.\nlearning rate is too high or too low), data quality, poorly crafted prompts,\nIt’s possible to reduce the amount of high-quality data needed by first finetuning your model using\nlower-quality or less-relevant data.\nYou want to finetune a model to answer legal questions.\nsensitive nature of this task, your data is limited.\namount of data to finetune your model first, then further finetune it on your real data.\njust finetuning with high-quality data.\ndata you’ll need.\ndoubling your data.\nof additional training examples on your model’s performance.\nWhile a larger number of finetuning examples generally improves a model’s\nmodel performance increased significantly when the number of finetuning\nThe diversity of data can be reflected in task types (such as summarization\nDiversity in finetuning number, measured by the number of tasks, can impact model\nHow much data to use for finetuning is determined not just by what you\nIf you budget $10,000 for data\nYou might also need to balance the budget for data and\nData Acquisition and Annotation\nThe goal of data acquisition is to produce a sufficiently large dataset with\nthe quality and diversity you need, while ensuring that your data practices\nleverages data generated by your users to continually improve your product,\nApplication data is ideal because it’s\nachieve with other data sources.\nUser-generated data can be user content,\nsystem-generated data from user usage, or user feedback.\nBefore investing in creating your own data, check available datasets first.\nyour dataset has a total of 9,000 high-quality examples.\ndata’s diversity, so you have to create more templates and generate fewer\nhundreds of thousands of datasets, and data.gov.in hosts tens of",
      "keywords": [
        "data",
        "model",
        "finetuning",
        "dataset",
        "training data",
        "data quality",
        "performance",
        "task",
        "training",
        "diversity",
        "Llama",
        "instructions",
        "data diversity",
        "quality",
        "n’t"
      ],
      "concepts": [
        "model",
        "examples",
        "dataset",
        "user",
        "instructions",
        "instruction",
        "perform",
        "performance",
        "diverse",
        "diversity"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 38,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 37,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 36,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "",
          "score": 0.501,
          "base_score": 0.501,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 34,
          "title": "",
          "score": 0.495,
          "base_score": 0.495,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "diversity",
          "quality",
          "data quality",
          "training"
        ],
        "semantic": [],
        "merged": [
          "data",
          "diversity",
          "quality",
          "data quality",
          "training"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3204739911770077,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604730+00:00"
      }
    },
    {
      "chapter_number": 36,
      "title": "Segment 36 (pages 725-745)",
      "start_page": 725,
      "end_page": 745,
      "summary": "Often, you might need to annotate your own data for finetuning.\nexamples to synthesize new data.\nData Augmentation and Synthesis\nTogether with compute and talent, data is the hardest challenge of AI.\nbeen a long-term goal of the whole industry to be able to generate data\nand data synthesis:\nData augmentation creates new data from existing data (which is real).\nData synthesis generates data to mimic the properties of real data.\ngenerate data for what bot movements would look like.\nIn other words, augmented data is derived from real data, whereas synthetic\nIn this chapter, I’ll often use data synthesis to refer to both.\nArtificially generated data has a long history in software engineering.\noriginally used to generate fake data for testing purposes.\nlibraries like Faker and Chance let you generate data in simple formats\nfake data generators to generate addresses in different countries and states\nWith AI being capable of generating data indistinguishable from that\ngenerate data and enables more synthetic data use cases.\nhuman-generated data, synthetic data doesn’t completely replace human\ndata.\nIn many use cases, as discussed in “Limitations to AI-generated data”,\nmixing human- and AI-generated data often produces the best value.\nWhy Data Synthesis\nYou can synthesize data to\nsynthesize data to mitigate privacy concerns and distill models:\ntesting AI models.\nMore data, in theory, helps models generalize to a\nYou can generate data with targeted characteristics to improve model\nIt’s also possible to generate data for the rare\nvarious data synthesis techniques to generate specific datasets that\nIn other words, you can use synthetic data to increase data coverage:\ngenerate targeted data to cover the areas where existing data is\nlower quality than human-generated data, sometimes, the reverse can\ncause human-generated data to be of lower quality than AI-\ngenerated data.\nOne example is tool use data discussed earlier—\nAnother example is in generating complex math\nSome teams also prefer using AI to generate preference data.\nAI-generated preference ratings, in\nSynthetic data is often the only option for use cases where you can’t\nuse human-generated data due to privacy concerns.\nreal patient records to train a model, you can generate synthetic\ndata generated by the original model.\nData synthesis isn’t unique to AI.\nUsing algorithms to generate data is also called\nMost data generation techniques\nadvanced AI models is using AI itself to synthesize data.\nAI-powered data synthesis in the next section.\nThe simplest way to generate data is to use predefined rules and templates.\nDue to the sensitivity of transaction data, many fraud detection models are\nfirst trained on synthetic transaction data generated from templates like this\nIt’s common to use templates to generate documents that follow a specific\nalso be used to generate data that follows a certain grammar and syntax,\ngenerate math equations for AI models to solve.\nYou can procedurally generate new data from existing data by applying\nOriginal data\nAugmented data\nto generate new data.\nYou can train your model on perturbed data.\naugmented assets are then used to train AI models.\nSimilarly, it’s very common to simulate training data for robotics in a\nSimulations are common to generate data to teach models to use tools.\nSimulations are particularly valuable for generating data for rare events.\nassemblies to generate data to train anomaly detection and quality control\nThis synthetic data is then fed into AI models, enabling\nuse cases, but it wasn’t until AI become capable of generating realistic and\nAI-Powered Data Synthesis\nJust as there are virtually infinite ways for humans to generate data, AI can\nPowerful AI models open many new possibilities for simulations.\n“StableToolBench” (Guo et al., 2024) demonstrates how to use AI to\nwhich might be costly or slow—you can use an AI model to simulate the\nThey showed that their models, trained on this new dataset,\nIt’s common to use AI to translate data in high-resource languages (more\nYou can use AI to translate code written in one language to another.\ngenerate useful data.\nFor example, they used back-translation to generate code explanations and\nStarting with code snippets, they used AI to generate\nThey then again used AI to generate code\nOnly if the generated\nAI can generate data for both pre-training and post-training, though\nsynthetic data is intentionally included much more often in post-training\ngoal is to increase the model’s knowledge, and while AI can synthesize\nHowever, as the internet becomes flooded with AI-generated content,\nmodels that rely on internet data are likely already pre-trained on synthetic\ndata.\nData synthesis for post-training is also more common because post-training\nThe next section will focus on how to use AI to synthesize instruction data\nInstruction data synthesis\nFor example, you can use AI to generate instructions and humans to\nYou can also use humans to write instructions and AI to\ngenerate responses:\ngenerated by AI.\nmodel, text-davinci-003, to generate 52,000 (instruction, response) pairs\nThere are also many creative ways to synthesize instruction data with\nlonger content than shorter content, it’s harder for AI to generate high-\nWhat if we use human-generated\nresponses with AI-generated instructions?\nstories, books, and Wikipedia articles and use AI to generate prompts that\navoiding AI-generated hallucinations in the responses.\nmodels without adding manually annotated data.\n2. Use this weak model to generate instructions for existing high-quality\ncontent to create high-quality instruction data.\n3. Finetune the weak model with this new high-quality instruction data.\nA creative approach is to use synthetic data to finetune a model for\nThis trains the model to use the extended context to answer\ntwo ways in which Llama 3 synthesized data: code translation and code\nBoth of these methods generate more data from existing\n1. Use AI to generate a large collection of programming problem\nTo ensure the quality of the generated data, they employed a rigorous\nthey used AI to generate these unit tests.\n1. Use AI to generate problem descriptions.",
      "keywords": [
        "data",
        "data synthesis",
        "generate data",
        "generate",
        "synthetic data",
        "model",
        "instruction data",
        "Instruction data synthesis",
        "Synthesis",
        "Data synthesis generates",
        "data synthesis techniques",
        "synthetic",
        "synthesize data",
        "synthesize instruction data",
        "instruction"
      ],
      "concepts": [
        "data",
        "generate",
        "generated",
        "generators",
        "generation",
        "models",
        "ais",
        "dataset",
        "simulate",
        "simulation"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 37,
          "title": "",
          "score": 0.863,
          "base_score": 0.713,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 38,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "",
          "score": 0.546,
          "base_score": 0.396,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 4,
          "title": "",
          "score": 0.497,
          "base_score": 0.497,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.424,
          "base_score": 0.424,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "generate",
          "generate data",
          "synthesis",
          "ai"
        ],
        "semantic": [],
        "merged": [
          "data",
          "generate",
          "generate data",
          "synthesis",
          "ai"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.24306102187027914,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604766+00:00"
      }
    },
    {
      "chapter_number": 37,
      "title": "Segment 37 (pages 746-766)",
      "start_page": 746,
      "end_page": 766,
      "summary": "Data verification\nGiven the importance of data quality in the model’s performance, it’s\ncrucial that we have a way to verify the quality of data.\ngenerated data can be measured the same way you’d evaluate other AI\nWhile this section focuses on synthetic data, most of the techniques can be\nused to evaluate the quality of training data in general.\nmost commonly synthesized data.\nMost of the synthetic data used to train\ninstruct the model to determine if a data example meets these requirements.\nDepending on the use case and the generated data, you can also get creative.\nOr, if you want the synthetic data to resemble high-quality academic work,\nYou can have a model to detect the topic of each generated example and\nexpect all data to follow a similar pattern, you can also use anomaly\nJust like real data, synthetic data can also be filtered using heuristics.\n(Wang et al., 2022) filtered out generated examples using the following\nEven though there are many techniques to evaluate synthetic data,\nquality test for AI-generated data is its real-world performance—whether it\ncan improve the model’s performance—and synthetic data has passed this\nLimitations to AI-generated data\nGiven the increasing usefulness of synthetic data, it’s exciting to imagine\nimportance over time, AI-generated data might never entirely replace\nhuman-generated data.\nmodel collapse, and the way AI generation of data obscures its lineage.\nAI’s generated data can be of low quality, and, as people never tire of\nhesitant to use synthetic data if they can’t verify its quality.\nmaking synthetic data more useful.\ntraining data.\nTraining a student model on\nIt’s also unclear how much AI-generated data a model can train on.\nstudies have shown that recursively using AI-generated data in training\nData Makes Models Forget”, Shumailov et al.\nOne possible explanation is that AI models are more likely to generate\nrepresented in the generated data.\nwhile model collapse is inevitable if the entire training dataset is synthetic,\namount of synthetic data.\nFor example, “Common 7B Language Models\nIn their experiments, synthetic data shows no\nAI-generated data might also perpetuate biases.\n2023) demonstrates that when models are trained on datasets that include\nThis limitation of AI-generated data is more subtle.\ndata lineage.\nAI models are influenced by their training data and can\nsay you use model X to generate data to train your model.\ntrained on data with copyright violations, your model might also violate\nHowever, if model X was also trained on\nWe’ve discussed how to use AI to generate data and how to evaluate the\ngenerated data, as well as its limitations.\ngears to discuss one special use case of data synthesis where AI-generated\ndata isn’t just supplementary but is required: model distillation.\nFor example, DistilBERT, a model distilled from\nThe student model can be trained from scratch like DistilBERT or finetuned\nSynthetic instruction data is commonly used together with adapter-based\nmodel using LoRA and examples generated by OpenAI’s text-davinci-003.\nNote that not all training with synthetic data is model distillation.\nModel\nHowever, it’s possible to use synthetic data to train a student\nModel bootstrapping with reverse instruction (Li et al., 2023), discussed in\ninstruction and preference data generated by Mixtral-8x7B-Instruct-v0.1\nThe Llama 3 paper notes that while training on data generated by a more\nindiscriminately on self-generated data doesn’t improve the model’s\nthey were able to continually improve a model using its generated data.\nData Processing\nData needs to be processed according to the requirements of each use case.\nprocessed data.\nif it takes more time to clean each example than to deduplicate data, you might want to remove\nscripts to all your data.\nYou or another team might need to process the data in different ways for other applications.\nInspect Data\nWhere does the data\nDoes the data use any special tokens?\nyou get a distribution of the topics and languages in the data?\nhelpful not only to evaluate data but also to evaluate models.\nOne statistic you can use is the distribution of (verb, direct object noun) in your data.\nPlot these distributions by data source, time, annotator, etc.\nLook at your data to see if the examples make sense.\nIf it’s annotated data,\nDeduplicate Data\nDuplicated data can skew the data distribution and introduce biases into\nyour model.\nWhen splitting duplicated data into train and test sets, one example might\nduplications on model performance; see Lee et al.\nDepending on the data, there are many forms of duplication, some of which\nData deduplication is\ncan deduplicate data:\nClean and Filter Data\nData needs to be cleaned to make your model performant and safe.\nYou also might want to remove low-quality data, using techniques\ndiscussed in “Data verification” to detect low-quality data.\ndata might help you notice patterns that you can use as heuristics to detect\nlow-quality data.\nHeuristics to detect low-quality data might be non-\nIf there is more data than you need or can afford to use (e.g., due to your\nMeta researchers, in their paper on data pruning (Sorscher et al.,\nFormat Data\nEach model uses a\nsufficient training examples, the model can learn the expected behavior of\nThe training data for finetuning will look",
      "keywords": [
        "Data",
        "model",
        "synthetic data",
        "synthetic",
        "generated data",
        "AI-generated data",
        "generated",
        "training data",
        "student model",
        "model distillation",
        "model collapse",
        "training",
        "quality",
        "low-quality data",
        "real data"
      ],
      "concepts": [
        "model",
        "data",
        "training",
        "instruct",
        "instructions",
        "instruction",
        "generate",
        "generated",
        "generation",
        "generations"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 36,
          "title": "",
          "score": 0.863,
          "base_score": 0.713,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 38,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "",
          "score": 0.6,
          "base_score": 0.45,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.529,
          "base_score": 0.529,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.469,
          "base_score": 0.469,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "data",
          "synthetic",
          "generated data",
          "synthetic data",
          "generated"
        ],
        "semantic": [],
        "merged": [
          "data",
          "synthetic",
          "generated data",
          "synthetic data",
          "generated"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.32598328626287876,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604822+00:00"
      }
    },
    {
      "chapter_number": 38,
      "title": "Segment 38 (pages 767-788)",
      "start_page": 767,
      "end_page": 788,
      "summary": "Example training data used for a food classification task.\nOnce the model is finetuned, you can use a prompt as simple as:\nDifferent finetuning data formats can impact your finetuned model’s\nWhen you use the finetuned model, make sure that the prompts you use\nFor example, if the training data\nEven though the actual process of creating training data is incredibly\nTo build a dataset to train a model, you start by thinking\nWhat data you need depends not only on your use case but also on the\nPre-training requires different data from instruction\nWhile how much data a model is trained on grabs headlines, having high-\nto improving their models’ performance.\nWhile generating data programmatically has long\nsynthetic data became a practical solution for many more use cases.\nchapter discussed different techniques for data synthesis with a deep dive\nJust like real data, synthetic data must be evaluated to ensure its quality\nbefore being used to train models.\nEvaluating AI-generated data is just as\ngenerated data that they can reliably evaluate.\nIt’s hard to automate data generation, but it’s even\nWhile data synthesis helps generate more\nevaluate data.\nHow should you serve this model?\ndiscuss how to optimize inference for latency and cost.\nfiltering, and deduplicating, and conducting overlap analysis on the training data.\nin different data processes.\nthrough data providers.\nIf you use a lot of data, ensuring data compliance alone can be a full-time job.\non data quality in a broad range of use cases.\nHere, I want to focus on data quality for finetuning.\nMy book, Designing Machine Learning Systems, discusses data augmentation in Chapter 4.\nOne obvious example that I didn’t include in the main text is when you want to train a model to\nThe implication of this is that, in theory, it’s possible to train a model that can continually improve\nInference Optimization\ndiscussed various techniques for making models better.\nNo matter how good your model is, if it’s too slow, your users might lose\nInference optimization can be done at the model, hardware, and service\nAt the model level, you can reduce a trained model’s size or develop\nThe inference service runs the model on the given hardware to\nmodels for specific hardware.\nThis chapter discusses bottlenecks for AI inference and techniques to\nIt’ll focus mostly on optimization at the model and service\ntechnique that speeds up a model can also reduce its cost.\nreducing a model’s precision makes it smaller and faster.\nmake your model run faster but at a higher cost.\ntechniques will help you evaluate inference services and frameworks.\nThere are two distinct phases in an AI model’s lifecycle: training and\ninference.\nTraining refers to the process of building a model.\nInference\nrefers to the process of using a model to compute an output for a given\nUnless you train or finetune a model, you’ll mostly need to care\nabout inference.\nIn production, the component that runs model inference is called an\nModel APIs like those provided by OpenAI and Google are inference\nHowever, if you host a model\ninference service.\nbottlenecks, compute-bound and memory bandwidth-bound:\nsystem, such as the speed of data movement between memory and\nFor example, if you store your data in the CPU memory\nand train a model on GPUs, you have to move data from the CPU to\nMemory-bound is also used by some people to refer to tasks whose time-to-\ncomputation because of the time it takes to transfer data between the CPU\nThe concepts of compute-bound or memory bandwidth-bound were\nan operation can be classified as compute-bound or memory bandwidth-\ncompute-bound or memory bandwidth-bound, as shown in Figure 9-2.\nFor example, inference for image generators\nlike Stable Diffusion is typically compute-bound, whereas inference for\nautoregression language models is typically memory bandwidth-bound.\nAs an illustration, let’s look into language model inference.\nChapter 2 that inference for a transformer-based language model consists of\nThe model processes the input tokens in parallel.\nThe model generates one output token at a time.\ndata into memory.\nDecoding is, therefore, memory bandwidth-bound.\nAutoregressive language models follow two steps for inference: prefill and decode.\nhardware advancements will be able to make AI and data workloads\nSynthetic data generation\nMigrating to a new model that requires reprocessing of all the data\nautoregressive decoding, it can take a long time for a model to complete a\nA batch API for foundation models differs from batch inference for traditional ML.\nOnline inference means that predictions are computed after requests have arrived.\nBatch inference means that predictions are precomputed before requests have arrived.\nfoundation model use cases where the inputs are open-ended, it’s hard to predict all user prompts.\n(response quality is a property of the model itself, not of the inference\nLatency measures the time from when users send a query until they receive\nTTFT measures how quickly the first token is generated after users\nTime per output token\nTPOT measures how quickly each output token is generated after the\nIn the streaming mode, where users read each token as it’s generated,\ninvolving CoT (chain-of-thought) or agentic queries where models generate\npublish to make it explicit that it measures time to the first token users see.\nConsider the scenario where, after a user sends a query, the model performs\n3. Based on these outputs, generate a final response to show the user.\nFrom the model’s perspective, the first token is generated in step 1.\nwhen the model internally begins its token generation process.\nhowever, only sees the first token of the final output generated in step 3.\nhalf of the requests take longer than 100 ms to generate the first token, and\nThroughput measures the number of output tokens per second an inference\nservice can generate across all users and requests.\nHowever, since processing input tokens (prefilling) and generating output\ntokens (decoding) have different computational bottlenecks and are often\ndecoupled in modern inference servers, input and output throughput should\nHowever, for applications built on top of foundation models, a request\nan inference service handles concurrent requests.",
      "keywords": [
        "data",
        "model",
        "inference",
        "inference service",
        "memory",
        "tokens",
        "n’t",
        "users",
        "memory bandwidth-bound",
        "latency",
        "TTFT",
        "Inference Optimization",
        "requests",
        "time",
        "output tokens"
      ],
      "concepts": [
        "model",
        "generating",
        "generation",
        "generate",
        "generators",
        "latency",
        "users",
        "different",
        "difference",
        "differs"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 37,
          "title": "",
          "score": 0.86,
          "base_score": 0.71,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 35,
          "title": "",
          "score": 0.768,
          "base_score": 0.618,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 36,
          "title": "",
          "score": 0.758,
          "base_score": 0.608,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.592,
          "base_score": 0.592,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 30,
          "title": "",
          "score": 0.536,
          "base_score": 0.536,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "inference",
          "bound",
          "data",
          "memory",
          "memory bandwidth"
        ],
        "semantic": [],
        "merged": [
          "inference",
          "bound",
          "data",
          "memory",
          "memory bandwidth"
        ]
      },
      "topic_id": 5,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3892686143996824,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604871+00:00"
      }
    },
    {
      "chapter_number": 39,
      "title": "Segment 39 (pages 789-807)",
      "start_page": 789,
      "end_page": 807,
      "summary": "Throughput is directly linked to compute cost.\nIf your system costs $2/h in compute and its\neach request generates 200 output tokens on average, the cost for decoding\nSmaller models and higher-end chips typically result in\nEven for similarly sized models, hardware, and workloads, direct\nmetrics such as cost per request.\nA common but often misunderstood metric is GPU utilization, and NVIDIA\nOne metric this tool shows is GPU utilization,\nGPU utilization would be 50%.\nabout, out of all the operations a machine is capable of computing, is how\nThis metric is called MFU (Model FLOP/s\nSimilarly, because memory bandwidth is expensive, you might also want to\nknow how efficiently your hardware’s bandwidth is utilized.\nBandwidth Utilization) measures the percentage of achievable memory\nIf the chip’s peak bandwidth is 1 TB/s and your inference\nComputing the memory bandwidth being used for LLM inference is\nIf this is done on an A100-80GB GPU with a theoretical 2 TB/s of memory\nWhat’s considered a good MFU and MBU depends on the model, hardware,\nCompute-bound workloads typically have higher MFU and\nlower MBU, while bandwidth-bound workloads often show lower MFU\nis typically higher than MFU for inference.\ncompute-bound and decode is memory bandwidth-bound, MFU during\nMFU for several models and accelerators.\nchips\nFigure 9-5 shows the MBU for the inference process using Llama 2-70B in\ncomputational load per second with more users, shifting the workload from\nBandwidth utilization for Llama 2-70B in FP16 across three different chips shows a\nutilization rates for similar workloads on the same hardware generally mean\nget the chips with the highest utilization.\ncompute.\nAn accelerator is a chip designed to accelerate a specific type of\ncomputational workload.\nAn AI accelerator is designed for AI workloads.\ncapabilities, it imposes challenges on memory design and power\nThe success of NVIDIA GPUs has inspired many accelerators designed to\nWhile many chips can handle both training and inference, one big theme\nemerging is specialized chips for inference.\n(2023) shares that inference can exceed the cost of training in commonly\nAs discussed in Chapter 7, training demands much more memory due to\nConsequently, chips designed for inference are often optimized for lower\nprecision and faster memory access, rather than large memory capacity.\nChips designed for\nThere are also chips specialized for different model architectures, such as\nMany chips are designed for data\nDifferent hardware architectures have different memory layouts and\nA chip might have a mixture of different compute units optimized for\noperations, but many modern GPUs now include tensor cores optimized for\ntensor operations as their primary compute primitive.\na model on a hardware architecture, its memory layout and compute\nmemory size and bandwidth, and power consumption.\nComputational capabilities are typically measured by the number of\noperations a chip can perform in a given time.\nThe number of operations a chip can perform in a second depends on the\nnumerical precision—the higher the precision, the fewer operations the chip\noperations a chip can perform in a given time is not exactly half that of 16-\nbit operations because of different chips’ optimization.\nNVIDIA H100 SXM chips.\nFLOP/s specs for NVIDIA H100 SXM chips.\nMemory size and bandwidth\nmoved from the memory to these cores, and, therefore, data transfer speed\nTherefore, GPU memory needs to have higher bandwidth and lower latency\nthan CPU memory, and thus, GPU memory requires more advanced\nThis is one of the factors that makes GPU memory\nbandwidth memory), which has a 3D stacked structure.\nAn accelerator’s memory is measured by its size and bandwidth.\naccelerator, such as a GPU, typically interacts with three levels of memory,\nCPU memory usually has the lowest bandwidth among these\nmemory types, with data transfer speeds ranging from 25 GB/s to 50\nGPU high-bandwidth memory (HBM)\nfor faster access than CPU memory.\nGPU on-chip SRAM\nIntegrated directly into the chip, this memory is used to store\non-chip memory, which also includes other components like register\nThe memory hierarchy of an AI accelerator.\nChips rely on transistors to perform computation.\nEach computation is done\nAccelerators typically specify their power consumption under maximum\nwhen the chip operates under typical workloads.\nWhat accelerators to use depends on your workload.\ncompute-bound, you might want to look for chips with more FLOP/s.\nyour workloads are memory-bound, shelling out money for chips with\nhigher bandwidth and more memory will make your life easier.\nFLOP/s, memory size, and memory bandwidth are the three big numbers",
      "keywords": [
        "GPU",
        "memory",
        "GPUs",
        "MFU",
        "CPU memory",
        "GPU utilization",
        "FLOP",
        "inference",
        "chips",
        "NVIDIA GPU utilization",
        "Utilization",
        "MBU",
        "GPU memory",
        "bandwidth",
        "Throughput"
      ],
      "concepts": [
        "compute",
        "computing",
        "computations",
        "typically",
        "typical",
        "chips",
        "throughput",
        "model",
        "processing",
        "process"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.632,
          "base_score": 0.482,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.589,
          "base_score": 0.439,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "",
          "score": 0.493,
          "base_score": 0.343,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "memory",
          "chips",
          "bandwidth",
          "gpu",
          "utilization"
        ],
        "semantic": [],
        "merged": [
          "memory",
          "chips",
          "bandwidth",
          "gpu",
          "utilization"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.23518408153289944,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604905+00:00"
      }
    },
    {
      "chapter_number": 40,
      "title": "Segment 40 (pages 808-829)",
      "start_page": 808,
      "end_page": 829,
      "summary": "Inference optimization can be done at the model, hardware, or service level.\nModel-level optimization is\nIdeally, optimizing a model for speed and cost shouldn’t change the model’s\nHowever, many techniques might cause model degradation.\nFigure 9-8 shows the same Llama models’ performance on different\nAn inference service provider might use optimization techniques that can alter a model’s\ntechniques at the model and service levels.\nModel Optimization\nModel-level optimization aims to make the model more efficient, often by\nThese models have three\ncharacteristics that make inference resource-intensive: model size,\nModel compression\nModel compression involves techniques that reduce a model’s size.\na model smaller can also make it faster.\ntwo model compression techniques: quantization and distillation.\nQuantization, reducing the precision of a model to reduce its memory\nModel\nmodel, is discussed in Chapter 8.\nCould it be that within the large model,\nThis makes the model more sparse, which both\nreduces the model’s storage space and speeds up computation.\nPruned models can be used as-is or be further finetuned to adjust the\nPruning can help discover promising model\nmodels, and not all hardware architectures are designed to take advantage\nReducing a model’s precision from 32 bits to 16 bits reduces its memory\nAs discussed in Chapter 2, autoregressive language models generate one\ntoken after another.\n100 tokens will take 10 s.\nAcross model API providers, an output token costs approximately two to\nless powerful model to generate a sequence of tokens, which are then\nverified by the target model.\nThe target model is the model you want to use.\n1. The draft model generates a sequence of K tokens: x\n2. The target model verifies these K generated tokens in parallel.\n3. The target model accepts the longest subsequence of draft tokens, from\nleft to right, which the target model agrees to use.\n4. Let’s say the target model accepts j draft tokens, x\nThe target model then generates one extra token, x\nThe process returns to step 1, with the draft model generating K tokens\nIf no draft token is accepted, this loop produces only one token generated\nby the target model.\n1 tokens, with K generated by the draft model and one by the target model.\nA draft model generates a sequence of K tokens, and the main model accepts the longest\nIf all draft sequences are rejected, the target model must generate the entire\n1. The time it takes for the target model to verify a sequence of tokens is\ndraft tokens.\nK mean fewer verifying calls for the target model but a low acceptance rate\nof the draft tokens.\nThe draft model can be of any architecture, though\nmodel.\nYou can train a custom draft model or use an existing weaker model.\nDeepMind trained a 4B-parameter draft model of the same architecture\nThe draft model can generate a token eight times faster\nthan the target model (1.8 ms/token compared to 14.1 ms/token).\nmodel generate these repeated tokens, what if we copy these tokens from\nusing a model to generate draft tokens, it selects draft tokens from the input.\nextra model.\nInstead of making autoregressive generation faster with draft tokens, some\nsequence of tokens x , x ,…,x , these techniques attempt to generate x\nThis means that the model generates x\nThe parallel tokens can be generated by the same decoder, as in Lookahead\nIf the original model\nis trained to predict the next token x\n. These heads are trained together with the original model, but the\ntoken generation by up to 1.9× on their HGX H200 GPUs (Eassa et al.,\nHowever, because these tokens aren’t generated sequentially, they need to\nJacobi method  to verify the generated tokens, which works as follows:\n1. K future tokens are generated in parallel.\ntokens, the model regenerates or adjusts only these failed tokens.\nThe model keeps refining the generated tokens until they all pass\nIn Medusa (Cai et al., 2024), each head predicts several options for a token position.\nRecall from Chapter 2 that generating the next token requires the key and\nvalue vectors for all previous tokens.\nGenerating token x  requires the key and value vectors for tokens x , x ,\nGenerating token x\nrequires the key and value vectors for tokens x ,\nWhen generating token x\nvalue vectors for reuse is called the KV cache.\nTo avoid recomputing the key and value vectors at each decoding step, use a KV cache\nA KV cache is used only during inference, not training.\nsequence are known in advance, next token generation can be computed all at once instead of\nBecause generating a token requires computing the attention scores with all\nprevious tokens, the number of attention computations grows exponentially\ncalculated that for a 500B+ model with multi-head attention, batch size 512,\nand context length 2048, the KV cache totals 3TB (Pope et al., 2022).\nis three times the size of that model’s weights.\nattention mechanism, optimizing the KV cache, and writing kernels for\nThe memory needed for the KV cache, without any optimization, is\nexample, LLama 2 13B has 40 layers and a model dimension of 5,120.\nmemory needed for its KV cache, without any optimization, is 2 × 32 ×\na model’s architecture directly, they can be applied only during training or\nFor example, when generating a new token, instead of attending to all\nprevious tokens, local windowed attention attends only to a fixed size\nwindow of nearby tokens (Beltagy et al., 2020).\nsequence length to a fixed size window, reducing both the KV cache and the\nattending to a window size of 1,000 tokens reduces the KV cache size by 10\nvalue vectors means reducing the KV cache three times.\nmulti-query attention shares key-value vectors across query heads.\nOptimizing the KV cache size\nOther techniques include KV cache quantization (Hooper et al., 2024; Kang\net al., 2024), adaptive KV cache compression (Ge et al., 2023), and\nselective KV cache (Liu et al., 2024).\nWriting kernels for attention computation\nOne of the most well-known kernels optimized for attention computation is\noperations commonly used in a transformer-based model to make them run",
      "keywords": [
        "model",
        "tokens",
        "target model",
        "draft model",
        "attention",
        "cache",
        "draft tokens",
        "decoding",
        "draft",
        "cache size",
        "Inference",
        "memory",
        "techniques",
        "Optimization",
        "size"
      ],
      "concepts": [
        "model",
        "token",
        "attention",
        "decoding",
        "decode",
        "memory",
        "optimization",
        "optimize",
        "optimized",
        "hardware"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.826,
          "base_score": 0.676,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.774,
          "base_score": 0.624,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "",
          "score": 0.691,
          "base_score": 0.541,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.665,
          "base_score": 0.515,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "draft",
          "tokens",
          "target model",
          "cache",
          "kv cache"
        ],
        "semantic": [],
        "merged": [
          "draft",
          "tokens",
          "target model",
          "cache",
          "kv cache"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.33647677547218185,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604948+00:00"
      }
    },
    {
      "chapter_number": 41,
      "title": "Segment 41 (pages 830-852)",
      "start_page": 830,
      "end_page": 852,
      "summary": "broadly across different models, operator fusion requires a deeper\nunderstanding of a model’s specific operators and architecture.\nA model script specifies a series of operations that need to be performed to\nexecute that model.\nCompilers bridge ML models and the\n1. Call torch.compile to compile the model into more efficient kernels.\nThroughput improvement by different optimization techniques in\nunclear how these optimization steps impact the model’s output quality.\nInference Service Optimization\nworkloads (inference requests from users that may involve different\nmodels), the goal is to efficiently allocate resources to these workloads to\noptimize for latency and cost.\nUnlike many model-level techniques, service-\nlevel techniques don’t modify models and shouldn’t change the output\ninference service might receive multiple requests simultaneously.\nprocessing each request separately, batching the requests that arrive around\nprocesses the batch either when it has four requests or when 100 ms has\nDynamic batching keeps the latency manageable but might be less compute-efficient.\nIf one request in a batch generates only 10\nAfter a request in a batch is completed\nand its response returned, the service can add another request into the batch\nWith continuous batching, completed responses can be returned immediately to users,\nOne common optimization technique for inference servers is to\nvolume of processed requests while adhering to latency requirements.\nprefill compute) and latency requirements (e.g., whether you want lower\nPrompt caching\nWithout a prompt cache, your model needs\nWith a prompt cache, the\nsystem prompt needs to be processed just once for the first query.\nPrompt caching is useful for queries that involve long documents.\nFor applications with long system prompts, prompt caching can\ntokens, and your application generates one million model API calls daily, a\nprompt cache will save you from processing approximately one billion\nUnless you use a model API with this functionality, implementing prompt\noffers prompt caching that promises up to 90% cost savings (the longer the\ncached context, the higher the savings) and up to 75% latency reduction.\nThe impact of prompt caching on the cost and latency of different scenarios\nCost and latency reduced by prompt caching.\ncached prompt)\nacross all models are data parallelism and model parallelism.\nAn optimization technique might involve multiple parallelism\nsimply creates multiple replicas of the model you want to serve.\nTrying to fit models of different sizes onto\nmore models, more replicas, and more chips.\nLet’s say you have a mixture of models of different sizes (e.g., 8B, 13B,\nreplicas to create for each model and what GPUs to use for each replica\nmodels on a 40 GB GPU, or should you reserve this GPU for one 34B\nmodel?\nIf you have a fixed number of model replicas, you need to decide what\nOften, your model is so big that it can’t fit into one machine.\nModel\nparallelism refers to the practice of splitting the same model across multiple\nFitting models onto chips can become an even more complicated\nproblem with model parallelism.\nThere are several ways to split a model.\nlarge models that don’t fit on single machines.\nAnother way to split a model is pipeline parallelism, which involves\ndividing a model’s computation into distinct stages and assigning each stage\nAs data flows through the model, each stage processes\nPipeline parallelism enables model splits to be executed in parallel.\nmicro-batch is processed on one machine, its output is passed onto the next\npart of the model on the next machine.\nWhile pipeline parallelism enables serving large models on multiple\nmachines, it increases the total latency for each request due to extra\nprocessing more efficient, including context parallelism and sequence\nIn context parallelism, the input sequence itself is split across different\nIn sequence parallelism, operators needed for the entire input are split\nA model’s usability depends heavily on its inference cost and latency.\nFor language model-based\ninference, latency can be broken into time to first token (TTFT), which is\nHow efficiently a model can run depends on the hardware it is run on.\nwhat it takes to optimize models on different accelerators.\nThe chapter then continued with different techniques for inference\nGiven the availability of model APIs, most application\nmodel APIs. This chapter also focused on optimization at the model level and the\nModel-level optimization often requires changing\nmodel intact and only changes how it’s served.\nModel-level techniques include model-agnostic techniques like quantization\nDifferent model architectures require their own\nFor example, because a key bottleneck of transformer models\nmodel is in its autoregressive decoding process, and consequently, many\nInference service-level techniques include various batching and parallelism\nlanguage models, including prefilling/decoding decoupling and prompt\nPrompt caching, on the other\nparallelism (which both reduces latency and enables serving larger models),\ntransformer models).\nInference optimization concludes the list of model adaptation techniques\nImagine you’re a model provider.\nmodel only makes sense if the money you can recover from inference for a model is more than its\nThe more a model is used in production, the more model providers can\ncalls on top of open source models.\nPrefilling effectively populates the initial KV cache for the transformer model.\nIf you run an inference service, separating your inference APIs into online and batch can help you\ntheir requests to the batch API, so that your service can focus on processing the online API requests\nWhile a chip can be developed to run one model architecture, a model architecture can be developed\ncheaper electricity but can increase network latency, making the data centers less appealing for use\ncases with stringent latency requirements like inference.\nEach token generation step necessitates the transfer of the entire model’s parameters from the\nBecause the model can produce only one token at a time, the process consumes only a small\nThe number of attention computations for an autoregressive model is O(n ).\nConvolution operations are often used in image generation models like Stable Diffusion.\nreading the code is that in a long conversation, it caches the previous messages and processes only\nDuring training, the same technique is called data parallelism.\nfoundation models to specific applications.\nmodels.",
      "keywords": [
        "model",
        "Parallelism",
        "Prompt",
        "INFERENCE",
        "latency",
        "Prompt caching",
        "techniques",
        "Batching",
        "requests",
        "prompt cache",
        "optimization",
        "cost",
        "GPU",
        "GPUs",
        "model parallelism"
      ],
      "concepts": [
        "model",
        "latency",
        "inference",
        "compute",
        "computational",
        "computations",
        "batching",
        "batches",
        "requests",
        "request"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.672,
          "base_score": 0.522,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.572,
          "base_score": 0.422,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 32,
          "title": "",
          "score": 0.557,
          "base_score": 0.407,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 31,
          "title": "",
          "score": 0.525,
          "base_score": 0.375,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 8,
          "title": "",
          "score": 0.487,
          "base_score": 0.337,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "parallelism",
          "inference",
          "latency",
          "prompt caching",
          "prompt"
        ],
        "semantic": [],
        "merged": [
          "parallelism",
          "inference",
          "latency",
          "prompt caching",
          "prompt"
        ]
      },
      "topic_id": 3,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.2935024885198262,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.604986+00:00"
      }
    },
    {
      "chapter_number": 42,
      "title": "Segment 42 (pages 853-871)",
      "start_page": 853,
      "end_page": 871,
      "summary": "model.\nThe model generates a response, which is returned to the user, as\nThe Model API box refers to both third-party APIs (e.g.,\nOpenAI, Google, Anthropic) and self-hosted models.\nserver for self-hosted models is discussed in Chapter 9.\n1. Enhance context input into a model by giving the model access to\n3. Add model router and gateway to support complex pipelines and add\nallow the system to construct the relevant context needed by the model to\ntools that allow the model to automatically gather information through APIs\nContext construction is like feature engineering for foundation models.\ngives the model the necessary information to produce an output.\nuniversally supported by model API providers.\nmodels to use tools.\nHowever, just like models differ in their capabilities, these providers differ\nvector database can accommodate, but a generic model API might let you\neliminated, due to the inherent nature of how models generate responses as\nexternal model APIs when you need to send your data outside your\nsending it to external APIs. Output guardrails\nA model can fail in many different ways.\nThe easiest failure to detect is when a model returns\nFor example, the application expects JSON, and the model generates\nFactually inconsistent responses hallucinated by the model.\nFor example, you ask the model to write an\nAI models are\nto fail before retrying, you send this query to the model twice at the same\nSome teams use a specialized model to decide when to transfer a\nhuman operators when their sentiment analysis model detects anger in\nguardrails because they can significantly increase the application’s latency.\nself-host your models or use third-party APIs. While you can implement\nneed to implement since API providers typically provide many guardrails\nModel providers give their\nmodels guardrails to make their models better and more secure.\nof risks in inputs and outputs, a guardrail solution will likely provide\nSome model gateways also provide\nmodel APIs since scorers are often AI-powered, even if scorers are typically\nsmaller and faster than generative models.\nApplication architecture with the addition of input and output guardrails.\nAdd Model Router and Gateway\nAs applications grow to involve more models, routers and gateways emerge\nto help you manage the complexity and costs of serving multiple models.\nInstead of using one model for all queries, you can have different solutions\nallows specialized models, which can potentially perform better than a\ngeneral-purpose model for specific queries.\nmodel for all queries, you can route simpler queries to cheaper models.\nOther routers can aid the model in deciding what to do next.\nnext-action predictor: should the model use a code interpreter or a search\nFor a model with a memory system, a router can predict which\npart of the memory hierarchy the model should pull information from.\nMelbourne?” The model needs to decide whether to rely on the information\nfoundation models.\nMany teams adapt smaller language models like GPT-2,\nWhen routing queries to models with varying context limits, the query’s\nquery that is slated for a model with a 4K context limit.\nmodel or route the query to a model with a larger context limit.\nBecause routing is usually done by models, I put routing inside the Model\nmodels used for generation.\nGrouping routers together with other models makes models easier to\nA model gateway is an intermediate layer that allows your organization to\ninterface with different models in a unified and secure manner.\nbasic functionality of a model gateway is to provide a unified interface to\ndifferent models, including self-hosted models and models behind\ncommercial APIs. A model gateway makes it easier to maintain your code.\nIf a model API changes, you only need to update the gateway instead of\nlevel visualization of a model gateway.\nA model gateway provides a unified interface to work with different models.\nIn its simplest form, a model gateway is a unified wrapper.\ncode example gives you an idea of how a model gateway might be\ndef openai_model(input_data, model_name, max_toke\nengine=model_name,\ndef gemini_model(input_data, model_name, max_toke\nresponse = model.generate_content(input_data,\n@app.route('/model', methods=['POST'])\ndef model_gateway():\nmodel_type = data.get(\"model_type\")\nmodel_name = data.get(\"model_name\")\nif model_type == \"openai\":\nresult = openai_model(input_data, m\nelif model_type == \"gemini\":\nresult = gemini_model(input_data, m\nA model gateway provides access control and cost management.\nmodel gateway, creating a centralized and controlled point of access.\nuser or application should have access to which model.\nA model gateway can also be used to implement fallback policies to\nalternative models, retry after a short wait, or handle failures gracefully in\nSince requests and responses are already flowing through the gateway, it’s a\nIn our architecture, the gateway now replaces the model API box, as shown\nFor example, if a user asks a model to summarize a product, the",
      "keywords": [
        "model",
        "model gateway",
        "API",
        "Model API",
        "guardrails",
        "gateway",
        "APIs",
        "query",
        "context",
        "Model API box",
        "response",
        "model APIs",
        "application",
        "information",
        "data"
      ],
      "concepts": [
        "model",
        "response",
        "guardrails",
        "gateway",
        "api",
        "apis",
        "different",
        "differ",
        "context",
        "user"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.681,
          "base_score": 0.531,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 18,
          "title": "",
          "score": 0.678,
          "base_score": 0.528,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.674,
          "base_score": 0.524,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.576,
          "base_score": 0.426,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "gateway",
          "model gateway",
          "guardrails",
          "api",
          "input_data"
        ],
        "semantic": [],
        "merged": [
          "gateway",
          "model gateway",
          "guardrails",
          "api",
          "input_data"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3428741498764786,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605032+00:00"
      }
    },
    {
      "chapter_number": 43,
      "title": "Segment 43 (pages 872-889)",
      "start_page": 872,
      "end_page": 889,
      "summary": "How long to keep a query in the cache depends on how likely this query is\nUser-specific queries, such as “What’s the status of my\nSimilarly, it makes less sense to cache time-sensitive\npredict whether a query should be cached.\nuser X’s information and then generates a response containing X’s information.\ncached result is returned, revealing X’s information to Y.\nSemantic caching\nsemantic caching, the system can reuse the answer from the first query\nsemantic caching can reduce your model’s performance.\nSemantic caching works only if you have a reliable way of determining if\ntwo queries are similar.\n1. For each query, generate its embedding using an embedding model.\n2. Use vector search to find the cached embedding with the highest similar\n3. If X is higher than a certain similarity threshold, the cached query is\nconsidered similar, and the cached results are returned.\nthis current query and cache it together with its embedding and results.\nqueries.\nsimilarity metric.\nfor one similar to another query, the returned response, fetched from the\nIn addition, semantic cache can be time-consuming and compute-intensive,\ncomplexities of a semantic cache, make sure to evaluate the associated\nWith the added cache systems, the platform looks like Figure 10-8.\ncache and prompt cache are typically implemented by model API providers,\ncache.\nAn AI application architecture with the added caches.\nEach query follows\ncomplex applications.\nA model’s outputs also can be used to invoke write actions, such as\ncan help discover opportunities for application improvement and cost\nThree metrics can help evaluate the quality of your system’s observability,\nEvaluation metrics should translate well to monitoring\nmetrics, meaning that a model that does well during evaluation should also\nmonitor the external outputs of the system to figure out when something\noutputs will help you figure out what goes wrong.\nlooking at the system’s logs and metrics without having to ship new code to\nMetrics\nWhen discussing monitoring, most people think of metrics.\nwhat your application’s output relevancy score is unless it serves a purpose.\nThe purpose of a metric is to tell you when something is wrong and to\nBefore listing what metrics to track, it’s important to understand what\nfailure modes you want to catch and design your metrics around these\ndesign metrics that help you detect hallucinations.\nOne relevant metric\nmight be whether an application’s output can be inferred from the context.\nIf you don’t want your application to burn through your API credit, track\nmetrics related to API costs, such as the number of input and output tokens\nper request or your cache’s cost and your cache’s hit rate.\nBecause foundation models can generate open-ended outputs, there are\nMetrics design requires analytical\nWhich metrics you\nshould track are highly application-specific.\nThis book has covered many different types of model quality metrics\nrelevant generation quality metrics such as conciseness, creativity, or\nMany of these metrics can be computed using AI judges.\nIf safety is an issue, you can track toxicity-related metrics and detect private\nFor example, some easy metrics you can track\nWhat’s the model’s output token distribution?\nLength-related metrics are also important for tracking latency and costs, as\nEach component in an application pipeline has its own metrics.\nexample, in a RAG application, the retrieval quality is often evaluated using\nGiven that you’ll likely have multiple metrics, it’s useful to measure how\nstar metrics, which can be DAU (daily active user), session duration (the\nlength of time a user spends actively engaged with the application), or\nMetrics that are not at all\nCommon latency metrics, as discussed in Chapter 9, include:\nTime per output token (TPOT): the time it takes to generate each output\nTrack all these metrics per user to see how your system scales with more\nCost-related metrics are the number of\nqueries and the volume of input and output tokens, such as tokens per\nHowever, there are many questions that metrics\n1. Metrics tell you something went wrong five minutes ago, but they don’t\n3. Correlate the errors in the logs to the metrics to make sure that you’ve\nFor fast detection, metrics need to be computed quickly.\nLog the user query, the final prompt sent to the model, the output, and the\nLog the tool outputs.\nentire process from when a user sends a query to when the final response is\nretrieved context was irrelevant, or the model generated a wrong response.\nexample, people have already figured out how to frame their queries\nIf you look only at metrics, it might not",
      "keywords": [
        "system",
        "metrics",
        "application",
        "model",
        "query",
        "cache",
        "Monitoring",
        "outputs",
        "user",
        "time",
        "logs",
        "System prompt",
        "observability",
        "prompt",
        "log"
      ],
      "concepts": [
        "metric",
        "cache",
        "cached",
        "user",
        "query",
        "queries",
        "application",
        "applications",
        "monitoring",
        "response"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "",
          "score": 0.639,
          "base_score": 0.489,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 25,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "",
          "score": 0.34,
          "base_score": 0.34,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 39,
          "title": "",
          "score": 0.324,
          "base_score": 0.324,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 13,
          "title": "",
          "score": 0.323,
          "base_score": 0.323,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "metrics",
          "cache",
          "query",
          "semantic",
          "track"
        ],
        "semantic": [],
        "merged": [
          "metrics",
          "cache",
          "query",
          "semantic",
          "track"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.19004999541241466,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605064+00:00"
      }
    },
    {
      "chapter_number": 44,
      "title": "Segment 44 (pages 890-909)",
      "start_page": 890,
      "end_page": 909,
      "summary": "the orchestrator the steps your system takes from receiving the user\n4. The model generates a response based on the prompt.\n6. If the response is considered good, return it to the user.\nThe orchestrator is responsible for passing data between components.\nto use a Llama model, check if the orchestrator supports that.\nConsider the user-friendliness of the orchestrator.\nUser Feedback\nUser feedback has always played a critical role in software applications in\nHowever, in AI applications, user feedback takes on an even\nUser feedback is proprietary data, and data is a\nA well-designed user feedback system is necessary\nUser feedback can be used not only to personalize models for individual\nusers but also to train future iterations of the models.\nIt’s important to remember that user feedback is user data.\nLeveraging user\nUser\nUsers have the right to know how their data is\nExtracting Conversational Feedback\ninformation users provide in response to explicit requests for feedback in\nImplicit feedback is information inferred from user actions.\nwhat actions a user can do within each application and is, therefore, highly\napplications and, with them, many genres of implicit feedback.\nThe conversational interface that many AI applications use makes it easier\nfor users to give feedback.\nlanguage that a user uses to give directions to AI can convey feedback about\nboth the application’s performance and the user’s preference.\nUser feedback, extracted from conversations, can be used for evaluation,\nPersonalization: personalize the application to each user\nImplicit conversational feedback can be inferred from both the content of\nBecause feedback is\nsignals to look for, rigorous data analysis and user studies are necessary to\nfeedback.\nIf a user terminates a response early, e.g., stopping a response generation\nIf a user starts their follow-up with “No, …” or “I meant, …”, the model’s\nTo correct errors, users might try to rephrase their requests.\nshows an example of a user’s attempt to correct the model’s\nBecause the user both terminates the generation early and rephrases the question, it can\nUsers can also point out specific things the model should’ve done\nFor example, if a user asks the model to summarize a story and\nthe model confuses a character, this user can give feedback such as: “Bill is\nthe suspect, not the victim.” The model should be able to take this feedback\nuse cases where users might nudge the agent toward more optional actions.\nFor example, if a user assigns the agent the task of doing market analysis\nabout company XYZ, this user might give feedback such as “You should\nSometimes, users might want the model to correct itself by asking for\nthe user is looking for.\nSome applications let users edit the model’s responses directly.\nexample, if a user asks the model to generate code, and the user corrects the\nUser edits also serve as a valuable source of preference data.\nEach user edit\nOften, users just complain about your application’s outputs without trying\nthe user to find out the answer on\nUnderstanding how the bot fails the user is crucial in making it better.\nexample, if you know that the user doesn’t like verbose answers, you can\nIf the user is unhappy\nSome call centers track users’ voices throughout the\nNatural language feedback can also be inferred from the model’s responses.\nOther conversational feedback\nOther types of conversational feedback can be derived from user actions\nMany applications let users generate another response, sometimes with a\nresponse is adequate, but the user wants options to compare.\nWith usage-based billing, users\nChatGPT asks for comparative feedback when a user regenerates another response.\nembarrassing conversation and the user wants to remove its trace.\nAI companions, a long conversation might indicate that the user enjoys the\ninefficient in helping users resolve their issues.\nusers.\nSince many users may not be willing to put in this additional work,\nexplicit feedback can be sparse, especially in applications with smaller user\nunhappy users might be more likely to complain, causing the feedback to\nmine mostly shares conversations when the model has made some glaring\nIt’s important to study your users to understand why they do\nFor example, if the user\nFeedback can and should be collected throughout the user journey.\nUsers\nfew places where user feedback might be particularly valuable.\nWhen a user has just signed up, user feedback can help calibrate the\napplication for the user.\nFor other applications, however, initial feedback should be\noptional, as it creates friction for users to try out your product.\nIf a user\ngenerates a compromising image, or takes too long to respond, users should\nYou can give users the option to\nUsers might just give conversational feedback like “You’re wrong”,\nIdeally, when your product makes mistakes, users should still be able to\nproduct, users can edit the category.\nLet users collaborate with the AI.\nsupport bots offer to transfer users to human agents if the conversation\ndrags on or if users seem frustrated.\nIf a generated image isn’t exactly what the user needs,",
      "keywords": [
        "Feedback",
        "user",
        "User Feedback",
        "model",
        "response",
        "conversation",
        "natural language feedback",
        "language feedback",
        "data",
        "applications",
        "Conversational Feedback",
        "orchestrator",
        "Explicit feedback",
        "give feedback",
        "Implicit feedback"
      ],
      "concepts": [
        "users",
        "model",
        "feedback",
        "conversational",
        "conversations",
        "conversely",
        "data",
        "response",
        "responsible",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.682,
          "base_score": 0.532,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 45,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.65,
          "base_score": 0.5,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 42,
          "title": "",
          "score": 0.575,
          "base_score": 0.425,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.547,
          "base_score": 0.397,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feedback",
          "user",
          "users",
          "user feedback",
          "conversational"
        ],
        "semantic": [],
        "merged": [
          "feedback",
          "user",
          "users",
          "user feedback",
          "conversational"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3030633950471632,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605105+00:00"
      }
    },
    {
      "chapter_number": 45,
      "title": "Segment 45 (pages 910-929)",
      "start_page": 910,
      "end_page": 929,
      "summary": "When a model is uncertain about an action, you can ask the user for\nThe user can\nShowing two full responses for the user to choose means asking that user\nfor explicit feedback.\nUsers might not have time to read two full responses\nUsers can click to expand the response\nresponses side by side gives more reliable feedback.\nGoogle Gemini shows partial responses side by side for comparative feedback.\nUsers\nhave to click on the response they want to read more about, which gives feedback about which\nWhen unsure if two people are the same, it can ask you for feedback,\nGoogle Photos asks for user feedback when unsure.\nagainst asking for both positive and negative feedback.\nAsking for feedback on good results\npositive feedback because it reveals the features users love enough to give\nenthusiastic feedback about.\nSome avoid asking for positive feedback out of concern it may clutter the\nFor example, if you have a large user base,\nsufficient feedback without disrupting the experience for most users.\nfeedback biases.\nHow to collect feedback\nFeedback should seamlessly integrate into the user’s workflow.\neasy for users to provide feedback without extra work.\nFeedback collection\nshould be incentives for users to give good feedback.\nOne example often cited as good feedback design is from the image\n(four) images and gives the user the following options, as shown in\nMidjourney which of the four photos is considered by the user to be the\nHowever, users might choose to\nproviding feedback.\nmaking it hard to collect high-quality feedback the way integrated products\nThe feedback alone might be helpful for product analytics.\nFor applications without such terms, user feedback might be tied to a user\ncontext for this feedback.\nExplaining to users how their feedback is used can motivate them to give\nmore and better feedback.\nDo you use a user’s feedback to personalize the\ncomparative signals from users, don’t ask them to choose between two\nAn example of ChatGPT asking a user to select the response the user prefers.\na design that can confuse users.\nfeedback.\ncollect feedback.\nBe mindful of whether you want users’ feedback to be private or public.\nexample, if a user likes something, do you want this information shown to\nother users?\nstar rating should’ve been, some users mistakenly picked it for positive reviews.\nexample, hiding likes prevents users from finding tweets their connections\nFeedback Limitations\nThere’s no doubt of the value of user feedback to an application developer.\nLike any other data, user feedback has biases.\nthese biases and design your feedback system around them.\nHere are a few examples of feedback biases\nlook at the distribution of your user ratings to detect this bias.\nFor example, instead of showing users numbers one to five,\nshow users options such as the following:\nUsers often provide random feedback, not out of malice, but because\nThe position in which an option is presented to users influences how\nUsers are generally more likely to click on\nIf a user clicks on the first\nWhen designing your feedback system, this bias can be mitigated by\nMany other biases can affect a person’s feedback, some of which\nIt’s important to inspect your user feedback to uncover its biases.\nUnderstanding these biases will help you interpret the feedback correctly,\nKeep in mind that user feedback is incomplete.\nYou only get feedback on\nwhat you show users.\nIn a system where user feedback is used to modify a model’s behavior,\nImagine that initially, a small number of users give feedback that they like\nActing on user feedback can also turn a conversational agent into, for lack\non user feedback can teach it to give users what it thinks users want, even if\n(2023) show that AI models trained on human feedback tend toward.\nThey are more likely to present user responses matching this\nUser feedback is crucial for improving user experience, but if used\nincorporating feedback into your product, make sure that you understand\nconversational feedback and how to design your application to effectively\nTraditionally, user feedback design has been seen as a product responsibility\nHowever, since user feedback is a crucial source of data for\nIt’s possible that users ask the model to return an empty response.\nis that it’s a lot harder to collect user feedback.\nUsers can take your open source application and\nNot only can you collect feedback about AI applications, you can use AI to analyze feedback, too.\nshowing full responses gives more reliable feedback because it gives users more information to make\nAt the same time, some people think that once users have read full responses, there’s no",
      "keywords": [
        "feedback",
        "user feedback",
        "Users",
        "n’t",
        "system",
        "users give feedback",
        "give",
        "product",
        "good",
        "photos",
        "application",
        "bias",
        "responses",
        "model",
        "biases"
      ],
      "concepts": [
        "user",
        "feedback",
        "likes",
        "liked",
        "liking",
        "production",
        "product",
        "biases",
        "bias",
        "good"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 44,
          "title": "",
          "score": 0.669,
          "base_score": 0.519,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 22,
          "title": "",
          "score": 0.555,
          "base_score": 0.405,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.465,
          "base_score": 0.315,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.421,
          "base_score": 0.271,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 23,
          "title": "",
          "score": 0.395,
          "base_score": 0.245,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "feedback",
          "user",
          "users",
          "user feedback",
          "biases"
        ],
        "semantic": [],
        "merged": [
          "feedback",
          "user",
          "users",
          "user feedback",
          "biases"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.1975202016537113,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605141+00:00"
      }
    },
    {
      "chapter_number": 46,
      "title": "Segment 46 (pages 930-949)",
      "start_page": 930,
      "end_page": 949,
      "summary": "models.\nMore systematic approaches to AI engineering, such as those\nFor more resources about AI engineering, check out the book’s GitHub\nAI engineering has a lot of challenges.\nFoundation models as planners\nAI application planning (see application planning)\nAI engineering (AIE)\ndefined, From Foundation Models to AI Engineering\nML engineering versus, AI Engineering Versus ML Engineering-AI\nrise of AI engineering, The Rise of AI Engineering-From Foundation\nModels to AI Engineering\nAI engineering architecture (see engineering architecture)\nAI engineering stack (see engineering stack)\n(see also AI-as-a-judge)\nAI systems evaluation (see systems evaluation)\nAI-as-a-judge, AI as a Judge-What Models Can Act as Judges?\nreasons, Why AI as a Judge?\nreference-based, What Models Can Act as Judges?\nAI-powered data synthesis (see data synthesis, AI-powered)\nanomaly detection, Similarity Measurements Against Reference Data\ninverse scaling and alignment training, Model Size\napplication building, Introduction to Building AI Applications with\nFoundation Models-Summary\napplication planning, Planning AI Applications-Maintenance\nuse case evaluation, Use Case Evaluation-AI product defensibility\nengineering stack, The AI Engineering Stack-AI Engineering Versus\nAI engineering versus ML engineering, AI Engineering Versus ML\nEngineering-AI interface\nfull-stack engineering versus, AI Engineering Versus Full-Stack\nLayers of the AI Stack\nfoundation model use cases, Foundation Model Use Cases-Workflow\nrise of AI engineering, The Rise of AI Engineering-From Foundation\nModels to AI Engineering\nfoundation models to AI engineering, From Foundation Models to\nAI Engineering-From Foundation Models to AI Engineering\napplication development, Three Layers of the AI Stack, Application\ndevelopment-AI interface\nprompt engineering and context construction, Prompt engineering and\napplication planning, Planning AI Applications-Maintenance\nuse case evaluation, Use Case Evaluation-AI product defensibility\noptimization, Attention mechanism optimization-Writing kernels for\nwiring kernels for attention computation, Writing kernels for\naugmentation of data\ndefined, Data Augmentation and Synthesis\ndata contamination detection, Perplexity Interpretation and Use Cases\nmodel-centric versus data-centric, Dataset Engineering\nTime to Think, Data Curation\ncomparative evaluation, Ranking Models with Comparative\neffect on AI investment, From Foundation Models to AI Engineering\nPrompt Engineering\nChinchilla scaling law, Scaling law: Building compute-optimal models\nCLIP, From Large Language Models to Foundation Models, Domain-\nSpecific Models, Introduction to Embedding\nclustering, Similarity Measurements Against Reference Data\nCommon Crawl dataset, Training Data-Multilingual Models\ncomparative evaluation, Ranking Models with Comparative Evaluation-\ncomparison data, Reward model\ncomputational capabilities, of AI accelerators, Computational\ncompute-optimal models, Scaling law: Building compute-optimal\nmodels-Scaling law: Building compute-optimal models\nmodels\ncontext construction, Prompt engineering and context construction,\ncopyright, model training and, Data lineage and copyright\ndata augmentation, Data Augmentation and Synthesis-Model Distillation\ndefined, Data Augmentation and Synthesis\ndata contamination, Data contamination with public benchmarks-\ndata synthesis, Data Augmentation and Synthesis-Model Distillation\nAI-powered, AI-Powered Data Synthesis-Obscure data lineage\ndata synthesis\nlimitations, Limitations to AI-generated data-Obscure data lineage\nModel Distillation\ndata-centric view of AI, Dataset Engineering\ndefensive prompt engineering\nbase, Base models\ndomain-specific training data models, Domain-Specific Models-Domain-\nSpecific Models\nElo, Ranking Models with Comparative Evaluation, Scalability\nembedding model, From Large Language Models to Foundation Models\nembedding models, Introduction to Embedding\nengineering architecture, AI Engineering Architecture-AI Pipeline\nstep 3: adding model router and gateway, Step 3.\nAdd Model Router\nengineering stack, Three Layers of the AI Stack-Three Layers of the AI\napplication development, Three Layers of the AI Stack\nprompt engineering and context construction, Prompt engineering\ninfrastructure, Three Layers of the AI Stack\nML engineering versus, Model development-Inference optimization\nmodel development, Three Layers of the AI Stack\nAI as a judge, AI as a Judge-What Models Can Act as Judges?\nAI systems evaluation (see systems evaluation)\nchallenges of foundation model evaluation, Challenges of Evaluating\nFoundation Models-Challenges of Evaluating Foundation Models\nlanguage model for computing text perplexity, Perplexity",
      "keywords": [
        "data",
        "foundation models",
        "engineering",
        "models",
        "prompt engineering",
        "data synthesis",
        "Data Augmentation",
        "evaluation",
        "Engineering foundation models",
        "engineering versus",
        "Evaluation prompt engineering",
        "prompt",
        "foundation",
        "comparative evaluation",
        "context"
      ],
      "concepts": [
        "models",
        "evaluation",
        "evaluating",
        "engineering",
        "write",
        "writing",
        "applications",
        "computational",
        "compute",
        "prompt"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 1,
          "title": "",
          "score": 0.846,
          "base_score": 0.696,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 14,
          "title": "",
          "score": 0.736,
          "base_score": 0.586,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 16,
          "title": "",
          "score": 0.726,
          "base_score": 0.576,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 5,
          "title": "",
          "score": 0.699,
          "base_score": 0.549,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 20,
          "title": "",
          "score": 0.635,
          "base_score": 0.485,
          "topic_boost": 0.15,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "engineering",
          "ai",
          "ai engineering",
          "stack",
          "data"
        ],
        "semantic": [],
        "merged": [
          "engineering",
          "ai",
          "ai engineering",
          "stack",
          "data"
        ]
      },
      "topic_id": 1,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3413827234022581,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605183+00:00"
      }
    },
    {
      "chapter_number": 47,
      "title": "Segment 47 (pages 950-972)",
      "start_page": 950,
      "end_page": 972,
      "summary": "rank models with comparative evaluation, Ranking Models with\nstep 3: defining evaluation methods and data, Step 3.\nEvaluation Methods and Data-Iterate\nevaluation data\nfederated learning, Model Merging and Multi-Task Finetuning\nwhen the model has low confidence, When the model has low\nconfidence-When the model has low confidence\ndefined, Modeling and training\ntechniques, Finetuning Techniques-Prompt loss weight\nmodel merging and multi-task finetuning, Model Merging and\nFLOP (floating point operation), Model Size\nfoundation models, From Foundation Models to AI Engineering,\nUnderstanding Foundation Models-Summary\nevaluation challenges, Challenges of Evaluating Foundation Models-\nChallenges of Evaluating Foundation Models\ninverse scaling, Model Size\nmodeling, Modeling-Scaling bottlenecks\nmodel size, Model Size-Scaling bottlenecks\npost-training, Post-Training-Finetuning using the reward model\nreward model\ntraining data, Training Data-Domain-Specific Models\ndomain-specific models, Domain-Specific Models-Domain-\nSpecific Models\nuse cases, Foundation Model Use Cases-Workflow Automation\nfull finetuning, Parameter-Efficient Finetuning-Quantized LoRA\nGemini, Evaluation, Test Time Compute, Prompt caching, When the\nmodel has low confidence\nH3 architecture, Other model architectures\nhard attributes, Model Selection Workflow\nKV cache size calculation, Attention mechanism optimization\nat model/hardware/service levels, Inference Optimization\nmodel optimization, Model Optimization-Kernels and compilers\noptimization-Writing kernels for attention computation\nmodel compression, Model compression\ndefined, Open source models versus model APIs\nJamba architecture, Other model architectures\nlanguage modeling metrics, Understanding Language Modeling Metrics-\nlanguage models, Language models-Language models, Perplexity\nlarge language models, From Large Language Models to Foundation\nModels-From Large Language Models to Foundation Models\nlarge multimodal model (LMM), From Large Language Models to\nFoundation Models\ninference optimization, Kernels and compilers\nmodel distillation, Model Distillation\nopen source models, Open source, open weight, and model licenses\nscaling law and, Scaling law: Building compute-optimal models\nLMM (large multimodal model), From Large Language Models to\nFoundation Models\nMamba architecture, Other model architectures\nmasked language models, Language models\nmatches, Ranking Models with Comparative Evaluation\nMBU (model bandwidth utilization), Utilization, MFU, and MBU-\nlanguage modeling (see language modeling metrics)\nMFU (model FLOPs utilization), Utilization, MFU, and MBU-\nmixture-of-experts (MoE) models, Model Size, Layer stacking\nmodel APIs, open source models versus (see open source models, model\nmodel bandwidth utilization (MBU), Utilization, MFU, and MBU-\nmodel compression, Model compression\nmodel development, Three Layers of the AI Stack, Model development-\nInference optimization\nmodeling and training, Modeling and training-Modeling and training\nmodel distillation, Model Distillation\nmodel FLOPs utilization (MFU), Utilization, MFU, and MBU-\nmodel inference, Maintenance\nmodel merging, Model Merging and Multi-Task Finetuning-\nmodel optimization, Model Optimization-Kernels and compilers\noptimization-Writing kernels for attention computation\nmodel compression, Model compression\nmodel ranking, Ranking Models with Comparative Evaluation-The\nmodel router, Step 3.\nAdd Model Router and Gateway-Gateway\nmodel selection, Model Selection-Handling data contamination\nmodel build versus buy, Model Build Versus Buy-On-device\nopen source models versus model APIs, Open source models\nversus model APIs-On-device deployment\nopen source, open weight, and model licenses, Open source, open\nweight, and model licenses-Open source, open weight, and model\nmodel selection workflow, Model Selection Workflow-Model\nmodel size, Model Size-Scaling bottlenecks\nscaling law: building compute-optimal models, Scaling law: Building\ncompute-optimal models-Scaling law: Building compute-optimal\nmodels\nmodel-centric AI, Dataset Engineering\nmodel-level defense, Model-level defense\nmodeling, Modeling-Scaling bottlenecks\nmodel size, Model Size-Scaling bottlenecks\nMoE (mixture-of-experts) models, Layer stacking\nmulti-task finetuning, Model Merging and Multi-Task Finetuning\nmultilingual training data models, Multilingual Models-Multilingual\nModels\nmultimodal models, From Large Language Models to Foundation\nModels\nOpen CLIP, Domain-Specific Models\nopen source licenses, Open source, open weight, and model licenses-\nOpen source, open weight, and model licenses\nopen source models, model APIs versus, Open source models versus\nmodel APIs-On-device deployment\nopen weight models, Open source, open weight, and model licenses\nfirst GPT model, Self-supervision\ninstruction hierarchy for model-level defense, Model-level defense\nmodel as a service, From Foundation Models to AI Engineering\nnatural language supervision, From Large Language Models to\nFoundation Models\nopen source APIs, Open source models versus model APIs\nprogression/distillation paths, Base models\nquality of updated models, Custom leaderboards with public",
      "keywords": [
        "Model",
        "models versus model",
        "large language models",
        "Open source models",
        "inference performance metrics",
        "inference",
        "language models",
        "foundation models",
        "MBU model inference",
        "Finetuning",
        "memory",
        "source models versus",
        "data",
        "inference optimization",
        "inference performance"
      ],
      "concepts": [
        "models",
        "data",
        "evaluation",
        "evaluating",
        "evaluate",
        "memory",
        "inference",
        "prompt",
        "retrieval",
        "metrics"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 2,
          "title": "",
          "score": 0.761,
          "base_score": 0.611,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 6,
          "title": "",
          "score": 0.711,
          "base_score": 0.561,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.699,
          "base_score": 0.699,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 11,
          "title": "",
          "score": 0.612,
          "base_score": 0.462,
          "topic_boost": 0.15,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 48,
          "title": "",
          "score": 0.598,
          "base_score": 0.598,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "open source",
          "source",
          "open",
          "source models",
          "versus"
        ],
        "semantic": [],
        "merged": [
          "open source",
          "source",
          "open",
          "source models",
          "versus"
        ]
      },
      "topic_id": 0,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.3821687244441855,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605231+00:00"
      }
    },
    {
      "chapter_number": 48,
      "title": "Segment 48 (pages 973-990)",
      "start_page": 973,
      "end_page": 990,
      "summary": "drift detection, Drift detection\npointwise evaluation, Reward model, Ranking Models with Comparative\nEvaluation\npost-training, Modeling and training, Post-Training-Finetuning using the\nreward model\nreward model\nPPO (proximal policy optimization), Finetuning using the reward model\npre-training, Modeling and training\nreward model, Finetuning Overview\npreference models, What Models Can Act as Judges?\nproduct quantization, Embedding-based retrieval\nprompt attacks, Defensive Prompt Engineering, Jailbreaking and Prompt\ndefense against, Defenses Against Prompt Attacks-System-level\nVersion Prompts\nevaluating prompt engineering tools, Evaluate Prompt Engineering\nTools-Evaluate Prompt Engineering Tools\nthe Model Time to Think\nOrganize and Version Prompts\ndefensive engineering, Defensive Prompt Engineering-System-level\nproprietary prompts and reverse prompt engineering, Proprietary\nPrompts and Reverse Prompt Engineering-Proprietary Prompts\nand Reverse Prompt Engineering\ndefined, Prompt engineering and context construction\nrestricting model knowledge to its context, Provide Sufficient Context\nterminology ambiguity: prompt versus context, In-Context Learning:\nprompt optimization, Evaluate Prompt Engineering Tools\nPrompts\nEngineering-Proprietary Prompts and Reverse Prompt Engineering\nproximal policy optimization (PPO), Finetuning using the reward model\nQPS (queries per second), Comparing retrieval algorithms\nqueries per second (QPS), Comparing retrieval algorithms\nRAG (retrieval-augmented generation), RAG-RAG with tabular data\nEmbedding-based retrieval\nrating algorithms, Ranking Models with Comparative Evaluation\nrecall, Comparing retrieval algorithms\nreference-based judges, What Models Can Act as Judges?\nreinforcement learning from human feedback (RLHF), Preference\nFinetuning-Finetuning using the reward model\nbased retrieval\nretrieval optimization\nretrieval-augmented generation (see RAG)\nretrievers\nquality evaluation, Comparing retrieval algorithms\nEngineering-Proprietary Prompts and Reverse Prompt Engineering\nRLHF (reinforcement learning from human feedback), Preference\nFinetuning-Finetuning using the reward model\nS4 architecture, Other model architectures\nscaling law, Scaling law: Building compute-optimal models-Scaling law:\nBuilding compute-optimal models\nself-evaluation, What Models Can Act as Judges?\nself-supervision language models, Self-supervision-Self-supervision\nsequential finetuning, Model Merging and Multi-Task Finetuning\nsimultaneous finetuning, Model Merging and Multi-Task Finetuning\nslicing, Annotate evaluation data\nsoft attributes, Model Selection Workflow\nsystem components evaluation, Step 1.\nUser Prompt\nsystems evaluation, Evaluate AI Systems-Summary\nstep 1: creating an evaluation guideline, Step 2.\nEvaluate All\nstep 3: defining evaluation methods and data, Step 3.\nEvaluation Methods and Data-Iterate\nmodel selection, Model Selection-Handling data contamination\nOpenAI model quality, Custom leaderboards with public benchmarks\ntask-based evaluation, Step 1.\ntime per output token (TPOT), Setting Expectations, Latency, TTFT, and\ntokenization, Multilingual Models, Model Size, Bits-per-Character and\nBits-per-Byte, Term-based retrieval, Chunking strategy\ndefined, Language models\ntokens, Language models, Model Size\nTPOT (time per output token), Setting Expectations, Latency, TTFT, and\ntraining, Modeling and training-Modeling and training\ntraining data, Training Data-Domain-Specific Models\nModels\nturn-based evaluation, Step 1.\nuse case evaluation, Use Case Evaluation-AI product defensibility\nvector database, Embedding-based retrieval-Embedding-based retrieval\ndefined, Language models\nThe animal on the cover of AI Engineering is an Omani owl (Strix butleri),\nAn owl collected in 1878 was dubbed Strix butleri after its discoverer,\nsame as Strix omanensis, and distinct from the more common owl found\nkept the original name Strix butleri and the more common owl was given",
      "keywords": [
        "reverse prompt engineering",
        "Prompt Engineering",
        "prompt",
        "Prompts prompt engineering",
        "prompt engineering tools",
        "reverse prompt",
        "Version Prompts prompt",
        "retrieval",
        "retrieval algorithms",
        "model",
        "Evaluate Prompt Engineering",
        "finetuning",
        "Version Prompts",
        "Comparing retrieval algorithms",
        "Reward model"
      ],
      "concepts": [
        "model",
        "prompt",
        "evaluation",
        "evaluating",
        "evaluate",
        "data",
        "feedback",
        "retrieval",
        "retrievers",
        "owl"
      ],
      "similar_chapters": [
        {
          "book": "AI Engineering Building Applications",
          "chapter": 47,
          "title": "",
          "score": 0.598,
          "base_score": 0.598,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 21,
          "title": "",
          "score": 0.558,
          "base_score": 0.558,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 10,
          "title": "",
          "score": 0.503,
          "base_score": 0.503,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 24,
          "title": "",
          "score": 0.486,
          "base_score": 0.486,
          "topic_boost": 0.0,
          "method": "sbert"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 12,
          "title": "",
          "score": 0.477,
          "base_score": 0.477,
          "topic_boost": 0.0,
          "method": "sbert"
        }
      ],
      "enriched_keywords": {
        "tfidf": [
          "prompt engineering",
          "retrieval",
          "prompt",
          "engineering",
          "evaluation"
        ],
        "semantic": [],
        "merged": [
          "prompt engineering",
          "retrieval",
          "prompt",
          "engineering",
          "evaluation"
        ]
      },
      "topic_id": null,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.35123843781337144,
        "topic_boost": 0.0,
        "timestamp": "2025-12-17T23:00:51.605273+00:00"
      }
    },
    {
      "chapter_number": 49,
      "title": "Segment 49 (pages 991-991)",
      "start_page": 991,
      "end_page": 991,
      "summary": "The cover illustration is by Karen Montgomery, based on an antique line\nThe cover",
      "keywords": [
        "IUCN conservation status",
        "IUCN conservation",
        "Omani owl",
        "Royal Natural History",
        "data deficient",
        "conservation status",
        "owl is data",
        "Karen Montgomery",
        "Adobe Myriad Condensed",
        "IUCN",
        "Omani",
        "Natural History",
        "Lydekker ’s Royal",
        "Royal Natural",
        "Montgomery"
      ],
      "concepts": [
        "mono",
        "guardian",
        "dalton",
        "data",
        "deficient",
        "natural",
        "fonts",
        "royal",
        "history",
        "freedman"
      ],
      "similar_chapters": [],
      "enriched_keywords": {
        "tfidf": [
          "montgomery",
          "conservation",
          "royal",
          "iucn",
          "natural"
        ],
        "semantic": [],
        "merged": [
          "montgomery",
          "conservation",
          "royal",
          "iucn",
          "natural"
        ]
      },
      "topic_id": 4,
      "chapter_provenance": {
        "methods_used": [
          "sbert",
          "tfidf",
          "bertopic"
        ],
        "sbert_score": 0.03311311030048132,
        "topic_boost": 0.15,
        "timestamp": "2025-12-17T23:00:51.605304+00:00"
      }
    }
  ],
  "total_chapters": 49,
  "enrichment_provenance": {
    "taxonomy_id": "none",
    "taxonomy_version": "none",
    "taxonomy_path": "none",
    "taxonomy_checksum": "sha256:none",
    "source_metadata_file": "AI Engineering Building Applications_metadata.json",
    "enrichment_date": "2025-12-17T23:00:51.615953+00:00",
    "enrichment_method": "msep",
    "model_version": "ai-agents-msep-v1",
    "processing_time_ms": 4682.982668000477,
    "total_similar_chapters": 240
  }
}